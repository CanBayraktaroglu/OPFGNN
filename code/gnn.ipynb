{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from helper import *\n",
    "from heterognn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The State of the nth node is expressed by 4 real scalars:\n",
    "\n",
    "v_n -> the voltage at the node\n",
    "delta_n -> the voltage angle at the node (relative to the slack bus)\n",
    "p_n -> the net active power flowing into the node\n",
    "q_n -> the net reactive power flowing into the node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical characteristics of the network are described by the power flow equations:\n",
    "\n",
    "p = P(v, delta, W)\n",
    "q = Q(v, delta, W)\n",
    "\n",
    "-> Relate local net power generation with the global state\n",
    "-> Depends on the topology W of the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electrical grid => Weighted Graph\n",
    "\n",
    "Nodes in the graph produce/consume power\n",
    "\n",
    "Edges represent electrical connections between nodes\n",
    "\n",
    "State Matrix X element of  R(N x 4) => graph signal with 4 features\n",
    "    => Each row is the state of the corresponding Node\n",
    "\n",
    "Adjacency Matrix A => sparse matrix to represent the connections of each node, element of R(N x N), Aij = 1 if node i is connected to node j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the GNN as a mode phi(X, A, H)\n",
    "\n",
    "We want to imitate the OPF solution p*\n",
    "-> We want to minimize a loss L over a dataset T = {{X, p*}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Function:min arg H of sum over T L(p*,phi(X, A, H)) and we use L = Mean Squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once GNN model phi is trained, we do not need the costly p* from pandapower to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input Data X - R^(Nx4): Uniformly sample p_ref and q_ref of each load L with P_L ~ Uniform(0.9 * p_ref, 1.1 * p_ref) and Q_L ~ Uniform(0.9 * q_ref, 1.1 * q_ref)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pseudocode for X and y in supervised learning:\n",
    "for each P_L and Q_L:\n",
    "    Create X with sub-optimal DCOPF results\n",
    "    Create y with Pandapower calculating p* ACOPF with IPOPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatio-Temporal GNN -> superposition of a gnn with spatial info and a temporal layer (Temporal Conv,LSTM etc.) ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bus => Node in GNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['1-complete_data-mixed-all-0-sw',\n '1-complete_data-mixed-all-1-sw',\n '1-complete_data-mixed-all-2-sw',\n '1-EHVHVMVLV-mixed-all-0-sw',\n '1-EHVHVMVLV-mixed-all-1-sw',\n '1-EHVHVMVLV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-0-sw',\n '1-EHVHV-mixed-all-0-no_sw',\n '1-EHVHV-mixed-all-1-sw',\n '1-EHVHV-mixed-all-1-no_sw',\n '1-EHVHV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-2-no_sw',\n '1-EHVHV-mixed-1-0-sw',\n '1-EHVHV-mixed-1-0-no_sw',\n '1-EHVHV-mixed-1-1-sw',\n '1-EHVHV-mixed-1-1-no_sw',\n '1-EHVHV-mixed-1-2-sw',\n '1-EHVHV-mixed-1-2-no_sw',\n '1-EHVHV-mixed-2-0-sw',\n '1-EHVHV-mixed-2-0-no_sw',\n '1-EHVHV-mixed-2-1-sw',\n '1-EHVHV-mixed-2-1-no_sw',\n '1-EHVHV-mixed-2-2-sw',\n '1-EHVHV-mixed-2-2-no_sw',\n '1-EHV-mixed--0-sw',\n '1-EHV-mixed--0-no_sw',\n '1-EHV-mixed--1-sw',\n '1-EHV-mixed--1-no_sw',\n '1-EHV-mixed--2-sw',\n '1-EHV-mixed--2-no_sw',\n '1-HVMV-mixed-all-0-sw',\n '1-HVMV-mixed-all-0-no_sw',\n '1-HVMV-mixed-all-1-sw',\n '1-HVMV-mixed-all-1-no_sw',\n '1-HVMV-mixed-all-2-sw',\n '1-HVMV-mixed-all-2-no_sw',\n '1-HVMV-mixed-1.105-0-sw',\n '1-HVMV-mixed-1.105-0-no_sw',\n '1-HVMV-mixed-1.105-1-sw',\n '1-HVMV-mixed-1.105-1-no_sw',\n '1-HVMV-mixed-1.105-2-sw',\n '1-HVMV-mixed-1.105-2-no_sw',\n '1-HVMV-mixed-2.102-0-sw',\n '1-HVMV-mixed-2.102-0-no_sw',\n '1-HVMV-mixed-2.102-1-sw',\n '1-HVMV-mixed-2.102-1-no_sw',\n '1-HVMV-mixed-2.102-2-sw',\n '1-HVMV-mixed-2.102-2-no_sw',\n '1-HVMV-mixed-4.101-0-sw',\n '1-HVMV-mixed-4.101-0-no_sw',\n '1-HVMV-mixed-4.101-1-sw',\n '1-HVMV-mixed-4.101-1-no_sw',\n '1-HVMV-mixed-4.101-2-sw',\n '1-HVMV-mixed-4.101-2-no_sw',\n '1-HVMV-urban-all-0-sw',\n '1-HVMV-urban-all-0-no_sw',\n '1-HVMV-urban-all-1-sw',\n '1-HVMV-urban-all-1-no_sw',\n '1-HVMV-urban-all-2-sw',\n '1-HVMV-urban-all-2-no_sw',\n '1-HVMV-urban-2.203-0-sw',\n '1-HVMV-urban-2.203-0-no_sw',\n '1-HVMV-urban-2.203-1-sw',\n '1-HVMV-urban-2.203-1-no_sw',\n '1-HVMV-urban-2.203-2-sw',\n '1-HVMV-urban-2.203-2-no_sw',\n '1-HVMV-urban-3.201-0-sw',\n '1-HVMV-urban-3.201-0-no_sw',\n '1-HVMV-urban-3.201-1-sw',\n '1-HVMV-urban-3.201-1-no_sw',\n '1-HVMV-urban-3.201-2-sw',\n '1-HVMV-urban-3.201-2-no_sw',\n '1-HVMV-urban-4.201-0-sw',\n '1-HVMV-urban-4.201-0-no_sw',\n '1-HVMV-urban-4.201-1-sw',\n '1-HVMV-urban-4.201-1-no_sw',\n '1-HVMV-urban-4.201-2-sw',\n '1-HVMV-urban-4.201-2-no_sw',\n '1-HV-mixed--0-sw',\n '1-HV-mixed--0-no_sw',\n '1-HV-mixed--1-sw',\n '1-HV-mixed--1-no_sw',\n '1-HV-mixed--2-sw',\n '1-HV-mixed--2-no_sw',\n '1-HV-urban--0-sw',\n '1-HV-urban--0-no_sw',\n '1-HV-urban--1-sw',\n '1-HV-urban--1-no_sw',\n '1-HV-urban--2-sw',\n '1-HV-urban--2-no_sw',\n '1-MVLV-rural-all-0-sw',\n '1-MVLV-rural-all-0-no_sw',\n '1-MVLV-rural-all-1-sw',\n '1-MVLV-rural-all-1-no_sw',\n '1-MVLV-rural-all-2-sw',\n '1-MVLV-rural-all-2-no_sw',\n '1-MVLV-rural-1.108-0-sw',\n '1-MVLV-rural-1.108-0-no_sw',\n '1-MVLV-rural-1.108-1-sw',\n '1-MVLV-rural-1.108-1-no_sw',\n '1-MVLV-rural-1.108-2-sw',\n '1-MVLV-rural-1.108-2-no_sw',\n '1-MVLV-rural-2.107-0-sw',\n '1-MVLV-rural-2.107-0-no_sw',\n '1-MVLV-rural-2.107-1-sw',\n '1-MVLV-rural-2.107-1-no_sw',\n '1-MVLV-rural-2.107-2-sw',\n '1-MVLV-rural-2.107-2-no_sw',\n '1-MVLV-rural-4.101-0-sw',\n '1-MVLV-rural-4.101-0-no_sw',\n '1-MVLV-rural-4.101-1-sw',\n '1-MVLV-rural-4.101-1-no_sw',\n '1-MVLV-rural-4.101-2-sw',\n '1-MVLV-rural-4.101-2-no_sw',\n '1-MVLV-semiurb-all-0-sw',\n '1-MVLV-semiurb-all-0-no_sw',\n '1-MVLV-semiurb-all-1-sw',\n '1-MVLV-semiurb-all-1-no_sw',\n '1-MVLV-semiurb-all-2-sw',\n '1-MVLV-semiurb-all-2-no_sw',\n '1-MVLV-semiurb-3.202-0-sw',\n '1-MVLV-semiurb-3.202-0-no_sw',\n '1-MVLV-semiurb-3.202-1-sw',\n '1-MVLV-semiurb-3.202-1-no_sw',\n '1-MVLV-semiurb-3.202-2-sw',\n '1-MVLV-semiurb-3.202-2-no_sw',\n '1-MVLV-semiurb-4.201-0-sw',\n '1-MVLV-semiurb-4.201-0-no_sw',\n '1-MVLV-semiurb-4.201-1-sw',\n '1-MVLV-semiurb-4.201-1-no_sw',\n '1-MVLV-semiurb-4.201-2-sw',\n '1-MVLV-semiurb-4.201-2-no_sw',\n '1-MVLV-semiurb-5.220-0-sw',\n '1-MVLV-semiurb-5.220-0-no_sw',\n '1-MVLV-semiurb-5.220-1-sw',\n '1-MVLV-semiurb-5.220-1-no_sw',\n '1-MVLV-semiurb-5.220-2-sw',\n '1-MVLV-semiurb-5.220-2-no_sw',\n '1-MVLV-urban-all-0-sw',\n '1-MVLV-urban-all-0-no_sw',\n '1-MVLV-urban-all-1-sw',\n '1-MVLV-urban-all-1-no_sw',\n '1-MVLV-urban-all-2-sw',\n '1-MVLV-urban-all-2-no_sw',\n '1-MVLV-urban-5.303-0-sw',\n '1-MVLV-urban-5.303-0-no_sw',\n '1-MVLV-urban-5.303-1-sw',\n '1-MVLV-urban-5.303-1-no_sw',\n '1-MVLV-urban-5.303-2-sw',\n '1-MVLV-urban-5.303-2-no_sw',\n '1-MVLV-urban-6.305-0-sw',\n '1-MVLV-urban-6.305-0-no_sw',\n '1-MVLV-urban-6.305-1-sw',\n '1-MVLV-urban-6.305-1-no_sw',\n '1-MVLV-urban-6.305-2-sw',\n '1-MVLV-urban-6.305-2-no_sw',\n '1-MVLV-urban-6.309-0-sw',\n '1-MVLV-urban-6.309-0-no_sw',\n '1-MVLV-urban-6.309-1-sw',\n '1-MVLV-urban-6.309-1-no_sw',\n '1-MVLV-urban-6.309-2-sw',\n '1-MVLV-urban-6.309-2-no_sw',\n '1-MVLV-comm-all-0-sw',\n '1-MVLV-comm-all-0-no_sw',\n '1-MVLV-comm-all-1-sw',\n '1-MVLV-comm-all-1-no_sw',\n '1-MVLV-comm-all-2-sw',\n '1-MVLV-comm-all-2-no_sw',\n '1-MVLV-comm-3.403-0-sw',\n '1-MVLV-comm-3.403-0-no_sw',\n '1-MVLV-comm-3.403-1-sw',\n '1-MVLV-comm-3.403-1-no_sw',\n '1-MVLV-comm-3.403-2-sw',\n '1-MVLV-comm-3.403-2-no_sw',\n '1-MVLV-comm-4.416-0-sw',\n '1-MVLV-comm-4.416-0-no_sw',\n '1-MVLV-comm-4.416-1-sw',\n '1-MVLV-comm-4.416-1-no_sw',\n '1-MVLV-comm-4.416-2-sw',\n '1-MVLV-comm-4.416-2-no_sw',\n '1-MVLV-comm-5.401-0-sw',\n '1-MVLV-comm-5.401-0-no_sw',\n '1-MVLV-comm-5.401-1-sw',\n '1-MVLV-comm-5.401-1-no_sw',\n '1-MVLV-comm-5.401-2-sw',\n '1-MVLV-comm-5.401-2-no_sw',\n '1-MV-rural--0-sw',\n '1-MV-rural--0-no_sw',\n '1-MV-rural--1-sw',\n '1-MV-rural--1-no_sw',\n '1-MV-rural--2-sw',\n '1-MV-rural--2-no_sw',\n '1-MV-semiurb--0-sw',\n '1-MV-semiurb--0-no_sw',\n '1-MV-semiurb--1-sw',\n '1-MV-semiurb--1-no_sw',\n '1-MV-semiurb--2-sw',\n '1-MV-semiurb--2-no_sw',\n '1-MV-urban--0-sw',\n '1-MV-urban--0-no_sw',\n '1-MV-urban--1-sw',\n '1-MV-urban--1-no_sw',\n '1-MV-urban--2-sw',\n '1-MV-urban--2-no_sw',\n '1-MV-comm--0-sw',\n '1-MV-comm--0-no_sw',\n '1-MV-comm--1-sw',\n '1-MV-comm--1-no_sw',\n '1-MV-comm--2-sw',\n '1-MV-comm--2-no_sw',\n '1-LV-rural1--0-sw',\n '1-LV-rural1--0-no_sw',\n '1-LV-rural1--1-sw',\n '1-LV-rural1--1-no_sw',\n '1-LV-rural1--2-sw',\n '1-LV-rural1--2-no_sw',\n '1-LV-rural2--0-sw',\n '1-LV-rural2--0-no_sw',\n '1-LV-rural2--1-sw',\n '1-LV-rural2--1-no_sw',\n '1-LV-rural2--2-sw',\n '1-LV-rural2--2-no_sw',\n '1-LV-rural3--0-sw',\n '1-LV-rural3--0-no_sw',\n '1-LV-rural3--1-sw',\n '1-LV-rural3--1-no_sw',\n '1-LV-rural3--2-sw',\n '1-LV-rural3--2-no_sw',\n '1-LV-semiurb4--0-sw',\n '1-LV-semiurb4--0-no_sw',\n '1-LV-semiurb4--1-sw',\n '1-LV-semiurb4--1-no_sw',\n '1-LV-semiurb4--2-sw',\n '1-LV-semiurb4--2-no_sw',\n '1-LV-semiurb5--0-sw',\n '1-LV-semiurb5--0-no_sw',\n '1-LV-semiurb5--1-sw',\n '1-LV-semiurb5--1-no_sw',\n '1-LV-semiurb5--2-sw',\n '1-LV-semiurb5--2-no_sw',\n '1-LV-urban6--0-sw',\n '1-LV-urban6--0-no_sw',\n '1-LV-urban6--1-sw',\n '1-LV-urban6--1-no_sw',\n '1-LV-urban6--2-sw',\n '1-LV-urban6--2-no_sw']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get lists of simbench codes\n",
    "all_simbench_codes = sb.collect_all_simbench_codes()\n",
    "all_simbench_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'/'.join(os.path.dirname(os.path.abspath(__file__).split(\"\\\\\"))) + r\"/Models/SelfSupervised/base_model.pt\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net.bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_json('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'\n",
    "#Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "#TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "#TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "#NETWORK CONSTRAINTS\n",
    "\n",
    "#Maximize the branch limits\n",
    "\n",
    "#max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "#for i in range(len(max_i_ka)):\n",
    "# max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "#Maximize line loading percents\n",
    "max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo loading percent\n",
    "max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo3w loading percent\n",
    "max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Cost assignment\n",
    "\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "pp.runopp(net,verbose=True)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = read_unsupervised_dataset('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_data[0].has_isolated_nodes()\n",
    "#train_data[0].has_self_loops()\n",
    "#train_data[0].is_undirected()\n",
    "x_dict = train_data[0].to_dict()\n",
    "#to_json(train_dict)\n",
    "ln = len(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"][0])\n",
    "print(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"])#[:, :int(ln/2)]\n",
    "x_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')\n",
    "idx_mapper, node_types_as_dict = extract_node_types_as_dict(net)\n",
    "for key in node_types_as_dict:\n",
    "    print(f\"Bus Type: {key}\")\n",
    "    for i in range(len(node_types_as_dict[key])):\n",
    "        #node_types_as_dict[key][i] = idx_mapper[node_types_as_dict[key][i]]\n",
    "        print(str(node_types_as_dict[key][i]))\n",
    "    print(\"-------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_dict[\"PQ\"]['x'][2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_available = []\n",
    "for nw_name in all_simbench_codes:\n",
    "        net = sb.get_simbench_net(nw_name)\n",
    "        print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "        #dict_probs = pp.diagnostic(net,report_style='None')\n",
    "        #for bus_num in dict_probs['multiple_voltage_controlling_elements_per_bus']['buses_with_gens_and_ext_grids']:\n",
    "        #    net.gen = net.gen.drop(net.gen[net.gen.bus == bus_num].index)\n",
    "\n",
    "        #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "        #Set upper and lower limits of active-reactive powers of loads\n",
    "        min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "        p_mw = list(net.load.p_mw.values)\n",
    "        q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "        for i in range(len(p_mw)):\n",
    "            min_p_mw_val.append(p_mw[i])\n",
    "            max_p_mw_val.append(p_mw[i])\n",
    "            min_q_mvar_val.append(q_mvar[i])\n",
    "            max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "        net.load.min_p_mw = min_p_mw_val\n",
    "        net.load.max_p_mw = max_p_mw_val\n",
    "        net.load.min_q_mvar = min_q_mvar_val\n",
    "        net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "        #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "        ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "        pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "        #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "        #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "        #NETWORK CONSTRAINTS\n",
    "\n",
    "        #Maximize the branch limits\n",
    "\n",
    "        #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "        #for i in range(len(max_i_ka)):\n",
    "        # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "        #Maximize line loading percents\n",
    "        max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo loading percent\n",
    "        max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo3w loading percent\n",
    "        max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Cost assignment\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "        try:\n",
    "            pp.runpm_dc_opf(net) # Run DCOPP\n",
    "        except pp.OPFNotConverged:\n",
    "            text = \"DC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "            print(text)\n",
    "            continue\n",
    "        print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR DCOPF\")\n",
    "        grids_available.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_available:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) # dcopp on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_ready = []\n",
    "for nw_name in all_simbench_codes[6:]:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "    #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "    #Set upper and lower limits of active-reactive powers of loads\n",
    "    min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "    p_mw = list(net.load.p_mw.values)\n",
    "    q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "    for i in range(len(p_mw)):\n",
    "        min_p_mw_val.append(p_mw[i])\n",
    "        max_p_mw_val.append(p_mw[i])\n",
    "        min_q_mvar_val.append(q_mvar[i])\n",
    "        max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "    net.load.min_p_mw = min_p_mw_val\n",
    "    net.load.max_p_mw = max_p_mw_val\n",
    "    net.load.min_q_mvar = min_q_mvar_val\n",
    "    net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "    #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "    ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "    pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "    #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "    #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "    #NETWORK CONSTRAINTS\n",
    "\n",
    "    #Maximize the branch limits\n",
    "\n",
    "    #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "    #for i in range(len(max_i_ka)):\n",
    "    # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "    #Maximize line loading percents\n",
    "    max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo loading percent\n",
    "    max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo3w loading percent\n",
    "    max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Cost assignment\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "    #ac_converged = True\n",
    "\n",
    "    #start_vec_name = \"\"\n",
    "    #for init in [\"pf\", \"flat\", \"results\"]:\n",
    "    #    try:\n",
    "    #        pp.runopp(net, init=init)  # Calculate ACOPF with IPFOPT\n",
    "    #    except pp.OPFNotConverged:\n",
    "    #        if init == \"results\":\n",
    "    #            text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \". SKIPPING THIS GRID.\"\n",
    "    #            print(text)\n",
    "    #            break\n",
    "    #        continue\n",
    "    #    start_vec_name = init\n",
    "    #    ac_converged = True\n",
    "    #    break\n",
    "    #if ac_converged:\n",
    "    #    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + start_vec_name + \".\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        pp.runpm_ac_opf(net) # Run DCOPP\n",
    "    except pp.OPFNotConverged:\n",
    "        text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "        print(text)\n",
    "        continue\n",
    "    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + \".\")\n",
    "    grids_ready.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_acopf_available_grid_names = [\"1-HV-mixed--0-no_sw\",\"1-HV-urban--0-no_sw\", \"1-MV-comm--0-no_sw\", \"1-MV-semiurb--0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_acopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf_and_acopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_available_grid_names = [\"1-HVMV-mixed-all-0-no_sw\", \"1-HVMV-mixed-1.105-0-no_sw\", \"1-HVMV-mixed-2.102-0-no_sw\",\"1-HVMV-mixed-4.101-0-no_sw\", \"1-HVMV-urban-all-0-no_sw\", \"1-HVMV-urban-2.203-0-no_sw\", \"1-HVMV-urban-3.201-0-no_sw\", \"1-HVMV-urban-4.201-0-no_sw\", \"1-HV-mixed--0-no_sw\", \"1-HV-urban--0-no_sw\", \"1-MVLV-rural-all-0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Unsupervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this revised version of the compute_node_embeddings function, the node features are weighted by the reverse admittance values in the adjacency matrix before they are summed to compute the node embeddings. The resulting node embeddings will reflect the strength of the connections between the nodes.\n",
    "\n",
    "To use the reverse admittance values as the edge weights, you would need to pass the Ybus matrix as the adjacency matrix when calling the compute_node_embeddings function. The Ybus matrix should be converted to a PyTorch tensor before passing it to the function.\n",
    "\n",
    "use the reverse admittance values as edge weights, you can modify the computation of the node embeddings to weight the node features by the reverse admittance values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the node types\n",
    "node_types = ['Slack Node', 'Generator Node', 'Load Node']\n",
    "\n",
    "# Define the number of nodes of each type in the graph\n",
    "num_nodes = {\n",
    "    'Slack Node': 1,\n",
    "    'Generator Node': 20,\n",
    "    'Load Node': 99\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_names = [_ for _ in os.listdir(os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\\")]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphdata_lst = read_multiple_supervised_datasets(grid_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(4, 256, num_layers, 4, dropout=0.0, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=\"last\",layer_type=\"TransConv\", activation=\"elu\")#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_datasets_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "    run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"OPF-GNN-Supervised-Homo\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Homo-GNN\",\n",
    "    \"num_layers\": num_layers,\n",
    "    \"dataset\": grid_names[i],\n",
    "    \"epochs\": 1000,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"output_channels\": out_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"channel type\": layer_type,\n",
    "    \"norm\": \"BatchNorm\",\n",
    "    \"scaler\": scaler\n",
    "\n",
    "    }\n",
    "    )\n",
    "    num_epochs = wandb.run.config.epochs\n",
    "    run.watch(model)\n",
    "    grid_name = graphdata.grid_name\n",
    "    run.config.dataset = grid_name\n",
    "    train_data = graphdata.train_data\n",
    "    run.config[\"number of busses\"] = np.shape(train_data[0].x)[0]\n",
    "    val_data = graphdata.val_data\n",
    "    test_data = graphdata.test_data\n",
    "    test_datasets_lst.append(test_data)\n",
    "    training_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    validation_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "    #test_loader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "    for _ in range(num_epochs):\n",
    "        #train_one_epoch(i, optimizer, training_loader, model, nn.MSELoss(), edge_index, edge_weights)\n",
    "        train_validate_one_epoch(_, grid_name, optimizer, training_loader, validation_loader, model, nn.MSELoss(), scaler)\n",
    "    print(\"Training and Validation finished \" + \"for GraphData \" + str(i) + \".\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader_lst = []\n",
    "val_loader_lst = []\n",
    "test_loader_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "\n",
    "    # Divide training data into chunks, load into Dataloaders and append to the list of training loaders\n",
    "    for train_data in divide_chunks(graphdata.train_data, 5):\n",
    "        train_loader_lst.append(DataLoader(train_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Divide validation data into chunks, load into Dataloaders and append to the list of validation loaders\n",
    "    for val_data in divide_chunks(graphdata.val_data, 5):\n",
    "        val_loader_lst.append(DataLoader(val_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Append the test data to the list\n",
    "    for test_data in divide_chunks(graphdata.test_data, 5):\n",
    "        test_loader_lst.append(DataLoader(test_data, batch_size=1, shuffle=True))\n",
    "\n",
    "print(\"Data Preparation finished.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr/100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"OPF-GNN-Supervised-Homo\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Homo-GNN\",\n",
    "    \"num_layers\": num_layers,\n",
    "    \"epochs\": 2000,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"output_channels\": out_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"channel type\": layer_type,\n",
    "    \"norm\": \"BatchNorm\",\n",
    "    \"scaler\": scaler\n",
    "    }\n",
    "    )\n",
    "\"\"\"\n",
    "for _ in range(wandb.run.config.epochs):\n",
    "    # Training\n",
    "    random.shuffle(train_loader_lst)\n",
    "    print(\"Training the model for epoch \" + str(_))\n",
    "    train_all_one_epoch(_, optimizer, train_loader_lst, model, nn.MSELoss())\n",
    "\n",
    "    # Validation\n",
    "    random.shuffle(val_loader_lst)\n",
    "    print(\"Validating the model for epoch \" + str(_))\n",
    "    validate_all_one_epoch(_, val_loader_lst, model, nn.MSELoss())\n",
    "\n",
    "    outputs = test_all_one_epoch(test_loader_lst, model, nn.MSELoss())\n",
    "print(\"Training and Validation finished.\")\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = test_all_one_epoch(test_loader_lst, model, nn.MSELoss())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output,target = outputs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\" #os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\Models\\\\Supervised\\\\\" + \"basemodel.pt\" #r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\"\n",
    "torch.save(model.state_dict(), \"supervisedmodel.pt\")\n",
    "print(\"done\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'/'.join(os.path.dirname(os.path.abspath(\"gnn.ipynb\")).split(\"\\\\\"))+ r\"/Models/SelfSupervised/base_model.pt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandapower.plotting.simple_plot import simple_plot\n",
    "from pandapower.plotting.plotly.simple_plotly import simple_plotly\n",
    "#simple_plot(net, plot_gens=True, plot_loads=True, plot_sgens=True, library=\"igraph\")\n",
    "simple_plotly(net, map_style=\"satellite\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runopp(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx_mapper, node_types_as_dict = extract_node_types_as_dict(net)\n",
    "node_types_as_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Module.parameters of GNN(4, 4, num_layers=5)>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "net = process_network(grid_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " HETEROGENEOUS GNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "index_mappers,net,data = generate_unsupervised_input('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 380.,    0.,    0.,    0.,  342.,  418.,  350., -350.,  350., -350.,\n          350.]], dtype=torch.float64, grad_fn=<StackBackward0>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"SB\"].x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_ACOPFGNN_model(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    }
   ],
   "source": [
    "x_dict, constraint_dict, edge_idx_dict, edge_attr_dict, bus_idx_neighbors_dict, scaler, angle_params, res_bus_dict = extract_unsupervised_inputs(data, net, index_mappers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'SB': tensor([[ 5.3624, -1.1287,  0.1318,  0.1443]], requires_grad=True),\n 'PQ': tensor([[-0.2242, -0.9261,  0.1855,  0.1735],\n         [-0.2242, -0.9261,  0.1876,  0.1740],\n         [-0.2242, -0.9261,  0.5085,  0.3566],\n         [-0.2242, -0.9261,  0.8787,  0.5507],\n         [-0.2242, -0.9261,  0.1823,  0.1737],\n         [-0.2242, -0.9261,  0.1851,  0.1765],\n         [-0.2242, -0.9261,  0.7988,  0.5809],\n         [-0.2242, -0.9261,  0.1863,  0.1734],\n         [-0.2242, -0.9261,  0.1867,  0.1750],\n         [-0.2242, -0.9261,  0.1859,  0.1759],\n         [-0.2242, -0.9261,  0.5017,  0.3234],\n         [-0.2242, -0.9261,  0.4428,  0.3478],\n         [-0.2242, -0.9261,  0.1850,  0.1719],\n         [-0.2242, -0.9261,  0.1877,  0.1745],\n         [-0.2242, -0.9261,  0.1855,  0.1764]], requires_grad=True),\n 'PV': tensor([[ 5.3624e+00, -9.2614e-01, -2.8817e+00, -4.3336e+00],\n         [ 2.0518e+00, -9.2614e-01, -2.4512e+00, -3.6939e+00],\n         [-2.2418e-01, -9.2614e-01,  5.7386e-02,  1.0016e-01],\n         [-2.2418e-01, -9.2614e-01, -7.4906e-02,  2.2673e-02],\n         [-2.2418e-01, -9.2614e-01,  2.2714e-02,  8.2273e-02],\n         [-2.2418e-01, -9.2614e-01, -1.1516e+00, -6.0946e-01],\n         [-2.2418e-01, -9.2614e-01, -2.9466e-01, -1.0793e-01],\n         [-2.2418e-01, -9.2614e-01, -1.4915e+00, -8.1052e-01],\n         [-2.2418e-01, -9.2614e-01, -2.0738e-01, -5.5428e-02],\n         [-2.2418e-01, -9.2614e-01, -7.6784e-03,  6.2356e-02],\n         [-2.2418e-01, -9.2614e-01, -3.4897e-01, -1.4393e-01],\n         [-2.2418e-01, -9.2614e-01, -1.1272e-01,  1.8092e-03],\n         [-2.2418e-01, -9.2614e-01, -9.8136e-01, -5.0980e-01],\n         [-2.2418e-01, -9.2614e-01, -3.0475e-01, -1.0172e-01],\n         [-2.2418e-01, -9.2614e-01,  7.5322e-02,  1.1110e-01],\n         [-2.2418e-01, -9.2614e-01, -5.7378e-01, -2.6986e-01],\n         [-2.2418e-01, -9.2614e-01,  3.7141e-02,  8.9300e-02],\n         [-2.2418e-01, -9.2614e-01, -4.2391e-02,  4.2921e-02],\n         [-2.2418e-01, -9.2614e-01, -6.9198e-01, -3.4512e-01],\n         [-2.2418e-01, -9.2614e-01, -2.5587e-02,  5.1840e-02],\n         [-2.2418e-01, -9.2614e-01, -6.1336e-01, -2.8357e-01],\n         [-2.2418e-01, -9.2614e-01, -4.4179e-01, -1.9509e-01],\n         [-2.2418e-01, -9.2614e-01,  6.1374e-02,  1.0291e-01],\n         [-2.2418e-01, -9.2614e-01,  2.2123e-02,  7.7446e-02],\n         [-2.2418e-01, -9.2614e-01, -2.9866e-01, -9.7611e-02],\n         [-2.2418e-01, -9.2614e-01,  6.1353e-02,  1.0200e-01],\n         [-2.2418e-01, -9.2614e-01,  5.5587e-02,  9.9328e-02],\n         [-2.2418e-01, -9.2614e-01,  3.4563e-02,  8.8339e-02],\n         [-2.2418e-01, -9.2614e-01, -2.1648e-01, -6.0706e-02],\n         [-2.2418e-01, -9.2614e-01, -3.2151e-01, -1.2195e-01],\n         [-2.2418e-01, -9.2614e-01,  5.1375e-02,  9.8303e-02],\n         [-2.2418e-01, -9.2614e-01,  5.2180e-02,  9.6424e-02],\n         [-2.2418e-01, -9.2614e-01, -2.4821e-02,  5.3573e-02],\n         [-2.2418e-01, -9.2614e-01, -3.4189e-01, -1.3095e-01],\n         [-2.2418e-01, -9.2614e-01,  3.0583e-02,  8.4228e-02],\n         [-2.2418e-01, -9.2614e-01, -3.5932e-01, -1.4345e-01],\n         [-2.2418e-01, -9.2614e-01,  1.1315e-02,  7.3480e-02],\n         [-2.2418e-01, -9.2614e-01,  3.2260e-02,  8.5170e-02],\n         [-2.2418e-01, -9.2614e-01,  5.2399e-02,  9.6106e-02],\n         [-2.2418e-01, -9.2614e-01, -4.1604e-02,  4.3883e-02],\n         [-2.2418e-01, -9.2614e-01,  1.1319e-03,  6.8834e-02],\n         [-2.2418e-01, -9.2614e-01, -9.2208e-02,  1.1550e-02],\n         [-2.2418e-01, -9.2614e-01, -1.0091e-02,  6.1869e-02]],\n        requires_grad=True),\n 'NB': tensor([[-0.2242, -0.9261,  0.1318,  0.1443],\n         [-0.2242, -0.9261,  0.1318,  0.1443],\n         [-0.2242, -0.9261,  0.1318,  0.1443],\n         [-0.2242, -0.9261,  0.1318,  0.1443],\n         [-0.2242, -0.9261,  0.1318,  0.1443]], requires_grad=True)}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(torch.Tensor,\n            {'PQ': tensor([[-0.9727,  0.4975,  0.1855,  0.1735],\n                     [-0.9881, -0.3441,  0.1876,  0.1740],\n                     [-0.9433, -0.1555,  0.5085,  0.3566],\n                     [-0.8736, -0.6474,  0.8787,  0.5507],\n                     [-0.9996,  2.1459,  0.1823,  0.1737],\n                     [-0.7947, -0.3202,  0.1851,  0.1765],\n                     [-1.0000,  6.4361,  0.7988,  0.5809],\n                     [-0.8129, -0.4006,  0.1863,  0.1734],\n                     [-0.9954,  4.7195,  0.1867,  0.1750],\n                     [-1.0000,  3.5074,  0.1859,  0.1759],\n                     [-0.9077, -0.3457,  0.5017,  0.3234],\n                     [-0.9938,  0.8982,  0.4428,  0.3478],\n                     [ 0.1649,  0.0599,  0.1850,  0.1719],\n                     [-0.9517,  1.1979,  0.1877,  0.1745],\n                     [-0.9716,  0.3346,  0.1855,  0.1764]], grad_fn=<CatBackward0>),\n             'NB': tensor([[-0.5042, -0.9679,  0.1318,  0.1443],\n                     [-0.4228, -0.5739,  0.1318,  0.1443],\n                     [-0.0776, -0.9174,  0.1318,  0.1443],\n                     [-0.9629, -0.8821,  0.1318,  0.1443],\n                     [-0.7104, -0.5152,  0.1318,  0.1443]], grad_fn=<CatBackward0>),\n             'SB': tensor([[-0.7722,  0.6120,  0.1318,  0.1443]], grad_fn=<CatBackward0>),\n             'PV': tensor([[-9.7774e-01, -8.5157e-01, -2.8817e+00, -4.3336e+00],\n                     [-4.5090e-01, -7.3275e-01, -2.4512e+00, -3.6939e+00],\n                     [-8.8442e-01,  2.3983e+00,  5.7386e-02,  1.0016e-01],\n                     [-2.7672e-01,  9.2331e-01, -7.4906e-02,  2.2673e-02],\n                     [-1.6263e-01,  8.0254e-01,  2.2714e-02,  8.2273e-02],\n                     [-9.9844e-01,  1.2012e+01, -1.1516e+00, -6.0946e-01],\n                     [-4.7029e-02,  7.0152e-01, -2.9466e-01, -1.0793e-01],\n                     [-8.0817e-01,  5.4861e+00, -1.4915e+00, -8.1052e-01],\n                     [-6.1014e-01,  2.7211e+00, -2.0738e-01, -5.5428e-02],\n                     [-1.5199e-01,  7.9273e-01, -7.6784e-03,  6.2356e-02],\n                     [-2.5229e-02,  6.8396e-01, -3.4897e-01, -1.4393e-01],\n                     [-8.9097e-01,  2.1765e+00, -1.1272e-01,  1.8092e-03],\n                     [ 2.3027e-01,  4.8371e-01, -9.8136e-01, -5.0980e-01],\n                     [-4.4762e-02,  6.9927e-01, -3.0475e-01, -1.0172e-01],\n                     [-4.4939e-01, -9.5437e-02,  7.5322e-02,  1.1110e-01],\n                     [-9.5586e-01,  5.3520e+00, -5.7378e-01, -2.6986e-01],\n                     [-9.7663e-01,  3.5803e+00,  3.7141e-02,  8.9300e-02],\n                     [-8.7474e-01,  4.7665e+00, -4.2391e-02,  4.2921e-02],\n                     [-4.3252e-01,  2.7053e+00, -6.9198e-01, -3.4512e-01],\n                     [-6.1359e-01,  1.3603e+00, -2.5587e-02,  5.1840e-02],\n                     [-5.7900e-01,  1.2790e+00, -6.1336e-01, -2.8357e-01],\n                     [ 1.8788e-01,  5.6472e-01, -4.4179e-01, -1.9509e-01],\n                     [-9.8367e-01,  6.0517e+00,  6.1374e-02,  1.0291e-01],\n                     [-5.9596e-01,  2.7398e+00,  2.2123e-02,  7.7446e-02],\n                     [-4.3141e-01,  2.3136e+00, -2.9866e-01, -9.7611e-02],\n                     [-3.5326e-01,  1.8666e+00,  6.1353e-02,  1.0200e-01],\n                     [-8.4697e-01,  2.1156e+00,  5.5587e-02,  9.9328e-02],\n                     [-2.0836e-01,  8.7470e-01,  3.4563e-02,  8.8339e-02],\n                     [-9.2869e-01,  5.0721e+00, -2.1648e-01, -6.0706e-02],\n                     [-7.7225e-01,  1.7022e+00, -3.2151e-01, -1.2195e-01],\n                     [-2.9127e-01,  1.8119e+00,  5.1375e-02,  9.8303e-02],\n                     [-2.7762e-01,  9.3781e-01,  5.2180e-02,  9.6424e-02],\n                     [ 9.2224e-02,  6.7261e-01, -2.4821e-02,  5.3573e-02],\n                     [-8.4763e-01,  2.0518e+00, -3.4189e-01, -1.3095e-01],\n                     [ 3.0258e-01,  5.1016e-01,  3.0583e-02,  8.4228e-02],\n                     [-9.7691e-01,  3.2098e+00, -3.5932e-01, -1.4345e-01],\n                     [ 2.4285e-01,  5.4805e-01,  1.1315e-02,  7.3480e-02],\n                     [-1.0202e-01,  8.3838e-01,  3.2260e-02,  8.5170e-02],\n                     [-4.0031e-01,  1.0560e+00,  5.2399e-02,  9.6106e-02],\n                     [ 6.5637e-02,  7.8332e-01, -4.1604e-02,  4.3883e-02],\n                     [-1.5517e-01,  7.9563e-01,  1.1319e-03,  6.8834e-02],\n                     [-1.2226e-01,  7.6581e-01, -9.2208e-02,  1.1550e-02],\n                     [-1.5127e-01,  7.9204e-01, -1.0091e-02,  6.1869e-02]],\n                    grad_fn=<CatBackward0>)})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder_model(x_dict, constraint_dict, edge_idx_dict, edge_attr_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0.23723669614964715"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.167**2 + 0.1685**2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 342.       418.       350.      -350.       350.      -350.\n",
      "   350.00003]]\n",
      "[[ 99.        121.          3.1069336   2.8781586   2.8781586   1.1700897\n",
      "    1.1700897]\n",
      " [ 99.        121.          3.257019    3.0235138   3.0235138   1.2110443\n",
      "    1.2110443]\n",
      " [ 99.        121.         37.16533     8.023071   34.423065    3.5772247\n",
      "   14.011185 ]\n",
      " [ 99.        121.         51.020508   33.0876     46.6476     15.307114\n",
      "   20.66623  ]\n",
      " [ 99.        121.          3.0006409   2.704483    2.704483    1.2998047\n",
      "    1.2998047]\n",
      " [ 99.        121.          3.2549133   3.059372    3.059372    1.1112213\n",
      "    1.1112213]\n",
      " [ 99.        121.         49.814194   31.622696   45.182693   15.616501\n",
      "   20.975632 ]\n",
      " [ 99.        121.          3.423462    3.208374    3.208374    1.1942902\n",
      "    1.1942902]\n",
      " [ 99.        121.          3.0209045   2.8138428   2.8138428   1.0991058\n",
      "    1.0991058]\n",
      " [ 99.        121.          3.4769897   3.2351685   3.2351685   1.2740631\n",
      "    1.2740631]\n",
      " [ 99.        121.         31.884735    5.5887756  29.388763    2.9603271\n",
      "   12.366684 ]\n",
      " [ 99.        121.         32.33777     6.388916   30.18892     2.185028\n",
      "   11.5914   ]\n",
      " [ 99.        121.          3.4465942   3.223648    3.223648    1.2194519\n",
      "    1.2194519]\n",
      " [ 99.        121.          2.9874878   2.776535    2.776535    1.1026306\n",
      "    1.1026306]\n",
      " [ 99.        121.          2.9378662   2.7017365   2.7017365   1.1539612\n",
      "    1.1539612]]\n",
      "[[ 3.42000000e+02  4.18000000e+02  3.50000000e+02 -3.50000000e+02\n",
      "  -1.52587891e-05 -3.50000000e+02  1.52587891e-05]\n",
      " [ 1.98000000e+02  2.42000000e+02  3.00000000e+02 -3.00000000e+02\n",
      "  -1.52587891e-05 -3.00000000e+02  1.52587891e-05]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.10754395e+00 -5.67999268e+00\n",
      "  -2.70428467e+00 -1.25527954e+00 -2.24487305e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  2.23441010e+01 -2.07800140e+01\n",
      "  -2.90278625e+00 -1.30058289e+00 -8.21281433e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.02581177e+01 -9.54000854e+00\n",
      "  -3.05505371e+00 -1.07026672e+00 -3.77052307e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.60280014e+02 -1.49059998e+02\n",
      "  -1.52587891e-05  1.52587891e-05 -5.89134369e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.39892426e+01 -3.16100006e+01\n",
      "  -1.89137115e+01 -6.66284180e+00 -1.24929810e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.70806427e+02 -1.58850006e+02\n",
      "  -3.03179932e+01 -1.35118561e+01 -6.27813721e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.90108032e+01 -3.62799988e+01\n",
      "  -3.11334229e+00 -1.28262329e+00 -1.43388977e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.74194183e+01 -1.61999969e+01\n",
      "  -1.52587891e-05  1.52587891e-05 -6.40275574e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  4.15376129e+01 -3.86300049e+01\n",
      "  -1.68571930e+01 -7.40226746e+00 -1.52674561e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  2.70538025e+01 -2.51600037e+01\n",
      "  -3.01489258e+00 -1.22120667e+00 -9.94395447e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.35645020e+02 -1.26150009e+02\n",
      "  -3.19332886e+00 -1.18431091e+00 -4.98572235e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.43118134e+01 -3.19100037e+01\n",
      "  -1.65105896e+01 -6.15379333e+00 -1.26115570e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  7.05377197e+00 -6.56001282e+00\n",
      "  -1.52587891e-05  1.52587891e-05 -2.59266663e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.78279114e+01 -6.30800095e+01\n",
      "  -1.76498260e+01 -6.88507080e+00 -2.49306335e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  8.46238708e+00 -7.87001038e+00\n",
      "  -3.05123901e+00 -1.14131165e+00 -3.11041260e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.84516144e+01 -1.71600037e+01\n",
      "  -3.27821350e+00 -1.17807007e+00 -6.78202820e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.36668549e+01 -4.92400055e+01\n",
      "  -4.84744797e+01 -2.13436737e+01 -1.94607849e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.96558990e+01 -1.82799988e+01\n",
      "  -1.52587891e-05  1.52587891e-05 -7.22467041e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  7.37203979e+01 -6.85599976e+01\n",
      "  -1.61134033e+01 -7.38481140e+00 -2.70965576e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.32795105e+01 -4.95500031e+01\n",
      "  -1.73746796e+01 -6.95779419e+00 -1.95832214e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  8.79570007e+00 -8.18000793e+00\n",
      "  -1.52587891e-05  1.52587891e-05 -3.23294067e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.07419128e+01 -9.99002075e+00\n",
      "  -2.93855286e+00 -1.27900696e+00 -3.94818115e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.36128845e+01 -3.12600098e+01\n",
      "  -1.58906250e+01 -6.95913696e+00 -1.23546753e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.44085693e+00 -5.05999756e+00\n",
      "  -2.73703003e+00 -1.09313965e+00 -1.99983215e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.18280029e+00 -5.75000000e+00\n",
      "  -3.27565002e+00 -1.08125305e+00 -2.27253723e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  8.92474365e+00 -8.30001831e+00\n",
      "  -3.24577332e+00 -1.19587708e+00 -3.28036499e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  4.01828156e+01 -3.73700104e+01\n",
      "  -2.94935608e+00 -1.19841003e+00 -1.47695618e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.66129150e+01 -5.26500092e+01\n",
      "  -1.52587891e-05  1.52587891e-05 -2.08085938e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.80645752e+00 -6.33000183e+00\n",
      "  -2.77929688e+00 -1.22323608e+00 -2.50175476e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.93548584e+00 -6.44999695e+00\n",
      "  -2.98042297e+00 -1.17948914e+00 -2.54917908e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.62688141e+01 -1.51300049e+01\n",
      "  -3.23254395e+00 -1.09085083e+00 -5.97970581e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  4.12580109e+01 -3.83700104e+01\n",
      "  -1.73695374e+01 -6.78463745e+00 -1.51645966e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  9.63442993e+00 -8.96000671e+00\n",
      "  -3.09129333e+00 -1.14685059e+00 -3.54119873e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.80753021e+01 -5.40100021e+01\n",
      "  -3.20039368e+00 -1.29483032e+00 -2.13461914e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.20108032e+01 -1.11700134e+01\n",
      "  -3.07546997e+00 -1.19583130e+00 -4.41477966e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  9.04301453e+00 -8.41000366e+00\n",
      "  -2.99108887e+00 -1.26058960e+00 -3.32382202e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.84945679e+00 -6.36999512e+00\n",
      "  -3.14657593e+00 -1.17970276e+00 -2.51757812e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.82687988e+01 -1.69900055e+01\n",
      "  -3.12025452e+00 -1.22277832e+00 -6.71482849e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.29140167e+01 -1.20100098e+01\n",
      "  -2.92842102e+00 -1.26052856e+00 -4.74670410e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  2.47419128e+01 -2.30100098e+01\n",
      "  -2.88661194e+00 -1.24166870e+00 -9.09402466e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.44839020e+01 -1.34700165e+01\n",
      "  -3.08052063e+00 -1.30276489e+00 -5.32376099e+00]]\n",
      "[[ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]]\n"
     ]
    }
   ],
   "source": [
    "for node_type in constraint_dict:\n",
    "    print(custom_standard_inverse_transform(scaler, constraint_dict[node_type].detach().numpy()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ACOPFOutput(x_dict, scalers_dict, None, index_mappers).output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_names = sb.collect_all_simbench_codes()[25:]\n",
    "acopf_ok_grid_names = []\n",
    "\n",
    "for grid_name in grid_names:\n",
    "    print(f\"Trying grid {grid_name}\")\n",
    "    net = process_network(grid_name)\n",
    "\n",
    "    # try:\n",
    "    #     acopf_ok_grid_names.append(grid_name)\n",
    "    #     pp.runpm_ac_opf(net)\n",
    "    #     #print(net.res_bus)\n",
    "    # except:\n",
    "    #     print(f\"Julia didnt converge for {grid_name}\")\n",
    "    #     acopf_ok_grid_names.remove(grid_name)\n",
    "    try:\n",
    "        acopf_ok_grid_names.append(grid_name)\n",
    "        pp.runopp(net)\n",
    "        #print(net.res_bus)\n",
    "    except:\n",
    "        print(f\"OPP didnt converge for {grid_name}\")\n",
    "        acopf_ok_grid_names.remove(grid_name)\n",
    "acopf_ok_grid_names\n",
    "#save_multiple_unsupervised_inputs(grid_names, 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "data": {
      "text/plain": "        vm_pu   va_degree      p_mw     q_mvar     lam_p         lam_q\n0    1.025000    0.000000 -8.057173 -11.835759  1.000000  2.259515e-21\n2    1.001425  209.119600 -1.176327   0.135443  1.000714  1.887024e-03\n4    0.999942  209.185313  0.397800   0.183689  1.002495  4.950700e-03\n5    0.999154  209.227646  0.293150   0.148678  1.003357  6.811706e-03\n6    0.998564  209.265084  0.239906   0.171257  1.003935  8.382988e-03\n..        ...         ...       ...        ...       ...           ...\n112  0.987459  209.245271  0.237492   0.096853  1.020719  1.774877e-02\n113  0.987333  209.251486  0.036984   0.085200  1.020904  1.802879e-02\n114  0.987261  209.254787 -0.074431   0.034476  1.021013  1.818089e-02\n115  0.987149  209.258063  0.118649   0.137547  1.021198  1.835478e-02\n116  0.987060  209.260287  0.159486   0.137561  1.021350  1.848019e-02\n\n[115 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vm_pu</th>\n      <th>va_degree</th>\n      <th>p_mw</th>\n      <th>q_mvar</th>\n      <th>lam_p</th>\n      <th>lam_q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.025000</td>\n      <td>0.000000</td>\n      <td>-8.057173</td>\n      <td>-11.835759</td>\n      <td>1.000000</td>\n      <td>2.259515e-21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.001425</td>\n      <td>209.119600</td>\n      <td>-1.176327</td>\n      <td>0.135443</td>\n      <td>1.000714</td>\n      <td>1.887024e-03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.999942</td>\n      <td>209.185313</td>\n      <td>0.397800</td>\n      <td>0.183689</td>\n      <td>1.002495</td>\n      <td>4.950700e-03</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.999154</td>\n      <td>209.227646</td>\n      <td>0.293150</td>\n      <td>0.148678</td>\n      <td>1.003357</td>\n      <td>6.811706e-03</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.998564</td>\n      <td>209.265084</td>\n      <td>0.239906</td>\n      <td>0.171257</td>\n      <td>1.003935</td>\n      <td>8.382988e-03</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>0.987459</td>\n      <td>209.245271</td>\n      <td>0.237492</td>\n      <td>0.096853</td>\n      <td>1.020719</td>\n      <td>1.774877e-02</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>0.987333</td>\n      <td>209.251486</td>\n      <td>0.036984</td>\n      <td>0.085200</td>\n      <td>1.020904</td>\n      <td>1.802879e-02</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>0.987261</td>\n      <td>209.254787</td>\n      <td>-0.074431</td>\n      <td>0.034476</td>\n      <td>1.021013</td>\n      <td>1.818089e-02</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>0.987149</td>\n      <td>209.258063</td>\n      <td>0.118649</td>\n      <td>0.137547</td>\n      <td>1.021198</td>\n      <td>1.835478e-02</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>0.987060</td>\n      <td>209.260287</td>\n      <td>0.159486</td>\n      <td>0.137561</td>\n      <td>1.021350</td>\n      <td>1.848019e-02</td>\n    </tr>\n  </tbody>\n</table>\n<p>115 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acopf_ok_grid_names = ['1-HV-mixed--1-sw','1-MV-semiurb--0-no_sw','1-MV-comm--0-sw','1-MV-comm--0-no_sw']\n",
    "net = process_network(acopf_ok_grid_names[1])\n",
    "pp.runopp(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acopf_grid_names = ['1-HV-mixed--0-no_sw','1-MV-semiurb--0-no_sw', '1-MV-comm--0-no_sw']\n",
    "save_multiple_unsupervised_inputs(acopf_grid_names, num_samples=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "inputs = load_multiple_unsupervised_inputs()\n",
    "for i,input in enumerate(inputs):\n",
    "    if input.res_bus is None:\n",
    "        print(f\"None at {i}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_unsupervised_inputs('1-HV-mixed--0-no_sw', 200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = load_unsupervised_inputs('1-HV-mixed--0-no_sw')\n",
    "inputs[0].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs[57].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display Plot of Customized Sigmoid Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as p\n",
    "import torch\n",
    "\n",
    "def custom_tanh(x: torch.Tensor, lower_bound: float, upper_bound: float) -> torch.Tensor:\n",
    "    width = upper_bound - lower_bound\n",
    "    return 0.5 * width * torch.tanh(x) + 0.5 * (upper_bound + lower_bound)\n",
    "\n",
    "\n",
    "lst = []\n",
    "min_val = 0\n",
    "max_val = 0.1\n",
    "range_ = []\n",
    "i = -2\n",
    "while i < 2:\n",
    "    range_.append(i)\n",
    "    i += 0.01\n",
    "\n",
    "for i in range_:\n",
    "    val = custom_tanh(torch.tensor(i), min_val, max_val)\n",
    "    #val = torch.tanh(torch.tensor(i))\n",
    "    lst.append(val)\n",
    "\n",
    "# Now, plot the values in lst\n",
    "p.plot(range_, lst)\n",
    "p.xlabel('Input')\n",
    "p.ylabel('Value')\n",
    "p.title('Plot of Enforcing Activation Function')\n",
    "p.grid(True)\n",
    "p.show()\n",
    "print(range_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Heterogeneous Self Supervised Model and Process Inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'PQ': tensor([[-0.3404,  0.4297,  0.2627, -0.3390],\n",
      "        [ 5.2914, -0.7396,  1.2969,  4.7733],\n",
      "        [-0.4595,  0.5880,  0.2941, -0.4409],\n",
      "        [-0.5007,  0.6194,  0.3256, -0.4599],\n",
      "        [ 1.6187, -0.5973,  0.4409,  1.5799],\n",
      "        [-0.4982,  0.6073,  0.2925, -0.4751],\n",
      "        [ 1.0997, -0.6364,  0.1321,  0.9262],\n",
      "        [ 0.7430, -0.4017,  0.3454,  0.8069],\n",
      "        [-0.4755,  0.6062,  0.3273, -0.4384],\n",
      "        [ 0.1863, -0.1025,  0.1672,  0.0295],\n",
      "        [ 4.7190, -0.8107,  0.9037,  4.1483],\n",
      "        [-0.4805,  0.6122,  0.3222, -0.4476],\n",
      "        [ 0.1433, -0.0297,  0.3895,  0.3056],\n",
      "        [ 0.9695,  0.0323,  0.7686,  1.0610],\n",
      "        [-0.4516,  0.5641,  0.2389, -0.4691],\n",
      "        [-0.4463,  0.5778,  0.2557, -0.4572]], grad_fn=<EluBackward0>), 'PV': tensor([[-0.3094, -0.5684,  0.2777,  0.2449],\n",
      "        [-0.0909, -0.0795,  0.1359, -0.1535],\n",
      "        [ 0.2182,  0.5203,  0.3709,  0.0032],\n",
      "        [ 0.4915,  0.4823,  0.2856,  0.0581],\n",
      "        [ 0.1011,  0.2911,  0.1750, -0.1894],\n",
      "        [-0.6561, -0.9028,  0.1487,  0.4483],\n",
      "        [ 0.1998,  0.3998,  0.2228, -0.1360],\n",
      "        [-0.6395, -0.8039,  0.0587,  0.0098],\n",
      "        [-0.3958, -0.1880,  0.1518, -0.2526],\n",
      "        [ 0.1383,  0.3288,  0.1870, -0.1754],\n",
      "        [ 0.2326,  0.4277,  0.2333, -0.1206],\n",
      "        [ 1.1695,  0.7581,  0.5374,  0.6250],\n",
      "        [ 0.6297,  0.6815,  0.3730,  0.1458],\n",
      "        [ 0.2003,  0.3992,  0.2223, -0.1360],\n",
      "        [ 1.1900,  0.3805,  0.5037,  0.7868],\n",
      "        [-0.5711, -0.7724,  0.0704,  0.0590],\n",
      "        [ 1.3699,  0.7725,  0.6162,  0.8320],\n",
      "        [-0.0450, -0.1897,  0.2201,  0.0513],\n",
      "        [ 0.7438,  0.6180,  0.3688,  0.2378],\n",
      "        [ 0.9960,  0.7284,  0.4785,  0.4663],\n",
      "        [ 0.4175,  0.4744,  0.2917,  0.0311],\n",
      "        [-0.5028, -0.6337,  0.0478, -0.1196],\n",
      "        [-0.2076, -0.2015,  0.1130, -0.1779],\n",
      "        [-0.2703, -0.0268,  0.1749, -0.2273],\n",
      "        [-0.1485,  0.1050,  0.1632, -0.2337],\n",
      "        [ 0.9207,  0.6789,  0.4372,  0.3956],\n",
      "        [ 0.3363,  0.3644,  0.2322, -0.0374],\n",
      "        [-0.5094, -0.5485,  0.0708, -0.1843],\n",
      "        [ 1.1288,  0.7590,  0.5229,  0.5828],\n",
      "        [-0.1387,  0.0629,  0.1339, -0.2444],\n",
      "        [ 0.3753,  0.3852,  0.2410, -0.0155],\n",
      "        [ 0.1568,  0.2483,  0.1917, -0.1225],\n",
      "        [-0.3755, -0.4583,  0.1482, -0.0540],\n",
      "        [ 0.0317,  0.1535,  0.1660, -0.1683],\n",
      "        [ 2.0218,  0.8252,  0.8945,  1.5244],\n",
      "        [ 0.0914,  0.2000,  0.1794, -0.1458],\n",
      "        [ 0.1383,  0.2236,  0.1878, -0.1244],\n",
      "        [ 0.4960,  0.4652,  0.2785,  0.0596],\n",
      "        [ 0.0105,  0.1702,  0.1816, -0.1660],\n",
      "        [ 0.1142,  0.3049,  0.1796, -0.1843],\n",
      "        [ 0.1694,  0.3618,  0.2006, -0.1599],\n",
      "        [ 0.1215,  0.3123,  0.1822, -0.1813]], grad_fn=<EluBackward0>), 'SB': tensor([[ 0.7092, -0.5609,  0.9157,  1.4806]], grad_fn=<EluBackward0>), 'NB': tensor([[ 1.5040, -0.8545,  1.9369,  3.7934],\n",
      "        [-0.4020, -0.0866,  0.2028, -0.3387],\n",
      "        [ 0.4084, -0.6598,  1.1268,  1.7238],\n",
      "        [ 0.9130, -0.7336,  1.4748,  2.6310],\n",
      "        [-0.3931,  0.2077,  0.0215, -0.5200]], grad_fn=<EluBackward0>)})\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ACOPFGNN:\n\tMissing key(s) in state_dict: \"lin_2.weight\", \"lin_2.bias\", \"fcs.0.SB.weight\", \"fcs.0.SB.bias\", \"fcs.0.PQ.weight\", \"fcs.0.PQ.bias\", \"fcs.0.PV.weight\", \"fcs.0.PV.bias\", \"fcs.0.NB.weight\", \"fcs.0.NB.bias\", \"fcs.1.SB.weight\", \"fcs.1.SB.bias\", \"fcs.1.PQ.weight\", \"fcs.1.PQ.bias\", \"fcs.1.PV.weight\", \"fcs.1.PV.bias\", \"fcs.1.NB.weight\", \"fcs.1.NB.bias\", \"fcs.2.SB.weight\", \"fcs.2.SB.bias\", \"fcs.2.PQ.weight\", \"fcs.2.PQ.bias\", \"fcs.2.PV.weight\", \"fcs.2.PV.bias\", \"fcs.2.NB.weight\", \"fcs.2.NB.bias\", \"fcs.3.SB.weight\", \"fcs.3.SB.bias\", \"fcs.3.PQ.weight\", \"fcs.3.PQ.bias\", \"fcs.3.PV.weight\", \"fcs.3.PV.bias\", \"fcs.3.NB.weight\", \"fcs.3.NB.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_22972\\1311777219.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Load the Model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0membedder_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_ACOPFGNN_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrid_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"embedder_model.pt\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_channels\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_layers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mload_ACOPFGNN_model\u001B[1;34m(grid_name, model_name, hidden_channels, num_layers)\u001B[0m\n\u001B[0;32m   2991\u001B[0m     \u001B[1;31m# Load the saved model parameter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2992\u001B[0m     \u001B[0mordered_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2993\u001B[1;33m     \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mordered_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2994\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2995\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mload_state_dict\u001B[1;34m(self, state_dict, strict)\u001B[0m\n\u001B[0;32m   1603\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror_msgs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1604\u001B[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001B[1;32m-> 1605\u001B[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001B[0m\u001B[0;32m   1606\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_IncompatibleKeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmissing_keys\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munexpected_keys\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1607\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for ACOPFGNN:\n\tMissing key(s) in state_dict: \"lin_2.weight\", \"lin_2.bias\", \"fcs.0.SB.weight\", \"fcs.0.SB.bias\", \"fcs.0.PQ.weight\", \"fcs.0.PQ.bias\", \"fcs.0.PV.weight\", \"fcs.0.PV.bias\", \"fcs.0.NB.weight\", \"fcs.0.NB.bias\", \"fcs.1.SB.weight\", \"fcs.1.SB.bias\", \"fcs.1.PQ.weight\", \"fcs.1.PQ.bias\", \"fcs.1.PV.weight\", \"fcs.1.PV.bias\", \"fcs.1.NB.weight\", \"fcs.1.NB.bias\", \"fcs.2.SB.weight\", \"fcs.2.SB.bias\", \"fcs.2.PQ.weight\", \"fcs.2.PQ.bias\", \"fcs.2.PV.weight\", \"fcs.2.PV.bias\", \"fcs.2.NB.weight\", \"fcs.2.NB.bias\", \"fcs.3.SB.weight\", \"fcs.3.SB.bias\", \"fcs.3.PQ.weight\", \"fcs.3.PQ.bias\", \"fcs.3.PV.weight\", \"fcs.3.PV.bias\", \"fcs.3.NB.weight\", \"fcs.3.NB.bias\". "
     ]
    }
   ],
   "source": [
    "# Load the Model\n",
    "embedder_model = load_ACOPFGNN_model(grid_name, \"embedder_model.pt\", hidden_channels=4, num_layers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'SB': tensor([[ 0.1041,  0.1142, -0.4791,  0.7446]], grad_fn=<EluBackward0>), 'PQ': tensor([[ 0.2601,  0.0198, -0.0580,  0.4459],\n",
      "        [ 0.2241,  0.0918,  0.0923,  0.5312],\n",
      "        [ 0.3650, -0.1676, -0.0951,  0.2352],\n",
      "        [ 0.4566, -0.2931, -0.1155,  0.0503],\n",
      "        [ 0.0474,  0.4948,  0.1282,  0.8651],\n",
      "        [ 0.2946, -0.0845, -0.0996,  0.3877],\n",
      "        [ 0.3538, -0.2359, -0.2082,  0.2790],\n",
      "        [ 0.2371,  0.0829, -0.0202,  0.4880],\n",
      "        [ 0.3421, -0.1565, -0.1316,  0.2869],\n",
      "        [ 0.2901, -0.3365, -0.3776,  0.4726],\n",
      "        [ 0.0320,  0.6127,  0.1320,  0.8688],\n",
      "        [ 0.3712, -0.1951, -0.1204,  0.2278],\n",
      "        [ 0.2325,  0.1152,  0.0070,  0.4927],\n",
      "        [ 0.4929, -0.3178, -0.3468, -0.0792],\n",
      "        [ 0.3148, -0.1695, -0.1970,  0.3557],\n",
      "        [ 0.3062, -0.0926, -0.1085,  0.3589]], grad_fn=<EluBackward0>), 'NB': tensor([[-0.1595,  0.3800, -0.1613,  1.4756],\n",
      "        [-0.0402,  0.2727, -0.4049,  1.0814],\n",
      "        [-0.1306,  0.3673, -0.3230,  1.3394],\n",
      "        [ 0.5922, -0.4364, -0.2856, -0.2303],\n",
      "        [-0.0065,  0.1294, -0.2934,  1.0887]], grad_fn=<EluBackward0>), 'PV': tensor([[ 0.2841,  0.1865, -0.7155,  0.1012],\n",
      "        [ 0.2557,  0.0358, -0.6641,  0.2689],\n",
      "        [ 0.1147, -0.1133, -0.3655,  0.8004],\n",
      "        [ 0.0462,  0.1929, -0.3464,  0.8665],\n",
      "        [ 0.0357,  0.2122, -0.2095,  0.9236],\n",
      "        [ 0.2446, -0.3619, -0.5050,  0.5697],\n",
      "        [ 0.0467,  0.1925, -0.2601,  0.8895],\n",
      "        [ 0.2172, -0.0308, -0.5972,  0.4291],\n",
      "        [ 0.0814,  0.1530, -0.4164,  0.7711],\n",
      "        [ 0.0370,  0.2158, -0.2290,  0.9142],\n",
      "        [ 0.0502,  0.1898, -0.2822,  0.8758],\n",
      "        [ 0.0418,  0.2368, -0.4987,  0.8127],\n",
      "        [ 0.0893,  0.2451, -0.5436,  0.6720],\n",
      "        [ 0.0472,  0.1905, -0.2592,  0.8893],\n",
      "        [ 0.0167,  0.2804, -0.2406,  0.9396],\n",
      "        [ 0.1422, -0.1263, -0.4287,  0.7204],\n",
      "        [ 0.0448,  0.2328, -0.5436,  0.7887],\n",
      "        [ 0.0986,  0.0013, -0.4495,  0.7733],\n",
      "        [ 0.0419,  0.2136, -0.3941,  0.8558],\n",
      "        [ 0.0602,  0.1664, -0.4546,  0.8071],\n",
      "        [ 0.0623,  0.1600, -0.3736,  0.8294],\n",
      "        [ 0.0987,  0.0058, -0.5008,  0.7551],\n",
      "        [ 0.0696,  0.1032, -0.3587,  0.8358],\n",
      "        [ 0.0724,  0.1279, -0.3229,  0.8304],\n",
      "        [ 0.0469,  0.1802, -0.2413,  0.8987],\n",
      "        [ 0.0455,  0.1970, -0.4457,  0.8354],\n",
      "        [ 0.0433,  0.1931, -0.3113,  0.8839],\n",
      "        [ 0.1139, -0.0758, -0.4027,  0.7777],\n",
      "        [ 0.0475,  0.2181, -0.4814,  0.8114],\n",
      "        [ 0.0433,  0.1847, -0.2245,  0.9104],\n",
      "        [ 0.0422,  0.1968, -0.3171,  0.8836],\n",
      "        [ 0.0460,  0.1856, -0.2895,  0.8858],\n",
      "        [ 0.1128, -0.0367, -0.4868,  0.7409],\n",
      "        [ 0.0454,  0.1879, -0.2745,  0.8906],\n",
      "        [ 0.0619,  0.2913, -0.6120,  0.6940],\n",
      "        [ 0.0455,  0.1884, -0.2826,  0.8880],\n",
      "        [ 0.0482,  0.1669, -0.2842,  0.8877],\n",
      "        [ 0.0417,  0.2027, -0.3397,  0.8766],\n",
      "        [ 0.0595,  0.1282, -0.2879,  0.8713],\n",
      "        [ 0.0362,  0.2125, -0.2146,  0.9210],\n",
      "        [ 0.0399,  0.2114, -0.2440,  0.9045],\n",
      "        [ 0.0366,  0.2123, -0.2179,  0.9191]], grad_fn=<EluBackward0>)})\n"
     ]
    }
   ],
   "source": [
    "# Create ACOPFGNN model and Optimizer\n",
    "index_mappers,net,data = generate_unsupervised_input(grid_name)\n",
    "model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "learning_rate = 2.5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "2321"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mcanbay96\u001B[0m (\u001B[33mcbml\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\wandb\\run-20231019_023541-i8m08wvo</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/i8m08wvo' target=\"_blank\">generous-salad-227</a></strong> to <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/i8m08wvo' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/i8m08wvo</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_NOTEBOOK_NAME = \"msi\"\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"ACOPF-GNN-SelfSupervised-Hetero\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            #\"learning_rate\": optimizer.,\n",
    "            \"architecture\": \"Hetero-SelfSupervised-GNN\",\n",
    "            \"num_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "            \"num_heads\": model.heads,\n",
    "            \"num_layers\": model.num_layers,\n",
    "            \"grid_name\": grid_name,\n",
    "            \"activation\": model.act_fn,\n",
    "            \"dropout\": model.dropout,\n",
    "            \"in_channels\": model.in_channels,\n",
    "            \"output_channels\": model.out_channels,\n",
    "            \"hidden_channels\": model.hidden_channels,\n",
    "            \"channel type\": \"TransformerConv\",\n",
    "            \"scaler\": \"MinMax\",\n",
    "            \"model\": \"base_model_wired\"\n",
    "        }\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load Inputs\n",
    "inputs = load_unsupervised_inputs(grid_name)\n",
    "#inputs = load_multiple_unsupervised_inputs()\n",
    "n = len(inputs)\n",
    "train_inputs = inputs[:int(n*0.8)]\n",
    "val_inputs = inputs[int(n*0.8): int(n*0.95)]\n",
    "test_inputs = inputs[int(n*0.95):]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------   CURRENT LEARNING RATE: 0.001   ---------------\n",
      "Epoch: 0\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.0 \n",
      "     Training Step: 1 Training Loss: 0.0 \n",
      "     Training Step: 2 Training Loss: 0.0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17588\\836682791.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mloss_weights\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mmodel_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"embedder_model_specialized.pt\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mtrain_input\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mval_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_validate_ACOPF\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mtrain_validate_ACOPF\u001B[1;34m(model, model_name, optimizer, train_inputs, val_inputs, loss_weights, start_epoch, num_epochs, return_outputs, save_model)\u001B[0m\n\u001B[0;32m   2287\u001B[0m             \u001B[1;31m#physics_loss, mre_loss, loss, penalty_loss, unsupervised_loss = self_supervised_hetero_obj_fn(out_dict,bus_idx_neighbors_dict,constraint_dict,scaler, alpha,beta, gamma)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2288\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2289\u001B[1;33m             \u001B[0mphysics_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmre_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpenalty_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munsupervised_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2290\u001B[0m             \u001B[0mphysics_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2291\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mself_supervised_embedder_obj_fn\u001B[1;34m(out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   2182\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2183\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2184\u001B[1;33m     \u001B[0mphysics_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_physics_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2185\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2186\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mphysics_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mcalc_physics_loss\u001B[1;34m(out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   3058\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3059\u001B[0m                 \u001B[1;31m# ACOPF Equation for P_i\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3060\u001B[1;33m                 \u001B[0mP_ij\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1.0\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_i\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_j\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mG_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mB_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3061\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3062\u001B[0m                 \u001B[0mP\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mformat_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mformat_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mextract_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    198\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m     \u001B[0mstack\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mStackSummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwalk_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m     \u001B[0mstack\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreverse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mstack\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    357\u001B[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001B[0;32m    358\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mfilename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfnames\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 359\u001B[1;33m             \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckcache\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    360\u001B[0m         \u001B[1;31m# If immediate lookup was desired, trigger lookups now.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlookup_lines\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\IPython\\core\\compilerop.py\u001B[0m in \u001B[0;36mcheck_linecache_ipython\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \"\"\"\n\u001B[0;32m    184\u001B[0m     \u001B[1;31m# First call the original checkcache as intended\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 185\u001B[1;33m     \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_checkcache_ori\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    186\u001B[0m     \u001B[1;31m# Then, update back the cache with our data, so that tracebacks related\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m     \u001B[1;31m# to our compiled codes can be produced.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\linecache.py\u001B[0m in \u001B[0;36mcheckcache\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mcontinue\u001B[0m   \u001B[1;31m# no-op for files loaded via a __loader__\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m             \u001B[0mstat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfullname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mcache\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 1\n",
    "num_epochs = 100\n",
    "loss_weights = (0.0, 1.0, 0.0)\n",
    "model_name = \"embedder_model_specialized.pt\"\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF(model, model_name, optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------   CURRENT LEARNING RATE: 4e-05   ---------------\n",
      "Epoch: 60\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 6.849725723266602 \n",
      "     Training Step: 1 Training Loss: 7.994863033294678 \n",
      "     Training Step: 2 Training Loss: 7.418996334075928 \n",
      "     Training Step: 3 Training Loss: 4.887266635894775 \n",
      "     Training Step: 4 Training Loss: 5.012649059295654 \n",
      "     Training Step: 5 Training Loss: 3.0778064727783203 \n",
      "     Training Step: 6 Training Loss: 11.675814628601074 \n",
      "     Training Step: 7 Training Loss: 10.785894393920898 \n",
      "     Training Step: 8 Training Loss: 8.542866706848145 \n",
      "     Training Step: 9 Training Loss: 9.035882949829102 \n",
      "     Training Step: 10 Training Loss: 9.545356750488281 \n",
      "     Training Step: 11 Training Loss: 8.43463134765625 \n",
      "     Training Step: 12 Training Loss: 12.55498218536377 \n",
      "     Training Step: 13 Training Loss: 5.320827484130859 \n",
      "     Training Step: 14 Training Loss: 6.62136173248291 \n",
      "     Training Step: 15 Training Loss: 5.096195220947266 \n",
      "     Training Step: 16 Training Loss: 9.50904655456543 \n",
      "     Training Step: 17 Training Loss: 7.444781303405762 \n",
      "     Training Step: 18 Training Loss: 6.758533477783203 \n",
      "     Training Step: 19 Training Loss: 6.758355617523193 \n",
      "     Training Step: 20 Training Loss: 7.661477565765381 \n",
      "     Training Step: 21 Training Loss: 6.79921293258667 \n",
      "     Training Step: 22 Training Loss: 10.037893295288086 \n",
      "     Training Step: 23 Training Loss: 9.075201034545898 \n",
      "     Training Step: 24 Training Loss: 4.894169807434082 \n",
      "     Training Step: 25 Training Loss: 7.776320934295654 \n",
      "     Training Step: 26 Training Loss: 3.6629114151000977 \n",
      "     Training Step: 27 Training Loss: 7.650761127471924 \n",
      "     Training Step: 28 Training Loss: 7.431985378265381 \n",
      "     Training Step: 29 Training Loss: 7.586144924163818 \n",
      "     Training Step: 30 Training Loss: 12.974316596984863 \n",
      "     Training Step: 31 Training Loss: 12.894868850708008 \n",
      "     Training Step: 32 Training Loss: 6.032436370849609 \n",
      "     Training Step: 33 Training Loss: 4.2507405281066895 \n",
      "     Training Step: 34 Training Loss: 5.452780723571777 \n",
      "     Training Step: 35 Training Loss: 13.459784507751465 \n",
      "     Training Step: 36 Training Loss: 5.302128314971924 \n",
      "     Training Step: 37 Training Loss: 13.050894737243652 \n",
      "     Training Step: 38 Training Loss: 13.304935455322266 \n",
      "     Training Step: 39 Training Loss: 3.919915199279785 \n",
      "     Training Step: 40 Training Loss: 5.36727237701416 \n",
      "     Training Step: 41 Training Loss: 5.129926681518555 \n",
      "     Training Step: 42 Training Loss: 5.51865291595459 \n",
      "     Training Step: 43 Training Loss: 7.853332042694092 \n",
      "     Training Step: 44 Training Loss: 5.049275875091553 \n",
      "     Training Step: 45 Training Loss: 5.308444499969482 \n",
      "     Training Step: 46 Training Loss: 6.933781623840332 \n",
      "     Training Step: 47 Training Loss: 10.68270206451416 \n",
      "     Training Step: 48 Training Loss: 3.287910223007202 \n",
      "     Training Step: 49 Training Loss: 6.718997955322266 \n",
      "     Training Step: 50 Training Loss: 7.86416482925415 \n",
      "     Training Step: 51 Training Loss: 4.65550422668457 \n",
      "     Training Step: 52 Training Loss: 7.46273136138916 \n",
      "     Training Step: 53 Training Loss: 4.859919548034668 \n",
      "     Training Step: 54 Training Loss: 4.085713863372803 \n",
      "     Training Step: 55 Training Loss: 7.330260753631592 \n",
      "     Training Step: 56 Training Loss: 5.473433494567871 \n",
      "     Training Step: 57 Training Loss: 4.964493274688721 \n",
      "     Training Step: 58 Training Loss: 11.09634780883789 \n",
      "     Training Step: 59 Training Loss: 5.558187484741211 \n",
      "     Training Step: 60 Training Loss: 10.589357376098633 \n",
      "     Training Step: 61 Training Loss: 5.367259502410889 \n",
      "     Training Step: 62 Training Loss: 14.799457550048828 \n",
      "     Training Step: 63 Training Loss: 13.711862564086914 \n",
      "     Training Step: 64 Training Loss: 3.6749696731567383 \n",
      "     Training Step: 65 Training Loss: 9.222043991088867 \n",
      "     Training Step: 66 Training Loss: 6.734905242919922 \n",
      "     Training Step: 67 Training Loss: 8.710442543029785 \n",
      "     Training Step: 68 Training Loss: 7.278345584869385 \n",
      "     Training Step: 69 Training Loss: 8.809966087341309 \n",
      "     Training Step: 70 Training Loss: 8.195501327514648 \n",
      "     Training Step: 71 Training Loss: 8.988158226013184 \n",
      "     Training Step: 72 Training Loss: 9.117876052856445 \n",
      "     Training Step: 73 Training Loss: 5.26828670501709 \n",
      "     Training Step: 74 Training Loss: 5.45643424987793 \n",
      "     Training Step: 75 Training Loss: 3.749128818511963 \n",
      "     Training Step: 76 Training Loss: 10.256917953491211 \n",
      "     Training Step: 77 Training Loss: 12.184932708740234 \n",
      "     Training Step: 78 Training Loss: 12.044828414916992 \n",
      "     Training Step: 79 Training Loss: 8.22641372680664 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 6.2922682762146 \n",
      "     Validation Step: 1 Validation Loss: 7.970943450927734 \n",
      "     Validation Step: 2 Validation Loss: 9.62479305267334 \n",
      "     Validation Step: 3 Validation Loss: 8.7261381149292 \n",
      "     Validation Step: 4 Validation Loss: 12.534525871276855 \n",
      "     Validation Step: 5 Validation Loss: 11.159050941467285 \n",
      "     Validation Step: 6 Validation Loss: 3.3455405235290527 \n",
      "     Validation Step: 7 Validation Loss: 4.4155097007751465 \n",
      "     Validation Step: 8 Validation Loss: 5.8844828605651855 \n",
      "     Validation Step: 9 Validation Loss: 4.783188819885254 \n",
      "     Validation Step: 10 Validation Loss: 9.90269947052002 \n",
      "     Validation Step: 11 Validation Loss: 6.927859783172607 \n",
      "     Validation Step: 12 Validation Loss: 13.698420524597168 \n",
      "     Validation Step: 13 Validation Loss: 12.103291511535645 \n",
      "     Validation Step: 14 Validation Loss: 5.034355640411377 \n",
      "Epoch: 61\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.6355698108673096 \n",
      "     Training Step: 1 Training Loss: 3.2790489196777344 \n",
      "     Training Step: 2 Training Loss: 2.0579960346221924 \n",
      "     Training Step: 3 Training Loss: 3.4389989376068115 \n",
      "     Training Step: 4 Training Loss: 2.5847768783569336 \n",
      "     Training Step: 5 Training Loss: 2.554276704788208 \n",
      "     Training Step: 6 Training Loss: 2.485879898071289 \n",
      "     Training Step: 7 Training Loss: 2.446885585784912 \n",
      "     Training Step: 8 Training Loss: 2.324148654937744 \n",
      "     Training Step: 9 Training Loss: 2.551927089691162 \n",
      "     Training Step: 10 Training Loss: 2.7412290573120117 \n",
      "     Training Step: 11 Training Loss: 2.4219207763671875 \n",
      "     Training Step: 12 Training Loss: 2.387845516204834 \n",
      "     Training Step: 13 Training Loss: 2.9680519104003906 \n",
      "     Training Step: 14 Training Loss: 2.352520227432251 \n",
      "     Training Step: 15 Training Loss: 2.8653669357299805 \n",
      "     Training Step: 16 Training Loss: 3.28800106048584 \n",
      "     Training Step: 17 Training Loss: 2.507077693939209 \n",
      "     Training Step: 18 Training Loss: 3.054689884185791 \n",
      "     Training Step: 19 Training Loss: 2.3753955364227295 \n",
      "     Training Step: 20 Training Loss: 3.495718479156494 \n",
      "     Training Step: 21 Training Loss: 2.7977917194366455 \n",
      "     Training Step: 22 Training Loss: 2.6439530849456787 \n",
      "     Training Step: 23 Training Loss: 3.3722167015075684 \n",
      "     Training Step: 24 Training Loss: 2.768357276916504 \n",
      "     Training Step: 25 Training Loss: 2.9859795570373535 \n",
      "     Training Step: 26 Training Loss: 3.1551802158355713 \n",
      "     Training Step: 27 Training Loss: 2.31691837310791 \n",
      "     Training Step: 28 Training Loss: 3.2671611309051514 \n",
      "     Training Step: 29 Training Loss: 2.217538833618164 \n",
      "     Training Step: 30 Training Loss: 2.7339463233947754 \n",
      "     Training Step: 31 Training Loss: 2.6649608612060547 \n",
      "     Training Step: 32 Training Loss: 2.751533031463623 \n",
      "     Training Step: 33 Training Loss: 2.944531202316284 \n",
      "     Training Step: 34 Training Loss: 2.8072686195373535 \n",
      "     Training Step: 35 Training Loss: 2.9901790618896484 \n",
      "     Training Step: 36 Training Loss: 2.7854702472686768 \n",
      "     Training Step: 37 Training Loss: 2.4883780479431152 \n",
      "     Training Step: 38 Training Loss: 2.213292121887207 \n",
      "     Training Step: 39 Training Loss: 2.548321008682251 \n",
      "     Training Step: 40 Training Loss: 3.0088748931884766 \n",
      "     Training Step: 41 Training Loss: 3.368178129196167 \n",
      "     Training Step: 42 Training Loss: 2.822603225708008 \n",
      "     Training Step: 43 Training Loss: 3.030764102935791 \n",
      "     Training Step: 44 Training Loss: 2.229597568511963 \n",
      "     Training Step: 45 Training Loss: 2.8533804416656494 \n",
      "     Training Step: 46 Training Loss: 2.0107736587524414 \n",
      "     Training Step: 47 Training Loss: 2.209465980529785 \n",
      "     Training Step: 48 Training Loss: 2.3026845455169678 \n",
      "     Training Step: 49 Training Loss: 2.301964044570923 \n",
      "     Training Step: 50 Training Loss: 3.3419573307037354 \n",
      "     Training Step: 51 Training Loss: 3.6319408416748047 \n",
      "     Training Step: 52 Training Loss: 3.3818068504333496 \n",
      "     Training Step: 53 Training Loss: 2.5126912593841553 \n",
      "     Training Step: 54 Training Loss: 3.7466955184936523 \n",
      "     Training Step: 55 Training Loss: 2.8762738704681396 \n",
      "     Training Step: 56 Training Loss: 3.5802860260009766 \n",
      "     Training Step: 57 Training Loss: 3.4996771812438965 \n",
      "     Training Step: 58 Training Loss: 2.536621570587158 \n",
      "     Training Step: 59 Training Loss: 2.3883039951324463 \n",
      "     Training Step: 60 Training Loss: 3.328050136566162 \n",
      "     Training Step: 61 Training Loss: 2.908033609390259 \n",
      "     Training Step: 62 Training Loss: 2.9240691661834717 \n",
      "     Training Step: 63 Training Loss: 2.533116102218628 \n",
      "     Training Step: 64 Training Loss: 2.4196269512176514 \n",
      "     Training Step: 65 Training Loss: 2.5194225311279297 \n",
      "     Training Step: 66 Training Loss: 2.468540668487549 \n",
      "     Training Step: 67 Training Loss: 2.6171860694885254 \n",
      "     Training Step: 68 Training Loss: 2.3061156272888184 \n",
      "     Training Step: 69 Training Loss: 2.3685994148254395 \n",
      "     Training Step: 70 Training Loss: 2.282291889190674 \n",
      "     Training Step: 71 Training Loss: 3.3871171474456787 \n",
      "     Training Step: 72 Training Loss: 3.9170775413513184 \n",
      "     Training Step: 73 Training Loss: 3.755721092224121 \n",
      "     Training Step: 74 Training Loss: 2.3278894424438477 \n",
      "     Training Step: 75 Training Loss: 3.6431431770324707 \n",
      "     Training Step: 76 Training Loss: 2.7319881916046143 \n",
      "     Training Step: 77 Training Loss: 2.9789109230041504 \n",
      "     Training Step: 78 Training Loss: 2.6027445793151855 \n",
      "     Training Step: 79 Training Loss: 3.4189162254333496 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.321836471557617 \n",
      "     Validation Step: 1 Validation Loss: 2.7259020805358887 \n",
      "     Validation Step: 2 Validation Loss: 3.4820737838745117 \n",
      "     Validation Step: 3 Validation Loss: 2.551522731781006 \n",
      "     Validation Step: 4 Validation Loss: 3.259505271911621 \n",
      "     Validation Step: 5 Validation Loss: 2.785046100616455 \n",
      "     Validation Step: 6 Validation Loss: 2.381053924560547 \n",
      "     Validation Step: 7 Validation Loss: 3.0370004177093506 \n",
      "     Validation Step: 8 Validation Loss: 3.1503448486328125 \n",
      "     Validation Step: 9 Validation Loss: 3.0699212551116943 \n",
      "     Validation Step: 10 Validation Loss: 2.853329658508301 \n",
      "     Validation Step: 11 Validation Loss: 2.3067870140075684 \n",
      "     Validation Step: 12 Validation Loss: 2.680511951446533 \n",
      "     Validation Step: 13 Validation Loss: 3.0116653442382812 \n",
      "     Validation Step: 14 Validation Loss: 2.2327804565429688 \n",
      "Epoch: 62\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.0609235763549805 \n",
      "     Training Step: 1 Training Loss: 3.17659592628479 \n",
      "     Training Step: 2 Training Loss: 2.1783833503723145 \n",
      "     Training Step: 3 Training Loss: 2.174607515335083 \n",
      "     Training Step: 4 Training Loss: 2.0741963386535645 \n",
      "     Training Step: 5 Training Loss: 3.1043784618377686 \n",
      "     Training Step: 6 Training Loss: 3.13986873626709 \n",
      "     Training Step: 7 Training Loss: 2.9091439247131348 \n",
      "     Training Step: 8 Training Loss: 3.4113197326660156 \n",
      "     Training Step: 9 Training Loss: 2.7322945594787598 \n",
      "     Training Step: 10 Training Loss: 2.6169681549072266 \n",
      "     Training Step: 11 Training Loss: 2.9376585483551025 \n",
      "     Training Step: 12 Training Loss: 3.0303378105163574 \n",
      "     Training Step: 13 Training Loss: 2.6589856147766113 \n",
      "     Training Step: 14 Training Loss: 2.191561698913574 \n",
      "     Training Step: 15 Training Loss: 2.44677472114563 \n",
      "     Training Step: 16 Training Loss: 2.3249590396881104 \n",
      "     Training Step: 17 Training Loss: 2.220390796661377 \n",
      "     Training Step: 18 Training Loss: 3.325751781463623 \n",
      "     Training Step: 19 Training Loss: 2.368943929672241 \n",
      "     Training Step: 20 Training Loss: 3.3433055877685547 \n",
      "     Training Step: 21 Training Loss: 2.7937192916870117 \n",
      "     Training Step: 22 Training Loss: 3.1934733390808105 \n",
      "     Training Step: 23 Training Loss: 3.539168357849121 \n",
      "     Training Step: 24 Training Loss: 4.160991191864014 \n",
      "     Training Step: 25 Training Loss: 2.595266819000244 \n",
      "     Training Step: 26 Training Loss: 2.7317471504211426 \n",
      "     Training Step: 27 Training Loss: 2.6427972316741943 \n",
      "     Training Step: 28 Training Loss: 2.339905261993408 \n",
      "     Training Step: 29 Training Loss: 2.443847894668579 \n",
      "     Training Step: 30 Training Loss: 2.3639633655548096 \n",
      "     Training Step: 31 Training Loss: 2.397139549255371 \n",
      "     Training Step: 32 Training Loss: 3.443136692047119 \n",
      "     Training Step: 33 Training Loss: 2.488542079925537 \n",
      "     Training Step: 34 Training Loss: 3.13734769821167 \n",
      "     Training Step: 35 Training Loss: 3.054051160812378 \n",
      "     Training Step: 36 Training Loss: 2.646345615386963 \n",
      "     Training Step: 37 Training Loss: 2.6646361351013184 \n",
      "     Training Step: 38 Training Loss: 2.7199504375457764 \n",
      "     Training Step: 39 Training Loss: 2.992504119873047 \n",
      "     Training Step: 40 Training Loss: 3.421041488647461 \n",
      "     Training Step: 41 Training Loss: 3.175034284591675 \n",
      "     Training Step: 42 Training Loss: 2.222857713699341 \n",
      "     Training Step: 43 Training Loss: 3.186616897583008 \n",
      "     Training Step: 44 Training Loss: 3.084123134613037 \n",
      "     Training Step: 45 Training Loss: 3.214345693588257 \n",
      "     Training Step: 46 Training Loss: 3.729248523712158 \n",
      "     Training Step: 47 Training Loss: 2.296299695968628 \n",
      "     Training Step: 48 Training Loss: 2.6865251064300537 \n",
      "     Training Step: 49 Training Loss: 2.6053147315979004 \n",
      "     Training Step: 50 Training Loss: 2.660512924194336 \n",
      "     Training Step: 51 Training Loss: 2.4678382873535156 \n",
      "     Training Step: 52 Training Loss: 2.597283363342285 \n",
      "     Training Step: 53 Training Loss: 2.315915822982788 \n",
      "     Training Step: 54 Training Loss: 3.7120602130889893 \n",
      "     Training Step: 55 Training Loss: 4.433568954467773 \n",
      "     Training Step: 56 Training Loss: 4.211912155151367 \n",
      "     Training Step: 57 Training Loss: 4.363589286804199 \n",
      "     Training Step: 58 Training Loss: 2.1148648262023926 \n",
      "     Training Step: 59 Training Loss: 2.2037737369537354 \n",
      "     Training Step: 60 Training Loss: 2.6702089309692383 \n",
      "     Training Step: 61 Training Loss: 2.6055774688720703 \n",
      "     Training Step: 62 Training Loss: 3.20627498626709 \n",
      "     Training Step: 63 Training Loss: 3.055093765258789 \n",
      "     Training Step: 64 Training Loss: 3.664633274078369 \n",
      "     Training Step: 65 Training Loss: 3.743229866027832 \n",
      "     Training Step: 66 Training Loss: 3.6341564655303955 \n",
      "     Training Step: 67 Training Loss: 2.6190099716186523 \n",
      "     Training Step: 68 Training Loss: 3.978950023651123 \n",
      "     Training Step: 69 Training Loss: 3.9750943183898926 \n",
      "     Training Step: 70 Training Loss: 2.541748046875 \n",
      "     Training Step: 71 Training Loss: 3.103971481323242 \n",
      "     Training Step: 72 Training Loss: 3.233975410461426 \n",
      "     Training Step: 73 Training Loss: 2.6943447589874268 \n",
      "     Training Step: 74 Training Loss: 3.0929341316223145 \n",
      "     Training Step: 75 Training Loss: 3.601835250854492 \n",
      "     Training Step: 76 Training Loss: 3.197295665740967 \n",
      "     Training Step: 77 Training Loss: 3.3342061042785645 \n",
      "     Training Step: 78 Training Loss: 2.585078477859497 \n",
      "     Training Step: 79 Training Loss: 2.988157033920288 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.6997427940368652 \n",
      "     Validation Step: 1 Validation Loss: 2.272503137588501 \n",
      "     Validation Step: 2 Validation Loss: 2.965146064758301 \n",
      "     Validation Step: 3 Validation Loss: 3.2630581855773926 \n",
      "     Validation Step: 4 Validation Loss: 2.7251014709472656 \n",
      "     Validation Step: 5 Validation Loss: 3.580699920654297 \n",
      "     Validation Step: 6 Validation Loss: 3.2723846435546875 \n",
      "     Validation Step: 7 Validation Loss: 2.634354591369629 \n",
      "     Validation Step: 8 Validation Loss: 3.1506710052490234 \n",
      "     Validation Step: 9 Validation Loss: 2.446561813354492 \n",
      "     Validation Step: 10 Validation Loss: 3.5477006435394287 \n",
      "     Validation Step: 11 Validation Loss: 3.0839805603027344 \n",
      "     Validation Step: 12 Validation Loss: 2.4819352626800537 \n",
      "     Validation Step: 13 Validation Loss: 2.7969632148742676 \n",
      "     Validation Step: 14 Validation Loss: 3.4660515785217285 \n",
      "Epoch: 63\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.350492477416992 \n",
      "     Training Step: 1 Training Loss: 2.7761542797088623 \n",
      "     Training Step: 2 Training Loss: 3.0742547512054443 \n",
      "     Training Step: 3 Training Loss: 3.4398179054260254 \n",
      "     Training Step: 4 Training Loss: 3.5629544258117676 \n",
      "     Training Step: 5 Training Loss: 3.8351540565490723 \n",
      "     Training Step: 6 Training Loss: 2.3371591567993164 \n",
      "     Training Step: 7 Training Loss: 2.1466307640075684 \n",
      "     Training Step: 8 Training Loss: 3.4183337688446045 \n",
      "     Training Step: 9 Training Loss: 3.439323663711548 \n",
      "     Training Step: 10 Training Loss: 3.4735283851623535 \n",
      "     Training Step: 11 Training Loss: 2.482759714126587 \n",
      "     Training Step: 12 Training Loss: 2.5249476432800293 \n",
      "     Training Step: 13 Training Loss: 3.8131227493286133 \n",
      "     Training Step: 14 Training Loss: 2.285106658935547 \n",
      "     Training Step: 15 Training Loss: 2.5044875144958496 \n",
      "     Training Step: 16 Training Loss: 2.2803661823272705 \n",
      "     Training Step: 17 Training Loss: 2.891028881072998 \n",
      "     Training Step: 18 Training Loss: 2.4770584106445312 \n",
      "     Training Step: 19 Training Loss: 2.6894140243530273 \n",
      "     Training Step: 20 Training Loss: 2.416494369506836 \n",
      "     Training Step: 21 Training Loss: 2.9659314155578613 \n",
      "     Training Step: 22 Training Loss: 2.8433446884155273 \n",
      "     Training Step: 23 Training Loss: 2.8131070137023926 \n",
      "     Training Step: 24 Training Loss: 2.9125990867614746 \n",
      "     Training Step: 25 Training Loss: 2.824246644973755 \n",
      "     Training Step: 26 Training Loss: 3.9514894485473633 \n",
      "     Training Step: 27 Training Loss: 4.339582920074463 \n",
      "     Training Step: 28 Training Loss: 2.129723072052002 \n",
      "     Training Step: 29 Training Loss: 3.8834619522094727 \n",
      "     Training Step: 30 Training Loss: 3.5999560356140137 \n",
      "     Training Step: 31 Training Loss: 2.211543083190918 \n",
      "     Training Step: 32 Training Loss: 3.1559441089630127 \n",
      "     Training Step: 33 Training Loss: 3.321014642715454 \n",
      "     Training Step: 34 Training Loss: 2.520155668258667 \n",
      "     Training Step: 35 Training Loss: 2.9687461853027344 \n",
      "     Training Step: 36 Training Loss: 2.6559042930603027 \n",
      "     Training Step: 37 Training Loss: 3.444615602493286 \n",
      "     Training Step: 38 Training Loss: 2.644380807876587 \n",
      "     Training Step: 39 Training Loss: 3.680511474609375 \n",
      "     Training Step: 40 Training Loss: 2.8726015090942383 \n",
      "     Training Step: 41 Training Loss: 2.729069709777832 \n",
      "     Training Step: 42 Training Loss: 2.273205041885376 \n",
      "     Training Step: 43 Training Loss: 2.4821088314056396 \n",
      "     Training Step: 44 Training Loss: 2.5093166828155518 \n",
      "     Training Step: 45 Training Loss: 3.199690580368042 \n",
      "     Training Step: 46 Training Loss: 2.885711669921875 \n",
      "     Training Step: 47 Training Loss: 2.489046812057495 \n",
      "     Training Step: 48 Training Loss: 2.793769598007202 \n",
      "     Training Step: 49 Training Loss: 3.263688802719116 \n",
      "     Training Step: 50 Training Loss: 2.9064230918884277 \n",
      "     Training Step: 51 Training Loss: 2.7272562980651855 \n",
      "     Training Step: 52 Training Loss: 4.3142218589782715 \n",
      "     Training Step: 53 Training Loss: 3.3402628898620605 \n",
      "     Training Step: 54 Training Loss: 2.689541816711426 \n",
      "     Training Step: 55 Training Loss: 3.015148878097534 \n",
      "     Training Step: 56 Training Loss: 2.851558208465576 \n",
      "     Training Step: 57 Training Loss: 2.5671162605285645 \n",
      "     Training Step: 58 Training Loss: 2.8845326900482178 \n",
      "     Training Step: 59 Training Loss: 3.1151041984558105 \n",
      "     Training Step: 60 Training Loss: 3.166029214859009 \n",
      "     Training Step: 61 Training Loss: 2.046379804611206 \n",
      "     Training Step: 62 Training Loss: 3.0691781044006348 \n",
      "     Training Step: 63 Training Loss: 3.0757110118865967 \n",
      "     Training Step: 64 Training Loss: 2.6397342681884766 \n",
      "     Training Step: 65 Training Loss: 2.235316276550293 \n",
      "     Training Step: 66 Training Loss: 2.7301552295684814 \n",
      "     Training Step: 67 Training Loss: 2.3605520725250244 \n",
      "     Training Step: 68 Training Loss: 4.518928527832031 \n",
      "     Training Step: 69 Training Loss: 2.4143919944763184 \n",
      "     Training Step: 70 Training Loss: 2.604386806488037 \n",
      "     Training Step: 71 Training Loss: 4.575492858886719 \n",
      "     Training Step: 72 Training Loss: 2.312904119491577 \n",
      "     Training Step: 73 Training Loss: 2.4002461433410645 \n",
      "     Training Step: 74 Training Loss: 3.2159948348999023 \n",
      "     Training Step: 75 Training Loss: 2.4122750759124756 \n",
      "     Training Step: 76 Training Loss: 2.3115389347076416 \n",
      "     Training Step: 77 Training Loss: 2.249664068222046 \n",
      "     Training Step: 78 Training Loss: 2.3040928840637207 \n",
      "     Training Step: 79 Training Loss: 2.7971134185791016 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.239210844039917 \n",
      "     Validation Step: 1 Validation Loss: 2.3554508686065674 \n",
      "     Validation Step: 2 Validation Loss: 3.5278046131134033 \n",
      "     Validation Step: 3 Validation Loss: 3.0248594284057617 \n",
      "     Validation Step: 4 Validation Loss: 2.554131507873535 \n",
      "     Validation Step: 5 Validation Loss: 2.6708338260650635 \n",
      "     Validation Step: 6 Validation Loss: 3.3792877197265625 \n",
      "     Validation Step: 7 Validation Loss: 3.4421427249908447 \n",
      "     Validation Step: 8 Validation Loss: 3.602975368499756 \n",
      "     Validation Step: 9 Validation Loss: 3.724350690841675 \n",
      "     Validation Step: 10 Validation Loss: 2.8725876808166504 \n",
      "     Validation Step: 11 Validation Loss: 2.9635519981384277 \n",
      "     Validation Step: 12 Validation Loss: 2.778569221496582 \n",
      "     Validation Step: 13 Validation Loss: 2.422628879547119 \n",
      "     Validation Step: 14 Validation Loss: 3.1461822986602783 \n",
      "Epoch: 64\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.8166370391845703 \n",
      "     Training Step: 1 Training Loss: 3.3043031692504883 \n",
      "     Training Step: 2 Training Loss: 2.995069742202759 \n",
      "     Training Step: 3 Training Loss: 2.9556267261505127 \n",
      "     Training Step: 4 Training Loss: 4.579167366027832 \n",
      "     Training Step: 5 Training Loss: 2.63482666015625 \n",
      "     Training Step: 6 Training Loss: 3.885169506072998 \n",
      "     Training Step: 7 Training Loss: 3.189358711242676 \n",
      "     Training Step: 8 Training Loss: 2.163419485092163 \n",
      "     Training Step: 9 Training Loss: 2.4347879886627197 \n",
      "     Training Step: 10 Training Loss: 3.018167018890381 \n",
      "     Training Step: 11 Training Loss: 3.377235174179077 \n",
      "     Training Step: 12 Training Loss: 2.4053196907043457 \n",
      "     Training Step: 13 Training Loss: 2.1560871601104736 \n",
      "     Training Step: 14 Training Loss: 2.700906753540039 \n",
      "     Training Step: 15 Training Loss: 3.9529075622558594 \n",
      "     Training Step: 16 Training Loss: 3.056607246398926 \n",
      "     Training Step: 17 Training Loss: 3.8545994758605957 \n",
      "     Training Step: 18 Training Loss: 3.3856749534606934 \n",
      "     Training Step: 19 Training Loss: 3.9476754665374756 \n",
      "     Training Step: 20 Training Loss: 2.929025411605835 \n",
      "     Training Step: 21 Training Loss: 2.7837371826171875 \n",
      "     Training Step: 22 Training Loss: 2.6997005939483643 \n",
      "     Training Step: 23 Training Loss: 2.4101946353912354 \n",
      "     Training Step: 24 Training Loss: 4.0705037117004395 \n",
      "     Training Step: 25 Training Loss: 2.2036521434783936 \n",
      "     Training Step: 26 Training Loss: 4.069825172424316 \n",
      "     Training Step: 27 Training Loss: 2.2287724018096924 \n",
      "     Training Step: 28 Training Loss: 3.7236714363098145 \n",
      "     Training Step: 29 Training Loss: 3.2921931743621826 \n",
      "     Training Step: 30 Training Loss: 4.292503356933594 \n",
      "     Training Step: 31 Training Loss: 3.0457520484924316 \n",
      "     Training Step: 32 Training Loss: 3.6483469009399414 \n",
      "     Training Step: 33 Training Loss: 2.474405288696289 \n",
      "     Training Step: 34 Training Loss: 3.161085605621338 \n",
      "     Training Step: 35 Training Loss: 3.968397617340088 \n",
      "     Training Step: 36 Training Loss: 2.4151289463043213 \n",
      "     Training Step: 37 Training Loss: 3.075357437133789 \n",
      "     Training Step: 38 Training Loss: 2.1406006813049316 \n",
      "     Training Step: 39 Training Loss: 3.006072521209717 \n",
      "     Training Step: 40 Training Loss: 3.510308027267456 \n",
      "     Training Step: 41 Training Loss: 3.576425075531006 \n",
      "     Training Step: 42 Training Loss: 2.5647549629211426 \n",
      "     Training Step: 43 Training Loss: 2.3023178577423096 \n",
      "     Training Step: 44 Training Loss: 2.391667127609253 \n",
      "     Training Step: 45 Training Loss: 2.377190113067627 \n",
      "     Training Step: 46 Training Loss: 2.3370378017425537 \n",
      "     Training Step: 47 Training Loss: 2.341648817062378 \n",
      "     Training Step: 48 Training Loss: 2.265803337097168 \n",
      "     Training Step: 49 Training Loss: 2.798945665359497 \n",
      "     Training Step: 50 Training Loss: 2.6837716102600098 \n",
      "     Training Step: 51 Training Loss: 3.2978055477142334 \n",
      "     Training Step: 52 Training Loss: 2.975473165512085 \n",
      "     Training Step: 53 Training Loss: 3.124541997909546 \n",
      "     Training Step: 54 Training Loss: 3.680816173553467 \n",
      "     Training Step: 55 Training Loss: 4.100606918334961 \n",
      "     Training Step: 56 Training Loss: 2.8304805755615234 \n",
      "     Training Step: 57 Training Loss: 3.435638427734375 \n",
      "     Training Step: 58 Training Loss: 3.1693568229675293 \n",
      "     Training Step: 59 Training Loss: 3.089113235473633 \n",
      "     Training Step: 60 Training Loss: 3.567826509475708 \n",
      "     Training Step: 61 Training Loss: 3.1940441131591797 \n",
      "     Training Step: 62 Training Loss: 3.0819849967956543 \n",
      "     Training Step: 63 Training Loss: 2.978015661239624 \n",
      "     Training Step: 64 Training Loss: 2.742379665374756 \n",
      "     Training Step: 65 Training Loss: 2.5247392654418945 \n",
      "     Training Step: 66 Training Loss: 3.7250044345855713 \n",
      "     Training Step: 67 Training Loss: 2.448546886444092 \n",
      "     Training Step: 68 Training Loss: 3.9751317501068115 \n",
      "     Training Step: 69 Training Loss: 2.4564361572265625 \n",
      "     Training Step: 70 Training Loss: 2.963017463684082 \n",
      "     Training Step: 71 Training Loss: 3.46236515045166 \n",
      "     Training Step: 72 Training Loss: 2.551771879196167 \n",
      "     Training Step: 73 Training Loss: 2.571284294128418 \n",
      "     Training Step: 74 Training Loss: 2.9263148307800293 \n",
      "     Training Step: 75 Training Loss: 3.1761281490325928 \n",
      "     Training Step: 76 Training Loss: 2.353303909301758 \n",
      "     Training Step: 77 Training Loss: 3.092446804046631 \n",
      "     Training Step: 78 Training Loss: 2.3967509269714355 \n",
      "     Training Step: 79 Training Loss: 2.622293472290039 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.6334776878356934 \n",
      "     Validation Step: 1 Validation Loss: 3.3547286987304688 \n",
      "     Validation Step: 2 Validation Loss: 2.700427770614624 \n",
      "     Validation Step: 3 Validation Loss: 2.5452282428741455 \n",
      "     Validation Step: 4 Validation Loss: 2.4194741249084473 \n",
      "     Validation Step: 5 Validation Loss: 3.811025619506836 \n",
      "     Validation Step: 6 Validation Loss: 3.8318610191345215 \n",
      "     Validation Step: 7 Validation Loss: 2.7649917602539062 \n",
      "     Validation Step: 8 Validation Loss: 3.5420398712158203 \n",
      "     Validation Step: 9 Validation Loss: 2.287759304046631 \n",
      "     Validation Step: 10 Validation Loss: 3.3077077865600586 \n",
      "     Validation Step: 11 Validation Loss: 3.264392375946045 \n",
      "     Validation Step: 12 Validation Loss: 3.200045108795166 \n",
      "     Validation Step: 13 Validation Loss: 2.9103503227233887 \n",
      "     Validation Step: 14 Validation Loss: 3.4357707500457764 \n",
      "Epoch: 65\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.3412466049194336 \n",
      "     Training Step: 1 Training Loss: 2.4244332313537598 \n",
      "     Training Step: 2 Training Loss: 3.6601033210754395 \n",
      "     Training Step: 3 Training Loss: 2.4363794326782227 \n",
      "     Training Step: 4 Training Loss: 3.0832571983337402 \n",
      "     Training Step: 5 Training Loss: 2.882855176925659 \n",
      "     Training Step: 6 Training Loss: 2.520778179168701 \n",
      "     Training Step: 7 Training Loss: 3.821288585662842 \n",
      "     Training Step: 8 Training Loss: 2.736907482147217 \n",
      "     Training Step: 9 Training Loss: 3.9853460788726807 \n",
      "     Training Step: 10 Training Loss: 3.300044059753418 \n",
      "     Training Step: 11 Training Loss: 2.325242757797241 \n",
      "     Training Step: 12 Training Loss: 3.136544704437256 \n",
      "     Training Step: 13 Training Loss: 2.4410109519958496 \n",
      "     Training Step: 14 Training Loss: 2.2346086502075195 \n",
      "     Training Step: 15 Training Loss: 3.0691933631896973 \n",
      "     Training Step: 16 Training Loss: 3.402808904647827 \n",
      "     Training Step: 17 Training Loss: 2.297043561935425 \n",
      "     Training Step: 18 Training Loss: 3.5698938369750977 \n",
      "     Training Step: 19 Training Loss: 2.8692615032196045 \n",
      "     Training Step: 20 Training Loss: 3.5164237022399902 \n",
      "     Training Step: 21 Training Loss: 3.423877239227295 \n",
      "     Training Step: 22 Training Loss: 2.6200037002563477 \n",
      "     Training Step: 23 Training Loss: 2.2629904747009277 \n",
      "     Training Step: 24 Training Loss: 2.506802558898926 \n",
      "     Training Step: 25 Training Loss: 3.1912684440612793 \n",
      "     Training Step: 26 Training Loss: 3.4551334381103516 \n",
      "     Training Step: 27 Training Loss: 2.1556410789489746 \n",
      "     Training Step: 28 Training Loss: 3.0390005111694336 \n",
      "     Training Step: 29 Training Loss: 3.740813970565796 \n",
      "     Training Step: 30 Training Loss: 2.935110092163086 \n",
      "     Training Step: 31 Training Loss: 3.4247875213623047 \n",
      "     Training Step: 32 Training Loss: 3.8621678352355957 \n",
      "     Training Step: 33 Training Loss: 3.6848859786987305 \n",
      "     Training Step: 34 Training Loss: 2.7361297607421875 \n",
      "     Training Step: 35 Training Loss: 2.7050461769104004 \n",
      "     Training Step: 36 Training Loss: 2.411414384841919 \n",
      "     Training Step: 37 Training Loss: 3.1775684356689453 \n",
      "     Training Step: 38 Training Loss: 2.3705430030822754 \n",
      "     Training Step: 39 Training Loss: 2.130967140197754 \n",
      "     Training Step: 40 Training Loss: 2.2590174674987793 \n",
      "     Training Step: 41 Training Loss: 2.489447593688965 \n",
      "     Training Step: 42 Training Loss: 2.657982587814331 \n",
      "     Training Step: 43 Training Loss: 2.8750996589660645 \n",
      "     Training Step: 44 Training Loss: 3.3974289894104004 \n",
      "     Training Step: 45 Training Loss: 3.55523419380188 \n",
      "     Training Step: 46 Training Loss: 2.7299013137817383 \n",
      "     Training Step: 47 Training Loss: 2.7185142040252686 \n",
      "     Training Step: 48 Training Loss: 3.033860683441162 \n",
      "     Training Step: 49 Training Loss: 3.5523123741149902 \n",
      "     Training Step: 50 Training Loss: 2.528726100921631 \n",
      "     Training Step: 51 Training Loss: 2.5876331329345703 \n",
      "     Training Step: 52 Training Loss: 3.132133960723877 \n",
      "     Training Step: 53 Training Loss: 2.201655149459839 \n",
      "     Training Step: 54 Training Loss: 2.3190412521362305 \n",
      "     Training Step: 55 Training Loss: 3.4573025703430176 \n",
      "     Training Step: 56 Training Loss: 2.556990623474121 \n",
      "     Training Step: 57 Training Loss: 2.734179735183716 \n",
      "     Training Step: 58 Training Loss: 2.9428014755249023 \n",
      "     Training Step: 59 Training Loss: 2.816711902618408 \n",
      "     Training Step: 60 Training Loss: 2.7931809425354004 \n",
      "     Training Step: 61 Training Loss: 3.556525707244873 \n",
      "     Training Step: 62 Training Loss: 5.194555282592773 \n",
      "     Training Step: 63 Training Loss: 3.178161144256592 \n",
      "     Training Step: 64 Training Loss: 2.440124988555908 \n",
      "     Training Step: 65 Training Loss: 3.2478036880493164 \n",
      "     Training Step: 66 Training Loss: 2.6442463397979736 \n",
      "     Training Step: 67 Training Loss: 2.8246521949768066 \n",
      "     Training Step: 68 Training Loss: 4.507791519165039 \n",
      "     Training Step: 69 Training Loss: 2.335753917694092 \n",
      "     Training Step: 70 Training Loss: 3.1959481239318848 \n",
      "     Training Step: 71 Training Loss: 3.152040958404541 \n",
      "     Training Step: 72 Training Loss: 2.980825185775757 \n",
      "     Training Step: 73 Training Loss: 2.6741528511047363 \n",
      "     Training Step: 74 Training Loss: 3.1745309829711914 \n",
      "     Training Step: 75 Training Loss: 2.3682074546813965 \n",
      "     Training Step: 76 Training Loss: 4.582268714904785 \n",
      "     Training Step: 77 Training Loss: 3.6945347785949707 \n",
      "     Training Step: 78 Training Loss: 3.4372501373291016 \n",
      "     Training Step: 79 Training Loss: 2.620643138885498 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.034815549850464 \n",
      "     Validation Step: 1 Validation Loss: 3.5666956901550293 \n",
      "     Validation Step: 2 Validation Loss: 2.7859580516815186 \n",
      "     Validation Step: 3 Validation Loss: 3.471325635910034 \n",
      "     Validation Step: 4 Validation Loss: 3.576883316040039 \n",
      "     Validation Step: 5 Validation Loss: 4.05910587310791 \n",
      "     Validation Step: 6 Validation Loss: 3.9909300804138184 \n",
      "     Validation Step: 7 Validation Loss: 4.253793716430664 \n",
      "     Validation Step: 8 Validation Loss: 2.90686297416687 \n",
      "     Validation Step: 9 Validation Loss: 2.7579565048217773 \n",
      "     Validation Step: 10 Validation Loss: 2.430593967437744 \n",
      "     Validation Step: 11 Validation Loss: 3.007106304168701 \n",
      "     Validation Step: 12 Validation Loss: 2.9753942489624023 \n",
      "     Validation Step: 13 Validation Loss: 3.0002613067626953 \n",
      "     Validation Step: 14 Validation Loss: 3.536858081817627 \n",
      "Epoch: 66\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.757416248321533 \n",
      "     Training Step: 1 Training Loss: 2.7919888496398926 \n",
      "     Training Step: 2 Training Loss: 2.768770217895508 \n",
      "     Training Step: 3 Training Loss: 3.4729301929473877 \n",
      "     Training Step: 4 Training Loss: 3.3766136169433594 \n",
      "     Training Step: 5 Training Loss: 2.8275256156921387 \n",
      "     Training Step: 6 Training Loss: 2.5979864597320557 \n",
      "     Training Step: 7 Training Loss: 2.731060743331909 \n",
      "     Training Step: 8 Training Loss: 4.051044464111328 \n",
      "     Training Step: 9 Training Loss: 3.178060293197632 \n",
      "     Training Step: 10 Training Loss: 3.6726932525634766 \n",
      "     Training Step: 11 Training Loss: 4.170749187469482 \n",
      "     Training Step: 12 Training Loss: 3.206249713897705 \n",
      "     Training Step: 13 Training Loss: 2.413889169692993 \n",
      "     Training Step: 14 Training Loss: 3.694615125656128 \n",
      "     Training Step: 15 Training Loss: 3.8220558166503906 \n",
      "     Training Step: 16 Training Loss: 2.7899537086486816 \n",
      "     Training Step: 17 Training Loss: 2.8846139907836914 \n",
      "     Training Step: 18 Training Loss: 2.8885769844055176 \n",
      "     Training Step: 19 Training Loss: 3.6845200061798096 \n",
      "     Training Step: 20 Training Loss: 2.6363768577575684 \n",
      "     Training Step: 21 Training Loss: 2.3099935054779053 \n",
      "     Training Step: 22 Training Loss: 2.7882025241851807 \n",
      "     Training Step: 23 Training Loss: 3.1490566730499268 \n",
      "     Training Step: 24 Training Loss: 2.84049654006958 \n",
      "     Training Step: 25 Training Loss: 3.1084470748901367 \n",
      "     Training Step: 26 Training Loss: 2.650998115539551 \n",
      "     Training Step: 27 Training Loss: 2.5145320892333984 \n",
      "     Training Step: 28 Training Loss: 3.225149154663086 \n",
      "     Training Step: 29 Training Loss: 2.3865747451782227 \n",
      "     Training Step: 30 Training Loss: 3.2863335609436035 \n",
      "     Training Step: 31 Training Loss: 2.550858497619629 \n",
      "     Training Step: 32 Training Loss: 4.010443210601807 \n",
      "     Training Step: 33 Training Loss: 2.6733767986297607 \n",
      "     Training Step: 34 Training Loss: 2.793221950531006 \n",
      "     Training Step: 35 Training Loss: 3.3221027851104736 \n",
      "     Training Step: 36 Training Loss: 2.415220260620117 \n",
      "     Training Step: 37 Training Loss: 2.185811996459961 \n",
      "     Training Step: 38 Training Loss: 2.963890552520752 \n",
      "     Training Step: 39 Training Loss: 2.699791431427002 \n",
      "     Training Step: 40 Training Loss: 2.819584369659424 \n",
      "     Training Step: 41 Training Loss: 4.393510818481445 \n",
      "     Training Step: 42 Training Loss: 2.8043837547302246 \n",
      "     Training Step: 43 Training Loss: 2.8925158977508545 \n",
      "     Training Step: 44 Training Loss: 2.8223044872283936 \n",
      "     Training Step: 45 Training Loss: 2.4004061222076416 \n",
      "     Training Step: 46 Training Loss: 2.777791976928711 \n",
      "     Training Step: 47 Training Loss: 2.7563300132751465 \n",
      "     Training Step: 48 Training Loss: 3.068240165710449 \n",
      "     Training Step: 49 Training Loss: 2.8629727363586426 \n",
      "     Training Step: 50 Training Loss: 4.65223503112793 \n",
      "     Training Step: 51 Training Loss: 3.260021686553955 \n",
      "     Training Step: 52 Training Loss: 2.828136444091797 \n",
      "     Training Step: 53 Training Loss: 3.045400619506836 \n",
      "     Training Step: 54 Training Loss: 2.4707388877868652 \n",
      "     Training Step: 55 Training Loss: 3.355945110321045 \n",
      "     Training Step: 56 Training Loss: 3.379423141479492 \n",
      "     Training Step: 57 Training Loss: 3.264540195465088 \n",
      "     Training Step: 58 Training Loss: 3.2019405364990234 \n",
      "     Training Step: 59 Training Loss: 3.252058506011963 \n",
      "     Training Step: 60 Training Loss: 3.6671204566955566 \n",
      "     Training Step: 61 Training Loss: 2.8773813247680664 \n",
      "     Training Step: 62 Training Loss: 3.275329828262329 \n",
      "     Training Step: 63 Training Loss: 2.8429057598114014 \n",
      "     Training Step: 64 Training Loss: 4.12801456451416 \n",
      "     Training Step: 65 Training Loss: 2.37164568901062 \n",
      "     Training Step: 66 Training Loss: 4.199367046356201 \n",
      "     Training Step: 67 Training Loss: 2.6401526927948 \n",
      "     Training Step: 68 Training Loss: 2.4857144355773926 \n",
      "     Training Step: 69 Training Loss: 2.565760612487793 \n",
      "     Training Step: 70 Training Loss: 2.721193552017212 \n",
      "     Training Step: 71 Training Loss: 3.8316667079925537 \n",
      "     Training Step: 72 Training Loss: 3.205169439315796 \n",
      "     Training Step: 73 Training Loss: 3.3253824710845947 \n",
      "     Training Step: 74 Training Loss: 2.7015182971954346 \n",
      "     Training Step: 75 Training Loss: 2.681657552719116 \n",
      "     Training Step: 76 Training Loss: 2.801240921020508 \n",
      "     Training Step: 77 Training Loss: 3.0749926567077637 \n",
      "     Training Step: 78 Training Loss: 2.207120895385742 \n",
      "     Training Step: 79 Training Loss: 4.0178303718566895 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.828429698944092 \n",
      "     Validation Step: 1 Validation Loss: 4.197119235992432 \n",
      "     Validation Step: 2 Validation Loss: 3.5465245246887207 \n",
      "     Validation Step: 3 Validation Loss: 3.6235432624816895 \n",
      "     Validation Step: 4 Validation Loss: 2.683095693588257 \n",
      "     Validation Step: 5 Validation Loss: 4.505313873291016 \n",
      "     Validation Step: 6 Validation Loss: 3.656599283218384 \n",
      "     Validation Step: 7 Validation Loss: 4.333347320556641 \n",
      "     Validation Step: 8 Validation Loss: 3.793977737426758 \n",
      "     Validation Step: 9 Validation Loss: 3.3418149948120117 \n",
      "     Validation Step: 10 Validation Loss: 2.5750203132629395 \n",
      "     Validation Step: 11 Validation Loss: 3.6512160301208496 \n",
      "     Validation Step: 12 Validation Loss: 2.595144510269165 \n",
      "     Validation Step: 13 Validation Loss: 4.450430870056152 \n",
      "     Validation Step: 14 Validation Loss: 3.0166468620300293 \n",
      "Epoch: 67\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.9800796508789062 \n",
      "     Training Step: 1 Training Loss: 2.404939651489258 \n",
      "     Training Step: 2 Training Loss: 2.968441963195801 \n",
      "     Training Step: 3 Training Loss: 3.6953983306884766 \n",
      "     Training Step: 4 Training Loss: 3.8599987030029297 \n",
      "     Training Step: 5 Training Loss: 3.1228983402252197 \n",
      "     Training Step: 6 Training Loss: 2.333810329437256 \n",
      "     Training Step: 7 Training Loss: 3.110743522644043 \n",
      "     Training Step: 8 Training Loss: 3.337332248687744 \n",
      "     Training Step: 9 Training Loss: 2.82924222946167 \n",
      "     Training Step: 10 Training Loss: 2.4106948375701904 \n",
      "     Training Step: 11 Training Loss: 3.095079183578491 \n",
      "     Training Step: 12 Training Loss: 2.444843053817749 \n",
      "     Training Step: 13 Training Loss: 4.1123199462890625 \n",
      "     Training Step: 14 Training Loss: 3.220590829849243 \n",
      "     Training Step: 15 Training Loss: 2.131993532180786 \n",
      "     Training Step: 16 Training Loss: 2.8694229125976562 \n",
      "     Training Step: 17 Training Loss: 4.137569904327393 \n",
      "     Training Step: 18 Training Loss: 2.2587108612060547 \n",
      "     Training Step: 19 Training Loss: 3.3002805709838867 \n",
      "     Training Step: 20 Training Loss: 3.308314561843872 \n",
      "     Training Step: 21 Training Loss: 2.6860976219177246 \n",
      "     Training Step: 22 Training Loss: 2.951270341873169 \n",
      "     Training Step: 23 Training Loss: 3.1124143600463867 \n",
      "     Training Step: 24 Training Loss: 3.300198554992676 \n",
      "     Training Step: 25 Training Loss: 3.1401734352111816 \n",
      "     Training Step: 26 Training Loss: 3.372130870819092 \n",
      "     Training Step: 27 Training Loss: 2.4699552059173584 \n",
      "     Training Step: 28 Training Loss: 3.412726879119873 \n",
      "     Training Step: 29 Training Loss: 3.546781539916992 \n",
      "     Training Step: 30 Training Loss: 2.3688292503356934 \n",
      "     Training Step: 31 Training Loss: 3.266605854034424 \n",
      "     Training Step: 32 Training Loss: 2.29292893409729 \n",
      "     Training Step: 33 Training Loss: 2.540274143218994 \n",
      "     Training Step: 34 Training Loss: 3.1423349380493164 \n",
      "     Training Step: 35 Training Loss: 2.8153107166290283 \n",
      "     Training Step: 36 Training Loss: 2.2511684894561768 \n",
      "     Training Step: 37 Training Loss: 2.931042194366455 \n",
      "     Training Step: 38 Training Loss: 3.5365376472473145 \n",
      "     Training Step: 39 Training Loss: 2.0464277267456055 \n",
      "     Training Step: 40 Training Loss: 2.648054599761963 \n",
      "     Training Step: 41 Training Loss: 3.1463382244110107 \n",
      "     Training Step: 42 Training Loss: 3.9189534187316895 \n",
      "     Training Step: 43 Training Loss: 2.12688946723938 \n",
      "     Training Step: 44 Training Loss: 2.584662675857544 \n",
      "     Training Step: 45 Training Loss: 2.915128231048584 \n",
      "     Training Step: 46 Training Loss: 3.7320687770843506 \n",
      "     Training Step: 47 Training Loss: 3.1199893951416016 \n",
      "     Training Step: 48 Training Loss: 2.626598834991455 \n",
      "     Training Step: 49 Training Loss: 3.555039882659912 \n",
      "     Training Step: 50 Training Loss: 3.595332622528076 \n",
      "     Training Step: 51 Training Loss: 3.6844005584716797 \n",
      "     Training Step: 52 Training Loss: 3.7816548347473145 \n",
      "     Training Step: 53 Training Loss: 2.503875255584717 \n",
      "     Training Step: 54 Training Loss: 2.49776029586792 \n",
      "     Training Step: 55 Training Loss: 4.026512622833252 \n",
      "     Training Step: 56 Training Loss: 4.050997257232666 \n",
      "     Training Step: 57 Training Loss: 2.4530670642852783 \n",
      "     Training Step: 58 Training Loss: 3.1619176864624023 \n",
      "     Training Step: 59 Training Loss: 2.446878671646118 \n",
      "     Training Step: 60 Training Loss: 3.659313201904297 \n",
      "     Training Step: 61 Training Loss: 3.094566822052002 \n",
      "     Training Step: 62 Training Loss: 2.3307206630706787 \n",
      "     Training Step: 63 Training Loss: 2.230520248413086 \n",
      "     Training Step: 64 Training Loss: 2.4143435955047607 \n",
      "     Training Step: 65 Training Loss: 2.4176065921783447 \n",
      "     Training Step: 66 Training Loss: 2.1693615913391113 \n",
      "     Training Step: 67 Training Loss: 2.4591567516326904 \n",
      "     Training Step: 68 Training Loss: 2.457047939300537 \n",
      "     Training Step: 69 Training Loss: 3.645663261413574 \n",
      "     Training Step: 70 Training Loss: 2.6148295402526855 \n",
      "     Training Step: 71 Training Loss: 3.6586437225341797 \n",
      "     Training Step: 72 Training Loss: 3.2987351417541504 \n",
      "     Training Step: 73 Training Loss: 2.7996246814727783 \n",
      "     Training Step: 74 Training Loss: 3.1399145126342773 \n",
      "     Training Step: 75 Training Loss: 3.702789306640625 \n",
      "     Training Step: 76 Training Loss: 2.5563578605651855 \n",
      "     Training Step: 77 Training Loss: 3.049520492553711 \n",
      "     Training Step: 78 Training Loss: 3.159393310546875 \n",
      "     Training Step: 79 Training Loss: 2.6528210639953613 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.245046854019165 \n",
      "     Validation Step: 1 Validation Loss: 2.7663326263427734 \n",
      "     Validation Step: 2 Validation Loss: 2.2890074253082275 \n",
      "     Validation Step: 3 Validation Loss: 3.5367319583892822 \n",
      "     Validation Step: 4 Validation Loss: 3.752258777618408 \n",
      "     Validation Step: 5 Validation Loss: 3.3443093299865723 \n",
      "     Validation Step: 6 Validation Loss: 2.5215110778808594 \n",
      "     Validation Step: 7 Validation Loss: 3.1630852222442627 \n",
      "     Validation Step: 8 Validation Loss: 2.254012107849121 \n",
      "     Validation Step: 9 Validation Loss: 2.578977584838867 \n",
      "     Validation Step: 10 Validation Loss: 2.926823139190674 \n",
      "     Validation Step: 11 Validation Loss: 2.5314865112304688 \n",
      "     Validation Step: 12 Validation Loss: 2.9408857822418213 \n",
      "     Validation Step: 13 Validation Loss: 2.9075193405151367 \n",
      "     Validation Step: 14 Validation Loss: 3.76043701171875 \n",
      "Epoch: 68\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.92633056640625 \n",
      "     Training Step: 1 Training Loss: 3.888920783996582 \n",
      "     Training Step: 2 Training Loss: 4.255284786224365 \n",
      "     Training Step: 3 Training Loss: 2.32720685005188 \n",
      "     Training Step: 4 Training Loss: 3.0711750984191895 \n",
      "     Training Step: 5 Training Loss: 3.0128839015960693 \n",
      "     Training Step: 6 Training Loss: 3.2994914054870605 \n",
      "     Training Step: 7 Training Loss: 4.189970970153809 \n",
      "     Training Step: 8 Training Loss: 3.446687698364258 \n",
      "     Training Step: 9 Training Loss: 3.4511466026306152 \n",
      "     Training Step: 10 Training Loss: 2.2584075927734375 \n",
      "     Training Step: 11 Training Loss: 2.229454278945923 \n",
      "     Training Step: 12 Training Loss: 3.8915820121765137 \n",
      "     Training Step: 13 Training Loss: 2.9185805320739746 \n",
      "     Training Step: 14 Training Loss: 3.120506763458252 \n",
      "     Training Step: 15 Training Loss: 3.2052130699157715 \n",
      "     Training Step: 16 Training Loss: 2.2704856395721436 \n",
      "     Training Step: 17 Training Loss: 2.7452425956726074 \n",
      "     Training Step: 18 Training Loss: 2.514890432357788 \n",
      "     Training Step: 19 Training Loss: 2.288527727127075 \n",
      "     Training Step: 20 Training Loss: 2.964277744293213 \n",
      "     Training Step: 21 Training Loss: 2.6581850051879883 \n",
      "     Training Step: 22 Training Loss: 2.586643695831299 \n",
      "     Training Step: 23 Training Loss: 3.4925711154937744 \n",
      "     Training Step: 24 Training Loss: 3.0198493003845215 \n",
      "     Training Step: 25 Training Loss: 3.110100269317627 \n",
      "     Training Step: 26 Training Loss: 3.5676517486572266 \n",
      "     Training Step: 27 Training Loss: 3.1490564346313477 \n",
      "     Training Step: 28 Training Loss: 2.0429420471191406 \n",
      "     Training Step: 29 Training Loss: 2.8243188858032227 \n",
      "     Training Step: 30 Training Loss: 2.632249355316162 \n",
      "     Training Step: 31 Training Loss: 2.1323814392089844 \n",
      "     Training Step: 32 Training Loss: 3.132200241088867 \n",
      "     Training Step: 33 Training Loss: 3.4895029067993164 \n",
      "     Training Step: 34 Training Loss: 3.348206043243408 \n",
      "     Training Step: 35 Training Loss: 2.472032308578491 \n",
      "     Training Step: 36 Training Loss: 2.5806922912597656 \n",
      "     Training Step: 37 Training Loss: 2.9500184059143066 \n",
      "     Training Step: 38 Training Loss: 3.2351291179656982 \n",
      "     Training Step: 39 Training Loss: 2.5871729850769043 \n",
      "     Training Step: 40 Training Loss: 3.031161308288574 \n",
      "     Training Step: 41 Training Loss: 2.394306182861328 \n",
      "     Training Step: 42 Training Loss: 3.5740151405334473 \n",
      "     Training Step: 43 Training Loss: 3.485480785369873 \n",
      "     Training Step: 44 Training Loss: 2.946516513824463 \n",
      "     Training Step: 45 Training Loss: 3.057931423187256 \n",
      "     Training Step: 46 Training Loss: 2.6497881412506104 \n",
      "     Training Step: 47 Training Loss: 3.3137545585632324 \n",
      "     Training Step: 48 Training Loss: 2.2678942680358887 \n",
      "     Training Step: 49 Training Loss: 3.773848533630371 \n",
      "     Training Step: 50 Training Loss: 2.6342127323150635 \n",
      "     Training Step: 51 Training Loss: 4.194113731384277 \n",
      "     Training Step: 52 Training Loss: 3.086387872695923 \n",
      "     Training Step: 53 Training Loss: 2.8747386932373047 \n",
      "     Training Step: 54 Training Loss: 2.9177238941192627 \n",
      "     Training Step: 55 Training Loss: 3.993530750274658 \n",
      "     Training Step: 56 Training Loss: 3.256727695465088 \n",
      "     Training Step: 57 Training Loss: 2.7596187591552734 \n",
      "     Training Step: 58 Training Loss: 2.404909610748291 \n",
      "     Training Step: 59 Training Loss: 2.506186008453369 \n",
      "     Training Step: 60 Training Loss: 3.189793586730957 \n",
      "     Training Step: 61 Training Loss: 2.401590585708618 \n",
      "     Training Step: 62 Training Loss: 2.659193992614746 \n",
      "     Training Step: 63 Training Loss: 3.0411224365234375 \n",
      "     Training Step: 64 Training Loss: 2.4407236576080322 \n",
      "     Training Step: 65 Training Loss: 2.955921173095703 \n",
      "     Training Step: 66 Training Loss: 2.8083810806274414 \n",
      "     Training Step: 67 Training Loss: 3.055178165435791 \n",
      "     Training Step: 68 Training Loss: 3.1392412185668945 \n",
      "     Training Step: 69 Training Loss: 2.0809638500213623 \n",
      "     Training Step: 70 Training Loss: 2.4605884552001953 \n",
      "     Training Step: 71 Training Loss: 3.4012644290924072 \n",
      "     Training Step: 72 Training Loss: 2.8914644718170166 \n",
      "     Training Step: 73 Training Loss: 3.0658347606658936 \n",
      "     Training Step: 74 Training Loss: 3.0409350395202637 \n",
      "     Training Step: 75 Training Loss: 3.206784248352051 \n",
      "     Training Step: 76 Training Loss: 3.6537787914276123 \n",
      "     Training Step: 77 Training Loss: 3.303849697113037 \n",
      "     Training Step: 78 Training Loss: 2.143547534942627 \n",
      "     Training Step: 79 Training Loss: 2.591796398162842 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1738250255584717 \n",
      "     Validation Step: 1 Validation Loss: 2.976388692855835 \n",
      "     Validation Step: 2 Validation Loss: 2.900782823562622 \n",
      "     Validation Step: 3 Validation Loss: 2.64689040184021 \n",
      "     Validation Step: 4 Validation Loss: 3.5986595153808594 \n",
      "     Validation Step: 5 Validation Loss: 2.8018083572387695 \n",
      "     Validation Step: 6 Validation Loss: 3.9189863204956055 \n",
      "     Validation Step: 7 Validation Loss: 2.3344802856445312 \n",
      "     Validation Step: 8 Validation Loss: 3.2001137733459473 \n",
      "     Validation Step: 9 Validation Loss: 3.630046844482422 \n",
      "     Validation Step: 10 Validation Loss: 2.641007423400879 \n",
      "     Validation Step: 11 Validation Loss: 3.10603666305542 \n",
      "     Validation Step: 12 Validation Loss: 3.5892157554626465 \n",
      "     Validation Step: 13 Validation Loss: 3.2980189323425293 \n",
      "     Validation Step: 14 Validation Loss: 2.805398464202881 \n",
      "Epoch: 69\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.4569618701934814 \n",
      "     Training Step: 1 Training Loss: 3.118788480758667 \n",
      "     Training Step: 2 Training Loss: 2.4244837760925293 \n",
      "     Training Step: 3 Training Loss: 3.1173019409179688 \n",
      "     Training Step: 4 Training Loss: 2.9197497367858887 \n",
      "     Training Step: 5 Training Loss: 3.452955722808838 \n",
      "     Training Step: 6 Training Loss: 2.28100323677063 \n",
      "     Training Step: 7 Training Loss: 3.7038564682006836 \n",
      "     Training Step: 8 Training Loss: 3.1633975505828857 \n",
      "     Training Step: 9 Training Loss: 2.5192508697509766 \n",
      "     Training Step: 10 Training Loss: 3.100217819213867 \n",
      "     Training Step: 11 Training Loss: 2.558281421661377 \n",
      "     Training Step: 12 Training Loss: 2.336671829223633 \n",
      "     Training Step: 13 Training Loss: 2.6309447288513184 \n",
      "     Training Step: 14 Training Loss: 3.7643752098083496 \n",
      "     Training Step: 15 Training Loss: 3.329288959503174 \n",
      "     Training Step: 16 Training Loss: 2.8244190216064453 \n",
      "     Training Step: 17 Training Loss: 3.1597585678100586 \n",
      "     Training Step: 18 Training Loss: 2.4332666397094727 \n",
      "     Training Step: 19 Training Loss: 3.504819393157959 \n",
      "     Training Step: 20 Training Loss: 2.9182848930358887 \n",
      "     Training Step: 21 Training Loss: 3.1757771968841553 \n",
      "     Training Step: 22 Training Loss: 3.2260148525238037 \n",
      "     Training Step: 23 Training Loss: 2.7132151126861572 \n",
      "     Training Step: 24 Training Loss: 2.0834109783172607 \n",
      "     Training Step: 25 Training Loss: 2.6420178413391113 \n",
      "     Training Step: 26 Training Loss: 3.084001302719116 \n",
      "     Training Step: 27 Training Loss: 2.719355583190918 \n",
      "     Training Step: 28 Training Loss: 2.2316341400146484 \n",
      "     Training Step: 29 Training Loss: 2.5876529216766357 \n",
      "     Training Step: 30 Training Loss: 2.870124101638794 \n",
      "     Training Step: 31 Training Loss: 4.296316146850586 \n",
      "     Training Step: 32 Training Loss: 3.0355005264282227 \n",
      "     Training Step: 33 Training Loss: 3.12387752532959 \n",
      "     Training Step: 34 Training Loss: 2.9071879386901855 \n",
      "     Training Step: 35 Training Loss: 3.0135061740875244 \n",
      "     Training Step: 36 Training Loss: 3.276231288909912 \n",
      "     Training Step: 37 Training Loss: 3.941862106323242 \n",
      "     Training Step: 38 Training Loss: 3.645176410675049 \n",
      "     Training Step: 39 Training Loss: 2.52834415435791 \n",
      "     Training Step: 40 Training Loss: 2.8216867446899414 \n",
      "     Training Step: 41 Training Loss: 2.282526969909668 \n",
      "     Training Step: 42 Training Loss: 2.809553623199463 \n",
      "     Training Step: 43 Training Loss: 3.769899368286133 \n",
      "     Training Step: 44 Training Loss: 2.608783721923828 \n",
      "     Training Step: 45 Training Loss: 2.8472492694854736 \n",
      "     Training Step: 46 Training Loss: 3.091465950012207 \n",
      "     Training Step: 47 Training Loss: 3.4579672813415527 \n",
      "     Training Step: 48 Training Loss: 3.446979284286499 \n",
      "     Training Step: 49 Training Loss: 2.7618069648742676 \n",
      "     Training Step: 50 Training Loss: 4.327534198760986 \n",
      "     Training Step: 51 Training Loss: 2.286712169647217 \n",
      "     Training Step: 52 Training Loss: 3.675461530685425 \n",
      "     Training Step: 53 Training Loss: 2.493455410003662 \n",
      "     Training Step: 54 Training Loss: 2.844625949859619 \n",
      "     Training Step: 55 Training Loss: 2.324678421020508 \n",
      "     Training Step: 56 Training Loss: 2.46878719329834 \n",
      "     Training Step: 57 Training Loss: 3.6717286109924316 \n",
      "     Training Step: 58 Training Loss: 2.870272159576416 \n",
      "     Training Step: 59 Training Loss: 2.6702914237976074 \n",
      "     Training Step: 60 Training Loss: 3.5796926021575928 \n",
      "     Training Step: 61 Training Loss: 3.891183376312256 \n",
      "     Training Step: 62 Training Loss: 3.489076614379883 \n",
      "     Training Step: 63 Training Loss: 2.6481752395629883 \n",
      "     Training Step: 64 Training Loss: 4.025807857513428 \n",
      "     Training Step: 65 Training Loss: 3.1932716369628906 \n",
      "     Training Step: 66 Training Loss: 2.8548648357391357 \n",
      "     Training Step: 67 Training Loss: 2.895651340484619 \n",
      "     Training Step: 68 Training Loss: 3.1967365741729736 \n",
      "     Training Step: 69 Training Loss: 3.0581817626953125 \n",
      "     Training Step: 70 Training Loss: 3.4135994911193848 \n",
      "     Training Step: 71 Training Loss: 3.4416608810424805 \n",
      "     Training Step: 72 Training Loss: 3.0653493404388428 \n",
      "     Training Step: 73 Training Loss: 2.87869930267334 \n",
      "     Training Step: 74 Training Loss: 2.4079794883728027 \n",
      "     Training Step: 75 Training Loss: 3.728273868560791 \n",
      "     Training Step: 76 Training Loss: 2.7098488807678223 \n",
      "     Training Step: 77 Training Loss: 2.8159189224243164 \n",
      "     Training Step: 78 Training Loss: 2.86495304107666 \n",
      "     Training Step: 79 Training Loss: 2.741276264190674 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.4629292488098145 \n",
      "     Validation Step: 1 Validation Loss: 3.186652660369873 \n",
      "     Validation Step: 2 Validation Loss: 3.0839099884033203 \n",
      "     Validation Step: 3 Validation Loss: 3.952679395675659 \n",
      "     Validation Step: 4 Validation Loss: 2.7806310653686523 \n",
      "     Validation Step: 5 Validation Loss: 3.3821873664855957 \n",
      "     Validation Step: 6 Validation Loss: 3.904107093811035 \n",
      "     Validation Step: 7 Validation Loss: 4.131795883178711 \n",
      "     Validation Step: 8 Validation Loss: 3.0460362434387207 \n",
      "     Validation Step: 9 Validation Loss: 3.387998104095459 \n",
      "     Validation Step: 10 Validation Loss: 3.5851681232452393 \n",
      "     Validation Step: 11 Validation Loss: 3.3068745136260986 \n",
      "     Validation Step: 12 Validation Loss: 2.8595941066741943 \n",
      "     Validation Step: 13 Validation Loss: 2.758230686187744 \n",
      "     Validation Step: 14 Validation Loss: 3.1515703201293945 \n",
      "Epoch: 70\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.092508554458618 \n",
      "     Training Step: 1 Training Loss: 2.3504714965820312 \n",
      "     Training Step: 2 Training Loss: 3.7740707397460938 \n",
      "     Training Step: 3 Training Loss: 3.581231117248535 \n",
      "     Training Step: 4 Training Loss: 2.3271303176879883 \n",
      "     Training Step: 5 Training Loss: 2.433692216873169 \n",
      "     Training Step: 6 Training Loss: 2.3223042488098145 \n",
      "     Training Step: 7 Training Loss: 3.330448865890503 \n",
      "     Training Step: 8 Training Loss: 2.610761880874634 \n",
      "     Training Step: 9 Training Loss: 3.693258047103882 \n",
      "     Training Step: 10 Training Loss: 3.0851614475250244 \n",
      "     Training Step: 11 Training Loss: 3.4503250122070312 \n",
      "     Training Step: 12 Training Loss: 2.4077529907226562 \n",
      "     Training Step: 13 Training Loss: 2.4707589149475098 \n",
      "     Training Step: 14 Training Loss: 2.511570692062378 \n",
      "     Training Step: 15 Training Loss: 2.7364187240600586 \n",
      "     Training Step: 16 Training Loss: 2.5472476482391357 \n",
      "     Training Step: 17 Training Loss: 2.656698703765869 \n",
      "     Training Step: 18 Training Loss: 2.339851140975952 \n",
      "     Training Step: 19 Training Loss: 3.4255573749542236 \n",
      "     Training Step: 20 Training Loss: 3.1218056678771973 \n",
      "     Training Step: 21 Training Loss: 2.902228832244873 \n",
      "     Training Step: 22 Training Loss: 2.4731690883636475 \n",
      "     Training Step: 23 Training Loss: 2.4494121074676514 \n",
      "     Training Step: 24 Training Loss: 2.1316781044006348 \n",
      "     Training Step: 25 Training Loss: 2.2452292442321777 \n",
      "     Training Step: 26 Training Loss: 3.619837522506714 \n",
      "     Training Step: 27 Training Loss: 2.962334632873535 \n",
      "     Training Step: 28 Training Loss: 2.156996488571167 \n",
      "     Training Step: 29 Training Loss: 2.778700113296509 \n",
      "     Training Step: 30 Training Loss: 2.9550037384033203 \n",
      "     Training Step: 31 Training Loss: 2.5129175186157227 \n",
      "     Training Step: 32 Training Loss: 4.55842399597168 \n",
      "     Training Step: 33 Training Loss: 3.26843523979187 \n",
      "     Training Step: 34 Training Loss: 3.4783873558044434 \n",
      "     Training Step: 35 Training Loss: 2.663784980773926 \n",
      "     Training Step: 36 Training Loss: 4.338017463684082 \n",
      "     Training Step: 37 Training Loss: 2.3144054412841797 \n",
      "     Training Step: 38 Training Loss: 2.8360068798065186 \n",
      "     Training Step: 39 Training Loss: 2.4673337936401367 \n",
      "     Training Step: 40 Training Loss: 2.619072914123535 \n",
      "     Training Step: 41 Training Loss: 2.926816940307617 \n",
      "     Training Step: 42 Training Loss: 3.1883339881896973 \n",
      "     Training Step: 43 Training Loss: 2.591238498687744 \n",
      "     Training Step: 44 Training Loss: 4.169231414794922 \n",
      "     Training Step: 45 Training Loss: 3.551642894744873 \n",
      "     Training Step: 46 Training Loss: 2.6757893562316895 \n",
      "     Training Step: 47 Training Loss: 2.7308573722839355 \n",
      "     Training Step: 48 Training Loss: 3.8725790977478027 \n",
      "     Training Step: 49 Training Loss: 2.725267171859741 \n",
      "     Training Step: 50 Training Loss: 2.5984106063842773 \n",
      "     Training Step: 51 Training Loss: 4.013891220092773 \n",
      "     Training Step: 52 Training Loss: 2.3238179683685303 \n",
      "     Training Step: 53 Training Loss: 2.9592602252960205 \n",
      "     Training Step: 54 Training Loss: 3.466717004776001 \n",
      "     Training Step: 55 Training Loss: 2.2259469032287598 \n",
      "     Training Step: 56 Training Loss: 2.4811506271362305 \n",
      "     Training Step: 57 Training Loss: 2.326690673828125 \n",
      "     Training Step: 58 Training Loss: 3.901884078979492 \n",
      "     Training Step: 59 Training Loss: 2.9461183547973633 \n",
      "     Training Step: 60 Training Loss: 3.6660494804382324 \n",
      "     Training Step: 61 Training Loss: 2.8513550758361816 \n",
      "     Training Step: 62 Training Loss: 3.0426859855651855 \n",
      "     Training Step: 63 Training Loss: 3.4102654457092285 \n",
      "     Training Step: 64 Training Loss: 3.62593936920166 \n",
      "     Training Step: 65 Training Loss: 2.2639670372009277 \n",
      "     Training Step: 66 Training Loss: 2.8863027095794678 \n",
      "     Training Step: 67 Training Loss: 2.6659774780273438 \n",
      "     Training Step: 68 Training Loss: 3.024656295776367 \n",
      "     Training Step: 69 Training Loss: 2.8956503868103027 \n",
      "     Training Step: 70 Training Loss: 2.6805779933929443 \n",
      "     Training Step: 71 Training Loss: 3.0748329162597656 \n",
      "     Training Step: 72 Training Loss: 3.0382089614868164 \n",
      "     Training Step: 73 Training Loss: 3.0359930992126465 \n",
      "     Training Step: 74 Training Loss: 4.26733922958374 \n",
      "     Training Step: 75 Training Loss: 3.740424156188965 \n",
      "     Training Step: 76 Training Loss: 3.260685443878174 \n",
      "     Training Step: 77 Training Loss: 2.789181709289551 \n",
      "     Training Step: 78 Training Loss: 2.4530529975891113 \n",
      "     Training Step: 79 Training Loss: 2.1625802516937256 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.839892864227295 \n",
      "     Validation Step: 1 Validation Loss: 3.652275800704956 \n",
      "     Validation Step: 2 Validation Loss: 3.140718460083008 \n",
      "     Validation Step: 3 Validation Loss: 3.0058722496032715 \n",
      "     Validation Step: 4 Validation Loss: 3.616770029067993 \n",
      "     Validation Step: 5 Validation Loss: 3.241367816925049 \n",
      "     Validation Step: 6 Validation Loss: 2.8264873027801514 \n",
      "     Validation Step: 7 Validation Loss: 3.220012903213501 \n",
      "     Validation Step: 8 Validation Loss: 3.7282748222351074 \n",
      "     Validation Step: 9 Validation Loss: 3.033780813217163 \n",
      "     Validation Step: 10 Validation Loss: 2.0371928215026855 \n",
      "     Validation Step: 11 Validation Loss: 3.1720359325408936 \n",
      "     Validation Step: 12 Validation Loss: 3.2643210887908936 \n",
      "     Validation Step: 13 Validation Loss: 3.8247547149658203 \n",
      "     Validation Step: 14 Validation Loss: 2.994004487991333 \n",
      "Epoch: 71\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.2144975662231445 \n",
      "     Training Step: 1 Training Loss: 3.11665678024292 \n",
      "     Training Step: 2 Training Loss: 3.1273820400238037 \n",
      "     Training Step: 3 Training Loss: 2.870293140411377 \n",
      "     Training Step: 4 Training Loss: 2.6075549125671387 \n",
      "     Training Step: 5 Training Loss: 3.7189786434173584 \n",
      "     Training Step: 6 Training Loss: 3.311227321624756 \n",
      "     Training Step: 7 Training Loss: 3.038839817047119 \n",
      "     Training Step: 8 Training Loss: 3.7889065742492676 \n",
      "     Training Step: 9 Training Loss: 2.5586297512054443 \n",
      "     Training Step: 10 Training Loss: 2.867814064025879 \n",
      "     Training Step: 11 Training Loss: 2.6700611114501953 \n",
      "     Training Step: 12 Training Loss: 2.5887975692749023 \n",
      "     Training Step: 13 Training Loss: 2.653841972351074 \n",
      "     Training Step: 14 Training Loss: 3.4666523933410645 \n",
      "     Training Step: 15 Training Loss: 2.835458517074585 \n",
      "     Training Step: 16 Training Loss: 2.85123348236084 \n",
      "     Training Step: 17 Training Loss: 2.133788585662842 \n",
      "     Training Step: 18 Training Loss: 3.039170026779175 \n",
      "     Training Step: 19 Training Loss: 2.2482900619506836 \n",
      "     Training Step: 20 Training Loss: 2.7020068168640137 \n",
      "     Training Step: 21 Training Loss: 2.0196237564086914 \n",
      "     Training Step: 22 Training Loss: 3.631206750869751 \n",
      "     Training Step: 23 Training Loss: 2.964311122894287 \n",
      "     Training Step: 24 Training Loss: 2.4331789016723633 \n",
      "     Training Step: 25 Training Loss: 3.993518352508545 \n",
      "     Training Step: 26 Training Loss: 3.9214391708374023 \n",
      "     Training Step: 27 Training Loss: 4.110648155212402 \n",
      "     Training Step: 28 Training Loss: 2.516742706298828 \n",
      "     Training Step: 29 Training Loss: 4.3122029304504395 \n",
      "     Training Step: 30 Training Loss: 3.9425315856933594 \n",
      "     Training Step: 31 Training Loss: 3.1601760387420654 \n",
      "     Training Step: 32 Training Loss: 3.585658073425293 \n",
      "     Training Step: 33 Training Loss: 3.7493906021118164 \n",
      "     Training Step: 34 Training Loss: 3.2103147506713867 \n",
      "     Training Step: 35 Training Loss: 2.6787304878234863 \n",
      "     Training Step: 36 Training Loss: 2.920753240585327 \n",
      "     Training Step: 37 Training Loss: 3.4049129486083984 \n",
      "     Training Step: 38 Training Loss: 3.2271852493286133 \n",
      "     Training Step: 39 Training Loss: 3.935506820678711 \n",
      "     Training Step: 40 Training Loss: 3.004455089569092 \n",
      "     Training Step: 41 Training Loss: 2.689412832260132 \n",
      "     Training Step: 42 Training Loss: 2.643517255783081 \n",
      "     Training Step: 43 Training Loss: 3.3351805210113525 \n",
      "     Training Step: 44 Training Loss: 2.72222900390625 \n",
      "     Training Step: 45 Training Loss: 3.2947134971618652 \n",
      "     Training Step: 46 Training Loss: 2.178083896636963 \n",
      "     Training Step: 47 Training Loss: 3.057300090789795 \n",
      "     Training Step: 48 Training Loss: 3.946108341217041 \n",
      "     Training Step: 49 Training Loss: 2.2175238132476807 \n",
      "     Training Step: 50 Training Loss: 2.203476905822754 \n",
      "     Training Step: 51 Training Loss: 2.7790305614471436 \n",
      "     Training Step: 52 Training Loss: 2.2491555213928223 \n",
      "     Training Step: 53 Training Loss: 3.3149826526641846 \n",
      "     Training Step: 54 Training Loss: 3.2599215507507324 \n",
      "     Training Step: 55 Training Loss: 4.5201568603515625 \n",
      "     Training Step: 56 Training Loss: 2.9347825050354004 \n",
      "     Training Step: 57 Training Loss: 3.4656643867492676 \n",
      "     Training Step: 58 Training Loss: 3.0578501224517822 \n",
      "     Training Step: 59 Training Loss: 3.6030285358428955 \n",
      "     Training Step: 60 Training Loss: 2.422071933746338 \n",
      "     Training Step: 61 Training Loss: 3.132904529571533 \n",
      "     Training Step: 62 Training Loss: 3.4488561153411865 \n",
      "     Training Step: 63 Training Loss: 2.3296589851379395 \n",
      "     Training Step: 64 Training Loss: 2.7239973545074463 \n",
      "     Training Step: 65 Training Loss: 2.173391819000244 \n",
      "     Training Step: 66 Training Loss: 3.9351296424865723 \n",
      "     Training Step: 67 Training Loss: 3.5744895935058594 \n",
      "     Training Step: 68 Training Loss: 2.530240058898926 \n",
      "     Training Step: 69 Training Loss: 3.099202871322632 \n",
      "     Training Step: 70 Training Loss: 2.7750301361083984 \n",
      "     Training Step: 71 Training Loss: 3.0274832248687744 \n",
      "     Training Step: 72 Training Loss: 2.688926935195923 \n",
      "     Training Step: 73 Training Loss: 2.340763568878174 \n",
      "     Training Step: 74 Training Loss: 2.9146933555603027 \n",
      "     Training Step: 75 Training Loss: 3.010559558868408 \n",
      "     Training Step: 76 Training Loss: 3.3687686920166016 \n",
      "     Training Step: 77 Training Loss: 2.8069818019866943 \n",
      "     Training Step: 78 Training Loss: 3.9904985427856445 \n",
      "     Training Step: 79 Training Loss: 3.0871338844299316 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9581360816955566 \n",
      "     Validation Step: 1 Validation Loss: 3.515228271484375 \n",
      "     Validation Step: 2 Validation Loss: 3.146388530731201 \n",
      "     Validation Step: 3 Validation Loss: 3.9771599769592285 \n",
      "     Validation Step: 4 Validation Loss: 2.77168869972229 \n",
      "     Validation Step: 5 Validation Loss: 3.3409769535064697 \n",
      "     Validation Step: 6 Validation Loss: 4.107226371765137 \n",
      "     Validation Step: 7 Validation Loss: 2.9078633785247803 \n",
      "     Validation Step: 8 Validation Loss: 3.814162015914917 \n",
      "     Validation Step: 9 Validation Loss: 3.182302474975586 \n",
      "     Validation Step: 10 Validation Loss: 2.985215663909912 \n",
      "     Validation Step: 11 Validation Loss: 2.8925960063934326 \n",
      "     Validation Step: 12 Validation Loss: 3.254363536834717 \n",
      "     Validation Step: 13 Validation Loss: 3.6353023052215576 \n",
      "     Validation Step: 14 Validation Loss: 2.3038971424102783 \n",
      "Epoch: 72\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.2357876300811768 \n",
      "     Training Step: 1 Training Loss: 3.831292152404785 \n",
      "     Training Step: 2 Training Loss: 2.3062307834625244 \n",
      "     Training Step: 3 Training Loss: 3.4777581691741943 \n",
      "     Training Step: 4 Training Loss: 3.375150203704834 \n",
      "     Training Step: 5 Training Loss: 2.6379878520965576 \n",
      "     Training Step: 6 Training Loss: 2.910569190979004 \n",
      "     Training Step: 7 Training Loss: 3.5169525146484375 \n",
      "     Training Step: 8 Training Loss: 3.017564296722412 \n",
      "     Training Step: 9 Training Loss: 2.569835662841797 \n",
      "     Training Step: 10 Training Loss: 3.069490432739258 \n",
      "     Training Step: 11 Training Loss: 2.8517491817474365 \n",
      "     Training Step: 12 Training Loss: 2.9531140327453613 \n",
      "     Training Step: 13 Training Loss: 2.6418185234069824 \n",
      "     Training Step: 14 Training Loss: 4.1732025146484375 \n",
      "     Training Step: 15 Training Loss: 2.8207523822784424 \n",
      "     Training Step: 16 Training Loss: 2.351916790008545 \n",
      "     Training Step: 17 Training Loss: 2.1709036827087402 \n",
      "     Training Step: 18 Training Loss: 3.387037992477417 \n",
      "     Training Step: 19 Training Loss: 2.5357494354248047 \n",
      "     Training Step: 20 Training Loss: 3.3668394088745117 \n",
      "     Training Step: 21 Training Loss: 2.658308744430542 \n",
      "     Training Step: 22 Training Loss: 3.1958506107330322 \n",
      "     Training Step: 23 Training Loss: 3.3137245178222656 \n",
      "     Training Step: 24 Training Loss: 2.7852232456207275 \n",
      "     Training Step: 25 Training Loss: 3.22206449508667 \n",
      "     Training Step: 26 Training Loss: 2.6629831790924072 \n",
      "     Training Step: 27 Training Loss: 2.731424331665039 \n",
      "     Training Step: 28 Training Loss: 3.108336925506592 \n",
      "     Training Step: 29 Training Loss: 2.4564194679260254 \n",
      "     Training Step: 30 Training Loss: 3.2378435134887695 \n",
      "     Training Step: 31 Training Loss: 3.816606044769287 \n",
      "     Training Step: 32 Training Loss: 3.4593095779418945 \n",
      "     Training Step: 33 Training Loss: 3.0671849250793457 \n",
      "     Training Step: 34 Training Loss: 3.0405023097991943 \n",
      "     Training Step: 35 Training Loss: 2.400937557220459 \n",
      "     Training Step: 36 Training Loss: 3.151981830596924 \n",
      "     Training Step: 37 Training Loss: 2.869431257247925 \n",
      "     Training Step: 38 Training Loss: 3.344102621078491 \n",
      "     Training Step: 39 Training Loss: 3.0962142944335938 \n",
      "     Training Step: 40 Training Loss: 3.543107032775879 \n",
      "     Training Step: 41 Training Loss: 2.6534597873687744 \n",
      "     Training Step: 42 Training Loss: 2.943896770477295 \n",
      "     Training Step: 43 Training Loss: 3.830364465713501 \n",
      "     Training Step: 44 Training Loss: 2.929915189743042 \n",
      "     Training Step: 45 Training Loss: 3.6600940227508545 \n",
      "     Training Step: 46 Training Loss: 3.2512903213500977 \n",
      "     Training Step: 47 Training Loss: 2.7292771339416504 \n",
      "     Training Step: 48 Training Loss: 2.690009832382202 \n",
      "     Training Step: 49 Training Loss: 3.6105761528015137 \n",
      "     Training Step: 50 Training Loss: 2.111645221710205 \n",
      "     Training Step: 51 Training Loss: 2.672157049179077 \n",
      "     Training Step: 52 Training Loss: 2.1913962364196777 \n",
      "     Training Step: 53 Training Loss: 3.032698154449463 \n",
      "     Training Step: 54 Training Loss: 3.598752498626709 \n",
      "     Training Step: 55 Training Loss: 2.919260025024414 \n",
      "     Training Step: 56 Training Loss: 2.6615428924560547 \n",
      "     Training Step: 57 Training Loss: 3.5123515129089355 \n",
      "     Training Step: 58 Training Loss: 2.9950942993164062 \n",
      "     Training Step: 59 Training Loss: 3.5167832374572754 \n",
      "     Training Step: 60 Training Loss: 2.7507050037384033 \n",
      "     Training Step: 61 Training Loss: 3.177415370941162 \n",
      "     Training Step: 62 Training Loss: 3.277305841445923 \n",
      "     Training Step: 63 Training Loss: 2.6854164600372314 \n",
      "     Training Step: 64 Training Loss: 2.4980900287628174 \n",
      "     Training Step: 65 Training Loss: 3.2005231380462646 \n",
      "     Training Step: 66 Training Loss: 3.3334035873413086 \n",
      "     Training Step: 67 Training Loss: 2.148317337036133 \n",
      "     Training Step: 68 Training Loss: 2.682422161102295 \n",
      "     Training Step: 69 Training Loss: 2.5581765174865723 \n",
      "     Training Step: 70 Training Loss: 4.279880523681641 \n",
      "     Training Step: 71 Training Loss: 2.411449909210205 \n",
      "     Training Step: 72 Training Loss: 3.9151601791381836 \n",
      "     Training Step: 73 Training Loss: 3.0481910705566406 \n",
      "     Training Step: 74 Training Loss: 2.3934521675109863 \n",
      "     Training Step: 75 Training Loss: 3.141209363937378 \n",
      "     Training Step: 76 Training Loss: 3.080069065093994 \n",
      "     Training Step: 77 Training Loss: 2.8056023120880127 \n",
      "     Training Step: 78 Training Loss: 2.5628015995025635 \n",
      "     Training Step: 79 Training Loss: 2.6715610027313232 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.650590419769287 \n",
      "     Validation Step: 1 Validation Loss: 3.399604320526123 \n",
      "     Validation Step: 2 Validation Loss: 3.1098127365112305 \n",
      "     Validation Step: 3 Validation Loss: 2.6424341201782227 \n",
      "     Validation Step: 4 Validation Loss: 3.693294048309326 \n",
      "     Validation Step: 5 Validation Loss: 2.7313904762268066 \n",
      "     Validation Step: 6 Validation Loss: 3.2968146800994873 \n",
      "     Validation Step: 7 Validation Loss: 3.377579689025879 \n",
      "     Validation Step: 8 Validation Loss: 3.16683292388916 \n",
      "     Validation Step: 9 Validation Loss: 2.9194869995117188 \n",
      "     Validation Step: 10 Validation Loss: 2.903313398361206 \n",
      "     Validation Step: 11 Validation Loss: 3.196204900741577 \n",
      "     Validation Step: 12 Validation Loss: 3.4085075855255127 \n",
      "     Validation Step: 13 Validation Loss: 2.658904790878296 \n",
      "     Validation Step: 14 Validation Loss: 2.350834846496582 \n",
      "Epoch: 73\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.6072278022766113 \n",
      "     Training Step: 1 Training Loss: 2.474151134490967 \n",
      "     Training Step: 2 Training Loss: 2.723017692565918 \n",
      "     Training Step: 3 Training Loss: 3.2742176055908203 \n",
      "     Training Step: 4 Training Loss: 3.094348192214966 \n",
      "     Training Step: 5 Training Loss: 3.2585525512695312 \n",
      "     Training Step: 6 Training Loss: 3.94815993309021 \n",
      "     Training Step: 7 Training Loss: 3.2425780296325684 \n",
      "     Training Step: 8 Training Loss: 3.633665084838867 \n",
      "     Training Step: 9 Training Loss: 2.936885356903076 \n",
      "     Training Step: 10 Training Loss: 2.7462894916534424 \n",
      "     Training Step: 11 Training Loss: 2.3317325115203857 \n",
      "     Training Step: 12 Training Loss: 3.3927252292633057 \n",
      "     Training Step: 13 Training Loss: 2.7892704010009766 \n",
      "     Training Step: 14 Training Loss: 3.242896795272827 \n",
      "     Training Step: 15 Training Loss: 3.456852436065674 \n",
      "     Training Step: 16 Training Loss: 2.1038448810577393 \n",
      "     Training Step: 17 Training Loss: 2.8094582557678223 \n",
      "     Training Step: 18 Training Loss: 2.6828455924987793 \n",
      "     Training Step: 19 Training Loss: 2.973184823989868 \n",
      "     Training Step: 20 Training Loss: 2.243375301361084 \n",
      "     Training Step: 21 Training Loss: 4.42046594619751 \n",
      "     Training Step: 22 Training Loss: 3.538667917251587 \n",
      "     Training Step: 23 Training Loss: 3.1004042625427246 \n",
      "     Training Step: 24 Training Loss: 2.643651247024536 \n",
      "     Training Step: 25 Training Loss: 3.6242311000823975 \n",
      "     Training Step: 26 Training Loss: 2.1702218055725098 \n",
      "     Training Step: 27 Training Loss: 2.2070789337158203 \n",
      "     Training Step: 28 Training Loss: 2.1011810302734375 \n",
      "     Training Step: 29 Training Loss: 2.9630744457244873 \n",
      "     Training Step: 30 Training Loss: 2.9432194232940674 \n",
      "     Training Step: 31 Training Loss: 2.7028679847717285 \n",
      "     Training Step: 32 Training Loss: 2.56927227973938 \n",
      "     Training Step: 33 Training Loss: 2.7826180458068848 \n",
      "     Training Step: 34 Training Loss: 3.1068992614746094 \n",
      "     Training Step: 35 Training Loss: 3.715306043624878 \n",
      "     Training Step: 36 Training Loss: 3.334745407104492 \n",
      "     Training Step: 37 Training Loss: 3.111304521560669 \n",
      "     Training Step: 38 Training Loss: 2.5773258209228516 \n",
      "     Training Step: 39 Training Loss: 2.8552913665771484 \n",
      "     Training Step: 40 Training Loss: 2.808370590209961 \n",
      "     Training Step: 41 Training Loss: 3.2276172637939453 \n",
      "     Training Step: 42 Training Loss: 2.372655153274536 \n",
      "     Training Step: 43 Training Loss: 3.2118000984191895 \n",
      "     Training Step: 44 Training Loss: 3.524588108062744 \n",
      "     Training Step: 45 Training Loss: 3.010941743850708 \n",
      "     Training Step: 46 Training Loss: 3.201352596282959 \n",
      "     Training Step: 47 Training Loss: 2.309192180633545 \n",
      "     Training Step: 48 Training Loss: 4.142763137817383 \n",
      "     Training Step: 49 Training Loss: 2.677260398864746 \n",
      "     Training Step: 50 Training Loss: 2.428391456604004 \n",
      "     Training Step: 51 Training Loss: 2.9517111778259277 \n",
      "     Training Step: 52 Training Loss: 3.3177807331085205 \n",
      "     Training Step: 53 Training Loss: 3.4611868858337402 \n",
      "     Training Step: 54 Training Loss: 2.7352242469787598 \n",
      "     Training Step: 55 Training Loss: 2.6941545009613037 \n",
      "     Training Step: 56 Training Loss: 2.6906094551086426 \n",
      "     Training Step: 57 Training Loss: 2.672316551208496 \n",
      "     Training Step: 58 Training Loss: 3.070032835006714 \n",
      "     Training Step: 59 Training Loss: 2.4976673126220703 \n",
      "     Training Step: 60 Training Loss: 2.725794792175293 \n",
      "     Training Step: 61 Training Loss: 3.648561477661133 \n",
      "     Training Step: 62 Training Loss: 4.511281967163086 \n",
      "     Training Step: 63 Training Loss: 2.9737601280212402 \n",
      "     Training Step: 64 Training Loss: 2.4902820587158203 \n",
      "     Training Step: 65 Training Loss: 3.118831157684326 \n",
      "     Training Step: 66 Training Loss: 2.783702850341797 \n",
      "     Training Step: 67 Training Loss: 2.789210796356201 \n",
      "     Training Step: 68 Training Loss: 3.61007022857666 \n",
      "     Training Step: 69 Training Loss: 4.237696647644043 \n",
      "     Training Step: 70 Training Loss: 2.967020034790039 \n",
      "     Training Step: 71 Training Loss: 2.2867612838745117 \n",
      "     Training Step: 72 Training Loss: 3.621962547302246 \n",
      "     Training Step: 73 Training Loss: 3.668031692504883 \n",
      "     Training Step: 74 Training Loss: 2.463397979736328 \n",
      "     Training Step: 75 Training Loss: 2.4368886947631836 \n",
      "     Training Step: 76 Training Loss: 3.248748540878296 \n",
      "     Training Step: 77 Training Loss: 2.149308681488037 \n",
      "     Training Step: 78 Training Loss: 2.6298398971557617 \n",
      "     Training Step: 79 Training Loss: 2.9218690395355225 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.9335880279541016 \n",
      "     Validation Step: 1 Validation Loss: 2.983700752258301 \n",
      "     Validation Step: 2 Validation Loss: 3.4396328926086426 \n",
      "     Validation Step: 3 Validation Loss: 2.907719612121582 \n",
      "     Validation Step: 4 Validation Loss: 3.122973680496216 \n",
      "     Validation Step: 5 Validation Loss: 2.828704595565796 \n",
      "     Validation Step: 6 Validation Loss: 3.3585364818573 \n",
      "     Validation Step: 7 Validation Loss: 3.3923885822296143 \n",
      "     Validation Step: 8 Validation Loss: 2.3764214515686035 \n",
      "     Validation Step: 9 Validation Loss: 3.5788984298706055 \n",
      "     Validation Step: 10 Validation Loss: 3.9011011123657227 \n",
      "     Validation Step: 11 Validation Loss: 2.6351089477539062 \n",
      "     Validation Step: 12 Validation Loss: 2.8502767086029053 \n",
      "     Validation Step: 13 Validation Loss: 4.010265827178955 \n",
      "     Validation Step: 14 Validation Loss: 2.8790955543518066 \n",
      "Epoch: 74\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.36830997467041 \n",
      "     Training Step: 1 Training Loss: 2.213787794113159 \n",
      "     Training Step: 2 Training Loss: 2.9758782386779785 \n",
      "     Training Step: 3 Training Loss: 2.676769733428955 \n",
      "     Training Step: 4 Training Loss: 3.296267509460449 \n",
      "     Training Step: 5 Training Loss: 3.3955254554748535 \n",
      "     Training Step: 6 Training Loss: 2.9434192180633545 \n",
      "     Training Step: 7 Training Loss: 2.2294187545776367 \n",
      "     Training Step: 8 Training Loss: 2.2586917877197266 \n",
      "     Training Step: 9 Training Loss: 3.054830551147461 \n",
      "     Training Step: 10 Training Loss: 3.0235915184020996 \n",
      "     Training Step: 11 Training Loss: 2.8315305709838867 \n",
      "     Training Step: 12 Training Loss: 3.228639841079712 \n",
      "     Training Step: 13 Training Loss: 4.023061752319336 \n",
      "     Training Step: 14 Training Loss: 2.2308502197265625 \n",
      "     Training Step: 15 Training Loss: 3.9324965476989746 \n",
      "     Training Step: 16 Training Loss: 2.5270309448242188 \n",
      "     Training Step: 17 Training Loss: 3.0235469341278076 \n",
      "     Training Step: 18 Training Loss: 3.983541488647461 \n",
      "     Training Step: 19 Training Loss: 3.286546468734741 \n",
      "     Training Step: 20 Training Loss: 3.3651742935180664 \n",
      "     Training Step: 21 Training Loss: 4.387045860290527 \n",
      "     Training Step: 22 Training Loss: 2.789139747619629 \n",
      "     Training Step: 23 Training Loss: 3.3941264152526855 \n",
      "     Training Step: 24 Training Loss: 2.4398069381713867 \n",
      "     Training Step: 25 Training Loss: 3.480649471282959 \n",
      "     Training Step: 26 Training Loss: 2.7214252948760986 \n",
      "     Training Step: 27 Training Loss: 4.423488140106201 \n",
      "     Training Step: 28 Training Loss: 3.700007438659668 \n",
      "     Training Step: 29 Training Loss: 3.362921953201294 \n",
      "     Training Step: 30 Training Loss: 2.658388376235962 \n",
      "     Training Step: 31 Training Loss: 2.8448829650878906 \n",
      "     Training Step: 32 Training Loss: 2.5693845748901367 \n",
      "     Training Step: 33 Training Loss: 2.2262120246887207 \n",
      "     Training Step: 34 Training Loss: 3.0490190982818604 \n",
      "     Training Step: 35 Training Loss: 2.8120126724243164 \n",
      "     Training Step: 36 Training Loss: 2.8118057250976562 \n",
      "     Training Step: 37 Training Loss: 2.747690200805664 \n",
      "     Training Step: 38 Training Loss: 2.432555675506592 \n",
      "     Training Step: 39 Training Loss: 3.4359848499298096 \n",
      "     Training Step: 40 Training Loss: 2.967259407043457 \n",
      "     Training Step: 41 Training Loss: 3.0997462272644043 \n",
      "     Training Step: 42 Training Loss: 3.1353912353515625 \n",
      "     Training Step: 43 Training Loss: 2.6590702533721924 \n",
      "     Training Step: 44 Training Loss: 2.7202694416046143 \n",
      "     Training Step: 45 Training Loss: 3.5267374515533447 \n",
      "     Training Step: 46 Training Loss: 3.0902862548828125 \n",
      "     Training Step: 47 Training Loss: 4.272139072418213 \n",
      "     Training Step: 48 Training Loss: 4.03371524810791 \n",
      "     Training Step: 49 Training Loss: 3.0557732582092285 \n",
      "     Training Step: 50 Training Loss: 3.224226474761963 \n",
      "     Training Step: 51 Training Loss: 3.438476085662842 \n",
      "     Training Step: 52 Training Loss: 3.2036120891571045 \n",
      "     Training Step: 53 Training Loss: 2.09706449508667 \n",
      "     Training Step: 54 Training Loss: 3.5103085041046143 \n",
      "     Training Step: 55 Training Loss: 2.3200836181640625 \n",
      "     Training Step: 56 Training Loss: 2.0889973640441895 \n",
      "     Training Step: 57 Training Loss: 3.0868663787841797 \n",
      "     Training Step: 58 Training Loss: 3.371553659439087 \n",
      "     Training Step: 59 Training Loss: 2.5582587718963623 \n",
      "     Training Step: 60 Training Loss: 3.028298854827881 \n",
      "     Training Step: 61 Training Loss: 2.476532220840454 \n",
      "     Training Step: 62 Training Loss: 3.2174906730651855 \n",
      "     Training Step: 63 Training Loss: 3.177546739578247 \n",
      "     Training Step: 64 Training Loss: 4.2251386642456055 \n",
      "     Training Step: 65 Training Loss: 2.56494402885437 \n",
      "     Training Step: 66 Training Loss: 2.633150577545166 \n",
      "     Training Step: 67 Training Loss: 3.1078600883483887 \n",
      "     Training Step: 68 Training Loss: 2.27175235748291 \n",
      "     Training Step: 69 Training Loss: 2.3254826068878174 \n",
      "     Training Step: 70 Training Loss: 2.4935810565948486 \n",
      "     Training Step: 71 Training Loss: 3.3230721950531006 \n",
      "     Training Step: 72 Training Loss: 3.488147258758545 \n",
      "     Training Step: 73 Training Loss: 3.819162368774414 \n",
      "     Training Step: 74 Training Loss: 3.361384391784668 \n",
      "     Training Step: 75 Training Loss: 2.508281707763672 \n",
      "     Training Step: 76 Training Loss: 2.9857022762298584 \n",
      "     Training Step: 77 Training Loss: 3.1332077980041504 \n",
      "     Training Step: 78 Training Loss: 2.670274257659912 \n",
      "     Training Step: 79 Training Loss: 2.7195205688476562 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1401309967041016 \n",
      "     Validation Step: 1 Validation Loss: 3.1364247798919678 \n",
      "     Validation Step: 2 Validation Loss: 2.894442558288574 \n",
      "     Validation Step: 3 Validation Loss: 2.9268288612365723 \n",
      "     Validation Step: 4 Validation Loss: 2.9046664237976074 \n",
      "     Validation Step: 5 Validation Loss: 3.997066020965576 \n",
      "     Validation Step: 6 Validation Loss: 3.7985336780548096 \n",
      "     Validation Step: 7 Validation Loss: 3.666475772857666 \n",
      "     Validation Step: 8 Validation Loss: 3.213355541229248 \n",
      "     Validation Step: 9 Validation Loss: 3.0626416206359863 \n",
      "     Validation Step: 10 Validation Loss: 3.2968697547912598 \n",
      "     Validation Step: 11 Validation Loss: 3.0547640323638916 \n",
      "     Validation Step: 12 Validation Loss: 3.48704195022583 \n",
      "     Validation Step: 13 Validation Loss: 3.177858352661133 \n",
      "     Validation Step: 14 Validation Loss: 2.029367446899414 \n",
      "Epoch: 75\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.180116653442383 \n",
      "     Training Step: 1 Training Loss: 3.9012348651885986 \n",
      "     Training Step: 2 Training Loss: 2.827202081680298 \n",
      "     Training Step: 3 Training Loss: 2.695744037628174 \n",
      "     Training Step: 4 Training Loss: 2.5722551345825195 \n",
      "     Training Step: 5 Training Loss: 2.210800886154175 \n",
      "     Training Step: 6 Training Loss: 3.189393997192383 \n",
      "     Training Step: 7 Training Loss: 2.2418363094329834 \n",
      "     Training Step: 8 Training Loss: 2.4260473251342773 \n",
      "     Training Step: 9 Training Loss: 3.291635274887085 \n",
      "     Training Step: 10 Training Loss: 2.347437858581543 \n",
      "     Training Step: 11 Training Loss: 3.3452117443084717 \n",
      "     Training Step: 12 Training Loss: 3.9449121952056885 \n",
      "     Training Step: 13 Training Loss: 3.6541993618011475 \n",
      "     Training Step: 14 Training Loss: 3.1219892501831055 \n",
      "     Training Step: 15 Training Loss: 3.824615478515625 \n",
      "     Training Step: 16 Training Loss: 2.2590880393981934 \n",
      "     Training Step: 17 Training Loss: 2.7911434173583984 \n",
      "     Training Step: 18 Training Loss: 3.167377471923828 \n",
      "     Training Step: 19 Training Loss: 2.387650966644287 \n",
      "     Training Step: 20 Training Loss: 2.8902769088745117 \n",
      "     Training Step: 21 Training Loss: 2.8234500885009766 \n",
      "     Training Step: 22 Training Loss: 3.236628770828247 \n",
      "     Training Step: 23 Training Loss: 3.565706491470337 \n",
      "     Training Step: 24 Training Loss: 2.7676615715026855 \n",
      "     Training Step: 25 Training Loss: 2.7174487113952637 \n",
      "     Training Step: 26 Training Loss: 3.7677183151245117 \n",
      "     Training Step: 27 Training Loss: 3.4326610565185547 \n",
      "     Training Step: 28 Training Loss: 4.003658294677734 \n",
      "     Training Step: 29 Training Loss: 2.820348024368286 \n",
      "     Training Step: 30 Training Loss: 2.9434118270874023 \n",
      "     Training Step: 31 Training Loss: 2.2808871269226074 \n",
      "     Training Step: 32 Training Loss: 2.4419260025024414 \n",
      "     Training Step: 33 Training Loss: 3.484832286834717 \n",
      "     Training Step: 34 Training Loss: 2.172816038131714 \n",
      "     Training Step: 35 Training Loss: 2.866917848587036 \n",
      "     Training Step: 36 Training Loss: 3.321173906326294 \n",
      "     Training Step: 37 Training Loss: 3.5436339378356934 \n",
      "     Training Step: 38 Training Loss: 2.7170791625976562 \n",
      "     Training Step: 39 Training Loss: 3.1505866050720215 \n",
      "     Training Step: 40 Training Loss: 3.0826239585876465 \n",
      "     Training Step: 41 Training Loss: 2.9940643310546875 \n",
      "     Training Step: 42 Training Loss: 3.561765432357788 \n",
      "     Training Step: 43 Training Loss: 3.8895013332366943 \n",
      "     Training Step: 44 Training Loss: 3.0317587852478027 \n",
      "     Training Step: 45 Training Loss: 3.3417458534240723 \n",
      "     Training Step: 46 Training Loss: 3.0824801921844482 \n",
      "     Training Step: 47 Training Loss: 3.825640916824341 \n",
      "     Training Step: 48 Training Loss: 2.5748307704925537 \n",
      "     Training Step: 49 Training Loss: 3.140195369720459 \n",
      "     Training Step: 50 Training Loss: 3.0125348567962646 \n",
      "     Training Step: 51 Training Loss: 2.219028949737549 \n",
      "     Training Step: 52 Training Loss: 2.611403226852417 \n",
      "     Training Step: 53 Training Loss: 3.430675745010376 \n",
      "     Training Step: 54 Training Loss: 2.69344425201416 \n",
      "     Training Step: 55 Training Loss: 2.807689666748047 \n",
      "     Training Step: 56 Training Loss: 3.147580862045288 \n",
      "     Training Step: 57 Training Loss: 2.386720895767212 \n",
      "     Training Step: 58 Training Loss: 3.8042032718658447 \n",
      "     Training Step: 59 Training Loss: 2.2433595657348633 \n",
      "     Training Step: 60 Training Loss: 2.6262779235839844 \n",
      "     Training Step: 61 Training Loss: 3.601726531982422 \n",
      "     Training Step: 62 Training Loss: 2.287811040878296 \n",
      "     Training Step: 63 Training Loss: 3.4251346588134766 \n",
      "     Training Step: 64 Training Loss: 2.7931101322174072 \n",
      "     Training Step: 65 Training Loss: 2.4542887210845947 \n",
      "     Training Step: 66 Training Loss: 2.7031123638153076 \n",
      "     Training Step: 67 Training Loss: 2.9929275512695312 \n",
      "     Training Step: 68 Training Loss: 2.906451463699341 \n",
      "     Training Step: 69 Training Loss: 3.181394100189209 \n",
      "     Training Step: 70 Training Loss: 3.754263162612915 \n",
      "     Training Step: 71 Training Loss: 2.765831232070923 \n",
      "     Training Step: 72 Training Loss: 2.624915838241577 \n",
      "     Training Step: 73 Training Loss: 2.8048057556152344 \n",
      "     Training Step: 74 Training Loss: 2.336503028869629 \n",
      "     Training Step: 75 Training Loss: 3.0114965438842773 \n",
      "     Training Step: 76 Training Loss: 2.679286241531372 \n",
      "     Training Step: 77 Training Loss: 2.847494602203369 \n",
      "     Training Step: 78 Training Loss: 2.7680935859680176 \n",
      "     Training Step: 79 Training Loss: 3.508425235748291 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.106451988220215 \n",
      "     Validation Step: 1 Validation Loss: 3.509291410446167 \n",
      "     Validation Step: 2 Validation Loss: 3.256592035293579 \n",
      "     Validation Step: 3 Validation Loss: 2.9855923652648926 \n",
      "     Validation Step: 4 Validation Loss: 3.0498852729797363 \n",
      "     Validation Step: 5 Validation Loss: 3.3161821365356445 \n",
      "     Validation Step: 6 Validation Loss: 2.307995080947876 \n",
      "     Validation Step: 7 Validation Loss: 3.9152331352233887 \n",
      "     Validation Step: 8 Validation Loss: 2.6748526096343994 \n",
      "     Validation Step: 9 Validation Loss: 3.4143712520599365 \n",
      "     Validation Step: 10 Validation Loss: 3.756159782409668 \n",
      "     Validation Step: 11 Validation Loss: 3.707510232925415 \n",
      "     Validation Step: 12 Validation Loss: 2.9449567794799805 \n",
      "     Validation Step: 13 Validation Loss: 3.2289469242095947 \n",
      "     Validation Step: 14 Validation Loss: 3.0439116954803467 \n",
      "Epoch: 76\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.6906557083129883 \n",
      "     Training Step: 1 Training Loss: 3.1448099613189697 \n",
      "     Training Step: 2 Training Loss: 4.483408451080322 \n",
      "     Training Step: 3 Training Loss: 2.3854570388793945 \n",
      "     Training Step: 4 Training Loss: 2.222639560699463 \n",
      "     Training Step: 5 Training Loss: 2.705263614654541 \n",
      "     Training Step: 6 Training Loss: 3.184998035430908 \n",
      "     Training Step: 7 Training Loss: 2.9107205867767334 \n",
      "     Training Step: 8 Training Loss: 2.396313190460205 \n",
      "     Training Step: 9 Training Loss: 3.1088781356811523 \n",
      "     Training Step: 10 Training Loss: 3.1708521842956543 \n",
      "     Training Step: 11 Training Loss: 3.082336664199829 \n",
      "     Training Step: 12 Training Loss: 2.517308235168457 \n",
      "     Training Step: 13 Training Loss: 2.4434762001037598 \n",
      "     Training Step: 14 Training Loss: 2.692354917526245 \n",
      "     Training Step: 15 Training Loss: 3.6212515830993652 \n",
      "     Training Step: 16 Training Loss: 2.3145158290863037 \n",
      "     Training Step: 17 Training Loss: 3.366025447845459 \n",
      "     Training Step: 18 Training Loss: 2.7297699451446533 \n",
      "     Training Step: 19 Training Loss: 2.904444456100464 \n",
      "     Training Step: 20 Training Loss: 2.6992673873901367 \n",
      "     Training Step: 21 Training Loss: 2.8550591468811035 \n",
      "     Training Step: 22 Training Loss: 2.536966323852539 \n",
      "     Training Step: 23 Training Loss: 3.3366246223449707 \n",
      "     Training Step: 24 Training Loss: 2.860541343688965 \n",
      "     Training Step: 25 Training Loss: 3.010401725769043 \n",
      "     Training Step: 26 Training Loss: 2.7450568675994873 \n",
      "     Training Step: 27 Training Loss: 3.541822671890259 \n",
      "     Training Step: 28 Training Loss: 4.118041038513184 \n",
      "     Training Step: 29 Training Loss: 3.4197750091552734 \n",
      "     Training Step: 30 Training Loss: 2.663890838623047 \n",
      "     Training Step: 31 Training Loss: 2.3978466987609863 \n",
      "     Training Step: 32 Training Loss: 2.707815647125244 \n",
      "     Training Step: 33 Training Loss: 2.744950771331787 \n",
      "     Training Step: 34 Training Loss: 3.0873847007751465 \n",
      "     Training Step: 35 Training Loss: 2.700430393218994 \n",
      "     Training Step: 36 Training Loss: 2.7610743045806885 \n",
      "     Training Step: 37 Training Loss: 2.987617015838623 \n",
      "     Training Step: 38 Training Loss: 2.1622142791748047 \n",
      "     Training Step: 39 Training Loss: 2.994565010070801 \n",
      "     Training Step: 40 Training Loss: 2.267965316772461 \n",
      "     Training Step: 41 Training Loss: 2.2554855346679688 \n",
      "     Training Step: 42 Training Loss: 4.490623474121094 \n",
      "     Training Step: 43 Training Loss: 2.6334242820739746 \n",
      "     Training Step: 44 Training Loss: 2.922849178314209 \n",
      "     Training Step: 45 Training Loss: 3.262566566467285 \n",
      "     Training Step: 46 Training Loss: 2.6787071228027344 \n",
      "     Training Step: 47 Training Loss: 3.0300092697143555 \n",
      "     Training Step: 48 Training Loss: 2.250009536743164 \n",
      "     Training Step: 49 Training Loss: 3.321133613586426 \n",
      "     Training Step: 50 Training Loss: 3.0122904777526855 \n",
      "     Training Step: 51 Training Loss: 2.62545108795166 \n",
      "     Training Step: 52 Training Loss: 4.007596492767334 \n",
      "     Training Step: 53 Training Loss: 2.9905593395233154 \n",
      "     Training Step: 54 Training Loss: 2.5315494537353516 \n",
      "     Training Step: 55 Training Loss: 2.7027535438537598 \n",
      "     Training Step: 56 Training Loss: 2.3913321495056152 \n",
      "     Training Step: 57 Training Loss: 3.249868631362915 \n",
      "     Training Step: 58 Training Loss: 3.0146424770355225 \n",
      "     Training Step: 59 Training Loss: 2.7205405235290527 \n",
      "     Training Step: 60 Training Loss: 2.3377246856689453 \n",
      "     Training Step: 61 Training Loss: 3.7590861320495605 \n",
      "     Training Step: 62 Training Loss: 3.201151132583618 \n",
      "     Training Step: 63 Training Loss: 2.7590160369873047 \n",
      "     Training Step: 64 Training Loss: 2.578536033630371 \n",
      "     Training Step: 65 Training Loss: 2.600534677505493 \n",
      "     Training Step: 66 Training Loss: 2.4302966594696045 \n",
      "     Training Step: 67 Training Loss: 3.155036211013794 \n",
      "     Training Step: 68 Training Loss: 3.7380762100219727 \n",
      "     Training Step: 69 Training Loss: 2.666973352432251 \n",
      "     Training Step: 70 Training Loss: 3.13130784034729 \n",
      "     Training Step: 71 Training Loss: 2.881897449493408 \n",
      "     Training Step: 72 Training Loss: 2.5852389335632324 \n",
      "     Training Step: 73 Training Loss: 2.80853271484375 \n",
      "     Training Step: 74 Training Loss: 2.5448126792907715 \n",
      "     Training Step: 75 Training Loss: 2.7864789962768555 \n",
      "     Training Step: 76 Training Loss: 3.2834455966949463 \n",
      "     Training Step: 77 Training Loss: 3.495901107788086 \n",
      "     Training Step: 78 Training Loss: 2.8513426780700684 \n",
      "     Training Step: 79 Training Loss: 3.3656275272369385 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9917943477630615 \n",
      "     Validation Step: 1 Validation Loss: 3.208130359649658 \n",
      "     Validation Step: 2 Validation Loss: 3.4326629638671875 \n",
      "     Validation Step: 3 Validation Loss: 3.3852386474609375 \n",
      "     Validation Step: 4 Validation Loss: 2.5970935821533203 \n",
      "     Validation Step: 5 Validation Loss: 2.7846333980560303 \n",
      "     Validation Step: 6 Validation Loss: 2.938565731048584 \n",
      "     Validation Step: 7 Validation Loss: 2.823119878768921 \n",
      "     Validation Step: 8 Validation Loss: 2.510255813598633 \n",
      "     Validation Step: 9 Validation Loss: 2.8588528633117676 \n",
      "     Validation Step: 10 Validation Loss: 2.566067695617676 \n",
      "     Validation Step: 11 Validation Loss: 3.6342761516571045 \n",
      "     Validation Step: 12 Validation Loss: 2.4093048572540283 \n",
      "     Validation Step: 13 Validation Loss: 3.237705707550049 \n",
      "     Validation Step: 14 Validation Loss: 3.5911431312561035 \n",
      "Epoch: 77\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.253333806991577 \n",
      "     Training Step: 1 Training Loss: 3.611982822418213 \n",
      "     Training Step: 2 Training Loss: 2.967888355255127 \n",
      "     Training Step: 3 Training Loss: 2.2910187244415283 \n",
      "     Training Step: 4 Training Loss: 4.393502235412598 \n",
      "     Training Step: 5 Training Loss: 2.5497779846191406 \n",
      "     Training Step: 6 Training Loss: 3.0745229721069336 \n",
      "     Training Step: 7 Training Loss: 2.463163375854492 \n",
      "     Training Step: 8 Training Loss: 2.4339473247528076 \n",
      "     Training Step: 9 Training Loss: 2.9367291927337646 \n",
      "     Training Step: 10 Training Loss: 3.0262460708618164 \n",
      "     Training Step: 11 Training Loss: 3.447087049484253 \n",
      "     Training Step: 12 Training Loss: 2.23892879486084 \n",
      "     Training Step: 13 Training Loss: 2.5888073444366455 \n",
      "     Training Step: 14 Training Loss: 3.396327257156372 \n",
      "     Training Step: 15 Training Loss: 2.33026385307312 \n",
      "     Training Step: 16 Training Loss: 3.006371021270752 \n",
      "     Training Step: 17 Training Loss: 2.3793210983276367 \n",
      "     Training Step: 18 Training Loss: 2.9919979572296143 \n",
      "     Training Step: 19 Training Loss: 3.6892409324645996 \n",
      "     Training Step: 20 Training Loss: 3.182088613510132 \n",
      "     Training Step: 21 Training Loss: 3.919311285018921 \n",
      "     Training Step: 22 Training Loss: 2.602883815765381 \n",
      "     Training Step: 23 Training Loss: 2.816218852996826 \n",
      "     Training Step: 24 Training Loss: 3.097594738006592 \n",
      "     Training Step: 25 Training Loss: 3.2296059131622314 \n",
      "     Training Step: 26 Training Loss: 3.267519235610962 \n",
      "     Training Step: 27 Training Loss: 3.6589126586914062 \n",
      "     Training Step: 28 Training Loss: 3.335725784301758 \n",
      "     Training Step: 29 Training Loss: 2.7696285247802734 \n",
      "     Training Step: 30 Training Loss: 2.4296469688415527 \n",
      "     Training Step: 31 Training Loss: 2.2349541187286377 \n",
      "     Training Step: 32 Training Loss: 2.3866941928863525 \n",
      "     Training Step: 33 Training Loss: 2.6908345222473145 \n",
      "     Training Step: 34 Training Loss: 3.1795053482055664 \n",
      "     Training Step: 35 Training Loss: 3.519381284713745 \n",
      "     Training Step: 36 Training Loss: 2.1940736770629883 \n",
      "     Training Step: 37 Training Loss: 4.428057670593262 \n",
      "     Training Step: 38 Training Loss: 2.3858790397644043 \n",
      "     Training Step: 39 Training Loss: 3.17104172706604 \n",
      "     Training Step: 40 Training Loss: 3.304830312728882 \n",
      "     Training Step: 41 Training Loss: 2.8035504817962646 \n",
      "     Training Step: 42 Training Loss: 2.7132768630981445 \n",
      "     Training Step: 43 Training Loss: 3.212916374206543 \n",
      "     Training Step: 44 Training Loss: 3.0459532737731934 \n",
      "     Training Step: 45 Training Loss: 2.6921491622924805 \n",
      "     Training Step: 46 Training Loss: 3.2373132705688477 \n",
      "     Training Step: 47 Training Loss: 3.177863597869873 \n",
      "     Training Step: 48 Training Loss: 3.143580675125122 \n",
      "     Training Step: 49 Training Loss: 2.246386766433716 \n",
      "     Training Step: 50 Training Loss: 3.373112916946411 \n",
      "     Training Step: 51 Training Loss: 2.733898162841797 \n",
      "     Training Step: 52 Training Loss: 2.5535707473754883 \n",
      "     Training Step: 53 Training Loss: 3.745854139328003 \n",
      "     Training Step: 54 Training Loss: 2.762432098388672 \n",
      "     Training Step: 55 Training Loss: 3.077760696411133 \n",
      "     Training Step: 56 Training Loss: 2.1482934951782227 \n",
      "     Training Step: 57 Training Loss: 3.751736640930176 \n",
      "     Training Step: 58 Training Loss: 2.700411558151245 \n",
      "     Training Step: 59 Training Loss: 2.872659683227539 \n",
      "     Training Step: 60 Training Loss: 2.5108916759490967 \n",
      "     Training Step: 61 Training Loss: 2.9822235107421875 \n",
      "     Training Step: 62 Training Loss: 4.022724151611328 \n",
      "     Training Step: 63 Training Loss: 2.493865966796875 \n",
      "     Training Step: 64 Training Loss: 3.7419655323028564 \n",
      "     Training Step: 65 Training Loss: 2.351465940475464 \n",
      "     Training Step: 66 Training Loss: 3.917543888092041 \n",
      "     Training Step: 67 Training Loss: 3.3788070678710938 \n",
      "     Training Step: 68 Training Loss: 3.3501710891723633 \n",
      "     Training Step: 69 Training Loss: 2.137472629547119 \n",
      "     Training Step: 70 Training Loss: 3.3238914012908936 \n",
      "     Training Step: 71 Training Loss: 3.053617000579834 \n",
      "     Training Step: 72 Training Loss: 2.247985601425171 \n",
      "     Training Step: 73 Training Loss: 2.5122203826904297 \n",
      "     Training Step: 74 Training Loss: 3.496245861053467 \n",
      "     Training Step: 75 Training Loss: 2.6178674697875977 \n",
      "     Training Step: 76 Training Loss: 2.603571891784668 \n",
      "     Training Step: 77 Training Loss: 2.5116868019104004 \n",
      "     Training Step: 78 Training Loss: 2.4991352558135986 \n",
      "     Training Step: 79 Training Loss: 3.6932373046875 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.588927984237671 \n",
      "     Validation Step: 1 Validation Loss: 2.319812297821045 \n",
      "     Validation Step: 2 Validation Loss: 2.3365395069122314 \n",
      "     Validation Step: 3 Validation Loss: 2.6731886863708496 \n",
      "     Validation Step: 4 Validation Loss: 2.900085926055908 \n",
      "     Validation Step: 5 Validation Loss: 4.268548488616943 \n",
      "     Validation Step: 6 Validation Loss: 3.07485032081604 \n",
      "     Validation Step: 7 Validation Loss: 3.9524197578430176 \n",
      "     Validation Step: 8 Validation Loss: 3.2198009490966797 \n",
      "     Validation Step: 9 Validation Loss: 2.6315245628356934 \n",
      "     Validation Step: 10 Validation Loss: 3.6886258125305176 \n",
      "     Validation Step: 11 Validation Loss: 4.037630081176758 \n",
      "     Validation Step: 12 Validation Loss: 3.0970702171325684 \n",
      "     Validation Step: 13 Validation Loss: 2.4332892894744873 \n",
      "     Validation Step: 14 Validation Loss: 2.7937655448913574 \n",
      "---------------   LEARNING RATE HALVING DOWN TO: 8.000000000000001e-06   ---------------\n",
      "---------------   CURRENT LEARNING RATE: 8.000000000000001e-06   ---------------\n",
      "Epoch: 78\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.8465168476104736 \n",
      "     Training Step: 1 Training Loss: 3.2983169555664062 \n",
      "     Training Step: 2 Training Loss: 3.8158044815063477 \n",
      "     Training Step: 3 Training Loss: 3.83827543258667 \n",
      "     Training Step: 4 Training Loss: 3.498547077178955 \n",
      "     Training Step: 5 Training Loss: 2.681875228881836 \n",
      "     Training Step: 6 Training Loss: 2.158905506134033 \n",
      "     Training Step: 7 Training Loss: 3.473330020904541 \n",
      "     Training Step: 8 Training Loss: 2.7305426597595215 \n",
      "     Training Step: 9 Training Loss: 3.1231517791748047 \n",
      "     Training Step: 10 Training Loss: 3.3123342990875244 \n",
      "     Training Step: 11 Training Loss: 2.800264596939087 \n",
      "     Training Step: 12 Training Loss: 2.0819578170776367 \n",
      "     Training Step: 13 Training Loss: 3.3878846168518066 \n",
      "     Training Step: 14 Training Loss: 3.1154916286468506 \n",
      "     Training Step: 15 Training Loss: 3.700608253479004 \n",
      "     Training Step: 16 Training Loss: 2.6553831100463867 \n",
      "     Training Step: 17 Training Loss: 2.804182529449463 \n",
      "     Training Step: 18 Training Loss: 3.0499918460845947 \n",
      "     Training Step: 19 Training Loss: 2.495487689971924 \n",
      "     Training Step: 20 Training Loss: 3.2302772998809814 \n",
      "     Training Step: 21 Training Loss: 3.631469488143921 \n",
      "     Training Step: 22 Training Loss: 2.1254262924194336 \n",
      "     Training Step: 23 Training Loss: 2.456265687942505 \n",
      "     Training Step: 24 Training Loss: 3.616889476776123 \n",
      "     Training Step: 25 Training Loss: 3.616501569747925 \n",
      "     Training Step: 26 Training Loss: 2.5304174423217773 \n",
      "     Training Step: 27 Training Loss: 2.351551055908203 \n",
      "     Training Step: 28 Training Loss: 2.712475299835205 \n",
      "     Training Step: 29 Training Loss: 2.6418566703796387 \n",
      "     Training Step: 30 Training Loss: 2.1195802688598633 \n",
      "     Training Step: 31 Training Loss: 3.531841278076172 \n",
      "     Training Step: 32 Training Loss: 3.546875476837158 \n",
      "     Training Step: 33 Training Loss: 2.978020668029785 \n",
      "     Training Step: 34 Training Loss: 2.9578938484191895 \n",
      "     Training Step: 35 Training Loss: 2.893446922302246 \n",
      "     Training Step: 36 Training Loss: 3.117391347885132 \n",
      "     Training Step: 37 Training Loss: 3.1512279510498047 \n",
      "     Training Step: 38 Training Loss: 2.407409906387329 \n",
      "     Training Step: 39 Training Loss: 3.326674461364746 \n",
      "     Training Step: 40 Training Loss: 3.1475136280059814 \n",
      "     Training Step: 41 Training Loss: 2.3428897857666016 \n",
      "     Training Step: 42 Training Loss: 3.4315273761749268 \n",
      "     Training Step: 43 Training Loss: 2.613539218902588 \n",
      "     Training Step: 44 Training Loss: 3.9765663146972656 \n",
      "     Training Step: 45 Training Loss: 3.6021087169647217 \n",
      "     Training Step: 46 Training Loss: 3.109848976135254 \n",
      "     Training Step: 47 Training Loss: 3.1973462104797363 \n",
      "     Training Step: 48 Training Loss: 3.425299644470215 \n",
      "     Training Step: 49 Training Loss: 3.7161810398101807 \n",
      "     Training Step: 50 Training Loss: 2.304710865020752 \n",
      "     Training Step: 51 Training Loss: 3.9968349933624268 \n",
      "     Training Step: 52 Training Loss: 3.053882122039795 \n",
      "     Training Step: 53 Training Loss: 3.0132970809936523 \n",
      "     Training Step: 54 Training Loss: 2.3319778442382812 \n",
      "     Training Step: 55 Training Loss: 2.3858938217163086 \n",
      "     Training Step: 56 Training Loss: 3.19389009475708 \n",
      "     Training Step: 57 Training Loss: 2.9416627883911133 \n",
      "     Training Step: 58 Training Loss: 3.274895668029785 \n",
      "     Training Step: 59 Training Loss: 2.556326150894165 \n",
      "     Training Step: 60 Training Loss: 2.37503981590271 \n",
      "     Training Step: 61 Training Loss: 2.6359591484069824 \n",
      "     Training Step: 62 Training Loss: 2.3616862297058105 \n",
      "     Training Step: 63 Training Loss: 2.412137508392334 \n",
      "     Training Step: 64 Training Loss: 4.094822406768799 \n",
      "     Training Step: 65 Training Loss: 3.3870444297790527 \n",
      "     Training Step: 66 Training Loss: 2.8077621459960938 \n",
      "     Training Step: 67 Training Loss: 3.5541934967041016 \n",
      "     Training Step: 68 Training Loss: 3.5502703189849854 \n",
      "     Training Step: 69 Training Loss: 3.598839521408081 \n",
      "     Training Step: 70 Training Loss: 2.941814422607422 \n",
      "     Training Step: 71 Training Loss: 2.563458204269409 \n",
      "     Training Step: 72 Training Loss: 3.486198902130127 \n",
      "     Training Step: 73 Training Loss: 2.807600975036621 \n",
      "     Training Step: 74 Training Loss: 2.393235445022583 \n",
      "     Training Step: 75 Training Loss: 2.8345518112182617 \n",
      "     Training Step: 76 Training Loss: 4.4990234375 \n",
      "     Training Step: 77 Training Loss: 2.2257485389709473 \n",
      "     Training Step: 78 Training Loss: 3.9385738372802734 \n",
      "     Training Step: 79 Training Loss: 3.047114372253418 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.505666971206665 \n",
      "     Validation Step: 1 Validation Loss: 3.086466073989868 \n",
      "     Validation Step: 2 Validation Loss: 3.5051286220550537 \n",
      "     Validation Step: 3 Validation Loss: 3.6440563201904297 \n",
      "     Validation Step: 4 Validation Loss: 3.3745131492614746 \n",
      "     Validation Step: 5 Validation Loss: 2.872417449951172 \n",
      "     Validation Step: 6 Validation Loss: 3.328947067260742 \n",
      "     Validation Step: 7 Validation Loss: 3.7055747509002686 \n",
      "     Validation Step: 8 Validation Loss: 2.758960723876953 \n",
      "     Validation Step: 9 Validation Loss: 2.4656853675842285 \n",
      "     Validation Step: 10 Validation Loss: 3.3248109817504883 \n",
      "     Validation Step: 11 Validation Loss: 2.360898971557617 \n",
      "     Validation Step: 12 Validation Loss: 3.001498222351074 \n",
      "     Validation Step: 13 Validation Loss: 3.562925338745117 \n",
      "     Validation Step: 14 Validation Loss: 2.7631828784942627 \n",
      "Epoch: 79\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.4428610801696777 \n",
      "     Training Step: 1 Training Loss: 2.43595552444458 \n",
      "     Training Step: 2 Training Loss: 2.5482499599456787 \n",
      "     Training Step: 3 Training Loss: 4.053919315338135 \n",
      "     Training Step: 4 Training Loss: 3.187623977661133 \n",
      "     Training Step: 5 Training Loss: 2.5454044342041016 \n",
      "     Training Step: 6 Training Loss: 2.9727704524993896 \n",
      "     Training Step: 7 Training Loss: 3.0815649032592773 \n",
      "     Training Step: 8 Training Loss: 3.0299458503723145 \n",
      "     Training Step: 9 Training Loss: 2.915590286254883 \n",
      "     Training Step: 10 Training Loss: 2.139655590057373 \n",
      "     Training Step: 11 Training Loss: 2.743821144104004 \n",
      "     Training Step: 12 Training Loss: 2.2150959968566895 \n",
      "     Training Step: 13 Training Loss: 2.410475730895996 \n",
      "     Training Step: 14 Training Loss: 3.6081955432891846 \n",
      "     Training Step: 15 Training Loss: 2.9336228370666504 \n",
      "     Training Step: 16 Training Loss: 3.3451614379882812 \n",
      "     Training Step: 17 Training Loss: 2.590536594390869 \n",
      "     Training Step: 18 Training Loss: 2.382948637008667 \n",
      "     Training Step: 19 Training Loss: 2.5863213539123535 \n",
      "     Training Step: 20 Training Loss: 2.5216660499572754 \n",
      "     Training Step: 21 Training Loss: 3.9700989723205566 \n",
      "     Training Step: 22 Training Loss: 3.1373391151428223 \n",
      "     Training Step: 23 Training Loss: 2.0943644046783447 \n",
      "     Training Step: 24 Training Loss: 4.555177211761475 \n",
      "     Training Step: 25 Training Loss: 3.3272223472595215 \n",
      "     Training Step: 26 Training Loss: 2.48223876953125 \n",
      "     Training Step: 27 Training Loss: 2.878681182861328 \n",
      "     Training Step: 28 Training Loss: 2.507866859436035 \n",
      "     Training Step: 29 Training Loss: 3.5600907802581787 \n",
      "     Training Step: 30 Training Loss: 2.94711971282959 \n",
      "     Training Step: 31 Training Loss: 2.5485939979553223 \n",
      "     Training Step: 32 Training Loss: 3.174128532409668 \n",
      "     Training Step: 33 Training Loss: 3.1950631141662598 \n",
      "     Training Step: 34 Training Loss: 2.705336570739746 \n",
      "     Training Step: 35 Training Loss: 2.4566173553466797 \n",
      "     Training Step: 36 Training Loss: 2.8718881607055664 \n",
      "     Training Step: 37 Training Loss: 2.763195514678955 \n",
      "     Training Step: 38 Training Loss: 3.1134090423583984 \n",
      "     Training Step: 39 Training Loss: 3.159371852874756 \n",
      "     Training Step: 40 Training Loss: 2.9173154830932617 \n",
      "     Training Step: 41 Training Loss: 4.041712284088135 \n",
      "     Training Step: 42 Training Loss: 3.2261650562286377 \n",
      "     Training Step: 43 Training Loss: 3.422537326812744 \n",
      "     Training Step: 44 Training Loss: 3.5806033611297607 \n",
      "     Training Step: 45 Training Loss: 2.5699801445007324 \n",
      "     Training Step: 46 Training Loss: 2.1046414375305176 \n",
      "     Training Step: 47 Training Loss: 3.0386922359466553 \n",
      "     Training Step: 48 Training Loss: 4.135958671569824 \n",
      "     Training Step: 49 Training Loss: 2.477879047393799 \n",
      "     Training Step: 50 Training Loss: 2.6239142417907715 \n",
      "     Training Step: 51 Training Loss: 2.4046874046325684 \n",
      "     Training Step: 52 Training Loss: 3.0605971813201904 \n",
      "     Training Step: 53 Training Loss: 3.300178289413452 \n",
      "     Training Step: 54 Training Loss: 2.539167642593384 \n",
      "     Training Step: 55 Training Loss: 3.7343649864196777 \n",
      "     Training Step: 56 Training Loss: 3.382639169692993 \n",
      "     Training Step: 57 Training Loss: 3.126925230026245 \n",
      "     Training Step: 58 Training Loss: 2.9336581230163574 \n",
      "     Training Step: 59 Training Loss: 3.126133680343628 \n",
      "     Training Step: 60 Training Loss: 2.9480576515197754 \n",
      "     Training Step: 61 Training Loss: 3.146925449371338 \n",
      "     Training Step: 62 Training Loss: 2.449462890625 \n",
      "     Training Step: 63 Training Loss: 2.6353559494018555 \n",
      "     Training Step: 64 Training Loss: 2.500396251678467 \n",
      "     Training Step: 65 Training Loss: 3.419274091720581 \n",
      "     Training Step: 66 Training Loss: 3.7351455688476562 \n",
      "     Training Step: 67 Training Loss: 3.1139512062072754 \n",
      "     Training Step: 68 Training Loss: 2.442211151123047 \n",
      "     Training Step: 69 Training Loss: 3.2096290588378906 \n",
      "     Training Step: 70 Training Loss: 2.0993080139160156 \n",
      "     Training Step: 71 Training Loss: 3.1943435668945312 \n",
      "     Training Step: 72 Training Loss: 3.221226215362549 \n",
      "     Training Step: 73 Training Loss: 2.7548716068267822 \n",
      "     Training Step: 74 Training Loss: 2.7282285690307617 \n",
      "     Training Step: 75 Training Loss: 2.9285836219787598 \n",
      "     Training Step: 76 Training Loss: 2.36222767829895 \n",
      "     Training Step: 77 Training Loss: 2.4731523990631104 \n",
      "     Training Step: 78 Training Loss: 2.9850025177001953 \n",
      "     Training Step: 79 Training Loss: 3.2627859115600586 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.5505378246307373 \n",
      "     Validation Step: 1 Validation Loss: 2.200185775756836 \n",
      "     Validation Step: 2 Validation Loss: 3.482311725616455 \n",
      "     Validation Step: 3 Validation Loss: 3.1316211223602295 \n",
      "     Validation Step: 4 Validation Loss: 2.8587546348571777 \n",
      "     Validation Step: 5 Validation Loss: 3.5185394287109375 \n",
      "     Validation Step: 6 Validation Loss: 3.3684372901916504 \n",
      "     Validation Step: 7 Validation Loss: 2.9385013580322266 \n",
      "     Validation Step: 8 Validation Loss: 2.9692955017089844 \n",
      "     Validation Step: 9 Validation Loss: 2.6158688068389893 \n",
      "     Validation Step: 10 Validation Loss: 3.061396360397339 \n",
      "     Validation Step: 11 Validation Loss: 3.5351338386535645 \n",
      "     Validation Step: 12 Validation Loss: 3.074495315551758 \n",
      "     Validation Step: 13 Validation Loss: 2.7067956924438477 \n",
      "     Validation Step: 14 Validation Loss: 3.8193655014038086 \n",
      "Epoch: 80\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.1053011417388916 \n",
      "     Training Step: 1 Training Loss: 2.047422170639038 \n",
      "     Training Step: 2 Training Loss: 3.2456345558166504 \n",
      "     Training Step: 3 Training Loss: 4.762136459350586 \n",
      "     Training Step: 4 Training Loss: 2.3856003284454346 \n",
      "     Training Step: 5 Training Loss: 2.501915216445923 \n",
      "     Training Step: 6 Training Loss: 3.045243263244629 \n",
      "     Training Step: 7 Training Loss: 2.082569122314453 \n",
      "     Training Step: 8 Training Loss: 2.701554775238037 \n",
      "     Training Step: 9 Training Loss: 3.3373026847839355 \n",
      "     Training Step: 10 Training Loss: 3.5481371879577637 \n",
      "     Training Step: 11 Training Loss: 3.0162253379821777 \n",
      "     Training Step: 12 Training Loss: 2.960237503051758 \n",
      "     Training Step: 13 Training Loss: 3.632028341293335 \n",
      "     Training Step: 14 Training Loss: 3.027132034301758 \n",
      "     Training Step: 15 Training Loss: 3.0979084968566895 \n",
      "     Training Step: 16 Training Loss: 3.8975019454956055 \n",
      "     Training Step: 17 Training Loss: 2.5876898765563965 \n",
      "     Training Step: 18 Training Loss: 2.4830422401428223 \n",
      "     Training Step: 19 Training Loss: 3.1082205772399902 \n",
      "     Training Step: 20 Training Loss: 3.3334035873413086 \n",
      "     Training Step: 21 Training Loss: 3.507380247116089 \n",
      "     Training Step: 22 Training Loss: 2.863346576690674 \n",
      "     Training Step: 23 Training Loss: 2.218003749847412 \n",
      "     Training Step: 24 Training Loss: 2.9287567138671875 \n",
      "     Training Step: 25 Training Loss: 2.5128684043884277 \n",
      "     Training Step: 26 Training Loss: 3.710806369781494 \n",
      "     Training Step: 27 Training Loss: 3.721517562866211 \n",
      "     Training Step: 28 Training Loss: 2.617638111114502 \n",
      "     Training Step: 29 Training Loss: 2.2387800216674805 \n",
      "     Training Step: 30 Training Loss: 2.4110424518585205 \n",
      "     Training Step: 31 Training Loss: 3.185530185699463 \n",
      "     Training Step: 32 Training Loss: 2.1952123641967773 \n",
      "     Training Step: 33 Training Loss: 2.400541305541992 \n",
      "     Training Step: 34 Training Loss: 4.086997985839844 \n",
      "     Training Step: 35 Training Loss: 3.673971176147461 \n",
      "     Training Step: 36 Training Loss: 2.960618734359741 \n",
      "     Training Step: 37 Training Loss: 3.8210291862487793 \n",
      "     Training Step: 38 Training Loss: 3.9791665077209473 \n",
      "     Training Step: 39 Training Loss: 2.6183812618255615 \n",
      "     Training Step: 40 Training Loss: 2.4447174072265625 \n",
      "     Training Step: 41 Training Loss: 3.168586492538452 \n",
      "     Training Step: 42 Training Loss: 3.612621307373047 \n",
      "     Training Step: 43 Training Loss: 3.4541077613830566 \n",
      "     Training Step: 44 Training Loss: 2.4829816818237305 \n",
      "     Training Step: 45 Training Loss: 2.6817357540130615 \n",
      "     Training Step: 46 Training Loss: 3.0894904136657715 \n",
      "     Training Step: 47 Training Loss: 3.358582019805908 \n",
      "     Training Step: 48 Training Loss: 2.8841171264648438 \n",
      "     Training Step: 49 Training Loss: 3.586322784423828 \n",
      "     Training Step: 50 Training Loss: 2.7681946754455566 \n",
      "     Training Step: 51 Training Loss: 3.0360727310180664 \n",
      "     Training Step: 52 Training Loss: 4.32019567489624 \n",
      "     Training Step: 53 Training Loss: 3.21964168548584 \n",
      "     Training Step: 54 Training Loss: 2.850090742111206 \n",
      "     Training Step: 55 Training Loss: 2.6580073833465576 \n",
      "     Training Step: 56 Training Loss: 3.265004873275757 \n",
      "     Training Step: 57 Training Loss: 2.2537033557891846 \n",
      "     Training Step: 58 Training Loss: 2.4816579818725586 \n",
      "     Training Step: 59 Training Loss: 2.8058362007141113 \n",
      "     Training Step: 60 Training Loss: 2.818178653717041 \n",
      "     Training Step: 61 Training Loss: 3.720198631286621 \n",
      "     Training Step: 62 Training Loss: 3.2439093589782715 \n",
      "     Training Step: 63 Training Loss: 4.0763983726501465 \n",
      "     Training Step: 64 Training Loss: 2.6007275581359863 \n",
      "     Training Step: 65 Training Loss: 2.949584484100342 \n",
      "     Training Step: 66 Training Loss: 2.248359441757202 \n",
      "     Training Step: 67 Training Loss: 3.49746036529541 \n",
      "     Training Step: 68 Training Loss: 3.3443603515625 \n",
      "     Training Step: 69 Training Loss: 2.975252151489258 \n",
      "     Training Step: 70 Training Loss: 3.161510467529297 \n",
      "     Training Step: 71 Training Loss: 2.9075334072113037 \n",
      "     Training Step: 72 Training Loss: 2.583110809326172 \n",
      "     Training Step: 73 Training Loss: 2.39263916015625 \n",
      "     Training Step: 74 Training Loss: 3.6690282821655273 \n",
      "     Training Step: 75 Training Loss: 3.076624870300293 \n",
      "     Training Step: 76 Training Loss: 2.8659088611602783 \n",
      "     Training Step: 77 Training Loss: 2.7498583793640137 \n",
      "     Training Step: 78 Training Loss: 2.7592711448669434 \n",
      "     Training Step: 79 Training Loss: 3.0359745025634766 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.005242347717285 \n",
      "     Validation Step: 1 Validation Loss: 3.5008604526519775 \n",
      "     Validation Step: 2 Validation Loss: 3.0489578247070312 \n",
      "     Validation Step: 3 Validation Loss: 3.2635321617126465 \n",
      "     Validation Step: 4 Validation Loss: 2.842008113861084 \n",
      "     Validation Step: 5 Validation Loss: 3.3503973484039307 \n",
      "     Validation Step: 6 Validation Loss: 3.3997604846954346 \n",
      "     Validation Step: 7 Validation Loss: 3.541621685028076 \n",
      "     Validation Step: 8 Validation Loss: 3.517094612121582 \n",
      "     Validation Step: 9 Validation Loss: 2.663109302520752 \n",
      "     Validation Step: 10 Validation Loss: 3.084322214126587 \n",
      "     Validation Step: 11 Validation Loss: 3.2294657230377197 \n",
      "     Validation Step: 12 Validation Loss: 2.1533703804016113 \n",
      "     Validation Step: 13 Validation Loss: 3.8317036628723145 \n",
      "     Validation Step: 14 Validation Loss: 3.818826913833618 \n",
      "---------------   CURRENT LEARNING RATE: 8.000000000000001e-06   ---------------\n",
      "Epoch: 80\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 5.708128452301025 \n",
      "     Training Step: 1 Training Loss: 5.0616044998168945 \n",
      "     Training Step: 2 Training Loss: 6.351646423339844 \n",
      "     Training Step: 3 Training Loss: 7.297306060791016 \n",
      "     Training Step: 4 Training Loss: 4.279818534851074 \n",
      "     Training Step: 5 Training Loss: 3.6887454986572266 \n",
      "     Training Step: 6 Training Loss: 6.1683030128479 \n",
      "     Training Step: 7 Training Loss: 2.831423282623291 \n",
      "     Training Step: 8 Training Loss: 5.634777545928955 \n",
      "     Training Step: 9 Training Loss: 5.386972427368164 \n",
      "     Training Step: 10 Training Loss: 7.844235897064209 \n",
      "     Training Step: 11 Training Loss: 3.690275192260742 \n",
      "     Training Step: 12 Training Loss: 7.334329605102539 \n",
      "     Training Step: 13 Training Loss: 6.757225036621094 \n",
      "     Training Step: 14 Training Loss: 5.898207187652588 \n",
      "     Training Step: 15 Training Loss: 6.673759937286377 \n",
      "     Training Step: 16 Training Loss: 7.907372951507568 \n",
      "     Training Step: 17 Training Loss: 4.030007362365723 \n",
      "     Training Step: 18 Training Loss: 6.641307830810547 \n",
      "     Training Step: 19 Training Loss: 7.4419074058532715 \n",
      "     Training Step: 20 Training Loss: 4.1867475509643555 \n",
      "     Training Step: 21 Training Loss: 5.800421714782715 \n",
      "     Training Step: 22 Training Loss: 9.041643142700195 \n",
      "     Training Step: 23 Training Loss: 8.33658504486084 \n",
      "     Training Step: 24 Training Loss: 7.482402324676514 \n",
      "     Training Step: 25 Training Loss: 6.579489707946777 \n",
      "     Training Step: 26 Training Loss: 5.233977317810059 \n",
      "     Training Step: 27 Training Loss: 5.413455963134766 \n",
      "     Training Step: 28 Training Loss: 6.951939105987549 \n",
      "     Training Step: 29 Training Loss: 8.413948059082031 \n",
      "     Training Step: 30 Training Loss: 4.569578170776367 \n",
      "     Training Step: 31 Training Loss: 6.206502437591553 \n",
      "     Training Step: 32 Training Loss: 5.134590148925781 \n",
      "     Training Step: 33 Training Loss: 3.3431272506713867 \n",
      "     Training Step: 34 Training Loss: 5.097894668579102 \n",
      "     Training Step: 35 Training Loss: 2.6871330738067627 \n",
      "     Training Step: 36 Training Loss: 6.048884391784668 \n",
      "     Training Step: 37 Training Loss: 6.670421600341797 \n",
      "     Training Step: 38 Training Loss: 3.05116605758667 \n",
      "     Training Step: 39 Training Loss: 9.6887845993042 \n",
      "     Training Step: 40 Training Loss: 5.908835411071777 \n",
      "     Training Step: 41 Training Loss: 10.079747200012207 \n",
      "     Training Step: 42 Training Loss: 3.647167682647705 \n",
      "     Training Step: 43 Training Loss: 6.850049018859863 \n",
      "     Training Step: 44 Training Loss: 11.411752700805664 \n",
      "     Training Step: 45 Training Loss: 11.921300888061523 \n",
      "     Training Step: 46 Training Loss: 13.086312294006348 \n",
      "     Training Step: 47 Training Loss: 4.610090255737305 \n",
      "     Training Step: 48 Training Loss: 4.974365711212158 \n",
      "     Training Step: 49 Training Loss: 5.103796482086182 \n",
      "     Training Step: 50 Training Loss: 5.729461193084717 \n",
      "     Training Step: 51 Training Loss: 6.642345905303955 \n",
      "     Training Step: 52 Training Loss: 3.602734327316284 \n",
      "     Training Step: 53 Training Loss: 8.085168838500977 \n",
      "     Training Step: 54 Training Loss: 5.44284725189209 \n",
      "     Training Step: 55 Training Loss: 5.706419944763184 \n",
      "     Training Step: 56 Training Loss: 8.380205154418945 \n",
      "     Training Step: 57 Training Loss: 6.340789794921875 \n",
      "     Training Step: 58 Training Loss: 3.7628843784332275 \n",
      "     Training Step: 59 Training Loss: 9.587419509887695 \n",
      "     Training Step: 60 Training Loss: 5.441897392272949 \n",
      "     Training Step: 61 Training Loss: 4.853376388549805 \n",
      "     Training Step: 62 Training Loss: 5.480179786682129 \n",
      "     Training Step: 63 Training Loss: 4.741135597229004 \n",
      "     Training Step: 64 Training Loss: 11.849407196044922 \n",
      "     Training Step: 65 Training Loss: 6.118941307067871 \n",
      "     Training Step: 66 Training Loss: 5.6376800537109375 \n",
      "     Training Step: 67 Training Loss: 6.668901443481445 \n",
      "     Training Step: 68 Training Loss: 5.393822193145752 \n",
      "     Training Step: 69 Training Loss: 7.164230823516846 \n",
      "     Training Step: 70 Training Loss: 4.940639972686768 \n",
      "     Training Step: 71 Training Loss: 5.406057834625244 \n",
      "     Training Step: 72 Training Loss: 6.0453972816467285 \n",
      "     Training Step: 73 Training Loss: 5.252697944641113 \n",
      "     Training Step: 74 Training Loss: 6.14147424697876 \n",
      "     Training Step: 75 Training Loss: 8.027912139892578 \n",
      "     Training Step: 76 Training Loss: 6.9921464920043945 \n",
      "     Training Step: 77 Training Loss: 4.940079212188721 \n",
      "     Training Step: 78 Training Loss: 7.112635135650635 \n",
      "     Training Step: 79 Training Loss: 5.864377021789551 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 8.939371109008789 \n",
      "     Validation Step: 1 Validation Loss: 6.873490333557129 \n",
      "     Validation Step: 2 Validation Loss: 3.310596466064453 \n",
      "     Validation Step: 3 Validation Loss: 5.768277168273926 \n",
      "     Validation Step: 4 Validation Loss: 5.6228928565979 \n",
      "     Validation Step: 5 Validation Loss: 6.3052778244018555 \n",
      "     Validation Step: 6 Validation Loss: 10.116703033447266 \n",
      "     Validation Step: 7 Validation Loss: 8.134273529052734 \n",
      "     Validation Step: 8 Validation Loss: 6.651108741760254 \n",
      "     Validation Step: 9 Validation Loss: 5.687870025634766 \n",
      "     Validation Step: 10 Validation Loss: 6.628545761108398 \n",
      "     Validation Step: 11 Validation Loss: 8.442007064819336 \n",
      "     Validation Step: 12 Validation Loss: 8.13235092163086 \n",
      "     Validation Step: 13 Validation Loss: 6.307669639587402 \n",
      "     Validation Step: 14 Validation Loss: 6.4690728187561035 \n",
      "Epoch: 81\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.086559772491455 \n",
      "     Training Step: 1 Training Loss: 2.6032207012176514 \n",
      "     Training Step: 2 Training Loss: 2.8844566345214844 \n",
      "     Training Step: 3 Training Loss: 2.698699951171875 \n",
      "     Training Step: 4 Training Loss: 2.7250454425811768 \n",
      "     Training Step: 5 Training Loss: 2.3071517944335938 \n",
      "     Training Step: 6 Training Loss: 3.4518611431121826 \n",
      "     Training Step: 7 Training Loss: 2.3750720024108887 \n",
      "     Training Step: 8 Training Loss: 2.770284414291382 \n",
      "     Training Step: 9 Training Loss: 3.2188706398010254 \n",
      "     Training Step: 10 Training Loss: 2.6442368030548096 \n",
      "     Training Step: 11 Training Loss: 3.5284266471862793 \n",
      "     Training Step: 12 Training Loss: 2.63737154006958 \n",
      "     Training Step: 13 Training Loss: 2.839735269546509 \n",
      "     Training Step: 14 Training Loss: 3.8272385597229004 \n",
      "     Training Step: 15 Training Loss: 3.2846412658691406 \n",
      "     Training Step: 16 Training Loss: 4.690441131591797 \n",
      "     Training Step: 17 Training Loss: 2.752335548400879 \n",
      "     Training Step: 18 Training Loss: 2.7858242988586426 \n",
      "     Training Step: 19 Training Loss: 2.4743103981018066 \n",
      "     Training Step: 20 Training Loss: 3.0812809467315674 \n",
      "     Training Step: 21 Training Loss: 2.8036718368530273 \n",
      "     Training Step: 22 Training Loss: 3.13495135307312 \n",
      "     Training Step: 23 Training Loss: 2.781329870223999 \n",
      "     Training Step: 24 Training Loss: 2.109100580215454 \n",
      "     Training Step: 25 Training Loss: 2.607881784439087 \n",
      "     Training Step: 26 Training Loss: 2.3975229263305664 \n",
      "     Training Step: 27 Training Loss: 3.647634506225586 \n",
      "     Training Step: 28 Training Loss: 2.5144546031951904 \n",
      "     Training Step: 29 Training Loss: 2.6480438709259033 \n",
      "     Training Step: 30 Training Loss: 3.8623335361480713 \n",
      "     Training Step: 31 Training Loss: 2.264103412628174 \n",
      "     Training Step: 32 Training Loss: 2.8324875831604004 \n",
      "     Training Step: 33 Training Loss: 2.837465286254883 \n",
      "     Training Step: 34 Training Loss: 3.1614983081817627 \n",
      "     Training Step: 35 Training Loss: 2.7472715377807617 \n",
      "     Training Step: 36 Training Loss: 2.4263548851013184 \n",
      "     Training Step: 37 Training Loss: 2.1645007133483887 \n",
      "     Training Step: 38 Training Loss: 3.49346923828125 \n",
      "     Training Step: 39 Training Loss: 3.5307531356811523 \n",
      "     Training Step: 40 Training Loss: 3.1451799869537354 \n",
      "     Training Step: 41 Training Loss: 2.6909878253936768 \n",
      "     Training Step: 42 Training Loss: 2.428027868270874 \n",
      "     Training Step: 43 Training Loss: 2.4473180770874023 \n",
      "     Training Step: 44 Training Loss: 3.4027352333068848 \n",
      "     Training Step: 45 Training Loss: 2.6311416625976562 \n",
      "     Training Step: 46 Training Loss: 2.7158641815185547 \n",
      "     Training Step: 47 Training Loss: 2.9303901195526123 \n",
      "     Training Step: 48 Training Loss: 3.2809700965881348 \n",
      "     Training Step: 49 Training Loss: 2.973729372024536 \n",
      "     Training Step: 50 Training Loss: 3.5920233726501465 \n",
      "     Training Step: 51 Training Loss: 2.9725379943847656 \n",
      "     Training Step: 52 Training Loss: 3.703645706176758 \n",
      "     Training Step: 53 Training Loss: 3.2726693153381348 \n",
      "     Training Step: 54 Training Loss: 3.8442864418029785 \n",
      "     Training Step: 55 Training Loss: 3.673194169998169 \n",
      "     Training Step: 56 Training Loss: 2.7781097888946533 \n",
      "     Training Step: 57 Training Loss: 2.578400135040283 \n",
      "     Training Step: 58 Training Loss: 3.0522279739379883 \n",
      "     Training Step: 59 Training Loss: 3.805231809616089 \n",
      "     Training Step: 60 Training Loss: 3.233771562576294 \n",
      "     Training Step: 61 Training Loss: 2.675901174545288 \n",
      "     Training Step: 62 Training Loss: 2.3373446464538574 \n",
      "     Training Step: 63 Training Loss: 3.0575108528137207 \n",
      "     Training Step: 64 Training Loss: 3.3774733543395996 \n",
      "     Training Step: 65 Training Loss: 3.04549503326416 \n",
      "     Training Step: 66 Training Loss: 2.2598183155059814 \n",
      "     Training Step: 67 Training Loss: 2.3789844512939453 \n",
      "     Training Step: 68 Training Loss: 3.669931411743164 \n",
      "     Training Step: 69 Training Loss: 2.9435977935791016 \n",
      "     Training Step: 70 Training Loss: 3.9075326919555664 \n",
      "     Training Step: 71 Training Loss: 2.544290781021118 \n",
      "     Training Step: 72 Training Loss: 3.448119878768921 \n",
      "     Training Step: 73 Training Loss: 2.0231308937072754 \n",
      "     Training Step: 74 Training Loss: 3.095055103302002 \n",
      "     Training Step: 75 Training Loss: 3.3098113536834717 \n",
      "     Training Step: 76 Training Loss: 3.5548038482666016 \n",
      "     Training Step: 77 Training Loss: 3.0559029579162598 \n",
      "     Training Step: 78 Training Loss: 2.4447126388549805 \n",
      "     Training Step: 79 Training Loss: 2.5189476013183594 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9866862297058105 \n",
      "     Validation Step: 1 Validation Loss: 3.010317325592041 \n",
      "     Validation Step: 2 Validation Loss: 2.7734875679016113 \n",
      "     Validation Step: 3 Validation Loss: 2.998879909515381 \n",
      "     Validation Step: 4 Validation Loss: 2.9110989570617676 \n",
      "     Validation Step: 5 Validation Loss: 3.375516653060913 \n",
      "     Validation Step: 6 Validation Loss: 2.2739477157592773 \n",
      "     Validation Step: 7 Validation Loss: 3.079072952270508 \n",
      "     Validation Step: 8 Validation Loss: 3.5197818279266357 \n",
      "     Validation Step: 9 Validation Loss: 2.6084208488464355 \n",
      "     Validation Step: 10 Validation Loss: 3.7785234451293945 \n",
      "     Validation Step: 11 Validation Loss: 2.6990315914154053 \n",
      "     Validation Step: 12 Validation Loss: 3.5027804374694824 \n",
      "     Validation Step: 13 Validation Loss: 3.5302255153656006 \n",
      "     Validation Step: 14 Validation Loss: 3.5408215522766113 \n",
      "Epoch: 82\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.217957019805908 \n",
      "     Training Step: 1 Training Loss: 2.6543519496917725 \n",
      "     Training Step: 2 Training Loss: 3.4251625537872314 \n",
      "     Training Step: 3 Training Loss: 2.886777877807617 \n",
      "     Training Step: 4 Training Loss: 2.600367546081543 \n",
      "     Training Step: 5 Training Loss: 3.57438325881958 \n",
      "     Training Step: 6 Training Loss: 2.376802444458008 \n",
      "     Training Step: 7 Training Loss: 2.7816805839538574 \n",
      "     Training Step: 8 Training Loss: 2.3087077140808105 \n",
      "     Training Step: 9 Training Loss: 3.134568452835083 \n",
      "     Training Step: 10 Training Loss: 3.2983570098876953 \n",
      "     Training Step: 11 Training Loss: 2.8590731620788574 \n",
      "     Training Step: 12 Training Loss: 3.1525025367736816 \n",
      "     Training Step: 13 Training Loss: 3.601762294769287 \n",
      "     Training Step: 14 Training Loss: 2.2896034717559814 \n",
      "     Training Step: 15 Training Loss: 3.1149544715881348 \n",
      "     Training Step: 16 Training Loss: 2.7201621532440186 \n",
      "     Training Step: 17 Training Loss: 2.424678087234497 \n",
      "     Training Step: 18 Training Loss: 2.9927988052368164 \n",
      "     Training Step: 19 Training Loss: 4.754209518432617 \n",
      "     Training Step: 20 Training Loss: 2.5750064849853516 \n",
      "     Training Step: 21 Training Loss: 3.1400856971740723 \n",
      "     Training Step: 22 Training Loss: 4.397074222564697 \n",
      "     Training Step: 23 Training Loss: 2.547865390777588 \n",
      "     Training Step: 24 Training Loss: 2.745936632156372 \n",
      "     Training Step: 25 Training Loss: 2.810767650604248 \n",
      "     Training Step: 26 Training Loss: 2.640425682067871 \n",
      "     Training Step: 27 Training Loss: 3.2430970668792725 \n",
      "     Training Step: 28 Training Loss: 3.1366002559661865 \n",
      "     Training Step: 29 Training Loss: 2.4214515686035156 \n",
      "     Training Step: 30 Training Loss: 4.032630443572998 \n",
      "     Training Step: 31 Training Loss: 2.418978691101074 \n",
      "     Training Step: 32 Training Loss: 3.517320156097412 \n",
      "     Training Step: 33 Training Loss: 2.9707932472229004 \n",
      "     Training Step: 34 Training Loss: 2.8194613456726074 \n",
      "     Training Step: 35 Training Loss: 2.6662447452545166 \n",
      "     Training Step: 36 Training Loss: 2.8752946853637695 \n",
      "     Training Step: 37 Training Loss: 2.8364453315734863 \n",
      "     Training Step: 38 Training Loss: 3.0270256996154785 \n",
      "     Training Step: 39 Training Loss: 2.765998601913452 \n",
      "     Training Step: 40 Training Loss: 2.834052324295044 \n",
      "     Training Step: 41 Training Loss: 3.04887056350708 \n",
      "     Training Step: 42 Training Loss: 2.912754774093628 \n",
      "     Training Step: 43 Training Loss: 3.108745813369751 \n",
      "     Training Step: 44 Training Loss: 2.7887613773345947 \n",
      "     Training Step: 45 Training Loss: 3.0303235054016113 \n",
      "     Training Step: 46 Training Loss: 2.2843830585479736 \n",
      "     Training Step: 47 Training Loss: 3.3314671516418457 \n",
      "     Training Step: 48 Training Loss: 2.990368366241455 \n",
      "     Training Step: 49 Training Loss: 3.1822383403778076 \n",
      "     Training Step: 50 Training Loss: 2.045358657836914 \n",
      "     Training Step: 51 Training Loss: 4.184443950653076 \n",
      "     Training Step: 52 Training Loss: 3.2367660999298096 \n",
      "     Training Step: 53 Training Loss: 3.330759286880493 \n",
      "     Training Step: 54 Training Loss: 2.103499412536621 \n",
      "     Training Step: 55 Training Loss: 2.2948684692382812 \n",
      "     Training Step: 56 Training Loss: 3.757002115249634 \n",
      "     Training Step: 57 Training Loss: 3.0203380584716797 \n",
      "     Training Step: 58 Training Loss: 2.047933578491211 \n",
      "     Training Step: 59 Training Loss: 3.7297606468200684 \n",
      "     Training Step: 60 Training Loss: 2.7907397747039795 \n",
      "     Training Step: 61 Training Loss: 2.712275266647339 \n",
      "     Training Step: 62 Training Loss: 2.7345776557922363 \n",
      "     Training Step: 63 Training Loss: 2.125119209289551 \n",
      "     Training Step: 64 Training Loss: 2.245864152908325 \n",
      "     Training Step: 65 Training Loss: 3.395695686340332 \n",
      "     Training Step: 66 Training Loss: 2.6921215057373047 \n",
      "     Training Step: 67 Training Loss: 2.9338831901550293 \n",
      "     Training Step: 68 Training Loss: 2.6114554405212402 \n",
      "     Training Step: 69 Training Loss: 2.3860597610473633 \n",
      "     Training Step: 70 Training Loss: 2.211223602294922 \n",
      "     Training Step: 71 Training Loss: 3.20210862159729 \n",
      "     Training Step: 72 Training Loss: 3.1447858810424805 \n",
      "     Training Step: 73 Training Loss: 2.5607175827026367 \n",
      "     Training Step: 74 Training Loss: 2.629347562789917 \n",
      "     Training Step: 75 Training Loss: 3.0870532989501953 \n",
      "     Training Step: 76 Training Loss: 4.2584733963012695 \n",
      "     Training Step: 77 Training Loss: 3.7181615829467773 \n",
      "     Training Step: 78 Training Loss: 3.140690326690674 \n",
      "     Training Step: 79 Training Loss: 2.3366973400115967 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.4476161003112793 \n",
      "     Validation Step: 1 Validation Loss: 3.675938129425049 \n",
      "     Validation Step: 2 Validation Loss: 2.745270252227783 \n",
      "     Validation Step: 3 Validation Loss: 2.347513198852539 \n",
      "     Validation Step: 4 Validation Loss: 2.925234794616699 \n",
      "     Validation Step: 5 Validation Loss: 3.4181244373321533 \n",
      "     Validation Step: 6 Validation Loss: 2.5165815353393555 \n",
      "     Validation Step: 7 Validation Loss: 2.8816349506378174 \n",
      "     Validation Step: 8 Validation Loss: 3.4058310985565186 \n",
      "     Validation Step: 9 Validation Loss: 3.538769006729126 \n",
      "     Validation Step: 10 Validation Loss: 3.2505245208740234 \n",
      "     Validation Step: 11 Validation Loss: 2.6454224586486816 \n",
      "     Validation Step: 12 Validation Loss: 3.7935171127319336 \n",
      "     Validation Step: 13 Validation Loss: 3.0302605628967285 \n",
      "     Validation Step: 14 Validation Loss: 2.9759395122528076 \n",
      "Epoch: 83\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.518609046936035 \n",
      "     Training Step: 1 Training Loss: 2.9087748527526855 \n",
      "     Training Step: 2 Training Loss: 3.41182804107666 \n",
      "     Training Step: 3 Training Loss: 4.675167560577393 \n",
      "     Training Step: 4 Training Loss: 3.3233213424682617 \n",
      "     Training Step: 5 Training Loss: 3.438235282897949 \n",
      "     Training Step: 6 Training Loss: 2.9404735565185547 \n",
      "     Training Step: 7 Training Loss: 2.5120959281921387 \n",
      "     Training Step: 8 Training Loss: 2.9856019020080566 \n",
      "     Training Step: 9 Training Loss: 2.370168924331665 \n",
      "     Training Step: 10 Training Loss: 2.218284845352173 \n",
      "     Training Step: 11 Training Loss: 2.7955238819122314 \n",
      "     Training Step: 12 Training Loss: 3.5405614376068115 \n",
      "     Training Step: 13 Training Loss: 2.8733768463134766 \n",
      "     Training Step: 14 Training Loss: 2.5358452796936035 \n",
      "     Training Step: 15 Training Loss: 2.9243569374084473 \n",
      "     Training Step: 16 Training Loss: 3.2161359786987305 \n",
      "     Training Step: 17 Training Loss: 3.103351354598999 \n",
      "     Training Step: 18 Training Loss: 2.2321441173553467 \n",
      "     Training Step: 19 Training Loss: 2.4087255001068115 \n",
      "     Training Step: 20 Training Loss: 3.2018496990203857 \n",
      "     Training Step: 21 Training Loss: 3.39274263381958 \n",
      "     Training Step: 22 Training Loss: 2.2617015838623047 \n",
      "     Training Step: 23 Training Loss: 2.1513142585754395 \n",
      "     Training Step: 24 Training Loss: 3.0081608295440674 \n",
      "     Training Step: 25 Training Loss: 2.720475673675537 \n",
      "     Training Step: 26 Training Loss: 3.517817258834839 \n",
      "     Training Step: 27 Training Loss: 2.9935760498046875 \n",
      "     Training Step: 28 Training Loss: 3.7686927318573 \n",
      "     Training Step: 29 Training Loss: 3.5994808673858643 \n",
      "     Training Step: 30 Training Loss: 3.3750782012939453 \n",
      "     Training Step: 31 Training Loss: 2.12869930267334 \n",
      "     Training Step: 32 Training Loss: 3.1339540481567383 \n",
      "     Training Step: 33 Training Loss: 3.135340690612793 \n",
      "     Training Step: 34 Training Loss: 2.5547561645507812 \n",
      "     Training Step: 35 Training Loss: 2.2397708892822266 \n",
      "     Training Step: 36 Training Loss: 3.053311347961426 \n",
      "     Training Step: 37 Training Loss: 2.3429341316223145 \n",
      "     Training Step: 38 Training Loss: 3.7658936977386475 \n",
      "     Training Step: 39 Training Loss: 4.331502437591553 \n",
      "     Training Step: 40 Training Loss: 2.254896402359009 \n",
      "     Training Step: 41 Training Loss: 2.709838390350342 \n",
      "     Training Step: 42 Training Loss: 2.5059914588928223 \n",
      "     Training Step: 43 Training Loss: 3.2229342460632324 \n",
      "     Training Step: 44 Training Loss: 3.745121955871582 \n",
      "     Training Step: 45 Training Loss: 2.1831231117248535 \n",
      "     Training Step: 46 Training Loss: 2.887815475463867 \n",
      "     Training Step: 47 Training Loss: 4.226601600646973 \n",
      "     Training Step: 48 Training Loss: 3.3141632080078125 \n",
      "     Training Step: 49 Training Loss: 3.6704258918762207 \n",
      "     Training Step: 50 Training Loss: 2.963486671447754 \n",
      "     Training Step: 51 Training Loss: 2.350727081298828 \n",
      "     Training Step: 52 Training Loss: 3.695646286010742 \n",
      "     Training Step: 53 Training Loss: 4.081585884094238 \n",
      "     Training Step: 54 Training Loss: 3.0897555351257324 \n",
      "     Training Step: 55 Training Loss: 3.2786262035369873 \n",
      "     Training Step: 56 Training Loss: 3.239119052886963 \n",
      "     Training Step: 57 Training Loss: 3.447690486907959 \n",
      "     Training Step: 58 Training Loss: 2.864189624786377 \n",
      "     Training Step: 59 Training Loss: 2.933520555496216 \n",
      "     Training Step: 60 Training Loss: 3.6089649200439453 \n",
      "     Training Step: 61 Training Loss: 3.22349214553833 \n",
      "     Training Step: 62 Training Loss: 2.2455716133117676 \n",
      "     Training Step: 63 Training Loss: 2.6375536918640137 \n",
      "     Training Step: 64 Training Loss: 2.5401315689086914 \n",
      "     Training Step: 65 Training Loss: 2.789238929748535 \n",
      "     Training Step: 66 Training Loss: 2.5678601264953613 \n",
      "     Training Step: 67 Training Loss: 2.467031478881836 \n",
      "     Training Step: 68 Training Loss: 2.539113998413086 \n",
      "     Training Step: 69 Training Loss: 3.1953444480895996 \n",
      "     Training Step: 70 Training Loss: 2.7810111045837402 \n",
      "     Training Step: 71 Training Loss: 2.4906094074249268 \n",
      "     Training Step: 72 Training Loss: 3.5963706970214844 \n",
      "     Training Step: 73 Training Loss: 2.8969216346740723 \n",
      "     Training Step: 74 Training Loss: 2.895810127258301 \n",
      "     Training Step: 75 Training Loss: 3.012211322784424 \n",
      "     Training Step: 76 Training Loss: 4.113494873046875 \n",
      "     Training Step: 77 Training Loss: 3.611851692199707 \n",
      "     Training Step: 78 Training Loss: 3.2607884407043457 \n",
      "     Training Step: 79 Training Loss: 3.1315221786499023 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.439946174621582 \n",
      "     Validation Step: 1 Validation Loss: 2.936877727508545 \n",
      "     Validation Step: 2 Validation Loss: 2.6656341552734375 \n",
      "     Validation Step: 3 Validation Loss: 3.1389780044555664 \n",
      "     Validation Step: 4 Validation Loss: 2.536752462387085 \n",
      "     Validation Step: 5 Validation Loss: 3.8935165405273438 \n",
      "     Validation Step: 6 Validation Loss: 3.6038947105407715 \n",
      "     Validation Step: 7 Validation Loss: 3.316214084625244 \n",
      "     Validation Step: 8 Validation Loss: 3.517888307571411 \n",
      "     Validation Step: 9 Validation Loss: 2.9546091556549072 \n",
      "     Validation Step: 10 Validation Loss: 2.4665098190307617 \n",
      "     Validation Step: 11 Validation Loss: 3.813391923904419 \n",
      "     Validation Step: 12 Validation Loss: 2.8785815238952637 \n",
      "     Validation Step: 13 Validation Loss: 2.4916083812713623 \n",
      "     Validation Step: 14 Validation Loss: 3.4663760662078857 \n",
      "Epoch: 84\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.38728404045105 \n",
      "     Training Step: 1 Training Loss: 3.1741981506347656 \n",
      "     Training Step: 2 Training Loss: 3.525338888168335 \n",
      "     Training Step: 3 Training Loss: 2.6523959636688232 \n",
      "     Training Step: 4 Training Loss: 3.225336790084839 \n",
      "     Training Step: 5 Training Loss: 2.076201915740967 \n",
      "     Training Step: 6 Training Loss: 3.128523826599121 \n",
      "     Training Step: 7 Training Loss: 3.2845053672790527 \n",
      "     Training Step: 8 Training Loss: 3.1131067276000977 \n",
      "     Training Step: 9 Training Loss: 2.1884140968322754 \n",
      "     Training Step: 10 Training Loss: 2.7116241455078125 \n",
      "     Training Step: 11 Training Loss: 3.16505765914917 \n",
      "     Training Step: 12 Training Loss: 3.5473155975341797 \n",
      "     Training Step: 13 Training Loss: 2.7740306854248047 \n",
      "     Training Step: 14 Training Loss: 3.502828598022461 \n",
      "     Training Step: 15 Training Loss: 3.6237149238586426 \n",
      "     Training Step: 16 Training Loss: 4.013299465179443 \n",
      "     Training Step: 17 Training Loss: 3.3610262870788574 \n",
      "     Training Step: 18 Training Loss: 3.0156168937683105 \n",
      "     Training Step: 19 Training Loss: 4.172622203826904 \n",
      "     Training Step: 20 Training Loss: 2.9073867797851562 \n",
      "     Training Step: 21 Training Loss: 3.851027011871338 \n",
      "     Training Step: 22 Training Loss: 3.321139097213745 \n",
      "     Training Step: 23 Training Loss: 2.4062411785125732 \n",
      "     Training Step: 24 Training Loss: 3.21982479095459 \n",
      "     Training Step: 25 Training Loss: 2.771190881729126 \n",
      "     Training Step: 26 Training Loss: 2.4972565174102783 \n",
      "     Training Step: 27 Training Loss: 3.5654966831207275 \n",
      "     Training Step: 28 Training Loss: 3.160788059234619 \n",
      "     Training Step: 29 Training Loss: 3.2022571563720703 \n",
      "     Training Step: 30 Training Loss: 3.5427823066711426 \n",
      "     Training Step: 31 Training Loss: 2.2561752796173096 \n",
      "     Training Step: 32 Training Loss: 2.3752987384796143 \n",
      "     Training Step: 33 Training Loss: 3.6131303310394287 \n",
      "     Training Step: 34 Training Loss: 2.9422290325164795 \n",
      "     Training Step: 35 Training Loss: 2.941422462463379 \n",
      "     Training Step: 36 Training Loss: 3.2885079383850098 \n",
      "     Training Step: 37 Training Loss: 2.4878501892089844 \n",
      "     Training Step: 38 Training Loss: 3.153085231781006 \n",
      "     Training Step: 39 Training Loss: 2.8327066898345947 \n",
      "     Training Step: 40 Training Loss: 2.9410758018493652 \n",
      "     Training Step: 41 Training Loss: 2.4702653884887695 \n",
      "     Training Step: 42 Training Loss: 2.417506694793701 \n",
      "     Training Step: 43 Training Loss: 3.3215763568878174 \n",
      "     Training Step: 44 Training Loss: 2.4295270442962646 \n",
      "     Training Step: 45 Training Loss: 2.853627920150757 \n",
      "     Training Step: 46 Training Loss: 2.917360305786133 \n",
      "     Training Step: 47 Training Loss: 3.3668341636657715 \n",
      "     Training Step: 48 Training Loss: 2.6600341796875 \n",
      "     Training Step: 49 Training Loss: 3.193892240524292 \n",
      "     Training Step: 50 Training Loss: 2.3059463500976562 \n",
      "     Training Step: 51 Training Loss: 4.245398998260498 \n",
      "     Training Step: 52 Training Loss: 2.4777026176452637 \n",
      "     Training Step: 53 Training Loss: 2.1574440002441406 \n",
      "     Training Step: 54 Training Loss: 2.2494468688964844 \n",
      "     Training Step: 55 Training Loss: 2.0846996307373047 \n",
      "     Training Step: 56 Training Loss: 3.187422752380371 \n",
      "     Training Step: 57 Training Loss: 2.8199336528778076 \n",
      "     Training Step: 58 Training Loss: 3.23756742477417 \n",
      "     Training Step: 59 Training Loss: 2.454979419708252 \n",
      "     Training Step: 60 Training Loss: 2.125231981277466 \n",
      "     Training Step: 61 Training Loss: 2.5891623497009277 \n",
      "     Training Step: 62 Training Loss: 3.268108367919922 \n",
      "     Training Step: 63 Training Loss: 2.814229726791382 \n",
      "     Training Step: 64 Training Loss: 2.5525217056274414 \n",
      "     Training Step: 65 Training Loss: 3.0107879638671875 \n",
      "     Training Step: 66 Training Loss: 3.839345932006836 \n",
      "     Training Step: 67 Training Loss: 3.194847583770752 \n",
      "     Training Step: 68 Training Loss: 2.6320526599884033 \n",
      "     Training Step: 69 Training Loss: 4.349871635437012 \n",
      "     Training Step: 70 Training Loss: 2.3838508129119873 \n",
      "     Training Step: 71 Training Loss: 2.572817802429199 \n",
      "     Training Step: 72 Training Loss: 2.3561010360717773 \n",
      "     Training Step: 73 Training Loss: 2.1318366527557373 \n",
      "     Training Step: 74 Training Loss: 2.886932373046875 \n",
      "     Training Step: 75 Training Loss: 4.665594100952148 \n",
      "     Training Step: 76 Training Loss: 3.535118341445923 \n",
      "     Training Step: 77 Training Loss: 3.0864248275756836 \n",
      "     Training Step: 78 Training Loss: 3.1318140029907227 \n",
      "     Training Step: 79 Training Loss: 2.2319788932800293 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9864466190338135 \n",
      "     Validation Step: 1 Validation Loss: 3.5210254192352295 \n",
      "     Validation Step: 2 Validation Loss: 2.8180460929870605 \n",
      "     Validation Step: 3 Validation Loss: 3.3701045513153076 \n",
      "     Validation Step: 4 Validation Loss: 3.0676605701446533 \n",
      "     Validation Step: 5 Validation Loss: 3.3449041843414307 \n",
      "     Validation Step: 6 Validation Loss: 3.5838003158569336 \n",
      "     Validation Step: 7 Validation Loss: 2.294213056564331 \n",
      "     Validation Step: 8 Validation Loss: 3.8158645629882812 \n",
      "     Validation Step: 9 Validation Loss: 3.532930612564087 \n",
      "     Validation Step: 10 Validation Loss: 2.6032323837280273 \n",
      "     Validation Step: 11 Validation Loss: 2.7069320678710938 \n",
      "     Validation Step: 12 Validation Loss: 3.0803539752960205 \n",
      "     Validation Step: 13 Validation Loss: 3.1353132724761963 \n",
      "     Validation Step: 14 Validation Loss: 3.649554967880249 \n",
      "Epoch: 85\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.091872215270996 \n",
      "     Training Step: 1 Training Loss: 3.25300931930542 \n",
      "     Training Step: 2 Training Loss: 2.8835887908935547 \n",
      "     Training Step: 3 Training Loss: 3.021296977996826 \n",
      "     Training Step: 4 Training Loss: 2.9260478019714355 \n",
      "     Training Step: 5 Training Loss: 2.6895155906677246 \n",
      "     Training Step: 6 Training Loss: 2.974738597869873 \n",
      "     Training Step: 7 Training Loss: 2.4053540229797363 \n",
      "     Training Step: 8 Training Loss: 2.801892042160034 \n",
      "     Training Step: 9 Training Loss: 3.390512466430664 \n",
      "     Training Step: 10 Training Loss: 3.3344790935516357 \n",
      "     Training Step: 11 Training Loss: 3.656019926071167 \n",
      "     Training Step: 12 Training Loss: 2.8586156368255615 \n",
      "     Training Step: 13 Training Loss: 3.5995054244995117 \n",
      "     Training Step: 14 Training Loss: 2.303649425506592 \n",
      "     Training Step: 15 Training Loss: 2.5525312423706055 \n",
      "     Training Step: 16 Training Loss: 3.1913931369781494 \n",
      "     Training Step: 17 Training Loss: 3.205334186553955 \n",
      "     Training Step: 18 Training Loss: 3.1050515174865723 \n",
      "     Training Step: 19 Training Loss: 2.176088809967041 \n",
      "     Training Step: 20 Training Loss: 3.4001386165618896 \n",
      "     Training Step: 21 Training Loss: 2.8457069396972656 \n",
      "     Training Step: 22 Training Loss: 3.272106409072876 \n",
      "     Training Step: 23 Training Loss: 3.3476076126098633 \n",
      "     Training Step: 24 Training Loss: 2.138439655303955 \n",
      "     Training Step: 25 Training Loss: 2.7616934776306152 \n",
      "     Training Step: 26 Training Loss: 3.775212526321411 \n",
      "     Training Step: 27 Training Loss: 3.5504112243652344 \n",
      "     Training Step: 28 Training Loss: 2.7775216102600098 \n",
      "     Training Step: 29 Training Loss: 3.544908046722412 \n",
      "     Training Step: 30 Training Loss: 2.067155599594116 \n",
      "     Training Step: 31 Training Loss: 3.7705116271972656 \n",
      "     Training Step: 32 Training Loss: 2.546492576599121 \n",
      "     Training Step: 33 Training Loss: 3.827967882156372 \n",
      "     Training Step: 34 Training Loss: 3.3220973014831543 \n",
      "     Training Step: 35 Training Loss: 2.781067371368408 \n",
      "     Training Step: 36 Training Loss: 2.9459569454193115 \n",
      "     Training Step: 37 Training Loss: 2.547894239425659 \n",
      "     Training Step: 38 Training Loss: 2.3619585037231445 \n",
      "     Training Step: 39 Training Loss: 4.388194561004639 \n",
      "     Training Step: 40 Training Loss: 2.278374671936035 \n",
      "     Training Step: 41 Training Loss: 3.351933479309082 \n",
      "     Training Step: 42 Training Loss: 3.153779983520508 \n",
      "     Training Step: 43 Training Loss: 3.189206600189209 \n",
      "     Training Step: 44 Training Loss: 3.263881206512451 \n",
      "     Training Step: 45 Training Loss: 4.306209564208984 \n",
      "     Training Step: 46 Training Loss: 3.147155284881592 \n",
      "     Training Step: 47 Training Loss: 3.7056961059570312 \n",
      "     Training Step: 48 Training Loss: 2.654726505279541 \n",
      "     Training Step: 49 Training Loss: 2.4235119819641113 \n",
      "     Training Step: 50 Training Loss: 3.083383560180664 \n",
      "     Training Step: 51 Training Loss: 3.286625385284424 \n",
      "     Training Step: 52 Training Loss: 3.0780386924743652 \n",
      "     Training Step: 53 Training Loss: 2.5783023834228516 \n",
      "     Training Step: 54 Training Loss: 2.5459556579589844 \n",
      "     Training Step: 55 Training Loss: 3.735978603363037 \n",
      "     Training Step: 56 Training Loss: 2.315192699432373 \n",
      "     Training Step: 57 Training Loss: 2.8051376342773438 \n",
      "     Training Step: 58 Training Loss: 2.888334274291992 \n",
      "     Training Step: 59 Training Loss: 2.1620235443115234 \n",
      "     Training Step: 60 Training Loss: 3.591176748275757 \n",
      "     Training Step: 61 Training Loss: 2.6902503967285156 \n",
      "     Training Step: 62 Training Loss: 2.4199085235595703 \n",
      "     Training Step: 63 Training Loss: 2.9160618782043457 \n",
      "     Training Step: 64 Training Loss: 2.8181943893432617 \n",
      "     Training Step: 65 Training Loss: 3.022254228591919 \n",
      "     Training Step: 66 Training Loss: 2.708348512649536 \n",
      "     Training Step: 67 Training Loss: 4.193424701690674 \n",
      "     Training Step: 68 Training Loss: 3.676685333251953 \n",
      "     Training Step: 69 Training Loss: 4.676959991455078 \n",
      "     Training Step: 70 Training Loss: 2.546271562576294 \n",
      "     Training Step: 71 Training Loss: 3.9400644302368164 \n",
      "     Training Step: 72 Training Loss: 2.181351661682129 \n",
      "     Training Step: 73 Training Loss: 2.2509055137634277 \n",
      "     Training Step: 74 Training Loss: 3.3700947761535645 \n",
      "     Training Step: 75 Training Loss: 2.7915310859680176 \n",
      "     Training Step: 76 Training Loss: 2.5491061210632324 \n",
      "     Training Step: 77 Training Loss: 2.2379684448242188 \n",
      "     Training Step: 78 Training Loss: 2.5590994358062744 \n",
      "     Training Step: 79 Training Loss: 2.6066455841064453 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.612302303314209 \n",
      "     Validation Step: 1 Validation Loss: 2.795193672180176 \n",
      "     Validation Step: 2 Validation Loss: 2.9524128437042236 \n",
      "     Validation Step: 3 Validation Loss: 3.5017666816711426 \n",
      "     Validation Step: 4 Validation Loss: 2.337484836578369 \n",
      "     Validation Step: 5 Validation Loss: 3.173386573791504 \n",
      "     Validation Step: 6 Validation Loss: 3.136164903640747 \n",
      "     Validation Step: 7 Validation Loss: 3.7089147567749023 \n",
      "     Validation Step: 8 Validation Loss: 3.6887660026550293 \n",
      "     Validation Step: 9 Validation Loss: 2.7029404640197754 \n",
      "     Validation Step: 10 Validation Loss: 3.101780891418457 \n",
      "     Validation Step: 11 Validation Loss: 3.046189785003662 \n",
      "     Validation Step: 12 Validation Loss: 2.7491936683654785 \n",
      "     Validation Step: 13 Validation Loss: 3.95540189743042 \n",
      "     Validation Step: 14 Validation Loss: 3.5894627571105957 \n",
      "Epoch: 86\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.3255105018615723 \n",
      "     Training Step: 1 Training Loss: 2.8626692295074463 \n",
      "     Training Step: 2 Training Loss: 2.449577808380127 \n",
      "     Training Step: 3 Training Loss: 3.263604164123535 \n",
      "     Training Step: 4 Training Loss: 3.3550057411193848 \n",
      "     Training Step: 5 Training Loss: 4.2283501625061035 \n",
      "     Training Step: 6 Training Loss: 3.643430233001709 \n",
      "     Training Step: 7 Training Loss: 2.447567939758301 \n",
      "     Training Step: 8 Training Loss: 2.883234977722168 \n",
      "     Training Step: 9 Training Loss: 3.8454086780548096 \n",
      "     Training Step: 10 Training Loss: 3.17038631439209 \n",
      "     Training Step: 11 Training Loss: 2.3422203063964844 \n",
      "     Training Step: 12 Training Loss: 2.924044609069824 \n",
      "     Training Step: 13 Training Loss: 3.588984966278076 \n",
      "     Training Step: 14 Training Loss: 2.858341693878174 \n",
      "     Training Step: 15 Training Loss: 3.3558571338653564 \n",
      "     Training Step: 16 Training Loss: 2.449436664581299 \n",
      "     Training Step: 17 Training Loss: 4.433424949645996 \n",
      "     Training Step: 18 Training Loss: 3.5092239379882812 \n",
      "     Training Step: 19 Training Loss: 4.032950401306152 \n",
      "     Training Step: 20 Training Loss: 3.505547046661377 \n",
      "     Training Step: 21 Training Loss: 2.643428325653076 \n",
      "     Training Step: 22 Training Loss: 2.913130283355713 \n",
      "     Training Step: 23 Training Loss: 3.322554588317871 \n",
      "     Training Step: 24 Training Loss: 2.884974241256714 \n",
      "     Training Step: 25 Training Loss: 2.7687127590179443 \n",
      "     Training Step: 26 Training Loss: 4.0425944328308105 \n",
      "     Training Step: 27 Training Loss: 2.4328341484069824 \n",
      "     Training Step: 28 Training Loss: 3.041455030441284 \n",
      "     Training Step: 29 Training Loss: 3.113090991973877 \n",
      "     Training Step: 30 Training Loss: 2.858217239379883 \n",
      "     Training Step: 31 Training Loss: 3.334868907928467 \n",
      "     Training Step: 32 Training Loss: 2.5075924396514893 \n",
      "     Training Step: 33 Training Loss: 3.145880699157715 \n",
      "     Training Step: 34 Training Loss: 3.5430705547332764 \n",
      "     Training Step: 35 Training Loss: 3.5137081146240234 \n",
      "     Training Step: 36 Training Loss: 3.4810707569122314 \n",
      "     Training Step: 37 Training Loss: 2.511800527572632 \n",
      "     Training Step: 38 Training Loss: 3.595231056213379 \n",
      "     Training Step: 39 Training Loss: 2.719109058380127 \n",
      "     Training Step: 40 Training Loss: 3.149237632751465 \n",
      "     Training Step: 41 Training Loss: 3.068948268890381 \n",
      "     Training Step: 42 Training Loss: 2.5792593955993652 \n",
      "     Training Step: 43 Training Loss: 2.3914949893951416 \n",
      "     Training Step: 44 Training Loss: 2.5089831352233887 \n",
      "     Training Step: 45 Training Loss: 2.2132856845855713 \n",
      "     Training Step: 46 Training Loss: 3.3819360733032227 \n",
      "     Training Step: 47 Training Loss: 2.8015241622924805 \n",
      "     Training Step: 48 Training Loss: 2.283967971801758 \n",
      "     Training Step: 49 Training Loss: 3.3370397090911865 \n",
      "     Training Step: 50 Training Loss: 2.9781360626220703 \n",
      "     Training Step: 51 Training Loss: 2.3080296516418457 \n",
      "     Training Step: 52 Training Loss: 2.988764762878418 \n",
      "     Training Step: 53 Training Loss: 2.5692691802978516 \n",
      "     Training Step: 54 Training Loss: 3.040128469467163 \n",
      "     Training Step: 55 Training Loss: 2.251573085784912 \n",
      "     Training Step: 56 Training Loss: 2.6372179985046387 \n",
      "     Training Step: 57 Training Loss: 3.376347541809082 \n",
      "     Training Step: 58 Training Loss: 3.4681694507598877 \n",
      "     Training Step: 59 Training Loss: 2.470069646835327 \n",
      "     Training Step: 60 Training Loss: 3.342589855194092 \n",
      "     Training Step: 61 Training Loss: 3.219027042388916 \n",
      "     Training Step: 62 Training Loss: 3.6005196571350098 \n",
      "     Training Step: 63 Training Loss: 2.2954635620117188 \n",
      "     Training Step: 64 Training Loss: 3.387645721435547 \n",
      "     Training Step: 65 Training Loss: 2.3325748443603516 \n",
      "     Training Step: 66 Training Loss: 3.5368988513946533 \n",
      "     Training Step: 67 Training Loss: 2.144184112548828 \n",
      "     Training Step: 68 Training Loss: 2.263021945953369 \n",
      "     Training Step: 69 Training Loss: 2.449587345123291 \n",
      "     Training Step: 70 Training Loss: 2.385305166244507 \n",
      "     Training Step: 71 Training Loss: 2.727949619293213 \n",
      "     Training Step: 72 Training Loss: 4.239418029785156 \n",
      "     Training Step: 73 Training Loss: 2.1363537311553955 \n",
      "     Training Step: 74 Training Loss: 2.7418413162231445 \n",
      "     Training Step: 75 Training Loss: 2.5991499423980713 \n",
      "     Training Step: 76 Training Loss: 2.7097535133361816 \n",
      "     Training Step: 77 Training Loss: 2.7397823333740234 \n",
      "     Training Step: 78 Training Loss: 2.7124691009521484 \n",
      "     Training Step: 79 Training Loss: 2.2739005088806152 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.793488025665283 \n",
      "     Validation Step: 1 Validation Loss: 3.17032790184021 \n",
      "     Validation Step: 2 Validation Loss: 3.0180840492248535 \n",
      "     Validation Step: 3 Validation Loss: 3.6177215576171875 \n",
      "     Validation Step: 4 Validation Loss: 2.95013689994812 \n",
      "     Validation Step: 5 Validation Loss: 3.2886240482330322 \n",
      "     Validation Step: 6 Validation Loss: 2.2323057651519775 \n",
      "     Validation Step: 7 Validation Loss: 3.7893505096435547 \n",
      "     Validation Step: 8 Validation Loss: 4.018948554992676 \n",
      "     Validation Step: 9 Validation Loss: 3.5942862033843994 \n",
      "     Validation Step: 10 Validation Loss: 2.841087579727173 \n",
      "     Validation Step: 11 Validation Loss: 2.726402521133423 \n",
      "     Validation Step: 12 Validation Loss: 2.8361544609069824 \n",
      "     Validation Step: 13 Validation Loss: 3.2694146633148193 \n",
      "     Validation Step: 14 Validation Loss: 3.1145646572113037 \n",
      "Epoch: 87\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.7150373458862305 \n",
      "     Training Step: 1 Training Loss: 2.2198479175567627 \n",
      "     Training Step: 2 Training Loss: 3.1893959045410156 \n",
      "     Training Step: 3 Training Loss: 2.4008288383483887 \n",
      "     Training Step: 4 Training Loss: 3.3093252182006836 \n",
      "     Training Step: 5 Training Loss: 2.6786646842956543 \n",
      "     Training Step: 6 Training Loss: 2.8016884326934814 \n",
      "     Training Step: 7 Training Loss: 3.3465306758880615 \n",
      "     Training Step: 8 Training Loss: 3.2154245376586914 \n",
      "     Training Step: 9 Training Loss: 2.4015047550201416 \n",
      "     Training Step: 10 Training Loss: 3.138758420944214 \n",
      "     Training Step: 11 Training Loss: 3.151937484741211 \n",
      "     Training Step: 12 Training Loss: 2.5708117485046387 \n",
      "     Training Step: 13 Training Loss: 2.4368667602539062 \n",
      "     Training Step: 14 Training Loss: 3.526869773864746 \n",
      "     Training Step: 15 Training Loss: 2.6356186866760254 \n",
      "     Training Step: 16 Training Loss: 2.6402788162231445 \n",
      "     Training Step: 17 Training Loss: 2.166137933731079 \n",
      "     Training Step: 18 Training Loss: 4.646235466003418 \n",
      "     Training Step: 19 Training Loss: 4.101316452026367 \n",
      "     Training Step: 20 Training Loss: 3.443274974822998 \n",
      "     Training Step: 21 Training Loss: 2.361635684967041 \n",
      "     Training Step: 22 Training Loss: 3.302861213684082 \n",
      "     Training Step: 23 Training Loss: 2.951455593109131 \n",
      "     Training Step: 24 Training Loss: 2.9583005905151367 \n",
      "     Training Step: 25 Training Loss: 2.4004931449890137 \n",
      "     Training Step: 26 Training Loss: 3.132235527038574 \n",
      "     Training Step: 27 Training Loss: 2.4752798080444336 \n",
      "     Training Step: 28 Training Loss: 3.0251784324645996 \n",
      "     Training Step: 29 Training Loss: 2.3990237712860107 \n",
      "     Training Step: 30 Training Loss: 3.563828468322754 \n",
      "     Training Step: 31 Training Loss: 3.133082866668701 \n",
      "     Training Step: 32 Training Loss: 3.597074508666992 \n",
      "     Training Step: 33 Training Loss: 2.32191801071167 \n",
      "     Training Step: 34 Training Loss: 2.6684587001800537 \n",
      "     Training Step: 35 Training Loss: 4.005645275115967 \n",
      "     Training Step: 36 Training Loss: 3.2065558433532715 \n",
      "     Training Step: 37 Training Loss: 2.5772836208343506 \n",
      "     Training Step: 38 Training Loss: 3.590231418609619 \n",
      "     Training Step: 39 Training Loss: 2.895346164703369 \n",
      "     Training Step: 40 Training Loss: 3.1115076541900635 \n",
      "     Training Step: 41 Training Loss: 2.585206985473633 \n",
      "     Training Step: 42 Training Loss: 3.577181339263916 \n",
      "     Training Step: 43 Training Loss: 2.540447473526001 \n",
      "     Training Step: 44 Training Loss: 3.20686936378479 \n",
      "     Training Step: 45 Training Loss: 2.390407085418701 \n",
      "     Training Step: 46 Training Loss: 2.9117813110351562 \n",
      "     Training Step: 47 Training Loss: 3.1079843044281006 \n",
      "     Training Step: 48 Training Loss: 2.8766837120056152 \n",
      "     Training Step: 49 Training Loss: 3.68471097946167 \n",
      "     Training Step: 50 Training Loss: 2.2864437103271484 \n",
      "     Training Step: 51 Training Loss: 4.252653121948242 \n",
      "     Training Step: 52 Training Loss: 2.265568733215332 \n",
      "     Training Step: 53 Training Loss: 2.8175692558288574 \n",
      "     Training Step: 54 Training Loss: 3.4350688457489014 \n",
      "     Training Step: 55 Training Loss: 3.40364146232605 \n",
      "     Training Step: 56 Training Loss: 2.910163402557373 \n",
      "     Training Step: 57 Training Loss: 3.0154523849487305 \n",
      "     Training Step: 58 Training Loss: 3.8376710414886475 \n",
      "     Training Step: 59 Training Loss: 2.9092564582824707 \n",
      "     Training Step: 60 Training Loss: 2.101503849029541 \n",
      "     Training Step: 61 Training Loss: 3.8880653381347656 \n",
      "     Training Step: 62 Training Loss: 2.0911107063293457 \n",
      "     Training Step: 63 Training Loss: 3.2845423221588135 \n",
      "     Training Step: 64 Training Loss: 2.63049054145813 \n",
      "     Training Step: 65 Training Loss: 3.1731033325195312 \n",
      "     Training Step: 66 Training Loss: 3.3332858085632324 \n",
      "     Training Step: 67 Training Loss: 2.537311553955078 \n",
      "     Training Step: 68 Training Loss: 2.9684982299804688 \n",
      "     Training Step: 69 Training Loss: 2.8253397941589355 \n",
      "     Training Step: 70 Training Loss: 2.614457130432129 \n",
      "     Training Step: 71 Training Loss: 4.5815582275390625 \n",
      "     Training Step: 72 Training Loss: 3.290616512298584 \n",
      "     Training Step: 73 Training Loss: 3.8493595123291016 \n",
      "     Training Step: 74 Training Loss: 2.494840383529663 \n",
      "     Training Step: 75 Training Loss: 3.143540859222412 \n",
      "     Training Step: 76 Training Loss: 2.805475950241089 \n",
      "     Training Step: 77 Training Loss: 2.3246989250183105 \n",
      "     Training Step: 78 Training Loss: 3.698075294494629 \n",
      "     Training Step: 79 Training Loss: 2.8909494876861572 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.2078216075897217 \n",
      "     Validation Step: 1 Validation Loss: 3.688000440597534 \n",
      "     Validation Step: 2 Validation Loss: 3.3806185722351074 \n",
      "     Validation Step: 3 Validation Loss: 3.784027099609375 \n",
      "     Validation Step: 4 Validation Loss: 3.018496036529541 \n",
      "     Validation Step: 5 Validation Loss: 3.89376163482666 \n",
      "     Validation Step: 6 Validation Loss: 2.74837589263916 \n",
      "     Validation Step: 7 Validation Loss: 3.5197973251342773 \n",
      "     Validation Step: 8 Validation Loss: 3.6542398929595947 \n",
      "     Validation Step: 9 Validation Loss: 2.7580032348632812 \n",
      "     Validation Step: 10 Validation Loss: 3.0644168853759766 \n",
      "     Validation Step: 11 Validation Loss: 3.2672390937805176 \n",
      "     Validation Step: 12 Validation Loss: 2.136946678161621 \n",
      "     Validation Step: 13 Validation Loss: 3.1080169677734375 \n",
      "     Validation Step: 14 Validation Loss: 3.1324942111968994 \n",
      "Epoch: 88\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.7150704860687256 \n",
      "     Training Step: 1 Training Loss: 3.4704365730285645 \n",
      "     Training Step: 2 Training Loss: 2.576143980026245 \n",
      "     Training Step: 3 Training Loss: 2.330226421356201 \n",
      "     Training Step: 4 Training Loss: 3.001173496246338 \n",
      "     Training Step: 5 Training Loss: 3.1167964935302734 \n",
      "     Training Step: 6 Training Loss: 2.726775646209717 \n",
      "     Training Step: 7 Training Loss: 3.069571018218994 \n",
      "     Training Step: 8 Training Loss: 2.7892210483551025 \n",
      "     Training Step: 9 Training Loss: 2.971072196960449 \n",
      "     Training Step: 10 Training Loss: 2.3231260776519775 \n",
      "     Training Step: 11 Training Loss: 2.1011996269226074 \n",
      "     Training Step: 12 Training Loss: 2.891474723815918 \n",
      "     Training Step: 13 Training Loss: 4.694244861602783 \n",
      "     Training Step: 14 Training Loss: 3.0829498767852783 \n",
      "     Training Step: 15 Training Loss: 2.213449001312256 \n",
      "     Training Step: 16 Training Loss: 3.487527370452881 \n",
      "     Training Step: 17 Training Loss: 2.4228453636169434 \n",
      "     Training Step: 18 Training Loss: 2.593773365020752 \n",
      "     Training Step: 19 Training Loss: 2.912029266357422 \n",
      "     Training Step: 20 Training Loss: 3.3726792335510254 \n",
      "     Training Step: 21 Training Loss: 2.7822117805480957 \n",
      "     Training Step: 22 Training Loss: 2.379072427749634 \n",
      "     Training Step: 23 Training Loss: 3.4669113159179688 \n",
      "     Training Step: 24 Training Loss: 2.086198329925537 \n",
      "     Training Step: 25 Training Loss: 3.8421053886413574 \n",
      "     Training Step: 26 Training Loss: 3.4474070072174072 \n",
      "     Training Step: 27 Training Loss: 3.111692428588867 \n",
      "     Training Step: 28 Training Loss: 3.1604197025299072 \n",
      "     Training Step: 29 Training Loss: 3.7488393783569336 \n",
      "     Training Step: 30 Training Loss: 3.819148063659668 \n",
      "     Training Step: 31 Training Loss: 2.741431713104248 \n",
      "     Training Step: 32 Training Loss: 2.752406597137451 \n",
      "     Training Step: 33 Training Loss: 2.6067559719085693 \n",
      "     Training Step: 34 Training Loss: 3.4501752853393555 \n",
      "     Training Step: 35 Training Loss: 2.2457194328308105 \n",
      "     Training Step: 36 Training Loss: 2.016894817352295 \n",
      "     Training Step: 37 Training Loss: 2.756019115447998 \n",
      "     Training Step: 38 Training Loss: 3.419651985168457 \n",
      "     Training Step: 39 Training Loss: 4.317755222320557 \n",
      "     Training Step: 40 Training Loss: 3.2624218463897705 \n",
      "     Training Step: 41 Training Loss: 2.2272658348083496 \n",
      "     Training Step: 42 Training Loss: 2.3267390727996826 \n",
      "     Training Step: 43 Training Loss: 2.5512993335723877 \n",
      "     Training Step: 44 Training Loss: 4.274260997772217 \n",
      "     Training Step: 45 Training Loss: 3.1310625076293945 \n",
      "     Training Step: 46 Training Loss: 2.2463128566741943 \n",
      "     Training Step: 47 Training Loss: 2.3484554290771484 \n",
      "     Training Step: 48 Training Loss: 2.91024112701416 \n",
      "     Training Step: 49 Training Loss: 2.6797356605529785 \n",
      "     Training Step: 50 Training Loss: 3.3492093086242676 \n",
      "     Training Step: 51 Training Loss: 3.415419578552246 \n",
      "     Training Step: 52 Training Loss: 3.3697328567504883 \n",
      "     Training Step: 53 Training Loss: 3.7331180572509766 \n",
      "     Training Step: 54 Training Loss: 4.126865863800049 \n",
      "     Training Step: 55 Training Loss: 3.0637869834899902 \n",
      "     Training Step: 56 Training Loss: 3.061859130859375 \n",
      "     Training Step: 57 Training Loss: 3.016322612762451 \n",
      "     Training Step: 58 Training Loss: 2.108281373977661 \n",
      "     Training Step: 59 Training Loss: 2.480276107788086 \n",
      "     Training Step: 60 Training Loss: 3.2515711784362793 \n",
      "     Training Step: 61 Training Loss: 3.14566969871521 \n",
      "     Training Step: 62 Training Loss: 3.2411980628967285 \n",
      "     Training Step: 63 Training Loss: 2.517873764038086 \n",
      "     Training Step: 64 Training Loss: 2.40559983253479 \n",
      "     Training Step: 65 Training Loss: 2.626087188720703 \n",
      "     Training Step: 66 Training Loss: 2.9322268962860107 \n",
      "     Training Step: 67 Training Loss: 3.6437370777130127 \n",
      "     Training Step: 68 Training Loss: 2.42643404006958 \n",
      "     Training Step: 69 Training Loss: 3.318863868713379 \n",
      "     Training Step: 70 Training Loss: 3.086365222930908 \n",
      "     Training Step: 71 Training Loss: 2.748363971710205 \n",
      "     Training Step: 72 Training Loss: 2.247922420501709 \n",
      "     Training Step: 73 Training Loss: 3.2798662185668945 \n",
      "     Training Step: 74 Training Loss: 2.886857271194458 \n",
      "     Training Step: 75 Training Loss: 4.321902275085449 \n",
      "     Training Step: 76 Training Loss: 2.7416975498199463 \n",
      "     Training Step: 77 Training Loss: 3.0019726753234863 \n",
      "     Training Step: 78 Training Loss: 3.4357175827026367 \n",
      "     Training Step: 79 Training Loss: 2.4774856567382812 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.545858860015869 \n",
      "     Validation Step: 1 Validation Loss: 3.2608470916748047 \n",
      "     Validation Step: 2 Validation Loss: 2.922305107116699 \n",
      "     Validation Step: 3 Validation Loss: 2.629533290863037 \n",
      "     Validation Step: 4 Validation Loss: 2.6337528228759766 \n",
      "     Validation Step: 5 Validation Loss: 2.236945152282715 \n",
      "     Validation Step: 6 Validation Loss: 3.531165838241577 \n",
      "     Validation Step: 7 Validation Loss: 3.4877564907073975 \n",
      "     Validation Step: 8 Validation Loss: 3.1007323265075684 \n",
      "     Validation Step: 9 Validation Loss: 3.6592249870300293 \n",
      "     Validation Step: 10 Validation Loss: 2.9570930004119873 \n",
      "     Validation Step: 11 Validation Loss: 3.7629802227020264 \n",
      "     Validation Step: 12 Validation Loss: 3.597275733947754 \n",
      "     Validation Step: 13 Validation Loss: 3.0210633277893066 \n",
      "     Validation Step: 14 Validation Loss: 2.8538804054260254 \n",
      "Epoch: 89\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.4896774291992188 \n",
      "     Training Step: 1 Training Loss: 3.4474825859069824 \n",
      "     Training Step: 2 Training Loss: 2.7035231590270996 \n",
      "     Training Step: 3 Training Loss: 2.2351791858673096 \n",
      "     Training Step: 4 Training Loss: 2.9535393714904785 \n",
      "     Training Step: 5 Training Loss: 4.231462001800537 \n",
      "     Training Step: 6 Training Loss: 2.5712900161743164 \n",
      "     Training Step: 7 Training Loss: 2.700444221496582 \n",
      "     Training Step: 8 Training Loss: 3.4919145107269287 \n",
      "     Training Step: 9 Training Loss: 2.5210938453674316 \n",
      "     Training Step: 10 Training Loss: 2.7468056678771973 \n",
      "     Training Step: 11 Training Loss: 3.7284929752349854 \n",
      "     Training Step: 12 Training Loss: 3.133463144302368 \n",
      "     Training Step: 13 Training Loss: 2.4511566162109375 \n",
      "     Training Step: 14 Training Loss: 3.9244422912597656 \n",
      "     Training Step: 15 Training Loss: 3.5796947479248047 \n",
      "     Training Step: 16 Training Loss: 3.3360953330993652 \n",
      "     Training Step: 17 Training Loss: 2.3276681900024414 \n",
      "     Training Step: 18 Training Loss: 3.580852508544922 \n",
      "     Training Step: 19 Training Loss: 2.879276752471924 \n",
      "     Training Step: 20 Training Loss: 2.268873929977417 \n",
      "     Training Step: 21 Training Loss: 2.5897746086120605 \n",
      "     Training Step: 22 Training Loss: 3.54268741607666 \n",
      "     Training Step: 23 Training Loss: 3.0259134769439697 \n",
      "     Training Step: 24 Training Loss: 2.922410011291504 \n",
      "     Training Step: 25 Training Loss: 2.986604690551758 \n",
      "     Training Step: 26 Training Loss: 2.295498847961426 \n",
      "     Training Step: 27 Training Loss: 2.399272918701172 \n",
      "     Training Step: 28 Training Loss: 2.8694820404052734 \n",
      "     Training Step: 29 Training Loss: 3.2559471130371094 \n",
      "     Training Step: 30 Training Loss: 3.050182580947876 \n",
      "     Training Step: 31 Training Loss: 3.246593952178955 \n",
      "     Training Step: 32 Training Loss: 3.2903506755828857 \n",
      "     Training Step: 33 Training Loss: 2.8741092681884766 \n",
      "     Training Step: 34 Training Loss: 2.3991923332214355 \n",
      "     Training Step: 35 Training Loss: 3.6318554878234863 \n",
      "     Training Step: 36 Training Loss: 3.4287867546081543 \n",
      "     Training Step: 37 Training Loss: 2.7999684810638428 \n",
      "     Training Step: 38 Training Loss: 2.8091044425964355 \n",
      "     Training Step: 39 Training Loss: 2.8794472217559814 \n",
      "     Training Step: 40 Training Loss: 2.022498607635498 \n",
      "     Training Step: 41 Training Loss: 2.6528162956237793 \n",
      "     Training Step: 42 Training Loss: 2.387523651123047 \n",
      "     Training Step: 43 Training Loss: 3.087002992630005 \n",
      "     Training Step: 44 Training Loss: 2.2946383953094482 \n",
      "     Training Step: 45 Training Loss: 3.6076064109802246 \n",
      "     Training Step: 46 Training Loss: 3.094944715499878 \n",
      "     Training Step: 47 Training Loss: 2.084029197692871 \n",
      "     Training Step: 48 Training Loss: 3.957095146179199 \n",
      "     Training Step: 49 Training Loss: 2.837617874145508 \n",
      "     Training Step: 50 Training Loss: 2.1626181602478027 \n",
      "     Training Step: 51 Training Loss: 2.6228318214416504 \n",
      "     Training Step: 52 Training Loss: 3.639329433441162 \n",
      "     Training Step: 53 Training Loss: 2.461021661758423 \n",
      "     Training Step: 54 Training Loss: 2.4162817001342773 \n",
      "     Training Step: 55 Training Loss: 2.9307010173797607 \n",
      "     Training Step: 56 Training Loss: 2.982326030731201 \n",
      "     Training Step: 57 Training Loss: 3.4354066848754883 \n",
      "     Training Step: 58 Training Loss: 2.604248046875 \n",
      "     Training Step: 59 Training Loss: 3.8915185928344727 \n",
      "     Training Step: 60 Training Loss: 2.7444143295288086 \n",
      "     Training Step: 61 Training Loss: 4.150150299072266 \n",
      "     Training Step: 62 Training Loss: 3.0524516105651855 \n",
      "     Training Step: 63 Training Loss: 3.0738461017608643 \n",
      "     Training Step: 64 Training Loss: 3.728804349899292 \n",
      "     Training Step: 65 Training Loss: 4.301446437835693 \n",
      "     Training Step: 66 Training Loss: 2.3892955780029297 \n",
      "     Training Step: 67 Training Loss: 4.970714569091797 \n",
      "     Training Step: 68 Training Loss: 2.3421578407287598 \n",
      "     Training Step: 69 Training Loss: 3.1208295822143555 \n",
      "     Training Step: 70 Training Loss: 2.8445889949798584 \n",
      "     Training Step: 71 Training Loss: 2.710920572280884 \n",
      "     Training Step: 72 Training Loss: 2.30183744430542 \n",
      "     Training Step: 73 Training Loss: 2.4997925758361816 \n",
      "     Training Step: 74 Training Loss: 2.20389986038208 \n",
      "     Training Step: 75 Training Loss: 4.370721340179443 \n",
      "     Training Step: 76 Training Loss: 2.613894462585449 \n",
      "     Training Step: 77 Training Loss: 2.4739809036254883 \n",
      "     Training Step: 78 Training Loss: 3.8901612758636475 \n",
      "     Training Step: 79 Training Loss: 3.2167181968688965 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.3656461238861084 \n",
      "     Validation Step: 1 Validation Loss: 3.8279683589935303 \n",
      "     Validation Step: 2 Validation Loss: 3.5350427627563477 \n",
      "     Validation Step: 3 Validation Loss: 3.570497512817383 \n",
      "     Validation Step: 4 Validation Loss: 3.2054190635681152 \n",
      "     Validation Step: 5 Validation Loss: 3.0311803817749023 \n",
      "     Validation Step: 6 Validation Loss: 3.067458391189575 \n",
      "     Validation Step: 7 Validation Loss: 3.0877323150634766 \n",
      "     Validation Step: 8 Validation Loss: 2.734236240386963 \n",
      "     Validation Step: 9 Validation Loss: 3.636996269226074 \n",
      "     Validation Step: 10 Validation Loss: 3.7235381603240967 \n",
      "     Validation Step: 11 Validation Loss: 2.725142478942871 \n",
      "     Validation Step: 12 Validation Loss: 3.648669719696045 \n",
      "     Validation Step: 13 Validation Loss: 2.8229589462280273 \n",
      "     Validation Step: 14 Validation Loss: 2.9804320335388184 \n",
      "Epoch: 90\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.4094128608703613 \n",
      "     Training Step: 1 Training Loss: 3.048114776611328 \n",
      "     Training Step: 2 Training Loss: 2.137676477432251 \n",
      "     Training Step: 3 Training Loss: 2.775991678237915 \n",
      "     Training Step: 4 Training Loss: 2.2191545963287354 \n",
      "     Training Step: 5 Training Loss: 3.197131872177124 \n",
      "     Training Step: 6 Training Loss: 2.8422505855560303 \n",
      "     Training Step: 7 Training Loss: 2.3635597229003906 \n",
      "     Training Step: 8 Training Loss: 3.0276365280151367 \n",
      "     Training Step: 9 Training Loss: 2.2365708351135254 \n",
      "     Training Step: 10 Training Loss: 4.224048614501953 \n",
      "     Training Step: 11 Training Loss: 3.2518157958984375 \n",
      "     Training Step: 12 Training Loss: 2.1617298126220703 \n",
      "     Training Step: 13 Training Loss: 3.3616716861724854 \n",
      "     Training Step: 14 Training Loss: 3.402005910873413 \n",
      "     Training Step: 15 Training Loss: 2.6649386882781982 \n",
      "     Training Step: 16 Training Loss: 2.4457342624664307 \n",
      "     Training Step: 17 Training Loss: 2.5285842418670654 \n",
      "     Training Step: 18 Training Loss: 2.776412010192871 \n",
      "     Training Step: 19 Training Loss: 3.419966697692871 \n",
      "     Training Step: 20 Training Loss: 4.165882587432861 \n",
      "     Training Step: 21 Training Loss: 2.365170955657959 \n",
      "     Training Step: 22 Training Loss: 4.261285781860352 \n",
      "     Training Step: 23 Training Loss: 2.1185760498046875 \n",
      "     Training Step: 24 Training Loss: 3.342085838317871 \n",
      "     Training Step: 25 Training Loss: 2.168840169906616 \n",
      "     Training Step: 26 Training Loss: 4.664874076843262 \n",
      "     Training Step: 27 Training Loss: 3.133126735687256 \n",
      "     Training Step: 28 Training Loss: 2.880192756652832 \n",
      "     Training Step: 29 Training Loss: 2.8660597801208496 \n",
      "     Training Step: 30 Training Loss: 2.5050578117370605 \n",
      "     Training Step: 31 Training Loss: 2.2820606231689453 \n",
      "     Training Step: 32 Training Loss: 3.5859932899475098 \n",
      "     Training Step: 33 Training Loss: 3.575594425201416 \n",
      "     Training Step: 34 Training Loss: 2.984313488006592 \n",
      "     Training Step: 35 Training Loss: 2.9088973999023438 \n",
      "     Training Step: 36 Training Loss: 2.6866683959960938 \n",
      "     Training Step: 37 Training Loss: 2.994209051132202 \n",
      "     Training Step: 38 Training Loss: 2.844332456588745 \n",
      "     Training Step: 39 Training Loss: 3.6513352394104004 \n",
      "     Training Step: 40 Training Loss: 3.6788296699523926 \n",
      "     Training Step: 41 Training Loss: 3.586456775665283 \n",
      "     Training Step: 42 Training Loss: 3.3184475898742676 \n",
      "     Training Step: 43 Training Loss: 3.035761833190918 \n",
      "     Training Step: 44 Training Loss: 2.777871608734131 \n",
      "     Training Step: 45 Training Loss: 3.0831613540649414 \n",
      "     Training Step: 46 Training Loss: 2.8974204063415527 \n",
      "     Training Step: 47 Training Loss: 4.020031929016113 \n",
      "     Training Step: 48 Training Loss: 2.2223153114318848 \n",
      "     Training Step: 49 Training Loss: 3.4182662963867188 \n",
      "     Training Step: 50 Training Loss: 3.397616147994995 \n",
      "     Training Step: 51 Training Loss: 3.2123894691467285 \n",
      "     Training Step: 52 Training Loss: 3.9827446937561035 \n",
      "     Training Step: 53 Training Loss: 2.9330763816833496 \n",
      "     Training Step: 54 Training Loss: 2.8129820823669434 \n",
      "     Training Step: 55 Training Loss: 3.117760419845581 \n",
      "     Training Step: 56 Training Loss: 2.3970704078674316 \n",
      "     Training Step: 57 Training Loss: 2.1628010272979736 \n",
      "     Training Step: 58 Training Loss: 2.5652554035186768 \n",
      "     Training Step: 59 Training Loss: 3.4399328231811523 \n",
      "     Training Step: 60 Training Loss: 2.856962203979492 \n",
      "     Training Step: 61 Training Loss: 2.69626522064209 \n",
      "     Training Step: 62 Training Loss: 3.5806102752685547 \n",
      "     Training Step: 63 Training Loss: 3.666116952896118 \n",
      "     Training Step: 64 Training Loss: 2.7009310722351074 \n",
      "     Training Step: 65 Training Loss: 2.6141676902770996 \n",
      "     Training Step: 66 Training Loss: 2.8449771404266357 \n",
      "     Training Step: 67 Training Loss: 2.692082166671753 \n",
      "     Training Step: 68 Training Loss: 3.338902711868286 \n",
      "     Training Step: 69 Training Loss: 2.452305793762207 \n",
      "     Training Step: 70 Training Loss: 2.737590789794922 \n",
      "     Training Step: 71 Training Loss: 2.5259454250335693 \n",
      "     Training Step: 72 Training Loss: 3.2756340503692627 \n",
      "     Training Step: 73 Training Loss: 2.9825754165649414 \n",
      "     Training Step: 74 Training Loss: 2.6073455810546875 \n",
      "     Training Step: 75 Training Loss: 3.455657958984375 \n",
      "     Training Step: 76 Training Loss: 3.249573230743408 \n",
      "     Training Step: 77 Training Loss: 2.7145633697509766 \n",
      "     Training Step: 78 Training Loss: 3.8696792125701904 \n",
      "     Training Step: 79 Training Loss: 2.660336494445801 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1312599182128906 \n",
      "     Validation Step: 1 Validation Loss: 2.910292625427246 \n",
      "     Validation Step: 2 Validation Loss: 3.200751304626465 \n",
      "     Validation Step: 3 Validation Loss: 3.698988914489746 \n",
      "     Validation Step: 4 Validation Loss: 3.6525702476501465 \n",
      "     Validation Step: 5 Validation Loss: 3.613636016845703 \n",
      "     Validation Step: 6 Validation Loss: 2.7382125854492188 \n",
      "     Validation Step: 7 Validation Loss: 3.3476197719573975 \n",
      "     Validation Step: 8 Validation Loss: 3.201974868774414 \n",
      "     Validation Step: 9 Validation Loss: 3.9522387981414795 \n",
      "     Validation Step: 10 Validation Loss: 2.908823013305664 \n",
      "     Validation Step: 11 Validation Loss: 3.649005174636841 \n",
      "     Validation Step: 12 Validation Loss: 3.0539817810058594 \n",
      "     Validation Step: 13 Validation Loss: 2.235006809234619 \n",
      "     Validation Step: 14 Validation Loss: 3.140752077102661 \n",
      "Epoch: 91\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.029672145843506 \n",
      "     Training Step: 1 Training Loss: 3.2876136302948 \n",
      "     Training Step: 2 Training Loss: 3.7960824966430664 \n",
      "     Training Step: 3 Training Loss: 2.4352777004241943 \n",
      "     Training Step: 4 Training Loss: 3.1337027549743652 \n",
      "     Training Step: 5 Training Loss: 3.245396375656128 \n",
      "     Training Step: 6 Training Loss: 2.377049207687378 \n",
      "     Training Step: 7 Training Loss: 2.7101686000823975 \n",
      "     Training Step: 8 Training Loss: 2.738079071044922 \n",
      "     Training Step: 9 Training Loss: 2.7537598609924316 \n",
      "     Training Step: 10 Training Loss: 2.070563316345215 \n",
      "     Training Step: 11 Training Loss: 2.1815764904022217 \n",
      "     Training Step: 12 Training Loss: 3.5093390941619873 \n",
      "     Training Step: 13 Training Loss: 3.07993221282959 \n",
      "     Training Step: 14 Training Loss: 3.0622448921203613 \n",
      "     Training Step: 15 Training Loss: 3.5635266304016113 \n",
      "     Training Step: 16 Training Loss: 3.5513885021209717 \n",
      "     Training Step: 17 Training Loss: 3.462193489074707 \n",
      "     Training Step: 18 Training Loss: 3.8501667976379395 \n",
      "     Training Step: 19 Training Loss: 2.8139073848724365 \n",
      "     Training Step: 20 Training Loss: 3.805854320526123 \n",
      "     Training Step: 21 Training Loss: 4.01821231842041 \n",
      "     Training Step: 22 Training Loss: 3.0212039947509766 \n",
      "     Training Step: 23 Training Loss: 3.470486640930176 \n",
      "     Training Step: 24 Training Loss: 2.297631025314331 \n",
      "     Training Step: 25 Training Loss: 3.3223819732666016 \n",
      "     Training Step: 26 Training Loss: 3.2508084774017334 \n",
      "     Training Step: 27 Training Loss: 2.197683811187744 \n",
      "     Training Step: 28 Training Loss: 2.5467026233673096 \n",
      "     Training Step: 29 Training Loss: 3.2455873489379883 \n",
      "     Training Step: 30 Training Loss: 3.101336717605591 \n",
      "     Training Step: 31 Training Loss: 3.7264842987060547 \n",
      "     Training Step: 32 Training Loss: 2.6748363971710205 \n",
      "     Training Step: 33 Training Loss: 2.9894895553588867 \n",
      "     Training Step: 34 Training Loss: 2.205472469329834 \n",
      "     Training Step: 35 Training Loss: 4.200348854064941 \n",
      "     Training Step: 36 Training Loss: 2.4944310188293457 \n",
      "     Training Step: 37 Training Loss: 2.734151601791382 \n",
      "     Training Step: 38 Training Loss: 3.216517448425293 \n",
      "     Training Step: 39 Training Loss: 2.5204596519470215 \n",
      "     Training Step: 40 Training Loss: 3.1186957359313965 \n",
      "     Training Step: 41 Training Loss: 3.953965187072754 \n",
      "     Training Step: 42 Training Loss: 2.342527151107788 \n",
      "     Training Step: 43 Training Loss: 2.475182056427002 \n",
      "     Training Step: 44 Training Loss: 2.5645642280578613 \n",
      "     Training Step: 45 Training Loss: 2.6347603797912598 \n",
      "     Training Step: 46 Training Loss: 3.364511013031006 \n",
      "     Training Step: 47 Training Loss: 3.3551902770996094 \n",
      "     Training Step: 48 Training Loss: 3.19913911819458 \n",
      "     Training Step: 49 Training Loss: 2.5226945877075195 \n",
      "     Training Step: 50 Training Loss: 3.4496641159057617 \n",
      "     Training Step: 51 Training Loss: 2.2533090114593506 \n",
      "     Training Step: 52 Training Loss: 2.3915600776672363 \n",
      "     Training Step: 53 Training Loss: 2.857853889465332 \n",
      "     Training Step: 54 Training Loss: 2.515749931335449 \n",
      "     Training Step: 55 Training Loss: 3.073017120361328 \n",
      "     Training Step: 56 Training Loss: 2.8472301959991455 \n",
      "     Training Step: 57 Training Loss: 2.9923501014709473 \n",
      "     Training Step: 58 Training Loss: 3.4097211360931396 \n",
      "     Training Step: 59 Training Loss: 2.4780004024505615 \n",
      "     Training Step: 60 Training Loss: 3.204059362411499 \n",
      "     Training Step: 61 Training Loss: 2.2219786643981934 \n",
      "     Training Step: 62 Training Loss: 2.7445623874664307 \n",
      "     Training Step: 63 Training Loss: 2.7366490364074707 \n",
      "     Training Step: 64 Training Loss: 3.1149532794952393 \n",
      "     Training Step: 65 Training Loss: 4.407847881317139 \n",
      "     Training Step: 66 Training Loss: 3.253920078277588 \n",
      "     Training Step: 67 Training Loss: 2.9683828353881836 \n",
      "     Training Step: 68 Training Loss: 2.7156312465667725 \n",
      "     Training Step: 69 Training Loss: 4.861326217651367 \n",
      "     Training Step: 70 Training Loss: 2.391031503677368 \n",
      "     Training Step: 71 Training Loss: 2.294001579284668 \n",
      "     Training Step: 72 Training Loss: 3.4922871589660645 \n",
      "     Training Step: 73 Training Loss: 3.081552028656006 \n",
      "     Training Step: 74 Training Loss: 2.644550323486328 \n",
      "     Training Step: 75 Training Loss: 2.4502274990081787 \n",
      "     Training Step: 76 Training Loss: 2.5879578590393066 \n",
      "     Training Step: 77 Training Loss: 3.1472907066345215 \n",
      "     Training Step: 78 Training Loss: 3.388946533203125 \n",
      "     Training Step: 79 Training Loss: 2.050861358642578 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1239171028137207 \n",
      "     Validation Step: 1 Validation Loss: 3.730982780456543 \n",
      "     Validation Step: 2 Validation Loss: 2.1557955741882324 \n",
      "     Validation Step: 3 Validation Loss: 3.0776944160461426 \n",
      "     Validation Step: 4 Validation Loss: 3.6106858253479004 \n",
      "     Validation Step: 5 Validation Loss: 2.7023298740386963 \n",
      "     Validation Step: 6 Validation Loss: 3.895204782485962 \n",
      "     Validation Step: 7 Validation Loss: 3.1668126583099365 \n",
      "     Validation Step: 8 Validation Loss: 3.1383001804351807 \n",
      "     Validation Step: 9 Validation Loss: 2.969607353210449 \n",
      "     Validation Step: 10 Validation Loss: 3.2103636264801025 \n",
      "     Validation Step: 11 Validation Loss: 3.5771279335021973 \n",
      "     Validation Step: 12 Validation Loss: 2.763946771621704 \n",
      "     Validation Step: 13 Validation Loss: 3.36226749420166 \n",
      "     Validation Step: 14 Validation Loss: 3.627530574798584 \n",
      "Epoch: 92\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.144289255142212 \n",
      "     Training Step: 1 Training Loss: 3.51198673248291 \n",
      "     Training Step: 2 Training Loss: 2.522665023803711 \n",
      "     Training Step: 3 Training Loss: 2.771652936935425 \n",
      "     Training Step: 4 Training Loss: 2.151346206665039 \n",
      "     Training Step: 5 Training Loss: 2.820138454437256 \n",
      "     Training Step: 6 Training Loss: 2.7578189373016357 \n",
      "     Training Step: 7 Training Loss: 2.7575597763061523 \n",
      "     Training Step: 8 Training Loss: 4.801661491394043 \n",
      "     Training Step: 9 Training Loss: 2.7499523162841797 \n",
      "     Training Step: 10 Training Loss: 2.295717716217041 \n",
      "     Training Step: 11 Training Loss: 2.4068727493286133 \n",
      "     Training Step: 12 Training Loss: 2.478588581085205 \n",
      "     Training Step: 13 Training Loss: 3.0789248943328857 \n",
      "     Training Step: 14 Training Loss: 2.97831392288208 \n",
      "     Training Step: 15 Training Loss: 2.8641278743743896 \n",
      "     Training Step: 16 Training Loss: 2.543696403503418 \n",
      "     Training Step: 17 Training Loss: 4.2708258628845215 \n",
      "     Training Step: 18 Training Loss: 3.6369104385375977 \n",
      "     Training Step: 19 Training Loss: 2.6657519340515137 \n",
      "     Training Step: 20 Training Loss: 2.6176087856292725 \n",
      "     Training Step: 21 Training Loss: 3.137373208999634 \n",
      "     Training Step: 22 Training Loss: 3.1721138954162598 \n",
      "     Training Step: 23 Training Loss: 2.784409999847412 \n",
      "     Training Step: 24 Training Loss: 3.0956602096557617 \n",
      "     Training Step: 25 Training Loss: 3.507070541381836 \n",
      "     Training Step: 26 Training Loss: 3.442574977874756 \n",
      "     Training Step: 27 Training Loss: 2.5795862674713135 \n",
      "     Training Step: 28 Training Loss: 3.419154167175293 \n",
      "     Training Step: 29 Training Loss: 2.391674041748047 \n",
      "     Training Step: 30 Training Loss: 2.373014450073242 \n",
      "     Training Step: 31 Training Loss: 3.870110511779785 \n",
      "     Training Step: 32 Training Loss: 2.9875361919403076 \n",
      "     Training Step: 33 Training Loss: 3.8568296432495117 \n",
      "     Training Step: 34 Training Loss: 2.5918331146240234 \n",
      "     Training Step: 35 Training Loss: 3.5127930641174316 \n",
      "     Training Step: 36 Training Loss: 3.2652487754821777 \n",
      "     Training Step: 37 Training Loss: 3.1832923889160156 \n",
      "     Training Step: 38 Training Loss: 2.9729530811309814 \n",
      "     Training Step: 39 Training Loss: 2.0757017135620117 \n",
      "     Training Step: 40 Training Loss: 2.3455281257629395 \n",
      "     Training Step: 41 Training Loss: 2.76314115524292 \n",
      "     Training Step: 42 Training Loss: 3.0752780437469482 \n",
      "     Training Step: 43 Training Loss: 3.5174241065979004 \n",
      "     Training Step: 44 Training Loss: 2.2107551097869873 \n",
      "     Training Step: 45 Training Loss: 3.7393908500671387 \n",
      "     Training Step: 46 Training Loss: 2.9216079711914062 \n",
      "     Training Step: 47 Training Loss: 3.423491954803467 \n",
      "     Training Step: 48 Training Loss: 3.7606401443481445 \n",
      "     Training Step: 49 Training Loss: 2.1467418670654297 \n",
      "     Training Step: 50 Training Loss: 3.2660720348358154 \n",
      "     Training Step: 51 Training Loss: 3.4552149772644043 \n",
      "     Training Step: 52 Training Loss: 3.383545398712158 \n",
      "     Training Step: 53 Training Loss: 2.856365203857422 \n",
      "     Training Step: 54 Training Loss: 2.0971035957336426 \n",
      "     Training Step: 55 Training Loss: 4.255249500274658 \n",
      "     Training Step: 56 Training Loss: 3.6452908515930176 \n",
      "     Training Step: 57 Training Loss: 2.912602424621582 \n",
      "     Training Step: 58 Training Loss: 3.4586377143859863 \n",
      "     Training Step: 59 Training Loss: 3.035710573196411 \n",
      "     Training Step: 60 Training Loss: 3.03918194770813 \n",
      "     Training Step: 61 Training Loss: 2.956879138946533 \n",
      "     Training Step: 62 Training Loss: 2.9084155559539795 \n",
      "     Training Step: 63 Training Loss: 2.5603485107421875 \n",
      "     Training Step: 64 Training Loss: 2.386723518371582 \n",
      "     Training Step: 65 Training Loss: 3.0747532844543457 \n",
      "     Training Step: 66 Training Loss: 3.7679994106292725 \n",
      "     Training Step: 67 Training Loss: 3.1243739128112793 \n",
      "     Training Step: 68 Training Loss: 2.5502262115478516 \n",
      "     Training Step: 69 Training Loss: 2.484179735183716 \n",
      "     Training Step: 70 Training Loss: 3.082000255584717 \n",
      "     Training Step: 71 Training Loss: 4.414720058441162 \n",
      "     Training Step: 72 Training Loss: 3.4877452850341797 \n",
      "     Training Step: 73 Training Loss: 3.921194553375244 \n",
      "     Training Step: 74 Training Loss: 3.0089588165283203 \n",
      "     Training Step: 75 Training Loss: 3.074615478515625 \n",
      "     Training Step: 76 Training Loss: 2.9552552700042725 \n",
      "     Training Step: 77 Training Loss: 3.334071159362793 \n",
      "     Training Step: 78 Training Loss: 2.0345211029052734 \n",
      "     Training Step: 79 Training Loss: 2.5237512588500977 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.2679333686828613 \n",
      "     Validation Step: 1 Validation Loss: 2.8829739093780518 \n",
      "     Validation Step: 2 Validation Loss: 2.9843480587005615 \n",
      "     Validation Step: 3 Validation Loss: 3.3269217014312744 \n",
      "     Validation Step: 4 Validation Loss: 3.7905521392822266 \n",
      "     Validation Step: 5 Validation Loss: 3.7847371101379395 \n",
      "     Validation Step: 6 Validation Loss: 3.117025136947632 \n",
      "     Validation Step: 7 Validation Loss: 3.5869083404541016 \n",
      "     Validation Step: 8 Validation Loss: 2.8452723026275635 \n",
      "     Validation Step: 9 Validation Loss: 3.0112905502319336 \n",
      "     Validation Step: 10 Validation Loss: 3.1604223251342773 \n",
      "     Validation Step: 11 Validation Loss: 3.6508336067199707 \n",
      "     Validation Step: 12 Validation Loss: 2.188167095184326 \n",
      "     Validation Step: 13 Validation Loss: 3.1145284175872803 \n",
      "     Validation Step: 14 Validation Loss: 3.9410974979400635 \n",
      "Epoch: 93\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.0471529960632324 \n",
      "     Training Step: 1 Training Loss: 2.983989715576172 \n",
      "     Training Step: 2 Training Loss: 2.4976611137390137 \n",
      "     Training Step: 3 Training Loss: 2.798600196838379 \n",
      "     Training Step: 4 Training Loss: 2.63261079788208 \n",
      "     Training Step: 5 Training Loss: 3.7388672828674316 \n",
      "     Training Step: 6 Training Loss: 3.4831905364990234 \n",
      "     Training Step: 7 Training Loss: 3.9031267166137695 \n",
      "     Training Step: 8 Training Loss: 3.754131317138672 \n",
      "     Training Step: 9 Training Loss: 2.219414710998535 \n",
      "     Training Step: 10 Training Loss: 2.3000073432922363 \n",
      "     Training Step: 11 Training Loss: 3.392242908477783 \n",
      "     Training Step: 12 Training Loss: 3.468581199645996 \n",
      "     Training Step: 13 Training Loss: 2.7719650268554688 \n",
      "     Training Step: 14 Training Loss: 2.6266613006591797 \n",
      "     Training Step: 15 Training Loss: 2.3738622665405273 \n",
      "     Training Step: 16 Training Loss: 3.091411590576172 \n",
      "     Training Step: 17 Training Loss: 2.7690205574035645 \n",
      "     Training Step: 18 Training Loss: 4.012923240661621 \n",
      "     Training Step: 19 Training Loss: 2.941812515258789 \n",
      "     Training Step: 20 Training Loss: 2.6749765872955322 \n",
      "     Training Step: 21 Training Loss: 2.8623995780944824 \n",
      "     Training Step: 22 Training Loss: 2.5986106395721436 \n",
      "     Training Step: 23 Training Loss: 2.503997802734375 \n",
      "     Training Step: 24 Training Loss: 2.599747657775879 \n",
      "     Training Step: 25 Training Loss: 2.4378957748413086 \n",
      "     Training Step: 26 Training Loss: 2.158644199371338 \n",
      "     Training Step: 27 Training Loss: 3.543285369873047 \n",
      "     Training Step: 28 Training Loss: 4.0711894035339355 \n",
      "     Training Step: 29 Training Loss: 3.3700294494628906 \n",
      "     Training Step: 30 Training Loss: 4.6336259841918945 \n",
      "     Training Step: 31 Training Loss: 3.3354527950286865 \n",
      "     Training Step: 32 Training Loss: 2.8491289615631104 \n",
      "     Training Step: 33 Training Loss: 3.518646240234375 \n",
      "     Training Step: 34 Training Loss: 3.1786370277404785 \n",
      "     Training Step: 35 Training Loss: 3.2034850120544434 \n",
      "     Training Step: 36 Training Loss: 3.330155372619629 \n",
      "     Training Step: 37 Training Loss: 2.236557960510254 \n",
      "     Training Step: 38 Training Loss: 3.6649527549743652 \n",
      "     Training Step: 39 Training Loss: 2.8270535469055176 \n",
      "     Training Step: 40 Training Loss: 2.935959815979004 \n",
      "     Training Step: 41 Training Loss: 2.1568174362182617 \n",
      "     Training Step: 42 Training Loss: 3.53836989402771 \n",
      "     Training Step: 43 Training Loss: 3.117283821105957 \n",
      "     Training Step: 44 Training Loss: 3.2995429039001465 \n",
      "     Training Step: 45 Training Loss: 2.6591944694519043 \n",
      "     Training Step: 46 Training Loss: 4.455589294433594 \n",
      "     Training Step: 47 Training Loss: 2.591536045074463 \n",
      "     Training Step: 48 Training Loss: 2.675752639770508 \n",
      "     Training Step: 49 Training Loss: 2.8767037391662598 \n",
      "     Training Step: 50 Training Loss: 2.9782843589782715 \n",
      "     Training Step: 51 Training Loss: 2.739121913909912 \n",
      "     Training Step: 52 Training Loss: 3.267369031906128 \n",
      "     Training Step: 53 Training Loss: 3.2163238525390625 \n",
      "     Training Step: 54 Training Loss: 2.3039259910583496 \n",
      "     Training Step: 55 Training Loss: 2.3013124465942383 \n",
      "     Training Step: 56 Training Loss: 2.3767664432525635 \n",
      "     Training Step: 57 Training Loss: 3.0753636360168457 \n",
      "     Training Step: 58 Training Loss: 4.376291751861572 \n",
      "     Training Step: 59 Training Loss: 2.3059329986572266 \n",
      "     Training Step: 60 Training Loss: 3.3048148155212402 \n",
      "     Training Step: 61 Training Loss: 2.602348804473877 \n",
      "     Training Step: 62 Training Loss: 2.8361477851867676 \n",
      "     Training Step: 63 Training Loss: 2.3813648223876953 \n",
      "     Training Step: 64 Training Loss: 2.870296001434326 \n",
      "     Training Step: 65 Training Loss: 2.9352962970733643 \n",
      "     Training Step: 66 Training Loss: 3.288163185119629 \n",
      "     Training Step: 67 Training Loss: 2.855195999145508 \n",
      "     Training Step: 68 Training Loss: 2.8288474082946777 \n",
      "     Training Step: 69 Training Loss: 2.258194923400879 \n",
      "     Training Step: 70 Training Loss: 2.8165688514709473 \n",
      "     Training Step: 71 Training Loss: 2.094914436340332 \n",
      "     Training Step: 72 Training Loss: 2.457568407058716 \n",
      "     Training Step: 73 Training Loss: 3.874701976776123 \n",
      "     Training Step: 74 Training Loss: 3.496281623840332 \n",
      "     Training Step: 75 Training Loss: 3.001044511795044 \n",
      "     Training Step: 76 Training Loss: 2.809234142303467 \n",
      "     Training Step: 77 Training Loss: 3.2202353477478027 \n",
      "     Training Step: 78 Training Loss: 3.0168557167053223 \n",
      "     Training Step: 79 Training Loss: 3.1127381324768066 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.077273368835449 \n",
      "     Validation Step: 1 Validation Loss: 3.0265166759490967 \n",
      "     Validation Step: 2 Validation Loss: 3.0707626342773438 \n",
      "     Validation Step: 3 Validation Loss: 3.3024797439575195 \n",
      "     Validation Step: 4 Validation Loss: 4.126578330993652 \n",
      "     Validation Step: 5 Validation Loss: 2.953603744506836 \n",
      "     Validation Step: 6 Validation Loss: 3.4864113330841064 \n",
      "     Validation Step: 7 Validation Loss: 2.2104203701019287 \n",
      "     Validation Step: 8 Validation Loss: 3.8009068965911865 \n",
      "     Validation Step: 9 Validation Loss: 3.213028907775879 \n",
      "     Validation Step: 10 Validation Loss: 3.8550689220428467 \n",
      "     Validation Step: 11 Validation Loss: 3.293156623840332 \n",
      "     Validation Step: 12 Validation Loss: 3.7870893478393555 \n",
      "     Validation Step: 13 Validation Loss: 3.138817310333252 \n",
      "     Validation Step: 14 Validation Loss: 3.0465493202209473 \n",
      "Epoch: 94\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.9670426845550537 \n",
      "     Training Step: 1 Training Loss: 2.7002851963043213 \n",
      "     Training Step: 2 Training Loss: 2.884587526321411 \n",
      "     Training Step: 3 Training Loss: 2.969695568084717 \n",
      "     Training Step: 4 Training Loss: 3.8109147548675537 \n",
      "     Training Step: 5 Training Loss: 3.292884349822998 \n",
      "     Training Step: 6 Training Loss: 2.957505464553833 \n",
      "     Training Step: 7 Training Loss: 2.7427570819854736 \n",
      "     Training Step: 8 Training Loss: 3.714500665664673 \n",
      "     Training Step: 9 Training Loss: 2.713592052459717 \n",
      "     Training Step: 10 Training Loss: 2.569615364074707 \n",
      "     Training Step: 11 Training Loss: 2.6112117767333984 \n",
      "     Training Step: 12 Training Loss: 2.598848581314087 \n",
      "     Training Step: 13 Training Loss: 3.5122106075286865 \n",
      "     Training Step: 14 Training Loss: 3.274158477783203 \n",
      "     Training Step: 15 Training Loss: 3.510654926300049 \n",
      "     Training Step: 16 Training Loss: 2.197030544281006 \n",
      "     Training Step: 17 Training Loss: 2.8900578022003174 \n",
      "     Training Step: 18 Training Loss: 2.5531387329101562 \n",
      "     Training Step: 19 Training Loss: 2.671408176422119 \n",
      "     Training Step: 20 Training Loss: 2.9169771671295166 \n",
      "     Training Step: 21 Training Loss: 4.254810810089111 \n",
      "     Training Step: 22 Training Loss: 3.578528881072998 \n",
      "     Training Step: 23 Training Loss: 3.5517826080322266 \n",
      "     Training Step: 24 Training Loss: 3.4034316539764404 \n",
      "     Training Step: 25 Training Loss: 2.518969774246216 \n",
      "     Training Step: 26 Training Loss: 3.301757574081421 \n",
      "     Training Step: 27 Training Loss: 3.4059624671936035 \n",
      "     Training Step: 28 Training Loss: 2.1404895782470703 \n",
      "     Training Step: 29 Training Loss: 3.3725879192352295 \n",
      "     Training Step: 30 Training Loss: 3.10347843170166 \n",
      "     Training Step: 31 Training Loss: 2.863105297088623 \n",
      "     Training Step: 32 Training Loss: 2.561826467514038 \n",
      "     Training Step: 33 Training Loss: 2.511902332305908 \n",
      "     Training Step: 34 Training Loss: 2.8780243396759033 \n",
      "     Training Step: 35 Training Loss: 2.853890895843506 \n",
      "     Training Step: 36 Training Loss: 2.9024460315704346 \n",
      "     Training Step: 37 Training Loss: 3.3248353004455566 \n",
      "     Training Step: 38 Training Loss: 2.1493310928344727 \n",
      "     Training Step: 39 Training Loss: 2.411881446838379 \n",
      "     Training Step: 40 Training Loss: 2.8805348873138428 \n",
      "     Training Step: 41 Training Loss: 3.3364267349243164 \n",
      "     Training Step: 42 Training Loss: 2.1958730220794678 \n",
      "     Training Step: 43 Training Loss: 2.4041285514831543 \n",
      "     Training Step: 44 Training Loss: 2.3913965225219727 \n",
      "     Training Step: 45 Training Loss: 4.313570976257324 \n",
      "     Training Step: 46 Training Loss: 2.2303309440612793 \n",
      "     Training Step: 47 Training Loss: 2.1565632820129395 \n",
      "     Training Step: 48 Training Loss: 3.3235881328582764 \n",
      "     Training Step: 49 Training Loss: 2.734705924987793 \n",
      "     Training Step: 50 Training Loss: 2.4405150413513184 \n",
      "     Training Step: 51 Training Loss: 3.6935620307922363 \n",
      "     Training Step: 52 Training Loss: 3.1352906227111816 \n",
      "     Training Step: 53 Training Loss: 2.5078606605529785 \n",
      "     Training Step: 54 Training Loss: 2.1333327293395996 \n",
      "     Training Step: 55 Training Loss: 2.227288246154785 \n",
      "     Training Step: 56 Training Loss: 2.7495532035827637 \n",
      "     Training Step: 57 Training Loss: 3.7949774265289307 \n",
      "     Training Step: 58 Training Loss: 3.154949426651001 \n",
      "     Training Step: 59 Training Loss: 4.333450794219971 \n",
      "     Training Step: 60 Training Loss: 4.659541130065918 \n",
      "     Training Step: 61 Training Loss: 3.658644676208496 \n",
      "     Training Step: 62 Training Loss: 2.850323438644409 \n",
      "     Training Step: 63 Training Loss: 3.443662643432617 \n",
      "     Training Step: 64 Training Loss: 2.9152631759643555 \n",
      "     Training Step: 65 Training Loss: 2.350405216217041 \n",
      "     Training Step: 66 Training Loss: 2.4379186630249023 \n",
      "     Training Step: 67 Training Loss: 3.865067481994629 \n",
      "     Training Step: 68 Training Loss: 3.799654960632324 \n",
      "     Training Step: 69 Training Loss: 3.01965069770813 \n",
      "     Training Step: 70 Training Loss: 2.79600191116333 \n",
      "     Training Step: 71 Training Loss: 3.5627098083496094 \n",
      "     Training Step: 72 Training Loss: 2.9297661781311035 \n",
      "     Training Step: 73 Training Loss: 2.900482177734375 \n",
      "     Training Step: 74 Training Loss: 3.0525569915771484 \n",
      "     Training Step: 75 Training Loss: 3.1590216159820557 \n",
      "     Training Step: 76 Training Loss: 3.76373291015625 \n",
      "     Training Step: 77 Training Loss: 2.2688355445861816 \n",
      "     Training Step: 78 Training Loss: 2.96988844871521 \n",
      "     Training Step: 79 Training Loss: 2.9551801681518555 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6528444290161133 \n",
      "     Validation Step: 1 Validation Loss: 3.154562473297119 \n",
      "     Validation Step: 2 Validation Loss: 3.2980234622955322 \n",
      "     Validation Step: 3 Validation Loss: 3.0490777492523193 \n",
      "     Validation Step: 4 Validation Loss: 3.705639123916626 \n",
      "     Validation Step: 5 Validation Loss: 3.0770559310913086 \n",
      "     Validation Step: 6 Validation Loss: 2.2623448371887207 \n",
      "     Validation Step: 7 Validation Loss: 2.868234634399414 \n",
      "     Validation Step: 8 Validation Loss: 3.675847291946411 \n",
      "     Validation Step: 9 Validation Loss: 2.775637626647949 \n",
      "     Validation Step: 10 Validation Loss: 4.001750946044922 \n",
      "     Validation Step: 11 Validation Loss: 3.079986333847046 \n",
      "     Validation Step: 12 Validation Loss: 3.1032891273498535 \n",
      "     Validation Step: 13 Validation Loss: 3.6073131561279297 \n",
      "     Validation Step: 14 Validation Loss: 2.7147130966186523 \n",
      "Epoch: 95\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.8583121299743652 \n",
      "     Training Step: 1 Training Loss: 3.295095205307007 \n",
      "     Training Step: 2 Training Loss: 2.862745761871338 \n",
      "     Training Step: 3 Training Loss: 3.5557150840759277 \n",
      "     Training Step: 4 Training Loss: 2.464712142944336 \n",
      "     Training Step: 5 Training Loss: 2.177156686782837 \n",
      "     Training Step: 6 Training Loss: 4.315672397613525 \n",
      "     Training Step: 7 Training Loss: 3.3713631629943848 \n",
      "     Training Step: 8 Training Loss: 2.8379945755004883 \n",
      "     Training Step: 9 Training Loss: 3.0145883560180664 \n",
      "     Training Step: 10 Training Loss: 3.7620458602905273 \n",
      "     Training Step: 11 Training Loss: 2.5046887397766113 \n",
      "     Training Step: 12 Training Loss: 3.012058973312378 \n",
      "     Training Step: 13 Training Loss: 2.4222989082336426 \n",
      "     Training Step: 14 Training Loss: 2.977379322052002 \n",
      "     Training Step: 15 Training Loss: 2.372685432434082 \n",
      "     Training Step: 16 Training Loss: 2.609159469604492 \n",
      "     Training Step: 17 Training Loss: 3.6116795539855957 \n",
      "     Training Step: 18 Training Loss: 2.8499772548675537 \n",
      "     Training Step: 19 Training Loss: 3.299973726272583 \n",
      "     Training Step: 20 Training Loss: 3.6393661499023438 \n",
      "     Training Step: 21 Training Loss: 3.791935682296753 \n",
      "     Training Step: 22 Training Loss: 2.1753053665161133 \n",
      "     Training Step: 23 Training Loss: 2.6070032119750977 \n",
      "     Training Step: 24 Training Loss: 3.263322114944458 \n",
      "     Training Step: 25 Training Loss: 2.4951846599578857 \n",
      "     Training Step: 26 Training Loss: 3.0768537521362305 \n",
      "     Training Step: 27 Training Loss: 3.182544708251953 \n",
      "     Training Step: 28 Training Loss: 2.1218910217285156 \n",
      "     Training Step: 29 Training Loss: 2.952756643295288 \n",
      "     Training Step: 30 Training Loss: 3.2013494968414307 \n",
      "     Training Step: 31 Training Loss: 2.2699193954467773 \n",
      "     Training Step: 32 Training Loss: 4.03354549407959 \n",
      "     Training Step: 33 Training Loss: 3.279359817504883 \n",
      "     Training Step: 34 Training Loss: 2.605349540710449 \n",
      "     Training Step: 35 Training Loss: 3.933833122253418 \n",
      "     Training Step: 36 Training Loss: 2.554924964904785 \n",
      "     Training Step: 37 Training Loss: 3.597170829772949 \n",
      "     Training Step: 38 Training Loss: 4.542297840118408 \n",
      "     Training Step: 39 Training Loss: 2.385375738143921 \n",
      "     Training Step: 40 Training Loss: 3.1412456035614014 \n",
      "     Training Step: 41 Training Loss: 2.257488250732422 \n",
      "     Training Step: 42 Training Loss: 2.5134968757629395 \n",
      "     Training Step: 43 Training Loss: 3.362363815307617 \n",
      "     Training Step: 44 Training Loss: 3.353365421295166 \n",
      "     Training Step: 45 Training Loss: 2.870673418045044 \n",
      "     Training Step: 46 Training Loss: 2.9096415042877197 \n",
      "     Training Step: 47 Training Loss: 3.3750338554382324 \n",
      "     Training Step: 48 Training Loss: 2.389126777648926 \n",
      "     Training Step: 49 Training Loss: 3.2287817001342773 \n",
      "     Training Step: 50 Training Loss: 2.484628677368164 \n",
      "     Training Step: 51 Training Loss: 3.0429000854492188 \n",
      "     Training Step: 52 Training Loss: 2.546217441558838 \n",
      "     Training Step: 53 Training Loss: 3.392007827758789 \n",
      "     Training Step: 54 Training Loss: 2.397698402404785 \n",
      "     Training Step: 55 Training Loss: 3.2518975734710693 \n",
      "     Training Step: 56 Training Loss: 2.4193530082702637 \n",
      "     Training Step: 57 Training Loss: 3.1101319789886475 \n",
      "     Training Step: 58 Training Loss: 2.635831832885742 \n",
      "     Training Step: 59 Training Loss: 3.7047510147094727 \n",
      "     Training Step: 60 Training Loss: 2.4561104774475098 \n",
      "     Training Step: 61 Training Loss: 4.1928300857543945 \n",
      "     Training Step: 62 Training Loss: 3.446777820587158 \n",
      "     Training Step: 63 Training Loss: 2.701181411743164 \n",
      "     Training Step: 64 Training Loss: 3.0227956771850586 \n",
      "     Training Step: 65 Training Loss: 2.505579948425293 \n",
      "     Training Step: 66 Training Loss: 3.0744972229003906 \n",
      "     Training Step: 67 Training Loss: 2.162123680114746 \n",
      "     Training Step: 68 Training Loss: 3.296544075012207 \n",
      "     Training Step: 69 Training Loss: 2.768488883972168 \n",
      "     Training Step: 70 Training Loss: 2.4275803565979004 \n",
      "     Training Step: 71 Training Loss: 2.835064172744751 \n",
      "     Training Step: 72 Training Loss: 3.3759846687316895 \n",
      "     Training Step: 73 Training Loss: 2.5910439491271973 \n",
      "     Training Step: 74 Training Loss: 2.733579158782959 \n",
      "     Training Step: 75 Training Loss: 2.718569755554199 \n",
      "     Training Step: 76 Training Loss: 2.3161282539367676 \n",
      "     Training Step: 77 Training Loss: 3.5690832138061523 \n",
      "     Training Step: 78 Training Loss: 2.9154417514801025 \n",
      "     Training Step: 79 Training Loss: 3.151583671569824 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.257290363311768 \n",
      "     Validation Step: 1 Validation Loss: 3.7372124195098877 \n",
      "     Validation Step: 2 Validation Loss: 3.2424964904785156 \n",
      "     Validation Step: 3 Validation Loss: 3.282809019088745 \n",
      "     Validation Step: 4 Validation Loss: 2.975864887237549 \n",
      "     Validation Step: 5 Validation Loss: 3.821629524230957 \n",
      "     Validation Step: 6 Validation Loss: 3.208620071411133 \n",
      "     Validation Step: 7 Validation Loss: 2.931774377822876 \n",
      "     Validation Step: 8 Validation Loss: 3.5545334815979004 \n",
      "     Validation Step: 9 Validation Loss: 3.849673271179199 \n",
      "     Validation Step: 10 Validation Loss: 2.924466371536255 \n",
      "     Validation Step: 11 Validation Loss: 2.817079544067383 \n",
      "     Validation Step: 12 Validation Loss: 3.086731433868408 \n",
      "     Validation Step: 13 Validation Loss: 2.1978683471679688 \n",
      "     Validation Step: 14 Validation Loss: 3.27976655960083 \n",
      "Epoch: 96\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.1758575439453125 \n",
      "     Training Step: 1 Training Loss: 4.284124851226807 \n",
      "     Training Step: 2 Training Loss: 2.6659374237060547 \n",
      "     Training Step: 3 Training Loss: 2.3084375858306885 \n",
      "     Training Step: 4 Training Loss: 3.096527576446533 \n",
      "     Training Step: 5 Training Loss: 2.6934657096862793 \n",
      "     Training Step: 6 Training Loss: 2.4071359634399414 \n",
      "     Training Step: 7 Training Loss: 2.6044068336486816 \n",
      "     Training Step: 8 Training Loss: 2.2021291255950928 \n",
      "     Training Step: 9 Training Loss: 3.578951358795166 \n",
      "     Training Step: 10 Training Loss: 2.1252593994140625 \n",
      "     Training Step: 11 Training Loss: 2.7547709941864014 \n",
      "     Training Step: 12 Training Loss: 2.135944366455078 \n",
      "     Training Step: 13 Training Loss: 3.1926052570343018 \n",
      "     Training Step: 14 Training Loss: 3.679718017578125 \n",
      "     Training Step: 15 Training Loss: 3.030738353729248 \n",
      "     Training Step: 16 Training Loss: 3.414719581604004 \n",
      "     Training Step: 17 Training Loss: 2.3570032119750977 \n",
      "     Training Step: 18 Training Loss: 4.139590740203857 \n",
      "     Training Step: 19 Training Loss: 4.075984477996826 \n",
      "     Training Step: 20 Training Loss: 2.4463725090026855 \n",
      "     Training Step: 21 Training Loss: 4.15069055557251 \n",
      "     Training Step: 22 Training Loss: 3.198460817337036 \n",
      "     Training Step: 23 Training Loss: 4.526805400848389 \n",
      "     Training Step: 24 Training Loss: 3.0606436729431152 \n",
      "     Training Step: 25 Training Loss: 3.30251407623291 \n",
      "     Training Step: 26 Training Loss: 2.9501514434814453 \n",
      "     Training Step: 27 Training Loss: 3.453403949737549 \n",
      "     Training Step: 28 Training Loss: 2.9376566410064697 \n",
      "     Training Step: 29 Training Loss: 2.5296339988708496 \n",
      "     Training Step: 30 Training Loss: 2.867239236831665 \n",
      "     Training Step: 31 Training Loss: 3.2762131690979004 \n",
      "     Training Step: 32 Training Loss: 3.6220033168792725 \n",
      "     Training Step: 33 Training Loss: 2.9234824180603027 \n",
      "     Training Step: 34 Training Loss: 2.952936887741089 \n",
      "     Training Step: 35 Training Loss: 3.33876895904541 \n",
      "     Training Step: 36 Training Loss: 2.399492025375366 \n",
      "     Training Step: 37 Training Loss: 3.0928232669830322 \n",
      "     Training Step: 38 Training Loss: 2.5671205520629883 \n",
      "     Training Step: 39 Training Loss: 2.584242582321167 \n",
      "     Training Step: 40 Training Loss: 3.219764232635498 \n",
      "     Training Step: 41 Training Loss: 2.3149404525756836 \n",
      "     Training Step: 42 Training Loss: 2.4791226387023926 \n",
      "     Training Step: 43 Training Loss: 3.499415397644043 \n",
      "     Training Step: 44 Training Loss: 2.822922706604004 \n",
      "     Training Step: 45 Training Loss: 3.3246803283691406 \n",
      "     Training Step: 46 Training Loss: 3.484299659729004 \n",
      "     Training Step: 47 Training Loss: 3.0602645874023438 \n",
      "     Training Step: 48 Training Loss: 3.5649166107177734 \n",
      "     Training Step: 49 Training Loss: 2.500967264175415 \n",
      "     Training Step: 50 Training Loss: 3.0460660457611084 \n",
      "     Training Step: 51 Training Loss: 2.835379123687744 \n",
      "     Training Step: 52 Training Loss: 2.872720241546631 \n",
      "     Training Step: 53 Training Loss: 2.34902024269104 \n",
      "     Training Step: 54 Training Loss: 3.209944248199463 \n",
      "     Training Step: 55 Training Loss: 3.4260239601135254 \n",
      "     Training Step: 56 Training Loss: 3.202538013458252 \n",
      "     Training Step: 57 Training Loss: 4.595612525939941 \n",
      "     Training Step: 58 Training Loss: 2.737382411956787 \n",
      "     Training Step: 59 Training Loss: 3.121201515197754 \n",
      "     Training Step: 60 Training Loss: 2.3748767375946045 \n",
      "     Training Step: 61 Training Loss: 2.2261834144592285 \n",
      "     Training Step: 62 Training Loss: 2.1634671688079834 \n",
      "     Training Step: 63 Training Loss: 3.514111280441284 \n",
      "     Training Step: 64 Training Loss: 2.892747402191162 \n",
      "     Training Step: 65 Training Loss: 2.9569931030273438 \n",
      "     Training Step: 66 Training Loss: 2.743119955062866 \n",
      "     Training Step: 67 Training Loss: 3.946836471557617 \n",
      "     Training Step: 68 Training Loss: 3.710261106491089 \n",
      "     Training Step: 69 Training Loss: 2.8060967922210693 \n",
      "     Training Step: 70 Training Loss: 3.2899818420410156 \n",
      "     Training Step: 71 Training Loss: 3.5331950187683105 \n",
      "     Training Step: 72 Training Loss: 2.735991954803467 \n",
      "     Training Step: 73 Training Loss: 2.991211414337158 \n",
      "     Training Step: 74 Training Loss: 3.905003309249878 \n",
      "     Training Step: 75 Training Loss: 3.156360626220703 \n",
      "     Training Step: 76 Training Loss: 2.2835655212402344 \n",
      "     Training Step: 77 Training Loss: 3.0408923625946045 \n",
      "     Training Step: 78 Training Loss: 2.094468116760254 \n",
      "     Training Step: 79 Training Loss: 3.3355913162231445 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.629755973815918 \n",
      "     Validation Step: 1 Validation Loss: 3.0910205841064453 \n",
      "     Validation Step: 2 Validation Loss: 2.762714385986328 \n",
      "     Validation Step: 3 Validation Loss: 3.6323137283325195 \n",
      "     Validation Step: 4 Validation Loss: 2.925386428833008 \n",
      "     Validation Step: 5 Validation Loss: 3.521636962890625 \n",
      "     Validation Step: 6 Validation Loss: 3.553925037384033 \n",
      "     Validation Step: 7 Validation Loss: 3.8088316917419434 \n",
      "     Validation Step: 8 Validation Loss: 3.6608424186706543 \n",
      "     Validation Step: 9 Validation Loss: 2.9250786304473877 \n",
      "     Validation Step: 10 Validation Loss: 3.266768455505371 \n",
      "     Validation Step: 11 Validation Loss: 2.671116828918457 \n",
      "     Validation Step: 12 Validation Loss: 2.2161459922790527 \n",
      "     Validation Step: 13 Validation Loss: 3.2083332538604736 \n",
      "     Validation Step: 14 Validation Loss: 3.1086018085479736 \n",
      "Epoch: 97\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.246814727783203 \n",
      "     Training Step: 1 Training Loss: 3.272238254547119 \n",
      "     Training Step: 2 Training Loss: 2.279426097869873 \n",
      "     Training Step: 3 Training Loss: 2.3315892219543457 \n",
      "     Training Step: 4 Training Loss: 2.2442901134490967 \n",
      "     Training Step: 5 Training Loss: 3.149986505508423 \n",
      "     Training Step: 6 Training Loss: 3.669562339782715 \n",
      "     Training Step: 7 Training Loss: 2.352847099304199 \n",
      "     Training Step: 8 Training Loss: 2.9220893383026123 \n",
      "     Training Step: 9 Training Loss: 3.1691155433654785 \n",
      "     Training Step: 10 Training Loss: 2.5120580196380615 \n",
      "     Training Step: 11 Training Loss: 2.969611644744873 \n",
      "     Training Step: 12 Training Loss: 2.253098964691162 \n",
      "     Training Step: 13 Training Loss: 3.134154796600342 \n",
      "     Training Step: 14 Training Loss: 3.185368061065674 \n",
      "     Training Step: 15 Training Loss: 2.365828275680542 \n",
      "     Training Step: 16 Training Loss: 3.4464943408966064 \n",
      "     Training Step: 17 Training Loss: 3.1904854774475098 \n",
      "     Training Step: 18 Training Loss: 2.4177446365356445 \n",
      "     Training Step: 19 Training Loss: 3.1759872436523438 \n",
      "     Training Step: 20 Training Loss: 2.0460774898529053 \n",
      "     Training Step: 21 Training Loss: 4.080564975738525 \n",
      "     Training Step: 22 Training Loss: 2.9302821159362793 \n",
      "     Training Step: 23 Training Loss: 3.3286757469177246 \n",
      "     Training Step: 24 Training Loss: 4.2854509353637695 \n",
      "     Training Step: 25 Training Loss: 2.4850125312805176 \n",
      "     Training Step: 26 Training Loss: 4.031179428100586 \n",
      "     Training Step: 27 Training Loss: 2.484126329421997 \n",
      "     Training Step: 28 Training Loss: 3.1442651748657227 \n",
      "     Training Step: 29 Training Loss: 3.930412530899048 \n",
      "     Training Step: 30 Training Loss: 2.5568180084228516 \n",
      "     Training Step: 31 Training Loss: 2.172818660736084 \n",
      "     Training Step: 32 Training Loss: 2.973607063293457 \n",
      "     Training Step: 33 Training Loss: 2.8013994693756104 \n",
      "     Training Step: 34 Training Loss: 2.985851287841797 \n",
      "     Training Step: 35 Training Loss: 2.5748181343078613 \n",
      "     Training Step: 36 Training Loss: 2.8826608657836914 \n",
      "     Training Step: 37 Training Loss: 3.0155258178710938 \n",
      "     Training Step: 38 Training Loss: 2.9275832176208496 \n",
      "     Training Step: 39 Training Loss: 3.397418737411499 \n",
      "     Training Step: 40 Training Loss: 4.59384298324585 \n",
      "     Training Step: 41 Training Loss: 2.7990071773529053 \n",
      "     Training Step: 42 Training Loss: 2.6060588359832764 \n",
      "     Training Step: 43 Training Loss: 3.3015689849853516 \n",
      "     Training Step: 44 Training Loss: 2.562263250350952 \n",
      "     Training Step: 45 Training Loss: 2.3964319229125977 \n",
      "     Training Step: 46 Training Loss: 2.7565078735351562 \n",
      "     Training Step: 47 Training Loss: 2.721717357635498 \n",
      "     Training Step: 48 Training Loss: 3.4330811500549316 \n",
      "     Training Step: 49 Training Loss: 3.721797227859497 \n",
      "     Training Step: 50 Training Loss: 2.7389798164367676 \n",
      "     Training Step: 51 Training Loss: 2.178673028945923 \n",
      "     Training Step: 52 Training Loss: 3.309818744659424 \n",
      "     Training Step: 53 Training Loss: 3.249119997024536 \n",
      "     Training Step: 54 Training Loss: 3.312403678894043 \n",
      "     Training Step: 55 Training Loss: 3.12839412689209 \n",
      "     Training Step: 56 Training Loss: 2.519084930419922 \n",
      "     Training Step: 57 Training Loss: 3.585444450378418 \n",
      "     Training Step: 58 Training Loss: 2.7213943004608154 \n",
      "     Training Step: 59 Training Loss: 3.7406017780303955 \n",
      "     Training Step: 60 Training Loss: 2.793750762939453 \n",
      "     Training Step: 61 Training Loss: 2.390522003173828 \n",
      "     Training Step: 62 Training Loss: 3.254948377609253 \n",
      "     Training Step: 63 Training Loss: 3.0974299907684326 \n",
      "     Training Step: 64 Training Loss: 3.4352593421936035 \n",
      "     Training Step: 65 Training Loss: 2.6905336380004883 \n",
      "     Training Step: 66 Training Loss: 3.144760847091675 \n",
      "     Training Step: 67 Training Loss: 4.368508815765381 \n",
      "     Training Step: 68 Training Loss: 3.0772337913513184 \n",
      "     Training Step: 69 Training Loss: 3.524649143218994 \n",
      "     Training Step: 70 Training Loss: 3.1486244201660156 \n",
      "     Training Step: 71 Training Loss: 2.5672764778137207 \n",
      "     Training Step: 72 Training Loss: 3.635768413543701 \n",
      "     Training Step: 73 Training Loss: 2.6763389110565186 \n",
      "     Training Step: 74 Training Loss: 3.046825885772705 \n",
      "     Training Step: 75 Training Loss: 3.4799134731292725 \n",
      "     Training Step: 76 Training Loss: 2.784715175628662 \n",
      "     Training Step: 77 Training Loss: 2.073082447052002 \n",
      "     Training Step: 78 Training Loss: 2.4835476875305176 \n",
      "     Training Step: 79 Training Loss: 2.2348101139068604 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.5835700035095215 \n",
      "     Validation Step: 1 Validation Loss: 2.942688465118408 \n",
      "     Validation Step: 2 Validation Loss: 3.0685393810272217 \n",
      "     Validation Step: 3 Validation Loss: 3.654419183731079 \n",
      "     Validation Step: 4 Validation Loss: 2.8900294303894043 \n",
      "     Validation Step: 5 Validation Loss: 2.6563587188720703 \n",
      "     Validation Step: 6 Validation Loss: 2.893819570541382 \n",
      "     Validation Step: 7 Validation Loss: 3.128994941711426 \n",
      "     Validation Step: 8 Validation Loss: 3.3399057388305664 \n",
      "     Validation Step: 9 Validation Loss: 3.5879364013671875 \n",
      "     Validation Step: 10 Validation Loss: 2.354341983795166 \n",
      "     Validation Step: 11 Validation Loss: 3.2836475372314453 \n",
      "     Validation Step: 12 Validation Loss: 3.4041690826416016 \n",
      "     Validation Step: 13 Validation Loss: 3.621255397796631 \n",
      "     Validation Step: 14 Validation Loss: 2.5139718055725098 \n",
      "---------------   LEARNING RATE HALVING DOWN TO: 1.6000000000000004e-06   ---------------\n",
      "---------------   CURRENT LEARNING RATE: 1.6000000000000004e-06   ---------------\n",
      "Epoch: 98\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.4236338138580322 \n",
      "     Training Step: 1 Training Loss: 3.143789291381836 \n",
      "     Training Step: 2 Training Loss: 3.2711963653564453 \n",
      "     Training Step: 3 Training Loss: 2.865234851837158 \n",
      "     Training Step: 4 Training Loss: 2.8679311275482178 \n",
      "     Training Step: 5 Training Loss: 3.8365113735198975 \n",
      "     Training Step: 6 Training Loss: 2.526322841644287 \n",
      "     Training Step: 7 Training Loss: 4.090980529785156 \n",
      "     Training Step: 8 Training Loss: 2.4858005046844482 \n",
      "     Training Step: 9 Training Loss: 2.515143871307373 \n",
      "     Training Step: 10 Training Loss: 4.2565178871154785 \n",
      "     Training Step: 11 Training Loss: 3.291633129119873 \n",
      "     Training Step: 12 Training Loss: 3.1797170639038086 \n",
      "     Training Step: 13 Training Loss: 3.6083452701568604 \n",
      "     Training Step: 14 Training Loss: 2.137549638748169 \n",
      "     Training Step: 15 Training Loss: 2.765690803527832 \n",
      "     Training Step: 16 Training Loss: 2.3060436248779297 \n",
      "     Training Step: 17 Training Loss: 3.195580005645752 \n",
      "     Training Step: 18 Training Loss: 3.8886706829071045 \n",
      "     Training Step: 19 Training Loss: 2.854132890701294 \n",
      "     Training Step: 20 Training Loss: 2.619267463684082 \n",
      "     Training Step: 21 Training Loss: 3.2590270042419434 \n",
      "     Training Step: 22 Training Loss: 3.7726383209228516 \n",
      "     Training Step: 23 Training Loss: 3.7494616508483887 \n",
      "     Training Step: 24 Training Loss: 2.5857319831848145 \n",
      "     Training Step: 25 Training Loss: 2.6635653972625732 \n",
      "     Training Step: 26 Training Loss: 2.768526792526245 \n",
      "     Training Step: 27 Training Loss: 3.5535919666290283 \n",
      "     Training Step: 28 Training Loss: 2.425311326980591 \n",
      "     Training Step: 29 Training Loss: 3.7307281494140625 \n",
      "     Training Step: 30 Training Loss: 3.315551280975342 \n",
      "     Training Step: 31 Training Loss: 3.739380359649658 \n",
      "     Training Step: 32 Training Loss: 3.48099422454834 \n",
      "     Training Step: 33 Training Loss: 3.389674425125122 \n",
      "     Training Step: 34 Training Loss: 3.250063180923462 \n",
      "     Training Step: 35 Training Loss: 3.0135576725006104 \n",
      "     Training Step: 36 Training Loss: 2.794208526611328 \n",
      "     Training Step: 37 Training Loss: 3.0352742671966553 \n",
      "     Training Step: 38 Training Loss: 2.8439931869506836 \n",
      "     Training Step: 39 Training Loss: 4.754212379455566 \n",
      "     Training Step: 40 Training Loss: 2.970381736755371 \n",
      "     Training Step: 41 Training Loss: 3.3939337730407715 \n",
      "     Training Step: 42 Training Loss: 3.043278217315674 \n",
      "     Training Step: 43 Training Loss: 2.589749813079834 \n",
      "     Training Step: 44 Training Loss: 3.233120918273926 \n",
      "     Training Step: 45 Training Loss: 2.5819966793060303 \n",
      "     Training Step: 46 Training Loss: 2.9306414127349854 \n",
      "     Training Step: 47 Training Loss: 3.1567816734313965 \n",
      "     Training Step: 48 Training Loss: 2.552032947540283 \n",
      "     Training Step: 49 Training Loss: 2.9822137355804443 \n",
      "     Training Step: 50 Training Loss: 2.300781488418579 \n",
      "     Training Step: 51 Training Loss: 2.9807422161102295 \n",
      "     Training Step: 52 Training Loss: 2.898364305496216 \n",
      "     Training Step: 53 Training Loss: 3.0361528396606445 \n",
      "     Training Step: 54 Training Loss: 2.1364307403564453 \n",
      "     Training Step: 55 Training Loss: 3.1080803871154785 \n",
      "     Training Step: 56 Training Loss: 2.3438358306884766 \n",
      "     Training Step: 57 Training Loss: 2.6514174938201904 \n",
      "     Training Step: 58 Training Loss: 3.7840828895568848 \n",
      "     Training Step: 59 Training Loss: 2.4267897605895996 \n",
      "     Training Step: 60 Training Loss: 4.292131423950195 \n",
      "     Training Step: 61 Training Loss: 2.384554624557495 \n",
      "     Training Step: 62 Training Loss: 2.2435684204101562 \n",
      "     Training Step: 63 Training Loss: 2.2246885299682617 \n",
      "     Training Step: 64 Training Loss: 2.8096959590911865 \n",
      "     Training Step: 65 Training Loss: 2.7126312255859375 \n",
      "     Training Step: 66 Training Loss: 2.7508060932159424 \n",
      "     Training Step: 67 Training Loss: 3.2378430366516113 \n",
      "     Training Step: 68 Training Loss: 3.159191608428955 \n",
      "     Training Step: 69 Training Loss: 2.686997175216675 \n",
      "     Training Step: 70 Training Loss: 2.3977935314178467 \n",
      "     Training Step: 71 Training Loss: 2.8787424564361572 \n",
      "     Training Step: 72 Training Loss: 2.1733646392822266 \n",
      "     Training Step: 73 Training Loss: 3.550889492034912 \n",
      "     Training Step: 74 Training Loss: 2.269408702850342 \n",
      "     Training Step: 75 Training Loss: 2.028555154800415 \n",
      "     Training Step: 76 Training Loss: 2.482964038848877 \n",
      "     Training Step: 77 Training Loss: 2.5568182468414307 \n",
      "     Training Step: 78 Training Loss: 2.6702656745910645 \n",
      "     Training Step: 79 Training Loss: 2.9927186965942383 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.7072651386260986 \n",
      "     Validation Step: 1 Validation Loss: 2.7829554080963135 \n",
      "     Validation Step: 2 Validation Loss: 3.031236410140991 \n",
      "     Validation Step: 3 Validation Loss: 3.136042594909668 \n",
      "     Validation Step: 4 Validation Loss: 3.332934617996216 \n",
      "     Validation Step: 5 Validation Loss: 3.149477005004883 \n",
      "     Validation Step: 6 Validation Loss: 3.9500021934509277 \n",
      "     Validation Step: 7 Validation Loss: 3.1915624141693115 \n",
      "     Validation Step: 8 Validation Loss: 2.754042863845825 \n",
      "     Validation Step: 9 Validation Loss: 2.194363594055176 \n",
      "     Validation Step: 10 Validation Loss: 3.6274030208587646 \n",
      "     Validation Step: 11 Validation Loss: 3.5822763442993164 \n",
      "     Validation Step: 12 Validation Loss: 2.8847689628601074 \n",
      "     Validation Step: 13 Validation Loss: 3.648491621017456 \n",
      "     Validation Step: 14 Validation Loss: 3.059950351715088 \n",
      "Epoch: 99\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.74811053276062 \n",
      "     Training Step: 1 Training Loss: 3.464482545852661 \n",
      "     Training Step: 2 Training Loss: 2.6394898891448975 \n",
      "     Training Step: 3 Training Loss: 2.4765892028808594 \n",
      "     Training Step: 4 Training Loss: 2.7711377143859863 \n",
      "     Training Step: 5 Training Loss: 2.3915605545043945 \n",
      "     Training Step: 6 Training Loss: 3.5166666507720947 \n",
      "     Training Step: 7 Training Loss: 2.692929983139038 \n",
      "     Training Step: 8 Training Loss: 2.3649678230285645 \n",
      "     Training Step: 9 Training Loss: 2.687767267227173 \n",
      "     Training Step: 10 Training Loss: 2.2096519470214844 \n",
      "     Training Step: 11 Training Loss: 3.8083622455596924 \n",
      "     Training Step: 12 Training Loss: 2.2440192699432373 \n",
      "     Training Step: 13 Training Loss: 3.5663790702819824 \n",
      "     Training Step: 14 Training Loss: 2.4548587799072266 \n",
      "     Training Step: 15 Training Loss: 3.014925956726074 \n",
      "     Training Step: 16 Training Loss: 2.557511806488037 \n",
      "     Training Step: 17 Training Loss: 3.859846591949463 \n",
      "     Training Step: 18 Training Loss: 2.975245237350464 \n",
      "     Training Step: 19 Training Loss: 3.4168589115142822 \n",
      "     Training Step: 20 Training Loss: 3.096932888031006 \n",
      "     Training Step: 21 Training Loss: 3.819176435470581 \n",
      "     Training Step: 22 Training Loss: 2.5503759384155273 \n",
      "     Training Step: 23 Training Loss: 3.082725763320923 \n",
      "     Training Step: 24 Training Loss: 3.0812859535217285 \n",
      "     Training Step: 25 Training Loss: 3.037140130996704 \n",
      "     Training Step: 26 Training Loss: 3.5017402172088623 \n",
      "     Training Step: 27 Training Loss: 2.822924852371216 \n",
      "     Training Step: 28 Training Loss: 3.1222071647644043 \n",
      "     Training Step: 29 Training Loss: 2.9808571338653564 \n",
      "     Training Step: 30 Training Loss: 2.8266427516937256 \n",
      "     Training Step: 31 Training Loss: 3.5660252571105957 \n",
      "     Training Step: 32 Training Loss: 2.6878480911254883 \n",
      "     Training Step: 33 Training Loss: 2.621269941329956 \n",
      "     Training Step: 34 Training Loss: 2.330484390258789 \n",
      "     Training Step: 35 Training Loss: 4.1772780418396 \n",
      "     Training Step: 36 Training Loss: 3.850109577178955 \n",
      "     Training Step: 37 Training Loss: 3.1611084938049316 \n",
      "     Training Step: 38 Training Loss: 2.82487154006958 \n",
      "     Training Step: 39 Training Loss: 4.378697395324707 \n",
      "     Training Step: 40 Training Loss: 4.199602127075195 \n",
      "     Training Step: 41 Training Loss: 2.8836746215820312 \n",
      "     Training Step: 42 Training Loss: 2.1583542823791504 \n",
      "     Training Step: 43 Training Loss: 4.730981826782227 \n",
      "     Training Step: 44 Training Loss: 3.318988800048828 \n",
      "     Training Step: 45 Training Loss: 3.0501627922058105 \n",
      "     Training Step: 46 Training Loss: 3.1153178215026855 \n",
      "     Training Step: 47 Training Loss: 2.9472055435180664 \n",
      "     Training Step: 48 Training Loss: 3.5440025329589844 \n",
      "     Training Step: 49 Training Loss: 2.589965343475342 \n",
      "     Training Step: 50 Training Loss: 3.4472007751464844 \n",
      "     Training Step: 51 Training Loss: 2.555180788040161 \n",
      "     Training Step: 52 Training Loss: 2.227257013320923 \n",
      "     Training Step: 53 Training Loss: 2.159507989883423 \n",
      "     Training Step: 54 Training Loss: 2.742863178253174 \n",
      "     Training Step: 55 Training Loss: 3.0904226303100586 \n",
      "     Training Step: 56 Training Loss: 4.344768524169922 \n",
      "     Training Step: 57 Training Loss: 3.049825668334961 \n",
      "     Training Step: 58 Training Loss: 2.750709056854248 \n",
      "     Training Step: 59 Training Loss: 2.8026392459869385 \n",
      "     Training Step: 60 Training Loss: 2.751774787902832 \n",
      "     Training Step: 61 Training Loss: 2.3469491004943848 \n",
      "     Training Step: 62 Training Loss: 3.125153064727783 \n",
      "     Training Step: 63 Training Loss: 2.4209868907928467 \n",
      "     Training Step: 64 Training Loss: 2.8027830123901367 \n",
      "     Training Step: 65 Training Loss: 2.5554232597351074 \n",
      "     Training Step: 66 Training Loss: 3.2398557662963867 \n",
      "     Training Step: 67 Training Loss: 2.3904590606689453 \n",
      "     Training Step: 68 Training Loss: 3.4842991828918457 \n",
      "     Training Step: 69 Training Loss: 2.0853757858276367 \n",
      "     Training Step: 70 Training Loss: 3.2486419677734375 \n",
      "     Training Step: 71 Training Loss: 3.5110580921173096 \n",
      "     Training Step: 72 Training Loss: 2.618227005004883 \n",
      "     Training Step: 73 Training Loss: 2.0181989669799805 \n",
      "     Training Step: 74 Training Loss: 2.867245674133301 \n",
      "     Training Step: 75 Training Loss: 3.526066303253174 \n",
      "     Training Step: 76 Training Loss: 3.46686053276062 \n",
      "     Training Step: 77 Training Loss: 3.3358659744262695 \n",
      "     Training Step: 78 Training Loss: 3.338503360748291 \n",
      "     Training Step: 79 Training Loss: 2.461160898208618 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.8833868503570557 \n",
      "     Validation Step: 1 Validation Loss: 3.6867835521698 \n",
      "     Validation Step: 2 Validation Loss: 3.15159010887146 \n",
      "     Validation Step: 3 Validation Loss: 2.647331714630127 \n",
      "     Validation Step: 4 Validation Loss: 3.550321578979492 \n",
      "     Validation Step: 5 Validation Loss: 3.5120882987976074 \n",
      "     Validation Step: 6 Validation Loss: 3.020254611968994 \n",
      "     Validation Step: 7 Validation Loss: 3.0539143085479736 \n",
      "     Validation Step: 8 Validation Loss: 2.986816167831421 \n",
      "     Validation Step: 9 Validation Loss: 3.5336341857910156 \n",
      "     Validation Step: 10 Validation Loss: 2.6395719051361084 \n",
      "     Validation Step: 11 Validation Loss: 3.207979679107666 \n",
      "     Validation Step: 12 Validation Loss: 2.247368812561035 \n",
      "     Validation Step: 13 Validation Loss: 3.680840492248535 \n",
      "     Validation Step: 14 Validation Loss: 3.941417932510376 \n",
      "Epoch: 100\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.5835375785827637 \n",
      "     Training Step: 1 Training Loss: 3.4471583366394043 \n",
      "     Training Step: 2 Training Loss: 4.800356864929199 \n",
      "     Training Step: 3 Training Loss: 2.6118311882019043 \n",
      "     Training Step: 4 Training Loss: 2.4562840461730957 \n",
      "     Training Step: 5 Training Loss: 4.344081401824951 \n",
      "     Training Step: 6 Training Loss: 2.9438095092773438 \n",
      "     Training Step: 7 Training Loss: 2.305685520172119 \n",
      "     Training Step: 8 Training Loss: 2.974033832550049 \n",
      "     Training Step: 9 Training Loss: 2.698215961456299 \n",
      "     Training Step: 10 Training Loss: 3.2736847400665283 \n",
      "     Training Step: 11 Training Loss: 2.7612359523773193 \n",
      "     Training Step: 12 Training Loss: 2.732560634613037 \n",
      "     Training Step: 13 Training Loss: 2.8962814807891846 \n",
      "     Training Step: 14 Training Loss: 2.04600191116333 \n",
      "     Training Step: 15 Training Loss: 2.744941473007202 \n",
      "     Training Step: 16 Training Loss: 3.775704860687256 \n",
      "     Training Step: 17 Training Loss: 3.086117744445801 \n",
      "     Training Step: 18 Training Loss: 2.756448984146118 \n",
      "     Training Step: 19 Training Loss: 2.919142007827759 \n",
      "     Training Step: 20 Training Loss: 3.4886584281921387 \n",
      "     Training Step: 21 Training Loss: 3.1544992923736572 \n",
      "     Training Step: 22 Training Loss: 2.9154212474823 \n",
      "     Training Step: 23 Training Loss: 2.8137714862823486 \n",
      "     Training Step: 24 Training Loss: 3.3432974815368652 \n",
      "     Training Step: 25 Training Loss: 2.1451807022094727 \n",
      "     Training Step: 26 Training Loss: 4.025269985198975 \n",
      "     Training Step: 27 Training Loss: 3.0415704250335693 \n",
      "     Training Step: 28 Training Loss: 3.3463568687438965 \n",
      "     Training Step: 29 Training Loss: 2.221501111984253 \n",
      "     Training Step: 30 Training Loss: 3.203817844390869 \n",
      "     Training Step: 31 Training Loss: 2.884207248687744 \n",
      "     Training Step: 32 Training Loss: 3.028228759765625 \n",
      "     Training Step: 33 Training Loss: 2.579409122467041 \n",
      "     Training Step: 34 Training Loss: 3.6385560035705566 \n",
      "     Training Step: 35 Training Loss: 3.2481393814086914 \n",
      "     Training Step: 36 Training Loss: 2.0631613731384277 \n",
      "     Training Step: 37 Training Loss: 2.2290329933166504 \n",
      "     Training Step: 38 Training Loss: 2.1401662826538086 \n",
      "     Training Step: 39 Training Loss: 3.3932137489318848 \n",
      "     Training Step: 40 Training Loss: 4.361369609832764 \n",
      "     Training Step: 41 Training Loss: 2.191401958465576 \n",
      "     Training Step: 42 Training Loss: 2.3766977787017822 \n",
      "     Training Step: 43 Training Loss: 2.5316851139068604 \n",
      "     Training Step: 44 Training Loss: 2.6997766494750977 \n",
      "     Training Step: 45 Training Loss: 2.3866257667541504 \n",
      "     Training Step: 46 Training Loss: 2.251783847808838 \n",
      "     Training Step: 47 Training Loss: 2.995832681655884 \n",
      "     Training Step: 48 Training Loss: 3.476628303527832 \n",
      "     Training Step: 49 Training Loss: 2.2925732135772705 \n",
      "     Training Step: 50 Training Loss: 3.12672758102417 \n",
      "     Training Step: 51 Training Loss: 3.740662097930908 \n",
      "     Training Step: 52 Training Loss: 3.3709635734558105 \n",
      "     Training Step: 53 Training Loss: 3.3591904640197754 \n",
      "     Training Step: 54 Training Loss: 2.7097959518432617 \n",
      "     Training Step: 55 Training Loss: 3.897789478302002 \n",
      "     Training Step: 56 Training Loss: 2.451742172241211 \n",
      "     Training Step: 57 Training Loss: 2.4043540954589844 \n",
      "     Training Step: 58 Training Loss: 3.7503743171691895 \n",
      "     Training Step: 59 Training Loss: 3.273486375808716 \n",
      "     Training Step: 60 Training Loss: 3.3819451332092285 \n",
      "     Training Step: 61 Training Loss: 2.8759663105010986 \n",
      "     Training Step: 62 Training Loss: 3.375065326690674 \n",
      "     Training Step: 63 Training Loss: 3.0590999126434326 \n",
      "     Training Step: 64 Training Loss: 2.711000442504883 \n",
      "     Training Step: 65 Training Loss: 2.7533254623413086 \n",
      "     Training Step: 66 Training Loss: 3.105516195297241 \n",
      "     Training Step: 67 Training Loss: 3.3076038360595703 \n",
      "     Training Step: 68 Training Loss: 2.39894437789917 \n",
      "     Training Step: 69 Training Loss: 2.455385684967041 \n",
      "     Training Step: 70 Training Loss: 2.697624683380127 \n",
      "     Training Step: 71 Training Loss: 4.334470272064209 \n",
      "     Training Step: 72 Training Loss: 2.6160073280334473 \n",
      "     Training Step: 73 Training Loss: 2.3698480129241943 \n",
      "     Training Step: 74 Training Loss: 3.324705123901367 \n",
      "     Training Step: 75 Training Loss: 3.2095189094543457 \n",
      "     Training Step: 76 Training Loss: 3.1453146934509277 \n",
      "     Training Step: 77 Training Loss: 3.06174635887146 \n",
      "     Training Step: 78 Training Loss: 2.726444959640503 \n",
      "     Training Step: 79 Training Loss: 2.2966089248657227 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.108457088470459 \n",
      "     Validation Step: 1 Validation Loss: 2.976858377456665 \n",
      "     Validation Step: 2 Validation Loss: 3.2247462272644043 \n",
      "     Validation Step: 3 Validation Loss: 2.9186785221099854 \n",
      "     Validation Step: 4 Validation Loss: 3.6619794368743896 \n",
      "     Validation Step: 5 Validation Loss: 3.6651554107666016 \n",
      "     Validation Step: 6 Validation Loss: 2.776916980743408 \n",
      "     Validation Step: 7 Validation Loss: 3.1787269115448 \n",
      "     Validation Step: 8 Validation Loss: 2.842000961303711 \n",
      "     Validation Step: 9 Validation Loss: 3.6679060459136963 \n",
      "     Validation Step: 10 Validation Loss: 2.2333202362060547 \n",
      "     Validation Step: 11 Validation Loss: 3.0850014686584473 \n",
      "     Validation Step: 12 Validation Loss: 3.1831634044647217 \n",
      "     Validation Step: 13 Validation Loss: 3.7202131748199463 \n",
      "     Validation Step: 14 Validation Loss: 3.0660383701324463 \n",
      "---------------   CURRENT LEARNING RATE: 1.6000000000000004e-06   ---------------\n",
      "Epoch: 100\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 6.315705299377441 \n",
      "     Training Step: 1 Training Loss: 7.244068145751953 \n",
      "     Training Step: 2 Training Loss: 4.454128265380859 \n",
      "     Training Step: 3 Training Loss: 10.347792625427246 \n",
      "     Training Step: 4 Training Loss: 6.638555526733398 \n",
      "     Training Step: 5 Training Loss: 3.349789619445801 \n",
      "     Training Step: 6 Training Loss: 6.631289958953857 \n",
      "     Training Step: 7 Training Loss: 6.665380477905273 \n",
      "     Training Step: 8 Training Loss: 5.485795497894287 \n",
      "     Training Step: 9 Training Loss: 8.762036323547363 \n",
      "     Training Step: 10 Training Loss: 6.621796131134033 \n",
      "     Training Step: 11 Training Loss: 6.96030330657959 \n",
      "     Training Step: 12 Training Loss: 7.20860481262207 \n",
      "     Training Step: 13 Training Loss: 3.2659196853637695 \n",
      "     Training Step: 14 Training Loss: 4.065299034118652 \n",
      "     Training Step: 15 Training Loss: 5.064201354980469 \n",
      "     Training Step: 16 Training Loss: 5.191128253936768 \n",
      "     Training Step: 17 Training Loss: 8.967781066894531 \n",
      "     Training Step: 18 Training Loss: 3.3221254348754883 \n",
      "     Training Step: 19 Training Loss: 4.9340410232543945 \n",
      "     Training Step: 20 Training Loss: 4.848502159118652 \n",
      "     Training Step: 21 Training Loss: 8.221604347229004 \n",
      "     Training Step: 22 Training Loss: 5.564946174621582 \n",
      "     Training Step: 23 Training Loss: 4.55942440032959 \n",
      "     Training Step: 24 Training Loss: 12.346108436584473 \n",
      "     Training Step: 25 Training Loss: 4.906598091125488 \n",
      "     Training Step: 26 Training Loss: 10.074295997619629 \n",
      "     Training Step: 27 Training Loss: 8.8071870803833 \n",
      "     Training Step: 28 Training Loss: 4.3252458572387695 \n",
      "     Training Step: 29 Training Loss: 3.5359907150268555 \n",
      "     Training Step: 30 Training Loss: 7.300327301025391 \n",
      "     Training Step: 31 Training Loss: 2.978018045425415 \n",
      "     Training Step: 32 Training Loss: 5.47663688659668 \n",
      "     Training Step: 33 Training Loss: 6.132676124572754 \n",
      "     Training Step: 34 Training Loss: 6.874701023101807 \n",
      "     Training Step: 35 Training Loss: 4.960198879241943 \n",
      "     Training Step: 36 Training Loss: 6.147006511688232 \n",
      "     Training Step: 37 Training Loss: 5.99641752243042 \n",
      "     Training Step: 38 Training Loss: 4.395364284515381 \n",
      "     Training Step: 39 Training Loss: 7.417538642883301 \n",
      "     Training Step: 40 Training Loss: 10.200090408325195 \n",
      "     Training Step: 41 Training Loss: 8.067131996154785 \n",
      "     Training Step: 42 Training Loss: 3.6617894172668457 \n",
      "     Training Step: 43 Training Loss: 4.162550926208496 \n",
      "     Training Step: 44 Training Loss: 3.2720437049865723 \n",
      "     Training Step: 45 Training Loss: 6.642420768737793 \n",
      "     Training Step: 46 Training Loss: 8.452583312988281 \n",
      "     Training Step: 47 Training Loss: 6.987141132354736 \n",
      "     Training Step: 48 Training Loss: 4.296846866607666 \n",
      "     Training Step: 49 Training Loss: 6.07908821105957 \n",
      "     Training Step: 50 Training Loss: 10.636251449584961 \n",
      "     Training Step: 51 Training Loss: 5.307145118713379 \n",
      "     Training Step: 52 Training Loss: 3.6311068534851074 \n",
      "     Training Step: 53 Training Loss: 5.267079830169678 \n",
      "     Training Step: 54 Training Loss: 5.431302070617676 \n",
      "     Training Step: 55 Training Loss: 5.9543137550354 \n",
      "     Training Step: 56 Training Loss: 7.426760673522949 \n",
      "     Training Step: 57 Training Loss: 2.9321086406707764 \n",
      "     Training Step: 58 Training Loss: 2.755427360534668 \n",
      "     Training Step: 59 Training Loss: 3.95790433883667 \n",
      "     Training Step: 60 Training Loss: 6.133782863616943 \n",
      "     Training Step: 61 Training Loss: 7.361504554748535 \n",
      "     Training Step: 62 Training Loss: 6.340631484985352 \n",
      "     Training Step: 63 Training Loss: 5.9662699699401855 \n",
      "     Training Step: 64 Training Loss: 6.799368381500244 \n",
      "     Training Step: 65 Training Loss: 4.095131874084473 \n",
      "     Training Step: 66 Training Loss: 7.424806118011475 \n",
      "     Training Step: 67 Training Loss: 5.375712871551514 \n",
      "     Training Step: 68 Training Loss: 5.1369524002075195 \n",
      "     Training Step: 69 Training Loss: 6.192532539367676 \n",
      "     Training Step: 70 Training Loss: 7.568655967712402 \n",
      "     Training Step: 71 Training Loss: 3.5338616371154785 \n",
      "     Training Step: 72 Training Loss: 4.455545425415039 \n",
      "     Training Step: 73 Training Loss: 8.642576217651367 \n",
      "     Training Step: 74 Training Loss: 3.289175033569336 \n",
      "     Training Step: 75 Training Loss: 7.617958068847656 \n",
      "     Training Step: 76 Training Loss: 4.900948524475098 \n",
      "     Training Step: 77 Training Loss: 4.854245185852051 \n",
      "     Training Step: 78 Training Loss: 6.955386161804199 \n",
      "     Training Step: 79 Training Loss: 4.712984561920166 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 9.501187324523926 \n",
      "     Validation Step: 1 Validation Loss: 5.192089080810547 \n",
      "     Validation Step: 2 Validation Loss: 8.034553527832031 \n",
      "     Validation Step: 3 Validation Loss: 8.444432258605957 \n",
      "     Validation Step: 4 Validation Loss: 6.351536750793457 \n",
      "     Validation Step: 5 Validation Loss: 6.719906330108643 \n",
      "     Validation Step: 6 Validation Loss: 7.245396614074707 \n",
      "     Validation Step: 7 Validation Loss: 6.216196537017822 \n",
      "     Validation Step: 8 Validation Loss: 6.304421424865723 \n",
      "     Validation Step: 9 Validation Loss: 3.230677843093872 \n",
      "     Validation Step: 10 Validation Loss: 6.328394889831543 \n",
      "     Validation Step: 11 Validation Loss: 5.379700660705566 \n",
      "     Validation Step: 12 Validation Loss: 5.8099799156188965 \n",
      "     Validation Step: 13 Validation Loss: 8.407064437866211 \n",
      "     Validation Step: 14 Validation Loss: 8.230209350585938 \n",
      "Epoch: 101\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.534174919128418 \n",
      "     Training Step: 1 Training Loss: 3.339362144470215 \n",
      "     Training Step: 2 Training Loss: 2.730725049972534 \n",
      "     Training Step: 3 Training Loss: 2.190870523452759 \n",
      "     Training Step: 4 Training Loss: 3.310800552368164 \n",
      "     Training Step: 5 Training Loss: 3.065293788909912 \n",
      "     Training Step: 6 Training Loss: 2.996281623840332 \n",
      "     Training Step: 7 Training Loss: 3.3730907440185547 \n",
      "     Training Step: 8 Training Loss: 3.1550793647766113 \n",
      "     Training Step: 9 Training Loss: 2.75626802444458 \n",
      "     Training Step: 10 Training Loss: 2.6282081604003906 \n",
      "     Training Step: 11 Training Loss: 3.7943506240844727 \n",
      "     Training Step: 12 Training Loss: 4.2643914222717285 \n",
      "     Training Step: 13 Training Loss: 2.139247417449951 \n",
      "     Training Step: 14 Training Loss: 2.940213918685913 \n",
      "     Training Step: 15 Training Loss: 4.006777286529541 \n",
      "     Training Step: 16 Training Loss: 3.7020339965820312 \n",
      "     Training Step: 17 Training Loss: 3.127699375152588 \n",
      "     Training Step: 18 Training Loss: 3.300632953643799 \n",
      "     Training Step: 19 Training Loss: 2.923668622970581 \n",
      "     Training Step: 20 Training Loss: 2.470276117324829 \n",
      "     Training Step: 21 Training Loss: 3.3048155307769775 \n",
      "     Training Step: 22 Training Loss: 2.3307511806488037 \n",
      "     Training Step: 23 Training Loss: 2.912848472595215 \n",
      "     Training Step: 24 Training Loss: 3.153958797454834 \n",
      "     Training Step: 25 Training Loss: 2.685420036315918 \n",
      "     Training Step: 26 Training Loss: 3.3972012996673584 \n",
      "     Training Step: 27 Training Loss: 3.7390170097351074 \n",
      "     Training Step: 28 Training Loss: 3.2946677207946777 \n",
      "     Training Step: 29 Training Loss: 2.318368434906006 \n",
      "     Training Step: 30 Training Loss: 2.444420337677002 \n",
      "     Training Step: 31 Training Loss: 4.2265520095825195 \n",
      "     Training Step: 32 Training Loss: 2.6179676055908203 \n",
      "     Training Step: 33 Training Loss: 2.1637020111083984 \n",
      "     Training Step: 34 Training Loss: 3.727903366088867 \n",
      "     Training Step: 35 Training Loss: 2.9115748405456543 \n",
      "     Training Step: 36 Training Loss: 3.537853717803955 \n",
      "     Training Step: 37 Training Loss: 2.753843069076538 \n",
      "     Training Step: 38 Training Loss: 2.579254627227783 \n",
      "     Training Step: 39 Training Loss: 2.9746971130371094 \n",
      "     Training Step: 40 Training Loss: 2.471539258956909 \n",
      "     Training Step: 41 Training Loss: 3.6275429725646973 \n",
      "     Training Step: 42 Training Loss: 2.1791162490844727 \n",
      "     Training Step: 43 Training Loss: 2.740347146987915 \n",
      "     Training Step: 44 Training Loss: 3.341982841491699 \n",
      "     Training Step: 45 Training Loss: 2.8703856468200684 \n",
      "     Training Step: 46 Training Loss: 2.5244555473327637 \n",
      "     Training Step: 47 Training Loss: 2.670755386352539 \n",
      "     Training Step: 48 Training Loss: 3.126176118850708 \n",
      "     Training Step: 49 Training Loss: 2.3964037895202637 \n",
      "     Training Step: 50 Training Loss: 2.8114373683929443 \n",
      "     Training Step: 51 Training Loss: 2.6529159545898438 \n",
      "     Training Step: 52 Training Loss: 3.077800750732422 \n",
      "     Training Step: 53 Training Loss: 3.076714038848877 \n",
      "     Training Step: 54 Training Loss: 2.1965322494506836 \n",
      "     Training Step: 55 Training Loss: 3.7502732276916504 \n",
      "     Training Step: 56 Training Loss: 2.8922383785247803 \n",
      "     Training Step: 57 Training Loss: 2.3608968257904053 \n",
      "     Training Step: 58 Training Loss: 3.252342700958252 \n",
      "     Training Step: 59 Training Loss: 2.5100765228271484 \n",
      "     Training Step: 60 Training Loss: 3.207132339477539 \n",
      "     Training Step: 61 Training Loss: 2.1232049465179443 \n",
      "     Training Step: 62 Training Loss: 2.47808837890625 \n",
      "     Training Step: 63 Training Loss: 2.8137013912200928 \n",
      "     Training Step: 64 Training Loss: 3.195737838745117 \n",
      "     Training Step: 65 Training Loss: 2.6408843994140625 \n",
      "     Training Step: 66 Training Loss: 3.10884428024292 \n",
      "     Training Step: 67 Training Loss: 3.5987586975097656 \n",
      "     Training Step: 68 Training Loss: 3.174217700958252 \n",
      "     Training Step: 69 Training Loss: 2.611884355545044 \n",
      "     Training Step: 70 Training Loss: 2.613624095916748 \n",
      "     Training Step: 71 Training Loss: 2.8439886569976807 \n",
      "     Training Step: 72 Training Loss: 3.2982940673828125 \n",
      "     Training Step: 73 Training Loss: 2.520148754119873 \n",
      "     Training Step: 74 Training Loss: 4.813880443572998 \n",
      "     Training Step: 75 Training Loss: 4.412202835083008 \n",
      "     Training Step: 76 Training Loss: 2.324972629547119 \n",
      "     Training Step: 77 Training Loss: 3.3207430839538574 \n",
      "     Training Step: 78 Training Loss: 2.287454128265381 \n",
      "     Training Step: 79 Training Loss: 2.5182623863220215 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9079763889312744 \n",
      "     Validation Step: 1 Validation Loss: 3.043985366821289 \n",
      "     Validation Step: 2 Validation Loss: 3.864452838897705 \n",
      "     Validation Step: 3 Validation Loss: 3.0553600788116455 \n",
      "     Validation Step: 4 Validation Loss: 3.4146182537078857 \n",
      "     Validation Step: 5 Validation Loss: 3.568972587585449 \n",
      "     Validation Step: 6 Validation Loss: 2.6605892181396484 \n",
      "     Validation Step: 7 Validation Loss: 3.6585352420806885 \n",
      "     Validation Step: 8 Validation Loss: 2.2314600944519043 \n",
      "     Validation Step: 9 Validation Loss: 3.093435525894165 \n",
      "     Validation Step: 10 Validation Loss: 2.8367409706115723 \n",
      "     Validation Step: 11 Validation Loss: 3.7165117263793945 \n",
      "     Validation Step: 12 Validation Loss: 3.0662384033203125 \n",
      "     Validation Step: 13 Validation Loss: 3.170640468597412 \n",
      "     Validation Step: 14 Validation Loss: 3.6239943504333496 \n",
      "Epoch: 102\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.410940408706665 \n",
      "     Training Step: 1 Training Loss: 3.532611608505249 \n",
      "     Training Step: 2 Training Loss: 3.311034679412842 \n",
      "     Training Step: 3 Training Loss: 3.047023296356201 \n",
      "     Training Step: 4 Training Loss: 3.065577983856201 \n",
      "     Training Step: 5 Training Loss: 3.336151599884033 \n",
      "     Training Step: 6 Training Loss: 2.8117899894714355 \n",
      "     Training Step: 7 Training Loss: 2.755643129348755 \n",
      "     Training Step: 8 Training Loss: 3.401139497756958 \n",
      "     Training Step: 9 Training Loss: 2.760625123977661 \n",
      "     Training Step: 10 Training Loss: 2.675493001937866 \n",
      "     Training Step: 11 Training Loss: 2.5800113677978516 \n",
      "     Training Step: 12 Training Loss: 3.247086524963379 \n",
      "     Training Step: 13 Training Loss: 2.4009439945220947 \n",
      "     Training Step: 14 Training Loss: 2.4703874588012695 \n",
      "     Training Step: 15 Training Loss: 2.548342227935791 \n",
      "     Training Step: 16 Training Loss: 2.68371844291687 \n",
      "     Training Step: 17 Training Loss: 2.3128602504730225 \n",
      "     Training Step: 18 Training Loss: 3.1249144077301025 \n",
      "     Training Step: 19 Training Loss: 4.88879919052124 \n",
      "     Training Step: 20 Training Loss: 2.279970169067383 \n",
      "     Training Step: 21 Training Loss: 2.11140775680542 \n",
      "     Training Step: 22 Training Loss: 3.869757652282715 \n",
      "     Training Step: 23 Training Loss: 3.8850865364074707 \n",
      "     Training Step: 24 Training Loss: 3.128032684326172 \n",
      "     Training Step: 25 Training Loss: 2.760316848754883 \n",
      "     Training Step: 26 Training Loss: 2.686063528060913 \n",
      "     Training Step: 27 Training Loss: 3.2947463989257812 \n",
      "     Training Step: 28 Training Loss: 2.019613742828369 \n",
      "     Training Step: 29 Training Loss: 3.0977962017059326 \n",
      "     Training Step: 30 Training Loss: 2.797518253326416 \n",
      "     Training Step: 31 Training Loss: 2.8776278495788574 \n",
      "     Training Step: 32 Training Loss: 3.264803409576416 \n",
      "     Training Step: 33 Training Loss: 4.304278373718262 \n",
      "     Training Step: 34 Training Loss: 3.322181224822998 \n",
      "     Training Step: 35 Training Loss: 3.5724940299987793 \n",
      "     Training Step: 36 Training Loss: 2.603191375732422 \n",
      "     Training Step: 37 Training Loss: 3.1316518783569336 \n",
      "     Training Step: 38 Training Loss: 2.8618762493133545 \n",
      "     Training Step: 39 Training Loss: 4.385018825531006 \n",
      "     Training Step: 40 Training Loss: 3.1509640216827393 \n",
      "     Training Step: 41 Training Loss: 3.4047012329101562 \n",
      "     Training Step: 42 Training Loss: 4.123673915863037 \n",
      "     Training Step: 43 Training Loss: 2.2597222328186035 \n",
      "     Training Step: 44 Training Loss: 2.168398380279541 \n",
      "     Training Step: 45 Training Loss: 2.7288177013397217 \n",
      "     Training Step: 46 Training Loss: 2.353954315185547 \n",
      "     Training Step: 47 Training Loss: 3.8118669986724854 \n",
      "     Training Step: 48 Training Loss: 2.590787887573242 \n",
      "     Training Step: 49 Training Loss: 2.9156930446624756 \n",
      "     Training Step: 50 Training Loss: 3.4370412826538086 \n",
      "     Training Step: 51 Training Loss: 3.1468663215637207 \n",
      "     Training Step: 52 Training Loss: 2.4026145935058594 \n",
      "     Training Step: 53 Training Loss: 2.386634349822998 \n",
      "     Training Step: 54 Training Loss: 3.3301401138305664 \n",
      "     Training Step: 55 Training Loss: 2.6038742065429688 \n",
      "     Training Step: 56 Training Loss: 2.6732845306396484 \n",
      "     Training Step: 57 Training Loss: 2.912163257598877 \n",
      "     Training Step: 58 Training Loss: 3.314389228820801 \n",
      "     Training Step: 59 Training Loss: 2.6630613803863525 \n",
      "     Training Step: 60 Training Loss: 4.229770660400391 \n",
      "     Training Step: 61 Training Loss: 3.732722520828247 \n",
      "     Training Step: 62 Training Loss: 4.008252143859863 \n",
      "     Training Step: 63 Training Loss: 3.2810702323913574 \n",
      "     Training Step: 64 Training Loss: 2.9044346809387207 \n",
      "     Training Step: 65 Training Loss: 2.511885643005371 \n",
      "     Training Step: 66 Training Loss: 2.227639675140381 \n",
      "     Training Step: 67 Training Loss: 2.861402988433838 \n",
      "     Training Step: 68 Training Loss: 3.497581958770752 \n",
      "     Training Step: 69 Training Loss: 3.1221320629119873 \n",
      "     Training Step: 70 Training Loss: 3.264446258544922 \n",
      "     Training Step: 71 Training Loss: 2.9446182250976562 \n",
      "     Training Step: 72 Training Loss: 3.601438522338867 \n",
      "     Training Step: 73 Training Loss: 2.5485610961914062 \n",
      "     Training Step: 74 Training Loss: 2.1035518646240234 \n",
      "     Training Step: 75 Training Loss: 2.3459856510162354 \n",
      "     Training Step: 76 Training Loss: 2.505093574523926 \n",
      "     Training Step: 77 Training Loss: 2.9928526878356934 \n",
      "     Training Step: 78 Training Loss: 3.43483304977417 \n",
      "     Training Step: 79 Training Loss: 3.210428237915039 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6165685653686523 \n",
      "     Validation Step: 1 Validation Loss: 3.5164248943328857 \n",
      "     Validation Step: 2 Validation Loss: 3.2221481800079346 \n",
      "     Validation Step: 3 Validation Loss: 2.7527668476104736 \n",
      "     Validation Step: 4 Validation Loss: 3.156486988067627 \n",
      "     Validation Step: 5 Validation Loss: 3.1023974418640137 \n",
      "     Validation Step: 6 Validation Loss: 3.6144471168518066 \n",
      "     Validation Step: 7 Validation Loss: 2.652003288269043 \n",
      "     Validation Step: 8 Validation Loss: 2.8518929481506348 \n",
      "     Validation Step: 9 Validation Loss: 3.0081958770751953 \n",
      "     Validation Step: 10 Validation Loss: 2.3142411708831787 \n",
      "     Validation Step: 11 Validation Loss: 3.893293619155884 \n",
      "     Validation Step: 12 Validation Loss: 2.9595751762390137 \n",
      "     Validation Step: 13 Validation Loss: 3.746692180633545 \n",
      "     Validation Step: 14 Validation Loss: 3.5386345386505127 \n",
      "Epoch: 103\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.648138999938965 \n",
      "     Training Step: 1 Training Loss: 2.9892947673797607 \n",
      "     Training Step: 2 Training Loss: 2.2768237590789795 \n",
      "     Training Step: 3 Training Loss: 2.799866199493408 \n",
      "     Training Step: 4 Training Loss: 2.4189629554748535 \n",
      "     Training Step: 5 Training Loss: 2.8882827758789062 \n",
      "     Training Step: 6 Training Loss: 3.9585225582122803 \n",
      "     Training Step: 7 Training Loss: 4.269007682800293 \n",
      "     Training Step: 8 Training Loss: 2.4144952297210693 \n",
      "     Training Step: 9 Training Loss: 4.6563310623168945 \n",
      "     Training Step: 10 Training Loss: 3.4763317108154297 \n",
      "     Training Step: 11 Training Loss: 3.3468780517578125 \n",
      "     Training Step: 12 Training Loss: 3.3446762561798096 \n",
      "     Training Step: 13 Training Loss: 3.1695046424865723 \n",
      "     Training Step: 14 Training Loss: 3.0171847343444824 \n",
      "     Training Step: 15 Training Loss: 3.352200508117676 \n",
      "     Training Step: 16 Training Loss: 2.998873472213745 \n",
      "     Training Step: 17 Training Loss: 3.386735439300537 \n",
      "     Training Step: 18 Training Loss: 2.8948752880096436 \n",
      "     Training Step: 19 Training Loss: 3.4816155433654785 \n",
      "     Training Step: 20 Training Loss: 2.2031679153442383 \n",
      "     Training Step: 21 Training Loss: 2.8191657066345215 \n",
      "     Training Step: 22 Training Loss: 3.3855624198913574 \n",
      "     Training Step: 23 Training Loss: 2.6728579998016357 \n",
      "     Training Step: 24 Training Loss: 2.7884268760681152 \n",
      "     Training Step: 25 Training Loss: 2.5616390705108643 \n",
      "     Training Step: 26 Training Loss: 2.2602274417877197 \n",
      "     Training Step: 27 Training Loss: 3.323695182800293 \n",
      "     Training Step: 28 Training Loss: 3.188131332397461 \n",
      "     Training Step: 29 Training Loss: 2.478773355484009 \n",
      "     Training Step: 30 Training Loss: 3.789468288421631 \n",
      "     Training Step: 31 Training Loss: 2.4470529556274414 \n",
      "     Training Step: 32 Training Loss: 4.3180952072143555 \n",
      "     Training Step: 33 Training Loss: 2.6905622482299805 \n",
      "     Training Step: 34 Training Loss: 2.7983803749084473 \n",
      "     Training Step: 35 Training Loss: 3.360830783843994 \n",
      "     Training Step: 36 Training Loss: 4.083382606506348 \n",
      "     Training Step: 37 Training Loss: 3.2307586669921875 \n",
      "     Training Step: 38 Training Loss: 2.388859272003174 \n",
      "     Training Step: 39 Training Loss: 2.8826651573181152 \n",
      "     Training Step: 40 Training Loss: 2.946993350982666 \n",
      "     Training Step: 41 Training Loss: 2.1932783126831055 \n",
      "     Training Step: 42 Training Loss: 3.3830959796905518 \n",
      "     Training Step: 43 Training Loss: 2.4144978523254395 \n",
      "     Training Step: 44 Training Loss: 2.3941166400909424 \n",
      "     Training Step: 45 Training Loss: 2.5171072483062744 \n",
      "     Training Step: 46 Training Loss: 3.056046485900879 \n",
      "     Training Step: 47 Training Loss: 3.1376700401306152 \n",
      "     Training Step: 48 Training Loss: 2.766237735748291 \n",
      "     Training Step: 49 Training Loss: 2.592337131500244 \n",
      "     Training Step: 50 Training Loss: 3.278775691986084 \n",
      "     Training Step: 51 Training Loss: 3.86395263671875 \n",
      "     Training Step: 52 Training Loss: 2.16896653175354 \n",
      "     Training Step: 53 Training Loss: 3.320070266723633 \n",
      "     Training Step: 54 Training Loss: 2.5186586380004883 \n",
      "     Training Step: 55 Training Loss: 2.8593835830688477 \n",
      "     Training Step: 56 Training Loss: 3.698887348175049 \n",
      "     Training Step: 57 Training Loss: 2.9580163955688477 \n",
      "     Training Step: 58 Training Loss: 3.2136096954345703 \n",
      "     Training Step: 59 Training Loss: 2.8642892837524414 \n",
      "     Training Step: 60 Training Loss: 2.330922842025757 \n",
      "     Training Step: 61 Training Loss: 3.437296152114868 \n",
      "     Training Step: 62 Training Loss: 2.1598846912384033 \n",
      "     Training Step: 63 Training Loss: 2.2632458209991455 \n",
      "     Training Step: 64 Training Loss: 2.8880136013031006 \n",
      "     Training Step: 65 Training Loss: 2.495447874069214 \n",
      "     Training Step: 66 Training Loss: 2.151644229888916 \n",
      "     Training Step: 67 Training Loss: 2.451550006866455 \n",
      "     Training Step: 68 Training Loss: 2.062305450439453 \n",
      "     Training Step: 69 Training Loss: 2.991849184036255 \n",
      "     Training Step: 70 Training Loss: 3.783809185028076 \n",
      "     Training Step: 71 Training Loss: 3.2997899055480957 \n",
      "     Training Step: 72 Training Loss: 3.0370612144470215 \n",
      "     Training Step: 73 Training Loss: 2.220616340637207 \n",
      "     Training Step: 74 Training Loss: 3.090393543243408 \n",
      "     Training Step: 75 Training Loss: 3.166991710662842 \n",
      "     Training Step: 76 Training Loss: 3.572793483734131 \n",
      "     Training Step: 77 Training Loss: 2.8608930110931396 \n",
      "     Training Step: 78 Training Loss: 3.4956488609313965 \n",
      "     Training Step: 79 Training Loss: 3.1344923973083496 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.193514823913574 \n",
      "     Validation Step: 1 Validation Loss: 3.089625835418701 \n",
      "     Validation Step: 2 Validation Loss: 4.0391621589660645 \n",
      "     Validation Step: 3 Validation Loss: 3.1712398529052734 \n",
      "     Validation Step: 4 Validation Loss: 3.6554532051086426 \n",
      "     Validation Step: 5 Validation Loss: 3.148606061935425 \n",
      "     Validation Step: 6 Validation Loss: 3.6471035480499268 \n",
      "     Validation Step: 7 Validation Loss: 3.608064889907837 \n",
      "     Validation Step: 8 Validation Loss: 2.848747491836548 \n",
      "     Validation Step: 9 Validation Loss: 3.7084648609161377 \n",
      "     Validation Step: 10 Validation Loss: 2.706368923187256 \n",
      "     Validation Step: 11 Validation Loss: 2.272247791290283 \n",
      "     Validation Step: 12 Validation Loss: 3.391892671585083 \n",
      "     Validation Step: 13 Validation Loss: 3.1126391887664795 \n",
      "     Validation Step: 14 Validation Loss: 2.73085880279541 \n",
      "Epoch: 104\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.844653844833374 \n",
      "     Training Step: 1 Training Loss: 3.009634017944336 \n",
      "     Training Step: 2 Training Loss: 2.414318561553955 \n",
      "     Training Step: 3 Training Loss: 3.1627614498138428 \n",
      "     Training Step: 4 Training Loss: 3.34763765335083 \n",
      "     Training Step: 5 Training Loss: 2.1049671173095703 \n",
      "     Training Step: 6 Training Loss: 3.845095634460449 \n",
      "     Training Step: 7 Training Loss: 2.515392303466797 \n",
      "     Training Step: 8 Training Loss: 2.5980277061462402 \n",
      "     Training Step: 9 Training Loss: 3.2495455741882324 \n",
      "     Training Step: 10 Training Loss: 3.368612289428711 \n",
      "     Training Step: 11 Training Loss: 2.777829170227051 \n",
      "     Training Step: 12 Training Loss: 2.704043388366699 \n",
      "     Training Step: 13 Training Loss: 3.4856467247009277 \n",
      "     Training Step: 14 Training Loss: 2.0925285816192627 \n",
      "     Training Step: 15 Training Loss: 2.775172710418701 \n",
      "     Training Step: 16 Training Loss: 3.0750508308410645 \n",
      "     Training Step: 17 Training Loss: 3.0514683723449707 \n",
      "     Training Step: 18 Training Loss: 2.873255729675293 \n",
      "     Training Step: 19 Training Loss: 2.8823435306549072 \n",
      "     Training Step: 20 Training Loss: 2.298971176147461 \n",
      "     Training Step: 21 Training Loss: 3.0446696281433105 \n",
      "     Training Step: 22 Training Loss: 3.2583224773406982 \n",
      "     Training Step: 23 Training Loss: 2.528074026107788 \n",
      "     Training Step: 24 Training Loss: 3.8751895427703857 \n",
      "     Training Step: 25 Training Loss: 3.1331465244293213 \n",
      "     Training Step: 26 Training Loss: 2.909620761871338 \n",
      "     Training Step: 27 Training Loss: 3.203841209411621 \n",
      "     Training Step: 28 Training Loss: 2.6844305992126465 \n",
      "     Training Step: 29 Training Loss: 2.968883514404297 \n",
      "     Training Step: 30 Training Loss: 2.4032535552978516 \n",
      "     Training Step: 31 Training Loss: 3.3355050086975098 \n",
      "     Training Step: 32 Training Loss: 4.416654109954834 \n",
      "     Training Step: 33 Training Loss: 2.5274627208709717 \n",
      "     Training Step: 34 Training Loss: 2.3615946769714355 \n",
      "     Training Step: 35 Training Loss: 2.1793270111083984 \n",
      "     Training Step: 36 Training Loss: 2.805704355239868 \n",
      "     Training Step: 37 Training Loss: 2.7237532138824463 \n",
      "     Training Step: 38 Training Loss: 2.571726083755493 \n",
      "     Training Step: 39 Training Loss: 2.8222599029541016 \n",
      "     Training Step: 40 Training Loss: 2.8553051948547363 \n",
      "     Training Step: 41 Training Loss: 2.4203662872314453 \n",
      "     Training Step: 42 Training Loss: 4.214142322540283 \n",
      "     Training Step: 43 Training Loss: 2.7900562286376953 \n",
      "     Training Step: 44 Training Loss: 3.3696999549865723 \n",
      "     Training Step: 45 Training Loss: 3.5196433067321777 \n",
      "     Training Step: 46 Training Loss: 2.031001567840576 \n",
      "     Training Step: 47 Training Loss: 3.597087860107422 \n",
      "     Training Step: 48 Training Loss: 3.6659700870513916 \n",
      "     Training Step: 49 Training Loss: 3.092038154602051 \n",
      "     Training Step: 50 Training Loss: 2.248385190963745 \n",
      "     Training Step: 51 Training Loss: 2.3184616565704346 \n",
      "     Training Step: 52 Training Loss: 2.28808331489563 \n",
      "     Training Step: 53 Training Loss: 3.879870891571045 \n",
      "     Training Step: 54 Training Loss: 2.235431432723999 \n",
      "     Training Step: 55 Training Loss: 3.321155071258545 \n",
      "     Training Step: 56 Training Loss: 3.280120611190796 \n",
      "     Training Step: 57 Training Loss: 2.6782727241516113 \n",
      "     Training Step: 58 Training Loss: 3.1032140254974365 \n",
      "     Training Step: 59 Training Loss: 3.1449475288391113 \n",
      "     Training Step: 60 Training Loss: 4.144785404205322 \n",
      "     Training Step: 61 Training Loss: 2.972330093383789 \n",
      "     Training Step: 62 Training Loss: 2.8669803142547607 \n",
      "     Training Step: 63 Training Loss: 3.3711276054382324 \n",
      "     Training Step: 64 Training Loss: 2.785025119781494 \n",
      "     Training Step: 65 Training Loss: 2.0766806602478027 \n",
      "     Training Step: 66 Training Loss: 2.748809576034546 \n",
      "     Training Step: 67 Training Loss: 4.331437110900879 \n",
      "     Training Step: 68 Training Loss: 2.708777666091919 \n",
      "     Training Step: 69 Training Loss: 3.1863598823547363 \n",
      "     Training Step: 70 Training Loss: 3.4005520343780518 \n",
      "     Training Step: 71 Training Loss: 3.8390581607818604 \n",
      "     Training Step: 72 Training Loss: 3.0232276916503906 \n",
      "     Training Step: 73 Training Loss: 2.821971893310547 \n",
      "     Training Step: 74 Training Loss: 3.3777456283569336 \n",
      "     Training Step: 75 Training Loss: 4.699954032897949 \n",
      "     Training Step: 76 Training Loss: 2.55487060546875 \n",
      "     Training Step: 77 Training Loss: 2.446608543395996 \n",
      "     Training Step: 78 Training Loss: 2.98185396194458 \n",
      "     Training Step: 79 Training Loss: 2.957113265991211 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6135497093200684 \n",
      "     Validation Step: 1 Validation Loss: 3.0272293090820312 \n",
      "     Validation Step: 2 Validation Loss: 2.298826217651367 \n",
      "     Validation Step: 3 Validation Loss: 3.5106866359710693 \n",
      "     Validation Step: 4 Validation Loss: 3.310067653656006 \n",
      "     Validation Step: 5 Validation Loss: 3.522737979888916 \n",
      "     Validation Step: 6 Validation Loss: 3.7382748126983643 \n",
      "     Validation Step: 7 Validation Loss: 3.647136926651001 \n",
      "     Validation Step: 8 Validation Loss: 2.6746137142181396 \n",
      "     Validation Step: 9 Validation Loss: 2.9829044342041016 \n",
      "     Validation Step: 10 Validation Loss: 3.007905960083008 \n",
      "     Validation Step: 11 Validation Loss: 3.840679168701172 \n",
      "     Validation Step: 12 Validation Loss: 3.131558895111084 \n",
      "     Validation Step: 13 Validation Loss: 2.845050811767578 \n",
      "     Validation Step: 14 Validation Loss: 2.5810890197753906 \n",
      "Epoch: 105\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.0312955379486084 \n",
      "     Training Step: 1 Training Loss: 2.497365951538086 \n",
      "     Training Step: 2 Training Loss: 2.679490804672241 \n",
      "     Training Step: 3 Training Loss: 3.7252962589263916 \n",
      "     Training Step: 4 Training Loss: 3.9309134483337402 \n",
      "     Training Step: 5 Training Loss: 3.2401576042175293 \n",
      "     Training Step: 6 Training Loss: 2.6963818073272705 \n",
      "     Training Step: 7 Training Loss: 2.8472487926483154 \n",
      "     Training Step: 8 Training Loss: 2.269838333129883 \n",
      "     Training Step: 9 Training Loss: 2.986513614654541 \n",
      "     Training Step: 10 Training Loss: 2.203761577606201 \n",
      "     Training Step: 11 Training Loss: 2.7110445499420166 \n",
      "     Training Step: 12 Training Loss: 2.5165436267852783 \n",
      "     Training Step: 13 Training Loss: 2.468012571334839 \n",
      "     Training Step: 14 Training Loss: 2.813955783843994 \n",
      "     Training Step: 15 Training Loss: 2.8784570693969727 \n",
      "     Training Step: 16 Training Loss: 2.2854442596435547 \n",
      "     Training Step: 17 Training Loss: 3.273512363433838 \n",
      "     Training Step: 18 Training Loss: 2.286195755004883 \n",
      "     Training Step: 19 Training Loss: 3.2067158222198486 \n",
      "     Training Step: 20 Training Loss: 3.2978005409240723 \n",
      "     Training Step: 21 Training Loss: 3.0785107612609863 \n",
      "     Training Step: 22 Training Loss: 2.6303937435150146 \n",
      "     Training Step: 23 Training Loss: 3.264955520629883 \n",
      "     Training Step: 24 Training Loss: 3.341789960861206 \n",
      "     Training Step: 25 Training Loss: 4.298022747039795 \n",
      "     Training Step: 26 Training Loss: 4.33636474609375 \n",
      "     Training Step: 27 Training Loss: 3.5382556915283203 \n",
      "     Training Step: 28 Training Loss: 3.4535224437713623 \n",
      "     Training Step: 29 Training Loss: 2.1637771129608154 \n",
      "     Training Step: 30 Training Loss: 3.3866970539093018 \n",
      "     Training Step: 31 Training Loss: 3.6486358642578125 \n",
      "     Training Step: 32 Training Loss: 3.785438060760498 \n",
      "     Training Step: 33 Training Loss: 2.5676021575927734 \n",
      "     Training Step: 34 Training Loss: 3.1130213737487793 \n",
      "     Training Step: 35 Training Loss: 3.200063705444336 \n",
      "     Training Step: 36 Training Loss: 2.9048914909362793 \n",
      "     Training Step: 37 Training Loss: 2.8692502975463867 \n",
      "     Training Step: 38 Training Loss: 2.407701015472412 \n",
      "     Training Step: 39 Training Loss: 2.237729072570801 \n",
      "     Training Step: 40 Training Loss: 3.5480313301086426 \n",
      "     Training Step: 41 Training Loss: 3.2945170402526855 \n",
      "     Training Step: 42 Training Loss: 2.0863332748413086 \n",
      "     Training Step: 43 Training Loss: 2.570420742034912 \n",
      "     Training Step: 44 Training Loss: 3.5298399925231934 \n",
      "     Training Step: 45 Training Loss: 2.929485321044922 \n",
      "     Training Step: 46 Training Loss: 3.3268165588378906 \n",
      "     Training Step: 47 Training Loss: 3.9487195014953613 \n",
      "     Training Step: 48 Training Loss: 2.5739755630493164 \n",
      "     Training Step: 49 Training Loss: 2.9825024604797363 \n",
      "     Training Step: 50 Training Loss: 2.4128270149230957 \n",
      "     Training Step: 51 Training Loss: 3.378995180130005 \n",
      "     Training Step: 52 Training Loss: 3.158094882965088 \n",
      "     Training Step: 53 Training Loss: 3.360520124435425 \n",
      "     Training Step: 54 Training Loss: 4.735495090484619 \n",
      "     Training Step: 55 Training Loss: 3.4151740074157715 \n",
      "     Training Step: 56 Training Loss: 2.693044424057007 \n",
      "     Training Step: 57 Training Loss: 2.2233047485351562 \n",
      "     Training Step: 58 Training Loss: 2.1618711948394775 \n",
      "     Training Step: 59 Training Loss: 3.3031818866729736 \n",
      "     Training Step: 60 Training Loss: 3.3919615745544434 \n",
      "     Training Step: 61 Training Loss: 2.103034734725952 \n",
      "     Training Step: 62 Training Loss: 2.553053855895996 \n",
      "     Training Step: 63 Training Loss: 3.812894821166992 \n",
      "     Training Step: 64 Training Loss: 3.188171863555908 \n",
      "     Training Step: 65 Training Loss: 4.333383560180664 \n",
      "     Training Step: 66 Training Loss: 3.260923385620117 \n",
      "     Training Step: 67 Training Loss: 2.8927149772644043 \n",
      "     Training Step: 68 Training Loss: 2.790147304534912 \n",
      "     Training Step: 69 Training Loss: 2.8329718112945557 \n",
      "     Training Step: 70 Training Loss: 3.00767183303833 \n",
      "     Training Step: 71 Training Loss: 2.379845380783081 \n",
      "     Training Step: 72 Training Loss: 2.495676279067993 \n",
      "     Training Step: 73 Training Loss: 2.992323875427246 \n",
      "     Training Step: 74 Training Loss: 2.86972975730896 \n",
      "     Training Step: 75 Training Loss: 2.3685765266418457 \n",
      "     Training Step: 76 Training Loss: 2.969572067260742 \n",
      "     Training Step: 77 Training Loss: 2.8950681686401367 \n",
      "     Training Step: 78 Training Loss: 2.401794195175171 \n",
      "     Training Step: 79 Training Loss: 2.98016095161438 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.257733106613159 \n",
      "     Validation Step: 1 Validation Loss: 3.873067855834961 \n",
      "     Validation Step: 2 Validation Loss: 3.514416456222534 \n",
      "     Validation Step: 3 Validation Loss: 2.595580816268921 \n",
      "     Validation Step: 4 Validation Loss: 2.684910297393799 \n",
      "     Validation Step: 5 Validation Loss: 2.998843193054199 \n",
      "     Validation Step: 6 Validation Loss: 3.1151976585388184 \n",
      "     Validation Step: 7 Validation Loss: 2.831960678100586 \n",
      "     Validation Step: 8 Validation Loss: 3.5890872478485107 \n",
      "     Validation Step: 9 Validation Loss: 3.4902586936950684 \n",
      "     Validation Step: 10 Validation Loss: 2.3489413261413574 \n",
      "     Validation Step: 11 Validation Loss: 3.5567867755889893 \n",
      "     Validation Step: 12 Validation Loss: 3.735220432281494 \n",
      "     Validation Step: 13 Validation Loss: 3.045626163482666 \n",
      "     Validation Step: 14 Validation Loss: 3.0297341346740723 \n",
      "Epoch: 106\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.31824254989624 \n",
      "     Training Step: 1 Training Loss: 2.409783363342285 \n",
      "     Training Step: 2 Training Loss: 2.790457010269165 \n",
      "     Training Step: 3 Training Loss: 2.3644440174102783 \n",
      "     Training Step: 4 Training Loss: 2.268383502960205 \n",
      "     Training Step: 5 Training Loss: 2.3838210105895996 \n",
      "     Training Step: 6 Training Loss: 2.6898765563964844 \n",
      "     Training Step: 7 Training Loss: 2.7594826221466064 \n",
      "     Training Step: 8 Training Loss: 2.9485418796539307 \n",
      "     Training Step: 9 Training Loss: 3.862377643585205 \n",
      "     Training Step: 10 Training Loss: 3.0865273475646973 \n",
      "     Training Step: 11 Training Loss: 2.678732395172119 \n",
      "     Training Step: 12 Training Loss: 2.820394515991211 \n",
      "     Training Step: 13 Training Loss: 4.811887741088867 \n",
      "     Training Step: 14 Training Loss: 2.6212878227233887 \n",
      "     Training Step: 15 Training Loss: 3.4266703128814697 \n",
      "     Training Step: 16 Training Loss: 3.413390636444092 \n",
      "     Training Step: 17 Training Loss: 3.191603183746338 \n",
      "     Training Step: 18 Training Loss: 2.5185773372650146 \n",
      "     Training Step: 19 Training Loss: 2.1261868476867676 \n",
      "     Training Step: 20 Training Loss: 3.009683609008789 \n",
      "     Training Step: 21 Training Loss: 3.4427762031555176 \n",
      "     Training Step: 22 Training Loss: 3.1757278442382812 \n",
      "     Training Step: 23 Training Loss: 3.4771459102630615 \n",
      "     Training Step: 24 Training Loss: 2.8645074367523193 \n",
      "     Training Step: 25 Training Loss: 2.3143441677093506 \n",
      "     Training Step: 26 Training Loss: 2.4620461463928223 \n",
      "     Training Step: 27 Training Loss: 3.0199713706970215 \n",
      "     Training Step: 28 Training Loss: 3.200773239135742 \n",
      "     Training Step: 29 Training Loss: 3.2289175987243652 \n",
      "     Training Step: 30 Training Loss: 3.3099913597106934 \n",
      "     Training Step: 31 Training Loss: 3.090909957885742 \n",
      "     Training Step: 32 Training Loss: 3.1345882415771484 \n",
      "     Training Step: 33 Training Loss: 3.240922451019287 \n",
      "     Training Step: 34 Training Loss: 3.11160945892334 \n",
      "     Training Step: 35 Training Loss: 3.5591511726379395 \n",
      "     Training Step: 36 Training Loss: 4.267612934112549 \n",
      "     Training Step: 37 Training Loss: 2.684040069580078 \n",
      "     Training Step: 38 Training Loss: 3.120546579360962 \n",
      "     Training Step: 39 Training Loss: 2.528066873550415 \n",
      "     Training Step: 40 Training Loss: 2.82912540435791 \n",
      "     Training Step: 41 Training Loss: 2.700023651123047 \n",
      "     Training Step: 42 Training Loss: 3.5006422996520996 \n",
      "     Training Step: 43 Training Loss: 3.056896924972534 \n",
      "     Training Step: 44 Training Loss: 4.298721790313721 \n",
      "     Training Step: 45 Training Loss: 2.881610870361328 \n",
      "     Training Step: 46 Training Loss: 3.0488476753234863 \n",
      "     Training Step: 47 Training Loss: 2.1143126487731934 \n",
      "     Training Step: 48 Training Loss: 3.5300188064575195 \n",
      "     Training Step: 49 Training Loss: 2.2932186126708984 \n",
      "     Training Step: 50 Training Loss: 4.368923187255859 \n",
      "     Training Step: 51 Training Loss: 2.216409206390381 \n",
      "     Training Step: 52 Training Loss: 3.005960464477539 \n",
      "     Training Step: 53 Training Loss: 2.69663405418396 \n",
      "     Training Step: 54 Training Loss: 2.479884624481201 \n",
      "     Training Step: 55 Training Loss: 2.7888529300689697 \n",
      "     Training Step: 56 Training Loss: 3.5655012130737305 \n",
      "     Training Step: 57 Training Loss: 3.754546642303467 \n",
      "     Training Step: 58 Training Loss: 3.131117820739746 \n",
      "     Training Step: 59 Training Loss: 2.1603293418884277 \n",
      "     Training Step: 60 Training Loss: 2.490668773651123 \n",
      "     Training Step: 61 Training Loss: 3.086738109588623 \n",
      "     Training Step: 62 Training Loss: 2.373063087463379 \n",
      "     Training Step: 63 Training Loss: 2.409191608428955 \n",
      "     Training Step: 64 Training Loss: 2.93424916267395 \n",
      "     Training Step: 65 Training Loss: 2.505892038345337 \n",
      "     Training Step: 66 Training Loss: 3.9475674629211426 \n",
      "     Training Step: 67 Training Loss: 2.484174966812134 \n",
      "     Training Step: 68 Training Loss: 2.8043198585510254 \n",
      "     Training Step: 69 Training Loss: 2.475515842437744 \n",
      "     Training Step: 70 Training Loss: 2.91776704788208 \n",
      "     Training Step: 71 Training Loss: 3.6150436401367188 \n",
      "     Training Step: 72 Training Loss: 2.527533769607544 \n",
      "     Training Step: 73 Training Loss: 2.130436420440674 \n",
      "     Training Step: 74 Training Loss: 2.4207816123962402 \n",
      "     Training Step: 75 Training Loss: 2.946000576019287 \n",
      "     Training Step: 76 Training Loss: 3.307222604751587 \n",
      "     Training Step: 77 Training Loss: 3.7562432289123535 \n",
      "     Training Step: 78 Training Loss: 2.8162498474121094 \n",
      "     Training Step: 79 Training Loss: 2.2845311164855957 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.3142292499542236 \n",
      "     Validation Step: 1 Validation Loss: 2.7094669342041016 \n",
      "     Validation Step: 2 Validation Loss: 2.763568878173828 \n",
      "     Validation Step: 3 Validation Loss: 3.6142690181732178 \n",
      "     Validation Step: 4 Validation Loss: 2.7732033729553223 \n",
      "     Validation Step: 5 Validation Loss: 3.169400691986084 \n",
      "     Validation Step: 6 Validation Loss: 3.5586092472076416 \n",
      "     Validation Step: 7 Validation Loss: 3.759159803390503 \n",
      "     Validation Step: 8 Validation Loss: 3.6247003078460693 \n",
      "     Validation Step: 9 Validation Loss: 3.1004533767700195 \n",
      "     Validation Step: 10 Validation Loss: 2.5867154598236084 \n",
      "     Validation Step: 11 Validation Loss: 3.4638915061950684 \n",
      "     Validation Step: 12 Validation Loss: 3.3420181274414062 \n",
      "     Validation Step: 13 Validation Loss: 2.30340576171875 \n",
      "     Validation Step: 14 Validation Loss: 2.6589322090148926 \n",
      "Epoch: 107\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.424025297164917 \n",
      "     Training Step: 1 Training Loss: 2.7562594413757324 \n",
      "     Training Step: 2 Training Loss: 3.493189811706543 \n",
      "     Training Step: 3 Training Loss: 3.412118434906006 \n",
      "     Training Step: 4 Training Loss: 3.2106804847717285 \n",
      "     Training Step: 5 Training Loss: 3.666934013366699 \n",
      "     Training Step: 6 Training Loss: 2.4150004386901855 \n",
      "     Training Step: 7 Training Loss: 3.034579277038574 \n",
      "     Training Step: 8 Training Loss: 4.137019634246826 \n",
      "     Training Step: 9 Training Loss: 2.619919776916504 \n",
      "     Training Step: 10 Training Loss: 3.086203098297119 \n",
      "     Training Step: 11 Training Loss: 2.092261552810669 \n",
      "     Training Step: 12 Training Loss: 2.4787418842315674 \n",
      "     Training Step: 13 Training Loss: 2.1239819526672363 \n",
      "     Training Step: 14 Training Loss: 2.255584239959717 \n",
      "     Training Step: 15 Training Loss: 2.5380754470825195 \n",
      "     Training Step: 16 Training Loss: 2.894542694091797 \n",
      "     Training Step: 17 Training Loss: 2.1491575241088867 \n",
      "     Training Step: 18 Training Loss: 2.290374517440796 \n",
      "     Training Step: 19 Training Loss: 2.4125819206237793 \n",
      "     Training Step: 20 Training Loss: 2.7900922298431396 \n",
      "     Training Step: 21 Training Loss: 2.896637439727783 \n",
      "     Training Step: 22 Training Loss: 2.3840999603271484 \n",
      "     Training Step: 23 Training Loss: 3.0309219360351562 \n",
      "     Training Step: 24 Training Loss: 4.0980730056762695 \n",
      "     Training Step: 25 Training Loss: 3.8061108589172363 \n",
      "     Training Step: 26 Training Loss: 2.4452321529388428 \n",
      "     Training Step: 27 Training Loss: 2.9433093070983887 \n",
      "     Training Step: 28 Training Loss: 2.2557263374328613 \n",
      "     Training Step: 29 Training Loss: 3.151327610015869 \n",
      "     Training Step: 30 Training Loss: 2.753171920776367 \n",
      "     Training Step: 31 Training Loss: 2.7458553314208984 \n",
      "     Training Step: 32 Training Loss: 2.2492928504943848 \n",
      "     Training Step: 33 Training Loss: 4.4817352294921875 \n",
      "     Training Step: 34 Training Loss: 2.808375120162964 \n",
      "     Training Step: 35 Training Loss: 2.623173236846924 \n",
      "     Training Step: 36 Training Loss: 2.474823474884033 \n",
      "     Training Step: 37 Training Loss: 3.142068862915039 \n",
      "     Training Step: 38 Training Loss: 2.5516536235809326 \n",
      "     Training Step: 39 Training Loss: 3.1639366149902344 \n",
      "     Training Step: 40 Training Loss: 3.7822532653808594 \n",
      "     Training Step: 41 Training Loss: 2.9346373081207275 \n",
      "     Training Step: 42 Training Loss: 3.410726547241211 \n",
      "     Training Step: 43 Training Loss: 3.7653019428253174 \n",
      "     Training Step: 44 Training Loss: 3.4967150688171387 \n",
      "     Training Step: 45 Training Loss: 2.320300817489624 \n",
      "     Training Step: 46 Training Loss: 2.930216073989868 \n",
      "     Training Step: 47 Training Loss: 3.0022315979003906 \n",
      "     Training Step: 48 Training Loss: 2.66713809967041 \n",
      "     Training Step: 49 Training Loss: 4.732925891876221 \n",
      "     Training Step: 50 Training Loss: 3.9920742511749268 \n",
      "     Training Step: 51 Training Loss: 3.305591344833374 \n",
      "     Training Step: 52 Training Loss: 3.427945613861084 \n",
      "     Training Step: 53 Training Loss: 2.1032299995422363 \n",
      "     Training Step: 54 Training Loss: 3.238807201385498 \n",
      "     Training Step: 55 Training Loss: 3.389700412750244 \n",
      "     Training Step: 56 Training Loss: 3.5064258575439453 \n",
      "     Training Step: 57 Training Loss: 3.4420008659362793 \n",
      "     Training Step: 58 Training Loss: 2.609666347503662 \n",
      "     Training Step: 59 Training Loss: 3.3873565196990967 \n",
      "     Training Step: 60 Training Loss: 2.7717337608337402 \n",
      "     Training Step: 61 Training Loss: 3.490050792694092 \n",
      "     Training Step: 62 Training Loss: 2.5313568115234375 \n",
      "     Training Step: 63 Training Loss: 3.517199754714966 \n",
      "     Training Step: 64 Training Loss: 2.4571945667266846 \n",
      "     Training Step: 65 Training Loss: 2.182992458343506 \n",
      "     Training Step: 66 Training Loss: 3.2815749645233154 \n",
      "     Training Step: 67 Training Loss: 3.0247600078582764 \n",
      "     Training Step: 68 Training Loss: 3.0492115020751953 \n",
      "     Training Step: 69 Training Loss: 2.960871458053589 \n",
      "     Training Step: 70 Training Loss: 3.4074394702911377 \n",
      "     Training Step: 71 Training Loss: 3.180267333984375 \n",
      "     Training Step: 72 Training Loss: 2.6749467849731445 \n",
      "     Training Step: 73 Training Loss: 2.3006656169891357 \n",
      "     Training Step: 74 Training Loss: 2.945425510406494 \n",
      "     Training Step: 75 Training Loss: 2.895139217376709 \n",
      "     Training Step: 76 Training Loss: 4.3114013671875 \n",
      "     Training Step: 77 Training Loss: 3.6234328746795654 \n",
      "     Training Step: 78 Training Loss: 3.4240994453430176 \n",
      "     Training Step: 79 Training Loss: 2.5422420501708984 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.741196632385254 \n",
      "     Validation Step: 1 Validation Loss: 3.123640298843384 \n",
      "     Validation Step: 2 Validation Loss: 3.0390892028808594 \n",
      "     Validation Step: 3 Validation Loss: 3.2222228050231934 \n",
      "     Validation Step: 4 Validation Loss: 2.7278084754943848 \n",
      "     Validation Step: 5 Validation Loss: 3.435080051422119 \n",
      "     Validation Step: 6 Validation Loss: 3.1684210300445557 \n",
      "     Validation Step: 7 Validation Loss: 3.063100576400757 \n",
      "     Validation Step: 8 Validation Loss: 3.6655781269073486 \n",
      "     Validation Step: 9 Validation Loss: 2.7345476150512695 \n",
      "     Validation Step: 10 Validation Loss: 3.622384786605835 \n",
      "     Validation Step: 11 Validation Loss: 2.851771593093872 \n",
      "     Validation Step: 12 Validation Loss: 2.343575954437256 \n",
      "     Validation Step: 13 Validation Loss: 3.8603036403656006 \n",
      "     Validation Step: 14 Validation Loss: 3.626389980316162 \n",
      "Epoch: 108\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.459083080291748 \n",
      "     Training Step: 1 Training Loss: 2.6555593013763428 \n",
      "     Training Step: 2 Training Loss: 4.31363582611084 \n",
      "     Training Step: 3 Training Loss: 2.657334566116333 \n",
      "     Training Step: 4 Training Loss: 2.5623817443847656 \n",
      "     Training Step: 5 Training Loss: 2.443913698196411 \n",
      "     Training Step: 6 Training Loss: 3.393543004989624 \n",
      "     Training Step: 7 Training Loss: 2.8438405990600586 \n",
      "     Training Step: 8 Training Loss: 2.5134503841400146 \n",
      "     Training Step: 9 Training Loss: 3.2452404499053955 \n",
      "     Training Step: 10 Training Loss: 3.235895872116089 \n",
      "     Training Step: 11 Training Loss: 2.8303067684173584 \n",
      "     Training Step: 12 Training Loss: 3.4745185375213623 \n",
      "     Training Step: 13 Training Loss: 3.045832633972168 \n",
      "     Training Step: 14 Training Loss: 2.248875141143799 \n",
      "     Training Step: 15 Training Loss: 2.9846818447113037 \n",
      "     Training Step: 16 Training Loss: 2.1920695304870605 \n",
      "     Training Step: 17 Training Loss: 2.5135934352874756 \n",
      "     Training Step: 18 Training Loss: 2.9668848514556885 \n",
      "     Training Step: 19 Training Loss: 2.1077401638031006 \n",
      "     Training Step: 20 Training Loss: 3.7303061485290527 \n",
      "     Training Step: 21 Training Loss: 3.459404230117798 \n",
      "     Training Step: 22 Training Loss: 4.28342866897583 \n",
      "     Training Step: 23 Training Loss: 2.1615958213806152 \n",
      "     Training Step: 24 Training Loss: 2.763652801513672 \n",
      "     Training Step: 25 Training Loss: 2.2779576778411865 \n",
      "     Training Step: 26 Training Loss: 3.295576333999634 \n",
      "     Training Step: 27 Training Loss: 2.6028342247009277 \n",
      "     Training Step: 28 Training Loss: 3.373624324798584 \n",
      "     Training Step: 29 Training Loss: 3.013061285018921 \n",
      "     Training Step: 30 Training Loss: 2.5748610496520996 \n",
      "     Training Step: 31 Training Loss: 3.8558707237243652 \n",
      "     Training Step: 32 Training Loss: 2.562105178833008 \n",
      "     Training Step: 33 Training Loss: 4.074610710144043 \n",
      "     Training Step: 34 Training Loss: 4.56192684173584 \n",
      "     Training Step: 35 Training Loss: 2.748823642730713 \n",
      "     Training Step: 36 Training Loss: 3.506138801574707 \n",
      "     Training Step: 37 Training Loss: 2.9686920642852783 \n",
      "     Training Step: 38 Training Loss: 2.4404327869415283 \n",
      "     Training Step: 39 Training Loss: 3.6092405319213867 \n",
      "     Training Step: 40 Training Loss: 2.209695816040039 \n",
      "     Training Step: 41 Training Loss: 2.6291098594665527 \n",
      "     Training Step: 42 Training Loss: 3.5798075199127197 \n",
      "     Training Step: 43 Training Loss: 3.003260374069214 \n",
      "     Training Step: 44 Training Loss: 3.001732349395752 \n",
      "     Training Step: 45 Training Loss: 2.7560057640075684 \n",
      "     Training Step: 46 Training Loss: 2.7118496894836426 \n",
      "     Training Step: 47 Training Loss: 2.0886893272399902 \n",
      "     Training Step: 48 Training Loss: 2.135237216949463 \n",
      "     Training Step: 49 Training Loss: 2.9269373416900635 \n",
      "     Training Step: 50 Training Loss: 2.5668702125549316 \n",
      "     Training Step: 51 Training Loss: 3.0190634727478027 \n",
      "     Training Step: 52 Training Loss: 3.465615749359131 \n",
      "     Training Step: 53 Training Loss: 3.3109121322631836 \n",
      "     Training Step: 54 Training Loss: 3.015867233276367 \n",
      "     Training Step: 55 Training Loss: 3.3831324577331543 \n",
      "     Training Step: 56 Training Loss: 3.1532490253448486 \n",
      "     Training Step: 57 Training Loss: 3.4087750911712646 \n",
      "     Training Step: 58 Training Loss: 2.9238994121551514 \n",
      "     Training Step: 59 Training Loss: 2.9334518909454346 \n",
      "     Training Step: 60 Training Loss: 3.139880418777466 \n",
      "     Training Step: 61 Training Loss: 4.31405782699585 \n",
      "     Training Step: 62 Training Loss: 3.396536350250244 \n",
      "     Training Step: 63 Training Loss: 3.3391523361206055 \n",
      "     Training Step: 64 Training Loss: 3.7819247245788574 \n",
      "     Training Step: 65 Training Loss: 3.522488594055176 \n",
      "     Training Step: 66 Training Loss: 3.300093173980713 \n",
      "     Training Step: 67 Training Loss: 2.422318458557129 \n",
      "     Training Step: 68 Training Loss: 2.9614624977111816 \n",
      "     Training Step: 69 Training Loss: 2.6072921752929688 \n",
      "     Training Step: 70 Training Loss: 2.798790216445923 \n",
      "     Training Step: 71 Training Loss: 3.0411462783813477 \n",
      "     Training Step: 72 Training Loss: 2.6312098503112793 \n",
      "     Training Step: 73 Training Loss: 2.3651645183563232 \n",
      "     Training Step: 74 Training Loss: 3.4521560668945312 \n",
      "     Training Step: 75 Training Loss: 2.2630844116210938 \n",
      "     Training Step: 76 Training Loss: 2.6578078269958496 \n",
      "     Training Step: 77 Training Loss: 2.600497007369995 \n",
      "     Training Step: 78 Training Loss: 3.365212917327881 \n",
      "     Training Step: 79 Training Loss: 2.0741825103759766 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.7399401664733887 \n",
      "     Validation Step: 1 Validation Loss: 2.8345093727111816 \n",
      "     Validation Step: 2 Validation Loss: 3.203923225402832 \n",
      "     Validation Step: 3 Validation Loss: 3.7034592628479004 \n",
      "     Validation Step: 4 Validation Loss: 3.6741764545440674 \n",
      "     Validation Step: 5 Validation Loss: 3.1805953979492188 \n",
      "     Validation Step: 6 Validation Loss: 3.0757710933685303 \n",
      "     Validation Step: 7 Validation Loss: 3.973179340362549 \n",
      "     Validation Step: 8 Validation Loss: 3.1432290077209473 \n",
      "     Validation Step: 9 Validation Loss: 3.3866546154022217 \n",
      "     Validation Step: 10 Validation Loss: 3.071683406829834 \n",
      "     Validation Step: 11 Validation Loss: 2.7176461219787598 \n",
      "     Validation Step: 12 Validation Loss: 3.6762404441833496 \n",
      "     Validation Step: 13 Validation Loss: 2.8941471576690674 \n",
      "     Validation Step: 14 Validation Loss: 2.2865121364593506 \n",
      "Epoch: 109\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.079787254333496 \n",
      "     Training Step: 1 Training Loss: 3.1119656562805176 \n",
      "     Training Step: 2 Training Loss: 4.0956268310546875 \n",
      "     Training Step: 3 Training Loss: 2.9665985107421875 \n",
      "     Training Step: 4 Training Loss: 3.3276288509368896 \n",
      "     Training Step: 5 Training Loss: 2.4051616191864014 \n",
      "     Training Step: 6 Training Loss: 3.405362606048584 \n",
      "     Training Step: 7 Training Loss: 2.834078311920166 \n",
      "     Training Step: 8 Training Loss: 3.4448001384735107 \n",
      "     Training Step: 9 Training Loss: 2.7862660884857178 \n",
      "     Training Step: 10 Training Loss: 2.686722755432129 \n",
      "     Training Step: 11 Training Loss: 3.1942567825317383 \n",
      "     Training Step: 12 Training Loss: 2.103095769882202 \n",
      "     Training Step: 13 Training Loss: 4.759999752044678 \n",
      "     Training Step: 14 Training Loss: 2.902834415435791 \n",
      "     Training Step: 15 Training Loss: 3.3925538063049316 \n",
      "     Training Step: 16 Training Loss: 3.0105278491973877 \n",
      "     Training Step: 17 Training Loss: 3.3037490844726562 \n",
      "     Training Step: 18 Training Loss: 3.920823097229004 \n",
      "     Training Step: 19 Training Loss: 2.5497050285339355 \n",
      "     Training Step: 20 Training Loss: 2.547410488128662 \n",
      "     Training Step: 21 Training Loss: 2.539055585861206 \n",
      "     Training Step: 22 Training Loss: 2.141726493835449 \n",
      "     Training Step: 23 Training Loss: 3.226670265197754 \n",
      "     Training Step: 24 Training Loss: 2.8509764671325684 \n",
      "     Training Step: 25 Training Loss: 2.385498285293579 \n",
      "     Training Step: 26 Training Loss: 3.376948118209839 \n",
      "     Training Step: 27 Training Loss: 3.7921910285949707 \n",
      "     Training Step: 28 Training Loss: 3.895643711090088 \n",
      "     Training Step: 29 Training Loss: 2.3968167304992676 \n",
      "     Training Step: 30 Training Loss: 3.047213554382324 \n",
      "     Training Step: 31 Training Loss: 2.9769704341888428 \n",
      "     Training Step: 32 Training Loss: 2.1642532348632812 \n",
      "     Training Step: 33 Training Loss: 3.0964841842651367 \n",
      "     Training Step: 34 Training Loss: 2.7595152854919434 \n",
      "     Training Step: 35 Training Loss: 3.484549045562744 \n",
      "     Training Step: 36 Training Loss: 2.2871885299682617 \n",
      "     Training Step: 37 Training Loss: 3.065572738647461 \n",
      "     Training Step: 38 Training Loss: 3.51769757270813 \n",
      "     Training Step: 39 Training Loss: 3.8672986030578613 \n",
      "     Training Step: 40 Training Loss: 4.351761817932129 \n",
      "     Training Step: 41 Training Loss: 3.3749732971191406 \n",
      "     Training Step: 42 Training Loss: 3.324178695678711 \n",
      "     Training Step: 43 Training Loss: 3.341029167175293 \n",
      "     Training Step: 44 Training Loss: 2.6652605533599854 \n",
      "     Training Step: 45 Training Loss: 3.233234167098999 \n",
      "     Training Step: 46 Training Loss: 2.787309169769287 \n",
      "     Training Step: 47 Training Loss: 2.419916868209839 \n",
      "     Training Step: 48 Training Loss: 3.1889641284942627 \n",
      "     Training Step: 49 Training Loss: 2.7618415355682373 \n",
      "     Training Step: 50 Training Loss: 2.330004930496216 \n",
      "     Training Step: 51 Training Loss: 3.241288661956787 \n",
      "     Training Step: 52 Training Loss: 2.874824047088623 \n",
      "     Training Step: 53 Training Loss: 2.0948312282562256 \n",
      "     Training Step: 54 Training Loss: 3.2085461616516113 \n",
      "     Training Step: 55 Training Loss: 2.3370118141174316 \n",
      "     Training Step: 56 Training Loss: 2.9780025482177734 \n",
      "     Training Step: 57 Training Loss: 3.6598877906799316 \n",
      "     Training Step: 58 Training Loss: 2.555966854095459 \n",
      "     Training Step: 59 Training Loss: 2.6220862865448 \n",
      "     Training Step: 60 Training Loss: 3.481340169906616 \n",
      "     Training Step: 61 Training Loss: 2.9394214153289795 \n",
      "     Training Step: 62 Training Loss: 2.9462599754333496 \n",
      "     Training Step: 63 Training Loss: 2.2635750770568848 \n",
      "     Training Step: 64 Training Loss: 3.415780782699585 \n",
      "     Training Step: 65 Training Loss: 4.424476146697998 \n",
      "     Training Step: 66 Training Loss: 2.574235200881958 \n",
      "     Training Step: 67 Training Loss: 2.3873178958892822 \n",
      "     Training Step: 68 Training Loss: 2.766618251800537 \n",
      "     Training Step: 69 Training Loss: 3.884575366973877 \n",
      "     Training Step: 70 Training Loss: 3.497945547103882 \n",
      "     Training Step: 71 Training Loss: 2.7288691997528076 \n",
      "     Training Step: 72 Training Loss: 2.229881525039673 \n",
      "     Training Step: 73 Training Loss: 2.717273712158203 \n",
      "     Training Step: 74 Training Loss: 2.4164674282073975 \n",
      "     Training Step: 75 Training Loss: 3.1237943172454834 \n",
      "     Training Step: 76 Training Loss: 2.565497875213623 \n",
      "     Training Step: 77 Training Loss: 4.470588684082031 \n",
      "     Training Step: 78 Training Loss: 2.1827125549316406 \n",
      "     Training Step: 79 Training Loss: 3.2634263038635254 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.00185489654541 \n",
      "     Validation Step: 1 Validation Loss: 3.8162174224853516 \n",
      "     Validation Step: 2 Validation Loss: 3.6503336429595947 \n",
      "     Validation Step: 3 Validation Loss: 2.201084613800049 \n",
      "     Validation Step: 4 Validation Loss: 4.129164695739746 \n",
      "     Validation Step: 5 Validation Loss: 3.7719781398773193 \n",
      "     Validation Step: 6 Validation Loss: 3.208860397338867 \n",
      "     Validation Step: 7 Validation Loss: 3.104787826538086 \n",
      "     Validation Step: 8 Validation Loss: 2.797346591949463 \n",
      "     Validation Step: 9 Validation Loss: 3.7361557483673096 \n",
      "     Validation Step: 10 Validation Loss: 2.942852020263672 \n",
      "     Validation Step: 11 Validation Loss: 3.440669059753418 \n",
      "     Validation Step: 12 Validation Loss: 2.80312180519104 \n",
      "     Validation Step: 13 Validation Loss: 3.189363479614258 \n",
      "     Validation Step: 14 Validation Loss: 3.105189800262451 \n",
      "Epoch: 110\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.1510448455810547 \n",
      "     Training Step: 1 Training Loss: 3.1445374488830566 \n",
      "     Training Step: 2 Training Loss: 3.5305018424987793 \n",
      "     Training Step: 3 Training Loss: 3.473217487335205 \n",
      "     Training Step: 4 Training Loss: 4.209092140197754 \n",
      "     Training Step: 5 Training Loss: 4.350616455078125 \n",
      "     Training Step: 6 Training Loss: 2.563981533050537 \n",
      "     Training Step: 7 Training Loss: 2.9574809074401855 \n",
      "     Training Step: 8 Training Loss: 2.142652988433838 \n",
      "     Training Step: 9 Training Loss: 3.0962724685668945 \n",
      "     Training Step: 10 Training Loss: 3.1926589012145996 \n",
      "     Training Step: 11 Training Loss: 3.407097816467285 \n",
      "     Training Step: 12 Training Loss: 2.923154830932617 \n",
      "     Training Step: 13 Training Loss: 2.8869385719299316 \n",
      "     Training Step: 14 Training Loss: 2.328702449798584 \n",
      "     Training Step: 15 Training Loss: 3.0900115966796875 \n",
      "     Training Step: 16 Training Loss: 3.375457763671875 \n",
      "     Training Step: 17 Training Loss: 3.1065354347229004 \n",
      "     Training Step: 18 Training Loss: 3.3445146083831787 \n",
      "     Training Step: 19 Training Loss: 2.4533660411834717 \n",
      "     Training Step: 20 Training Loss: 3.27490234375 \n",
      "     Training Step: 21 Training Loss: 3.848959445953369 \n",
      "     Training Step: 22 Training Loss: 2.723942279815674 \n",
      "     Training Step: 23 Training Loss: 2.5760748386383057 \n",
      "     Training Step: 24 Training Loss: 2.243556499481201 \n",
      "     Training Step: 25 Training Loss: 2.5916080474853516 \n",
      "     Training Step: 26 Training Loss: 2.772091865539551 \n",
      "     Training Step: 27 Training Loss: 2.8855624198913574 \n",
      "     Training Step: 28 Training Loss: 3.727484941482544 \n",
      "     Training Step: 29 Training Loss: 3.3332791328430176 \n",
      "     Training Step: 30 Training Loss: 3.2501258850097656 \n",
      "     Training Step: 31 Training Loss: 2.446312189102173 \n",
      "     Training Step: 32 Training Loss: 2.7836480140686035 \n",
      "     Training Step: 33 Training Loss: 2.0727453231811523 \n",
      "     Training Step: 34 Training Loss: 3.92360258102417 \n",
      "     Training Step: 35 Training Loss: 2.849064588546753 \n",
      "     Training Step: 36 Training Loss: 3.128248453140259 \n",
      "     Training Step: 37 Training Loss: 3.3969101905822754 \n",
      "     Training Step: 38 Training Loss: 2.6780261993408203 \n",
      "     Training Step: 39 Training Loss: 3.0001673698425293 \n",
      "     Training Step: 40 Training Loss: 4.076138019561768 \n",
      "     Training Step: 41 Training Loss: 2.3188915252685547 \n",
      "     Training Step: 42 Training Loss: 2.7212202548980713 \n",
      "     Training Step: 43 Training Loss: 2.88891863822937 \n",
      "     Training Step: 44 Training Loss: 3.4612135887145996 \n",
      "     Training Step: 45 Training Loss: 2.882582187652588 \n",
      "     Training Step: 46 Training Loss: 2.1472010612487793 \n",
      "     Training Step: 47 Training Loss: 2.9986207485198975 \n",
      "     Training Step: 48 Training Loss: 3.665184497833252 \n",
      "     Training Step: 49 Training Loss: 2.0457606315612793 \n",
      "     Training Step: 50 Training Loss: 3.4171085357666016 \n",
      "     Training Step: 51 Training Loss: 3.0101101398468018 \n",
      "     Training Step: 52 Training Loss: 2.4000563621520996 \n",
      "     Training Step: 53 Training Loss: 2.3877921104431152 \n",
      "     Training Step: 54 Training Loss: 2.1267359256744385 \n",
      "     Training Step: 55 Training Loss: 2.237610340118408 \n",
      "     Training Step: 56 Training Loss: 3.3491225242614746 \n",
      "     Training Step: 57 Training Loss: 3.0887439250946045 \n",
      "     Training Step: 58 Training Loss: 4.738469123840332 \n",
      "     Training Step: 59 Training Loss: 3.522228717803955 \n",
      "     Training Step: 60 Training Loss: 4.324684143066406 \n",
      "     Training Step: 61 Training Loss: 3.2718253135681152 \n",
      "     Training Step: 62 Training Loss: 2.417093276977539 \n",
      "     Training Step: 63 Training Loss: 3.2108938694000244 \n",
      "     Training Step: 64 Training Loss: 2.8897547721862793 \n",
      "     Training Step: 65 Training Loss: 2.8295822143554688 \n",
      "     Training Step: 66 Training Loss: 2.4506664276123047 \n",
      "     Training Step: 67 Training Loss: 2.6036477088928223 \n",
      "     Training Step: 68 Training Loss: 2.234166145324707 \n",
      "     Training Step: 69 Training Loss: 2.5448946952819824 \n",
      "     Training Step: 70 Training Loss: 2.1156511306762695 \n",
      "     Training Step: 71 Training Loss: 3.741257667541504 \n",
      "     Training Step: 72 Training Loss: 2.7875213623046875 \n",
      "     Training Step: 73 Training Loss: 3.0126500129699707 \n",
      "     Training Step: 74 Training Loss: 2.763843059539795 \n",
      "     Training Step: 75 Training Loss: 2.472385883331299 \n",
      "     Training Step: 76 Training Loss: 3.112238645553589 \n",
      "     Training Step: 77 Training Loss: 3.2918872833251953 \n",
      "     Training Step: 78 Training Loss: 2.7269387245178223 \n",
      "     Training Step: 79 Training Loss: 2.5746524333953857 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.648650646209717 \n",
      "     Validation Step: 1 Validation Loss: 3.6295855045318604 \n",
      "     Validation Step: 2 Validation Loss: 3.141963005065918 \n",
      "     Validation Step: 3 Validation Loss: 3.0516672134399414 \n",
      "     Validation Step: 4 Validation Loss: 2.879690408706665 \n",
      "     Validation Step: 5 Validation Loss: 3.887348175048828 \n",
      "     Validation Step: 6 Validation Loss: 3.112386703491211 \n",
      "     Validation Step: 7 Validation Loss: 2.3002097606658936 \n",
      "     Validation Step: 8 Validation Loss: 2.6918630599975586 \n",
      "     Validation Step: 9 Validation Loss: 3.668266773223877 \n",
      "     Validation Step: 10 Validation Loss: 3.649857997894287 \n",
      "     Validation Step: 11 Validation Loss: 2.7480380535125732 \n",
      "     Validation Step: 12 Validation Loss: 3.371460199356079 \n",
      "     Validation Step: 13 Validation Loss: 3.0574426651000977 \n",
      "     Validation Step: 14 Validation Loss: 3.082287311553955 \n",
      "Epoch: 111\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.1047158241271973 \n",
      "     Training Step: 1 Training Loss: 3.3015518188476562 \n",
      "     Training Step: 2 Training Loss: 3.445871591567993 \n",
      "     Training Step: 3 Training Loss: 3.2075629234313965 \n",
      "     Training Step: 4 Training Loss: 2.4035837650299072 \n",
      "     Training Step: 5 Training Loss: 2.489659547805786 \n",
      "     Training Step: 6 Training Loss: 3.3953943252563477 \n",
      "     Training Step: 7 Training Loss: 2.533334493637085 \n",
      "     Training Step: 8 Training Loss: 3.122311592102051 \n",
      "     Training Step: 9 Training Loss: 2.4695522785186768 \n",
      "     Training Step: 10 Training Loss: 3.3872780799865723 \n",
      "     Training Step: 11 Training Loss: 3.468740224838257 \n",
      "     Training Step: 12 Training Loss: 2.477377414703369 \n",
      "     Training Step: 13 Training Loss: 2.7105019092559814 \n",
      "     Training Step: 14 Training Loss: 2.1687569618225098 \n",
      "     Training Step: 15 Training Loss: 2.184558153152466 \n",
      "     Training Step: 16 Training Loss: 2.0680172443389893 \n",
      "     Training Step: 17 Training Loss: 2.734732151031494 \n",
      "     Training Step: 18 Training Loss: 2.796990394592285 \n",
      "     Training Step: 19 Training Loss: 2.5751264095306396 \n",
      "     Training Step: 20 Training Loss: 2.772801399230957 \n",
      "     Training Step: 21 Training Loss: 4.756649971008301 \n",
      "     Training Step: 22 Training Loss: 3.8758459091186523 \n",
      "     Training Step: 23 Training Loss: 3.0899910926818848 \n",
      "     Training Step: 24 Training Loss: 3.1374008655548096 \n",
      "     Training Step: 25 Training Loss: 2.0958638191223145 \n",
      "     Training Step: 26 Training Loss: 2.6290833950042725 \n",
      "     Training Step: 27 Training Loss: 2.3250632286071777 \n",
      "     Training Step: 28 Training Loss: 2.992805004119873 \n",
      "     Training Step: 29 Training Loss: 2.407620429992676 \n",
      "     Training Step: 30 Training Loss: 3.0381100177764893 \n",
      "     Training Step: 31 Training Loss: 3.3450698852539062 \n",
      "     Training Step: 32 Training Loss: 2.311412811279297 \n",
      "     Training Step: 33 Training Loss: 2.2504916191101074 \n",
      "     Training Step: 34 Training Loss: 3.4324350357055664 \n",
      "     Training Step: 35 Training Loss: 3.110595226287842 \n",
      "     Training Step: 36 Training Loss: 2.8672525882720947 \n",
      "     Training Step: 37 Training Loss: 3.117765426635742 \n",
      "     Training Step: 38 Training Loss: 2.5917603969573975 \n",
      "     Training Step: 39 Training Loss: 3.3400113582611084 \n",
      "     Training Step: 40 Training Loss: 3.948350191116333 \n",
      "     Training Step: 41 Training Loss: 2.310126304626465 \n",
      "     Training Step: 42 Training Loss: 4.275699138641357 \n",
      "     Training Step: 43 Training Loss: 3.3055179119110107 \n",
      "     Training Step: 44 Training Loss: 3.080674886703491 \n",
      "     Training Step: 45 Training Loss: 3.2850584983825684 \n",
      "     Training Step: 46 Training Loss: 2.7203242778778076 \n",
      "     Training Step: 47 Training Loss: 3.285067558288574 \n",
      "     Training Step: 48 Training Loss: 2.8358161449432373 \n",
      "     Training Step: 49 Training Loss: 3.3046154975891113 \n",
      "     Training Step: 50 Training Loss: 3.463855028152466 \n",
      "     Training Step: 51 Training Loss: 2.941788673400879 \n",
      "     Training Step: 52 Training Loss: 2.613506317138672 \n",
      "     Training Step: 53 Training Loss: 2.9018216133117676 \n",
      "     Training Step: 54 Training Loss: 2.5107169151306152 \n",
      "     Training Step: 55 Training Loss: 3.5726070404052734 \n",
      "     Training Step: 56 Training Loss: 2.7613189220428467 \n",
      "     Training Step: 57 Training Loss: 3.7889339923858643 \n",
      "     Training Step: 58 Training Loss: 3.0914883613586426 \n",
      "     Training Step: 59 Training Loss: 2.239499568939209 \n",
      "     Training Step: 60 Training Loss: 4.374825477600098 \n",
      "     Training Step: 61 Training Loss: 2.087308168411255 \n",
      "     Training Step: 62 Training Loss: 3.10452938079834 \n",
      "     Training Step: 63 Training Loss: 2.2544941902160645 \n",
      "     Training Step: 64 Training Loss: 3.103834867477417 \n",
      "     Training Step: 65 Training Loss: 3.2754039764404297 \n",
      "     Training Step: 66 Training Loss: 3.4061195850372314 \n",
      "     Training Step: 67 Training Loss: 3.5716724395751953 \n",
      "     Training Step: 68 Training Loss: 3.006561756134033 \n",
      "     Training Step: 69 Training Loss: 2.860118865966797 \n",
      "     Training Step: 70 Training Loss: 2.7697699069976807 \n",
      "     Training Step: 71 Training Loss: 4.097087860107422 \n",
      "     Training Step: 72 Training Loss: 2.550084352493286 \n",
      "     Training Step: 73 Training Loss: 3.0745046138763428 \n",
      "     Training Step: 74 Training Loss: 2.721214532852173 \n",
      "     Training Step: 75 Training Loss: 4.308312892913818 \n",
      "     Training Step: 76 Training Loss: 2.496438503265381 \n",
      "     Training Step: 77 Training Loss: 2.109386682510376 \n",
      "     Training Step: 78 Training Loss: 2.90602970123291 \n",
      "     Training Step: 79 Training Loss: 3.8058314323425293 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6163930892944336 \n",
      "     Validation Step: 1 Validation Loss: 2.2915563583374023 \n",
      "     Validation Step: 2 Validation Loss: 3.0500638484954834 \n",
      "     Validation Step: 3 Validation Loss: 3.043020248413086 \n",
      "     Validation Step: 4 Validation Loss: 3.5857324600219727 \n",
      "     Validation Step: 5 Validation Loss: 2.63749623298645 \n",
      "     Validation Step: 6 Validation Loss: 3.474949359893799 \n",
      "     Validation Step: 7 Validation Loss: 3.636955976486206 \n",
      "     Validation Step: 8 Validation Loss: 2.743943214416504 \n",
      "     Validation Step: 9 Validation Loss: 3.1562652587890625 \n",
      "     Validation Step: 10 Validation Loss: 2.868917465209961 \n",
      "     Validation Step: 11 Validation Loss: 3.0181829929351807 \n",
      "     Validation Step: 12 Validation Loss: 3.222360849380493 \n",
      "     Validation Step: 13 Validation Loss: 3.697878837585449 \n",
      "     Validation Step: 14 Validation Loss: 3.9439549446105957 \n",
      "Epoch: 112\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.3264689445495605 \n",
      "     Training Step: 1 Training Loss: 3.130963087081909 \n",
      "     Training Step: 2 Training Loss: 2.2299811840057373 \n",
      "     Training Step: 3 Training Loss: 2.9974477291107178 \n",
      "     Training Step: 4 Training Loss: 3.8461546897888184 \n",
      "     Training Step: 5 Training Loss: 2.1565423011779785 \n",
      "     Training Step: 6 Training Loss: 3.114469528198242 \n",
      "     Training Step: 7 Training Loss: 2.9367523193359375 \n",
      "     Training Step: 8 Training Loss: 4.310655117034912 \n",
      "     Training Step: 9 Training Loss: 2.4639716148376465 \n",
      "     Training Step: 10 Training Loss: 2.6031334400177 \n",
      "     Training Step: 11 Training Loss: 3.1662240028381348 \n",
      "     Training Step: 12 Training Loss: 2.84735107421875 \n",
      "     Training Step: 13 Training Loss: 4.670844078063965 \n",
      "     Training Step: 14 Training Loss: 3.437558174133301 \n",
      "     Training Step: 15 Training Loss: 3.550942897796631 \n",
      "     Training Step: 16 Training Loss: 2.1452622413635254 \n",
      "     Training Step: 17 Training Loss: 3.3977365493774414 \n",
      "     Training Step: 18 Training Loss: 3.410094738006592 \n",
      "     Training Step: 19 Training Loss: 3.1916303634643555 \n",
      "     Training Step: 20 Training Loss: 2.696341037750244 \n",
      "     Training Step: 21 Training Loss: 2.5112617015838623 \n",
      "     Training Step: 22 Training Loss: 2.654033660888672 \n",
      "     Training Step: 23 Training Loss: 2.274489402770996 \n",
      "     Training Step: 24 Training Loss: 2.672341823577881 \n",
      "     Training Step: 25 Training Loss: 2.4598989486694336 \n",
      "     Training Step: 26 Training Loss: 2.9995248317718506 \n",
      "     Training Step: 27 Training Loss: 3.2445592880249023 \n",
      "     Training Step: 28 Training Loss: 3.657407760620117 \n",
      "     Training Step: 29 Training Loss: 3.7406511306762695 \n",
      "     Training Step: 30 Training Loss: 3.345615863800049 \n",
      "     Training Step: 31 Training Loss: 2.6492886543273926 \n",
      "     Training Step: 32 Training Loss: 2.8685848712921143 \n",
      "     Training Step: 33 Training Loss: 3.354419469833374 \n",
      "     Training Step: 34 Training Loss: 2.3562653064727783 \n",
      "     Training Step: 35 Training Loss: 3.4480514526367188 \n",
      "     Training Step: 36 Training Loss: 2.375253438949585 \n",
      "     Training Step: 37 Training Loss: 3.5826826095581055 \n",
      "     Training Step: 38 Training Loss: 3.145839214324951 \n",
      "     Training Step: 39 Training Loss: 2.122319221496582 \n",
      "     Training Step: 40 Training Loss: 2.979358673095703 \n",
      "     Training Step: 41 Training Loss: 4.055714130401611 \n",
      "     Training Step: 42 Training Loss: 2.995694637298584 \n",
      "     Training Step: 43 Training Loss: 3.174454927444458 \n",
      "     Training Step: 44 Training Loss: 2.5196785926818848 \n",
      "     Training Step: 45 Training Loss: 2.9857935905456543 \n",
      "     Training Step: 46 Training Loss: 2.919612169265747 \n",
      "     Training Step: 47 Training Loss: 2.507105827331543 \n",
      "     Training Step: 48 Training Loss: 2.873063802719116 \n",
      "     Training Step: 49 Training Loss: 3.3374927043914795 \n",
      "     Training Step: 50 Training Loss: 3.720635414123535 \n",
      "     Training Step: 51 Training Loss: 2.6385176181793213 \n",
      "     Training Step: 52 Training Loss: 3.156782627105713 \n",
      "     Training Step: 53 Training Loss: 2.3394222259521484 \n",
      "     Training Step: 54 Training Loss: 4.069370269775391 \n",
      "     Training Step: 55 Training Loss: 3.03141450881958 \n",
      "     Training Step: 56 Training Loss: 2.969257354736328 \n",
      "     Training Step: 57 Training Loss: 2.42669677734375 \n",
      "     Training Step: 58 Training Loss: 2.3938770294189453 \n",
      "     Training Step: 59 Training Loss: 3.448431968688965 \n",
      "     Training Step: 60 Training Loss: 3.372572898864746 \n",
      "     Training Step: 61 Training Loss: 3.28513765335083 \n",
      "     Training Step: 62 Training Loss: 2.769017219543457 \n",
      "     Training Step: 63 Training Loss: 3.6084845066070557 \n",
      "     Training Step: 64 Training Loss: 2.829066753387451 \n",
      "     Training Step: 65 Training Loss: 2.854928970336914 \n",
      "     Training Step: 66 Training Loss: 2.265648603439331 \n",
      "     Training Step: 67 Training Loss: 2.213322162628174 \n",
      "     Training Step: 68 Training Loss: 2.8626253604888916 \n",
      "     Training Step: 69 Training Loss: 2.99483060836792 \n",
      "     Training Step: 70 Training Loss: 2.892106533050537 \n",
      "     Training Step: 71 Training Loss: 3.867335557937622 \n",
      "     Training Step: 72 Training Loss: 4.403916358947754 \n",
      "     Training Step: 73 Training Loss: 2.367412567138672 \n",
      "     Training Step: 74 Training Loss: 2.5594184398651123 \n",
      "     Training Step: 75 Training Loss: 2.0382401943206787 \n",
      "     Training Step: 76 Training Loss: 3.145310878753662 \n",
      "     Training Step: 77 Training Loss: 2.6940717697143555 \n",
      "     Training Step: 78 Training Loss: 2.801596164703369 \n",
      "     Training Step: 79 Training Loss: 2.2802882194519043 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.891550302505493 \n",
      "     Validation Step: 1 Validation Loss: 2.684490442276001 \n",
      "     Validation Step: 2 Validation Loss: 3.694857597351074 \n",
      "     Validation Step: 3 Validation Loss: 2.803952217102051 \n",
      "     Validation Step: 4 Validation Loss: 2.2443554401397705 \n",
      "     Validation Step: 5 Validation Loss: 3.1220762729644775 \n",
      "     Validation Step: 6 Validation Loss: 3.6337342262268066 \n",
      "     Validation Step: 7 Validation Loss: 3.1820998191833496 \n",
      "     Validation Step: 8 Validation Loss: 3.0666632652282715 \n",
      "     Validation Step: 9 Validation Loss: 3.677046060562134 \n",
      "     Validation Step: 10 Validation Loss: 3.171067476272583 \n",
      "     Validation Step: 11 Validation Loss: 3.6345126628875732 \n",
      "     Validation Step: 12 Validation Loss: 3.4840357303619385 \n",
      "     Validation Step: 13 Validation Loss: 3.8943402767181396 \n",
      "     Validation Step: 14 Validation Loss: 3.0364060401916504 \n",
      "Epoch: 113\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.2503738403320312 \n",
      "     Training Step: 1 Training Loss: 2.494466781616211 \n",
      "     Training Step: 2 Training Loss: 3.3912272453308105 \n",
      "     Training Step: 3 Training Loss: 3.099205493927002 \n",
      "     Training Step: 4 Training Loss: 2.3039939403533936 \n",
      "     Training Step: 5 Training Loss: 2.7962610721588135 \n",
      "     Training Step: 6 Training Loss: 2.7973132133483887 \n",
      "     Training Step: 7 Training Loss: 3.3790712356567383 \n",
      "     Training Step: 8 Training Loss: 2.5514960289001465 \n",
      "     Training Step: 9 Training Loss: 2.093538761138916 \n",
      "     Training Step: 10 Training Loss: 2.9634029865264893 \n",
      "     Training Step: 11 Training Loss: 2.804880142211914 \n",
      "     Training Step: 12 Training Loss: 3.0570926666259766 \n",
      "     Training Step: 13 Training Loss: 2.367706060409546 \n",
      "     Training Step: 14 Training Loss: 3.1950740814208984 \n",
      "     Training Step: 15 Training Loss: 3.490361213684082 \n",
      "     Training Step: 16 Training Loss: 3.0279195308685303 \n",
      "     Training Step: 17 Training Loss: 3.108956813812256 \n",
      "     Training Step: 18 Training Loss: 3.0070199966430664 \n",
      "     Training Step: 19 Training Loss: 2.165013551712036 \n",
      "     Training Step: 20 Training Loss: 3.162808418273926 \n",
      "     Training Step: 21 Training Loss: 3.250082015991211 \n",
      "     Training Step: 22 Training Loss: 2.7554426193237305 \n",
      "     Training Step: 23 Training Loss: 3.883319854736328 \n",
      "     Training Step: 24 Training Loss: 4.405563831329346 \n",
      "     Training Step: 25 Training Loss: 2.561674118041992 \n",
      "     Training Step: 26 Training Loss: 2.5849246978759766 \n",
      "     Training Step: 27 Training Loss: 2.774076223373413 \n",
      "     Training Step: 28 Training Loss: 2.6111838817596436 \n",
      "     Training Step: 29 Training Loss: 2.326347827911377 \n",
      "     Training Step: 30 Training Loss: 3.4396307468414307 \n",
      "     Training Step: 31 Training Loss: 2.769031047821045 \n",
      "     Training Step: 32 Training Loss: 3.2974305152893066 \n",
      "     Training Step: 33 Training Loss: 2.7073161602020264 \n",
      "     Training Step: 34 Training Loss: 2.8805150985717773 \n",
      "     Training Step: 35 Training Loss: 2.1181201934814453 \n",
      "     Training Step: 36 Training Loss: 3.7889504432678223 \n",
      "     Training Step: 37 Training Loss: 2.392364978790283 \n",
      "     Training Step: 38 Training Loss: 3.392045021057129 \n",
      "     Training Step: 39 Training Loss: 3.249453067779541 \n",
      "     Training Step: 40 Training Loss: 2.3532700538635254 \n",
      "     Training Step: 41 Training Loss: 2.4866251945495605 \n",
      "     Training Step: 42 Training Loss: 3.8668088912963867 \n",
      "     Training Step: 43 Training Loss: 4.255780220031738 \n",
      "     Training Step: 44 Training Loss: 2.2841579914093018 \n",
      "     Training Step: 45 Training Loss: 2.7350075244903564 \n",
      "     Training Step: 46 Training Loss: 2.309896469116211 \n",
      "     Training Step: 47 Training Loss: 3.84513521194458 \n",
      "     Training Step: 48 Training Loss: 3.161237955093384 \n",
      "     Training Step: 49 Training Loss: 3.3517773151397705 \n",
      "     Training Step: 50 Training Loss: 3.7390308380126953 \n",
      "     Training Step: 51 Training Loss: 3.4957549571990967 \n",
      "     Training Step: 52 Training Loss: 2.139380931854248 \n",
      "     Training Step: 53 Training Loss: 4.306427478790283 \n",
      "     Training Step: 54 Training Loss: 2.9732680320739746 \n",
      "     Training Step: 55 Training Loss: 2.441354274749756 \n",
      "     Training Step: 56 Training Loss: 2.4852423667907715 \n",
      "     Training Step: 57 Training Loss: 2.373833179473877 \n",
      "     Training Step: 58 Training Loss: 4.697381019592285 \n",
      "     Training Step: 59 Training Loss: 2.8831276893615723 \n",
      "     Training Step: 60 Training Loss: 2.873347759246826 \n",
      "     Training Step: 61 Training Loss: 3.112499713897705 \n",
      "     Training Step: 62 Training Loss: 3.141979694366455 \n",
      "     Training Step: 63 Training Loss: 2.4474639892578125 \n",
      "     Training Step: 64 Training Loss: 3.877056121826172 \n",
      "     Training Step: 65 Training Loss: 2.5950376987457275 \n",
      "     Training Step: 66 Training Loss: 3.28928279876709 \n",
      "     Training Step: 67 Training Loss: 2.231175422668457 \n",
      "     Training Step: 68 Training Loss: 2.7950525283813477 \n",
      "     Training Step: 69 Training Loss: 3.4534130096435547 \n",
      "     Training Step: 70 Training Loss: 2.4614267349243164 \n",
      "     Training Step: 71 Training Loss: 4.274367809295654 \n",
      "     Training Step: 72 Training Loss: 2.952341079711914 \n",
      "     Training Step: 73 Training Loss: 2.9835634231567383 \n",
      "     Training Step: 74 Training Loss: 3.301823139190674 \n",
      "     Training Step: 75 Training Loss: 2.146746873855591 \n",
      "     Training Step: 76 Training Loss: 2.932392120361328 \n",
      "     Training Step: 77 Training Loss: 2.9341273307800293 \n",
      "     Training Step: 78 Training Loss: 3.40109920501709 \n",
      "     Training Step: 79 Training Loss: 3.588216543197632 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9956798553466797 \n",
      "     Validation Step: 1 Validation Loss: 2.688666343688965 \n",
      "     Validation Step: 2 Validation Loss: 2.6941707134246826 \n",
      "     Validation Step: 3 Validation Loss: 3.0538763999938965 \n",
      "     Validation Step: 4 Validation Loss: 3.154961585998535 \n",
      "     Validation Step: 5 Validation Loss: 3.6513609886169434 \n",
      "     Validation Step: 6 Validation Loss: 3.60962176322937 \n",
      "     Validation Step: 7 Validation Loss: 3.8553333282470703 \n",
      "     Validation Step: 8 Validation Loss: 3.548154354095459 \n",
      "     Validation Step: 9 Validation Loss: 3.2554802894592285 \n",
      "     Validation Step: 10 Validation Loss: 2.305352210998535 \n",
      "     Validation Step: 11 Validation Loss: 3.0710694789886475 \n",
      "     Validation Step: 12 Validation Loss: 3.721625328063965 \n",
      "     Validation Step: 13 Validation Loss: 2.86912202835083 \n",
      "     Validation Step: 14 Validation Loss: 3.5098674297332764 \n",
      "Epoch: 114\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.3065454959869385 \n",
      "     Training Step: 1 Training Loss: 2.955394744873047 \n",
      "     Training Step: 2 Training Loss: 3.416224956512451 \n",
      "     Training Step: 3 Training Loss: 3.708047389984131 \n",
      "     Training Step: 4 Training Loss: 2.983177661895752 \n",
      "     Training Step: 5 Training Loss: 3.540666103363037 \n",
      "     Training Step: 6 Training Loss: 3.387596607208252 \n",
      "     Training Step: 7 Training Loss: 2.179960250854492 \n",
      "     Training Step: 8 Training Loss: 2.885744333267212 \n",
      "     Training Step: 9 Training Loss: 3.1978421211242676 \n",
      "     Training Step: 10 Training Loss: 2.3311703205108643 \n",
      "     Training Step: 11 Training Loss: 2.9769744873046875 \n",
      "     Training Step: 12 Training Loss: 2.450533628463745 \n",
      "     Training Step: 13 Training Loss: 3.184807300567627 \n",
      "     Training Step: 14 Training Loss: 2.2255728244781494 \n",
      "     Training Step: 15 Training Loss: 2.572993755340576 \n",
      "     Training Step: 16 Training Loss: 2.8218255043029785 \n",
      "     Training Step: 17 Training Loss: 3.4658751487731934 \n",
      "     Training Step: 18 Training Loss: 3.3447155952453613 \n",
      "     Training Step: 19 Training Loss: 3.466202735900879 \n",
      "     Training Step: 20 Training Loss: 2.5347695350646973 \n",
      "     Training Step: 21 Training Loss: 3.2982521057128906 \n",
      "     Training Step: 22 Training Loss: 2.3298864364624023 \n",
      "     Training Step: 23 Training Loss: 3.5873003005981445 \n",
      "     Training Step: 24 Training Loss: 3.2966487407684326 \n",
      "     Training Step: 25 Training Loss: 2.650439739227295 \n",
      "     Training Step: 26 Training Loss: 2.518375873565674 \n",
      "     Training Step: 27 Training Loss: 2.1170248985290527 \n",
      "     Training Step: 28 Training Loss: 3.766205310821533 \n",
      "     Training Step: 29 Training Loss: 4.005674839019775 \n",
      "     Training Step: 30 Training Loss: 2.5045459270477295 \n",
      "     Training Step: 31 Training Loss: 3.1507182121276855 \n",
      "     Training Step: 32 Training Loss: 2.8264050483703613 \n",
      "     Training Step: 33 Training Loss: 2.9128506183624268 \n",
      "     Training Step: 34 Training Loss: 2.8491177558898926 \n",
      "     Training Step: 35 Training Loss: 4.297513961791992 \n",
      "     Training Step: 36 Training Loss: 2.423643112182617 \n",
      "     Training Step: 37 Training Loss: 4.760954856872559 \n",
      "     Training Step: 38 Training Loss: 2.5731475353240967 \n",
      "     Training Step: 39 Training Loss: 2.0738978385925293 \n",
      "     Training Step: 40 Training Loss: 3.040048837661743 \n",
      "     Training Step: 41 Training Loss: 3.432401180267334 \n",
      "     Training Step: 42 Training Loss: 3.2826826572418213 \n",
      "     Training Step: 43 Training Loss: 2.7440946102142334 \n",
      "     Training Step: 44 Training Loss: 2.398923873901367 \n",
      "     Training Step: 45 Training Loss: 2.795858144760132 \n",
      "     Training Step: 46 Training Loss: 3.445783853530884 \n",
      "     Training Step: 47 Training Loss: 3.4711053371429443 \n",
      "     Training Step: 48 Training Loss: 2.919178009033203 \n",
      "     Training Step: 49 Training Loss: 3.3619470596313477 \n",
      "     Training Step: 50 Training Loss: 3.192587375640869 \n",
      "     Training Step: 51 Training Loss: 4.361391067504883 \n",
      "     Training Step: 52 Training Loss: 3.4168896675109863 \n",
      "     Training Step: 53 Training Loss: 2.2292139530181885 \n",
      "     Training Step: 54 Training Loss: 2.5138206481933594 \n",
      "     Training Step: 55 Training Loss: 2.101846694946289 \n",
      "     Training Step: 56 Training Loss: 2.9777376651763916 \n",
      "     Training Step: 57 Training Loss: 3.759885787963867 \n",
      "     Training Step: 58 Training Loss: 3.2836050987243652 \n",
      "     Training Step: 59 Training Loss: 3.2758402824401855 \n",
      "     Training Step: 60 Training Loss: 2.5729281902313232 \n",
      "     Training Step: 61 Training Loss: 2.905344009399414 \n",
      "     Training Step: 62 Training Loss: 2.786004066467285 \n",
      "     Training Step: 63 Training Loss: 2.6979727745056152 \n",
      "     Training Step: 64 Training Loss: 2.0707082748413086 \n",
      "     Training Step: 65 Training Loss: 2.7559542655944824 \n",
      "     Training Step: 66 Training Loss: 3.1014404296875 \n",
      "     Training Step: 67 Training Loss: 3.289780378341675 \n",
      "     Training Step: 68 Training Loss: 3.169903039932251 \n",
      "     Training Step: 69 Training Loss: 2.922558546066284 \n",
      "     Training Step: 70 Training Loss: 3.2024569511413574 \n",
      "     Training Step: 71 Training Loss: 2.7703986167907715 \n",
      "     Training Step: 72 Training Loss: 3.161646604537964 \n",
      "     Training Step: 73 Training Loss: 4.3853631019592285 \n",
      "     Training Step: 74 Training Loss: 2.354405403137207 \n",
      "     Training Step: 75 Training Loss: 3.411539077758789 \n",
      "     Training Step: 76 Training Loss: 3.877626895904541 \n",
      "     Training Step: 77 Training Loss: 2.2726073265075684 \n",
      "     Training Step: 78 Training Loss: 2.901883125305176 \n",
      "     Training Step: 79 Training Loss: 2.4998981952667236 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6410274505615234 \n",
      "     Validation Step: 1 Validation Loss: 3.049943685531616 \n",
      "     Validation Step: 2 Validation Loss: 3.964813709259033 \n",
      "     Validation Step: 3 Validation Loss: 3.600968837738037 \n",
      "     Validation Step: 4 Validation Loss: 3.7096595764160156 \n",
      "     Validation Step: 5 Validation Loss: 2.864617109298706 \n",
      "     Validation Step: 6 Validation Loss: 3.111988067626953 \n",
      "     Validation Step: 7 Validation Loss: 3.223184585571289 \n",
      "     Validation Step: 8 Validation Loss: 2.7409610748291016 \n",
      "     Validation Step: 9 Validation Loss: 3.0518834590911865 \n",
      "     Validation Step: 10 Validation Loss: 2.287853717803955 \n",
      "     Validation Step: 11 Validation Loss: 3.4069368839263916 \n",
      "     Validation Step: 12 Validation Loss: 3.12314510345459 \n",
      "     Validation Step: 13 Validation Loss: 2.7483692169189453 \n",
      "     Validation Step: 14 Validation Loss: 3.6691770553588867 \n",
      "Epoch: 115\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.152364492416382 \n",
      "     Training Step: 1 Training Loss: 3.837851047515869 \n",
      "     Training Step: 2 Training Loss: 2.095391273498535 \n",
      "     Training Step: 3 Training Loss: 3.264643669128418 \n",
      "     Training Step: 4 Training Loss: 3.08890962600708 \n",
      "     Training Step: 5 Training Loss: 3.081197738647461 \n",
      "     Training Step: 6 Training Loss: 2.213881492614746 \n",
      "     Training Step: 7 Training Loss: 2.431447982788086 \n",
      "     Training Step: 8 Training Loss: 2.3318679332733154 \n",
      "     Training Step: 9 Training Loss: 3.366715908050537 \n",
      "     Training Step: 10 Training Loss: 3.054427146911621 \n",
      "     Training Step: 11 Training Loss: 2.6956419944763184 \n",
      "     Training Step: 12 Training Loss: 3.2714128494262695 \n",
      "     Training Step: 13 Training Loss: 3.4183266162872314 \n",
      "     Training Step: 14 Training Loss: 2.9533331394195557 \n",
      "     Training Step: 15 Training Loss: 2.7094550132751465 \n",
      "     Training Step: 16 Training Loss: 2.261425495147705 \n",
      "     Training Step: 17 Training Loss: 2.6155033111572266 \n",
      "     Training Step: 18 Training Loss: 2.658531665802002 \n",
      "     Training Step: 19 Training Loss: 4.290889739990234 \n",
      "     Training Step: 20 Training Loss: 2.1331334114074707 \n",
      "     Training Step: 21 Training Loss: 2.4254794120788574 \n",
      "     Training Step: 22 Training Loss: 2.6902356147766113 \n",
      "     Training Step: 23 Training Loss: 2.490699052810669 \n",
      "     Training Step: 24 Training Loss: 3.3513426780700684 \n",
      "     Training Step: 25 Training Loss: 3.5978002548217773 \n",
      "     Training Step: 26 Training Loss: 2.7732667922973633 \n",
      "     Training Step: 27 Training Loss: 2.400275707244873 \n",
      "     Training Step: 28 Training Loss: 2.7518017292022705 \n",
      "     Training Step: 29 Training Loss: 4.3497490882873535 \n",
      "     Training Step: 30 Training Loss: 3.736302137374878 \n",
      "     Training Step: 31 Training Loss: 2.8404645919799805 \n",
      "     Training Step: 32 Training Loss: 3.210864305496216 \n",
      "     Training Step: 33 Training Loss: 2.8116226196289062 \n",
      "     Training Step: 34 Training Loss: 2.066890239715576 \n",
      "     Training Step: 35 Training Loss: 3.1381046772003174 \n",
      "     Training Step: 36 Training Loss: 2.60526180267334 \n",
      "     Training Step: 37 Training Loss: 2.730992078781128 \n",
      "     Training Step: 38 Training Loss: 2.9855377674102783 \n",
      "     Training Step: 39 Training Loss: 2.538573980331421 \n",
      "     Training Step: 40 Training Loss: 3.3923683166503906 \n",
      "     Training Step: 41 Training Loss: 2.930882453918457 \n",
      "     Training Step: 42 Training Loss: 3.3310909271240234 \n",
      "     Training Step: 43 Training Loss: 2.8066883087158203 \n",
      "     Training Step: 44 Training Loss: 2.5388340950012207 \n",
      "     Training Step: 45 Training Loss: 3.1249585151672363 \n",
      "     Training Step: 46 Training Loss: 2.9095585346221924 \n",
      "     Training Step: 47 Training Loss: 2.43392014503479 \n",
      "     Training Step: 48 Training Loss: 3.8864641189575195 \n",
      "     Training Step: 49 Training Loss: 3.020242691040039 \n",
      "     Training Step: 50 Training Loss: 2.849588632583618 \n",
      "     Training Step: 51 Training Loss: 2.3365321159362793 \n",
      "     Training Step: 52 Training Loss: 2.203500986099243 \n",
      "     Training Step: 53 Training Loss: 2.926567554473877 \n",
      "     Training Step: 54 Training Loss: 2.147104024887085 \n",
      "     Training Step: 55 Training Loss: 3.5156893730163574 \n",
      "     Training Step: 56 Training Loss: 3.730750560760498 \n",
      "     Training Step: 57 Training Loss: 3.565216302871704 \n",
      "     Training Step: 58 Training Loss: 3.3302111625671387 \n",
      "     Training Step: 59 Training Loss: 3.343583106994629 \n",
      "     Training Step: 60 Training Loss: 2.8026294708251953 \n",
      "     Training Step: 61 Training Loss: 2.397629737854004 \n",
      "     Training Step: 62 Training Loss: 3.376314401626587 \n",
      "     Training Step: 63 Training Loss: 3.505168914794922 \n",
      "     Training Step: 64 Training Loss: 2.813850164413452 \n",
      "     Training Step: 65 Training Loss: 2.2202515602111816 \n",
      "     Training Step: 66 Training Loss: 4.752504348754883 \n",
      "     Training Step: 67 Training Loss: 4.33018684387207 \n",
      "     Training Step: 68 Training Loss: 2.176623821258545 \n",
      "     Training Step: 69 Training Loss: 3.2915115356445312 \n",
      "     Training Step: 70 Training Loss: 3.3865296840667725 \n",
      "     Training Step: 71 Training Loss: 3.7770562171936035 \n",
      "     Training Step: 72 Training Loss: 2.708651304244995 \n",
      "     Training Step: 73 Training Loss: 3.085448741912842 \n",
      "     Training Step: 74 Training Loss: 2.8348565101623535 \n",
      "     Training Step: 75 Training Loss: 3.1176705360412598 \n",
      "     Training Step: 76 Training Loss: 2.981830358505249 \n",
      "     Training Step: 77 Training Loss: 3.1920559406280518 \n",
      "     Training Step: 78 Training Loss: 2.5637001991271973 \n",
      "     Training Step: 79 Training Loss: 2.322854518890381 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.589653491973877 \n",
      "     Validation Step: 1 Validation Loss: 2.944298267364502 \n",
      "     Validation Step: 2 Validation Loss: 3.0999133586883545 \n",
      "     Validation Step: 3 Validation Loss: 3.5940005779266357 \n",
      "     Validation Step: 4 Validation Loss: 3.225945234298706 \n",
      "     Validation Step: 5 Validation Loss: 3.519503116607666 \n",
      "     Validation Step: 6 Validation Loss: 3.009119987487793 \n",
      "     Validation Step: 7 Validation Loss: 3.680816411972046 \n",
      "     Validation Step: 8 Validation Loss: 3.8819925785064697 \n",
      "     Validation Step: 9 Validation Loss: 3.450007438659668 \n",
      "     Validation Step: 10 Validation Loss: 2.7248692512512207 \n",
      "     Validation Step: 11 Validation Loss: 3.0997138023376465 \n",
      "     Validation Step: 12 Validation Loss: 2.7253668308258057 \n",
      "     Validation Step: 13 Validation Loss: 2.2478160858154297 \n",
      "     Validation Step: 14 Validation Loss: 2.8915340900421143 \n",
      "Epoch: 116\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.5114388465881348 \n",
      "     Training Step: 1 Training Loss: 2.109649896621704 \n",
      "     Training Step: 2 Training Loss: 2.2285642623901367 \n",
      "     Training Step: 3 Training Loss: 2.894392490386963 \n",
      "     Training Step: 4 Training Loss: 2.569591999053955 \n",
      "     Training Step: 5 Training Loss: 2.9251997470855713 \n",
      "     Training Step: 6 Training Loss: 3.01206636428833 \n",
      "     Training Step: 7 Training Loss: 2.2960681915283203 \n",
      "     Training Step: 8 Training Loss: 2.799711227416992 \n",
      "     Training Step: 9 Training Loss: 3.318735122680664 \n",
      "     Training Step: 10 Training Loss: 3.0710411071777344 \n",
      "     Training Step: 11 Training Loss: 3.2369000911712646 \n",
      "     Training Step: 12 Training Loss: 2.1425929069519043 \n",
      "     Training Step: 13 Training Loss: 4.385705947875977 \n",
      "     Training Step: 14 Training Loss: 2.945329189300537 \n",
      "     Training Step: 15 Training Loss: 2.0844078063964844 \n",
      "     Training Step: 16 Training Loss: 2.8022756576538086 \n",
      "     Training Step: 17 Training Loss: 3.190603733062744 \n",
      "     Training Step: 18 Training Loss: 4.392192840576172 \n",
      "     Training Step: 19 Training Loss: 4.12007474899292 \n",
      "     Training Step: 20 Training Loss: 2.166764736175537 \n",
      "     Training Step: 21 Training Loss: 3.390718698501587 \n",
      "     Training Step: 22 Training Loss: 3.5200276374816895 \n",
      "     Training Step: 23 Training Loss: 2.7779407501220703 \n",
      "     Training Step: 24 Training Loss: 3.8673243522644043 \n",
      "     Training Step: 25 Training Loss: 2.929924964904785 \n",
      "     Training Step: 26 Training Loss: 3.3838143348693848 \n",
      "     Training Step: 27 Training Loss: 3.653691291809082 \n",
      "     Training Step: 28 Training Loss: 3.1004490852355957 \n",
      "     Training Step: 29 Training Loss: 3.8000712394714355 \n",
      "     Training Step: 30 Training Loss: 3.3374977111816406 \n",
      "     Training Step: 31 Training Loss: 2.4525814056396484 \n",
      "     Training Step: 32 Training Loss: 4.283289909362793 \n",
      "     Training Step: 33 Training Loss: 2.247608184814453 \n",
      "     Training Step: 34 Training Loss: 2.582547187805176 \n",
      "     Training Step: 35 Training Loss: 3.1423239707946777 \n",
      "     Training Step: 36 Training Loss: 3.1690618991851807 \n",
      "     Training Step: 37 Training Loss: 2.255967855453491 \n",
      "     Training Step: 38 Training Loss: 2.614205837249756 \n",
      "     Training Step: 39 Training Loss: 3.563504219055176 \n",
      "     Training Step: 40 Training Loss: 3.2040281295776367 \n",
      "     Training Step: 41 Training Loss: 3.223921775817871 \n",
      "     Training Step: 42 Training Loss: 2.396366596221924 \n",
      "     Training Step: 43 Training Loss: 2.670949935913086 \n",
      "     Training Step: 44 Training Loss: 2.725729465484619 \n",
      "     Training Step: 45 Training Loss: 2.706000804901123 \n",
      "     Training Step: 46 Training Loss: 3.4419779777526855 \n",
      "     Training Step: 47 Training Loss: 3.457613468170166 \n",
      "     Training Step: 48 Training Loss: 3.2006871700286865 \n",
      "     Training Step: 49 Training Loss: 3.34460711479187 \n",
      "     Training Step: 50 Training Loss: 3.3776373863220215 \n",
      "     Training Step: 51 Training Loss: 3.185666084289551 \n",
      "     Training Step: 52 Training Loss: 3.3949289321899414 \n",
      "     Training Step: 53 Training Loss: 3.384164571762085 \n",
      "     Training Step: 54 Training Loss: 2.516188144683838 \n",
      "     Training Step: 55 Training Loss: 2.092073917388916 \n",
      "     Training Step: 56 Training Loss: 2.309370756149292 \n",
      "     Training Step: 57 Training Loss: 4.722136497497559 \n",
      "     Training Step: 58 Training Loss: 3.783351421356201 \n",
      "     Training Step: 59 Training Loss: 2.372407913208008 \n",
      "     Training Step: 60 Training Loss: 2.2272753715515137 \n",
      "     Training Step: 61 Training Loss: 2.9551210403442383 \n",
      "     Training Step: 62 Training Loss: 3.1649792194366455 \n",
      "     Training Step: 63 Training Loss: 3.196277141571045 \n",
      "     Training Step: 64 Training Loss: 2.88639760017395 \n",
      "     Training Step: 65 Training Loss: 3.479543924331665 \n",
      "     Training Step: 66 Training Loss: 3.0024828910827637 \n",
      "     Training Step: 67 Training Loss: 2.6358532905578613 \n",
      "     Training Step: 68 Training Loss: 2.832078695297241 \n",
      "     Training Step: 69 Training Loss: 3.780742883682251 \n",
      "     Training Step: 70 Training Loss: 2.852309226989746 \n",
      "     Training Step: 71 Training Loss: 2.3453757762908936 \n",
      "     Training Step: 72 Training Loss: 2.7398672103881836 \n",
      "     Training Step: 73 Training Loss: 2.388777017593384 \n",
      "     Training Step: 74 Training Loss: 2.8629050254821777 \n",
      "     Training Step: 75 Training Loss: 2.634068489074707 \n",
      "     Training Step: 76 Training Loss: 2.4481234550476074 \n",
      "     Training Step: 77 Training Loss: 3.7633752822875977 \n",
      "     Training Step: 78 Training Loss: 2.072077751159668 \n",
      "     Training Step: 79 Training Loss: 2.3706610202789307 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.996957778930664 \n",
      "     Validation Step: 1 Validation Loss: 2.5996549129486084 \n",
      "     Validation Step: 2 Validation Loss: 3.5237832069396973 \n",
      "     Validation Step: 3 Validation Loss: 2.846036911010742 \n",
      "     Validation Step: 4 Validation Loss: 3.755807876586914 \n",
      "     Validation Step: 5 Validation Loss: 3.119060516357422 \n",
      "     Validation Step: 6 Validation Loss: 3.4947004318237305 \n",
      "     Validation Step: 7 Validation Loss: 2.8715219497680664 \n",
      "     Validation Step: 8 Validation Loss: 3.435213565826416 \n",
      "     Validation Step: 9 Validation Loss: 2.869967222213745 \n",
      "     Validation Step: 10 Validation Loss: 2.362992286682129 \n",
      "     Validation Step: 11 Validation Loss: 3.3197643756866455 \n",
      "     Validation Step: 12 Validation Loss: 3.619647979736328 \n",
      "     Validation Step: 13 Validation Loss: 3.848829746246338 \n",
      "     Validation Step: 14 Validation Loss: 2.664219617843628 \n",
      "Epoch: 117\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.114311695098877 \n",
      "     Training Step: 1 Training Loss: 2.101064920425415 \n",
      "     Training Step: 2 Training Loss: 2.799286365509033 \n",
      "     Training Step: 3 Training Loss: 3.1056199073791504 \n",
      "     Training Step: 4 Training Loss: 2.7768421173095703 \n",
      "     Training Step: 5 Training Loss: 3.4538931846618652 \n",
      "     Training Step: 6 Training Loss: 2.4818763732910156 \n",
      "     Training Step: 7 Training Loss: 3.753843069076538 \n",
      "     Training Step: 8 Training Loss: 2.5273890495300293 \n",
      "     Training Step: 9 Training Loss: 3.4054436683654785 \n",
      "     Training Step: 10 Training Loss: 2.1618032455444336 \n",
      "     Training Step: 11 Training Loss: 2.520028591156006 \n",
      "     Training Step: 12 Training Loss: 3.5299072265625 \n",
      "     Training Step: 13 Training Loss: 2.3544769287109375 \n",
      "     Training Step: 14 Training Loss: 3.007082223892212 \n",
      "     Training Step: 15 Training Loss: 2.883880138397217 \n",
      "     Training Step: 16 Training Loss: 3.178610324859619 \n",
      "     Training Step: 17 Training Loss: 3.1875855922698975 \n",
      "     Training Step: 18 Training Loss: 3.1262526512145996 \n",
      "     Training Step: 19 Training Loss: 3.404749870300293 \n",
      "     Training Step: 20 Training Loss: 2.7784814834594727 \n",
      "     Training Step: 21 Training Loss: 3.366955518722534 \n",
      "     Training Step: 22 Training Loss: 3.3685784339904785 \n",
      "     Training Step: 23 Training Loss: 2.6693150997161865 \n",
      "     Training Step: 24 Training Loss: 3.3776140213012695 \n",
      "     Training Step: 25 Training Loss: 2.3252596855163574 \n",
      "     Training Step: 26 Training Loss: 4.3069047927856445 \n",
      "     Training Step: 27 Training Loss: 2.073556661605835 \n",
      "     Training Step: 28 Training Loss: 2.930051803588867 \n",
      "     Training Step: 29 Training Loss: 4.312292575836182 \n",
      "     Training Step: 30 Training Loss: 3.6047749519348145 \n",
      "     Training Step: 31 Training Loss: 2.242342948913574 \n",
      "     Training Step: 32 Training Loss: 3.50056791305542 \n",
      "     Training Step: 33 Training Loss: 4.189430236816406 \n",
      "     Training Step: 34 Training Loss: 2.2826991081237793 \n",
      "     Training Step: 35 Training Loss: 3.1241512298583984 \n",
      "     Training Step: 36 Training Loss: 2.5000009536743164 \n",
      "     Training Step: 37 Training Loss: 3.842679023742676 \n",
      "     Training Step: 38 Training Loss: 2.9329607486724854 \n",
      "     Training Step: 39 Training Loss: 2.4260001182556152 \n",
      "     Training Step: 40 Training Loss: 2.3047237396240234 \n",
      "     Training Step: 41 Training Loss: 2.424482583999634 \n",
      "     Training Step: 42 Training Loss: 3.1439993381500244 \n",
      "     Training Step: 43 Training Loss: 3.158565044403076 \n",
      "     Training Step: 44 Training Loss: 2.7770514488220215 \n",
      "     Training Step: 45 Training Loss: 3.2306618690490723 \n",
      "     Training Step: 46 Training Loss: 2.7263107299804688 \n",
      "     Training Step: 47 Training Loss: 2.7326056957244873 \n",
      "     Training Step: 48 Training Loss: 3.163259506225586 \n",
      "     Training Step: 49 Training Loss: 3.2775418758392334 \n",
      "     Training Step: 50 Training Loss: 2.9964358806610107 \n",
      "     Training Step: 51 Training Loss: 2.86460280418396 \n",
      "     Training Step: 52 Training Loss: 2.9483864307403564 \n",
      "     Training Step: 53 Training Loss: 2.4464495182037354 \n",
      "     Training Step: 54 Training Loss: 3.9108219146728516 \n",
      "     Training Step: 55 Training Loss: 3.107656240463257 \n",
      "     Training Step: 56 Training Loss: 2.642484664916992 \n",
      "     Training Step: 57 Training Loss: 3.3371715545654297 \n",
      "     Training Step: 58 Training Loss: 2.5977046489715576 \n",
      "     Training Step: 59 Training Loss: 2.5443575382232666 \n",
      "     Training Step: 60 Training Loss: 2.902135133743286 \n",
      "     Training Step: 61 Training Loss: 2.9886515140533447 \n",
      "     Training Step: 62 Training Loss: 3.373671531677246 \n",
      "     Training Step: 63 Training Loss: 2.4114623069763184 \n",
      "     Training Step: 64 Training Loss: 4.740794658660889 \n",
      "     Training Step: 65 Training Loss: 2.8937759399414062 \n",
      "     Training Step: 66 Training Loss: 3.7418365478515625 \n",
      "     Training Step: 67 Training Loss: 2.8753745555877686 \n",
      "     Training Step: 68 Training Loss: 2.1679601669311523 \n",
      "     Training Step: 69 Training Loss: 4.171690940856934 \n",
      "     Training Step: 70 Training Loss: 2.655672073364258 \n",
      "     Training Step: 71 Training Loss: 3.5064234733581543 \n",
      "     Training Step: 72 Training Loss: 2.1632070541381836 \n",
      "     Training Step: 73 Training Loss: 3.0994136333465576 \n",
      "     Training Step: 74 Training Loss: 2.5747363567352295 \n",
      "     Training Step: 75 Training Loss: 2.599743366241455 \n",
      "     Training Step: 76 Training Loss: 3.367751121520996 \n",
      "     Training Step: 77 Training Loss: 2.470771312713623 \n",
      "     Training Step: 78 Training Loss: 3.331718921661377 \n",
      "     Training Step: 79 Training Loss: 3.3790760040283203 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.0993833541870117 \n",
      "     Validation Step: 1 Validation Loss: 3.7130582332611084 \n",
      "     Validation Step: 2 Validation Loss: 3.140885829925537 \n",
      "     Validation Step: 3 Validation Loss: 3.0873050689697266 \n",
      "     Validation Step: 4 Validation Loss: 2.91176176071167 \n",
      "     Validation Step: 5 Validation Loss: 3.0080337524414062 \n",
      "     Validation Step: 6 Validation Loss: 3.7297306060791016 \n",
      "     Validation Step: 7 Validation Loss: 2.7209811210632324 \n",
      "     Validation Step: 8 Validation Loss: 3.4268012046813965 \n",
      "     Validation Step: 9 Validation Loss: 3.695101261138916 \n",
      "     Validation Step: 10 Validation Loss: 3.1860296726226807 \n",
      "     Validation Step: 11 Validation Loss: 3.9644832611083984 \n",
      "     Validation Step: 12 Validation Loss: 2.2774436473846436 \n",
      "     Validation Step: 13 Validation Loss: 2.793339252471924 \n",
      "     Validation Step: 14 Validation Loss: 3.7026450634002686 \n",
      "Epoch: 118\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.639540672302246 \n",
      "     Training Step: 1 Training Loss: 4.283015251159668 \n",
      "     Training Step: 2 Training Loss: 2.2183964252471924 \n",
      "     Training Step: 3 Training Loss: 2.7839431762695312 \n",
      "     Training Step: 4 Training Loss: 4.365029335021973 \n",
      "     Training Step: 5 Training Loss: 2.563002109527588 \n",
      "     Training Step: 6 Training Loss: 3.4435577392578125 \n",
      "     Training Step: 7 Training Loss: 3.5329084396362305 \n",
      "     Training Step: 8 Training Loss: 2.1181254386901855 \n",
      "     Training Step: 9 Training Loss: 3.241787910461426 \n",
      "     Training Step: 10 Training Loss: 3.0641772747039795 \n",
      "     Training Step: 11 Training Loss: 2.540522813796997 \n",
      "     Training Step: 12 Training Loss: 3.2491583824157715 \n",
      "     Training Step: 13 Training Loss: 3.1862130165100098 \n",
      "     Training Step: 14 Training Loss: 3.008882522583008 \n",
      "     Training Step: 15 Training Loss: 3.4835093021392822 \n",
      "     Training Step: 16 Training Loss: 2.5733795166015625 \n",
      "     Training Step: 17 Training Loss: 3.193955898284912 \n",
      "     Training Step: 18 Training Loss: 4.447254180908203 \n",
      "     Training Step: 19 Training Loss: 2.7416272163391113 \n",
      "     Training Step: 20 Training Loss: 3.8925094604492188 \n",
      "     Training Step: 21 Training Loss: 2.457242488861084 \n",
      "     Training Step: 22 Training Loss: 2.1317851543426514 \n",
      "     Training Step: 23 Training Loss: 3.753140687942505 \n",
      "     Training Step: 24 Training Loss: 2.799438953399658 \n",
      "     Training Step: 25 Training Loss: 2.6359617710113525 \n",
      "     Training Step: 26 Training Loss: 2.6391191482543945 \n",
      "     Training Step: 27 Training Loss: 3.0226352214813232 \n",
      "     Training Step: 28 Training Loss: 3.18959379196167 \n",
      "     Training Step: 29 Training Loss: 3.199270009994507 \n",
      "     Training Step: 30 Training Loss: 2.133272409439087 \n",
      "     Training Step: 31 Training Loss: 3.0509090423583984 \n",
      "     Training Step: 32 Training Loss: 2.9579033851623535 \n",
      "     Training Step: 33 Training Loss: 3.5375802516937256 \n",
      "     Training Step: 34 Training Loss: 3.7754180431365967 \n",
      "     Training Step: 35 Training Loss: 2.5564520359039307 \n",
      "     Training Step: 36 Training Loss: 3.0461208820343018 \n",
      "     Training Step: 37 Training Loss: 3.419884443283081 \n",
      "     Training Step: 38 Training Loss: 2.4861180782318115 \n",
      "     Training Step: 39 Training Loss: 2.359532356262207 \n",
      "     Training Step: 40 Training Loss: 2.5665206909179688 \n",
      "     Training Step: 41 Training Loss: 3.410984992980957 \n",
      "     Training Step: 42 Training Loss: 2.0673422813415527 \n",
      "     Training Step: 43 Training Loss: 2.1873769760131836 \n",
      "     Training Step: 44 Training Loss: 2.9589924812316895 \n",
      "     Training Step: 45 Training Loss: 3.23801851272583 \n",
      "     Training Step: 46 Training Loss: 3.355760335922241 \n",
      "     Training Step: 47 Training Loss: 2.781277656555176 \n",
      "     Training Step: 48 Training Loss: 2.8946235179901123 \n",
      "     Training Step: 49 Training Loss: 2.969050645828247 \n",
      "     Training Step: 50 Training Loss: 2.8366923332214355 \n",
      "     Training Step: 51 Training Loss: 2.7621686458587646 \n",
      "     Training Step: 52 Training Loss: 2.744326591491699 \n",
      "     Training Step: 53 Training Loss: 3.8569955825805664 \n",
      "     Training Step: 54 Training Loss: 2.991999864578247 \n",
      "     Training Step: 55 Training Loss: 2.7444286346435547 \n",
      "     Training Step: 56 Training Loss: 2.715848207473755 \n",
      "     Training Step: 57 Training Loss: 3.2288572788238525 \n",
      "     Training Step: 58 Training Loss: 2.6201374530792236 \n",
      "     Training Step: 59 Training Loss: 2.508044719696045 \n",
      "     Training Step: 60 Training Loss: 3.256213665008545 \n",
      "     Training Step: 61 Training Loss: 2.3679676055908203 \n",
      "     Training Step: 62 Training Loss: 3.6417887210845947 \n",
      "     Training Step: 63 Training Loss: 2.913808584213257 \n",
      "     Training Step: 64 Training Loss: 2.373244285583496 \n",
      "     Training Step: 65 Training Loss: 4.174419403076172 \n",
      "     Training Step: 66 Training Loss: 3.387328863143921 \n",
      "     Training Step: 67 Training Loss: 3.3940656185150146 \n",
      "     Training Step: 68 Training Loss: 2.5877633094787598 \n",
      "     Training Step: 69 Training Loss: 4.796707630157471 \n",
      "     Training Step: 70 Training Loss: 3.4730257987976074 \n",
      "     Training Step: 71 Training Loss: 3.1623592376708984 \n",
      "     Training Step: 72 Training Loss: 2.5333096981048584 \n",
      "     Training Step: 73 Training Loss: 2.3026559352874756 \n",
      "     Training Step: 74 Training Loss: 2.305558681488037 \n",
      "     Training Step: 75 Training Loss: 3.5059878826141357 \n",
      "     Training Step: 76 Training Loss: 2.1172213554382324 \n",
      "     Training Step: 77 Training Loss: 3.0664968490600586 \n",
      "     Training Step: 78 Training Loss: 2.8103227615356445 \n",
      "     Training Step: 79 Training Loss: 2.3036015033721924 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1147255897521973 \n",
      "     Validation Step: 1 Validation Loss: 3.0191171169281006 \n",
      "     Validation Step: 2 Validation Loss: 3.7175936698913574 \n",
      "     Validation Step: 3 Validation Loss: 3.492964267730713 \n",
      "     Validation Step: 4 Validation Loss: 3.6346864700317383 \n",
      "     Validation Step: 5 Validation Loss: 3.567767381668091 \n",
      "     Validation Step: 6 Validation Loss: 3.811770439147949 \n",
      "     Validation Step: 7 Validation Loss: 2.29551362991333 \n",
      "     Validation Step: 8 Validation Loss: 3.2547178268432617 \n",
      "     Validation Step: 9 Validation Loss: 3.560353994369507 \n",
      "     Validation Step: 10 Validation Loss: 2.904757261276245 \n",
      "     Validation Step: 11 Validation Loss: 2.8481850624084473 \n",
      "     Validation Step: 12 Validation Loss: 2.587007999420166 \n",
      "     Validation Step: 13 Validation Loss: 2.59348726272583 \n",
      "     Validation Step: 14 Validation Loss: 2.984287738800049 \n",
      "Epoch: 119\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.3443241119384766 \n",
      "     Training Step: 1 Training Loss: 2.3782973289489746 \n",
      "     Training Step: 2 Training Loss: 2.693917751312256 \n",
      "     Training Step: 3 Training Loss: 4.337988376617432 \n",
      "     Training Step: 4 Training Loss: 2.915689468383789 \n",
      "     Training Step: 5 Training Loss: 2.2437186241149902 \n",
      "     Training Step: 6 Training Loss: 2.3967041969299316 \n",
      "     Training Step: 7 Training Loss: 2.4045987129211426 \n",
      "     Training Step: 8 Training Loss: 2.1279029846191406 \n",
      "     Training Step: 9 Training Loss: 2.857691764831543 \n",
      "     Training Step: 10 Training Loss: 3.0848872661590576 \n",
      "     Training Step: 11 Training Loss: 3.396183490753174 \n",
      "     Training Step: 12 Training Loss: 2.5701677799224854 \n",
      "     Training Step: 13 Training Loss: 3.94398832321167 \n",
      "     Training Step: 14 Training Loss: 2.823821783065796 \n",
      "     Training Step: 15 Training Loss: 2.5236527919769287 \n",
      "     Training Step: 16 Training Loss: 3.7320163249969482 \n",
      "     Training Step: 17 Training Loss: 2.272620677947998 \n",
      "     Training Step: 18 Training Loss: 2.682178497314453 \n",
      "     Training Step: 19 Training Loss: 3.758547782897949 \n",
      "     Training Step: 20 Training Loss: 2.5161690711975098 \n",
      "     Training Step: 21 Training Loss: 2.6650407314300537 \n",
      "     Training Step: 22 Training Loss: 3.2932426929473877 \n",
      "     Training Step: 23 Training Loss: 3.144105911254883 \n",
      "     Training Step: 24 Training Loss: 2.9515600204467773 \n",
      "     Training Step: 25 Training Loss: 2.2638449668884277 \n",
      "     Training Step: 26 Training Loss: 2.3301165103912354 \n",
      "     Training Step: 27 Training Loss: 3.0524203777313232 \n",
      "     Training Step: 28 Training Loss: 2.2057011127471924 \n",
      "     Training Step: 29 Training Loss: 2.9324142932891846 \n",
      "     Training Step: 30 Training Loss: 2.828829765319824 \n",
      "     Training Step: 31 Training Loss: 3.3273556232452393 \n",
      "     Training Step: 32 Training Loss: 3.2890725135803223 \n",
      "     Training Step: 33 Training Loss: 3.497802972793579 \n",
      "     Training Step: 34 Training Loss: 3.0948591232299805 \n",
      "     Training Step: 35 Training Loss: 3.060394048690796 \n",
      "     Training Step: 36 Training Loss: 2.5205585956573486 \n",
      "     Training Step: 37 Training Loss: 3.150892734527588 \n",
      "     Training Step: 38 Training Loss: 3.5374817848205566 \n",
      "     Training Step: 39 Training Loss: 2.467552661895752 \n",
      "     Training Step: 40 Training Loss: 3.2734904289245605 \n",
      "     Training Step: 41 Training Loss: 2.194334030151367 \n",
      "     Training Step: 42 Training Loss: 3.553924083709717 \n",
      "     Training Step: 43 Training Loss: 2.812267541885376 \n",
      "     Training Step: 44 Training Loss: 2.6738240718841553 \n",
      "     Training Step: 45 Training Loss: 3.1672093868255615 \n",
      "     Training Step: 46 Training Loss: 3.2831621170043945 \n",
      "     Training Step: 47 Training Loss: 2.544614791870117 \n",
      "     Training Step: 48 Training Loss: 2.9051804542541504 \n",
      "     Training Step: 49 Training Loss: 3.3119606971740723 \n",
      "     Training Step: 50 Training Loss: 2.998565673828125 \n",
      "     Training Step: 51 Training Loss: 4.29559850692749 \n",
      "     Training Step: 52 Training Loss: 2.979539394378662 \n",
      "     Training Step: 53 Training Loss: 2.3454740047454834 \n",
      "     Training Step: 54 Training Loss: 3.712247133255005 \n",
      "     Training Step: 55 Training Loss: 3.659151077270508 \n",
      "     Training Step: 56 Training Loss: 2.971073865890503 \n",
      "     Training Step: 57 Training Loss: 3.4549670219421387 \n",
      "     Training Step: 58 Training Loss: 2.9371132850646973 \n",
      "     Training Step: 59 Training Loss: 2.6655631065368652 \n",
      "     Training Step: 60 Training Loss: 4.024302959442139 \n",
      "     Training Step: 61 Training Loss: 2.088395118713379 \n",
      "     Training Step: 62 Training Loss: 4.070937156677246 \n",
      "     Training Step: 63 Training Loss: 2.758918285369873 \n",
      "     Training Step: 64 Training Loss: 3.451594352722168 \n",
      "     Training Step: 65 Training Loss: 2.148129463195801 \n",
      "     Training Step: 66 Training Loss: 4.707956790924072 \n",
      "     Training Step: 67 Training Loss: 2.3271262645721436 \n",
      "     Training Step: 68 Training Loss: 3.3669896125793457 \n",
      "     Training Step: 69 Training Loss: 3.221914768218994 \n",
      "     Training Step: 70 Training Loss: 2.9770069122314453 \n",
      "     Training Step: 71 Training Loss: 2.3546454906463623 \n",
      "     Training Step: 72 Training Loss: 2.9626877307891846 \n",
      "     Training Step: 73 Training Loss: 3.362628936767578 \n",
      "     Training Step: 74 Training Loss: 2.927015781402588 \n",
      "     Training Step: 75 Training Loss: 3.3729238510131836 \n",
      "     Training Step: 76 Training Loss: 2.9957199096679688 \n",
      "     Training Step: 77 Training Loss: 3.1210641860961914 \n",
      "     Training Step: 78 Training Loss: 2.5691945552825928 \n",
      "     Training Step: 79 Training Loss: 2.768237352371216 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6733455657958984 \n",
      "     Validation Step: 1 Validation Loss: 3.019639492034912 \n",
      "     Validation Step: 2 Validation Loss: 3.272742509841919 \n",
      "     Validation Step: 3 Validation Loss: 2.9232053756713867 \n",
      "     Validation Step: 4 Validation Loss: 3.38655424118042 \n",
      "     Validation Step: 5 Validation Loss: 2.2692501544952393 \n",
      "     Validation Step: 6 Validation Loss: 3.1075360774993896 \n",
      "     Validation Step: 7 Validation Loss: 3.1542763710021973 \n",
      "     Validation Step: 8 Validation Loss: 3.624091386795044 \n",
      "     Validation Step: 9 Validation Loss: 2.7081170082092285 \n",
      "     Validation Step: 10 Validation Loss: 3.0489721298217773 \n",
      "     Validation Step: 11 Validation Loss: 3.6231274604797363 \n",
      "     Validation Step: 12 Validation Loss: 3.8984642028808594 \n",
      "     Validation Step: 13 Validation Loss: 3.6296353340148926 \n",
      "     Validation Step: 14 Validation Loss: 2.6839590072631836 \n",
      "Epoch: 120\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.8184571266174316 \n",
      "     Training Step: 1 Training Loss: 2.7674596309661865 \n",
      "     Training Step: 2 Training Loss: 3.4459853172302246 \n",
      "     Training Step: 3 Training Loss: 4.399515151977539 \n",
      "     Training Step: 4 Training Loss: 2.3217082023620605 \n",
      "     Training Step: 5 Training Loss: 3.232769727706909 \n",
      "     Training Step: 6 Training Loss: 2.256201982498169 \n",
      "     Training Step: 7 Training Loss: 2.7522172927856445 \n",
      "     Training Step: 8 Training Loss: 3.6892247200012207 \n",
      "     Training Step: 9 Training Loss: 3.0006022453308105 \n",
      "     Training Step: 10 Training Loss: 3.7134017944335938 \n",
      "     Training Step: 11 Training Loss: 2.522428512573242 \n",
      "     Training Step: 12 Training Loss: 2.5861051082611084 \n",
      "     Training Step: 13 Training Loss: 2.393810510635376 \n",
      "     Training Step: 14 Training Loss: 2.4002747535705566 \n",
      "     Training Step: 15 Training Loss: 2.5161662101745605 \n",
      "     Training Step: 16 Training Loss: 4.060181617736816 \n",
      "     Training Step: 17 Training Loss: 2.6435325145721436 \n",
      "     Training Step: 18 Training Loss: 2.1327714920043945 \n",
      "     Training Step: 19 Training Loss: 3.3853721618652344 \n",
      "     Training Step: 20 Training Loss: 3.7346372604370117 \n",
      "     Training Step: 21 Training Loss: 2.454202175140381 \n",
      "     Training Step: 22 Training Loss: 2.099893093109131 \n",
      "     Training Step: 23 Training Loss: 4.218985080718994 \n",
      "     Training Step: 24 Training Loss: 2.1746931076049805 \n",
      "     Training Step: 25 Training Loss: 2.9654293060302734 \n",
      "     Training Step: 26 Training Loss: 3.4265940189361572 \n",
      "     Training Step: 27 Training Loss: 2.872528553009033 \n",
      "     Training Step: 28 Training Loss: 2.801743984222412 \n",
      "     Training Step: 29 Training Loss: 2.5411057472229004 \n",
      "     Training Step: 30 Training Loss: 3.622469902038574 \n",
      "     Training Step: 31 Training Loss: 3.35349178314209 \n",
      "     Training Step: 32 Training Loss: 2.562302589416504 \n",
      "     Training Step: 33 Training Loss: 3.2368721961975098 \n",
      "     Training Step: 34 Training Loss: 2.923961877822876 \n",
      "     Training Step: 35 Training Loss: 2.9005837440490723 \n",
      "     Training Step: 36 Training Loss: 2.950882911682129 \n",
      "     Training Step: 37 Training Loss: 3.169602394104004 \n",
      "     Training Step: 38 Training Loss: 3.487182140350342 \n",
      "     Training Step: 39 Training Loss: 3.1479525566101074 \n",
      "     Training Step: 40 Training Loss: 2.3862578868865967 \n",
      "     Training Step: 41 Training Loss: 2.6191916465759277 \n",
      "     Training Step: 42 Training Loss: 2.8667616844177246 \n",
      "     Training Step: 43 Training Loss: 2.1541309356689453 \n",
      "     Training Step: 44 Training Loss: 3.410121440887451 \n",
      "     Training Step: 45 Training Loss: 2.879927635192871 \n",
      "     Training Step: 46 Training Loss: 3.0943961143493652 \n",
      "     Training Step: 47 Training Loss: 2.939063310623169 \n",
      "     Training Step: 48 Training Loss: 2.736440420150757 \n",
      "     Training Step: 49 Training Loss: 2.1512720584869385 \n",
      "     Training Step: 50 Training Loss: 3.412177085876465 \n",
      "     Training Step: 51 Training Loss: 2.204451084136963 \n",
      "     Training Step: 52 Training Loss: 3.755263328552246 \n",
      "     Training Step: 53 Training Loss: 3.0738682746887207 \n",
      "     Training Step: 54 Training Loss: 2.936056137084961 \n",
      "     Training Step: 55 Training Loss: 3.4027888774871826 \n",
      "     Training Step: 56 Training Loss: 3.3913867473602295 \n",
      "     Training Step: 57 Training Loss: 4.736418724060059 \n",
      "     Training Step: 58 Training Loss: 3.5709056854248047 \n",
      "     Training Step: 59 Training Loss: 3.892063856124878 \n",
      "     Training Step: 60 Training Loss: 2.777259349822998 \n",
      "     Training Step: 61 Training Loss: 3.19533634185791 \n",
      "     Training Step: 62 Training Loss: 2.9460530281066895 \n",
      "     Training Step: 63 Training Loss: 2.9682533740997314 \n",
      "     Training Step: 64 Training Loss: 3.6004772186279297 \n",
      "     Training Step: 65 Training Loss: 2.760150909423828 \n",
      "     Training Step: 66 Training Loss: 4.445935249328613 \n",
      "     Training Step: 67 Training Loss: 3.3407087326049805 \n",
      "     Training Step: 68 Training Loss: 2.65269136428833 \n",
      "     Training Step: 69 Training Loss: 2.1738007068634033 \n",
      "     Training Step: 70 Training Loss: 3.171143054962158 \n",
      "     Training Step: 71 Training Loss: 2.4115469455718994 \n",
      "     Training Step: 72 Training Loss: 3.09187912940979 \n",
      "     Training Step: 73 Training Loss: 3.08156156539917 \n",
      "     Training Step: 74 Training Loss: 2.4558913707733154 \n",
      "     Training Step: 75 Training Loss: 2.122551202774048 \n",
      "     Training Step: 76 Training Loss: 2.921473741531372 \n",
      "     Training Step: 77 Training Loss: 3.0631933212280273 \n",
      "     Training Step: 78 Training Loss: 2.3566360473632812 \n",
      "     Training Step: 79 Training Loss: 2.889777660369873 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6084280014038086 \n",
      "     Validation Step: 1 Validation Loss: 3.2343337535858154 \n",
      "     Validation Step: 2 Validation Loss: 3.1725916862487793 \n",
      "     Validation Step: 3 Validation Loss: 3.5506176948547363 \n",
      "     Validation Step: 4 Validation Loss: 2.6643166542053223 \n",
      "     Validation Step: 5 Validation Loss: 3.055948257446289 \n",
      "     Validation Step: 6 Validation Loss: 3.697077751159668 \n",
      "     Validation Step: 7 Validation Loss: 2.2536826133728027 \n",
      "     Validation Step: 8 Validation Loss: 3.704617500305176 \n",
      "     Validation Step: 9 Validation Loss: 3.975788116455078 \n",
      "     Validation Step: 10 Validation Loss: 3.033381938934326 \n",
      "     Validation Step: 11 Validation Loss: 2.691626787185669 \n",
      "     Validation Step: 12 Validation Loss: 2.9022395610809326 \n",
      "     Validation Step: 13 Validation Loss: 3.058525323867798 \n",
      "     Validation Step: 14 Validation Loss: 3.484422206878662 \n"
     ]
    }
   ],
   "source": [
    "for i in range(61, 120, 20) :\n",
    "    start_epoch = i\n",
    "    num_epochs = 20 + i\n",
    "    loss_weights = (1.0, 1.0, 1.0)\n",
    "    model_name = \"base_model_unsupervised_physics_constrained.pt\"\n",
    "    train_input,train_output,val_input, val_output = train_validate_ACOPF(model, model_name, optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-593.8486800193787 -243.53982478380203\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 9.08173089e-01, -1.83501959e+00, -5.90927734e+01,\n        -4.32426147e+01],\n       [ 9.80060136e-01,  0.00000000e+00,  9.09702148e+01,\n        -3.20258598e+01],\n       [ 1.03807630e+00, -1.48351908e+00, -5.60315399e+01,\n        -4.20662727e+01],\n       [ 1.03758441e+00,  1.54328156e+00, -8.17986012e-01,\n         7.37901390e-01],\n       [ 1.02849822e+00,  1.60070038e+00,  2.79828215e+00,\n         8.60434711e-01],\n       [ 1.02379331e+00,  1.61828518e+00,  2.95484495e+00,\n         1.07658315e+00],\n       [ 1.05319075e+00,  1.20048451e+00, -6.75554800e+00,\n        -1.33368182e+00],\n       [ 1.04970037e+00,  9.24345732e-01, -1.38833284e+01,\n        -2.18041611e+00],\n       [ 1.05298337e+00,  1.13721991e+00, -8.29765701e+00,\n        -1.68316507e+00],\n       [ 1.03909489e+00,  2.39133453e+00,  2.30472603e+01,\n         6.45330811e+00],\n       [ 1.06325094e+00, -8.12291145e-01, -5.12167511e+01,\n        -1.27561131e+01],\n       [ 1.06275891e+00,  5.54805279e-01, -2.24758530e+01,\n        -7.71057844e+00],\n       [ 1.11087286e+00, -1.08034587e+00, -5.49251671e+01,\n        -2.01222477e+01],\n       [ 1.04399310e+00,  6.38606310e-01, -2.13300972e+01,\n        -3.76218772e+00],\n       [ 1.03638708e+00,  1.57836246e+00, -1.58142567e-01,\n        -2.26446450e-01],\n       [ 1.11484070e+00,  2.71129608e+00,  3.74011345e+01,\n         1.65829735e+01],\n       [ 1.04800790e+00,  1.04877901e+00, -1.08388004e+01,\n        -9.84343350e-01],\n       [ 1.06187564e+00,  4.20533657e-01, -2.57897511e+01,\n        -8.35899353e+00],\n       [ 1.04667220e+00,  8.54233503e-01, -1.57999144e+01,\n        -2.50964928e+00],\n       [ 1.06539757e+00, -6.87378645e-01, -4.90792694e+01,\n        -1.07747526e+01],\n       [ 1.02700549e+00,  1.60033560e+00,  2.77895689e+00,\n         9.69036281e-01],\n       [ 1.03543417e+00,  1.56838799e+00, -4.76727962e-01,\n         2.44593799e-01],\n       [ 1.02984647e+00,  1.59551716e+00,  2.70907640e+00,\n         8.00673664e-01],\n       [ 1.06586664e+00,  5.02006054e-01, -2.36038208e+01,\n        -8.46225548e+00],\n       [ 1.04963691e+00,  1.20602751e+00, -7.01246834e+00,\n        -4.66603100e-01],\n       [ 1.03440482e+00,  1.59806633e+00, -1.05355740e-01,\n        -1.30631268e-01],\n       [ 1.09338767e+00,  2.67419767e+00,  3.54734154e+01,\n         1.68675766e+01],\n       [ 1.05735051e+00,  4.46565151e-02, -3.45174599e+01,\n        -1.08645945e+01],\n       [ 1.04879539e+00,  1.21448874e+00, -6.53895283e+00,\n        -1.32367253e+00],\n       [ 1.04342277e+00,  1.09987926e+00, -9.51759434e+00,\n        -2.23112369e+00],\n       [ 1.10490709e+00, -3.70104313e-02, -3.47962761e+01,\n        -1.86017208e+01],\n       [ 1.04623039e+00,  1.01613450e+00, -1.17663660e+01,\n        -1.00340390e+00],\n       [ 1.02900515e+00,  1.59984922e+00,  2.79258871e+00,\n         8.08014095e-01],\n       [ 1.02530927e+00,  1.62475967e+00,  3.26003313e+00,\n         1.46038699e+00],\n       [ 1.02368844e+00,  1.60961866e+00,  3.02610922e+00,\n         1.62239432e+00],\n       [ 1.06201997e+00, -9.00850296e-02, -3.73605499e+01,\n        -1.08894148e+01],\n       [ 1.03693182e+00,  1.56488276e+00, -3.30797195e-01,\n        -6.23310208e-02],\n       [ 1.06338501e+00,  1.99042082e-01, -3.09224625e+01,\n        -9.73800850e+00],\n       [ 1.04240612e+00,  1.27646208e+00, -5.38701153e+00,\n        -8.20039093e-01],\n       [ 1.04953565e+00,  1.17001128e+00, -7.57848072e+00,\n        -1.56568027e+00],\n       [ 1.06356250e+00,  5.47959566e-01, -2.25301609e+01,\n        -8.53936100e+00],\n       [ 1.05183709e+00,  1.23778319e+00, -5.87703037e+00,\n        -1.50604749e+00],\n       [ 1.05130449e+00,  1.22794557e+00, -6.15515661e+00,\n        -1.32155252e+00],\n       [ 1.05308221e+00,  1.15811086e+00, -7.79037762e+00,\n        -1.58836484e+00],\n       [ 1.04220928e+00,  6.41660690e-01, -2.13405972e+01,\n        -3.74810171e+00],\n       [ 1.04101694e+00,  3.45329285e-01, -2.85472622e+01,\n        -3.89841413e+00],\n       [ 1.05179631e+00,  1.21071792e+00, -6.54533386e+00,\n        -1.52291942e+00],\n       [ 1.04534447e+00,  2.38566589e+00,  2.35531597e+01,\n         7.09362507e+00],\n       [ 1.05292761e+00,  1.19509006e+00, -6.91728497e+00,\n        -1.41930890e+00],\n       [ 1.03545109e+00,  2.31351423e+00,  2.11422005e+01,\n         6.24062538e+00],\n       [ 1.02475579e+00,  1.61665726e+00,  3.06926107e+00,\n         1.43117452e+00],\n       [ 1.02805141e+00,  1.60891628e+00,  2.97897768e+00,\n         1.07247901e+00],\n       [ 1.05195403e+00,  1.03628588e+00, -1.08744020e+01,\n        -1.90766454e+00],\n       [ 1.05946364e+00,  4.51036692e-01, -2.51464062e+01,\n        -8.16252136e+00],\n       [ 1.05379444e+00,  1.13885927e+00, -8.23964405e+00,\n        -1.66914129e+00],\n       [ 1.04030803e+00,  3.25567722e-01, -2.89689369e+01,\n        -5.03463173e+00],\n       [ 1.05277446e+00,  1.11298800e+00, -8.89925766e+00,\n        -1.62536550e+00],\n       [ 1.05321558e+00,  1.16000962e+00, -7.73745680e+00,\n        -1.55750155e+00],\n       [ 1.02827717e+00,  1.60002565e+00,  2.76085043e+00,\n         8.79548252e-01],\n       [ 1.05259497e+00,  1.20167589e+00, -6.75953770e+00,\n        -1.40296769e+00],\n       [ 1.05170781e+00,  9.95320559e-01, -1.19557323e+01,\n        -1.98313093e+00],\n       [ 1.05246201e+00,  1.10347557e+00, -9.11016560e+00,\n        -1.74773812e+00],\n       [ 1.04958122e+00,  8.82838488e-01, -1.49465609e+01,\n        -2.44760275e+00],\n       [ 1.05187961e+00,  1.07718349e+00, -9.81683922e+00,\n        -1.76201677e+00]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(train_output.output[:, 2]), sum(train_output.output[:, 3]))\n",
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-511.95279839634895 -215.83535793423653\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 9.07937301e-01, -1.87421203e+00, -5.86987801e+01,\n        -4.31370201e+01],\n       [ 9.78537469e-01,  0.00000000e+00,  9.15387573e+01,\n        -3.18711929e+01],\n       [ 1.03759433e+00, -1.51700187e+00, -5.56474800e+01,\n        -4.19558372e+01],\n       [ 1.03786753e+00,  1.56929421e+00, -2.91552931e-01,\n         5.73150635e-01],\n       [ 1.02684687e+00,  1.61438823e+00,  2.79138613e+00,\n         9.32665348e-01],\n       [ 1.02212753e+00,  1.63119197e+00,  2.91438818e+00,\n         1.13415098e+00],\n       [ 1.05419103e+00,  1.20319247e+00, -6.55964136e+00,\n        -1.39674544e+00],\n       [ 1.05039056e+00,  9.21392202e-01, -1.37815561e+01,\n        -2.24824166e+00],\n       [ 1.05342338e+00,  1.15047312e+00, -7.85218763e+00,\n        -1.60835695e+00],\n       [ 1.03538874e+00,  2.41840339e+00,  2.30585728e+01,\n         6.94025660e+00],\n       [ 1.06390915e+00, -8.37588310e-01, -5.08798523e+01,\n        -1.26762056e+01],\n       [ 1.06576177e+00,  5.09796381e-01, -2.31083965e+01,\n        -8.29451561e+00],\n       [ 1.10853521e+00, -1.09562016e+00, -5.43942871e+01,\n        -1.96498795e+01],\n       [ 1.04392242e+00,  6.43275738e-01, -2.09690762e+01,\n        -3.62716198e+00],\n       [ 1.03664391e+00,  1.60454869e+00,  3.56376737e-01,\n        -3.78850937e-01],\n       [ 1.11313026e+00,  2.74871421e+00,  3.75459595e+01,\n         1.64656582e+01],\n       [ 1.04838319e+00,  1.05062175e+00, -1.06897240e+01,\n        -9.46598053e-01],\n       [ 1.06335775e+00,  4.12539959e-01, -2.55640736e+01,\n        -8.69000244e+00],\n       [ 1.04673282e+00,  8.57007980e-01, -1.55750837e+01,\n        -2.40364528e+00],\n       [ 1.06625338e+00, -7.12612152e-01, -4.87778969e+01,\n        -1.07718163e+01],\n       [ 1.02520024e+00,  1.60813451e+00,  2.57109118e+00,\n         8.72414589e-01],\n       [ 1.03570543e+00,  1.59455371e+00,  4.24204767e-02,\n         8.58158916e-02],\n       [ 1.02876060e+00,  1.61393094e+00,  2.87852621e+00,\n         9.05761242e-01],\n       [ 1.06686214e+00,  4.90499735e-01, -2.35167732e+01,\n        -8.59967327e+00],\n       [ 1.05005965e+00,  1.21517229e+00, -6.70573092e+00,\n        -3.89456272e-01],\n       [ 1.03465972e+00,  1.62460160e+00,  4.10573572e-01,\n        -2.82017231e-01],\n       [ 1.10395910e+00,  2.77285409e+00,  3.75991096e+01,\n         1.66821957e+01],\n       [ 1.05832637e+00,  1.22981071e-02, -3.46597633e+01,\n        -1.07392855e+01],\n       [ 1.04958635e+00,  1.21394610e+00, -6.45953321e+00,\n        -1.32239532e+00],\n       [ 1.04360254e+00,  1.10471535e+00, -9.33998680e+00,\n        -2.11040664e+00],\n       [ 1.03382131e+00,  2.90489364e+00,  3.31208191e+01,\n         8.79098511e+00],\n       [ 1.04661796e+00,  1.01696968e+00, -1.16286840e+01,\n        -9.67914581e-01],\n       [ 1.02757374e+00,  1.61341977e+00,  2.78863668e+00,\n         8.24258327e-01],\n       [ 1.02408746e+00,  1.63723826e+00,  3.22514486e+00,\n         1.43825531e+00],\n       [ 1.02224260e+00,  1.62494397e+00,  3.07503557e+00,\n         1.71696568e+00],\n       [ 1.06244354e+00, -9.74833965e-02, -3.69519081e+01,\n        -1.09176979e+01],\n       [ 1.03718948e+00,  1.59092259e+00,  1.84925169e-01,\n        -2.17633739e-01],\n       [ 1.06213760e+00,  2.03928709e-01, -3.04188366e+01,\n        -9.28130627e+00],\n       [ 1.04291569e+00,  1.28545308e+00, -5.10510206e+00,\n        -7.30353355e-01],\n       [ 1.05034270e+00,  1.17709088e+00, -7.27609539e+00,\n        -1.56540537e+00],\n       [ 1.06302365e+00,  5.46265602e-01, -2.23000622e+01,\n        -8.24017239e+00],\n       [ 1.05251389e+00,  1.24390006e+00, -5.63780022e+00,\n        -1.48716426e+00],\n       [ 1.05183092e+00,  1.22932982e+00, -6.05599022e+00,\n        -1.26505089e+00],\n       [ 1.05366565e+00,  1.16302705e+00, -7.56571531e+00,\n        -1.54746771e+00],\n       [ 1.04278280e+00,  6.29742146e-01, -2.13358345e+01,\n        -3.73330069e+00],\n       [ 1.04147651e+00,  3.35354567e-01, -2.83615379e+01,\n        -3.88021851e+00],\n       [ 1.05260807e+00,  1.21566653e+00, -6.31445551e+00,\n        -1.53221798e+00],\n       [ 1.03565771e+00,  2.35670972e+00,  2.17177143e+01,\n         6.64623117e+00],\n       [ 1.05362105e+00,  1.19829369e+00, -6.74004698e+00,\n        -1.40678382e+00],\n       [ 1.03633076e+00,  2.34963393e+00,  2.15431976e+01,\n         6.00116968e+00],\n       [ 1.02374913e+00,  1.63063979e+00,  3.08254194e+00,\n         1.36198950e+00],\n       [ 1.02660273e+00,  1.62159944e+00,  2.94081450e+00,\n         1.06041336e+00],\n       [ 1.05243177e+00,  1.03496718e+00, -1.07991285e+01,\n        -1.89943600e+00],\n       [ 1.05596931e+00,  4.97182846e-01, -2.37191906e+01,\n        -8.76403713e+00],\n       [ 1.05413430e+00,  1.14878464e+00, -7.89678144e+00,\n        -1.56792021e+00],\n       [ 1.04069984e+00,  3.14863920e-01, -2.87940044e+01,\n        -4.97088480e+00],\n       [ 1.05342858e+00,  1.11741328e+00, -8.65817070e+00,\n        -1.60007501e+00],\n       [ 1.05359691e+00,  1.16640639e+00, -7.49538946e+00,\n        -1.46536803e+00],\n       [ 1.02733265e+00,  1.61643720e+00,  2.85800314e+00,\n         9.82812881e-01],\n       [ 1.05317327e+00,  1.20813537e+00, -6.50923920e+00,\n        -1.36070895e+00],\n       [ 1.05211938e+00,  9.96773243e-01, -1.17909842e+01,\n        -1.98481584e+00],\n       [ 1.05259060e+00,  1.11171269e+00, -8.83128929e+00,\n        -1.61827540e+00],\n       [ 1.05000777e+00,  8.75979900e-01, -1.49445992e+01,\n        -2.42398238e+00],\n       [ 1.05225712e+00,  1.08231473e+00, -9.59457111e+00,\n        -1.72301269e+00]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(val_output.output[:, 2]), sum(val_output.output[:, 3]))\n",
    "val_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def save_ACOPFGNN_model(model: ACOPFGNN):\n",
    "    path = r\"./Models/SelfSupervised/embedder_model.pt\"\n",
    "\n",
    "    torch.save(model.state_dict(), path)\n",
    "save_ACOPFGNN_model(embedder_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CHAINED ACOPF MODELS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'PQ': tensor([[ 0.0357,  0.2132],\n",
      "        [ 0.3546, -0.4051],\n",
      "        [ 0.3367,  0.7079],\n",
      "        [ 0.6779,  1.2298],\n",
      "        [ 0.2732, -0.7593],\n",
      "        [ 0.2899,  0.8922],\n",
      "        [-0.0812, -0.6637],\n",
      "        [ 0.0920, -0.6473],\n",
      "        [ 0.3620,  0.6066],\n",
      "        [-0.6855, -0.7911],\n",
      "        [ 0.0236, -0.9724],\n",
      "        [ 0.3322,  0.8732],\n",
      "        [ 0.2427, -0.2146],\n",
      "        [ 0.1472, -0.0646],\n",
      "        [-0.0681,  0.3301],\n",
      "        [ 0.0695,  0.5528]], grad_fn=<EluBackward0>), 'NB': tensor([[ 1.0562,  1.2268],\n",
      "        [ 0.6800,  0.3162],\n",
      "        [ 0.5115,  0.8864],\n",
      "        [ 2.7919, -0.3620],\n",
      "        [ 0.6721, -0.0118]], grad_fn=<EluBackward0>), 'SB': tensor([[-0.1552, -0.5344]], grad_fn=<EluBackward0>), 'PV': tensor([[ 3.4214,  1.1388],\n",
      "        [ 2.3178,  0.5479],\n",
      "        [ 0.3299, -0.1493],\n",
      "        [-0.0295, -0.3002],\n",
      "        [-0.3908, -0.2448],\n",
      "        [ 1.1586,  4.4030],\n",
      "        [-0.2975, -0.2438],\n",
      "        [-0.3383,  1.8400],\n",
      "        [-0.6998, -0.0276],\n",
      "        [-0.3816, -0.2446],\n",
      "        [-0.2706, -0.2434],\n",
      "        [ 1.2707,  0.5701],\n",
      "        [-0.0187, -0.2390],\n",
      "        [-0.2923, -0.2435],\n",
      "        [-0.6921,  0.1382],\n",
      "        [-0.5824,  1.6911],\n",
      "        [ 2.3753,  1.3654],\n",
      "        [ 0.0067,  1.0301],\n",
      "        [ 0.3998, -0.0298],\n",
      "        [ 0.7874,  0.0520],\n",
      "        [-0.1209, -0.4281],\n",
      "        [ 0.7071,  2.0727],\n",
      "        [-0.3914,  0.0476],\n",
      "        [-0.6157, -0.1280],\n",
      "        [-0.6435, -0.2525],\n",
      "        [ 1.0033,  0.4131],\n",
      "        [-0.1723, -0.3562],\n",
      "        [ 0.1300,  1.1046],\n",
      "        [ 1.0296,  0.3297],\n",
      "        [-0.6041, -0.2151],\n",
      "        [-0.1206, -0.3228],\n",
      "        [-0.3227, -0.4567],\n",
      "        [ 0.2995,  1.7237],\n",
      "        [-0.4423, -0.5175],\n",
      "        [ 2.7898,  1.6096],\n",
      "        [-0.4074, -0.4993],\n",
      "        [-0.2743, -0.4209],\n",
      "        [ 0.0134, -0.2476],\n",
      "        [-0.3167, -0.4679],\n",
      "        [-0.3848, -0.2444],\n",
      "        [-0.3569, -0.2440],\n",
      "        [-0.3808, -0.2445]], grad_fn=<EluBackward0>)})\n"
     ]
    }
   ],
   "source": [
    "def load_ACOPFGNN_model(grid_name, model_name, hidden_channels, num_layers):\n",
    "    path = r\"./Models/SelfSupervised/\" + model_name\n",
    "    index_mappers, net, data = generate_unsupervised_input(grid_name)\n",
    "    model = create_ACOPFEmbedder_model(data, net, index_mappers, hidden_channels=hidden_channels, num_layers=num_layers)\n",
    "\n",
    "    # Load the saved model parameter\n",
    "    ordered_dict = torch.load(path)\n",
    "    model.load_state_dict(ordered_dict)\n",
    "\n",
    "    return model\n",
    "embedder_model = load_ACOPFGNN_model(grid_name, \"embedder_model.pt\",8,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'PQ': tensor([[ 0.7643, -0.2924],\n",
      "        [ 1.3948, -0.3719],\n",
      "        [ 0.4723, -0.4887],\n",
      "        [ 0.1594, -0.6713],\n",
      "        [ 2.5409,  0.4687],\n",
      "        [ 0.3421, -0.3662],\n",
      "        [ 1.0452,  1.2068],\n",
      "        [ 0.6408, -0.3298],\n",
      "        [-0.0715,  1.8799],\n",
      "        [ 5.0214,  1.0806],\n",
      "        [ 0.2718, -0.5127],\n",
      "        [ 1.3767, -0.3152],\n",
      "        [ 1.6039,  0.1640],\n",
      "        [ 0.3523, -0.2573],\n",
      "        [ 0.7157, -0.2564]], grad_fn=<EluBackward0>), 'NB': tensor([[-0.4999, -0.8842],\n",
      "        [ 0.3462, -0.4735],\n",
      "        [-0.3032, -0.8039],\n",
      "        [-0.2253,  1.3591],\n",
      "        [ 0.3844, -0.4709]], grad_fn=<EluBackward0>), 'SB': tensor([[0.9981, 0.0247]], grad_fn=<EluBackward0>), 'PV': tensor([[-0.3481, -0.5532],\n",
      "        [-0.2016, -0.3629],\n",
      "        [-0.1346, -0.4042],\n",
      "        [ 0.2581,  0.2171],\n",
      "        [ 0.1846,  0.1537],\n",
      "        [-0.7190, -0.9295],\n",
      "        [ 0.1374,  0.1848],\n",
      "        [-0.3030, -0.5192],\n",
      "        [ 0.1089, -0.0378],\n",
      "        [ 0.1797,  0.1571],\n",
      "        [ 0.1269,  0.1872],\n",
      "        [-0.0640,  0.3275],\n",
      "        [ 0.0335,  0.2438],\n",
      "        [ 0.1382,  0.1795],\n",
      "        [-0.5671,  0.3378],\n",
      "        [-0.5485, -0.5303],\n",
      "        [-0.2615,  0.1934],\n",
      "        [ 0.1412,  0.1804],\n",
      "        [ 0.0405, -0.1230],\n",
      "        [ 0.1490,  0.2565],\n",
      "        [ 0.0336,  0.3137],\n",
      "        [ 0.2742,  0.2181],\n",
      "        [-0.1264, -0.3965],\n",
      "        [ 0.2741, -0.0746],\n",
      "        [ 0.1588,  0.1205],\n",
      "        [ 0.2517,  0.0577],\n",
      "        [ 0.0193,  0.2104],\n",
      "        [ 0.3054,  0.1847],\n",
      "        [-0.0227, -0.3983],\n",
      "        [-0.0235,  0.3416],\n",
      "        [ 0.2585,  0.0489],\n",
      "        [ 0.2903,  0.1907],\n",
      "        [ 0.3579,  0.1495],\n",
      "        [-0.0742,  0.2750],\n",
      "        [ 0.4062,  0.1444],\n",
      "        [-0.3884,  0.5806],\n",
      "        [ 0.3892,  0.1553],\n",
      "        [ 0.3457,  0.1198],\n",
      "        [ 0.2493,  0.2188],\n",
      "        [ 0.3739,  0.0650],\n",
      "        [ 0.1811,  0.1570],\n",
      "        [ 0.1674,  0.1642],\n",
      "        [ 0.1794,  0.1573]], grad_fn=<EluBackward0>)})\n"
     ]
    }
   ],
   "source": [
    "# Create Minimizer, Enforcer and Embedder Models\n",
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "index_mappers,net,data = generate_unsupervised_input(grid_name)\n",
    "#minimizer_model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=1)\n",
    "#embedder_model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=1)\n",
    "\n",
    "#minimizer_model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=4)\n",
    "\n",
    "embedder_model = create_ACOPFEmbedder_model(data, net, index_mappers, hidden_channels=8, num_layers=1)\n",
    "\n",
    "#enforcer_model = create_ACOPFEnforcer_model(data, net, index_mappers, hidden_channels=4, num_layers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Initialize the Optimizer\n",
    "from itertools import chain\n",
    "#optimizer = torch.optim.Adam(chain(minimizer_model.parameters(), enforcer_model.parameters(), embedder_model.parameters()))\n",
    "ACOPF_optimizer = torch.optim.Adam(embedder_model.parameters(), lr=1e-3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "ACOPF_optimizer.param_groups[0]['lr']*=0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mcanbay96\u001B[0m (\u001B[33mcbml\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\wandb\\run-20231019_131118-u34ymjl1</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/u34ymjl1' target=\"_blank\">deft-cherry-258</a></strong> to <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/u34ymjl1' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/u34ymjl1</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_NOTEBOOK_NAME = \"msi\"\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"ACOPF-GNN-SelfSupervised-Hetero\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            #\"learning_rate\": optimizer.,\n",
    "            \"architecture\": \"Hetero-SelfSupervised-GNN\",\n",
    "            \"grid_name\": grid_name,\n",
    "            \"Training Type\": \"Chained Models\"\n",
    "\n",
    "        }\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Load Inputs\n",
    "inputs = load_unsupervised_inputs(grid_name)\n",
    "#inputs = load_multiple_unsupervised_inputs()\n",
    "n = len(inputs)\n",
    "train_inputs = inputs[:int(n*0.8)]\n",
    "val_inputs = inputs[int(n*0.8): int(n*0.95)]\n",
    "test_inputs = inputs[int(n*0.95):]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= 0.3622663861472244 --- Target= -8.159233733096016e-08\n",
      "Q: Output= 2.1193104213935827 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 0 Training Loss: 0.21346211433410645 \n",
      "P: Output= 0.8204967933171465 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 2.2066302509560902 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 1 Training Loss: 0.235146164894104 \n",
      "P: Output= -0.07886565718975191 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= 0.49240111723549873 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 2 Training Loss: 0.22146496176719666 \n",
      "P: Output= 0.5511021996207015 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 1.6426029632760901 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 3 Training Loss: 0.23362955451011658 \n",
      "P: Output= -0.29034264784450414 --- Target= 2.191291796904693e-07\n",
      "Q: Output= 0.48771047467349415 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 4 Training Loss: 0.22026273608207703 \n",
      "P: Output= 0.5379776862522574 --- Target= 6.895684645513711e-08\n",
      "Q: Output= 1.7646526520878476 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 5 Training Loss: 0.22893589735031128 \n",
      "P: Output= 0.49558506945809455 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 2.1185287791089573 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 6 Training Loss: 0.22634091973304749 \n",
      "P: Output= 0.7557665182127637 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 2.661888875313794 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 7 Training Loss: 0.22613310813903809 \n",
      "P: Output= 0.16354326731387214 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 1.7588495440711567 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 8 Training Loss: 0.21387557685375214 \n",
      "P: Output= 0.18606685154503477 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= 1.5739975468153977 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 9 Training Loss: 0.21106767654418945 \n",
      "P: Output= 0.8319122892605604 --- Target= 9.096384445683725e-08\n",
      "Q: Output= 2.443095765360291 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 10 Training Loss: 0.22610579431056976 \n",
      "P: Output= 0.9078215884805036 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 2.1888189452666644 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 11 Training Loss: 0.22192904353141785 \n",
      "P: Output= 0.8106891936767049 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= 1.7196021406556357 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 12 Training Loss: 0.2281171977519989 \n",
      "P: Output= 0.05023293552551156 --- Target= -2.741996745214692e-07\n",
      "Q: Output= 0.5438169958674779 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 13 Training Loss: 0.21253590285778046 \n",
      "P: Output= 0.05733942606033349 --- Target= -1.695987421612699e-07\n",
      "Q: Output= 0.7061395770908696 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 14 Training Loss: 0.21474415063858032 \n",
      "P: Output= 0.8906043627581139 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 2.0700165032415008 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 15 Training Loss: 0.21635326743125916 \n",
      "P: Output= 0.9220503096872097 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 2.549111134477915 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 16 Training Loss: 0.2196885645389557 \n",
      "P: Output= 0.8742916989026464 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 2.6865789449474526 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 17 Training Loss: 0.2277188003063202 \n",
      "P: Output= 0.24963964360009783 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= 1.8908296214176055 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 18 Training Loss: 0.2172297239303589 \n",
      "P: Output= 0.46777482854271835 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 2.3497035362078673 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 19 Training Loss: 0.20406430959701538 \n",
      "P: Output= 0.990867645688227 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 3.217054234646959 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 20 Training Loss: 0.22805188596248627 \n",
      "P: Output= 1.0253238023520757 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 3.306836249009198 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 21 Training Loss: 0.23035037517547607 \n",
      "P: Output= 0.8110983799358564 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 3.22207488628801 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 22 Training Loss: 0.2225322723388672 \n",
      "P: Output= 1.0706637791591556 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 3.2937483728729067 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 23 Training Loss: 0.21713754534721375 \n",
      "P: Output= 0.8876566816730183 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 3.0116771478030655 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 24 Training Loss: 0.2207217812538147 \n",
      "P: Output= 0.05015200773953499 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= 1.5584928608159618 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 25 Training Loss: 0.2161703109741211 \n",
      "P: Output= -0.09730873498942305 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 1.3420195509357429 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 26 Training Loss: 0.2123182713985443 \n",
      "P: Output= 0.4059336521283461 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 2.098352354200193 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 27 Training Loss: 0.2176365852355957 \n",
      "P: Output= 0.24658459447498782 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 1.7611914116612795 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 28 Training Loss: 0.22133678197860718 \n",
      "P: Output= -0.4997914495915916 --- Target= -9.226324593214486e-08\n",
      "Q: Output= 0.5679316220467667 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 29 Training Loss: 0.20903906226158142 \n",
      "P: Output= -0.5491999539368067 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= 0.5655664827588218 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 30 Training Loss: 0.20760393142700195 \n",
      "P: Output= -0.5285509586372994 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= 0.3940288757392576 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 31 Training Loss: 0.21239885687828064 \n",
      "P: Output= -0.3584264706196416 --- Target= -5.012515025271114e-08\n",
      "Q: Output= 0.8469970868352448 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 32 Training Loss: 0.21224632859230042 \n",
      "P: Output= 0.5891064826001138 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= 2.174262997235057 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 33 Training Loss: 0.21477484703063965 \n",
      "P: Output= 0.7910478732267201 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= 2.476656468355462 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 34 Training Loss: 0.21880128979682922 \n",
      "P: Output= 0.2549812382269012 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= 1.4644237034848908 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 35 Training Loss: 0.21993401646614075 \n",
      "P: Output= 0.36248251365055406 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= 1.6465274038921072 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 36 Training Loss: 0.2033264935016632 \n",
      "P: Output= 0.4752545356353952 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 1.6411663743579687 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 37 Training Loss: 0.20966243743896484 \n",
      "P: Output= 0.32656469549479095 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= 1.467406196131317 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 38 Training Loss: 0.2137567400932312 \n",
      "P: Output= 0.19154684077800432 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= 1.4613285228356157 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 39 Training Loss: 0.20673376321792603 \n",
      "P: Output= 0.23886558761896204 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 1.4211782865683356 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 40 Training Loss: 0.2085941582918167 \n",
      "P: Output= 0.6587803050232495 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 2.1529549165982074 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 41 Training Loss: 0.23251637816429138 \n",
      "P: Output= 0.08220282745256213 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 0.9677105938860269 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 42 Training Loss: 0.2158973515033722 \n",
      "P: Output= 0.5334258330657979 --- Target= 6.896767335007326e-08\n",
      "Q: Output= 1.9614836325584708 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 43 Training Loss: 0.23180809617042542 \n",
      "P: Output= 0.5184592873652871 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 1.9264808768531418 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 44 Training Loss: 0.23108640313148499 \n",
      "P: Output= -0.23079517869268962 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= 0.9140242904964611 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 45 Training Loss: 0.20647677779197693 \n",
      "P: Output= 0.518861588446093 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 2.006839037285676 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 46 Training Loss: 0.22014257311820984 \n",
      "P: Output= 0.48743080862945565 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 2.1916696130876314 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 47 Training Loss: 0.22467344999313354 \n",
      "P: Output= -0.10167984088680182 --- Target= -6.546786224248535e-09\n",
      "Q: Output= 1.2367685561716888 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 48 Training Loss: 0.21120482683181763 \n",
      "P: Output= 0.5020272695029702 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 2.190363172030123 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 49 Training Loss: 0.2259967029094696 \n",
      "P: Output= -0.12488036490074972 --- Target= -3.730653368450021e-07\n",
      "Q: Output= 1.3273064601102318 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 50 Training Loss: 0.2058543860912323 \n",
      "P: Output= 0.10319466755560036 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= 1.539508659453019 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 51 Training Loss: 0.20675142109394073 \n",
      "P: Output= 0.07131864135850297 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= 1.6768414456260734 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 52 Training Loss: 0.2118387222290039 \n",
      "P: Output= 0.85739044171389 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 3.0342447443556004 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 53 Training Loss: 0.22087505459785461 \n",
      "P: Output= 0.8401357503674607 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 3.1957792260129967 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 54 Training Loss: 0.22071069478988647 \n",
      "P: Output= 0.949719734516087 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 3.2739805870905165 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 55 Training Loss: 0.22600136697292328 \n",
      "P: Output= 1.035819160801613 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 3.432836520809577 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 56 Training Loss: 0.21360847353935242 \n",
      "P: Output= 0.8396479459405253 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 3.1729090775455493 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 57 Training Loss: 0.22032001614570618 \n",
      "P: Output= 0.7666997486145544 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= 2.76156983531766 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 58 Training Loss: 0.21934247016906738 \n",
      "P: Output= 0.6031066589054719 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 2.4098980356805653 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 59 Training Loss: 0.21455788612365723 \n",
      "P: Output= 0.35042962167516123 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 2.0437119492207696 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 60 Training Loss: 0.21266037225723267 \n",
      "P: Output= 0.20088820720289213 --- Target= 1.72196169323513e-07\n",
      "Q: Output= 1.6530772357214705 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 61 Training Loss: 0.21787874400615692 \n",
      "P: Output= 0.0014610240356223159 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 1.401806757190828 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 62 Training Loss: 0.21574333310127258 \n",
      "P: Output= 0.19662634866252748 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 1.2975089669392288 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 63 Training Loss: 0.2083936631679535 \n",
      "P: Output= -0.46780437077067294 --- Target= -4.072268247057309e-07\n",
      "Q: Output= 0.1259449800570387 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 64 Training Loss: 0.21136388182640076 \n",
      "P: Output= 0.22881163006526783 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 1.377335288133855 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 65 Training Loss: 0.21760350465774536 \n",
      "P: Output= 0.38256881632361495 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 1.5498292923227854 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 66 Training Loss: 0.21428291499614716 \n",
      "P: Output= 0.5338159357016652 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 1.7047648219983937 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 67 Training Loss: 0.21649891138076782 \n",
      "P: Output= -0.08167927095420069 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 0.6279102808067369 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 68 Training Loss: 0.21100331842899323 \n",
      "P: Output= -0.0665373836662777 --- Target= 1.239808575803636e-07\n",
      "Q: Output= 0.9711135462751823 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 69 Training Loss: 0.2102532982826233 \n",
      "P: Output= 0.10676160632269127 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= 1.4100568269485025 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 70 Training Loss: 0.2034549117088318 \n",
      "P: Output= 0.7988621610682705 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 2.8898560104825286 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 71 Training Loss: 0.21514394879341125 \n",
      "P: Output= 0.8536401207546103 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 3.0874056114418824 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 72 Training Loss: 0.2188154011964798 \n",
      "P: Output= 0.29450291894266467 --- Target= -3.576887390721595e-07\n",
      "Q: Output= 2.3123830824525813 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 73 Training Loss: 0.20646855235099792 \n",
      "P: Output= 0.39364064132201104 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 2.175606552623842 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 74 Training Loss: 0.2072458565235138 \n",
      "P: Output= 0.2862010420753247 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= 2.1553547224590943 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 75 Training Loss: 0.21039697527885437 \n",
      "P: Output= 0.8662161835083406 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 2.9348113606861244 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 76 Training Loss: 0.21392562985420227 \n",
      "P: Output= 0.1533742636159996 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= 1.8565941835961626 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 77 Training Loss: 0.21609516441822052 \n",
      "P: Output= 0.20638985523875242 --- Target= 9.813912260625557e-08\n",
      "Q: Output= 1.720040669514212 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 78 Training Loss: 0.204157292842865 \n",
      "P: Output= 0.5496647926480724 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 2.647483248865914 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 79 Training Loss: 0.2198501080274582 \n",
      "P: Output= 0.49413909117124355 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 2.4306185322088547 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 80 Training Loss: 0.216117262840271 \n",
      "P: Output= 0.03822155209643885 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= 1.5440592319641189 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 81 Training Loss: 0.2060580849647522 \n",
      "P: Output= -0.07598260134401436 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= 1.4784230964410625 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 82 Training Loss: 0.2008906453847885 \n",
      "P: Output= 0.5028281655879194 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 2.884014812759599 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 83 Training Loss: 0.21900787949562073 \n",
      "P: Output= 0.21576703274677822 --- Target= 6.39805808333449e-09\n",
      "Q: Output= 2.2674439917572347 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 84 Training Loss: 0.2022518813610077 \n",
      "P: Output= 0.918008187918681 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 3.6174182099232413 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 85 Training Loss: 0.20864573121070862 \n",
      "P: Output= 0.45592107457895814 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 2.646279446569485 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 86 Training Loss: 0.201630100607872 \n",
      "P: Output= 0.3659673151656282 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= 2.62397917575427 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 87 Training Loss: 0.19656868278980255 \n",
      "P: Output= 1.0889952126741864 --- Target= 5.936255753624664e-08\n",
      "Q: Output= 3.4465222179817676 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 88 Training Loss: 0.2139727771282196 \n",
      "P: Output= 0.9400065194879899 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 3.0201597944346394 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 89 Training Loss: 0.22854658961296082 \n",
      "P: Output= 0.7076889017819346 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 2.6700359411902514 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 90 Training Loss: 0.21097315847873688 \n",
      "P: Output= 0.3373299855255736 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= 1.8792974233655686 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 91 Training Loss: 0.21878379583358765 \n",
      "P: Output= -0.05908726516890361 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 1.2300959712470627 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 92 Training Loss: 0.21371415257453918 \n",
      "P: Output= 0.002822577468040599 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 0.781310261858855 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 93 Training Loss: 0.2145918756723404 \n",
      "P: Output= -0.2382252457395433 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 0.3551062482801761 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 94 Training Loss: 0.21636635065078735 \n",
      "P: Output= -0.3590866583237462 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 0.41632557684907656 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 95 Training Loss: 0.21145346760749817 \n",
      "P: Output= -0.31101986680687155 --- Target= -1.490649044200154e-07\n",
      "Q: Output= 0.5437374906352037 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 96 Training Loss: 0.21345269680023193 \n",
      "P: Output= -0.26409513300842047 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 0.9013352739317604 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 97 Training Loss: 0.21638771891593933 \n",
      "P: Output= -0.7869441300635556 --- Target= 1.606130251019522e-08\n",
      "Q: Output= 0.1711162983379797 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 98 Training Loss: 0.20362070202827454 \n",
      "P: Output= 0.011154773334293822 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 1.8185131191584318 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 99 Training Loss: 0.21667051315307617 \n",
      "P: Output= 0.33331710712513285 --- Target= 2.021405629548667e-07\n",
      "Q: Output= 2.4632966891085877 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 100 Training Loss: 0.20125752687454224 \n",
      "P: Output= -0.15164082239463106 --- Target= 3.465683571235445e-07\n",
      "Q: Output= 1.5266317756322012 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 101 Training Loss: 0.2022014856338501 \n",
      "P: Output= 0.6473804391089315 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 2.9438783258329657 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 102 Training Loss: 0.20780907571315765 \n",
      "P: Output= 0.6109256156265115 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 2.775378105888116 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 103 Training Loss: 0.21375977993011475 \n",
      "P: Output= 0.5007774061339365 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 2.5870251175330727 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 104 Training Loss: 0.20490014553070068 \n",
      "P: Output= 0.42923770316385923 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 2.3538991963678937 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 105 Training Loss: 0.21202996373176575 \n",
      "P: Output= -0.25027698812374766 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= 0.6986429204997133 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 106 Training Loss: 0.19530457258224487 \n",
      "P: Output= -0.24244773746380144 --- Target= 1.652745300617653e-07\n",
      "Q: Output= 0.4153348522640403 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 107 Training Loss: 0.20208200812339783 \n",
      "P: Output= -0.2987629537374472 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 0.3337077157037385 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 108 Training Loss: 0.2074785679578781 \n",
      "P: Output= -0.3648855615250195 --- Target= -9.128675593217395e-09\n",
      "Q: Output= 0.345465030558497 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 109 Training Loss: 0.19938689470291138 \n",
      "P: Output= -0.2802382998643127 --- Target= 9.865799821540122e-08\n",
      "Q: Output= 0.9247833177678242 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 110 Training Loss: 0.19575802981853485 \n",
      "P: Output= 0.40463656819266713 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 2.318220237317794 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 111 Training Loss: 0.20937380194664001 \n",
      "P: Output= -0.15058772597904202 --- Target= -8.441947496606872e-08\n",
      "Q: Output= 1.440757667321419 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 112 Training Loss: 0.2071099877357483 \n",
      "P: Output= 0.6430589230142667 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 3.285935388620664 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 113 Training Loss: 0.2029303014278412 \n",
      "P: Output= 0.14615549579296783 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= 2.609563094155276 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 114 Training Loss: 0.20404449105262756 \n",
      "P: Output= 0.37457388048386964 --- Target= 4.168930338721566e-07\n",
      "Q: Output= 3.137305917896658 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 115 Training Loss: 0.19752201437950134 \n",
      "P: Output= 0.9218813764973959 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 4.4165556539331625 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 116 Training Loss: 0.21733970940113068 \n",
      "P: Output= 0.5629926804486605 --- Target= 9.257290134456753e-08\n",
      "Q: Output= 3.561630522734273 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 117 Training Loss: 0.19664394855499268 \n",
      "P: Output= 0.6310071363385825 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= 3.6707808956092354 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 118 Training Loss: 0.19162821769714355 \n",
      "P: Output= 0.4553281367025548 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= 3.2202143868117457 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 119 Training Loss: 0.1943725198507309 \n",
      "P: Output= 0.21106433921677326 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= 2.678430824885913 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 120 Training Loss: 0.19670474529266357 \n",
      "P: Output= 0.5929112121900326 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 3.3491884891792623 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 121 Training Loss: 0.20243816077709198 \n",
      "P: Output= -0.2020645726059085 --- Target= -1.618743441511583e-07\n",
      "Q: Output= 1.9647773662688959 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 122 Training Loss: 0.194549560546875 \n",
      "P: Output= -0.3464662748777183 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= 1.5673656101790963 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 123 Training Loss: 0.19356462359428406 \n",
      "P: Output= 0.1636489055592314 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 2.642837613923856 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 124 Training Loss: 0.21502818167209625 \n",
      "P: Output= -0.3042013451626904 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= 1.7338413326524016 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 125 Training Loss: 0.2006133496761322 \n",
      "P: Output= 0.23363795495400197 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 3.0809110115223275 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 126 Training Loss: 0.21400371193885803 \n",
      "P: Output= 0.3347974635136941 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 3.3580566077768195 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 127 Training Loss: 0.20683272182941437 \n",
      "P: Output= 0.0422360118129852 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= 2.3602484189000696 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 128 Training Loss: 0.19889134168624878 \n",
      "P: Output= -0.011664548592540847 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= 2.2204159096960314 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 129 Training Loss: 0.19100797176361084 \n",
      "P: Output= 0.03244204799480155 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= 2.2911812846362816 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 130 Training Loss: 0.19874493777751923 \n",
      "P: Output= 0.436008654626316 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 3.114690272406465 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 131 Training Loss: 0.21250490844249725 \n",
      "P: Output= 0.03058981123980775 --- Target= 8.498954429114747e-08\n",
      "Q: Output= 1.9869030458530155 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 132 Training Loss: 0.19638730585575104 \n",
      "P: Output= 0.3599629448039696 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 2.924203944465914 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 133 Training Loss: 0.2109244465827942 \n",
      "P: Output= 0.1829907120602403 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 2.4897149921832407 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 134 Training Loss: 0.2112428992986679 \n",
      "P: Output= 0.010421554941114408 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 2.0418452994676395 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 135 Training Loss: 0.21512152254581451 \n",
      "P: Output= -0.8791180453186094 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= 0.7293455159039377 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 136 Training Loss: 0.20100313425064087 \n",
      "P: Output= -0.3395907326888299 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 1.6083197224869608 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 137 Training Loss: 0.20808403193950653 \n",
      "P: Output= -1.016180539461577 --- Target= 1.04987089244446e-07\n",
      "Q: Output= 0.2953106234299465 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 138 Training Loss: 0.19322749972343445 \n",
      "P: Output= -0.9665156557928736 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= -0.0445789708780957 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 139 Training Loss: 0.19586503505706787 \n",
      "P: Output= -0.25213200564125415 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 1.0603066459609938 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 140 Training Loss: 0.2147456407546997 \n",
      "P: Output= -0.20256223269479445 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 1.2565096136312448 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 141 Training Loss: 0.2135474681854248 \n",
      "P: Output= 0.06550517509462583 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= 1.2726587361163393 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 142 Training Loss: 0.21379710733890533 \n",
      "P: Output= 0.1570129975105239 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 1.4388593768934141 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 143 Training Loss: 0.20966452360153198 \n",
      "P: Output= 0.3891309474050795 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 1.3411050426775253 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 144 Training Loss: 0.21238435804843903 \n",
      "P: Output= 0.30822952203868503 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 1.9119081718188626 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 145 Training Loss: 0.21164309978485107 \n",
      "P: Output= -0.11210925480450928 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= 1.0376265211933289 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 146 Training Loss: 0.19449526071548462 \n",
      "P: Output= 0.3288201664528243 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 2.4755711987951727 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 147 Training Loss: 0.211419478058815 \n",
      "P: Output= -0.08554952289362383 --- Target= 1.885248828159547e-07\n",
      "Q: Output= 1.897040480783752 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 148 Training Loss: 0.1947268694639206 \n",
      "P: Output= 0.30070726635953093 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= 3.3389918085790846 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 149 Training Loss: 0.20707212388515472 \n",
      "P: Output= -0.2051678173350462 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= 2.3373160504433725 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 150 Training Loss: 0.19742991030216217 \n",
      "P: Output= -0.33293738840220577 --- Target= -9.703328984755899e-08\n",
      "Q: Output= 2.035653346883935 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 151 Training Loss: 0.19548237323760986 \n",
      "P: Output= -0.4619282229995223 --- Target= 9.79909344778207e-08\n",
      "Q: Output= 1.6934752627034229 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 152 Training Loss: 0.19599497318267822 \n",
      "P: Output= -0.597519555028887 --- Target= 9.40822530992591e-10\n",
      "Q: Output= 1.3528866765344008 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 153 Training Loss: 0.1922581046819687 \n",
      "P: Output= -0.4752963808554984 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= 1.23434089968507 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 154 Training Loss: 0.19666703045368195 \n",
      "P: Output= -0.5052965582124571 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= 1.310097865789654 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 155 Training Loss: 0.18747025728225708 \n",
      "P: Output= -0.24591704167665185 --- Target= 1.93652547331169e-07\n",
      "Q: Output= 1.447170937621923 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 156 Training Loss: 0.20071548223495483 \n",
      "P: Output= 0.25698289673112296 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 3.0669064807710242 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 157 Training Loss: 0.20329347252845764 \n",
      "P: Output= 0.01839247306422287 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 2.4859392448302913 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 158 Training Loss: 0.18978312611579895 \n",
      "P: Output= 0.8132644771780626 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= 4.289299574759843 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 159 Training Loss: 0.2018650323152542 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= 0.43070269931152527 --- Target= 4.268641484728164e-07\n",
      "Q: Output= 3.4706013858401574 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 0 Validation Loss: 0.1932278871536255 \n",
      "P: Output= 0.26599434570252534 --- Target= 3.239203305582805e-08\n",
      "Q: Output= 3.3775278301133937 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 1 Validation Loss: 0.18835973739624023 \n",
      "P: Output= 0.3095598671844826 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= 3.4470801998146428 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 2 Validation Loss: 0.19106507301330566 \n",
      "P: Output= 0.22436540864918975 --- Target= 2.239196774667107e-07\n",
      "Q: Output= 3.2873239850835176 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 3 Validation Loss: 0.19394880533218384 \n",
      "P: Output= 0.7877158203087813 --- Target= -1.907385298594022e-07\n",
      "Q: Output= 4.47025015960877 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 4 Validation Loss: 0.21584482491016388 \n",
      "P: Output= 0.3965529671863246 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= 3.4741230972474204 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 5 Validation Loss: 0.19019734859466553 \n",
      "P: Output= 0.2578536779467493 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= 3.307256372676017 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 6 Validation Loss: 0.19728007912635803 \n",
      "P: Output= 0.7652446435685745 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= 4.4863435129456555 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 7 Validation Loss: 0.2073402851819992 \n",
      "P: Output= 0.7498181198274256 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= 4.465849730166487 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 8 Validation Loss: 0.20887067914009094 \n",
      "P: Output= 0.3422145225109947 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= 3.2506987576128274 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 9 Validation Loss: 0.19500276446342468 \n",
      "P: Output= 0.5995302411321326 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= 4.461854798172125 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 10 Validation Loss: 0.2073044776916504 \n",
      "P: Output= 0.854956231625426 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= 4.529353789936502 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 11 Validation Loss: 0.20416656136512756 \n",
      "P: Output= 0.7338303827057908 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= 4.293631657079004 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 12 Validation Loss: 0.20508772134780884 \n",
      "P: Output= 0.2834935046113909 --- Target= -2.219336545650208e-07\n",
      "Q: Output= 3.3097346805191714 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 13 Validation Loss: 0.18955083191394806 \n",
      "P: Output= 0.7126179925351259 --- Target= -8.584417265922184e-08\n",
      "Q: Output= 4.388910415813518 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 14 Validation Loss: 0.21569529175758362 \n",
      "P: Output= 0.24897088461423245 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= 3.554139923262955 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 15 Validation Loss: 0.18771636486053467 \n",
      "P: Output= 0.6453235012444649 --- Target= 7.48511617132408e-08\n",
      "Q: Output= 4.327349769254857 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 16 Validation Loss: 0.2075868844985962 \n",
      "P: Output= 0.21551305541674637 --- Target= 2.911645777814442e-07\n",
      "Q: Output= 3.3487325198431455 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 17 Validation Loss: 0.19355508685112 \n",
      "P: Output= 0.8324791189118104 --- Target= 1.3670019605172e-07\n",
      "Q: Output= 4.482213076665003 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 18 Validation Loss: 0.19644248485565186 \n",
      "P: Output= 0.7369394370020679 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= 4.495103250378682 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 19 Validation Loss: 0.19924865663051605 \n",
      "P: Output= 0.814510415965322 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= 4.519931452983244 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 20 Validation Loss: 0.2170538753271103 \n",
      "P: Output= 0.23236945724093516 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= 3.3518008717608927 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 21 Validation Loss: 0.18822720646858215 \n",
      "P: Output= 0.590514075704526 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= 4.350352886993509 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 22 Validation Loss: 0.20455525815486908 \n",
      "P: Output= 0.8062407686703619 --- Target= 2.077415652834702e-07\n",
      "Q: Output= 4.6030169830808685 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 23 Validation Loss: 0.2057926058769226 \n",
      "P: Output= 0.7302713232668516 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= 4.3141939379795815 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 24 Validation Loss: 0.2080707848072052 \n",
      "P: Output= 0.7097172697951857 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= 4.405495081349462 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 25 Validation Loss: 0.2210294008255005 \n",
      "P: Output= 0.3044581102106312 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= 3.436940997826838 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 26 Validation Loss: 0.1888890564441681 \n",
      "P: Output= 0.7996194433152155 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= 4.4858250599101295 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 27 Validation Loss: 0.20787177979946136 \n",
      "P: Output= 0.8252225718235371 --- Target= 5.541669878539324e-08\n",
      "Q: Output= 4.475144309239646 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 28 Validation Loss: 0.2002004086971283 \n",
      "P: Output= 0.6377830578589654 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= 4.427702112250056 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 29 Validation Loss: 0.21001990139484406 \n",
      "Epoch: 1\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= 0.30576320147946934 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= 3.547348332791298 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 0 Training Loss: 0.19614005088806152 \n",
      "P: Output= 0.27998102240585965 --- Target= 9.40822530992591e-10\n",
      "Q: Output= 3.705899420437527 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 1 Training Loss: 0.19337522983551025 \n",
      "P: Output= 0.3655167580479084 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= 3.9267844426105976 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 2 Training Loss: 0.19199621677398682 \n",
      "P: Output= 0.6097948571897964 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 4.616994817846605 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 3 Training Loss: 0.20554855465888977 \n",
      "P: Output= 0.4727680887355614 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 4.290794869168286 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 4 Training Loss: 0.20453470945358276 \n",
      "P: Output= 0.2908601076277444 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 3.8631609540361938 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 5 Training Loss: 0.19507160782814026 \n",
      "P: Output= -0.1699585049961101 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 3.0215717001337596 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 6 Training Loss: 0.2001737654209137 \n",
      "P: Output= -0.1038378303564027 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 2.5975825045703695 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 7 Training Loss: 0.2100137621164322 \n",
      "P: Output= -0.33887915497798193 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 1.8035199306490286 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 8 Training Loss: 0.2138417661190033 \n",
      "P: Output= -0.3671555993710607 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 1.5037294133819001 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 9 Training Loss: 0.20185258984565735 \n",
      "P: Output= -0.9149552314512661 --- Target= 1.239808575803636e-07\n",
      "Q: Output= 0.07743672510097355 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 10 Training Loss: 0.19634845852851868 \n",
      "P: Output= -0.29172536174113883 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 1.2623190668744826 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 11 Training Loss: 0.20112672448158264 \n",
      "P: Output= -0.19721419161619558 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 1.5046781794194324 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 12 Training Loss: 0.2029104232788086 \n",
      "P: Output= -0.05553189927288127 --- Target= -1.490649044200154e-07\n",
      "Q: Output= 1.58737061073636 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 13 Training Loss: 0.19908112287521362 \n",
      "P: Output= -0.5787755148420013 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 0.7036561064377134 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 14 Training Loss: 0.19399294257164001 \n",
      "P: Output= -0.13880720861972673 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 2.016398024408332 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 15 Training Loss: 0.1950952708721161 \n",
      "P: Output= 0.06989946364507116 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 2.3059557194639524 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 16 Training Loss: 0.1970195472240448 \n",
      "P: Output= 0.028049182115112536 --- Target= 5.936255753624664e-08\n",
      "Q: Output= 2.183295844613008 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 17 Training Loss: 0.19955047965049744 \n",
      "P: Output= -0.6809562925899799 --- Target= 1.04987089244446e-07\n",
      "Q: Output= 1.1164697155170806 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 18 Training Loss: 0.1814645230770111 \n",
      "P: Output= -0.10917849527894674 --- Target= 6.896767335007326e-08\n",
      "Q: Output= 1.8790401408774473 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 19 Training Loss: 0.21988750994205475 \n",
      "P: Output= -0.5761095121138675 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 0.5237479160262923 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 20 Training Loss: 0.19663295149803162 \n",
      "P: Output= -0.05142584739384937 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 1.6603093192064842 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 21 Training Loss: 0.20411325991153717 \n",
      "P: Output= -0.03493860976514629 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 1.7802910667098848 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 22 Training Loss: 0.19923672080039978 \n",
      "P: Output= 0.025865772676630705 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 1.9546040871829362 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 23 Training Loss: 0.1965845823287964 \n",
      "P: Output= -0.05898868187386341 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 2.031556358803872 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 24 Training Loss: 0.19891951978206635 \n",
      "P: Output= -0.470177671762114 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= 1.1987059311151018 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 25 Training Loss: 0.1949133574962616 \n",
      "P: Output= -0.3977933846831405 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 1.223935373478044 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 26 Training Loss: 0.19254061579704285 \n",
      "P: Output= -0.395667665102005 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= 1.6282752594794925 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 27 Training Loss: 0.1916734278202057 \n",
      "P: Output= 0.1774977795884931 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 2.660044412586535 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 28 Training Loss: 0.198525071144104 \n",
      "P: Output= 0.0005427384078924291 --- Target= 1.72196169323513e-07\n",
      "Q: Output= 3.000405091545586 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 29 Training Loss: 0.1983281373977661 \n",
      "P: Output= -0.09117175199802041 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 3.1040036695716857 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 30 Training Loss: 0.2026616930961609 \n",
      "P: Output= -0.5171423015536982 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= 2.033620567946138 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 31 Training Loss: 0.19035932421684265 \n",
      "P: Output= -0.34794129129966045 --- Target= -2.741996745214692e-07\n",
      "Q: Output= 2.457256143708535 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 32 Training Loss: 0.18597358465194702 \n",
      "P: Output= -0.30037299650268423 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= 2.7486244288751713 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 33 Training Loss: 0.1874268501996994 \n",
      "P: Output= 0.1368612478551956 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 3.921648726331857 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 34 Training Loss: 0.20122775435447693 \n",
      "P: Output= 0.117582883989515 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 3.597930136218757 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 35 Training Loss: 0.20579281449317932 \n",
      "P: Output= -0.40047129444248064 --- Target= 9.865799821540122e-08\n",
      "Q: Output= 2.1430798601429446 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 36 Training Loss: 0.18523672223091125 \n",
      "P: Output= -0.11117034820941285 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 2.667791218827685 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 37 Training Loss: 0.19678592681884766 \n",
      "P: Output= -0.1304027531197578 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 1.9903592487725472 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 38 Training Loss: 0.19498968124389648 \n",
      "P: Output= -0.7497971852345398 --- Target= 2.191291796904693e-07\n",
      "Q: Output= 0.15257814392386582 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 39 Training Loss: 0.19317781925201416 \n",
      "P: Output= -0.6916556609814482 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= -0.007111012302560837 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 40 Training Loss: 0.19813117384910583 \n",
      "P: Output= -0.7282722460629527 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= -0.19535578073376936 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 41 Training Loss: 0.18622536957263947 \n",
      "P: Output= -0.5421110675689214 --- Target= 6.39805808333449e-09\n",
      "Q: Output= 0.23201796345517867 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 42 Training Loss: 0.18895462155342102 \n",
      "P: Output= -0.3224558851866153 --- Target= 1.885248828159547e-07\n",
      "Q: Output= 0.6771005554186917 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 43 Training Loss: 0.18777549266815186 \n",
      "P: Output= -0.14823285666356867 --- Target= 1.652745300617653e-07\n",
      "Q: Output= 1.5055705246681592 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 44 Training Loss: 0.1865462064743042 \n",
      "P: Output= 0.082663215761837 --- Target= 8.498954429114747e-08\n",
      "Q: Output= 2.3508206279110624 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 45 Training Loss: 0.18387874960899353 \n",
      "P: Output= 0.5819135456071569 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 4.375069775645228 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 46 Training Loss: 0.2016766369342804 \n",
      "P: Output= 0.24885826870599548 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= 3.849039388003302 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 47 Training Loss: 0.18276426196098328 \n",
      "P: Output= 0.3616513769509675 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= 4.170871663066022 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 48 Training Loss: 0.18993353843688965 \n",
      "P: Output= 0.6344701846053908 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 5.383710800335086 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 49 Training Loss: 0.19694843888282776 \n",
      "P: Output= -0.05785907097863685 --- Target= -3.730653368450021e-07\n",
      "Q: Output= 4.019258441247484 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 50 Training Loss: 0.17934007942676544 \n",
      "P: Output= 0.21897388565782006 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 4.467848188383433 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 51 Training Loss: 0.1868431717157364 \n",
      "P: Output= -0.7329265008634955 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= 2.520366777134269 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 52 Training Loss: 0.1858210265636444 \n",
      "P: Output= -0.5656002372984252 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 3.0621147819691164 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 53 Training Loss: 0.1901031732559204 \n",
      "P: Output= -0.5845345370446697 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 2.588481679728572 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 54 Training Loss: 0.20119959115982056 \n",
      "P: Output= -1.1805538391612442 --- Target= 3.465683571235445e-07\n",
      "Q: Output= 0.9484638469640476 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 55 Training Loss: 0.18477408587932587 \n",
      "P: Output= -1.1864706945195849 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= 0.562281249272484 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 56 Training Loss: 0.18478068709373474 \n",
      "P: Output= -1.2192768803664906 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= 0.28354463839587396 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 57 Training Loss: 0.18010587990283966 \n",
      "P: Output= -1.0457575286647538 --- Target= 9.79909344778207e-08\n",
      "Q: Output= 0.012469438661891985 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 58 Training Loss: 0.1862330287694931 \n",
      "P: Output= -0.3764496462454243 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 1.3069944707331613 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 59 Training Loss: 0.19586655497550964 \n",
      "P: Output= -0.08171207299819283 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 1.8710429045905483 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 60 Training Loss: 0.20446555316448212 \n",
      "P: Output= -0.31276784711246286 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 1.2267410245886872 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 61 Training Loss: 0.17867091298103333 \n",
      "P: Output= -0.1424350963616705 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= 1.7713813323624406 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 62 Training Loss: 0.18084943294525146 \n",
      "P: Output= 0.06648443274924531 --- Target= -6.546786224248535e-09\n",
      "Q: Output= 2.455884253702976 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 63 Training Loss: 0.17986899614334106 \n",
      "P: Output= 0.1109342363916559 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= 2.6883967271913214 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 64 Training Loss: 0.17804130911827087 \n",
      "P: Output= 0.5559198901678144 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= 3.7996163815089457 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 65 Training Loss: 0.20081686973571777 \n",
      "P: Output= 0.3161084690395084 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 3.9353645227340017 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 66 Training Loss: 0.19720153510570526 \n",
      "P: Output= 0.2086313704668834 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 3.580893355661127 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 67 Training Loss: 0.18853628635406494 \n",
      "P: Output= -0.5762070996474113 --- Target= -8.441947496606872e-08\n",
      "Q: Output= 1.742146918039543 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 68 Training Loss: 0.18882128596305847 \n",
      "P: Output= -0.3506590115174184 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 2.880997145396801 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 69 Training Loss: 0.1953427791595459 \n",
      "P: Output= -0.8054221055000212 --- Target= -9.226324593214486e-08\n",
      "Q: Output= 1.7575258355125403 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 70 Training Loss: 0.1823255717754364 \n",
      "P: Output= -0.3130321707164727 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 2.824394257407284 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 71 Training Loss: 0.19251897931098938 \n",
      "P: Output= -0.4454438605837874 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 2.8042604764839734 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 72 Training Loss: 0.19840532541275024 \n",
      "P: Output= -0.9061695087017432 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= 1.7357093611152088 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 73 Training Loss: 0.18599218130111694 \n",
      "P: Output= -0.9098389594434408 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= 1.6610208994326978 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 74 Training Loss: 0.1818956434726715 \n",
      "P: Output= -0.9263303956031628 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= 1.4367893360842023 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 75 Training Loss: 0.17917564511299133 \n",
      "P: Output= -0.8397933601656602 --- Target= -1.695987421612699e-07\n",
      "Q: Output= 1.375007713847892 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 76 Training Loss: 0.17951397597789764 \n",
      "P: Output= -0.2703217465609873 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 2.397504694855952 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 77 Training Loss: 0.1996917426586151 \n",
      "P: Output= -0.17213658332638992 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 2.525538979462473 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 78 Training Loss: 0.18996372818946838 \n",
      "P: Output= -0.49458059021727063 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 1.5689787782287858 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 79 Training Loss: 0.18447890877723694 \n",
      "P: Output= -0.1346524812965333 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 2.7470154236836075 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 80 Training Loss: 0.1856696903705597 \n",
      "P: Output= -0.5828123660435542 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 1.6593957330772682 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 81 Training Loss: 0.17866066098213196 \n",
      "P: Output= -0.4896050079936547 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 1.715832144326928 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 82 Training Loss: 0.18680039048194885 \n",
      "P: Output= -0.09007963488516335 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= 3.4164140243240686 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 83 Training Loss: 0.1861843764781952 \n",
      "P: Output= -0.5195506210825434 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= 2.646342038125187 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 84 Training Loss: 0.17776235938072205 \n",
      "P: Output= -0.38343174163568605 --- Target= 1.93652547331169e-07\n",
      "Q: Output= 3.0078578779680676 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 85 Training Loss: 0.1828635334968567 \n",
      "P: Output= -0.45410504392632056 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 3.3790840823500883 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 86 Training Loss: 0.17981195449829102 \n",
      "P: Output= -0.4636347582334155 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= 3.404881497841518 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 87 Training Loss: 0.18709895014762878 \n",
      "P: Output= 0.07175629969585451 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 4.302585761432415 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 88 Training Loss: 0.21697987616062164 \n",
      "P: Output= -0.47434387042561976 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= 3.2385369570210765 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 89 Training Loss: 0.17831972241401672 \n",
      "P: Output= 0.005194212533363718 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 3.9650979879321575 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 90 Training Loss: 0.19686271250247955 \n",
      "P: Output= -0.5441070093316087 --- Target= -1.618743441511583e-07\n",
      "Q: Output= 2.6025780993384338 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 91 Training Loss: 0.1754636913537979 \n",
      "P: Output= -0.2603781829352423 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= 2.83969331353177 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 92 Training Loss: 0.20850692689418793 \n",
      "P: Output= -0.9201684751738428 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= 0.9684689194082656 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 93 Training Loss: 0.17309798300266266 \n",
      "P: Output= -0.9369934556279356 --- Target= 9.257290134456753e-08\n",
      "Q: Output= 0.2618977952570374 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 94 Training Loss: 0.17540967464447021 \n",
      "P: Output= -1.0558171491550876 --- Target= 4.168930338721566e-07\n",
      "Q: Output= -0.3365826273473411 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 95 Training Loss: 0.1777750551700592 \n",
      "P: Output= -1.156727545728125 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= -0.7107295240301106 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 96 Training Loss: 0.17927315831184387 \n",
      "P: Output= -0.5571497933668343 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 0.17249320094899634 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 97 Training Loss: 0.1874941885471344 \n",
      "P: Output= -0.6315376158798625 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 0.5971043322141973 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 98 Training Loss: 0.18628071248531342 \n",
      "P: Output= -1.042687612966228 --- Target= 9.813912260625557e-08\n",
      "Q: Output= -0.2900206773152254 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 99 Training Loss: 0.17721518874168396 \n",
      "P: Output= -0.6176063640580391 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 1.300145131329991 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 100 Training Loss: 0.18808794021606445 \n",
      "P: Output= -0.5161269035913048 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 1.5442871005135377 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 101 Training Loss: 0.18782564997673035 \n",
      "P: Output= -1.1035086874218178 --- Target= -5.012515025271114e-08\n",
      "Q: Output= 0.7836334560230549 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 102 Training Loss: 0.18041333556175232 \n",
      "P: Output= -0.4474440858717639 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 2.243509049086616 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 103 Training Loss: 0.19499143958091736 \n",
      "P: Output= -0.3001293753280283 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 2.2631989473156606 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 104 Training Loss: 0.19672298431396484 \n",
      "P: Output= -0.6825985305429985 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= 1.700054908689654 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 105 Training Loss: 0.17674876749515533 \n",
      "P: Output= -0.3076880893251843 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 2.782349627106104 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 106 Training Loss: 0.1872105598449707 \n",
      "P: Output= -0.5523060892701706 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= 1.9141538114644536 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 107 Training Loss: 0.1827971190214157 \n",
      "P: Output= -0.05685325376907091 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= 3.4008359092773635 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 108 Training Loss: 0.191324383020401 \n",
      "P: Output= -0.12182314654599402 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 3.4982743455162106 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 109 Training Loss: 0.1842252016067505 \n",
      "P: Output= -0.10484816831651145 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= 3.2063512571475936 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 110 Training Loss: 0.18609406054019928 \n",
      "P: Output= -0.25294666583873937 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 2.8854297536080704 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 111 Training Loss: 0.19098393619060516 \n",
      "P: Output= -0.3308121827325996 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 2.5437664216642757 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 112 Training Loss: 0.19013391435146332 \n",
      "P: Output= -0.29008251557444886 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 2.3803993105717964 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 113 Training Loss: 0.1854781210422516 \n",
      "P: Output= -0.3735137684105627 --- Target= 6.895684645513711e-08\n",
      "Q: Output= 2.352549463780962 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 114 Training Loss: 0.18691858649253845 \n",
      "P: Output= -0.3944983723499291 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 2.7055943654876344 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 115 Training Loss: 0.1825421303510666 \n",
      "P: Output= -0.3957766296060585 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 2.7223768323283446 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 116 Training Loss: 0.18393464386463165 \n",
      "P: Output= -0.24266078730980478 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 2.867286908917185 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 117 Training Loss: 0.18457432091236115 \n",
      "P: Output= -0.721980458267522 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 1.6557833979703274 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 118 Training Loss: 0.17470648884773254 \n",
      "P: Output= -0.1259148593786259 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 3.039879900236423 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 119 Training Loss: 0.1824551522731781 \n",
      "P: Output= -0.07637765649655037 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 3.044718559188908 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 120 Training Loss: 0.18490710854530334 \n",
      "P: Output= -0.15970557127726792 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 2.7922105733704 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 121 Training Loss: 0.18618160486221313 \n",
      "P: Output= -0.14738024812016448 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 2.753976827045836 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 122 Training Loss: 0.18725422024726868 \n",
      "P: Output= -0.7644824232244929 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= 1.1273917988299447 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 123 Training Loss: 0.17344766855239868 \n",
      "P: Output= -0.39577201357117264 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 2.057992137184587 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 124 Training Loss: 0.19164833426475525 \n",
      "P: Output= -0.7563012570289693 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 1.1937391195340696 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 125 Training Loss: 0.16894210875034332 \n",
      "P: Output= -0.6423071959298028 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 1.2081568926030792 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 126 Training Loss: 0.1743934601545334 \n",
      "P: Output= -0.6407684306418631 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= 1.080270216716384 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 127 Training Loss: 0.18404023349285126 \n",
      "P: Output= -0.6703075166170658 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= 1.5979513150667284 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 128 Training Loss: 0.18040309846401215 \n",
      "P: Output= -0.410074334580643 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= 2.047124012161051 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 129 Training Loss: 0.17195463180541992 \n",
      "P: Output= 0.1629958155358624 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 3.6805630502492317 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 130 Training Loss: 0.18326249718666077 \n",
      "P: Output= 0.08500325117257646 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= 3.9989728323308587 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 131 Training Loss: 0.18353061378002167 \n",
      "P: Output= -0.3043646278921148 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= 2.820319948648999 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 132 Training Loss: 0.17268279194831848 \n",
      "P: Output= 0.03974525072097812 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 3.7725745551966305 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 133 Training Loss: 0.18964608013629913 \n",
      "P: Output= -0.1179269284787674 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 3.6549110385525214 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 134 Training Loss: 0.18090978264808655 \n",
      "P: Output= -0.7150672407058023 --- Target= -3.576887390721595e-07\n",
      "Q: Output= 2.0643715797357585 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 135 Training Loss: 0.17302697896957397 \n",
      "P: Output= -0.5323038541409231 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 2.62402665921476 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 136 Training Loss: 0.18845731019973755 \n",
      "P: Output= -0.5497554916075904 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 2.171419949623073 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 137 Training Loss: 0.1860903799533844 \n",
      "P: Output= -1.0399826182684118 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= 0.8310091027760533 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 138 Training Loss: 0.1686144769191742 \n",
      "P: Output= -0.4657110547533163 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= 1.720730203219012 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 139 Training Loss: 0.1869000643491745 \n",
      "P: Output= -0.46805951262946444 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 1.8372642358344864 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 140 Training Loss: 0.1801479458808899 \n",
      "P: Output= -0.9848422131177923 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= 0.5630792385927039 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 141 Training Loss: 0.16642311215400696 \n",
      "P: Output= -0.3869421858300983 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 1.9746490489626725 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 142 Training Loss: 0.18506918847560883 \n",
      "P: Output= -0.8378708744902417 --- Target= -4.072268247057309e-07\n",
      "Q: Output= 1.0014495978345899 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 143 Training Loss: 0.1756182461977005 \n",
      "P: Output= -0.24660201479526744 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 2.7970362955855075 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 144 Training Loss: 0.17283199727535248 \n",
      "P: Output= -0.16393672494047973 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 2.7105325805896126 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 145 Training Loss: 0.18362927436828613 \n",
      "P: Output= -0.1503876076173345 --- Target= 9.096384445683725e-08\n",
      "Q: Output= 2.9024580965293394 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 146 Training Loss: 0.1900520920753479 \n",
      "P: Output= -0.47370610242440403 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= 1.4820699628493816 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 147 Training Loss: 0.16949543356895447 \n",
      "P: Output= 0.011865272798434745 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= 2.687860549527869 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 148 Training Loss: 0.17441578209400177 \n",
      "P: Output= -0.50202564047731 --- Target= -9.703328984755899e-08\n",
      "Q: Output= 1.1737808093486644 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 149 Training Loss: 0.17091333866119385 \n",
      "P: Output= -0.2798680169362191 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 2.4337149694825904 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 150 Training Loss: 0.19224172830581665 \n",
      "P: Output= -0.7161685684746244 --- Target= -8.159233733096016e-08\n",
      "Q: Output= 0.9648125579814835 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 151 Training Loss: 0.1698034405708313 \n",
      "P: Output= -0.7878013471316079 --- Target= 1.606130251019522e-08\n",
      "Q: Output= 0.91646913025851 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 152 Training Loss: 0.17376866936683655 \n",
      "P: Output= -0.8441151926121107 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= 1.1881114907759418 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 153 Training Loss: 0.16785940527915955 \n",
      "P: Output= -0.34887927216348924 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 2.7331891559008064 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 154 Training Loss: 0.17396476864814758 \n",
      "P: Output= -0.6747398310807551 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= 1.9180328081298805 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 155 Training Loss: 0.16648972034454346 \n",
      "P: Output= -0.48807932425993794 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 3.265324870029694 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 156 Training Loss: 0.18050572276115417 \n",
      "P: Output= -0.6496105020196117 --- Target= -9.128675593217395e-09\n",
      "Q: Output= 2.1248467980747874 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 157 Training Loss: 0.17023304104804993 \n",
      "P: Output= -0.2569289355086122 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 3.576951343678614 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 158 Training Loss: 0.18815866112709045 \n",
      "P: Output= -0.04218469441950834 --- Target= 2.021405629548667e-07\n",
      "Q: Output= 3.656346831457623 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 159 Training Loss: 0.170608252286911 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -0.29026138422556436 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= 3.417589823433431 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 0 Validation Loss: 0.18172767758369446 \n",
      "P: Output= -0.09897087846179353 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= 3.388627788856893 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 1 Validation Loss: 0.17968879640102386 \n",
      "P: Output= -0.1382648642962785 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= 3.4056853322692557 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 2 Validation Loss: 0.17926757037639618 \n",
      "P: Output= -0.14707814256227802 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= 3.1904473790374492 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 3 Validation Loss: 0.178566575050354 \n",
      "P: Output= -0.5313266966953298 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= 2.2341029217571187 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 4 Validation Loss: 0.16930407285690308 \n",
      "P: Output= -0.6039520710360087 --- Target= 2.239196774667107e-07\n",
      "Q: Output= 2.0680424416175684 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 5 Validation Loss: 0.17236328125 \n",
      "P: Output= -0.06570484909070817 --- Target= 1.3670019605172e-07\n",
      "Q: Output= 3.377520873702254 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 6 Validation Loss: 0.17153894901275635 \n",
      "P: Output= -0.10146383088737387 --- Target= -1.907385298594022e-07\n",
      "Q: Output= 3.380614006492297 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 7 Validation Loss: 0.19101974368095398 \n",
      "P: Output= -0.07102807670410805 --- Target= 5.541669878539324e-08\n",
      "Q: Output= 3.374798320627793 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 8 Validation Loss: 0.1739211529493332 \n",
      "P: Output= -0.1674400720118996 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= 3.336021678443986 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 9 Validation Loss: 0.19681160151958466 \n",
      "P: Output= -0.1023981798420106 --- Target= 2.077415652834702e-07\n",
      "Q: Output= 3.5348678164876883 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 10 Validation Loss: 0.1788790374994278 \n",
      "P: Output= -0.580704258232176 --- Target= 3.239203305582805e-08\n",
      "Q: Output= 2.153995287980286 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 11 Validation Loss: 0.16445335745811462 \n",
      "P: Output= -0.17075476903496067 --- Target= -8.584417265922184e-08\n",
      "Q: Output= 3.3080383736834325 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 12 Validation Loss: 0.19064317643642426 \n",
      "P: Output= -0.5739850225219598 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= 2.0829118382142333 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 13 Validation Loss: 0.17770916223526 \n",
      "P: Output= -0.541876465161204 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= 2.2161270309625296 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 14 Validation Loss: 0.16534423828125 \n",
      "P: Output= -0.4122336725513378 --- Target= 4.268641484728164e-07\n",
      "Q: Output= 2.242305213652963 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 15 Validation Loss: 0.17197903990745544 \n",
      "P: Output= -0.6008512083352944 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= 2.1288932188172485 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 16 Validation Loss: 0.16534461081027985 \n",
      "P: Output= -0.24108577694460998 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= 3.356014065791598 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 17 Validation Loss: 0.18585672974586487 \n",
      "P: Output= -0.5488398915936479 --- Target= -2.219336545650208e-07\n",
      "Q: Output= 2.0671191616976055 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 18 Validation Loss: 0.16689252853393555 \n",
      "P: Output= -0.2340484130094609 --- Target= 7.48511617132408e-08\n",
      "Q: Output= 3.2451760245399814 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 19 Validation Loss: 0.18223172426223755 \n",
      "P: Output= -0.4547544685910516 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= 2.244102559904227 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 20 Validation Loss: 0.16662709414958954 \n",
      "P: Output= -0.04578233822330624 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= 3.4298874005782842 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 21 Validation Loss: 0.1770613044500351 \n",
      "P: Output= -0.14756617918908077 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= 3.386134633379208 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 22 Validation Loss: 0.18190661072731018 \n",
      "P: Output= -0.16262330103537526 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= 3.407801179304766 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 23 Validation Loss: 0.17171484231948853 \n",
      "P: Output= -0.6054285053629362 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= 2.379212806369817 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 24 Validation Loss: 0.1640278548002243 \n",
      "P: Output= -0.08302664388869463 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= 3.4242350618208386 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 25 Validation Loss: 0.19062088429927826 \n",
      "P: Output= -0.28858507845859815 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= 3.292067480981336 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 26 Validation Loss: 0.18123412132263184 \n",
      "P: Output= -0.6195551468171194 --- Target= 2.911645777814442e-07\n",
      "Q: Output= 2.15007651226132 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 27 Validation Loss: 0.1714949756860733 \n",
      "P: Output= -0.48375220430715427 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= 1.992783236543409 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 28 Validation Loss: 0.1737290471792221 \n",
      "P: Output= -0.1560668038246389 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= 3.201495905878769 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 29 Validation Loss: 0.1818009614944458 \n",
      "Epoch: 2\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -0.2520797991806356 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 3.3687040096712852 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 0 Training Loss: 0.18087615072727203 \n",
      "P: Output= -0.13082285881206523 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 3.505176721940426 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 1 Training Loss: 0.17618009448051453 \n",
      "P: Output= -0.18577032166473995 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 3.270654366555431 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 2 Training Loss: 0.17309069633483887 \n",
      "P: Output= -0.3761339801436021 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 2.8914264975189763 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 3 Training Loss: 0.18141815066337585 \n",
      "P: Output= -0.5291654455036072 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 2.6686304818993083 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 4 Training Loss: 0.1768866777420044 \n",
      "P: Output= -0.903284789197631 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= 1.4496254949639518 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 5 Training Loss: 0.17292991280555725 \n",
      "P: Output= -0.5245153914893921 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 2.8645229397668963 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 6 Training Loss: 0.1746593415737152 \n",
      "P: Output= -0.9617924920554701 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= 1.66894810104246 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 7 Training Loss: 0.17242220044136047 \n",
      "P: Output= -0.8020977154321995 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= 2.308209245679296 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 8 Training Loss: 0.16723719239234924 \n",
      "P: Output= -0.7906314469238058 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= 2.4569545480244264 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 9 Training Loss: 0.17200994491577148 \n",
      "P: Output= -0.6389686275760713 --- Target= -6.546786224248535e-09\n",
      "Q: Output= 2.759929260446432 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 10 Training Loss: 0.16706520318984985 \n",
      "P: Output= -0.6428032714353717 --- Target= 9.40822530992591e-10\n",
      "Q: Output= 2.2619900136157565 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 11 Training Loss: 0.17178396880626678 \n",
      "P: Output= -0.5631234985467284 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 2.080232802065299 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 12 Training Loss: 0.16134583950042725 \n",
      "P: Output= -0.6054634428636474 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= 1.1881039059027287 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 13 Training Loss: 0.16468892991542816 \n",
      "P: Output= -0.03250949778043566 --- Target= -1.490649044200154e-07\n",
      "Q: Output= 2.104164325163408 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 14 Training Loss: 0.1831732988357544 \n",
      "P: Output= -0.10395132173186195 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= 1.9872315521127097 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 15 Training Loss: 0.17546862363815308 \n",
      "P: Output= -0.44800269623120936 --- Target= 1.885248828159547e-07\n",
      "Q: Output= 0.6590429256546093 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 16 Training Loss: 0.1670495569705963 \n",
      "P: Output= -0.5162530200416544 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= 1.0777594624340336 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 17 Training Loss: 0.16714799404144287 \n",
      "P: Output= -0.021966402291584863 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 2.9611383614067632 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 18 Training Loss: 0.1779610961675644 \n",
      "P: Output= -0.6223433391204933 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= 1.854187466928665 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 19 Training Loss: 0.1646224558353424 \n",
      "P: Output= -0.2878902402104888 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 3.1225290744367964 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 20 Training Loss: 0.17637008428573608 \n",
      "P: Output= -0.42369393978632086 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 3.4868506126485235 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 21 Training Loss: 0.17531141638755798 \n",
      "P: Output= -0.47606365537357576 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 3.2729492627330377 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 22 Training Loss: 0.16593293845653534 \n",
      "P: Output= -1.144006226945197 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= 1.7115346892439431 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 23 Training Loss: 0.16150802373886108 \n",
      "P: Output= -0.7674384592692842 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 2.5959952108355813 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 24 Training Loss: 0.17650941014289856 \n",
      "P: Output= -1.2588489382610053 --- Target= -9.703328984755899e-08\n",
      "Q: Output= 0.9615884489553235 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 25 Training Loss: 0.16623356938362122 \n",
      "P: Output= -0.9491696797010807 --- Target= 9.096384445683725e-08\n",
      "Q: Output= 1.717593024755888 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 26 Training Loss: 0.1859787404537201 \n",
      "P: Output= -0.8864938271297831 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 1.1608100548489766 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 27 Training Loss: 0.17541074752807617 \n",
      "P: Output= -0.8355272500625732 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 0.7107619865602102 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 28 Training Loss: 0.17445853352546692 \n",
      "P: Output= -1.166704101513866 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= -0.8890357376811364 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 29 Training Loss: 0.16847142577171326 \n",
      "P: Output= -0.9716074195986417 --- Target= 9.257290134456753e-08\n",
      "Q: Output= -0.7970026777998136 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 30 Training Loss: 0.16369283199310303 \n",
      "P: Output= -0.7924129026287439 --- Target= -1.618743441511583e-07\n",
      "Q: Output= -0.3270905138585771 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 31 Training Loss: 0.17185872793197632 \n",
      "P: Output= -0.11851850853199508 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 1.6624351730757212 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 32 Training Loss: 0.17859184741973877 \n",
      "P: Output= -0.36424647339865235 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= 1.0989958052323425 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 33 Training Loss: 0.17393356561660767 \n",
      "P: Output= -0.2257875957728288 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 2.1137638193522976 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 34 Training Loss: 0.17217311263084412 \n",
      "P: Output= 0.16379918927690884 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 4.366550120734331 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 35 Training Loss: 0.18018841743469238 \n",
      "P: Output= -0.28064299561042194 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= 3.5560328621484043 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 36 Training Loss: 0.17369692027568817 \n",
      "P: Output= -0.3157891118185976 --- Target= 1.652745300617653e-07\n",
      "Q: Output= 3.8531470405998807 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 37 Training Loss: 0.17156442999839783 \n",
      "P: Output= -0.16748914809580295 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 4.735538556772946 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 38 Training Loss: 0.18337243795394897 \n",
      "P: Output= -0.7983641054060362 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= 3.220610169834748 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 39 Training Loss: 0.17049914598464966 \n",
      "P: Output= -0.8318505361421034 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= 3.036165554761423 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 40 Training Loss: 0.16610968112945557 \n",
      "P: Output= -0.8335736294008811 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 2.523312372510107 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 41 Training Loss: 0.16748486459255219 \n",
      "P: Output= -0.5791984583604153 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 3.119612331500095 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 42 Training Loss: 0.18404269218444824 \n",
      "P: Output= -0.6142778677622553 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 2.5700402421444455 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 43 Training Loss: 0.17170864343643188 \n",
      "P: Output= -1.0874854750806042 --- Target= -9.226324593214486e-08\n",
      "Q: Output= 0.8792479001925022 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 44 Training Loss: 0.16726943850517273 \n",
      "P: Output= -0.6047076605812629 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 2.0568271719112374 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 45 Training Loss: 0.17894868552684784 \n",
      "P: Output= -0.7357349875434291 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= 1.1356367239897383 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 46 Training Loss: 0.1630125790834427 \n",
      "P: Output= -0.19803825390150642 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 3.065700781333618 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 47 Training Loss: 0.16889341175556183 \n",
      "P: Output= -0.02274192313791712 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 3.6880973151308343 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 48 Training Loss: 0.19052350521087646 \n",
      "P: Output= -0.32166846848592634 --- Target= -2.741996745214692e-07\n",
      "Q: Output= 2.6444700475471814 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 49 Training Loss: 0.16111232340335846 \n",
      "P: Output= 0.18291826437901193 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 4.129045397211071 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 50 Training Loss: 0.18375447392463684 \n",
      "P: Output= 0.018965488017017584 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 4.262117240048925 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 51 Training Loss: 0.16675840318202972 \n",
      "P: Output= -0.392318707130654 --- Target= 3.465683571235445e-07\n",
      "Q: Output= 2.6000841723880335 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 52 Training Loss: 0.1673983335494995 \n",
      "P: Output= -0.6569522450422323 --- Target= 2.191291796904693e-07\n",
      "Q: Output= 1.9788319410065398 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 53 Training Loss: 0.168734610080719 \n",
      "P: Output= -0.8190090499013838 --- Target= 9.865799821540122e-08\n",
      "Q: Output= 1.3374581235602738 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 54 Training Loss: 0.15978358685970306 \n",
      "P: Output= -0.4830380947696771 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 1.8612947596930018 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 55 Training Loss: 0.17410904169082642 \n",
      "P: Output= -0.6285318782596443 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 1.5577369265947176 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 56 Training Loss: 0.1704919934272766 \n",
      "P: Output= -0.8197746572185922 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 1.2922531128161605 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 57 Training Loss: 0.16806384921073914 \n",
      "P: Output= -1.2937154964911457 --- Target= 9.79909344778207e-08\n",
      "Q: Output= -0.17906066056470582 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 58 Training Loss: 0.16547170281410217 \n",
      "P: Output= -0.7744391825734844 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= 1.3312337431399923 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 59 Training Loss: 0.17011010646820068 \n",
      "P: Output= -0.8051993092314689 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= 1.5441629976218358 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 60 Training Loss: 0.17079845070838928 \n",
      "P: Output= -0.7494552037438433 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 1.4528872083196482 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 61 Training Loss: 0.1630946695804596 \n",
      "P: Output= -1.0987610619368313 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= 0.12833553005073828 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 62 Training Loss: 0.16113895177841187 \n",
      "P: Output= -0.572531845067032 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 1.5647068485499762 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 63 Training Loss: 0.1724155843257904 \n",
      "P: Output= -0.6070814783697536 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 1.6184005970500221 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 64 Training Loss: 0.1747182309627533 \n",
      "P: Output= -0.5403201653675813 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 1.604228790959688 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 65 Training Loss: 0.17162755131721497 \n",
      "P: Output= -0.4116095707691523 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 1.809513310968641 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 66 Training Loss: 0.16866612434387207 \n",
      "P: Output= -0.43997517071106884 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 1.803933004515061 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 67 Training Loss: 0.1750640571117401 \n",
      "P: Output= -0.4268062851840426 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 1.9692615914840284 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 68 Training Loss: 0.172553151845932 \n",
      "P: Output= -0.893599808198041 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= 0.5803966015031881 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 69 Training Loss: 0.1635616570711136 \n",
      "P: Output= -0.9089222992015449 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= 0.8889847486677391 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 70 Training Loss: 0.16043031215667725 \n",
      "P: Output= -0.7532781675212918 --- Target= 9.813912260625557e-08\n",
      "Q: Output= 1.206824301268962 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 71 Training Loss: 0.16266345977783203 \n",
      "P: Output= -0.7687958419081831 --- Target= -8.441947496606872e-08\n",
      "Q: Output= 1.2086830700302054 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 72 Training Loss: 0.17116135358810425 \n",
      "P: Output= -0.5470121978267777 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 2.1379744542794956 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 73 Training Loss: 0.16228875517845154 \n",
      "P: Output= -0.023835921983777375 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 3.8505347010045625 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 74 Training Loss: 0.17063145339488983 \n",
      "P: Output= -0.4195371654851199 --- Target= -8.159233733096016e-08\n",
      "Q: Output= 2.749131827024515 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 75 Training Loss: 0.1610237956047058 \n",
      "P: Output= 0.08060796559878014 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 4.3285736755342406 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 76 Training Loss: 0.17139668762683868 \n",
      "P: Output= 0.06636431820010902 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 4.325688460691916 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 77 Training Loss: 0.17137184739112854 \n",
      "P: Output= -0.10220438720242253 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= 4.123762903753162 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 78 Training Loss: 0.1791476309299469 \n",
      "P: Output= -0.550207817048614 --- Target= 4.168930338721566e-07\n",
      "Q: Output= 2.5565274795765163 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 79 Training Loss: 0.16137760877609253 \n",
      "P: Output= -0.6037351428668289 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 2.0469427648258547 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 80 Training Loss: 0.16864247620105743 \n",
      "P: Output= -0.28424196435237903 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 3.228785413920516 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 81 Training Loss: 0.1750149130821228 \n",
      "P: Output= -0.9066468979195497 --- Target= -5.012515025271114e-08\n",
      "Q: Output= 1.7588215287746438 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 82 Training Loss: 0.16371864080429077 \n",
      "P: Output= -0.3500707477512961 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 2.733006779390573 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 83 Training Loss: 0.17281393706798553 \n",
      "P: Output= -0.8936383224699194 --- Target= -3.730653368450021e-07\n",
      "Q: Output= 1.6944522592011566 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 84 Training Loss: 0.1566392481327057 \n",
      "P: Output= -0.8511576311253863 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= 1.698024035037144 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 85 Training Loss: 0.15933546423912048 \n",
      "P: Output= -0.49312407258137814 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 3.0699728050046735 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 86 Training Loss: 0.18474215269088745 \n",
      "P: Output= -0.690197447476657 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= 1.8291665126845258 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 87 Training Loss: 0.16741424798965454 \n",
      "P: Output= -0.5599948908442496 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 2.2813483154207024 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 88 Training Loss: 0.16838636994361877 \n",
      "P: Output= -0.5413527271793814 --- Target= 1.606130251019522e-08\n",
      "Q: Output= 2.840517823678823 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 89 Training Loss: 0.16270244121551514 \n",
      "P: Output= -0.3772077450102662 --- Target= 8.498954429114747e-08\n",
      "Q: Output= 3.4519109534496213 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 90 Training Loss: 0.1601741760969162 \n",
      "P: Output= -0.06663553672538391 --- Target= 1.72196169323513e-07\n",
      "Q: Output= 4.91372418576152 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 91 Training Loss: 0.17083214223384857 \n",
      "P: Output= -0.4098714596984232 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= 3.6541695319882272 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 92 Training Loss: 0.1638643443584442 \n",
      "P: Output= -0.20976663624565006 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= 4.172558057238861 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 93 Training Loss: 0.17980340123176575 \n",
      "P: Output= -0.8658053746078558 --- Target= 1.04987089244446e-07\n",
      "Q: Output= 1.9531216725858584 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 94 Training Loss: 0.15380579233169556 \n",
      "P: Output= -0.9558863771118178 --- Target= -9.128675593217395e-09\n",
      "Q: Output= 0.3812319068697789 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 95 Training Loss: 0.15917536616325378 \n",
      "P: Output= -0.6473011922119651 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 0.9916740413439191 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 96 Training Loss: 0.1730080097913742 \n",
      "P: Output= -0.8124951849181734 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 0.5199868951846689 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 97 Training Loss: 0.17186065018177032 \n",
      "P: Output= -1.3392000927912413 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= -1.1760031898110315 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 98 Training Loss: 0.1562815010547638 \n",
      "P: Output= -1.252209172945661 --- Target= 1.93652547331169e-07\n",
      "Q: Output= -1.260930746126097 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 99 Training Loss: 0.17056620121002197 \n",
      "P: Output= -0.7064654775693953 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= 0.5912767283111071 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 100 Training Loss: 0.16635762155056 \n",
      "P: Output= -1.146279509651774 --- Target= 1.239808575803636e-07\n",
      "Q: Output= -0.23214861804590559 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 101 Training Loss: 0.16642338037490845 \n",
      "P: Output= -0.8726293605316018 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= 0.5873655055184095 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 102 Training Loss: 0.1641189306974411 \n",
      "P: Output= -0.4367306048723769 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 2.8394544470913186 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 103 Training Loss: 0.1725335419178009 \n",
      "P: Output= -0.5853441422549421 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= 2.7039560539971603 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 104 Training Loss: 0.16610372066497803 \n",
      "P: Output= -0.4762285581955821 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= 3.533356511168905 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 105 Training Loss: 0.1620372086763382 \n",
      "P: Output= -0.4105413123882533 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= 4.323413609334121 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 106 Training Loss: 0.16166657209396362 \n",
      "P: Output= -0.3817958238462271 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= 4.802490013456692 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 107 Training Loss: 0.1596461981534958 \n",
      "P: Output= -0.2677859395844182 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 5.44886341488915 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 108 Training Loss: 0.16907575726509094 \n",
      "P: Output= -0.3647919964046773 --- Target= 5.936255753624664e-08\n",
      "Q: Output= 4.795654160816799 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 109 Training Loss: 0.1823769211769104 \n",
      "P: Output= -0.6937834382542354 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 3.9351542058463824 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 110 Training Loss: 0.16797509789466858 \n",
      "P: Output= -1.4052371150283385 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 1.5203377230608686 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 111 Training Loss: 0.1593017280101776 \n",
      "P: Output= -1.279203566756017 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 1.710088243052498 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 112 Training Loss: 0.16689205169677734 \n",
      "P: Output= -1.1662216281545428 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 0.6735055734948752 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 113 Training Loss: 0.17133185267448425 \n",
      "P: Output= -1.0847777833186552 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 0.22568217998467333 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 114 Training Loss: 0.17655061185359955 \n",
      "P: Output= -1.6236178006609299 --- Target= -1.695987421612699e-07\n",
      "Q: Output= -1.1212435098809461 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 115 Training Loss: 0.16365405917167664 \n",
      "P: Output= -0.9910686388534442 --- Target= -4.617224291791899e-08\n",
      "Q: Output= -0.2544206360818233 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 116 Training Loss: 0.18659813702106476 \n",
      "P: Output= -0.7467281495576801 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= -0.2348551705765063 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 117 Training Loss: 0.16999535262584686 \n",
      "P: Output= -0.5497249476937753 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 0.48942478995083594 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 118 Training Loss: 0.1722702533006668 \n",
      "P: Output= -0.6524267528428922 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 0.6991132789648749 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 119 Training Loss: 0.16613197326660156 \n",
      "P: Output= -0.8238179600292073 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= 0.09807126054784998 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 120 Training Loss: 0.16052433848381042 \n",
      "P: Output= -0.18434015656473335 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= 1.3560105067646555 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 121 Training Loss: 0.1718725711107254 \n",
      "P: Output= -0.6809797008141851 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= 0.8138482060795047 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 122 Training Loss: 0.1599724292755127 \n",
      "P: Output= -0.09981618495363254 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 2.68824782715415 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 123 Training Loss: 0.16068419814109802 \n",
      "P: Output= -0.14303035766102123 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 2.9887080909830246 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 124 Training Loss: 0.1654646098613739 \n",
      "P: Output= -0.30560463407618776 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 3.291782996888367 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 125 Training Loss: 0.17622117698192596 \n",
      "P: Output= -0.710053735770245 --- Target= -3.576887390721595e-07\n",
      "Q: Output= 2.529580480317091 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 126 Training Loss: 0.1616734117269516 \n",
      "P: Output= -0.32467600810440267 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 3.809779774481454 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 127 Training Loss: 0.17131678760051727 \n",
      "P: Output= -0.900989640535804 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= 2.578213365953606 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 128 Training Loss: 0.1720091998577118 \n",
      "P: Output= -0.4594151429039455 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 3.7838617937351025 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 129 Training Loss: 0.16325214505195618 \n",
      "P: Output= -0.9679319193424787 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= 2.6862292904653096 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 130 Training Loss: 0.1598568856716156 \n",
      "P: Output= -0.6782964001065261 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 3.738633703715948 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 131 Training Loss: 0.16493183374404907 \n",
      "P: Output= -0.8733082952093163 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 3.63757756727009 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 132 Training Loss: 0.16577930748462677 \n",
      "P: Output= -0.9519852979266004 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 3.3675725896138156 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 133 Training Loss: 0.16481393575668335 \n",
      "P: Output= -0.8141846729337381 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 2.7918929541822735 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 134 Training Loss: 0.16333425045013428 \n",
      "P: Output= -0.9104952807387541 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 2.6093619858397625 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 135 Training Loss: 0.16828259825706482 \n",
      "P: Output= -1.3084888462030095 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 0.9997763054713369 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 136 Training Loss: 0.15913397073745728 \n",
      "P: Output= -0.867085338513383 --- Target= 6.896767335007326e-08\n",
      "Q: Output= 2.2096956210955083 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 137 Training Loss: 0.16931003332138062 \n",
      "P: Output= -0.8555029005929704 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 2.3555621892885545 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 138 Training Loss: 0.1726500689983368 \n",
      "P: Output= -0.6532081737682995 --- Target= 2.021405629548667e-07\n",
      "Q: Output= 2.824143860634817 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 139 Training Loss: 0.16843272745609283 \n",
      "P: Output= -1.0310030472117315 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= 1.8042866106534827 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 140 Training Loss: 0.15634429454803467 \n",
      "P: Output= -0.9901116978804216 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 2.1309516499881953 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 141 Training Loss: 0.15928785502910614 \n",
      "P: Output= -0.9390657666995352 --- Target= -4.072268247057309e-07\n",
      "Q: Output= 2.236246728152092 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 142 Training Loss: 0.16274863481521606 \n",
      "P: Output= -0.44781877239801027 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 3.55018493599035 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 143 Training Loss: 0.17802733182907104 \n",
      "P: Output= -0.7446557183352027 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= 2.241294066659173 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 144 Training Loss: 0.16278710961341858 \n",
      "P: Output= -0.6418359706011634 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= 2.416888222967869 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 145 Training Loss: 0.1597607582807541 \n",
      "P: Output= -0.15227583114102483 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 3.0884834683539513 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 146 Training Loss: 0.16349074244499207 \n",
      "P: Output= -0.7770443992060336 --- Target= 6.39805808333449e-09\n",
      "Q: Output= 1.5302316002953305 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 147 Training Loss: 0.1556035578250885 \n",
      "P: Output= -1.0123673916738607 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= 0.9846327186279265 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 148 Training Loss: 0.16412246227264404 \n",
      "P: Output= -0.4744255369390036 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 1.7579231722823465 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 149 Training Loss: 0.18486127257347107 \n",
      "P: Output= -1.1293333348607888 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 0.4955120151994379 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 150 Training Loss: 0.1576065719127655 \n",
      "P: Output= -0.6472626604106573 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 1.4527804063664629 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 151 Training Loss: 0.1714479625225067 \n",
      "P: Output= -0.7284687745285936 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 1.6375869570227346 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 152 Training Loss: 0.15979810059070587 \n",
      "P: Output= -0.8533647571964211 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 1.6829132272617349 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 153 Training Loss: 0.16452544927597046 \n",
      "P: Output= -0.9835309874943752 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 1.999744450899085 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 154 Training Loss: 0.1634064018726349 \n",
      "P: Output= -1.0687083693930486 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 0.9489503160135868 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 155 Training Loss: 0.16341349482536316 \n",
      "P: Output= -1.310728609943836 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= 0.8581877917792715 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 156 Training Loss: 0.15340331196784973 \n",
      "P: Output= -0.6913277283814594 --- Target= 6.895684645513711e-08\n",
      "Q: Output= 1.9268025909451563 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 157 Training Loss: 0.17830497026443481 \n",
      "P: Output= -0.726308950830358 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 1.9960076482319407 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 158 Training Loss: 0.17403458058834076 \n",
      "P: Output= -0.757885119810159 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 2.051142558036428 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 159 Training Loss: 0.16223934292793274 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -0.6618313599318171 --- Target= 1.3670019605172e-07\n",
      "Q: Output= 2.0645437179518247 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 0 Validation Loss: 0.1609344184398651 \n",
      "P: Output= -0.7360748698545763 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= 1.8970056445201253 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 1 Validation Loss: 0.16394111514091492 \n",
      "P: Output= -1.1995463548231395 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= 1.0426002281366582 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 2 Validation Loss: 0.1505890041589737 \n",
      "P: Output= -0.7025425968763024 --- Target= 2.077415652834702e-07\n",
      "Q: Output= 2.2367301523623837 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 3 Validation Loss: 0.1599317342042923 \n",
      "P: Output= -0.7367121473650684 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= 2.109548027662142 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 4 Validation Loss: 0.15975569188594818 \n",
      "P: Output= -1.1870697977650408 --- Target= 2.239196774667107e-07\n",
      "Q: Output= 0.7296694456865627 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 5 Validation Loss: 0.15896743535995483 \n",
      "P: Output= -1.160984091186723 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= 0.7287706234549045 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 6 Validation Loss: 0.1636275202035904 \n",
      "P: Output= -0.749650895550106 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= 1.894776174654929 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 7 Validation Loss: 0.16430619359016418 \n",
      "P: Output= -1.2048675757682084 --- Target= 2.911645777814442e-07\n",
      "Q: Output= 0.8169308154025172 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 8 Validation Loss: 0.16023385524749756 \n",
      "P: Output= -0.8267336960980041 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= 2.0706453146325687 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 9 Validation Loss: 0.16832193732261658 \n",
      "P: Output= -1.185672913591933 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= 0.7813192291418325 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 10 Validation Loss: 0.15305593609809875 \n",
      "P: Output= -0.6426904493588559 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= 2.118963899025097 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 11 Validation Loss: 0.16003355383872986 \n",
      "P: Output= -0.8751974692744344 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= 2.0170723067161163 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 12 Validation Loss: 0.17071178555488586 \n",
      "P: Output= -1.0675660487998728 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= 0.6291100580832678 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 13 Validation Loss: 0.16143101453781128 \n",
      "P: Output= -0.8220699174536703 --- Target= 7.48511617132408e-08\n",
      "Q: Output= 1.9608983664456723 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 14 Validation Loss: 0.17184200882911682 \n",
      "P: Output= -1.1351265879593315 --- Target= -2.219336545650208e-07\n",
      "Q: Output= 0.7066572189716975 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 15 Validation Loss: 0.1541883796453476 \n",
      "P: Output= -1.133997044134614 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= 0.861195315928386 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 16 Validation Loss: 0.15192607045173645 \n",
      "P: Output= -0.6918235552059198 --- Target= -1.907385298594022e-07\n",
      "Q: Output= 2.0861443335339453 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 17 Validation Loss: 0.1687343418598175 \n",
      "P: Output= -0.6791711008634564 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= 2.117159023118015 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 18 Validation Loss: 0.16927868127822876 \n",
      "P: Output= -0.743496780809819 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= 2.0918852664951055 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 19 Validation Loss: 0.16274672746658325 \n",
      "P: Output= -0.6954271544210631 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= 2.0854494312268947 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 20 Validation Loss: 0.1593058705329895 \n",
      "P: Output= -1.1763492127466897 --- Target= 3.239203305582805e-08\n",
      "Q: Output= 0.7960345148774133 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 21 Validation Loss: 0.15456390380859375 \n",
      "P: Output= -1.0006964702495305 --- Target= 4.268641484728164e-07\n",
      "Q: Output= 0.8847156489289807 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 22 Validation Loss: 0.15764220058918 \n",
      "P: Output= -0.7581679066720888 --- Target= -8.584417265922184e-08\n",
      "Q: Output= 2.0227252994869103 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 23 Validation Loss: 0.16893130540847778 \n",
      "P: Output= -1.11935180254609 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= 0.8846967313632224 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 24 Validation Loss: 0.15541696548461914 \n",
      "P: Output= -0.8809073554335498 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= 2.1467145312803453 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 25 Validation Loss: 0.16713105142116547 \n",
      "P: Output= -0.6664974677694753 --- Target= 5.541669878539324e-08\n",
      "Q: Output= 2.0666603988696224 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 26 Validation Loss: 0.1623743772506714 \n",
      "P: Output= -1.050409652347585 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= 0.882423354784577 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 27 Validation Loss: 0.15599241852760315 \n",
      "P: Output= -0.7495967561973824 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= 2.0631934251258155 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 28 Validation Loss: 0.17445218563079834 \n",
      "P: Output= -0.761201619169241 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= 2.103198816437419 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 29 Validation Loss: 0.1583923101425171 \n",
      "Epoch: 3\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -1.0778376947092294 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= 1.1199558833551597 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 0 Training Loss: 0.15764379501342773 \n",
      "P: Output= -1.0432949175680513 --- Target= 1.652745300617653e-07\n",
      "Q: Output= 1.187825586655685 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 1 Training Loss: 0.15873131155967712 \n",
      "P: Output= -0.9932858599298671 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= 1.5256191463689879 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 2 Training Loss: 0.1553439199924469 \n",
      "P: Output= -0.5513367891399996 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= 2.5791666381063028 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 3 Training Loss: 0.16469749808311462 \n",
      "P: Output= -1.0555556086635853 --- Target= -1.695987421612699e-07\n",
      "Q: Output= 1.262422539337038 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 4 Training Loss: 0.1544843316078186 \n",
      "P: Output= -0.9896973108780722 --- Target= 4.168930338721566e-07\n",
      "Q: Output= 1.2121460094358438 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 5 Training Loss: 0.15451031923294067 \n",
      "P: Output= -0.5970952768251783 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 2.4602131063076005 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 6 Training Loss: 0.17309382557868958 \n",
      "P: Output= -0.3803116723648614 --- Target= 6.895684645513711e-08\n",
      "Q: Output= 2.538131611282866 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 7 Training Loss: 0.16746830940246582 \n",
      "P: Output= -0.35790419985221966 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 2.5506762937632583 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 8 Training Loss: 0.16774246096611023 \n",
      "P: Output= -0.866114722222707 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= 1.204457471146954 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 9 Training Loss: 0.15709924697875977 \n",
      "P: Output= -0.8123155868171752 --- Target= -4.072268247057309e-07\n",
      "Q: Output= 1.058084880480476 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 10 Training Loss: 0.16043585538864136 \n",
      "P: Output= -0.2682330463832532 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= 2.6774367780639654 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 11 Training Loss: 0.16070355474948883 \n",
      "P: Output= -0.2592277888173351 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 2.8052446394872934 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 12 Training Loss: 0.1683635413646698 \n",
      "P: Output= -0.7682284045391672 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 1.2439802540896743 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 13 Training Loss: 0.151503786444664 \n",
      "P: Output= -0.469306894002508 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 2.831037652791985 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 14 Training Loss: 0.17736056447029114 \n",
      "P: Output= -0.3309215555084375 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 2.7001417739440905 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 15 Training Loss: 0.1714218556880951 \n",
      "P: Output= -0.48011123522131793 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 3.118891784017288 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 16 Training Loss: 0.16185647249221802 \n",
      "P: Output= -0.47475928719110083 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 3.334151603311989 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 17 Training Loss: 0.16288095712661743 \n",
      "P: Output= -0.723759199432374 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 3.2977234824237858 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 18 Training Loss: 0.16630247235298157 \n",
      "P: Output= -0.8228160459986169 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 3.189702479253909 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 19 Training Loss: 0.16223233938217163 \n",
      "P: Output= -0.6695461299583849 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 2.8244788093010698 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 20 Training Loss: 0.16093863546848297 \n",
      "P: Output= -0.7266984579433 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 2.304842189969049 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 21 Training Loss: 0.1642688810825348 \n",
      "P: Output= -0.8176467720234122 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 1.9172949799174184 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 22 Training Loss: 0.15703867375850677 \n",
      "P: Output= -1.3427395520161678 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= -0.03944089929761141 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 23 Training Loss: 0.16083936393260956 \n",
      "P: Output= -0.7992499611490596 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 0.6885883412991705 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 24 Training Loss: 0.16478198766708374 \n",
      "P: Output= -0.8170441083625839 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 0.6825703319931948 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 25 Training Loss: 0.16147887706756592 \n",
      "P: Output= -0.6380679642179352 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 0.7079607000284982 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 26 Training Loss: 0.15523213148117065 \n",
      "P: Output= -0.7679683451518287 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 1.1748721651532597 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 27 Training Loss: 0.16051265597343445 \n",
      "P: Output= -0.46426110615403093 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 1.682778015117866 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 28 Training Loss: 0.17278838157653809 \n",
      "P: Output= -0.27086652810596146 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 2.2634152216616874 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 29 Training Loss: 0.16735342144966125 \n",
      "P: Output= -0.1790633228991938 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 2.870591972517241 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 30 Training Loss: 0.1571674346923828 \n",
      "P: Output= -0.5961476216427766 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= 1.8511367646881736 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 31 Training Loss: 0.15466001629829407 \n",
      "P: Output= -0.5770235568226632 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= 2.1606549423666968 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 32 Training Loss: 0.15736024081707 \n",
      "P: Output= -0.44859235680837184 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 2.184744492391834 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 33 Training Loss: 0.16080880165100098 \n",
      "P: Output= -0.1430617175338531 --- Target= 5.936255753624664e-08\n",
      "Q: Output= 3.1665392785791635 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 34 Training Loss: 0.18635348975658417 \n",
      "P: Output= -0.3805879762785809 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 2.768248952160155 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 35 Training Loss: 0.16325166821479797 \n",
      "P: Output= -0.9359364080815338 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 0.7735579461635416 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 36 Training Loss: 0.16043896973133087 \n",
      "P: Output= -1.0353958955993745 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= 0.4646150494181187 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 37 Training Loss: 0.15690836310386658 \n",
      "P: Output= -1.1596135044586475 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= 0.42441144945923703 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 38 Training Loss: 0.15117357671260834 \n",
      "P: Output= -1.1654314618953654 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= 0.25635392071569374 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 39 Training Loss: 0.16290301084518433 \n",
      "P: Output= -0.908182111711775 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 0.8360191753979986 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 40 Training Loss: 0.15272441506385803 \n",
      "P: Output= -0.41305923676478695 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 2.455252972176355 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 41 Training Loss: 0.15786704421043396 \n",
      "P: Output= -0.39404797634531974 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 2.7737847172617762 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 42 Training Loss: 0.1653684675693512 \n",
      "P: Output= -0.6087634707627423 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 1.8581079913616083 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 43 Training Loss: 0.1519818902015686 \n",
      "P: Output= -0.14665930775143643 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 3.989825699531875 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 44 Training Loss: 0.15925882756710052 \n",
      "P: Output= -0.4617899616173782 --- Target= 2.191291796904693e-07\n",
      "Q: Output= 2.669878230016919 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 45 Training Loss: 0.15846364200115204 \n",
      "P: Output= -0.17632433610024734 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 4.371799117949532 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 46 Training Loss: 0.16547411680221558 \n",
      "P: Output= -0.08058182732162766 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 4.669533544227166 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 47 Training Loss: 0.16456171870231628 \n",
      "P: Output= -0.6969543552370707 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 3.3759848266403605 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 48 Training Loss: 0.15275216102600098 \n",
      "P: Output= -0.5107183531051387 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 4.236575083283741 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 49 Training Loss: 0.16570691764354706 \n",
      "P: Output= -1.1445671543275546 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= 2.503480974699058 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 50 Training Loss: 0.15200240910053253 \n",
      "P: Output= -1.3264383326384426 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= 1.9133706921609095 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 51 Training Loss: 0.15048573911190033 \n",
      "P: Output= -1.412786269341808 --- Target= 6.39805808333449e-09\n",
      "Q: Output= 1.393532895872637 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 52 Training Loss: 0.15264040231704712 \n",
      "P: Output= -1.4594310149071807 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= 0.833381883214197 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 53 Training Loss: 0.15595762431621552 \n",
      "P: Output= -0.970104087505498 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 1.9024247733293462 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 54 Training Loss: 0.15456993877887726 \n",
      "P: Output= -0.911918547034106 --- Target= 6.896767335007326e-08\n",
      "Q: Output= 1.6175973520379392 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 55 Training Loss: 0.17691010236740112 \n",
      "P: Output= -0.7347152109011121 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 1.6712068761091912 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 56 Training Loss: 0.15515056252479553 \n",
      "P: Output= -1.0601222338113754 --- Target= -8.441947496606872e-08\n",
      "Q: Output= 0.22514905509440908 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 57 Training Loss: 0.15722927451133728 \n",
      "P: Output= -0.7793262808713175 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= 1.1290124803926576 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 58 Training Loss: 0.14982876181602478 \n",
      "P: Output= -0.5297319489972239 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 1.3741784703192312 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 59 Training Loss: 0.15748882293701172 \n",
      "P: Output= -0.036071207247201365 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= 3.1699541085790184 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 60 Training Loss: 0.1651429533958435 \n",
      "P: Output= -0.5301591230858058 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= 2.3800660245666405 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 61 Training Loss: 0.14955976605415344 \n",
      "P: Output= -0.642929368211985 --- Target= -5.012515025271114e-08\n",
      "Q: Output= 2.196234663414674 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 62 Training Loss: 0.15201342105865479 \n",
      "P: Output= -0.5375122571838302 --- Target= 8.498954429114747e-08\n",
      "Q: Output= 1.80117480256102 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 63 Training Loss: 0.1488330215215683 \n",
      "P: Output= -0.4269325158588586 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 2.704389992808549 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 64 Training Loss: 0.16686584055423737 \n",
      "P: Output= -0.48405579815962874 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= 2.4056884610042166 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 65 Training Loss: 0.15773436427116394 \n",
      "P: Output= -1.0506310578456715 --- Target= -3.576887390721595e-07\n",
      "Q: Output= 0.4079680943727997 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 66 Training Loss: 0.15110421180725098 \n",
      "P: Output= -1.2029486549577904 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= -0.19285719791535882 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 67 Training Loss: 0.14602933824062347 \n",
      "P: Output= -0.617918722428179 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 1.2278528966566506 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 68 Training Loss: 0.16276822984218597 \n",
      "P: Output= -1.0273160623273112 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= -0.447615368142694 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 69 Training Loss: 0.15029913187026978 \n",
      "P: Output= -0.9681450434571293 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= -0.020063990359868278 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 70 Training Loss: 0.1505439281463623 \n",
      "P: Output= -0.4392420877960701 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 1.3180784445390197 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 71 Training Loss: 0.15703505277633667 \n",
      "P: Output= -0.39225225064930846 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 2.2754701281490304 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 72 Training Loss: 0.16646131873130798 \n",
      "P: Output= -0.33201605078471186 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= 2.9024930198728818 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 73 Training Loss: 0.15860958397388458 \n",
      "P: Output= -0.4702089124913602 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 3.5079918067395885 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 74 Training Loss: 0.15550664067268372 \n",
      "P: Output= -0.45202406577707155 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 3.6993294142150415 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 75 Training Loss: 0.1585179567337036 \n",
      "P: Output= -0.4751501819327091 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 3.495808167468338 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 76 Training Loss: 0.16609254479408264 \n",
      "P: Output= -0.5972091930889274 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 3.2880367524080114 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 77 Training Loss: 0.16894546151161194 \n",
      "P: Output= -0.6138202975465799 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 3.1687248555170866 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 78 Training Loss: 0.16510072350502014 \n",
      "P: Output= -0.6753976013129268 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 2.647566124347329 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 79 Training Loss: 0.15479342639446259 \n",
      "P: Output= -1.0556119943006603 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= 0.8116253761540007 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 80 Training Loss: 0.15174175798892975 \n",
      "P: Output= -0.7092480878323615 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 2.0039688817807164 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 81 Training Loss: 0.16733117401599884 \n",
      "P: Output= -0.9976140646677223 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 0.6323006090810006 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 82 Training Loss: 0.1466297060251236 \n",
      "P: Output= -0.9226865419384165 --- Target= 9.79909344778207e-08\n",
      "Q: Output= 0.1551942309182861 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 83 Training Loss: 0.15549668669700623 \n",
      "P: Output= -0.8667215908471455 --- Target= -2.741996745214692e-07\n",
      "Q: Output= 0.31915449590050127 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 84 Training Loss: 0.15074700117111206 \n",
      "P: Output= -0.897528465509426 --- Target= 1.04987089244446e-07\n",
      "Q: Output= 1.0083938872490066 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 85 Training Loss: 0.1439879983663559 \n",
      "P: Output= -0.33659478540269827 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 2.6963511312040787 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 86 Training Loss: 0.16704192757606506 \n",
      "P: Output= -0.3276811946264555 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 2.7996169735316503 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 87 Training Loss: 0.15149381756782532 \n",
      "P: Output= -0.5085594264189695 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= 2.7283298195194856 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 88 Training Loss: 0.16148385405540466 \n",
      "P: Output= -0.9816739878280547 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= 1.220849177193509 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 89 Training Loss: 0.15775595605373383 \n",
      "P: Output= -0.7092681319758309 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 2.576230632139591 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 90 Training Loss: 0.15719187259674072 \n",
      "P: Output= -0.5771145367878212 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 2.180310033798442 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 91 Training Loss: 0.16119222342967987 \n",
      "P: Output= -1.1914254405569276 --- Target= -9.128675593217395e-09\n",
      "Q: Output= 0.7002508285367606 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 92 Training Loss: 0.1485070288181305 \n",
      "P: Output= -1.1938703794857117 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= 0.9992530489587557 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 93 Training Loss: 0.1487053632736206 \n",
      "P: Output= -0.6212728283445426 --- Target= -1.490649044200154e-07\n",
      "Q: Output= 2.3416362410219635 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 94 Training Loss: 0.1551397442817688 \n",
      "P: Output= -1.0624512513530693 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= 1.1709143909680826 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 95 Training Loss: 0.14705970883369446 \n",
      "P: Output= -0.9246691473329367 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= 1.4267642171737087 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 96 Training Loss: 0.14531418681144714 \n",
      "P: Output= -0.7043461528568367 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 1.6803960927674684 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 97 Training Loss: 0.15279781818389893 \n",
      "P: Output= -0.6206839122628605 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 2.3098865141087987 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 98 Training Loss: 0.1488179713487625 \n",
      "P: Output= 0.008446842071923477 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 3.9351593803012648 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 99 Training Loss: 0.15702763199806213 \n",
      "P: Output= 0.0008669423046576696 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 3.938353880109517 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 100 Training Loss: 0.18180888891220093 \n",
      "P: Output= -0.00914579577935104 --- Target= 9.096384445683725e-08\n",
      "Q: Output= 4.245334696864785 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 101 Training Loss: 0.16495276987552643 \n",
      "P: Output= -0.08437724926459911 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 3.9834115503509118 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 102 Training Loss: 0.16003870964050293 \n",
      "P: Output= -0.21001305406373572 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 3.300743015085855 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 103 Training Loss: 0.15230394899845123 \n",
      "P: Output= -0.42668337761507136 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 2.444991684559769 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 104 Training Loss: 0.16551907360553741 \n",
      "P: Output= -1.0916925237510053 --- Target= -6.546786224248535e-09\n",
      "Q: Output= 0.27845713060621957 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 105 Training Loss: 0.14666371047496796 \n",
      "P: Output= -0.8331184219687282 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 0.834137434994199 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 106 Training Loss: 0.1659756600856781 \n",
      "P: Output= -1.5275425387155526 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= -1.1515726101986337 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 107 Training Loss: 0.15019947290420532 \n",
      "P: Output= -1.1178298684849377 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 0.21038067754838785 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 108 Training Loss: 0.16091978549957275 \n",
      "P: Output= -0.9893721647015283 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 0.2542575523953525 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 109 Training Loss: 0.1546122282743454 \n",
      "P: Output= -1.5997112968064329 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= -0.7261174487398421 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 110 Training Loss: 0.15623679757118225 \n",
      "P: Output= -1.2989629014398654 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= 0.016154141879309414 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 111 Training Loss: 0.15707579255104065 \n",
      "P: Output= -0.8578452382078465 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 1.68147580753722 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 112 Training Loss: 0.15413746237754822 \n",
      "P: Output= -0.9206833165977031 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= 0.9642832550685076 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 113 Training Loss: 0.15513265132904053 \n",
      "P: Output= -0.7109245129932926 --- Target= 9.257290134456753e-08\n",
      "Q: Output= 1.972209606653065 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 114 Training Loss: 0.15234039723873138 \n",
      "P: Output= -0.005459096078558545 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 3.8622261581448196 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 115 Training Loss: 0.15917812287807465 \n",
      "P: Output= 0.006105006186706952 --- Target= 2.021405629548667e-07\n",
      "Q: Output= 4.169453518894776 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 116 Training Loss: 0.1464427411556244 \n",
      "P: Output= -0.5212801629951418 --- Target= -1.618743441511583e-07\n",
      "Q: Output= 2.7162970444634436 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 117 Training Loss: 0.1446554809808731 \n",
      "P: Output= -0.5122883347584324 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= 2.4052040731971243 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 118 Training Loss: 0.15047293901443481 \n",
      "P: Output= -0.25718609383171653 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 3.798745926546051 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 119 Training Loss: 0.16421547532081604 \n",
      "P: Output= -0.7771959673992868 --- Target= 1.606130251019522e-08\n",
      "Q: Output= 1.8116078482577667 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 120 Training Loss: 0.14981189370155334 \n",
      "P: Output= -0.9407029524640924 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= 1.8438810814453266 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 121 Training Loss: 0.15983593463897705 \n",
      "P: Output= -0.8696409945569883 --- Target= -9.703328984755899e-08\n",
      "Q: Output= 1.7821667856631853 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 122 Training Loss: 0.14623227715492249 \n",
      "P: Output= -0.6210693803542551 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 3.008823677401111 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 123 Training Loss: 0.16240648925304413 \n",
      "P: Output= -0.6731970272144991 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 2.662502694037368 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 124 Training Loss: 0.16419178247451782 \n",
      "P: Output= -0.6209051438942428 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 2.1037387349112278 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 125 Training Loss: 0.15065807104110718 \n",
      "P: Output= -0.7339594495710715 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 1.395593612091666 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 126 Training Loss: 0.15832874178886414 \n",
      "P: Output= -1.0597231962996503 --- Target= 1.93652547331169e-07\n",
      "Q: Output= -0.3064962227535055 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 127 Training Loss: 0.15335358679294586 \n",
      "P: Output= -0.6110842133208179 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 1.3667688220899858 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 128 Training Loss: 0.15591825544834137 \n",
      "P: Output= -0.5991260432781109 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 1.5231584503495261 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 129 Training Loss: 0.16236160695552826 \n",
      "P: Output= -0.9912254780633551 --- Target= 1.239808575803636e-07\n",
      "Q: Output= 0.25429214175976345 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 130 Training Loss: 0.15135584771633148 \n",
      "P: Output= -0.49871215132717506 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 2.147239521822204 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 131 Training Loss: 0.1700640469789505 \n",
      "P: Output= -0.8192321753550962 --- Target= -8.159233733096016e-08\n",
      "Q: Output= 1.5000587045939628 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 132 Training Loss: 0.14855735003948212 \n",
      "P: Output= -0.8285975362866989 --- Target= 9.865799821540122e-08\n",
      "Q: Output= 1.8885029729021774 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 133 Training Loss: 0.14734597504138947 \n",
      "P: Output= -0.9178570565023945 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= 1.8735784308918815 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 134 Training Loss: 0.14672334492206573 \n",
      "P: Output= -0.846914412917906 --- Target= 9.813912260625557e-08\n",
      "Q: Output= 1.630034057499004 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 135 Training Loss: 0.14824631810188293 \n",
      "P: Output= -0.8461698836552705 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 0.9974783799860365 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 136 Training Loss: 0.147832989692688 \n",
      "P: Output= -0.6038131123631381 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= 1.7992722551636833 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 137 Training Loss: 0.14873889088630676 \n",
      "P: Output= -0.7165668760277315 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 0.9354635402665288 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 138 Training Loss: 0.1606721580028534 \n",
      "P: Output= -1.2133141631222308 --- Target= 9.40822530992591e-10\n",
      "Q: Output= -1.3228484700641996 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 139 Training Loss: 0.14476439356803894 \n",
      "P: Output= -1.066987003125484 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= -1.536677373506767 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 140 Training Loss: 0.1567780077457428 \n",
      "P: Output= -1.1945531005986147 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= -1.8651443739955766 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 141 Training Loss: 0.14972621202468872 \n",
      "P: Output= -0.5252240793385656 --- Target= 1.72196169323513e-07\n",
      "Q: Output= -0.29814828098616264 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 142 Training Loss: 0.15689796209335327 \n",
      "P: Output= -0.8331800517418886 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= -1.764377813612255 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 143 Training Loss: 0.14992472529411316 \n",
      "P: Output= -0.27310018946211745 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= -0.10250568213851796 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 144 Training Loss: 0.15986783802509308 \n",
      "P: Output= -0.17287522658901366 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 0.5884595340097123 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 145 Training Loss: 0.15436363220214844 \n",
      "P: Output= -0.19614251539840755 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= 1.3503652467397202 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 146 Training Loss: 0.1561882197856903 \n",
      "P: Output= -0.22453889594236198 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 2.2407124463985015 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 147 Training Loss: 0.15807963907718658 \n",
      "P: Output= -0.6757754071037141 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= 1.1183019179257299 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 148 Training Loss: 0.14625892043113708 \n",
      "P: Output= -0.2219446384013093 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 2.8989638757439584 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 149 Training Loss: 0.15659302473068237 \n",
      "P: Output= -0.2883208272479765 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 3.274775261686349 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 150 Training Loss: 0.16138550639152527 \n",
      "P: Output= -0.7563537206883959 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= 2.040096273309927 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 151 Training Loss: 0.15285241603851318 \n",
      "P: Output= -0.22836951969047892 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 3.910287170233806 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 152 Training Loss: 0.15379533171653748 \n",
      "P: Output= -0.7908873180427207 --- Target= -9.226324593214486e-08\n",
      "Q: Output= 2.442130782623164 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 153 Training Loss: 0.14541074633598328 \n",
      "P: Output= -0.7945308730838496 --- Target= 3.465683571235445e-07\n",
      "Q: Output= 2.304063295128266 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 154 Training Loss: 0.14718985557556152 \n",
      "P: Output= -0.4589045344707001 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 4.418909494086685 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 155 Training Loss: 0.1536259651184082 \n",
      "P: Output= -0.7538565014027778 --- Target= 1.885248828159547e-07\n",
      "Q: Output= 2.8247547716621524 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 156 Training Loss: 0.14425712823867798 \n",
      "P: Output= -0.9422545306361041 --- Target= -3.730653368450021e-07\n",
      "Q: Output= 2.8822298916626483 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 157 Training Loss: 0.14063747227191925 \n",
      "P: Output= -0.6087627828810813 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 3.859101700246798 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 158 Training Loss: 0.153267502784729 \n",
      "P: Output= -1.1094847484305568 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= 1.9555413098473124 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 159 Training Loss: 0.14313676953315735 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -0.8326564419064031 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= 3.146542567326861 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 0 Validation Loss: 0.16397279500961304 \n",
      "P: Output= -1.1199014424966256 --- Target= -2.219336545650208e-07\n",
      "Q: Output= 1.5934727584274846 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 1 Validation Loss: 0.14550811052322388 \n",
      "P: Output= -0.7849676764235491 --- Target= 7.48511617132408e-08\n",
      "Q: Output= 3.0529475819292564 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 2 Validation Loss: 0.16239914298057556 \n",
      "P: Output= -1.1686126198157378 --- Target= 2.239196774667107e-07\n",
      "Q: Output= 1.6249535153259247 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 3 Validation Loss: 0.15180063247680664 \n",
      "P: Output= -0.7222437245732563 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= 3.2267989799895727 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 4 Validation Loss: 0.15060679614543915 \n",
      "P: Output= -1.1127087206977562 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= 1.7919029019601895 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 5 Validation Loss: 0.14468951523303986 \n",
      "P: Output= -0.6446073178037901 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= 3.222869616020799 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 6 Validation Loss: 0.15876063704490662 \n",
      "P: Output= -1.1407574207747029 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= 1.611481185668894 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 7 Validation Loss: 0.15505728125572205 \n",
      "P: Output= -1.0592237592562936 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= 1.4670548581162226 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 8 Validation Loss: 0.15273544192314148 \n",
      "P: Output= -1.0972777280776018 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= 1.8122640484477142 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 9 Validation Loss: 0.14571169018745422 \n",
      "P: Output= -0.6106509204241553 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= 3.2340371774596353 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 10 Validation Loss: 0.15549060702323914 \n",
      "P: Output= -0.7244680093128668 --- Target= -8.584417265922184e-08\n",
      "Q: Output= 3.136962078346116 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 11 Validation Loss: 0.15976159274578094 \n",
      "P: Output= -0.7059240329731189 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= 2.9621631258562484 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 12 Validation Loss: 0.15668469667434692 \n",
      "P: Output= -0.6593259497205528 --- Target= -1.907385298594022e-07\n",
      "Q: Output= 3.1895663805561165 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 13 Validation Loss: 0.16031382977962494 \n",
      "P: Output= -0.7068161417114132 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= 3.217401476819977 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 14 Validation Loss: 0.15283960103988647 \n",
      "P: Output= -0.7163647810502773 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= 3.178076520653695 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 15 Validation Loss: 0.16676980257034302 \n",
      "P: Output= -0.6306133495049915 --- Target= 1.3670019605172e-07\n",
      "Q: Output= 3.1781856649796705 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 16 Validation Loss: 0.1542377918958664 \n",
      "P: Output= -0.7866868943683762 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= 3.1960366409475345 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 17 Validation Loss: 0.1639326810836792 \n",
      "P: Output= -1.18050768883736 --- Target= 2.911645777814442e-07\n",
      "Q: Output= 1.7517665060498375 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 18 Validation Loss: 0.15014615654945374 \n",
      "P: Output= -0.985174787588349 --- Target= 4.268641484728164e-07\n",
      "Q: Output= 1.7852972758525665 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 19 Validation Loss: 0.1522105187177658 \n",
      "P: Output= -0.7178312865537491 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= 2.9628125797598095 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 20 Validation Loss: 0.15394650399684906 \n",
      "P: Output= -0.6633015748010216 --- Target= 2.077415652834702e-07\n",
      "Q: Output= 3.395438529002261 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 21 Validation Loss: 0.15124793350696564 \n",
      "P: Output= -1.153955816657823 --- Target= 3.239203305582805e-08\n",
      "Q: Output= 1.702013138922882 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 22 Validation Loss: 0.14344948530197144 \n",
      "P: Output= -0.8365631982736845 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= 3.3142242101638004 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 23 Validation Loss: 0.15502643585205078 \n",
      "P: Output= -0.7018267591402347 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= 3.2552258846998097 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 24 Validation Loss: 0.14892780780792236 \n",
      "P: Output= -0.6632399418668431 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= 3.1985682968499463 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 25 Validation Loss: 0.1498759686946869 \n",
      "P: Output= -1.1656533728188938 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= 1.7049887253559692 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 26 Validation Loss: 0.14436469972133636 \n",
      "P: Output= -1.165845023600859 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= 2.0354366876160457 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 27 Validation Loss: 0.14043672382831573 \n",
      "P: Output= -1.0327018680178917 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= 1.7810601071944658 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 28 Validation Loss: 0.1456320583820343 \n",
      "P: Output= -0.6340967505842974 --- Target= 5.541669878539324e-08\n",
      "Q: Output= 3.1680864274454077 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 29 Validation Loss: 0.15434642136096954 \n",
      "Epoch: 4\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -0.7449880324939899 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 3.256804340470948 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 0 Training Loss: 0.15414538979530334 \n",
      "P: Output= -1.1652040319215464 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= 1.7114545430033035 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 1 Training Loss: 0.14647364616394043 \n",
      "P: Output= -1.147048687354859 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= 1.5779754163409763 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 2 Training Loss: 0.1533934473991394 \n",
      "P: Output= -1.0535456428306968 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 1.7823918135752113 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 3 Training Loss: 0.14581885933876038 \n",
      "P: Output= -0.8459847115198684 --- Target= 1.652745300617653e-07\n",
      "Q: Output= 2.3609550785360334 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 4 Training Loss: 0.14850735664367676 \n",
      "P: Output= -0.41326846445034704 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= 4.132988790155693 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 5 Training Loss: 0.14839130640029907 \n",
      "P: Output= -0.5829088311257706 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= 3.0218795225928705 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 6 Training Loss: 0.15021279454231262 \n",
      "P: Output= -0.501245943150483 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= 3.1949603080009155 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 7 Training Loss: 0.14712965488433838 \n",
      "P: Output= -0.03208539254210674 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 4.869511796610153 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 8 Training Loss: 0.15569448471069336 \n",
      "P: Output= -0.42699308444671846 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= 3.3861788353614504 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 9 Training Loss: 0.14550644159317017 \n",
      "P: Output= -0.13500300431998546 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 4.123543628815758 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 10 Training Loss: 0.1522553265094757 \n",
      "P: Output= -0.5499515848974514 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 2.6608083559819775 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 11 Training Loss: 0.14781805872917175 \n",
      "P: Output= -0.636303953043007 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 2.668550921305549 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 12 Training Loss: 0.1506105661392212 \n",
      "P: Output= -0.3963988577184425 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 3.8816359176704083 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 13 Training Loss: 0.16641435027122498 \n",
      "P: Output= -0.9854854547965219 --- Target= -5.012515025271114e-08\n",
      "Q: Output= 2.3055245505180944 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 14 Training Loss: 0.15008720755577087 \n",
      "P: Output= -0.9690808772073733 --- Target= 9.865799821540122e-08\n",
      "Q: Output= 1.719589073385423 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 15 Training Loss: 0.14174261689186096 \n",
      "P: Output= -1.0555527366557405 --- Target= 9.40822530992591e-10\n",
      "Q: Output= 1.4028131916178541 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 16 Training Loss: 0.13996171951293945 \n",
      "P: Output= -0.6750329060513751 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 2.764952658710552 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 17 Training Loss: 0.16194221377372742 \n",
      "P: Output= -0.6149231928128565 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 2.7749573060846497 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 18 Training Loss: 0.1486843228340149 \n",
      "P: Output= -0.867675897147989 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 1.9533347715750509 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 19 Training Loss: 0.15560472011566162 \n",
      "P: Output= -1.2327964288708406 --- Target= 1.885248828159547e-07\n",
      "Q: Output= -0.06542431905682378 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 20 Training Loss: 0.14355459809303284 \n",
      "P: Output= -0.9534954899616288 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 0.889294670452661 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 21 Training Loss: 0.14929905533790588 \n",
      "P: Output= -1.4003703059453256 --- Target= -3.576887390721595e-07\n",
      "Q: Output= -0.4420198666946389 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 22 Training Loss: 0.14491549134254456 \n",
      "P: Output= -0.9422701208928839 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 0.7983352986446439 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 23 Training Loss: 0.1622755229473114 \n",
      "P: Output= -1.2607253470362094 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= -0.6799062897643999 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 24 Training Loss: 0.14389902353286743 \n",
      "P: Output= -0.6701419521944976 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= 0.9888402481130347 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 25 Training Loss: 0.15164630115032196 \n",
      "P: Output= -1.066943879885728 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= -0.5524350506191134 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 26 Training Loss: 0.14219966530799866 \n",
      "P: Output= -0.8691596619353934 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= -0.30538880740873164 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 27 Training Loss: 0.14287316799163818 \n",
      "P: Output= -0.3075858501442639 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 1.383748575262187 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 28 Training Loss: 0.16694073379039764 \n",
      "P: Output= -0.301036695979799 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 1.8387064385404255 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 29 Training Loss: 0.15275096893310547 \n",
      "P: Output= -0.3484792305584241 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 1.7930376273593644 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 30 Training Loss: 0.16384854912757874 \n",
      "P: Output= -0.5288598149191337 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 1.9025090950674866 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 31 Training Loss: 0.1519719809293747 \n",
      "P: Output= -0.8523050438582338 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= 0.5722959819176374 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 32 Training Loss: 0.1446969360113144 \n",
      "P: Output= -0.9770431292794912 --- Target= -9.128675593217395e-09\n",
      "Q: Output= 0.5306616588371389 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 33 Training Loss: 0.144450843334198 \n",
      "P: Output= -0.5907795700888352 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 2.2520303514917215 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 34 Training Loss: 0.1599029004573822 \n",
      "P: Output= -1.128120450770841 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 1.2084481389907236 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 35 Training Loss: 0.14195413887500763 \n",
      "P: Output= -1.2590459224498929 --- Target= 1.239808575803636e-07\n",
      "Q: Output= 0.6195741896388123 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 36 Training Loss: 0.14733700454235077 \n",
      "P: Output= -0.6926627957602349 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 2.2839705053812773 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 37 Training Loss: 0.1524391770362854 \n",
      "P: Output= -1.1718164493301204 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= 0.9498631782570008 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 38 Training Loss: 0.14978384971618652 \n",
      "P: Output= -1.1008284039845453 --- Target= -3.730653368450021e-07\n",
      "Q: Output= 1.8420064062535308 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 39 Training Loss: 0.13926295936107635 \n",
      "P: Output= -0.9158285204345322 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= 2.4799516513041766 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 40 Training Loss: 0.14801767468452454 \n",
      "P: Output= -0.43527274428697993 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 4.370811949357378 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 41 Training Loss: 0.15165585279464722 \n",
      "P: Output= -0.2707952371006064 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 5.185168206385947 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 42 Training Loss: 0.17150896787643433 \n",
      "P: Output= -0.13686248847550253 --- Target= -1.490649044200154e-07\n",
      "Q: Output= 5.576241941160202 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 43 Training Loss: 0.1521376073360443 \n",
      "P: Output= -0.5722863266080811 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 4.759837307576955 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 44 Training Loss: 0.14422912895679474 \n",
      "P: Output= -0.28340611732948506 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 6.618494262609155 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 45 Training Loss: 0.14355137944221497 \n",
      "P: Output= -0.480111722945332 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 6.310311025162532 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 46 Training Loss: 0.1590442955493927 \n",
      "P: Output= -0.9326874091552355 --- Target= -1.618743441511583e-07\n",
      "Q: Output= 5.062952235102269 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 47 Training Loss: 0.1413116604089737 \n",
      "P: Output= -1.0848279192867585 --- Target= 1.606130251019522e-08\n",
      "Q: Output= 4.4064881378193395 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 48 Training Loss: 0.14570948481559753 \n",
      "P: Output= -0.9724994808873513 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= 5.375444722777079 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 49 Training Loss: 0.1537916213274002 \n",
      "P: Output= -1.588623574579925 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= 3.311036273381882 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 50 Training Loss: 0.14084956049919128 \n",
      "P: Output= -1.7355027082885703 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= 2.598977286147554 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 51 Training Loss: 0.139470174908638 \n",
      "P: Output= -1.680508728582569 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 3.0092361668782885 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 52 Training Loss: 0.15238647162914276 \n",
      "P: Output= -1.660274718552933 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 2.2468013097029687 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 53 Training Loss: 0.1542683243751526 \n",
      "P: Output= -2.0137653731227294 --- Target= 6.39805808333449e-09\n",
      "Q: Output= 0.36811244718391034 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 54 Training Loss: 0.14238128066062927 \n",
      "P: Output= -1.9605746073923358 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= -0.16306049500427555 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 55 Training Loss: 0.1400122493505478 \n",
      "P: Output= -1.3899451861773597 --- Target= 2.021405629548667e-07\n",
      "Q: Output= 0.7652312851561796 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 56 Training Loss: 0.14769741892814636 \n",
      "P: Output= -1.862124655595319 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= -0.7854614821258776 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 57 Training Loss: 0.1510622501373291 \n",
      "P: Output= -1.0989608490966134 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 0.5633866606473736 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 58 Training Loss: 0.15985164046287537 \n",
      "P: Output= -1.107799356320192 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 0.5944404683393048 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 59 Training Loss: 0.1409759372472763 \n",
      "P: Output= -1.053761022256995 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 0.9430420556936348 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 60 Training Loss: 0.15303601324558258 \n",
      "P: Output= -1.4686244747012722 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= -1.0348519662131803 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 61 Training Loss: 0.14340275526046753 \n",
      "P: Output= -1.0230158538547602 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 0.10086431616362201 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 62 Training Loss: 0.14820852875709534 \n",
      "P: Output= -1.0356101923973249 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= -0.4819182520615506 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 63 Training Loss: 0.15523576736450195 \n",
      "P: Output= -1.738960901776382 --- Target= 1.04987089244446e-07\n",
      "Q: Output= -1.6101464810456094 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 64 Training Loss: 0.1380658745765686 \n",
      "P: Output= -1.6378995475283418 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= -1.7353133714888878 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 65 Training Loss: 0.1479964256286621 \n",
      "P: Output= -1.7099291224226594 --- Target= -3.15492234115311e-07\n",
      "Q: Output= -1.8250947366911108 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 66 Training Loss: 0.1443687528371811 \n",
      "P: Output= -1.4385281171048936 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= -0.24833146214874802 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 67 Training Loss: 0.14823538064956665 \n",
      "P: Output= -1.438689446120346 --- Target= 5.936255753624664e-08\n",
      "Q: Output= -0.423239495313644 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 68 Training Loss: 0.1541396975517273 \n",
      "P: Output= -1.9654708125461315 --- Target= -2.741996745214692e-07\n",
      "Q: Output= -1.3440174100139481 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 69 Training Loss: 0.14162832498550415 \n",
      "P: Output= -2.093591134669478 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= -1.014246617650035 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 70 Training Loss: 0.14942634105682373 \n",
      "P: Output= -1.9342556092987362 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= -0.8890258354364615 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 71 Training Loss: 0.14620184898376465 \n",
      "P: Output= -1.7846417044280853 --- Target= 1.93652547331169e-07\n",
      "Q: Output= -1.1143474921088874 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 72 Training Loss: 0.14610609412193298 \n",
      "P: Output= -1.6853987236324484 --- Target= -9.703328984755899e-08\n",
      "Q: Output= -1.0170449341326 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 73 Training Loss: 0.14160554111003876 \n",
      "P: Output= -1.1763264926701478 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 0.46293850301175965 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 74 Training Loss: 0.1643078327178955 \n",
      "P: Output= -1.4316106144786094 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= -0.8047991486999226 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 75 Training Loss: 0.14760057628154755 \n",
      "P: Output= -0.8745277941531509 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 1.2688459852050986 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 76 Training Loss: 0.16065503656864166 \n",
      "P: Output= -1.1933558976919976 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= 0.2132928879491809 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 77 Training Loss: 0.14059673249721527 \n",
      "P: Output= -0.6604212855537179 --- Target= 1.72196169323513e-07\n",
      "Q: Output= 2.372481726992759 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 78 Training Loss: 0.149441659450531 \n",
      "P: Output= -0.5774340446378989 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 3.233088529078241 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 79 Training Loss: 0.1462879180908203 \n",
      "P: Output= -1.0747719984157795 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= 2.014403758952259 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 80 Training Loss: 0.1431257426738739 \n",
      "P: Output= -0.876620708522938 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 3.7599082389035994 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 81 Training Loss: 0.1450839787721634 \n",
      "P: Output= -0.7522641404775783 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 3.8850297688490114 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 82 Training Loss: 0.14312779903411865 \n",
      "P: Output= -1.2877457496728226 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= 1.8618856342358 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 83 Training Loss: 0.1517333686351776 \n",
      "P: Output= -0.8892586177967159 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= 3.5352296029013175 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 84 Training Loss: 0.1423843950033188 \n",
      "P: Output= -1.086810624277386 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 3.4481586037961574 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 85 Training Loss: 0.1417117565870285 \n",
      "P: Output= -1.5082444637714785 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= 2.132475994367022 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 86 Training Loss: 0.13608887791633606 \n",
      "P: Output= -0.9923955640693745 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 3.6109802309381642 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 87 Training Loss: 0.14096564054489136 \n",
      "P: Output= -0.8894733919184254 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= 4.049841486357861 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 88 Training Loss: 0.1591656506061554 \n",
      "P: Output= -0.6827871015916358 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 4.311845316950432 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 89 Training Loss: 0.14346802234649658 \n",
      "P: Output= -0.7127715126011722 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 4.842450988526776 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 90 Training Loss: 0.15192413330078125 \n",
      "P: Output= -0.632457480443219 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 5.07844906396333 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 91 Training Loss: 0.15531203150749207 \n",
      "P: Output= -0.6810448563614964 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 4.479750447756513 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 92 Training Loss: 0.15466615557670593 \n",
      "P: Output= -0.8655893951229849 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 4.021067285291622 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 93 Training Loss: 0.1501808762550354 \n",
      "P: Output= -1.0355119768009597 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 3.2755917797154783 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 94 Training Loss: 0.15554989874362946 \n",
      "P: Output= -1.2973820840602643 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 3.2335736472826815 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 95 Training Loss: 0.16226142644882202 \n",
      "P: Output= -1.3770163364958767 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 3.0076917121120523 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 96 Training Loss: 0.14950260519981384 \n",
      "P: Output= -1.5156474538504936 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 2.6626364585967375 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 97 Training Loss: 0.14335577189922333 \n",
      "P: Output= -1.9560966769051307 --- Target= 8.498954429114747e-08\n",
      "Q: Output= 0.8494979157228757 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 98 Training Loss: 0.1386912763118744 \n",
      "P: Output= -1.7856448176563848 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 2.0576294489255478 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 99 Training Loss: 0.1502642035484314 \n",
      "P: Output= -2.152221670412917 --- Target= -1.695987421612699e-07\n",
      "Q: Output= 0.2556552265482326 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 100 Training Loss: 0.14071586728096008 \n",
      "P: Output= -2.058323627663862 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= 0.21488369072952196 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 101 Training Loss: 0.1602001190185547 \n",
      "P: Output= -1.9098967640299032 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= 0.6393259821945723 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 102 Training Loss: 0.14599694311618805 \n",
      "P: Output= -1.6007721401915678 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 1.4395185109168693 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 103 Training Loss: 0.13784034550189972 \n",
      "P: Output= -1.464034504912405 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= 2.067162511783714 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 104 Training Loss: 0.14072130620479584 \n",
      "P: Output= -1.1801296317171541 --- Target= 9.813912260625557e-08\n",
      "Q: Output= 2.6156928574768745 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 105 Training Loss: 0.1380481719970703 \n",
      "P: Output= -0.723221233143394 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 4.3445736800914485 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 106 Training Loss: 0.1475183367729187 \n",
      "P: Output= -1.0351755434991645 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 3.357732409966693 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 107 Training Loss: 0.1400475800037384 \n",
      "P: Output= -1.2106914556635955 --- Target= 2.191291796904693e-07\n",
      "Q: Output= 3.222340404386757 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 108 Training Loss: 0.1439492106437683 \n",
      "P: Output= -0.9092358010188111 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 4.547057235766428 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 109 Training Loss: 0.14635087549686432 \n",
      "P: Output= -1.4489450473079195 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= 3.1862990866538015 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 110 Training Loss: 0.13527676463127136 \n",
      "P: Output= -1.1573884915506092 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 3.760710248225573 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 111 Training Loss: 0.15119296312332153 \n",
      "P: Output= -1.6202152698393233 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= 2.4073735153592697 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 112 Training Loss: 0.13632652163505554 \n",
      "P: Output= -1.52148256064266 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 3.3767153249081243 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 113 Training Loss: 0.1459629088640213 \n",
      "P: Output= -1.5891195496796087 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= 2.8017923239969935 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 114 Training Loss: 0.15133242309093475 \n",
      "P: Output= -1.634698580290662 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 2.3728659306750197 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 115 Training Loss: 0.14631903171539307 \n",
      "P: Output= -1.721401366481028 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 1.7979127070919194 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 116 Training Loss: 0.14980050921440125 \n",
      "P: Output= -1.4902891335753337 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 1.8328548859717824 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 117 Training Loss: 0.15824218094348907 \n",
      "P: Output= -1.9024237825513577 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= 0.7838950726905765 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 118 Training Loss: 0.1396876871585846 \n",
      "P: Output= -1.6029685927747028 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 2.2616664503314103 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 119 Training Loss: 0.14376819133758545 \n",
      "P: Output= -1.7131548817195803 --- Target= -8.441947496606872e-08\n",
      "Q: Output= 0.9419359319720808 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 120 Training Loss: 0.1436363160610199 \n",
      "P: Output= -1.211789210908102 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= 2.2393386644664828 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 121 Training Loss: 0.1498773992061615 \n",
      "P: Output= -1.5728868525497388 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= 1.007522659702584 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 122 Training Loss: 0.13439060747623444 \n",
      "P: Output= -1.221223758599975 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 1.7038389328116237 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 123 Training Loss: 0.13483040034770966 \n",
      "P: Output= -1.541858417677199 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 0.12087248630766378 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 124 Training Loss: 0.14135847985744476 \n",
      "P: Output= -1.3723380613808045 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 1.0085518540254546 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 125 Training Loss: 0.1580137014389038 \n",
      "P: Output= -1.9089004963095721 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= -0.8576853112532019 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 126 Training Loss: 0.13427019119262695 \n",
      "P: Output= -1.3791527235542675 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 0.14769935448238058 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 127 Training Loss: 0.1487937569618225 \n",
      "P: Output= -1.4024278814730673 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 0.10471971058879959 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 128 Training Loss: 0.15037107467651367 \n",
      "P: Output= -1.528428509810161 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 0.18263660159349104 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 129 Training Loss: 0.1510581076145172 \n",
      "P: Output= -1.5221659440389335 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 0.04162370755380884 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 130 Training Loss: 0.14606532454490662 \n",
      "P: Output= -1.9457887389541302 --- Target= -8.159233733096016e-08\n",
      "Q: Output= -1.4930576704025178 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 131 Training Loss: 0.14320287108421326 \n",
      "P: Output= -1.9862317679402146 --- Target= 4.168930338721566e-07\n",
      "Q: Output= -1.15719962211619 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 132 Training Loss: 0.14507576823234558 \n",
      "P: Output= -1.9286492552300727 --- Target= -6.546786224248535e-09\n",
      "Q: Output= -0.603058203714312 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 133 Training Loss: 0.1383533924818039 \n",
      "P: Output= -1.4732577254573709 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 1.233782246870076 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 134 Training Loss: 0.15817144513130188 \n",
      "P: Output= -1.3782976268114284 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 1.6360787731252922 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 135 Training Loss: 0.1559346467256546 \n",
      "P: Output= -1.455030026476929 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 2.0786542886146906 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 136 Training Loss: 0.15070784091949463 \n",
      "P: Output= -1.3002105596926556 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 2.235825329527569 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 137 Training Loss: 0.15734407305717468 \n",
      "P: Output= -1.5960752893260244 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= 1.0975635879453787 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 138 Training Loss: 0.14260758459568024 \n",
      "P: Output= -1.1417746500533639 --- Target= 6.895684645513711e-08\n",
      "Q: Output= 2.4954866855692615 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 139 Training Loss: 0.14797282218933105 \n",
      "P: Output= -1.1771888386674707 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 2.705722418776391 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 140 Training Loss: 0.15222157537937164 \n",
      "P: Output= -1.439100340744032 --- Target= 3.465683571235445e-07\n",
      "Q: Output= 1.2506765977657652 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 141 Training Loss: 0.1433737426996231 \n",
      "P: Output= -0.9826640198229066 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 2.5140595300727586 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 142 Training Loss: 0.1516335904598236 \n",
      "P: Output= -1.0105073098013477 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 2.402115578575544 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 143 Training Loss: 0.14553365111351013 \n",
      "P: Output= -1.0989933996747645 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 2.486642893025804 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 144 Training Loss: 0.1516808569431305 \n",
      "P: Output= -1.0549658618279052 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 2.5848412124669213 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 145 Training Loss: 0.16189485788345337 \n",
      "P: Output= -1.0480642941334004 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= 2.6901300586326276 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 146 Training Loss: 0.16269773244857788 \n",
      "P: Output= -1.5310674214910893 --- Target= 9.79909344778207e-08\n",
      "Q: Output= 1.7904609053749692 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 147 Training Loss: 0.14361414313316345 \n",
      "P: Output= -1.6428426299706445 --- Target= 9.257290134456753e-08\n",
      "Q: Output= 2.1361114283711107 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 148 Training Loss: 0.13955558836460114 \n",
      "P: Output= -1.6923424419892363 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= 2.018590018212585 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 149 Training Loss: 0.14018717408180237 \n",
      "P: Output= -1.3521024862679303 --- Target= 6.896767335007326e-08\n",
      "Q: Output= 3.238533503792228 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 150 Training Loss: 0.15216723084449768 \n",
      "P: Output= -1.7239631916017855 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= 1.8924013922237979 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 151 Training Loss: 0.13717463612556458 \n",
      "P: Output= -1.750639529088235 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= 2.0401583151360017 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 152 Training Loss: 0.1390048861503601 \n",
      "P: Output= -1.3063169656862632 --- Target= 9.096384445683725e-08\n",
      "Q: Output= 3.1128859540369698 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 153 Training Loss: 0.14745008945465088 \n",
      "P: Output= -1.608537167365296 --- Target= -9.226324593214486e-08\n",
      "Q: Output= 1.4408666105882384 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 154 Training Loss: 0.14046242833137512 \n",
      "P: Output= -1.2095204913050557 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 2.666643409360251 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 155 Training Loss: 0.14272964000701904 \n",
      "P: Output= -1.2161308592402493 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 2.2978636167690834 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 156 Training Loss: 0.1411675214767456 \n",
      "P: Output= -1.585996291767672 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 0.26355210808223806 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 157 Training Loss: 0.1352752447128296 \n",
      "P: Output= -1.0993850179507314 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 1.429106560369016 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 158 Training Loss: 0.15604162216186523 \n",
      "P: Output= -1.565413005183653 --- Target= -4.072268247057309e-07\n",
      "Q: Output= -0.16648291412794247 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 159 Training Loss: 0.14915773272514343 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -1.1903988086529846 --- Target= 7.48511617132408e-08\n",
      "Q: Output= 1.545944768283194 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 0 Validation Loss: 0.1543995589017868 \n",
      "P: Output= -1.4088787457268808 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= 0.3449157591451524 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 1 Validation Loss: 0.13535583019256592 \n",
      "P: Output= -1.1173382895046968 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= 1.7057624684051227 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 2 Validation Loss: 0.13860923051834106 \n",
      "P: Output= -1.25773373850451 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= 1.765162916066937 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 3 Validation Loss: 0.15445436537265778 \n",
      "P: Output= -1.5346887445795527 --- Target= 3.239203305582805e-08\n",
      "Q: Output= 0.2634047881331796 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 4 Validation Loss: 0.1329592764377594 \n",
      "P: Output= -1.5427951033376077 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= 0.2660879112063288 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 5 Validation Loss: 0.1344844102859497 \n",
      "P: Output= -1.017459812530805 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= 1.6940689722736968 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 6 Validation Loss: 0.15193293988704681 \n",
      "P: Output= -1.0711673375901967 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= 1.6664013663376176 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 7 Validation Loss: 0.14770960807800293 \n",
      "P: Output= -1.5127449146531609 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= 0.18897823606866915 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 8 Validation Loss: 0.1510428637266159 \n",
      "P: Output= -1.1195515983124542 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= 1.6652599239591872 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 9 Validation Loss: 0.1569199413061142 \n",
      "P: Output= -1.119109030154613 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= 1.6812278051827922 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 10 Validation Loss: 0.13911062479019165 \n",
      "P: Output= -1.4898633345629575 --- Target= -2.219336545650208e-07\n",
      "Q: Output= 0.16724598848449368 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 11 Validation Loss: 0.1375838667154312 \n",
      "P: Output= -1.5625349361721996 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= 0.5568371555892675 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 12 Validation Loss: 0.13520604372024536 \n",
      "P: Output= -1.3563224090377446 --- Target= 4.268641484728164e-07\n",
      "Q: Output= 0.35115834207688845 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 13 Validation Loss: 0.14446662366390228 \n",
      "P: Output= -1.0515318044535684 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= 1.6911311676565415 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 14 Validation Loss: 0.14315226674079895 \n",
      "P: Output= -1.0630543864804443 --- Target= -1.907385298594022e-07\n",
      "Q: Output= 1.6690823379380717 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 15 Validation Loss: 0.1490941047668457 \n",
      "P: Output= -1.5414499396783041 --- Target= 2.239196774667107e-07\n",
      "Q: Output= 0.20910505521897438 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 16 Validation Loss: 0.14271001517772675 \n",
      "P: Output= -1.0822649405054676 --- Target= 2.077415652834702e-07\n",
      "Q: Output= 1.8333172284233052 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 17 Validation Loss: 0.15047240257263184 \n",
      "P: Output= -1.135958770894005 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= 1.684440947259258 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 18 Validation Loss: 0.14627373218536377 \n",
      "P: Output= -1.562104673076405 --- Target= 2.911645777814442e-07\n",
      "Q: Output= 0.3138952031135922 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 19 Validation Loss: 0.13951587677001953 \n",
      "P: Output= -1.039086325640608 --- Target= 5.541669878539324e-08\n",
      "Q: Output= 1.6386670701832706 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 20 Validation Loss: 0.14973920583724976 \n",
      "P: Output= -1.1955593124783537 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= 1.666129317362457 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 21 Validation Loss: 0.16018801927566528 \n",
      "P: Output= -1.1187729416124164 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= 1.4568292394831266 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 22 Validation Loss: 0.13805529475212097 \n",
      "P: Output= -1.493934697550757 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= 0.3419351635656298 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 23 Validation Loss: 0.13998784124851227 \n",
      "P: Output= -1.245426118704473 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= 1.6195033506012706 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 24 Validation Loss: 0.15610742568969727 \n",
      "P: Output= -1.1300196429282163 --- Target= -8.584417265922184e-08\n",
      "Q: Output= 1.6157851594065047 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 25 Validation Loss: 0.14669063687324524 \n",
      "P: Output= -1.1041800052762527 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= 1.4676132469985879 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 26 Validation Loss: 0.1485426425933838 \n",
      "P: Output= -1.4181656982184236 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= 0.06768753633642 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 27 Validation Loss: 0.14162123203277588 \n",
      "P: Output= -1.0365844118355758 --- Target= 1.3670019605172e-07\n",
      "Q: Output= 1.6382814572158413 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 28 Validation Loss: 0.15376974642276764 \n",
      "P: Output= -1.476074069467777 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= 0.3672679778470682 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 29 Validation Loss: 0.14153322577476501 \n",
      "Epoch: 5\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -1.5378863512053513 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 0.4458511436947692 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 0 Training Loss: 0.1411644071340561 \n",
      "P: Output= -1.4962647077599671 --- Target= 9.40822530992591e-10\n",
      "Q: Output= 0.5821382073507273 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 1 Training Loss: 0.13938595354557037 \n",
      "P: Output= -1.3573428400211727 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= 1.061367683115253 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 2 Training Loss: 0.13853126764297485 \n",
      "P: Output= -1.3342809013357462 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= 0.8669404099666922 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 3 Training Loss: 0.14506667852401733 \n",
      "P: Output= -1.2445264614356653 --- Target= -9.128675593217395e-09\n",
      "Q: Output= 1.215833877135565 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 4 Training Loss: 0.13702969253063202 \n",
      "P: Output= -1.1994841515468515 --- Target= -8.441947496606872e-08\n",
      "Q: Output= 1.585110328850539 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 5 Training Loss: 0.1429351568222046 \n",
      "P: Output= -1.079989662591852 --- Target= 1.606130251019522e-08\n",
      "Q: Output= 2.0338153732749156 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 6 Training Loss: 0.1385403871536255 \n",
      "P: Output= -0.8184441761460164 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 3.8584357982674753 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 7 Training Loss: 0.14535117149353027 \n",
      "P: Output= -0.7852578451141747 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 3.888762111895076 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 8 Training Loss: 0.15281756222248077 \n",
      "P: Output= -1.0560102944735217 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 2.6742844482944204 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 9 Training Loss: 0.13772958517074585 \n",
      "P: Output= -1.1282102156036924 --- Target= 9.813912260625557e-08\n",
      "Q: Output= 2.526932043649193 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 10 Training Loss: 0.1423511952161789 \n",
      "P: Output= -0.8969137396804525 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= 3.0514924394789738 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 11 Training Loss: 0.1518426537513733 \n",
      "P: Output= -1.5427848762673069 --- Target= -1.618743441511583e-07\n",
      "Q: Output= 1.1638178171741362 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 12 Training Loss: 0.13481709361076355 \n",
      "P: Output= -1.7767877246515527 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= 0.40906867630568566 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 13 Training Loss: 0.1449715942144394 \n",
      "P: Output= -1.2637937763363993 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 1.669877123327356 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 14 Training Loss: 0.1527125984430313 \n",
      "P: Output= -1.793943543643854 --- Target= -5.012515025271114e-08\n",
      "Q: Output= 0.08767558896214744 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 15 Training Loss: 0.13927699625492096 \n",
      "P: Output= -1.6962867449685666 --- Target= 1.04987089244446e-07\n",
      "Q: Output= 0.21704837523511156 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 16 Training Loss: 0.13463151454925537 \n",
      "P: Output= -1.0538243711444721 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 1.547120513527351 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 17 Training Loss: 0.1469188779592514 \n",
      "P: Output= -1.4145818460766337 --- Target= -1.695987421612699e-07\n",
      "Q: Output= 0.1615168940223528 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 18 Training Loss: 0.13973385095596313 \n",
      "P: Output= -1.3475144313853713 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= -0.3847045968683558 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 19 Training Loss: 0.14043623208999634 \n",
      "P: Output= -1.02517121837319 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 0.8797336865981755 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 20 Training Loss: 0.14614126086235046 \n",
      "P: Output= -0.687008487144503 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= 0.8906487084221224 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 21 Training Loss: 0.14319482445716858 \n",
      "P: Output= -0.6915673773606335 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 1.0370880098969755 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 22 Training Loss: 0.13966907560825348 \n",
      "P: Output= -1.1591555510111586 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= -0.15368467038615385 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 23 Training Loss: 0.13716819882392883 \n",
      "P: Output= -0.7712721841303454 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 1.8727713228897889 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 24 Training Loss: 0.14141197502613068 \n",
      "P: Output= -0.7187099013889782 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 2.067562210540033 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 25 Training Loss: 0.15155908465385437 \n",
      "P: Output= -0.7281893065693019 --- Target= 1.72196169323513e-07\n",
      "Q: Output= 2.3303716615828343 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 26 Training Loss: 0.14418642222881317 \n",
      "P: Output= -0.6918133382911256 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 2.558992734894785 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 27 Training Loss: 0.13965465128421783 \n",
      "P: Output= -1.1298261036819897 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= 1.5360124760556015 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 28 Training Loss: 0.1328895092010498 \n",
      "P: Output= -0.7414973729605165 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 2.8518168039293155 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 29 Training Loss: 0.15147258341312408 \n",
      "P: Output= -0.6984810879527821 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 3.245657123115719 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 30 Training Loss: 0.1383068561553955 \n",
      "P: Output= -0.7972217051343176 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 3.242070235511334 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 31 Training Loss: 0.14224320650100708 \n",
      "P: Output= -1.1890483887393062 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 1.3574787053649802 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 32 Training Loss: 0.13282610476016998 \n",
      "P: Output= -0.8270660933214478 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 2.7558605077472347 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 33 Training Loss: 0.15417301654815674 \n",
      "P: Output= -0.9957985270814325 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 2.5760842471054515 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 34 Training Loss: 0.147487610578537 \n",
      "P: Output= -1.0541351010507345 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 2.983838833382719 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 35 Training Loss: 0.15221506357192993 \n",
      "P: Output= -0.9841705122398761 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 3.1774051880040446 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 36 Training Loss: 0.15313637256622314 \n",
      "P: Output= -1.0451808166212446 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 3.371815148584428 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 37 Training Loss: 0.14257654547691345 \n",
      "P: Output= -1.019313045698821 --- Target= 6.896767335007326e-08\n",
      "Q: Output= 3.4567763472181507 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 38 Training Loss: 0.15308257937431335 \n",
      "P: Output= -0.9578281009530727 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 3.680020292503041 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 39 Training Loss: 0.14555448293685913 \n",
      "P: Output= -1.273701394420108 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 3.2249183492653533 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 40 Training Loss: 0.14302875101566315 \n",
      "P: Output= -1.5225751268698282 --- Target= -2.741996745214692e-07\n",
      "Q: Output= 1.1695800364458124 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 41 Training Loss: 0.1352619230747223 \n",
      "P: Output= -1.2105310550885457 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 2.2924410298107407 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 42 Training Loss: 0.1510702222585678 \n",
      "P: Output= -1.220054792011899 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 1.7305180665729063 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 43 Training Loss: 0.14121675491333008 \n",
      "P: Output= -1.1046240099396982 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 1.0225013965054446 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 44 Training Loss: 0.1461505889892578 \n",
      "P: Output= -1.243174149941317 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 1.091753514814803 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 45 Training Loss: 0.1421896517276764 \n",
      "P: Output= -1.6936246421154415 --- Target= -3.576887390721595e-07\n",
      "Q: Output= -0.22206488093065602 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 46 Training Loss: 0.13592204451560974 \n",
      "P: Output= -1.0726592766482295 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 1.2138943819791947 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 47 Training Loss: 0.1575961410999298 \n",
      "P: Output= -1.471251668376227 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= 0.3834231152287648 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 48 Training Loss: 0.13245649635791779 \n",
      "P: Output= -1.0424340503302174 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 2.0862507099966967 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 49 Training Loss: 0.15754331648349762 \n",
      "P: Output= -0.9468313875996683 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 2.427148164033995 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 50 Training Loss: 0.1471017301082611 \n",
      "P: Output= -0.9098021592914121 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 2.2402585073937624 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 51 Training Loss: 0.1348794847726822 \n",
      "P: Output= -1.4283219630405348 --- Target= 9.865799821540122e-08\n",
      "Q: Output= 0.16516771570658406 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 52 Training Loss: 0.13231199979782104 \n",
      "P: Output= -1.0305544782449774 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 0.7307165862405798 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 53 Training Loss: 0.14751945436000824 \n",
      "P: Output= -1.2086486911301524 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 0.1434748045131693 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 54 Training Loss: 0.14552289247512817 \n",
      "P: Output= -1.6574099387390584 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= -2.0358057381013728 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 55 Training Loss: 0.13031309843063354 \n",
      "P: Output= -1.322481472767766 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= -1.3404353728557101 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 56 Training Loss: 0.14756757020950317 \n",
      "P: Output= -1.728943263699473 --- Target= -9.226324593214486e-08\n",
      "Q: Output= -3.1404979821150114 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 57 Training Loss: 0.1358075737953186 \n",
      "P: Output= -1.1449141349194498 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= -2.09436554255602 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 58 Training Loss: 0.14943355321884155 \n",
      "P: Output= -1.637621745084127 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= -2.9586956838945424 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 59 Training Loss: 0.13347160816192627 \n",
      "P: Output= -1.3943336229217342 --- Target= 8.498954429114747e-08\n",
      "Q: Output= -2.325554316423054 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 60 Training Loss: 0.13560032844543457 \n",
      "P: Output= -1.334148646905894 --- Target= 6.39805808333449e-09\n",
      "Q: Output= -1.4464202278194325 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 61 Training Loss: 0.13610324263572693 \n",
      "P: Output= -1.3678731577908652 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= -1.101191137558879 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 62 Training Loss: 0.13342468440532684 \n",
      "P: Output= -1.1303744156068127 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= -0.9554484332840376 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 63 Training Loss: 0.13696147501468658 \n",
      "P: Output= -0.8305060629846688 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 1.059674912425236 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 64 Training Loss: 0.14252886176109314 \n",
      "P: Output= -0.78225449138868 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 1.3502762228747365 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 65 Training Loss: 0.1448354423046112 \n",
      "P: Output= -0.7481995360517892 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 1.671884183539139 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 66 Training Loss: 0.13880209624767303 \n",
      "P: Output= -1.263508877360496 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= -0.10264419678940317 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 67 Training Loss: 0.1318804770708084 \n",
      "P: Output= -0.7657731318504153 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 1.9434738378863576 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 68 Training Loss: 0.15500539541244507 \n",
      "P: Output= -1.2108253060952405 --- Target= 1.652745300617653e-07\n",
      "Q: Output= 1.0288654743715506 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 69 Training Loss: 0.13457879424095154 \n",
      "P: Output= -1.2890499869933825 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= 1.710072575066552 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 70 Training Loss: 0.13312160968780518 \n",
      "P: Output= -1.273315811880801 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= 2.3095060022837 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 71 Training Loss: 0.1343807876110077 \n",
      "P: Output= -1.1408520907065203 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 3.166759671595244 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 72 Training Loss: 0.1299603432416916 \n",
      "P: Output= -0.8350655879561728 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 4.5507521660627495 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 73 Training Loss: 0.14176368713378906 \n",
      "P: Output= -0.7264797497663489 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 4.315063152303994 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 74 Training Loss: 0.13675996661186218 \n",
      "P: Output= -1.114832687970405 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 2.588868487853061 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 75 Training Loss: 0.13357457518577576 \n",
      "P: Output= -1.0663077310130031 --- Target= -4.072268247057309e-07\n",
      "Q: Output= 2.4191239197616685 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 76 Training Loss: 0.13826927542686462 \n",
      "P: Output= -0.9115378087607047 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 2.486633826850218 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 77 Training Loss: 0.13610008358955383 \n",
      "P: Output= -1.035608912694558 --- Target= 2.191291796904693e-07\n",
      "Q: Output= 2.4377191874291233 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 78 Training Loss: 0.14025577902793884 \n",
      "P: Output= -0.6247727578535134 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 4.391478310032171 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 79 Training Loss: 0.1360243409872055 \n",
      "P: Output= -0.6367487464282142 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 4.355857354367941 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 80 Training Loss: 0.14212486147880554 \n",
      "P: Output= -0.6882604701469957 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= 4.146155818258081 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 81 Training Loss: 0.1425722986459732 \n",
      "P: Output= -1.0664804400341863 --- Target= 1.885248828159547e-07\n",
      "Q: Output= 2.554784106057764 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 82 Training Loss: 0.1318913996219635 \n",
      "P: Output= -0.9624224635831178 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 4.549420993384511 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 83 Training Loss: 0.1399720013141632 \n",
      "P: Output= -0.9636345437649663 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 4.29532983749691 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 84 Training Loss: 0.14403431117534637 \n",
      "P: Output= -1.3437131965043188 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= 2.7687392966633952 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 85 Training Loss: 0.13541598618030548 \n",
      "P: Output= -1.0788704753378076 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 4.532562619301753 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 86 Training Loss: 0.1343764215707779 \n",
      "P: Output= -1.0561418875592379 --- Target= 6.895684645513711e-08\n",
      "Q: Output= 4.5738164772985925 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 87 Training Loss: 0.14321522414684296 \n",
      "P: Output= -1.1435833733848337 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 4.927059288709074 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 88 Training Loss: 0.13862454891204834 \n",
      "P: Output= -1.4103520822092133 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 3.801891028552271 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 89 Training Loss: 0.13544149696826935 \n",
      "P: Output= -1.6292251130345843 --- Target= 1.239808575803636e-07\n",
      "Q: Output= 3.5474137932511667 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 90 Training Loss: 0.13596320152282715 \n",
      "P: Output= -1.728012944550108 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= 3.6291892839345223 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 91 Training Loss: 0.12995561957359314 \n",
      "P: Output= -1.3393751180828497 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 5.082465778796312 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 92 Training Loss: 0.13378314673900604 \n",
      "P: Output= -1.524857795522883 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 4.80882284828755 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 93 Training Loss: 0.14501453936100006 \n",
      "P: Output= -1.481106617184217 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 4.579620126240678 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 94 Training Loss: 0.13293030858039856 \n",
      "P: Output= -1.7573093071885255 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 2.1893937554621195 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 95 Training Loss: 0.14026275277137756 \n",
      "P: Output= -1.8877035562466382 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= 1.6440608543270807 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 96 Training Loss: 0.13530243933200836 \n",
      "P: Output= -1.9328295466735845 --- Target= 9.79909344778207e-08\n",
      "Q: Output= 1.0203075447004508 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 97 Training Loss: 0.13355910778045654 \n",
      "P: Output= -1.5512246090651116 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 2.143812027451008 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 98 Training Loss: 0.12828420102596283 \n",
      "P: Output= -1.616946730837447 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 2.0893626514287913 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 99 Training Loss: 0.15293391048908234 \n",
      "P: Output= -1.5500994896218634 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 1.748971605458836 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 100 Training Loss: 0.13450977206230164 \n",
      "P: Output= -1.4170369756169023 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 1.4276756414979763 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 101 Training Loss: 0.1393962800502777 \n",
      "P: Output= -1.6247474773561361 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= 1.1131500793211098 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 102 Training Loss: 0.14386871457099915 \n",
      "P: Output= -2.0497094847228885 --- Target= -8.159233733096016e-08\n",
      "Q: Output= -1.182506891834839 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 103 Training Loss: 0.13152767717838287 \n",
      "P: Output= -1.7952762632508525 --- Target= 9.096384445683725e-08\n",
      "Q: Output= -0.5819916947372281 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 104 Training Loss: 0.1356908082962036 \n",
      "P: Output= -1.8347588181638397 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= -1.4862172126731017 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 105 Training Loss: 0.15700924396514893 \n",
      "P: Output= -1.8452347423749842 --- Target= -1.490649044200154e-07\n",
      "Q: Output= -2.3831900164184048 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 106 Training Loss: 0.15711547434329987 \n",
      "P: Output= -1.923793306127962 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= -2.720069905127958 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 107 Training Loss: 0.14011940360069275 \n",
      "P: Output= -2.6791139156646846 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= -4.281550254794459 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 108 Training Loss: 0.13990476727485657 \n",
      "P: Output= -2.5294547023963725 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= -4.084223316994322 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 109 Training Loss: 0.13558359444141388 \n",
      "P: Output= -2.4973223102616835 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= -4.141964097243513 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 110 Training Loss: 0.13421522080898285 \n",
      "P: Output= -2.0023334289067307 --- Target= -7.718915107091107e-08\n",
      "Q: Output= -2.326251923264041 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 111 Training Loss: 0.13569936156272888 \n",
      "P: Output= -1.982624614726297 --- Target= 2.601515447508973e-08\n",
      "Q: Output= -2.205377392787405 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 112 Training Loss: 0.1503177285194397 \n",
      "P: Output= -2.3471572093506534 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= -3.1542961924230384 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 113 Training Loss: 0.13091091811656952 \n",
      "P: Output= -2.183008508252559 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= -2.7710447307588724 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 114 Training Loss: 0.13773588836193085 \n",
      "P: Output= -2.1522415151916325 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= -2.176397441964342 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 115 Training Loss: 0.13860037922859192 \n",
      "P: Output= -1.9048762421811887 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= -0.43313700950661005 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 116 Training Loss: 0.14630551636219025 \n",
      "P: Output= -1.2915899152904684 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 2.056589585880751 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 117 Training Loss: 0.13791204988956451 \n",
      "P: Output= -1.16020369583656 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 3.1840165626935306 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 118 Training Loss: 0.1400832235813141 \n",
      "P: Output= -1.5217114056685288 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= 2.8927825596964603 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 119 Training Loss: 0.13811707496643066 \n",
      "P: Output= -1.5843724977616391 --- Target= 3.465683571235445e-07\n",
      "Q: Output= 3.113054497672808 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 120 Training Loss: 0.1337398737668991 \n",
      "P: Output= -1.5926851985516661 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 4.453963929334266 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 121 Training Loss: 0.1398601084947586 \n",
      "P: Output= -1.6551263577643178 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 4.231025443127784 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 122 Training Loss: 0.14581865072250366 \n",
      "P: Output= -2.148896170489226 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= 2.7798479688892552 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 123 Training Loss: 0.12932439148426056 \n",
      "P: Output= -2.0640064963869778 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 2.943503122314376 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 124 Training Loss: 0.13075998425483704 \n",
      "P: Output= -2.1466511074675507 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= 3.040032044223885 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 125 Training Loss: 0.13032886385917664 \n",
      "P: Output= -2.0027838517769103 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= 3.7239388910057496 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 126 Training Loss: 0.14161673188209534 \n",
      "P: Output= -1.5541874347729872 --- Target= 2.021405629548667e-07\n",
      "Q: Output= 5.4585953625521535 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 127 Training Loss: 0.13739517331123352 \n",
      "P: Output= -1.7508562398491598 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= 4.6927073163192405 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 128 Training Loss: 0.13414937257766724 \n",
      "P: Output= -1.4430208425283606 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 5.816675631940765 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 129 Training Loss: 0.1332770735025406 \n",
      "P: Output= -1.7560229965322938 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 4.1744631644299055 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 130 Training Loss: 0.13380217552185059 \n",
      "P: Output= -1.8166947209724666 --- Target= 1.93652547331169e-07\n",
      "Q: Output= 3.7738321417547276 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 131 Training Loss: 0.134667307138443 \n",
      "P: Output= -2.0432139929749003 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= 3.578917060305278 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 132 Training Loss: 0.13218477368354797 \n",
      "P: Output= -1.9647481964553295 --- Target= -9.703328984755899e-08\n",
      "Q: Output= 3.0087019090849356 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 133 Training Loss: 0.13185171782970428 \n",
      "P: Output= -1.880313476571077 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 3.5222315390990957 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 134 Training Loss: 0.14870281517505646 \n",
      "P: Output= -2.0203544064799033 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 2.6760812835125076 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 135 Training Loss: 0.13036659359931946 \n",
      "P: Output= -2.022483937999626 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= 1.8828202208531302 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 136 Training Loss: 0.1423671990633011 \n",
      "P: Output= -2.5995398435892074 --- Target= -3.730653368450021e-07\n",
      "Q: Output= 0.005998566478943701 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 137 Training Loss: 0.13103514909744263 \n",
      "P: Output= -2.64231084869707 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= -0.5466409761930207 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 138 Training Loss: 0.1343822479248047 \n",
      "P: Output= -2.5263770947527 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= -0.9852808151009329 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 139 Training Loss: 0.13986173272132874 \n",
      "P: Output= -2.5069869181268727 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= -1.0644233315665144 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 140 Training Loss: 0.138799250125885 \n",
      "P: Output= -2.426119339639312 --- Target= 9.257290134456753e-08\n",
      "Q: Output= -1.2589291491992274 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 141 Training Loss: 0.12693125009536743 \n",
      "P: Output= -1.911339832173348 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 0.3826442976396809 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 142 Training Loss: 0.13850656151771545 \n",
      "P: Output= -1.698163669116532 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 0.5326971904322866 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 143 Training Loss: 0.13673223555088043 \n",
      "P: Output= -1.9850047184064774 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= -0.15409564415967836 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 144 Training Loss: 0.1371365487575531 \n",
      "P: Output= -1.9431398342358444 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= 0.15910912057942195 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 145 Training Loss: 0.14227747917175293 \n",
      "P: Output= -1.5314052524233848 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= 1.4115813621494286 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 146 Training Loss: 0.14088627696037292 \n",
      "P: Output= -1.4935068596126166 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 1.2750285453390982 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 147 Training Loss: 0.14451786875724792 \n",
      "P: Output= -1.4803706398247334 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 1.1969377773074728 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 148 Training Loss: 0.14665274322032928 \n",
      "P: Output= -1.9734319388027926 --- Target= -6.546786224248535e-09\n",
      "Q: Output= 0.09327351439772169 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 149 Training Loss: 0.1289513111114502 \n",
      "P: Output= -1.4790188338141386 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 1.4668719947499227 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 150 Training Loss: 0.139102503657341 \n",
      "P: Output= -1.4899908478154336 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 1.312690401221344 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 151 Training Loss: 0.13474901020526886 \n",
      "P: Output= -1.4173559735679593 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 1.4799713421416358 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 152 Training Loss: 0.14349359273910522 \n",
      "P: Output= -1.400727734847547 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 1.781245801166711 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 153 Training Loss: 0.14376705884933472 \n",
      "P: Output= -1.2671224117645474 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 2.1147878772006 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 154 Training Loss: 0.13297057151794434 \n",
      "P: Output= -1.1793852686849595 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 2.570063890917904 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 155 Training Loss: 0.13423416018486023 \n",
      "P: Output= -1.07783691505513 --- Target= 5.936255753624664e-08\n",
      "Q: Output= 2.830047436067673 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 156 Training Loss: 0.13629919290542603 \n",
      "P: Output= -1.490055944852327 --- Target= 4.168930338721566e-07\n",
      "Q: Output= 2.117424582450412 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 157 Training Loss: 0.12791725993156433 \n",
      "P: Output= -1.4238986757203396 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 2.7207539100220846 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 158 Training Loss: 0.13217072188854218 \n",
      "P: Output= -1.0233338636994684 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 4.135218673447405 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 159 Training Loss: 0.16306470334529877 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -1.3040469448859735 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= 2.9983614813537933 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 0 Validation Loss: 0.1370205581188202 \n",
      "P: Output= -1.2805494434264526 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= 4.238718311251976 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 1 Validation Loss: 0.14518630504608154 \n",
      "P: Output= -1.1633477814550544 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= 4.36401999969617 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 2 Validation Loss: 0.13544106483459473 \n",
      "P: Output= -1.435038544817659 --- Target= 3.239203305582805e-08\n",
      "Q: Output= 3.1365408328788718 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 3 Validation Loss: 0.12656056880950928 \n",
      "P: Output= -1.0768252573486636 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= 4.387922854345825 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 4 Validation Loss: 0.16755595803260803 \n",
      "P: Output= -1.122482619530155 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= 4.177282236172353 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 5 Validation Loss: 0.1443326473236084 \n",
      "P: Output= -1.1705420875270178 --- Target= -8.584417265922184e-08\n",
      "Q: Output= 4.293059006706027 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 6 Validation Loss: 0.1672048568725586 \n",
      "P: Output= -1.2293514486984058 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= 4.319083365870746 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 7 Validation Loss: 0.15827509760856628 \n",
      "P: Output= -1.0504767908127475 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= 4.403728468724399 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 8 Validation Loss: 0.14396026730537415 \n",
      "P: Output= -1.314557034084773 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= 3.235969987960398 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 9 Validation Loss: 0.12709467113018036 \n",
      "P: Output= -1.0653124819812776 --- Target= 5.541669878539324e-08\n",
      "Q: Output= 4.345025539565805 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 10 Validation Loss: 0.13644233345985413 \n",
      "P: Output= -1.1561567779688273 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= 4.350021547481773 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 11 Validation Loss: 0.15629711747169495 \n",
      "P: Output= -1.3976097810602166 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= 3.236288739821795 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 12 Validation Loss: 0.12971901893615723 \n",
      "P: Output= -1.5054448341886193 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= 3.3610990748998013 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 13 Validation Loss: 0.12273386120796204 \n",
      "P: Output= -1.3950736075515175 --- Target= -2.219336545650208e-07\n",
      "Q: Output= 3.086705278042394 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 14 Validation Loss: 0.12765510380268097 \n",
      "P: Output= -1.1668418157806784 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= 4.323821426009019 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 15 Validation Loss: 0.17213627696037292 \n",
      "P: Output= -1.0692891793070665 --- Target= 1.3670019605172e-07\n",
      "Q: Output= 4.359763723608943 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 16 Validation Loss: 0.13179762661457062 \n",
      "P: Output= -1.2710510551693712 --- Target= 4.268641484728164e-07\n",
      "Q: Output= 3.2438416075322634 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 17 Validation Loss: 0.13468104600906372 \n",
      "P: Output= -1.1677745343206354 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= 4.387623117088893 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 18 Validation Loss: 0.15243908762931824 \n",
      "P: Output= -1.1075006481193084 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= 4.377757796486079 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 19 Validation Loss: 0.14870381355285645 \n",
      "P: Output= -1.4691081246817532 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= 3.1654214536042273 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 20 Validation Loss: 0.12546443939208984 \n",
      "P: Output= -1.4167933348202135 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= 3.230965764365111 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 21 Validation Loss: 0.1260647177696228 \n",
      "P: Output= -1.4036255062958451 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= 3.051110654957281 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 22 Validation Loss: 0.1427134871482849 \n",
      "P: Output= -1.213420862655048 --- Target= 7.48511617132408e-08\n",
      "Q: Output= 4.213398406125062 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 23 Validation Loss: 0.148408442735672 \n",
      "P: Output= -1.3118242086514549 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= 4.368469731253727 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 24 Validation Loss: 0.15378889441490173 \n",
      "P: Output= -1.1295444008061537 --- Target= 2.077415652834702e-07\n",
      "Q: Output= 4.480080042369515 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 25 Validation Loss: 0.14856186509132385 \n",
      "P: Output= -1.4909788732747913 --- Target= 2.911645777814442e-07\n",
      "Q: Output= 3.1583454993958116 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 26 Validation Loss: 0.13208353519439697 \n",
      "P: Output= -1.096884465862706 --- Target= -1.907385298594022e-07\n",
      "Q: Output= 4.361097583272293 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 27 Validation Loss: 0.16924643516540527 \n",
      "P: Output= -1.1291384627881076 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= 4.181297632536314 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 28 Validation Loss: 0.1579894870519638 \n",
      "P: Output= -1.4564951619535034 --- Target= 2.239196774667107e-07\n",
      "Q: Output= 3.088005607668739 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 29 Validation Loss: 0.13349156081676483 \n",
      "Epoch: 6\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -1.1248425731200689 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 4.323904680745269 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 0 Training Loss: 0.1386757791042328 \n",
      "P: Output= -1.5613094851383273 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= 2.660527501542113 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 1 Training Loss: 0.12693297863006592 \n",
      "P: Output= -1.4406482649116539 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 2.767285392239506 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 2 Training Loss: 0.16868847608566284 \n",
      "P: Output= -1.4893820641740048 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 2.401670943847127 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 3 Training Loss: 0.13995149731636047 \n",
      "P: Output= -1.7087467018582547 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 1.638079659457624 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 4 Training Loss: 0.13391107320785522 \n",
      "P: Output= -2.246042102664589 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= -0.5253688799224294 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 5 Training Loss: 0.13018766045570374 \n",
      "P: Output= -1.9084097852775992 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 0.5250520587518954 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 6 Training Loss: 0.14309847354888916 \n",
      "P: Output= -2.1661571823840164 --- Target= 1.93652547331169e-07\n",
      "Q: Output= -1.0758543847661475 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 7 Training Loss: 0.13496039807796478 \n",
      "P: Output= -2.240654280404784 --- Target= 9.257290134456753e-08\n",
      "Q: Output= -1.210018662573626 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 8 Training Loss: 0.1294446885585785 \n",
      "P: Output= -1.6564351244310593 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= -0.02498118113307335 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 9 Training Loss: 0.14303748309612274 \n",
      "P: Output= -1.7636243727879224 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 0.5753109983965814 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 10 Training Loss: 0.1378868669271469 \n",
      "P: Output= -2.016337057146931 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= -0.40616701262918564 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 11 Training Loss: 0.13052645325660706 \n",
      "P: Output= -1.9892633732160396 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= -0.006483382304770302 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 12 Training Loss: 0.1304934173822403 \n",
      "P: Output= -1.585521877800562 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 1.358957276820532 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 13 Training Loss: 0.13808557391166687 \n",
      "P: Output= -1.8092721149376372 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 0.29343679066462514 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 14 Training Loss: 0.12807613611221313 \n",
      "P: Output= -1.3576975690557562 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 1.4629686520709857 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 15 Training Loss: 0.14323750138282776 \n",
      "P: Output= -1.7426964179323008 --- Target= -8.441947496606872e-08\n",
      "Q: Output= -0.16737983931860523 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 16 Training Loss: 0.13526031374931335 \n",
      "P: Output= -1.3540705532034698 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 1.8418511395577433 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 17 Training Loss: 0.13536420464515686 \n",
      "P: Output= -1.7350403385780053 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= 0.569532399270587 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 18 Training Loss: 0.12793925404548645 \n",
      "P: Output= -1.2377053199661292 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 1.9300162723092367 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 19 Training Loss: 0.13560961186885834 \n",
      "P: Output= -1.5618743420719214 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 0.7204911740263418 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 20 Training Loss: 0.13359913229942322 \n",
      "P: Output= -1.7166430341074363 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= 0.4591120703786338 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 21 Training Loss: 0.13429376482963562 \n",
      "P: Output= -1.5481776209364906 --- Target= 6.39805808333449e-09\n",
      "Q: Output= 1.0588612521077154 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 22 Training Loss: 0.12681645154953003 \n",
      "P: Output= -1.3989376227274875 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= 1.4073952893409691 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 23 Training Loss: 0.13130910694599152 \n",
      "P: Output= -0.9993947743662703 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 3.5739846211586084 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 24 Training Loss: 0.14317035675048828 \n",
      "P: Output= -1.3386993909328808 --- Target= -5.012515025271114e-08\n",
      "Q: Output= 2.668579156322549 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 25 Training Loss: 0.1315128356218338 \n",
      "P: Output= -0.8108346152401227 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 3.8965065896910556 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 26 Training Loss: 0.1348893791437149 \n",
      "P: Output= -0.833015032120958 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 3.939705772492843 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 27 Training Loss: 0.1423310935497284 \n",
      "P: Output= -0.8746158435275317 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 3.548287886665369 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 28 Training Loss: 0.1339559704065323 \n",
      "P: Output= -1.335190663321928 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= 1.2779367366107097 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 29 Training Loss: 0.13117967545986176 \n",
      "P: Output= -1.1613546582562302 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 2.1262762298452964 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 30 Training Loss: 0.14207345247268677 \n",
      "P: Output= -1.2383538983985298 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 1.574936975735298 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 31 Training Loss: 0.14389309287071228 \n",
      "P: Output= -1.4745693499768393 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 1.2263886417139869 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 32 Training Loss: 0.14638805389404297 \n",
      "P: Output= -1.8337227289677829 --- Target= -2.741996745214692e-07\n",
      "Q: Output= 0.05682008995810239 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 33 Training Loss: 0.12974724173545837 \n",
      "P: Output= -1.3414897889413107 --- Target= 5.936255753624664e-08\n",
      "Q: Output= 2.0060855664697943 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 34 Training Loss: 0.13494877517223358 \n",
      "P: Output= -1.715380153727092 --- Target= 4.168930338721566e-07\n",
      "Q: Output= 1.5677549108662951 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 35 Training Loss: 0.1321622133255005 \n",
      "P: Output= -1.3585728853991021 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 3.4316276129813224 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 36 Training Loss: 0.15487554669380188 \n",
      "P: Output= -1.5181265031591957 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= 2.2653984996731324 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 37 Training Loss: 0.12466084212064743 \n",
      "P: Output= -1.407244771648264 --- Target= -3.730653368450021e-07\n",
      "Q: Output= 2.8405980846786214 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 38 Training Loss: 0.12575870752334595 \n",
      "P: Output= -0.8846983788272151 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 3.827630640288927 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 39 Training Loss: 0.16636432707309723 \n",
      "P: Output= -1.3211578638950172 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= 2.661280526790218 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 40 Training Loss: 0.12451782822608948 \n",
      "P: Output= -1.2450867748543528 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= 2.5209744222662724 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 41 Training Loss: 0.1262013167142868 \n",
      "P: Output= -1.0917366685939065 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 2.9410881373038738 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 42 Training Loss: 0.15476922690868378 \n",
      "P: Output= -1.594168402107476 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 1.1119024699409312 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 43 Training Loss: 0.13440349698066711 \n",
      "P: Output= -1.7492832314133429 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 0.23105068946499419 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 44 Training Loss: 0.1270894706249237 \n",
      "P: Output= -1.401455079396431 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 1.3424777756584065 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 45 Training Loss: 0.14098478853702545 \n",
      "P: Output= -1.4226233911441808 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 0.7757255245836543 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 46 Training Loss: 0.13435061275959015 \n",
      "P: Output= -1.5303749599821215 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 0.9679939062182195 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 47 Training Loss: 0.1390872597694397 \n",
      "P: Output= -1.7730114680622995 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= 0.12408481448228148 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 48 Training Loss: 0.12421968579292297 \n",
      "P: Output= -1.6701304010932194 --- Target= 1.885248828159547e-07\n",
      "Q: Output= 0.6959478757511439 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 49 Training Loss: 0.12521107494831085 \n",
      "P: Output= -1.2882234024720365 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 2.7071446547739377 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 50 Training Loss: 0.1328943967819214 \n",
      "P: Output= -1.5934923479749932 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= 1.556521636006055 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 51 Training Loss: 0.12863895297050476 \n",
      "P: Output= -1.4211491357276032 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= 2.217678583244222 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 52 Training Loss: 0.1342846304178238 \n",
      "P: Output= -1.042053017970633 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 3.8705192737615484 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 53 Training Loss: 0.13930392265319824 \n",
      "P: Output= -1.130997455113257 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 3.6659110767753162 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 54 Training Loss: 0.13304723799228668 \n",
      "P: Output= -1.0990082946411395 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 3.419147109503829 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 55 Training Loss: 0.13316787779331207 \n",
      "P: Output= -1.594801397009551 --- Target= 2.191291796904693e-07\n",
      "Q: Output= 1.669889722672881 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 56 Training Loss: 0.13448649644851685 \n",
      "P: Output= -1.6245862159921884 --- Target= 1.239808575803636e-07\n",
      "Q: Output= 1.6103078307389715 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 57 Training Loss: 0.13042570650577545 \n",
      "P: Output= -1.5703222782932222 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 1.231381267086312 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 58 Training Loss: 0.1338055431842804 \n",
      "P: Output= -1.6317995931249127 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 1.1895234113732283 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 59 Training Loss: 0.12464851140975952 \n",
      "P: Output= -1.3796265247868522 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 2.8280757537239074 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 60 Training Loss: 0.146536722779274 \n",
      "P: Output= -1.2306548741812922 --- Target= 9.096384445683725e-08\n",
      "Q: Output= 2.907582330496214 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 61 Training Loss: 0.14879411458969116 \n",
      "P: Output= -1.3796798796586263 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= 1.592429883994285 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 62 Training Loss: 0.13002850115299225 \n",
      "P: Output= -1.3997295526311495 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= 1.8541111520669737 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 63 Training Loss: 0.12660518288612366 \n",
      "P: Output= -1.3580571568999709 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= 2.508002918022555 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 64 Training Loss: 0.13608378171920776 \n",
      "P: Output= -1.0854445642318522 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 3.349599544219135 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 65 Training Loss: 0.13922041654586792 \n",
      "P: Output= -1.2869081837101026 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 2.793856661711514 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 66 Training Loss: 0.13449418544769287 \n",
      "P: Output= -1.4846499302897636 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 2.083616111769966 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 67 Training Loss: 0.12600573897361755 \n",
      "P: Output= -1.5455931062165398 --- Target= 6.895684645513711e-08\n",
      "Q: Output= 1.061475504600776 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 68 Training Loss: 0.13491389155387878 \n",
      "P: Output= -2.1115920039293616 --- Target= 9.865799821540122e-08\n",
      "Q: Output= -0.6006629051337153 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 69 Training Loss: 0.12159083783626556 \n",
      "P: Output= -2.1649055100982117 --- Target= 9.40822530992591e-10\n",
      "Q: Output= -0.8456610193827743 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 70 Training Loss: 0.12491554766893387 \n",
      "P: Output= -1.9831359928007286 --- Target= -1.269564124939393e-07\n",
      "Q: Output= -0.9870432675813436 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 71 Training Loss: 0.12629728019237518 \n",
      "P: Output= -2.055828062133055 --- Target= -1.618743441511583e-07\n",
      "Q: Output= -0.8375169407777587 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 72 Training Loss: 0.1245056763291359 \n",
      "P: Output= -1.5279557190204054 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 0.08947987098790833 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 73 Training Loss: 0.15404807031154633 \n",
      "P: Output= -1.8386851014191237 --- Target= 1.652745300617653e-07\n",
      "Q: Output= -1.863308207499458 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 74 Training Loss: 0.12923918664455414 \n",
      "P: Output= -1.4049593992231175 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= -0.5348663951300017 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 75 Training Loss: 0.1299131214618683 \n",
      "P: Output= -1.4456328558950409 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= -0.6389396132392617 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 76 Training Loss: 0.13539119064807892 \n",
      "P: Output= -1.2202062121952943 --- Target= -5.844421391287824e-08\n",
      "Q: Output= -0.12367965483526433 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 77 Training Loss: 0.13702240586280823 \n",
      "P: Output= -1.5655476236050339 --- Target= 9.813912260625557e-08\n",
      "Q: Output= -1.4879006217800486 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 78 Training Loss: 0.12832465767860413 \n",
      "P: Output= -1.2666987720167215 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 0.2602247092566854 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 79 Training Loss: 0.13423264026641846 \n",
      "P: Output= -1.6594577346415234 --- Target= -9.703328984755899e-08\n",
      "Q: Output= -1.628274171965661 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 80 Training Loss: 0.12599143385887146 \n",
      "P: Output= -1.41427267144016 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 0.23290068504759898 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 81 Training Loss: 0.1340012550354004 \n",
      "P: Output= -1.5142179552637254 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 0.26181510860664936 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 82 Training Loss: 0.1533036231994629 \n",
      "P: Output= -1.7870149925658474 --- Target= -8.159233733096016e-08\n",
      "Q: Output= -0.6227784500838816 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 83 Training Loss: 0.12543895840644836 \n",
      "P: Output= -1.7158636732311994 --- Target= 1.606130251019522e-08\n",
      "Q: Output= -0.05059580969331989 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 84 Training Loss: 0.1279563158750534 \n",
      "P: Output= -1.5312185838477976 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 0.7811745080418433 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 85 Training Loss: 0.1308310329914093 \n",
      "P: Output= -1.4831932605333593 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 1.8622096319047872 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 86 Training Loss: 0.126990407705307 \n",
      "P: Output= -1.065219204649953 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 3.965470157605081 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 87 Training Loss: 0.13945338129997253 \n",
      "P: Output= -1.241532381984654 --- Target= 3.465683571235445e-07\n",
      "Q: Output= 2.5198074813063833 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 88 Training Loss: 0.1272018700838089 \n",
      "P: Output= -0.9833231198267214 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 4.434696722831864 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 89 Training Loss: 0.1391143798828125 \n",
      "P: Output= -0.8954873657154039 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 4.916195021624953 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 90 Training Loss: 0.13696080446243286 \n",
      "P: Output= -1.1465641151176627 --- Target= 9.79909344778207e-08\n",
      "Q: Output= 3.3982886939206596 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 91 Training Loss: 0.1298220157623291 \n",
      "P: Output= -0.9045335726345218 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 4.537962871860105 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 92 Training Loss: 0.14036713540554047 \n",
      "P: Output= -1.3026475930065011 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 4.031946202065662 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 93 Training Loss: 0.13255858421325684 \n",
      "P: Output= -1.2902898588712333 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= 3.3568909846160384 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 94 Training Loss: 0.14004819095134735 \n",
      "P: Output= -1.7010956318495207 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= 1.8123585695059852 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 95 Training Loss: 0.1339794248342514 \n",
      "P: Output= -1.4836372909410445 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 2.8743790103132376 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 96 Training Loss: 0.1364077925682068 \n",
      "P: Output= -1.5635389603100283 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 2.53770129458844 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 97 Training Loss: 0.13411793112754822 \n",
      "P: Output= -1.723139682170939 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 1.2123052126406124 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 98 Training Loss: 0.13711941242218018 \n",
      "P: Output= -1.3613563910405206 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= 3.2247515336341053 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 99 Training Loss: 0.1466132402420044 \n",
      "P: Output= -1.713525929306443 --- Target= -1.695987421612699e-07\n",
      "Q: Output= 2.8587170269548707 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 100 Training Loss: 0.12351326644420624 \n",
      "P: Output= -1.3648672143721958 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 4.439489392013167 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 101 Training Loss: 0.13352450728416443 \n",
      "P: Output= -1.3801733361469983 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 4.012479223416781 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 102 Training Loss: 0.14563477039337158 \n",
      "P: Output= -1.2517511485951482 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 4.141884845445057 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 103 Training Loss: 0.13504700362682343 \n",
      "P: Output= -1.5225239723363577 --- Target= 8.498954429114747e-08\n",
      "Q: Output= 2.650495849372305 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 104 Training Loss: 0.12596774101257324 \n",
      "P: Output= -1.2632934776023097 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 3.810651244350506 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 105 Training Loss: 0.14223627746105194 \n",
      "P: Output= -1.7381483755012122 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= 2.266809023033513 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 106 Training Loss: 0.12324479222297668 \n",
      "P: Output= -1.5106637003097036 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 3.1559195161721245 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 107 Training Loss: 0.15279117226600647 \n",
      "P: Output= -2.0034601585287497 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= 1.2063724127869682 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 108 Training Loss: 0.12695732712745667 \n",
      "P: Output= -2.0633652258049073 --- Target= -9.226324593214486e-08\n",
      "Q: Output= 0.07979595008831275 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 109 Training Loss: 0.12929992377758026 \n",
      "P: Output= -1.7978504966282527 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 0.45020232333515064 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 110 Training Loss: 0.14984846115112305 \n",
      "P: Output= -1.7841140826753596 --- Target= 1.72196169323513e-07\n",
      "Q: Output= 0.14575351211435184 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 111 Training Loss: 0.13680312037467957 \n",
      "P: Output= -2.152948481725331 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= -1.7645441477278818 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 112 Training Loss: 0.12847504019737244 \n",
      "P: Output= -1.7132360281463725 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= -0.6475997811675231 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 113 Training Loss: 0.1301322877407074 \n",
      "P: Output= -2.0787961899711975 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= -2.433987507998665 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 114 Training Loss: 0.1257050335407257 \n",
      "P: Output= -1.6103444936307998 --- Target= 8.009064700331692e-08\n",
      "Q: Output= -0.8138959885951982 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 115 Training Loss: 0.1334737241268158 \n",
      "P: Output= -2.087334596919712 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= -1.903489772628976 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 116 Training Loss: 0.12650901079177856 \n",
      "P: Output= -1.820469803909087 --- Target= -1.917980831933619e-07\n",
      "Q: Output= -0.09795547244965608 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 117 Training Loss: 0.13557811081409454 \n",
      "P: Output= -1.5459891300208506 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= -0.15224414684563214 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 118 Training Loss: 0.13829797506332397 \n",
      "P: Output= -2.081849601784177 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= -1.923246953809834 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 119 Training Loss: 0.12655389308929443 \n",
      "P: Output= -1.845868664406054 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= -0.5451598881922433 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 120 Training Loss: 0.13879840075969696 \n",
      "P: Output= -2.0328883324142817 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= -1.8556061323818422 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 121 Training Loss: 0.1332521140575409 \n",
      "P: Output= -2.0084940248823697 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= -1.4641676214181079 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 122 Training Loss: 0.12250988930463791 \n",
      "P: Output= -1.923526352855836 --- Target= -9.128675593217395e-09\n",
      "Q: Output= -0.929104582204868 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 123 Training Loss: 0.12509731948375702 \n",
      "P: Output= -1.9486217133390973 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= 0.20162784073282403 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 124 Training Loss: 0.13220471143722534 \n",
      "P: Output= -1.280067619437526 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 2.3345155895921303 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 125 Training Loss: 0.12925408780574799 \n",
      "P: Output= -1.5751705694513811 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= 1.1478951977521366 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 126 Training Loss: 0.12147864699363708 \n",
      "P: Output= -1.1931204444703543 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 3.031059181589961 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 127 Training Loss: 0.13814066350460052 \n",
      "P: Output= -1.4971642546888821 --- Target= -4.072268247057309e-07\n",
      "Q: Output= 1.6133446473247846 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 128 Training Loss: 0.12979283928871155 \n",
      "P: Output= -1.0813746644261704 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= 3.454699703702371 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 129 Training Loss: 0.1304512321949005 \n",
      "P: Output= -1.5450209257921461 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= 1.6093095846375762 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 130 Training Loss: 0.12981921434402466 \n",
      "P: Output= -1.7436984098696966 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= 1.669554571729047 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 131 Training Loss: 0.11941404640674591 \n",
      "P: Output= -1.2904767840338351 --- Target= 2.021405629548667e-07\n",
      "Q: Output= 3.293061896203999 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 132 Training Loss: 0.13008001446723938 \n",
      "P: Output= -1.4529890781551789 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 2.8859008742388905 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 133 Training Loss: 0.1304592788219452 \n",
      "P: Output= -1.4228652610972041 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 2.9047991724225346 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 134 Training Loss: 0.13761869072914124 \n",
      "P: Output= -1.7865535323651116 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= 1.8567003454723752 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 135 Training Loss: 0.12531186640262604 \n",
      "P: Output= -1.2878355401835861 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 3.593974298337968 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 136 Training Loss: 0.1461062878370285 \n",
      "P: Output= -1.4749200570015821 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 4.1621788212950905 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 137 Training Loss: 0.13506948947906494 \n",
      "P: Output= -1.6816968992056038 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= 2.637214727713501 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 138 Training Loss: 0.13005560636520386 \n",
      "P: Output= -1.332966146910259 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 3.8434813144956275 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 139 Training Loss: 0.12931405007839203 \n",
      "P: Output= -1.5423593938248228 --- Target= -6.546786224248535e-09\n",
      "Q: Output= 2.6817242807868418 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 140 Training Loss: 0.12858739495277405 \n",
      "P: Output= -1.1279166334873487 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 4.38139019758683 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 141 Training Loss: 0.13001461327075958 \n",
      "P: Output= -1.4979982200245123 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= 2.8102423315320113 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 142 Training Loss: 0.12530601024627686 \n",
      "P: Output= -1.028850586216394 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 4.31075742117457 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 143 Training Loss: 0.1340147852897644 \n",
      "P: Output= -1.3585577512600864 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 4.822802146993908 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 144 Training Loss: 0.14070886373519897 \n",
      "P: Output= -1.3724666746641478 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 4.448122811634617 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 145 Training Loss: 0.14802777767181396 \n",
      "P: Output= -1.4594346607942503 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 4.1031722792183825 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 146 Training Loss: 0.14072787761688232 \n",
      "P: Output= -1.574158351192887 --- Target= -1.490649044200154e-07\n",
      "Q: Output= 3.3786676911206435 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 147 Training Loss: 0.14001567661762238 \n",
      "P: Output= -1.7992247759430722 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 2.4149336062440403 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 148 Training Loss: 0.12865965068340302 \n",
      "P: Output= -2.214262699544726 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 0.22784654450236275 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 149 Training Loss: 0.13595519959926605 \n",
      "P: Output= -1.9337127874830973 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 1.2764952558364664 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 150 Training Loss: 0.1468377411365509 \n",
      "P: Output= -2.284974881892367 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= -0.44109814328674357 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 151 Training Loss: 0.1278843879699707 \n",
      "P: Output= -1.8608334097669958 --- Target= 6.896767335007326e-08\n",
      "Q: Output= 0.5106165372583806 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 152 Training Loss: 0.13716548681259155 \n",
      "P: Output= -2.119764962393571 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= -1.3582938754477798 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 153 Training Loss: 0.1310957372188568 \n",
      "P: Output= -2.17057146222953 --- Target= -3.576887390721595e-07\n",
      "Q: Output= -1.4990226498969967 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 154 Training Loss: 0.12558767199516296 \n",
      "P: Output= -1.7491275236143995 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= -0.7012929713354428 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 155 Training Loss: 0.124493308365345 \n",
      "P: Output= -1.5777264209474806 --- Target= 3.967031148022215e-09\n",
      "Q: Output= -1.3146144406721776 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 156 Training Loss: 0.13589653372764587 \n",
      "P: Output= -1.7427086847683544 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= -1.9073224835310616 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 157 Training Loss: 0.1420418918132782 \n",
      "P: Output= -2.340125054038082 --- Target= 1.04987089244446e-07\n",
      "Q: Output= -3.448313364518298 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 158 Training Loss: 0.1221306174993515 \n",
      "P: Output= -1.8686020796516365 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= -2.417592797710678 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 159 Training Loss: 0.13097476959228516 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -2.3910078259021033 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= -4.448238431169394 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 0 Validation Loss: 0.12260231375694275 \n",
      "P: Output= -2.42227568818159 --- Target= 3.239203305582805e-08\n",
      "Q: Output= -4.443669417492833 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 1 Validation Loss: 0.12067610770463943 \n",
      "P: Output= -1.8520408465825655 --- Target= 1.3670019605172e-07\n",
      "Q: Output= -2.8874425578508593 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 2 Validation Loss: 0.13301342725753784 \n",
      "P: Output= -1.9165398171348382 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= -3.0283931602655105 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 3 Validation Loss: 0.12946084141731262 \n",
      "P: Output= -2.4125123223661733 --- Target= 2.911645777814442e-07\n",
      "Q: Output= -4.319505556504492 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 4 Validation Loss: 0.1339622586965561 \n",
      "P: Output= -2.001719416705006 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= -2.7407880252302657 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 5 Validation Loss: 0.14400914311408997 \n",
      "P: Output= -1.9057537497011756 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= -2.736749874415551 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 6 Validation Loss: 0.16731636226177216 \n",
      "P: Output= -2.2904763373611665 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= -4.7115711048961355 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 7 Validation Loss: 0.12709009647369385 \n",
      "P: Output= -1.9355324686644577 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= -2.7560300615751694 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 8 Validation Loss: 0.1330576390028 \n",
      "P: Output= -2.3944075810789025 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= -4.500321709061777 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 9 Validation Loss: 0.15123935043811798 \n",
      "P: Output= -1.8710556609581026 --- Target= -1.907385298594022e-07\n",
      "Q: Output= -2.8079268571001754 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 10 Validation Loss: 0.15308000147342682 \n",
      "P: Output= -1.8765433550240829 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= -2.809168497142963 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 11 Validation Loss: 0.14757876098155975 \n",
      "P: Output= -2.052817405868163 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= -2.725701383321612 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 12 Validation Loss: 0.14005808532238007 \n",
      "P: Output= -1.9447544598852815 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= -3.071467013342631 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 13 Validation Loss: 0.13631470501422882 \n",
      "P: Output= -2.3579921479005135 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= -4.378461795580414 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 14 Validation Loss: 0.11971527338027954 \n",
      "P: Output= -1.9022721487330143 --- Target= 2.077415652834702e-07\n",
      "Q: Output= -2.5799974345943464 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 15 Validation Loss: 0.14771908521652222 \n",
      "P: Output= -1.9651527137384797 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= -2.781396026716528 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 16 Validation Loss: 0.12441876530647278 \n",
      "P: Output= -1.8598375637583002 --- Target= 5.541669878539324e-08\n",
      "Q: Output= -2.8702948841870057 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 17 Validation Loss: 0.13076378405094147 \n",
      "P: Output= -2.424082474979696 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= -4.0374491072347 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 18 Validation Loss: 0.1221797913312912 \n",
      "P: Output= -2.3371314879938874 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= -4.328043447586297 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 19 Validation Loss: 0.13300612568855286 \n",
      "P: Output= -1.885092803930946 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= -2.8388952465970987 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 20 Validation Loss: 0.14326556026935577 \n",
      "P: Output= -2.291765968975918 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= -4.404048365754933 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 21 Validation Loss: 0.12377462536096573 \n",
      "P: Output= -2.0038120394704384 --- Target= 7.48511617132408e-08\n",
      "Q: Output= -2.8820862675984 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 22 Validation Loss: 0.13488569855690002 \n",
      "P: Output= -1.835771425941637 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= -2.820460825526263 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 23 Validation Loss: 0.13107553124427795 \n",
      "P: Output= -2.220487540427036 --- Target= 4.268641484728164e-07\n",
      "Q: Output= -4.394179735699752 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 24 Validation Loss: 0.12843559682369232 \n",
      "P: Output= -2.3952327528577975 --- Target= 2.239196774667107e-07\n",
      "Q: Output= -4.474548874945758 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 25 Validation Loss: 0.12623152136802673 \n",
      "P: Output= -2.353999879197275 --- Target= -2.219336545650208e-07\n",
      "Q: Output= -4.590036869068223 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 26 Validation Loss: 0.12349723279476166 \n",
      "P: Output= -2.060369207893835 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= -2.550952159073174 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 27 Validation Loss: 0.13125282526016235 \n",
      "P: Output= -1.9258114992957713 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= -2.7401389784239134 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 28 Validation Loss: 0.13408595323562622 \n",
      "P: Output= -1.928019987847561 --- Target= -8.584417265922184e-08\n",
      "Q: Output= -2.819274192434471 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 29 Validation Loss: 0.1505301296710968 \n",
      "Epoch: 7\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -1.8808301852609448 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= -2.85442402187845 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 0 Training Loss: 0.13599009811878204 \n",
      "P: Output= -2.420610227163963 --- Target= 1.606130251019522e-08\n",
      "Q: Output= -4.937481162323382 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 1 Training Loss: 0.1398627609014511 \n",
      "P: Output= -2.0222438111067023 --- Target= 2.011873458940272e-07\n",
      "Q: Output= -2.6314994673348764 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 2 Training Loss: 0.1478162407875061 \n",
      "P: Output= -2.3958597473976297 --- Target= -4.072268247057309e-07\n",
      "Q: Output= -2.7721739096679454 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 3 Training Loss: 0.12682649493217468 \n",
      "P: Output= -1.8033762458287521 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 0.49589895879191737 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 4 Training Loss: 0.1460443139076233 \n",
      "P: Output= -1.848337943320999 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 2.000712042433511 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 5 Training Loss: 0.13891175389289856 \n",
      "P: Output= -1.8309631494977658 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 2.818248543091501 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 6 Training Loss: 0.1508481651544571 \n",
      "P: Output= -2.03566718144621 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= 2.3728948503375333 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 7 Training Loss: 0.12505224347114563 \n",
      "P: Output= -1.527940695512938 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= 4.239488255943252 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 8 Training Loss: 0.1384344846010208 \n",
      "P: Output= -1.9366764786107167 --- Target= -9.226324593214486e-08\n",
      "Q: Output= 3.567971623010263 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 9 Training Loss: 0.12633386254310608 \n",
      "P: Output= -1.9942565551127895 --- Target= 9.865799821540122e-08\n",
      "Q: Output= 3.5947584076118986 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 10 Training Loss: 0.11904744803905487 \n",
      "P: Output= -1.9415584305917193 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= 3.332651104361318 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 11 Training Loss: 0.130168616771698 \n",
      "P: Output= -1.7454268963730684 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 4.811683544392121 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 12 Training Loss: 0.12662935256958008 \n",
      "P: Output= -1.8171598696717037 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 4.8941208410374495 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 13 Training Loss: 0.13156911730766296 \n",
      "P: Output= -2.1248491004210655 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= 3.8208195030007337 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 14 Training Loss: 0.12570837140083313 \n",
      "P: Output= -1.8193797918188697 --- Target= 2.021405629548667e-07\n",
      "Q: Output= 5.076180647569285 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 15 Training Loss: 0.13980230689048767 \n",
      "P: Output= -1.9785487578612928 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 4.824959003253996 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 16 Training Loss: 0.12952439486980438 \n",
      "P: Output= -1.9056057105585902 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= 4.473694549625062 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 17 Training Loss: 0.1365361213684082 \n",
      "P: Output= -2.323922645874264 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= 2.938622627641662 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 18 Training Loss: 0.12687528133392334 \n",
      "P: Output= -2.206887937102393 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 3.044104708717061 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 19 Training Loss: 0.13681240379810333 \n",
      "P: Output= -2.6952547877475608 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= 1.0175791465640591 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 20 Training Loss: 0.13454118371009827 \n",
      "P: Output= -2.533890455380848 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= 0.5862985269989309 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 21 Training Loss: 0.12143364548683167 \n",
      "P: Output= -2.57415290482367 --- Target= 1.239808575803636e-07\n",
      "Q: Output= -0.04013610964018799 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 22 Training Loss: 0.12290357053279877 \n",
      "P: Output= -2.0551218286882573 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 0.6928780308963267 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 23 Training Loss: 0.1564883291721344 \n",
      "P: Output= -2.4285158477606554 --- Target= 9.813912260625557e-08\n",
      "Q: Output= -1.0727377990279638 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 24 Training Loss: 0.12837892770767212 \n",
      "P: Output= -1.9812608701864498 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= -0.10356419779651116 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 25 Training Loss: 0.13047850131988525 \n",
      "P: Output= -2.4106927242754574 --- Target= -2.741996745214692e-07\n",
      "Q: Output= -2.1306027941819385 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 26 Training Loss: 0.12845182418823242 \n",
      "P: Output= -1.9961820014080063 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= -0.6154947815662535 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 27 Training Loss: 0.1327277421951294 \n",
      "P: Output= -2.4591988228909436 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= -1.727141913096255 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 28 Training Loss: 0.12196160852909088 \n",
      "P: Output= -2.5189265844750404 --- Target= 1.04987089244446e-07\n",
      "Q: Output= -1.1145627879361575 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 29 Training Loss: 0.11954367160797119 \n",
      "P: Output= -2.320805890112938 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= -0.9915955785255655 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 30 Training Loss: 0.11754671484231949 \n",
      "P: Output= -1.947950739669463 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 0.7155818485349181 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 31 Training Loss: 0.1421121507883072 \n",
      "P: Output= -1.8431422428787236 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 1.3898959187223738 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 32 Training Loss: 0.13190637528896332 \n",
      "P: Output= -2.1097647221338613 --- Target= 4.168930338721566e-07\n",
      "Q: Output= 0.4378007559745596 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 33 Training Loss: 0.12512969970703125 \n",
      "P: Output= -1.8525297160468837 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 0.7347811737446674 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 34 Training Loss: 0.12699097394943237 \n",
      "P: Output= -1.553109915690146 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 2.7594782656433123 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 35 Training Loss: 0.1252727061510086 \n",
      "P: Output= -1.5875900626864832 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 3.003488182452262 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 36 Training Loss: 0.1328783482313156 \n",
      "P: Output= -1.4193669450223148 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 3.7481143481418755 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 37 Training Loss: 0.1282431185245514 \n",
      "P: Output= -1.435419678414731 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 4.1075139509326775 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 38 Training Loss: 0.12706612050533295 \n",
      "P: Output= -1.507228519039022 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 4.342683445041578 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 39 Training Loss: 0.14701417088508606 \n",
      "P: Output= -1.8884636358450253 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 3.054863236658621 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 40 Training Loss: 0.12485544383525848 \n",
      "P: Output= -2.0070229147607295 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= 2.6020943506785947 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 41 Training Loss: 0.12757715582847595 \n",
      "P: Output= -1.9323825790526374 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 2.67591179880451 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 42 Training Loss: 0.12389904260635376 \n",
      "P: Output= -1.8417186842612558 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 2.429416435536904 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 43 Training Loss: 0.12939587235450745 \n",
      "P: Output= -1.6690470515842257 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 3.437857807360338 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 44 Training Loss: 0.14830945432186127 \n",
      "P: Output= -1.7815248063033424 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 2.9251972653489346 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 45 Training Loss: 0.13212847709655762 \n",
      "P: Output= -1.6008180980867088 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 2.3558837663704004 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 46 Training Loss: 0.13143950700759888 \n",
      "P: Output= -1.919115947655822 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 0.3354722440888045 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 47 Training Loss: 0.12127766013145447 \n",
      "P: Output= -1.973465840993602 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 1.138763964914582 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 48 Training Loss: 0.13140131533145905 \n",
      "P: Output= -2.2069808161496196 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= -0.7818143221656806 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 49 Training Loss: 0.12574905157089233 \n",
      "P: Output= -1.947834794765071 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 0.38344331593319225 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 50 Training Loss: 0.12967461347579956 \n",
      "P: Output= -2.0234620118678466 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= 0.3991914182154739 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 51 Training Loss: 0.14058347046375275 \n",
      "P: Output= -2.438453426078251 --- Target= 6.39805808333449e-09\n",
      "Q: Output= -0.8853430927854715 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 52 Training Loss: 0.12239305675029755 \n",
      "P: Output= -2.1522409219575156 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 0.2540760396213173 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 53 Training Loss: 0.13740992546081543 \n",
      "P: Output= -2.4009986379620827 --- Target= 8.498954429114747e-08\n",
      "Q: Output= -1.1493716342581912 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 54 Training Loss: 0.1201641708612442 \n",
      "P: Output= -2.4638176705065646 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= -1.1049252347199623 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 55 Training Loss: 0.1257324069738388 \n",
      "P: Output= -2.469086300270579 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= -0.7363554565868133 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 56 Training Loss: 0.1168598085641861 \n",
      "P: Output= -1.8658014013253155 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 0.9424563499293228 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 57 Training Loss: 0.12061648815870285 \n",
      "P: Output= -1.840778779639268 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 1.1483226451395545 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 58 Training Loss: 0.1305951178073883 \n",
      "P: Output= -1.9948675939503646 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= 0.1599039174646153 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 59 Training Loss: 0.12260882556438446 \n",
      "P: Output= -1.9640706015990643 --- Target= -6.546786224248535e-09\n",
      "Q: Output= 0.8564013676251738 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 60 Training Loss: 0.12045378983020782 \n",
      "P: Output= -1.7586603475846454 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 1.0756213251514524 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 61 Training Loss: 0.12657274305820465 \n",
      "P: Output= -1.423185120190868 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 2.8219908604197794 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 62 Training Loss: 0.13655738532543182 \n",
      "P: Output= -1.5166268728596544 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 3.3070654025606325 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 63 Training Loss: 0.1359591782093048 \n",
      "P: Output= -1.670728761569948 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 2.1018628566565356 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 64 Training Loss: 0.12231700122356415 \n",
      "P: Output= -1.6110600565526614 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 2.9981816250032765 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 65 Training Loss: 0.11794999241828918 \n",
      "P: Output= -1.53398524207911 --- Target= 3.465683571235445e-07\n",
      "Q: Output= 3.008060676391022 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 66 Training Loss: 0.12627741694450378 \n",
      "P: Output= -1.3917503982706814 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 4.235933493835589 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 67 Training Loss: 0.13078925013542175 \n",
      "P: Output= -1.8246045650728098 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= 3.095526789083494 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 68 Training Loss: 0.12166896462440491 \n",
      "P: Output= -1.869716327963821 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= 2.531879249592202 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 69 Training Loss: 0.11833702027797699 \n",
      "P: Output= -1.577700051865694 --- Target= 5.936255753624664e-08\n",
      "Q: Output= 3.4247795504130405 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 70 Training Loss: 0.14762373268604279 \n",
      "P: Output= -1.9388169785659128 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 3.028703131923587 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 71 Training Loss: 0.13375645875930786 \n",
      "P: Output= -2.1966708530323844 --- Target= 2.191291796904693e-07\n",
      "Q: Output= 1.369697029781408 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 72 Training Loss: 0.12921525537967682 \n",
      "P: Output= -1.9100099206999115 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 2.668808370280341 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 73 Training Loss: 0.12358421087265015 \n",
      "P: Output= -1.8974582043990527 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 2.5752226107557457 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 74 Training Loss: 0.1399361789226532 \n",
      "P: Output= -1.5967848657617703 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 2.4762297853686315 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 75 Training Loss: 0.13061925768852234 \n",
      "P: Output= -1.9480673263003832 --- Target= 1.885248828159547e-07\n",
      "Q: Output= 1.299759356869365 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 76 Training Loss: 0.12086039781570435 \n",
      "P: Output= -1.596532146181472 --- Target= 6.895684645513711e-08\n",
      "Q: Output= 2.745485367303619 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 77 Training Loss: 0.12360914051532745 \n",
      "P: Output= -1.9384228211718026 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 1.839503021467861 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 78 Training Loss: 0.12300069630146027 \n",
      "P: Output= -1.9376782028806714 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= 1.9494184854243208 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 79 Training Loss: 0.12278611958026886 \n",
      "P: Output= -1.7165293886369346 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= 3.3979247454085773 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 80 Training Loss: 0.15233726799488068 \n",
      "P: Output= -2.062205192656821 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= 1.8517354733118028 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 81 Training Loss: 0.11870507895946503 \n",
      "P: Output= -2.207943006926059 --- Target= 9.257290134456753e-08\n",
      "Q: Output= 1.2036564157711647 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 82 Training Loss: 0.12509992718696594 \n",
      "P: Output= -1.898234726860668 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 1.3594284369292007 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 83 Training Loss: 0.13917459547519684 \n",
      "P: Output= -2.1995853818557576 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 1.033660271619116 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 84 Training Loss: 0.14172247052192688 \n",
      "P: Output= -2.1489649529874857 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 0.6448858597157967 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 85 Training Loss: 0.12395764887332916 \n",
      "P: Output= -2.1764493320016793 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 0.5475677186045838 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 86 Training Loss: 0.1302214413881302 \n",
      "P: Output= -2.321140390559142 --- Target= 1.652745300617653e-07\n",
      "Q: Output= -1.3138825888686307 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 87 Training Loss: 0.12727531790733337 \n",
      "P: Output= -2.3555626125148947 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= -1.2629933231324815 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 88 Training Loss: 0.13036203384399414 \n",
      "P: Output= -1.6847948684179164 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 0.33745473798987025 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 89 Training Loss: 0.12801498174667358 \n",
      "P: Output= -2.0813358799567414 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= -0.5618399130248743 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 90 Training Loss: 0.1231686919927597 \n",
      "P: Output= -1.7750350597813114 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= -0.042350656843964885 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 91 Training Loss: 0.12171228229999542 \n",
      "P: Output= -1.385988042191011 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 1.740199182935119 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 92 Training Loss: 0.13821664452552795 \n",
      "P: Output= -1.5868981189243554 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 0.9954872323244315 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 93 Training Loss: 0.11851310729980469 \n",
      "P: Output= -1.5232561959401378 --- Target= 1.93652547331169e-07\n",
      "Q: Output= 1.0452501101175926 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 94 Training Loss: 0.12942278385162354 \n",
      "P: Output= -1.3763327080414625 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 3.091123607842735 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 95 Training Loss: 0.12393428385257721 \n",
      "P: Output= -1.4948944496357157 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 2.9156608140227647 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 96 Training Loss: 0.13014449179172516 \n",
      "P: Output= -1.54834122441141 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 2.2784336391401974 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 97 Training Loss: 0.12857769429683685 \n",
      "P: Output= -1.7854187915594713 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 1.8567258703746132 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 98 Training Loss: 0.12675949931144714 \n",
      "P: Output= -1.9423891113256175 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 0.9512138994681001 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 99 Training Loss: 0.12926778197288513 \n",
      "P: Output= -2.3038870082686493 --- Target= -9.128675593217395e-09\n",
      "Q: Output= -1.4573668526412016 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 100 Training Loss: 0.11902923882007599 \n",
      "P: Output= -1.9896295389544045 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 0.1857800043369391 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 101 Training Loss: 0.12870237231254578 \n",
      "P: Output= -1.863658516614782 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 0.043707322846878505 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 102 Training Loss: 0.12889812886714935 \n",
      "P: Output= -2.18171542127964 --- Target= -3.576887390721595e-07\n",
      "Q: Output= -1.261732558596548 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 103 Training Loss: 0.12084057927131653 \n",
      "P: Output= -1.9435702196989029 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= -1.0633339633190353 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 104 Training Loss: 0.12033826112747192 \n",
      "P: Output= -1.9791427814312916 --- Target= -5.012515025271114e-08\n",
      "Q: Output= -1.1662695980920184 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 105 Training Loss: 0.12319499999284744 \n",
      "P: Output= -1.3608824317457184 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 0.29238219258760356 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 106 Training Loss: 0.1327105462551117 \n",
      "P: Output= -1.2842697197147812 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= -0.22674904652420036 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 107 Training Loss: 0.13073262572288513 \n",
      "P: Output= -1.6108711971278238 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= -1.6421373887973711 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 108 Training Loss: 0.12157199531793594 \n",
      "P: Output= -1.6404270807235397 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= -1.6796404578176385 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 109 Training Loss: 0.1362977921962738 \n",
      "P: Output= -1.2892269362210396 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 1.016052014470966 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 110 Training Loss: 0.12590965628623962 \n",
      "P: Output= -1.7176003026989992 --- Target= -1.695987421612699e-07\n",
      "Q: Output= -0.03179982187251795 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 111 Training Loss: 0.1268644630908966 \n",
      "P: Output= -1.3926741375721967 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 2.093839090066231 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 112 Training Loss: 0.13554149866104126 \n",
      "P: Output= -1.3774784834876268 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 2.9935061115573927 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 113 Training Loss: 0.1284080147743225 \n",
      "P: Output= -1.4366212244302012 --- Target= 9.096384445683725e-08\n",
      "Q: Output= 3.270772133274291 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 114 Training Loss: 0.12992054224014282 \n",
      "P: Output= -1.809321580337114 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= 1.7555478967304285 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 115 Training Loss: 0.1196710392832756 \n",
      "P: Output= -1.4096104982145246 --- Target= -1.490649044200154e-07\n",
      "Q: Output= 3.035312307475774 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 116 Training Loss: 0.14743363857269287 \n",
      "P: Output= -1.8738201781847748 --- Target= -8.159233733096016e-08\n",
      "Q: Output= 1.4495657083539486 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 117 Training Loss: 0.12011270225048065 \n",
      "P: Output= -1.4831174962354385 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 3.361506492325775 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 118 Training Loss: 0.13385705649852753 \n",
      "P: Output= -1.341142306657126 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 3.5558707650743244 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 119 Training Loss: 0.12664508819580078 \n",
      "P: Output= -1.6524931933992724 --- Target= -9.703328984755899e-08\n",
      "Q: Output= 2.1659632588652435 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 120 Training Loss: 0.11829198896884918 \n",
      "P: Output= -1.6582898434762425 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= 2.5010276610198643 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 121 Training Loss: 0.12029668688774109 \n",
      "P: Output= -1.3837718590683794 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= 3.219731213456593 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 122 Training Loss: 0.11779855191707611 \n",
      "P: Output= -1.0673603750095024 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 5.213001517167326 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 123 Training Loss: 0.14193667471408844 \n",
      "P: Output= -1.0557203110861915 --- Target= 6.896767335007326e-08\n",
      "Q: Output= 5.164756750713176 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 124 Training Loss: 0.15348711609840393 \n",
      "P: Output= -1.27482108010045 --- Target= 9.79909344778207e-08\n",
      "Q: Output= 3.2657861527220886 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 125 Training Loss: 0.12289762496948242 \n",
      "P: Output= -1.169168912231041 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 4.150761455105954 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 126 Training Loss: 0.12487258017063141 \n",
      "P: Output= -1.620150269637496 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 1.8683361259852385 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 127 Training Loss: 0.12707598507404327 \n",
      "P: Output= -1.9306650895991098 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= 0.5146278436070659 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 128 Training Loss: 0.11740918457508087 \n",
      "P: Output= -1.845017163840149 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 1.0242227452280792 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 129 Training Loss: 0.12594866752624512 \n",
      "P: Output= -1.8039816093920624 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= -0.2262539325220807 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 130 Training Loss: 0.14146795868873596 \n",
      "P: Output= -2.3401947629464255 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= -1.8941770487282827 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 131 Training Loss: 0.14752037823200226 \n",
      "P: Output= -1.894286480524447 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= 0.20296203081420483 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 132 Training Loss: 0.1265357881784439 \n",
      "P: Output= -2.1515930211410677 --- Target= -3.730653368450021e-07\n",
      "Q: Output= -0.24341542724384713 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 133 Training Loss: 0.11571415513753891 \n",
      "P: Output= -1.5981802290652984 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 1.5770669278602059 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 134 Training Loss: 0.12617476284503937 \n",
      "P: Output= -1.6862634126474623 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 2.219957099936316 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 135 Training Loss: 0.1261892169713974 \n",
      "P: Output= -1.8360441993520835 --- Target= -1.618743441511583e-07\n",
      "Q: Output= 1.1315421995211752 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 136 Training Loss: 0.11846083402633667 \n",
      "P: Output= -1.836245115783556 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= 0.5830119043890427 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 137 Training Loss: 0.1189226508140564 \n",
      "P: Output= -1.8011652928969095 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= 0.8168512401248886 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 138 Training Loss: 0.11403079330921173 \n",
      "P: Output= -1.596856920254858 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 2.1294301207159796 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 139 Training Loss: 0.13524585962295532 \n",
      "P: Output= -1.572296566924976 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 1.9449612529435232 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 140 Training Loss: 0.12676697969436646 \n",
      "P: Output= -1.886545048748772 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= 0.5031185105929037 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 141 Training Loss: 0.12349288165569305 \n",
      "P: Output= -1.47771626109731 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 1.756633037844165 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 142 Training Loss: 0.1272195726633072 \n",
      "P: Output= -1.5149758748399362 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 1.1994950739003318 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 143 Training Loss: 0.12475212663412094 \n",
      "P: Output= -1.4829638144259345 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 1.3690057660061496 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 144 Training Loss: 0.12686952948570251 \n",
      "P: Output= -1.4070090529390216 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 0.8720704163390991 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 145 Training Loss: 0.1416221410036087 \n",
      "P: Output= -1.7625528911382977 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= -0.041986254908353615 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 146 Training Loss: 0.13264529407024384 \n",
      "P: Output= -1.2279440952669072 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 1.5308612126590475 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 147 Training Loss: 0.13934926688671112 \n",
      "P: Output= -1.4928505333063349 --- Target= -8.441947496606872e-08\n",
      "Q: Output= 0.8875126792441277 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 148 Training Loss: 0.12884247303009033 \n",
      "P: Output= -1.3183023944517016 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 3.0420268401743416 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 149 Training Loss: 0.12726661562919617 \n",
      "P: Output= -0.9698923659159693 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 3.32218590859385 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 150 Training Loss: 0.13323649764060974 \n",
      "P: Output= -1.088216231631769 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 3.730588082023866 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 151 Training Loss: 0.1425168663263321 \n",
      "P: Output= -1.1557507027846752 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 4.26250945790434 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 152 Training Loss: 0.13342809677124023 \n",
      "P: Output= -1.6046426679161394 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= 2.841262033001607 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 153 Training Loss: 0.12124443799257278 \n",
      "P: Output= -1.781171183618512 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= 2.6679167704178157 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 154 Training Loss: 0.13212743401527405 \n",
      "P: Output= -2.036753780649926 --- Target= 9.40822530992591e-10\n",
      "Q: Output= 1.4839108261857668 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 155 Training Loss: 0.13652221858501434 \n",
      "P: Output= -2.2709417563083036 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= -0.0619050818421627 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 156 Training Loss: 0.1198911964893341 \n",
      "P: Output= -1.956765402769328 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 0.17377993491144395 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 157 Training Loss: 0.14629730582237244 \n",
      "P: Output= -1.9924093259638713 --- Target= 1.72196169323513e-07\n",
      "Q: Output= -0.4410112598601117 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 158 Training Loss: 0.13791853189468384 \n",
      "P: Output= -1.9314124690487162 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= -0.9399053401231772 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 159 Training Loss: 0.13502316176891327 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -1.8096666066844787 --- Target= 2.077415652834702e-07\n",
      "Q: Output= -0.9111376239540148 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 0 Validation Loss: 0.1297873556613922 \n",
      "P: Output= -2.2480091289110042 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= -2.753265834600077 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 1 Validation Loss: 0.12032830715179443 \n",
      "P: Output= -1.83620445463524 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= -1.3785794388991284 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 2 Validation Loss: 0.13440968096256256 \n",
      "P: Output= -1.7750639153858492 --- Target= -1.907385298594022e-07\n",
      "Q: Output= -1.1197937741458652 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 3 Validation Loss: 0.13892050087451935 \n",
      "P: Output= -1.8360305868644646 --- Target= -8.584417265922184e-08\n",
      "Q: Output= -1.142423350222436 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 4 Validation Loss: 0.13602879643440247 \n",
      "P: Output= -1.839115035544424 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= -1.0818686715546058 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 5 Validation Loss: 0.13145846128463745 \n",
      "P: Output= -2.2331865055624913 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= -2.8416931699873227 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 6 Validation Loss: 0.1294611394405365 \n",
      "P: Output= -1.8368701316204987 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= -1.0562060378323546 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 7 Validation Loss: 0.13347838819026947 \n",
      "P: Output= -2.2474935136225964 --- Target= 2.239196774667107e-07\n",
      "Q: Output= -2.8004040089003746 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 8 Validation Loss: 0.12760479748249054 \n",
      "P: Output= -1.7590336568573912 --- Target= 5.541669878539324e-08\n",
      "Q: Output= -1.177448579435202 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 9 Validation Loss: 0.12669295072555542 \n",
      "P: Output= -2.2825488676223262 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= -2.36577072347979 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 10 Validation Loss: 0.12000791728496552 \n",
      "P: Output= -1.7384482968765793 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= -1.1223509623111871 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 11 Validation Loss: 0.1308324933052063 \n",
      "P: Output= -1.8622483254368642 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= -1.1012192093249373 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 12 Validation Loss: 0.12720510363578796 \n",
      "P: Output= -1.8183162032773037 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= -1.0667415478498556 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 13 Validation Loss: 0.13823489844799042 \n",
      "P: Output= -1.955284329122211 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= -1.0835385083982194 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 14 Validation Loss: 0.13139060139656067 \n",
      "P: Output= -1.755132968463779 --- Target= 1.3670019605172e-07\n",
      "Q: Output= -1.1856284708430582 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 15 Validation Loss: 0.12646034359931946 \n",
      "P: Output= -2.2632578493930824 --- Target= 3.239203305582805e-08\n",
      "Q: Output= -2.7681355133541867 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 16 Validation Loss: 0.12875042855739594 \n",
      "P: Output= -2.132116690756627 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= -3.0377014956275037 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 17 Validation Loss: 0.1240444928407669 \n",
      "P: Output= -2.202049011653477 --- Target= -2.219336545650208e-07\n",
      "Q: Output= -2.9007339582872147 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 18 Validation Loss: 0.12210257351398468 \n",
      "P: Output= -1.9041737937877672 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= -1.0753169590445362 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 19 Validation Loss: 0.1346002072095871 \n",
      "P: Output= -1.7895847078430016 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= -1.1397071312501952 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 20 Validation Loss: 0.12977036833763123 \n",
      "P: Output= -1.7742060118592669 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= -1.1184898564041923 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 21 Validation Loss: 0.13872873783111572 \n",
      "P: Output= -2.1363803244579254 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= -2.7116107537960006 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 22 Validation Loss: 0.1272917538881302 \n",
      "P: Output= -2.2691579036069864 --- Target= 2.911645777814442e-07\n",
      "Q: Output= -2.6492163938619875 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 23 Validation Loss: 0.1267297863960266 \n",
      "P: Output= -2.0717472261679593 --- Target= 4.268641484728164e-07\n",
      "Q: Output= -2.699368690734644 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 24 Validation Loss: 0.12918752431869507 \n",
      "P: Output= -2.1898508910110843 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= -2.6412432919899413 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 25 Validation Loss: 0.12016839534044266 \n",
      "P: Output= -1.9011470948463849 --- Target= 7.48511617132408e-08\n",
      "Q: Output= -1.2155898043312083 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 26 Validation Loss: 0.13051295280456543 \n",
      "P: Output= -1.971558973436312 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= -0.9084315921789505 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 27 Validation Loss: 0.1234050840139389 \n",
      "P: Output= -2.2111399405291747 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= -2.682388676307215 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 28 Validation Loss: 0.1274951696395874 \n",
      "P: Output= -1.8135641035784102 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= -1.3424737069554906 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 29 Validation Loss: 0.13039708137512207 \n",
      "Epoch: 8\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -1.88666782620139 --- Target= -9.425136404672685e-08\n",
      "Q: Output= -1.1478876832016587 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 0 Training Loss: 0.12662968039512634 \n",
      "P: Output= -2.081051813307676 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= -3.0242121333878695 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 1 Training Loss: 0.12753599882125854 \n",
      "P: Output= -1.4851641632051313 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= -0.7015828226148288 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 2 Training Loss: 0.1286148726940155 \n",
      "P: Output= -1.7588359889811018 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= -1.665479362103905 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 3 Training Loss: 0.12076963484287262 \n",
      "P: Output= -1.3804362116351383 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 0.1463805767900439 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 4 Training Loss: 0.1420711874961853 \n",
      "P: Output= -1.2134034028741425 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 0.7136521372077347 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 5 Training Loss: 0.14702332019805908 \n",
      "P: Output= -1.5586086412756632 --- Target= 1.586396098929299e-07\n",
      "Q: Output= -0.30749708976844303 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 6 Training Loss: 0.11722252517938614 \n",
      "P: Output= -1.5671115623762404 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= 0.5748468817413901 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 7 Training Loss: 0.13154350221157074 \n",
      "P: Output= -1.4226224284234323 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 3.009886193594708 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 8 Training Loss: 0.1487189531326294 \n",
      "P: Output= -1.4166159129616753 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 3.416011718566409 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 9 Training Loss: 0.14136573672294617 \n",
      "P: Output= -1.6658671484861047 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 1.9531054485388104 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 10 Training Loss: 0.12408699095249176 \n",
      "P: Output= -1.8830284768921217 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= 2.1350092562156187 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 11 Training Loss: 0.12326157838106155 \n",
      "P: Output= -2.007820094243902 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 2.23397616318238 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 12 Training Loss: 0.12621648609638214 \n",
      "P: Output= -1.626946577326489 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 3.732404027706015 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 13 Training Loss: 0.13072004914283752 \n",
      "P: Output= -2.0509165093028434 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= 2.8314202212110233 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 14 Training Loss: 0.12980517745018005 \n",
      "P: Output= -1.8075286160532285 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 3.668766379658539 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 15 Training Loss: 0.1312730610370636 \n",
      "P: Output= -1.8063200060831308 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 3.5623039408401773 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 16 Training Loss: 0.1337822675704956 \n",
      "P: Output= -2.066661080352505 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= 1.6378691996785433 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 17 Training Loss: 0.12350431084632874 \n",
      "P: Output= -1.7213694968993778 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 2.0336449733875375 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 18 Training Loss: 0.1530793309211731 \n",
      "P: Output= -2.0229583900491237 --- Target= 9.257290134456753e-08\n",
      "Q: Output= 0.18343985132390195 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 19 Training Loss: 0.11906811594963074 \n",
      "P: Output= -1.554065577231584 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 1.2344926655759298 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 20 Training Loss: 0.1279207319021225 \n",
      "P: Output= -1.8227740510461663 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= 0.10445867784792018 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 21 Training Loss: 0.12079966068267822 \n",
      "P: Output= -1.7376218383828537 --- Target= 1.04987089244446e-07\n",
      "Q: Output= 0.49412214400579924 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 22 Training Loss: 0.11560836434364319 \n",
      "P: Output= -1.2486890082442628 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 2.283480999383211 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 23 Training Loss: 0.1294489949941635 \n",
      "P: Output= -1.1550756839861824 --- Target= 1.72196169323513e-07\n",
      "Q: Output= 2.558948369091439 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 24 Training Loss: 0.13160933554172516 \n",
      "P: Output= -1.5231278382578752 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= 0.8440459842349934 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 25 Training Loss: 0.11474959552288055 \n",
      "P: Output= -1.5135796072140542 --- Target= 9.813912260625557e-08\n",
      "Q: Output= 0.616377673531705 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 26 Training Loss: 0.11796237528324127 \n",
      "P: Output= -1.4677283497108373 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 1.8437878630523281 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 27 Training Loss: 0.13247895240783691 \n",
      "P: Output= -1.3910359759408912 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 1.4240257565299022 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 28 Training Loss: 0.1308702528476715 \n",
      "P: Output= -1.9910727985497125 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= -0.06918434029551168 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 29 Training Loss: 0.1378025859594345 \n",
      "P: Output= -1.5602855779117402 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 0.9537161890761396 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 30 Training Loss: 0.1249488964676857 \n",
      "P: Output= -1.76604670596054 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 0.9615630736438963 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 31 Training Loss: 0.1257106214761734 \n",
      "P: Output= -1.9489452320804457 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= -0.34743125909074024 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 32 Training Loss: 0.11652190238237381 \n",
      "P: Output= -1.444672564118278 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 1.399399712584116 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 33 Training Loss: 0.12723833322525024 \n",
      "P: Output= -1.420474615172557 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= 1.7637829020628724 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 34 Training Loss: 0.12619666755199432 \n",
      "P: Output= -1.643079078448892 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= 0.20474361182069512 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 35 Training Loss: 0.12374173849821091 \n",
      "P: Output= -1.21441248650195 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 1.7045089487911262 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 36 Training Loss: 0.12517935037612915 \n",
      "P: Output= -1.5443481827991947 --- Target= -8.441947496606872e-08\n",
      "Q: Output= -0.47527209318720764 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 37 Training Loss: 0.1279783397912979 \n",
      "P: Output= -1.3590248060096224 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 1.2095539093119685 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 38 Training Loss: 0.12190812826156616 \n",
      "P: Output= -1.4728434454932895 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= -0.7801323037545052 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 39 Training Loss: 0.12363439798355103 \n",
      "P: Output= -1.5543543675302125 --- Target= -6.546786224248535e-09\n",
      "Q: Output= -0.48870829099429347 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 40 Training Loss: 0.11905944347381592 \n",
      "P: Output= -1.4700922402330354 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= -0.7667693132852644 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 41 Training Loss: 0.11975421011447906 \n",
      "P: Output= -1.1608076073926616 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= 1.4421037196016755 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 42 Training Loss: 0.12153971195220947 \n",
      "P: Output= -1.07074141078927 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 1.6765678480895296 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 43 Training Loss: 0.1291448473930359 \n",
      "P: Output= -1.2224134523105956 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 1.9649035362111 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 44 Training Loss: 0.1353941261768341 \n",
      "P: Output= -1.2383269869900024 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= 1.9539915064717075 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 45 Training Loss: 0.12504354119300842 \n",
      "P: Output= -1.3090084169671359 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 1.8643916232665854 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 46 Training Loss: 0.12609650194644928 \n",
      "P: Output= -1.7415868120336846 --- Target= -3.576887390721595e-07\n",
      "Q: Output= 0.712667659299413 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 47 Training Loss: 0.1179068386554718 \n",
      "P: Output= -1.5175509451401439 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 2.4381440896377287 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 48 Training Loss: 0.13661780953407288 \n",
      "P: Output= -1.426271067737022 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 2.1489376598620753 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 49 Training Loss: 0.12168294191360474 \n",
      "P: Output= -1.326019610392711 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 1.9013334195229836 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 50 Training Loss: 0.12408003211021423 \n",
      "P: Output= -1.7491074406761742 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 0.5910314601870468 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 51 Training Loss: 0.11512023210525513 \n",
      "P: Output= -1.4408678840972096 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 1.9560339953249422 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 52 Training Loss: 0.12482937425374985 \n",
      "P: Output= -1.3975638173884413 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 1.466546136968371 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 53 Training Loss: 0.12285806238651276 \n",
      "P: Output= -1.4119435240331528 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 1.3366689930880922 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 54 Training Loss: 0.13945823907852173 \n",
      "P: Output= -1.3179420394798527 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 1.8199061071180527 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 55 Training Loss: 0.11670815944671631 \n",
      "P: Output= -1.3981202969813546 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 1.5876084591698305 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 56 Training Loss: 0.1233508437871933 \n",
      "P: Output= -1.638424270805074 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 0.5421070617814108 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 57 Training Loss: 0.11608010530471802 \n",
      "P: Output= -1.2408106488166286 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 2.666854460759394 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 58 Training Loss: 0.11981716752052307 \n",
      "P: Output= -1.1821942645196648 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 2.71771711120436 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 59 Training Loss: 0.13156473636627197 \n",
      "P: Output= -1.5912704320434532 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= 1.764064681019633 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 60 Training Loss: 0.12028057873249054 \n",
      "P: Output= -1.4510084833822932 --- Target= 1.606130251019522e-08\n",
      "Q: Output= 1.923049500180844 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 61 Training Loss: 0.1196279376745224 \n",
      "P: Output= -1.3723209766425306 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 1.8827493085802312 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 62 Training Loss: 0.11761694401502609 \n",
      "P: Output= -1.496095860044516 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= 1.7900595737166558 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 63 Training Loss: 0.11408103257417679 \n",
      "P: Output= -1.2623357509498412 --- Target= 9.096384445683725e-08\n",
      "Q: Output= 3.6761501082974197 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 64 Training Loss: 0.13887058198451996 \n",
      "P: Output= -1.1878354023597852 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 3.428391478255808 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 65 Training Loss: 0.1220950037240982 \n",
      "P: Output= -1.2389610110252152 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 2.913422563890043 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 66 Training Loss: 0.12119614332914352 \n",
      "P: Output= -1.3166034192532337 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 2.4235251339659065 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 67 Training Loss: 0.12185893207788467 \n",
      "P: Output= -1.64498009285151 --- Target= -9.128675593217395e-09\n",
      "Q: Output= -0.09509056612085498 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 68 Training Loss: 0.11208876967430115 \n",
      "P: Output= -1.447505779041406 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 1.8127492609180438 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 69 Training Loss: 0.130631685256958 \n",
      "P: Output= -1.4057705557105837 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 1.6926078544936063 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 70 Training Loss: 0.12081267684698105 \n",
      "P: Output= -1.3563268971192892 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= 1.5849854120498161 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 71 Training Loss: 0.12569747865200043 \n",
      "P: Output= -1.8174720931566721 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= 0.5674644877360624 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 72 Training Loss: 0.12615132331848145 \n",
      "P: Output= -1.7500148154937483 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= 0.16741093621591396 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 73 Training Loss: 0.12201130390167236 \n",
      "P: Output= -1.7221834633234465 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= -0.11099262110317198 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 74 Training Loss: 0.1224418431520462 \n",
      "P: Output= -1.7787688389950072 --- Target= -5.012515025271114e-08\n",
      "Q: Output= -0.2043806499171037 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 75 Training Loss: 0.1236661970615387 \n",
      "P: Output= -1.6556130334386197 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= -0.30098430855690594 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 76 Training Loss: 0.11391547322273254 \n",
      "P: Output= -1.095548338671371 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 1.605541373436571 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 77 Training Loss: 0.12332525849342346 \n",
      "P: Output= -1.3650023226835373 --- Target= 4.168930338721566e-07\n",
      "Q: Output= -0.13559897659126907 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 78 Training Loss: 0.1198214516043663 \n",
      "P: Output= -1.2153423370417737 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= -0.09487891688634242 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 79 Training Loss: 0.1133142039179802 \n",
      "P: Output= -1.3546273583191022 --- Target= 2.191291796904693e-07\n",
      "Q: Output= -0.49516759483889583 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 80 Training Loss: 0.12114125490188599 \n",
      "P: Output= -1.369497196558073 --- Target= -8.159233733096016e-08\n",
      "Q: Output= -0.9596657078335147 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 81 Training Loss: 0.11630105972290039 \n",
      "P: Output= -1.4008896601887022 --- Target= 9.79909344778207e-08\n",
      "Q: Output= -1.1124408636102352 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 82 Training Loss: 0.11826089024543762 \n",
      "P: Output= -1.1941655503856108 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 0.8753477186294356 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 83 Training Loss: 0.13066188991069794 \n",
      "P: Output= -1.5861151790495711 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= -1.0315672762906303 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 84 Training Loss: 0.12360064685344696 \n",
      "P: Output= -1.7958009203594392 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= -1.2633170908617553 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 85 Training Loss: 0.1242929995059967 \n",
      "P: Output= -1.4751927352079353 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 1.62842966357828 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 86 Training Loss: 0.12686173617839813 \n",
      "P: Output= -1.6571328737571331 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 0.6995117372493747 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 87 Training Loss: 0.12124209851026535 \n",
      "P: Output= -1.3364223532977748 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 2.995113907057684 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 88 Training Loss: 0.12448494881391525 \n",
      "P: Output= -1.365566874660285 --- Target= 2.021405629548667e-07\n",
      "Q: Output= 3.381923698873959 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 89 Training Loss: 0.11960108578205109 \n",
      "P: Output= -1.3547656741517171 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 3.015045143794284 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 90 Training Loss: 0.12526103854179382 \n",
      "P: Output= -1.7647374858947238 --- Target= 1.239808575803636e-07\n",
      "Q: Output= 0.8327859841999521 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 91 Training Loss: 0.11981566250324249 \n",
      "P: Output= -1.7268080983755274 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= 0.5867664370512689 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 92 Training Loss: 0.11171773821115494 \n",
      "P: Output= -1.694241465059231 --- Target= -9.703328984755899e-08\n",
      "Q: Output= 0.1726200975790535 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 93 Training Loss: 0.11560609936714172 \n",
      "P: Output= -1.744792657118638 --- Target= -4.072268247057309e-07\n",
      "Q: Output= -0.11825144714393154 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 94 Training Loss: 0.121397465467453 \n",
      "P: Output= -1.441103212533828 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 1.855812886517012 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 95 Training Loss: 0.13244567811489105 \n",
      "P: Output= -1.7223604384796722 --- Target= -3.730653368450021e-07\n",
      "Q: Output= -0.215319236904306 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 96 Training Loss: 0.11150360852479935 \n",
      "P: Output= -1.1445905878178433 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 1.213713687243942 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 97 Training Loss: 0.12631811201572418 \n",
      "P: Output= -1.5212748345003053 --- Target= 3.465683571235445e-07\n",
      "Q: Output= -0.501605668824598 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 98 Training Loss: 0.11770352721214294 \n",
      "P: Output= -1.0933291172058226 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 2.3083704807688683 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 99 Training Loss: 0.12872648239135742 \n",
      "P: Output= -1.4328190402092345 --- Target= -1.618743441511583e-07\n",
      "Q: Output= 1.604427334307923 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 100 Training Loss: 0.11238528788089752 \n",
      "P: Output= -1.4369545788232871 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= 2.202662049454972 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 101 Training Loss: 0.11509037762880325 \n",
      "P: Output= -1.2761692496979276 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= 2.1713117911400897 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 102 Training Loss: 0.11452284455299377 \n",
      "P: Output= -1.086911421487632 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 4.116753659723054 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 103 Training Loss: 0.12869420647621155 \n",
      "P: Output= -1.1297107237090893 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 4.154877250703169 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 104 Training Loss: 0.1289258450269699 \n",
      "P: Output= -1.0465352280996658 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 4.578092769974449 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 105 Training Loss: 0.1208726167678833 \n",
      "P: Output= -1.0943888542596723 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= 4.218730445281501 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 106 Training Loss: 0.13517436385154724 \n",
      "P: Output= -1.0660248299914965 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 4.074005529503272 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 107 Training Loss: 0.11932186782360077 \n",
      "P: Output= -1.1847235896240642 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 3.335682826492924 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 108 Training Loss: 0.12219561636447906 \n",
      "P: Output= -1.4071365194695122 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= 0.871469328531914 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 109 Training Loss: 0.11820141971111298 \n",
      "P: Output= -1.4988186260127856 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= -0.033836511834635985 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 110 Training Loss: 0.11899034678936005 \n",
      "P: Output= -1.5987604766827763 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= -0.8238174476801499 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 111 Training Loss: 0.11551894247531891 \n",
      "P: Output= -1.4703204324823078 --- Target= 1.93652547331169e-07\n",
      "Q: Output= -1.7008993931608103 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 112 Training Loss: 0.12584923207759857 \n",
      "P: Output= -1.135543949411173 --- Target= 6.895684645513711e-08\n",
      "Q: Output= -0.08731406413825482 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 113 Training Loss: 0.14639660716056824 \n",
      "P: Output= -1.1582758703866682 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 0.16880630530457896 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 114 Training Loss: 0.13600246608257294 \n",
      "P: Output= -1.6596905956985566 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= -1.939137063477601 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 115 Training Loss: 0.11701598763465881 \n",
      "P: Output= -1.2425467934087608 --- Target= -1.490649044200154e-07\n",
      "Q: Output= -0.08015604855001701 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 116 Training Loss: 0.1240653246641159 \n",
      "P: Output= -1.4861705784410217 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= -0.10204441081738125 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 117 Training Loss: 0.1391739696264267 \n",
      "P: Output= -1.526761656392428 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 0.05791199411740244 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 118 Training Loss: 0.1319362223148346 \n",
      "P: Output= -1.498270996705787 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= -0.1749383487642433 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 119 Training Loss: 0.13887426257133484 \n",
      "P: Output= -1.8441678173604323 --- Target= -1.695987421612699e-07\n",
      "Q: Output= -1.1381046492783664 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 120 Training Loss: 0.12844528257846832 \n",
      "P: Output= -1.697166802431222 --- Target= 6.39805808333449e-09\n",
      "Q: Output= 0.1475852180257382 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 121 Training Loss: 0.11616633832454681 \n",
      "P: Output= -1.5005919564029417 --- Target= 1.885248828159547e-07\n",
      "Q: Output= 1.149747805888448 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 122 Training Loss: 0.11628071218729019 \n",
      "P: Output= -1.3046840645631583 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 3.6540140660816505 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 123 Training Loss: 0.13967490196228027 \n",
      "P: Output= -1.4044585826994567 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 3.9149274547722843 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 124 Training Loss: 0.12563717365264893 \n",
      "P: Output= -1.2871129076447216 --- Target= 8.498954429114747e-08\n",
      "Q: Output= 2.453074481220275 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 125 Training Loss: 0.11729668080806732 \n",
      "P: Output= -1.171375097728344 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 4.647821913502572 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 126 Training Loss: 0.12491011619567871 \n",
      "P: Output= -1.258589375563151 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 3.160100572475402 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 127 Training Loss: 0.12763917446136475 \n",
      "P: Output= -1.1971296371239593 --- Target= 5.936255753624664e-08\n",
      "Q: Output= 4.147620621682284 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 128 Training Loss: 0.1289200633764267 \n",
      "P: Output= -1.7272597917915853 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= 2.270942032459769 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 129 Training Loss: 0.12074418365955353 \n",
      "P: Output= -1.7833662697703536 --- Target= -2.741996745214692e-07\n",
      "Q: Output= 1.2996626746648632 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 130 Training Loss: 0.11537862569093704 \n",
      "P: Output= -1.6311919568409436 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 2.154432748427423 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 131 Training Loss: 0.14090630412101746 \n",
      "P: Output= -1.7111906748331212 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 1.5759170303443462 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 132 Training Loss: 0.1253749579191208 \n",
      "P: Output= -1.8259498757833397 --- Target= 1.726564144988174e-07\n",
      "Q: Output= -0.7169843929141351 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 133 Training Loss: 0.1260911524295807 \n",
      "P: Output= -1.552640623985404 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 0.909023674605189 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 134 Training Loss: 0.12307429313659668 \n",
      "P: Output= -1.7074928072590545 --- Target= -9.226324593214486e-08\n",
      "Q: Output= -0.6535250880183909 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 135 Training Loss: 0.11714144051074982 \n",
      "P: Output= -1.228310635032356 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= 1.1637085280701784 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 136 Training Loss: 0.13483983278274536 \n",
      "P: Output= -1.2586601117031941 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 2.1573417512012973 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 137 Training Loss: 0.132805734872818 \n",
      "P: Output= -1.4749425451955327 --- Target= 1.652745300617653e-07\n",
      "Q: Output= 0.47777607496195085 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 138 Training Loss: 0.11548459529876709 \n",
      "P: Output= -1.5365445255964447 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 0.9715971061378061 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 139 Training Loss: 0.11973422765731812 \n",
      "P: Output= -1.5667316364051773 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 2.704476097285644 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 140 Training Loss: 0.1260552555322647 \n",
      "P: Output= -1.7071254538832346 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= 1.4120103991964683 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 141 Training Loss: 0.11651657521724701 \n",
      "P: Output= -1.4228532731403902 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 1.9851105311888606 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 142 Training Loss: 0.12486636638641357 \n",
      "P: Output= -1.740170016620124 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 2.031856186973095 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 143 Training Loss: 0.13722720742225647 \n",
      "P: Output= -1.9668237898170862 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= -0.19227067077414883 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 144 Training Loss: 0.11544832587242126 \n",
      "P: Output= -1.9417131669879168 --- Target= -6.683027464760016e-08\n",
      "Q: Output= -0.09220354501039285 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 145 Training Loss: 0.11193720251321793 \n",
      "P: Output= -1.845639370257726 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= -0.40652773642048956 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 146 Training Loss: 0.1127789318561554 \n",
      "P: Output= -1.5685935036588488 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 1.5267519540218073 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 147 Training Loss: 0.12321573495864868 \n",
      "P: Output= -1.160452682754233 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 2.0172288689111486 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 148 Training Loss: 0.11954648792743683 \n",
      "P: Output= -1.1720553166146228 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 2.558492629115796 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 149 Training Loss: 0.11938019841909409 \n",
      "P: Output= -1.020509106943913 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 2.8984069832936923 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 150 Training Loss: 0.11596597731113434 \n",
      "P: Output= -0.9450213308656705 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 3.088944113431376 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 151 Training Loss: 0.13589712977409363 \n",
      "P: Output= -0.9882546692014138 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 2.5107561768775533 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 152 Training Loss: 0.12758825719356537 \n",
      "P: Output= -1.4245011678544657 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= 0.2670590416061396 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 153 Training Loss: 0.11292756348848343 \n",
      "P: Output= -1.5408468667570157 --- Target= 9.40822530992591e-10\n",
      "Q: Output= -0.23202601437454096 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 154 Training Loss: 0.11681462824344635 \n",
      "P: Output= -1.6319931910978545 --- Target= 9.865799821540122e-08\n",
      "Q: Output= -0.8588021088522586 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 155 Training Loss: 0.10881772637367249 \n",
      "P: Output= -1.5196700269191314 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 0.37688202408451144 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 156 Training Loss: 0.12355531007051468 \n",
      "P: Output= -1.4377089556019929 --- Target= 6.896767335007326e-08\n",
      "Q: Output= -0.12237377771085445 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 157 Training Loss: 0.12868300080299377 \n",
      "P: Output= -1.802269448902245 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= -2.3252708748495854 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 158 Training Loss: 0.11774004995822906 \n",
      "P: Output= -1.3776964069399122 --- Target= 8.384345484557798e-08\n",
      "Q: Output= -1.358448393669275 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 159 Training Loss: 0.1325361430644989 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -1.834675473767053 --- Target= 2.239196774667107e-07\n",
      "Q: Output= -3.4540791035279876 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 0 Validation Loss: 0.12069737166166306 \n",
      "P: Output= -1.5955956165843155 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= -1.469218080444311 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 1 Validation Loss: 0.13035784661769867 \n",
      "P: Output= -1.4500096478399858 --- Target= 2.077415652834702e-07\n",
      "Q: Output= -1.2858679284217276 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 2 Validation Loss: 0.1288014054298401 \n",
      "P: Output= -1.7242494084666626 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= -3.370087147788628 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 3 Validation Loss: 0.11241874098777771 \n",
      "P: Output= -1.543206843436976 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= -1.4737752022996404 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 4 Validation Loss: 0.14064240455627441 \n",
      "P: Output= -1.6153366643562288 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= -1.2607593730666897 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 5 Validation Loss: 0.12339602410793304 \n",
      "P: Output= -1.7991338676434863 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= -3.3207156064486103 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 6 Validation Loss: 0.11964654922485352 \n",
      "P: Output= -1.46895604692103 --- Target= -8.584417265922184e-08\n",
      "Q: Output= -1.554449426907421 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 7 Validation Loss: 0.1284709870815277 \n",
      "P: Output= -1.437613395791339 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= -1.7995274907205365 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 8 Validation Loss: 0.12561261653900146 \n",
      "P: Output= -1.4950931257522617 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= -1.5131608446818108 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 9 Validation Loss: 0.12073003500699997 \n",
      "P: Output= -1.4174837954467678 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= -1.5651839637339542 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 10 Validation Loss: 0.12534619867801666 \n",
      "P: Output= -1.4687169410091467 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= -1.4511210729817314 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 11 Validation Loss: 0.11630535125732422 \n",
      "P: Output= -1.4588355486738784 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= -1.842899065678269 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 12 Validation Loss: 0.11843186616897583 \n",
      "P: Output= -1.7164703024072878 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= -3.7460119835237347 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 13 Validation Loss: 0.1186446100473404 \n",
      "P: Output= -1.861432986558052 --- Target= 2.911645777814442e-07\n",
      "Q: Output= -3.2682935519625653 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 14 Validation Loss: 0.11577757447957993 \n",
      "P: Output= -1.3841214300756786 --- Target= 1.3670019605172e-07\n",
      "Q: Output= -1.6174818481893527 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 15 Validation Loss: 0.1266462504863739 \n",
      "P: Output= -1.827791587824275 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= -3.504782895287436 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 16 Validation Loss: 0.13253995776176453 \n",
      "P: Output= -1.369497002345721 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= -1.5477767869975967 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 17 Validation Loss: 0.12739625573158264 \n",
      "P: Output= -1.4060702207335218 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= -1.5454879118723253 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 18 Validation Loss: 0.12344293296337128 \n",
      "P: Output= -1.8347490102829775 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= -3.3977182275478324 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 19 Validation Loss: 0.11009476333856583 \n",
      "P: Output= -1.6622668279182635 --- Target= 4.268641484728164e-07\n",
      "Q: Output= -3.3550515053151098 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 20 Validation Loss: 0.1254863291978836 \n",
      "P: Output= -1.4070647408612622 --- Target= -1.907385298594022e-07\n",
      "Q: Output= -1.5431025751284833 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 21 Validation Loss: 0.13214372098445892 \n",
      "P: Output= -1.882816552156367 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= -2.9364611631770194 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 22 Validation Loss: 0.11081269383430481 \n",
      "P: Output= -1.5324457685519262 --- Target= 7.48511617132408e-08\n",
      "Q: Output= -1.6411260284317457 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 23 Validation Loss: 0.1282011866569519 \n",
      "P: Output= -1.8523867677590022 --- Target= 3.239203305582805e-08\n",
      "Q: Output= -3.4203217824630805 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 24 Validation Loss: 0.11124833673238754 \n",
      "P: Output= -1.7873022317312195 --- Target= -2.219336545650208e-07\n",
      "Q: Output= -3.577111279211027 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 25 Validation Loss: 0.11262267827987671 \n",
      "P: Output= -1.4549985697437897 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= -1.4675212364835786 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 26 Validation Loss: 0.13973066210746765 \n",
      "P: Output= -1.4726049228886549 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= -1.4860208117797016 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 27 Validation Loss: 0.11871031671762466 \n",
      "P: Output= -1.7828738805734847 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= -3.273963093139508 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 28 Validation Loss: 0.11600291728973389 \n",
      "P: Output= -1.3889930441247387 --- Target= 5.541669878539324e-08\n",
      "Q: Output= -1.6112796804024794 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 29 Validation Loss: 0.12234589457511902 \n",
      "Epoch: 9\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -1.499107577961543 --- Target= 9.096384445683725e-08\n",
      "Q: Output= -1.325977367360653 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 0 Training Loss: 0.11887218803167343 \n",
      "P: Output= -1.7971101359139334 --- Target= -8.159233733096016e-08\n",
      "Q: Output= -3.760597919308246 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 1 Training Loss: 0.11295855045318604 \n",
      "P: Output= -1.4016553791363116 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= -2.1088633152315484 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 2 Training Loss: 0.11874039471149445 \n",
      "P: Output= -1.4839789636110128 --- Target= 1.897620762747465e-07\n",
      "Q: Output= -2.2498752290391977 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 3 Training Loss: 0.13572019338607788 \n",
      "P: Output= -1.9648871550573208 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= -3.750381262164155 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 4 Training Loss: 0.1299506276845932 \n",
      "P: Output= -1.488723825452701 --- Target= 6.895684645513711e-08\n",
      "Q: Output= -1.2392594645527577 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 5 Training Loss: 0.12575115263462067 \n",
      "P: Output= -1.8647799676158021 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= -1.371833363096302 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 6 Training Loss: 0.12070862948894501 \n",
      "P: Output= -1.9157917557936486 --- Target= -6.546786224248535e-09\n",
      "Q: Output= 1.1753791908463898 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 7 Training Loss: 0.11044932901859283 \n",
      "P: Output= -1.4868045842777988 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 4.25618787927635 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 8 Training Loss: 0.1295577585697174 \n",
      "P: Output= -1.772829096130712 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 3.9816703049510958 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 9 Training Loss: 0.11618019640445709 \n",
      "P: Output= -1.8391767654243134 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 4.975017774878344 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 10 Training Loss: 0.11422087997198105 \n",
      "P: Output= -1.8856961320641163 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= 5.151119336432603 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 11 Training Loss: 0.11313564330339432 \n",
      "P: Output= -1.811713519015563 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 6.3397143768206305 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 12 Training Loss: 0.14721861481666565 \n",
      "P: Output= -1.926425289281509 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 5.381517356905004 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 13 Training Loss: 0.12229260802268982 \n",
      "P: Output= -2.2215175757345147 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= 2.7164265708624233 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 14 Training Loss: 0.11349133402109146 \n",
      "P: Output= -2.3284128818251135 --- Target= 9.40822530992591e-10\n",
      "Q: Output= 1.9956551325393983 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 15 Training Loss: 0.11126989871263504 \n",
      "P: Output= -2.0630191246111487 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 3.1514493671013515 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 16 Training Loss: 0.12086275219917297 \n",
      "P: Output= -2.325015999282117 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 0.9981929383181107 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 17 Training Loss: 0.12246237695217133 \n",
      "P: Output= -1.845319528250613 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 2.205163971737104 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 18 Training Loss: 0.12307688593864441 \n",
      "P: Output= -2.0586823506971923 --- Target= 9.865799821540122e-08\n",
      "Q: Output= 0.6932124039033516 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 19 Training Loss: 0.11124580353498459 \n",
      "P: Output= -1.9154484084410175 --- Target= -1.618743441511583e-07\n",
      "Q: Output= 1.4183550082058218 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 20 Training Loss: 0.10713768005371094 \n",
      "P: Output= -1.7567930732702486 --- Target= -3.576887390721595e-07\n",
      "Q: Output= 1.7082362252260266 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 21 Training Loss: 0.11109232157468796 \n",
      "P: Output= -1.2085600810394945 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 3.0419738763974724 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 22 Training Loss: 0.1273459941148758 \n",
      "P: Output= -1.3607807769148526 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 3.7843152818942007 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 23 Training Loss: 0.13074690103530884 \n",
      "P: Output= -1.1877209831802906 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 3.9888586858300856 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 24 Training Loss: 0.12132696807384491 \n",
      "P: Output= -1.2082208407554402 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 3.686202102733114 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 25 Training Loss: 0.12784114480018616 \n",
      "P: Output= -1.113421087610078 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 3.765823166958544 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 26 Training Loss: 0.12473243474960327 \n",
      "P: Output= -1.2396659349858243 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 3.332980966422083 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 27 Training Loss: 0.1272430568933487 \n",
      "P: Output= -1.3306247810382903 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= 2.7798465174547395 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 28 Training Loss: 0.12690944969654083 \n",
      "P: Output= -1.78988954214379 --- Target= 9.813912260625557e-08\n",
      "Q: Output= 0.9768493790928954 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 29 Training Loss: 0.12480156123638153 \n",
      "P: Output= -1.9904569100974179 --- Target= -4.072268247057309e-07\n",
      "Q: Output= -0.22120951605843064 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 30 Training Loss: 0.11491716653108597 \n",
      "P: Output= -2.064713886254671 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= -0.6945444549367439 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 31 Training Loss: 0.11725641787052155 \n",
      "P: Output= -2.039590722479784 --- Target= 3.465683571235445e-07\n",
      "Q: Output= -1.5316402731795504 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 32 Training Loss: 0.11386564373970032 \n",
      "P: Output= -1.6698285973726197 --- Target= 1.7009500474785e-07\n",
      "Q: Output= -0.1832397481193997 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 33 Training Loss: 0.12134626507759094 \n",
      "P: Output= -1.9500590164942606 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= -2.2365928148373797 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 34 Training Loss: 0.10909967869520187 \n",
      "P: Output= -1.7921234849718175 --- Target= 9.79909344778207e-08\n",
      "Q: Output= -2.652116283410521 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 35 Training Loss: 0.1172211766242981 \n",
      "P: Output= -1.3529847730001388 --- Target= 2.021405629548667e-07\n",
      "Q: Output= -0.6073559567245788 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 36 Training Loss: 0.11870894581079483 \n",
      "P: Output= -1.72755152263559 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= -2.355492547656725 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 37 Training Loss: 0.12493664026260376 \n",
      "P: Output= -1.4858565001344104 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= -1.7206332689963824 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 38 Training Loss: 0.12138526886701584 \n",
      "P: Output= -1.4830185238086866 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= -0.4415581793143053 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 39 Training Loss: 0.113487109541893 \n",
      "P: Output= -1.4097465875793178 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 0.7624201617469186 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 40 Training Loss: 0.11524735391139984 \n",
      "P: Output= -1.5314943872182525 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= 1.4069352863403086 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 41 Training Loss: 0.11890508234500885 \n",
      "P: Output= -1.5885536958004467 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 3.223923494685053 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 42 Training Loss: 0.1359894871711731 \n",
      "P: Output= -1.7524524698052373 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 3.0432857364421784 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 43 Training Loss: 0.12761610746383667 \n",
      "P: Output= -1.8845151746101614 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 2.288674960214233 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 44 Training Loss: 0.12377268075942993 \n",
      "P: Output= -2.1192864231071837 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 1.5955671963936124 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 45 Training Loss: 0.13224689662456512 \n",
      "P: Output= -2.1629063269047144 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 0.6822897847087663 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 46 Training Loss: 0.12159393727779388 \n",
      "P: Output= -2.5561174702878144 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= -1.2451833833605024 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 47 Training Loss: 0.11590275168418884 \n",
      "P: Output= -2.5100275674253716 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= -1.4000655766342422 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 48 Training Loss: 0.13764622807502747 \n",
      "P: Output= -2.3282761281640187 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 0.3194636858187838 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 49 Training Loss: 0.12925003468990326 \n",
      "P: Output= -2.334875396899462 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= -0.8335744211094092 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 50 Training Loss: 0.11466707289218903 \n",
      "P: Output= -2.1538989033892424 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= -0.5962329507888873 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 51 Training Loss: 0.11912229657173157 \n",
      "P: Output= -1.8147629799312686 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= 0.8997854123684537 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 52 Training Loss: 0.11373082548379898 \n",
      "P: Output= -1.6593554431584971 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= 1.921606284184091 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 53 Training Loss: 0.11470745503902435 \n",
      "P: Output= -1.169605091398255 --- Target= -1.490649044200154e-07\n",
      "Q: Output= 4.5727032927045315 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 54 Training Loss: 0.14271017909049988 \n",
      "P: Output= -1.2918352087422766 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 5.77061931701995 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 55 Training Loss: 0.14029724895954132 \n",
      "P: Output= -1.4831839199392087 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 6.061538720418442 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 56 Training Loss: 0.12857292592525482 \n",
      "P: Output= -1.4943860425259512 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 6.054992955433449 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 57 Training Loss: 0.12518003582954407 \n",
      "P: Output= -1.7535346854705889 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 4.190181459812273 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 58 Training Loss: 0.11693230271339417 \n",
      "P: Output= -1.9452643058744412 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 4.65020747653104 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 59 Training Loss: 0.13775020837783813 \n",
      "P: Output= -2.139634853626351 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= 3.5108652499476483 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 60 Training Loss: 0.12990590929985046 \n",
      "P: Output= -2.347534365992267 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 2.421933175398279 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 61 Training Loss: 0.1430206298828125 \n",
      "P: Output= -2.6459754793475287 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= -0.10018322293098159 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 62 Training Loss: 0.11009298264980316 \n",
      "P: Output= -2.5342361240365046 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= -0.4699035174046342 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 63 Training Loss: 0.11303064227104187 \n",
      "P: Output= -2.438085875550507 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= -0.5937064822737703 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 64 Training Loss: 0.1316359043121338 \n",
      "P: Output= -2.372926194883412 --- Target= -5.012515025271114e-08\n",
      "Q: Output= -0.5085587725318153 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 65 Training Loss: 0.11790004372596741 \n",
      "P: Output= -1.8552242779042984 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= 1.2418066256499873 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 66 Training Loss: 0.13775739073753357 \n",
      "P: Output= -1.6548831404067998 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 1.2390648106986184 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 67 Training Loss: 0.12115676701068878 \n",
      "P: Output= -1.6406421734332577 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 1.161463134770088 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 68 Training Loss: 0.1256493479013443 \n",
      "P: Output= -1.9544375570736454 --- Target= 4.168930338721566e-07\n",
      "Q: Output= -1.0094669214732717 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 69 Training Loss: 0.11816760897636414 \n",
      "P: Output= -1.7285622834244405 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= -0.08835994908264055 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 70 Training Loss: 0.1253013014793396 \n",
      "P: Output= -2.018700751502913 --- Target= 9.257290134456753e-08\n",
      "Q: Output= -2.2756238768394055 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 71 Training Loss: 0.11516932398080826 \n",
      "P: Output= -2.0098913979382607 --- Target= -9.226324593214486e-08\n",
      "Q: Output= -2.4054805222072857 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 72 Training Loss: 0.12051983922719955 \n",
      "P: Output= -1.6873818942876015 --- Target= -6.915814321217795e-08\n",
      "Q: Output= -0.4670576263095283 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 73 Training Loss: 0.13062354922294617 \n",
      "P: Output= -1.9544987350705645 --- Target= 6.39805808333449e-09\n",
      "Q: Output= -1.5984587806273343 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 74 Training Loss: 0.11435619741678238 \n",
      "P: Output= -1.627971459512171 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 0.18642742049688277 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 75 Training Loss: 0.13583993911743164 \n",
      "P: Output= -1.696477954425843 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 0.5629349375046422 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 76 Training Loss: 0.1178802102804184 \n",
      "P: Output= -1.7174864034016801 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 0.7511265934953126 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 77 Training Loss: 0.12650378048419952 \n",
      "P: Output= -1.634872040319685 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 1.261869363814113 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 78 Training Loss: 0.12186555564403534 \n",
      "P: Output= -1.570668961992328 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 1.8836981507624317 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 79 Training Loss: 0.12762965261936188 \n",
      "P: Output= -1.7658066885130559 --- Target= 8.498954429114747e-08\n",
      "Q: Output= 0.7680366044736147 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 80 Training Loss: 0.11288654804229736 \n",
      "P: Output= -1.7319165399203396 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 2.920693469686757 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 81 Training Loss: 0.13056032359600067 \n",
      "P: Output= -1.6920273055878283 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 3.009332700378902 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 82 Training Loss: 0.1356915682554245 \n",
      "P: Output= -1.8124719334041979 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= 1.8238860655905587 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 83 Training Loss: 0.1111932098865509 \n",
      "P: Output= -1.7831041973610118 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 2.2163020804435707 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 84 Training Loss: 0.11021000146865845 \n",
      "P: Output= -1.7692182346302383 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= 2.851621668612724 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 85 Training Loss: 0.1050892174243927 \n",
      "P: Output= -1.7294622929655201 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= 3.195866225482071 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 86 Training Loss: 0.10660168528556824 \n",
      "P: Output= -1.5736071931074251 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 4.995414549151996 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 87 Training Loss: 0.12062644958496094 \n",
      "P: Output= -1.551260346597422 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 4.638870908217034 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 88 Training Loss: 0.12075256556272507 \n",
      "P: Output= -1.772529266480233 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 3.95203388018376 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 89 Training Loss: 0.12026098370552063 \n",
      "P: Output= -1.8002520523854137 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= 2.826423150531479 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 90 Training Loss: 0.11527937650680542 \n",
      "P: Output= -1.9201653942779187 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 1.7202590796009547 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 91 Training Loss: 0.12912964820861816 \n",
      "P: Output= -2.2016107527811863 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 1.295318175640765 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 92 Training Loss: 0.12305125594139099 \n",
      "P: Output= -2.460867783081686 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= 0.10928236764698607 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 93 Training Loss: 0.12467934936285019 \n",
      "P: Output= -2.3856658435525526 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= 0.0786849849689295 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 94 Training Loss: 0.1189974844455719 \n",
      "P: Output= -2.513656525488675 --- Target= 1.04987089244446e-07\n",
      "Q: Output= 0.24191261932773855 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 95 Training Loss: 0.10808529704809189 \n",
      "P: Output= -2.312524008288392 --- Target= 2.191291796904693e-07\n",
      "Q: Output= 0.13194267618920819 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 96 Training Loss: 0.11897788941860199 \n",
      "P: Output= -2.099566207848345 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= 0.4367851905047573 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 97 Training Loss: 0.11507803946733475 \n",
      "P: Output= -1.8384135952158793 --- Target= 6.896767335007326e-08\n",
      "Q: Output= 1.7952998294762983 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 98 Training Loss: 0.14241145551204681 \n",
      "P: Output= -1.8944234875073684 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= 0.24147827271154743 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 99 Training Loss: 0.11039461195468903 \n",
      "P: Output= -1.5918291243984202 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 0.9758155004306239 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 100 Training Loss: 0.12828773260116577 \n",
      "P: Output= -1.9171475488868612 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= -0.8162374784112689 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 101 Training Loss: 0.10998499393463135 \n",
      "P: Output= -1.735890269055937 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= -0.32443131973079886 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 102 Training Loss: 0.120271697640419 \n",
      "P: Output= -1.6110461304710118 --- Target= 5.936255753624664e-08\n",
      "Q: Output= -1.244686173127172 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 103 Training Loss: 0.13859018683433533 \n",
      "P: Output= -2.1747627780047107 --- Target= -1.695987421612699e-07\n",
      "Q: Output= -2.8597312897539124 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 104 Training Loss: 0.12187476456165314 \n",
      "P: Output= -1.8593159925866685 --- Target= -6.944598229807752e-08\n",
      "Q: Output= -1.0377652064235825 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 105 Training Loss: 0.12991157174110413 \n",
      "P: Output= -1.8527810432922402 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= -0.9831766173035232 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 106 Training Loss: 0.12931427359580994 \n",
      "P: Output= -2.0007742815585274 --- Target= 1.72196169323513e-07\n",
      "Q: Output= -0.5176030833622178 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 107 Training Loss: 0.11862482130527496 \n",
      "P: Output= -2.0445896742709575 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= -0.06493471455876687 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 108 Training Loss: 0.1338946372270584 \n",
      "P: Output= -2.1650821742076687 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= -0.9861591205293196 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 109 Training Loss: 0.11113524436950684 \n",
      "P: Output= -1.8243450196651354 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 1.4158523720174214 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 110 Training Loss: 0.12309277057647705 \n",
      "P: Output= -1.9033207458886343 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 0.2691711127903975 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 111 Training Loss: 0.11332599818706512 \n",
      "P: Output= -1.7415962507893195 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 2.296340810870615 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 112 Training Loss: 0.1226075291633606 \n",
      "P: Output= -1.6297650201435756 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 2.566734094120969 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 113 Training Loss: 0.12693235278129578 \n",
      "P: Output= -1.624936016705325 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 2.3399552888994446 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 114 Training Loss: 0.11906292289495468 \n",
      "P: Output= -1.5602097003307396 --- Target= 1.606130251019522e-08\n",
      "Q: Output= 0.19065145143343987 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 115 Training Loss: 0.10908322036266327 \n",
      "P: Output= -1.538887582369095 --- Target= -2.741996745214692e-07\n",
      "Q: Output= 0.04543361390069656 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 116 Training Loss: 0.11203670501708984 \n",
      "P: Output= -1.4528276933970456 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 1.9382509233539507 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 117 Training Loss: 0.12218020111322403 \n",
      "P: Output= -1.2691220831257306 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 2.2690834750275757 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 118 Training Loss: 0.11862693727016449 \n",
      "P: Output= -1.4016449164701479 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 2.6446675117024903 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 119 Training Loss: 0.1215837299823761 \n",
      "P: Output= -1.284049385464697 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 2.524890823983344 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 120 Training Loss: 0.12716218829154968 \n",
      "P: Output= -1.579030471575698 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= 1.1999462101236977 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 121 Training Loss: 0.11662263423204422 \n",
      "P: Output= -1.4336734092408738 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 2.6319613891083113 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 122 Training Loss: 0.12022535502910614 \n",
      "P: Output= -1.7560983023588106 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= 0.9892122667644188 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 123 Training Loss: 0.10600107908248901 \n",
      "P: Output= -1.6779660322666388 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= 0.6396042898178962 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 124 Training Loss: 0.1069103330373764 \n",
      "P: Output= -1.3829178987896045 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 1.9926748020719574 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 125 Training Loss: 0.12958039343357086 \n",
      "P: Output= -1.607134070508569 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= 0.41773125055267357 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 126 Training Loss: 0.10997335612773895 \n",
      "P: Output= -1.5154927448323994 --- Target= 1.239808575803636e-07\n",
      "Q: Output= 0.4458208825246066 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 127 Training Loss: 0.11665324121713638 \n",
      "P: Output= -1.4197143715264833 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 0.8119390866494731 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 128 Training Loss: 0.10978841781616211 \n",
      "P: Output= -1.3105662193460814 --- Target= -9.128675593217395e-09\n",
      "Q: Output= 1.2828531566488452 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 129 Training Loss: 0.10848550498485565 \n",
      "P: Output= -1.1904398796468278 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 3.6920462273706036 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 130 Training Loss: 0.1212388277053833 \n",
      "P: Output= -1.2022584637785387 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 3.4572612109131313 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 131 Training Loss: 0.119542196393013 \n",
      "P: Output= -1.356193456884661 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= 3.0986773559506915 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 132 Training Loss: 0.125474214553833 \n",
      "P: Output= -1.6450044143672677 --- Target= 1.652745300617653e-07\n",
      "Q: Output= 1.0957096185827888 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 133 Training Loss: 0.11190278828144073 \n",
      "P: Output= -1.4100138300472231 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 2.04576763528498 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 134 Training Loss: 0.11642294377088547 \n",
      "P: Output= -1.6589346032876975 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 2.273832459887121 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 135 Training Loss: 0.14893829822540283 \n",
      "P: Output= -1.6903501788247315 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= 1.8488494974748306 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 136 Training Loss: 0.12412643432617188 \n",
      "P: Output= -1.7545205461459927 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 1.636046240160561 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 137 Training Loss: 0.1332738846540451 \n",
      "P: Output= -1.509827101527863 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 1.1477099759124165 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 138 Training Loss: 0.12009298801422119 \n",
      "P: Output= -1.8236874837120238 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= -0.7536839981324839 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 139 Training Loss: 0.11342400312423706 \n",
      "P: Output= -1.4676842127591838 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 0.9715853256145888 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 140 Training Loss: 0.14657366275787354 \n",
      "P: Output= -1.4947837877212269 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 1.9702520338374478 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 141 Training Loss: 0.11932973563671112 \n",
      "P: Output= -1.4920340011219393 --- Target= 1.93652547331169e-07\n",
      "Q: Output= 1.407199706060502 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 142 Training Loss: 0.11686981469392776 \n",
      "P: Output= -1.395289230240726 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 2.1088823239266166 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 143 Training Loss: 0.11982902139425278 \n",
      "P: Output= -1.3870397866161612 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 3.905187002308524 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 144 Training Loss: 0.1292845606803894 \n",
      "P: Output= -1.486632648222578 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 2.0812229872217216 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 145 Training Loss: 0.11328628659248352 \n",
      "P: Output= -1.4913968642798858 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 3.4643814188513 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 146 Training Loss: 0.13056710362434387 \n",
      "P: Output= -1.853026775904798 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= 1.6256128767607052 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 147 Training Loss: 0.10650897026062012 \n",
      "P: Output= -1.6390582790133728 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 2.4802452160319657 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 148 Training Loss: 0.11983330547809601 \n",
      "P: Output= -1.7522080912277742 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 2.30276650864457 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 149 Training Loss: 0.13754534721374512 \n",
      "P: Output= -1.9489204094812278 --- Target= -3.730653368450021e-07\n",
      "Q: Output= 0.5205410064164067 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 150 Training Loss: 0.11092021316289902 \n",
      "P: Output= -1.7935955562354238 --- Target= -8.441947496606872e-08\n",
      "Q: Output= -0.3148872640075977 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 151 Training Loss: 0.11958476901054382 \n",
      "P: Output= -1.4479227859109551 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 0.9924882387002745 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 152 Training Loss: 0.14176887273788452 \n",
      "P: Output= -1.5128787824565242 --- Target= 1.885248828159547e-07\n",
      "Q: Output= -0.8667764887960434 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 153 Training Loss: 0.10757604241371155 \n",
      "P: Output= -1.5797084626221425 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= -0.6425367758922267 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 154 Training Loss: 0.1201111376285553 \n",
      "P: Output= -1.527358212451987 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= -0.17639095255954196 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 155 Training Loss: 0.12553490698337555 \n",
      "P: Output= -1.2588485051966325 --- Target= -9.703328984755899e-08\n",
      "Q: Output= 0.19661960506551157 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 156 Training Loss: 0.11509303748607635 \n",
      "P: Output= -1.100817254487084 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 2.7866991708559095 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 157 Training Loss: 0.11942782998085022 \n",
      "P: Output= -1.0892582228098444 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 2.4709392264019368 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 158 Training Loss: 0.12695150077342987 \n",
      "P: Output= -1.3264815138154278 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 2.415251956122945 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 159 Training Loss: 0.11816024780273438 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -1.522699714462716 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= 1.9714525800313893 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 0 Validation Loss: 0.11976881325244904 \n",
      "P: Output= -1.7896643785619366 --- Target= 2.911645777814442e-07\n",
      "Q: Output= 0.41956311935045676 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 1 Validation Loss: 0.11670978367328644 \n",
      "P: Output= -1.6835020416781905 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= 2.106679160254579 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 2 Validation Loss: 0.1152004599571228 \n",
      "P: Output= -1.5186678835142642 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= 1.9752343074274492 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 3 Validation Loss: 0.13058465719223022 \n",
      "P: Output= -1.441673425341948 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= 1.9540721669538472 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 4 Validation Loss: 0.1331149935722351 \n",
      "P: Output= -1.593789624194545 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= 0.05894806055638924 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 5 Validation Loss: 0.10818444192409515 \n",
      "P: Output= -1.7467985858408204 --- Target= 2.239196774667107e-07\n",
      "Q: Output= 0.2807848156787225 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 6 Validation Loss: 0.10873964428901672 \n",
      "P: Output= -1.744300422797652 --- Target= 3.239203305582805e-08\n",
      "Q: Output= 0.31154347974458396 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 7 Validation Loss: 0.11364033818244934 \n",
      "P: Output= -1.5655415497481844 --- Target= 7.48511617132408e-08\n",
      "Q: Output= 1.8191209356894733 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 8 Validation Loss: 0.1210242360830307 \n",
      "P: Output= -1.6879208968639725 --- Target= -2.219336545650208e-07\n",
      "Q: Output= 0.21111242313912815 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 9 Validation Loss: 0.10922552645206451 \n",
      "P: Output= -1.8250651715073793 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= 0.7000071454315195 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 10 Validation Loss: 0.10648475587368011 \n",
      "P: Output= -1.7201612933675605 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= 0.4278682645502858 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 11 Validation Loss: 0.10600831359624863 \n",
      "P: Output= -1.4310069743561113 --- Target= 1.3670019605172e-07\n",
      "Q: Output= 1.9089509798806041 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 12 Validation Loss: 0.11692370474338531 \n",
      "P: Output= -1.4681868927875312 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= 1.94920433570962 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 13 Validation Loss: 0.11622254550457001 \n",
      "P: Output= -1.5735962733140587 --- Target= 4.268641484728164e-07\n",
      "Q: Output= 0.4018818047104249 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 14 Validation Loss: 0.11100387573242188 \n",
      "P: Output= -1.454138049218776 --- Target= -1.907385298594022e-07\n",
      "Q: Output= 1.947776453288113 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 15 Validation Loss: 0.13156385719776154 \n",
      "P: Output= -1.4785320665411676 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= 1.693924458043754 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 16 Validation Loss: 0.12481558322906494 \n",
      "P: Output= -1.701434087262757 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= 0.4501279442187345 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 17 Validation Loss: 0.10630154609680176 \n",
      "P: Output= -1.510872788404912 --- Target= 2.077415652834702e-07\n",
      "Q: Output= 2.140378417834456 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 18 Validation Loss: 0.11613649129867554 \n",
      "P: Output= -1.587908992747952 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= 1.965202994448446 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 19 Validation Loss: 0.1287214756011963 \n",
      "P: Output= -1.7610092655378473 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= 0.36169149892660357 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 20 Validation Loss: 0.10858452320098877 \n",
      "P: Output= -1.5323014505811612 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= 1.9618829418376809 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 21 Validation Loss: 0.11415663361549377 \n",
      "P: Output= -1.5247620936719954 --- Target= -8.584417265922184e-08\n",
      "Q: Output= 1.9127881601349435 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 22 Validation Loss: 0.1300768256187439 \n",
      "P: Output= -1.5362843061910327 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= 2.018302200996067 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 23 Validation Loss: 0.12490728497505188 \n",
      "P: Output= -1.7080490323408624 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= 0.2132910205062526 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 24 Validation Loss: 0.11435892432928085 \n",
      "P: Output= -1.6233230778907375 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= 0.3872007672388236 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 25 Validation Loss: 0.11469034850597382 \n",
      "P: Output= -1.4270680663874682 --- Target= 5.541669878539324e-08\n",
      "Q: Output= 1.898685244500764 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 26 Validation Loss: 0.11787958443164825 \n",
      "P: Output= -1.6418016161476139 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= 1.9229984516370022 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 27 Validation Loss: 0.12556859850883484 \n",
      "P: Output= -1.4675989226158412 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= 1.716418129944624 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 28 Validation Loss: 0.11609400808811188 \n",
      "P: Output= -1.4158628129454058 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= 1.96469448507007 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 29 Validation Loss: 0.11771538853645325 \n",
      "Epoch: 10\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -1.7672186404099754 --- Target= 1.9959909547395682e-07\n",
      "Q: Output= 0.34201425562867893 --- Target= -2.0505563647077452e-07\n",
      "     Training Step: 0 Training Loss: 0.10976223647594452 \n",
      "P: Output= -1.7840742106619318 --- Target= 1.3196417203431565e-07\n",
      "Q: Output= 0.155682782911442 --- Target= -2.2551265210779547e-07\n",
      "     Training Step: 1 Training Loss: 0.11177918314933777 \n",
      "P: Output= -1.8915078616514345 --- Target= -2.0687553448084373e-07\n",
      "Q: Output= -0.35053584569794083 --- Target= -1.85270812025351e-08\n",
      "     Training Step: 2 Training Loss: 0.10501788556575775 \n",
      "P: Output= -1.6566363641530284 --- Target= 1.649411185411509e-08\n",
      "Q: Output= 0.9383220751252601 --- Target= 4.3303144181550124e-08\n",
      "     Training Step: 3 Training Loss: 0.1250048577785492 \n",
      "P: Output= -1.9253871889524525 --- Target= 2.3607773780298658e-07\n",
      "Q: Output= -1.0941790222429528 --- Target= -7.898302012421254e-08\n",
      "     Training Step: 4 Training Loss: 0.12212754040956497 \n",
      "P: Output= -1.8483450691921366 --- Target= -1.0260375571391478e-07\n",
      "Q: Output= -0.7329011671840533 --- Target= 1.957085187243024e-07\n",
      "     Training Step: 5 Training Loss: 0.11573381721973419 \n",
      "P: Output= -1.5156876791954907 --- Target= -1.2774725366426765e-07\n",
      "Q: Output= 1.6323052577626491 --- Target= 1.2115504066656513e-07\n",
      "     Training Step: 6 Training Loss: 0.1248166561126709 \n",
      "P: Output= -1.5463763832845903 --- Target= 9.40822530992591e-10\n",
      "Q: Output= 1.0079407973167802 --- Target= -8.185179556363664e-08\n",
      "     Training Step: 7 Training Loss: 0.10850797593593597 \n",
      "P: Output= -1.288568329931672 --- Target= 1.93652547331169e-07\n",
      "Q: Output= 1.5895890157060144 --- Target= 4.858608626534533e-09\n",
      "     Training Step: 8 Training Loss: 0.1162894070148468 \n",
      "P: Output= -1.1897296012831493 --- Target= 9.096384445683725e-08\n",
      "Q: Output= 3.7077195678441877 --- Target= 2.8698211096411796e-07\n",
      "     Training Step: 9 Training Loss: 0.1171920895576477 \n",
      "P: Output= -1.3209329622846404 --- Target= -4.072268247057309e-07\n",
      "Q: Output= 1.9265678702536366 --- Target= 1.554114303914389e-07\n",
      "     Training Step: 10 Training Loss: 0.11679527163505554 \n",
      "P: Output= -1.1837441896690173 --- Target= 1.6043721462466465e-07\n",
      "Q: Output= 3.7081226459458314 --- Target= -3.1551598667078906e-08\n",
      "     Training Step: 11 Training Loss: 0.13269853591918945 \n",
      "P: Output= -1.3416830983048067 --- Target= -5.844421391287824e-08\n",
      "Q: Output= 3.5608273667687236 --- Target= -2.160720935506788e-07\n",
      "     Training Step: 12 Training Loss: 0.11836665123701096 \n",
      "P: Output= -1.623513498044253 --- Target= 2.140872394917892e-07\n",
      "Q: Output= 3.144735531065119 --- Target= 1.525407071767404e-07\n",
      "     Training Step: 13 Training Loss: 0.11235421895980835 \n",
      "P: Output= -1.5870808013947366 --- Target= -6.0007305791032195e-09\n",
      "Q: Output= 2.6377608522727023 --- Target= 2.32136144973083e-07\n",
      "     Training Step: 14 Training Loss: 0.12226855009794235 \n",
      "P: Output= -1.9714388956904267 --- Target= -9.226324593214486e-08\n",
      "Q: Output= 0.5745481883338508 --- Target= -1.6367540656148094e-07\n",
      "     Training Step: 15 Training Loss: 0.1130789965391159 \n",
      "P: Output= -1.695059869766169 --- Target= 1.9598974265733204e-07\n",
      "Q: Output= 1.364310288994787 --- Target= -1.8141136859384233e-07\n",
      "     Training Step: 16 Training Loss: 0.12318318337202072 \n",
      "P: Output= -1.7620212193908378 --- Target= -2.0388128785242543e-07\n",
      "Q: Output= 1.3285194048008755 --- Target= -1.4795158431724076e-07\n",
      "     Training Step: 17 Training Loss: 0.11835134774446487 \n",
      "P: Output= -1.6814882649205805 --- Target= -3.103065893128587e-08\n",
      "Q: Output= 0.9915067751716391 --- Target= -1.7049592848650263e-08\n",
      "     Training Step: 18 Training Loss: 0.13567794859409332 \n",
      "P: Output= -1.5408460648761952 --- Target= 1.1391786447489949e-07\n",
      "Q: Output= 1.0028935054796868 --- Target= 5.240414679974492e-08\n",
      "     Training Step: 19 Training Loss: 0.13468961417675018 \n",
      "P: Output= -1.5965493320171698 --- Target= -1.2321683495741809e-07\n",
      "Q: Output= -0.9255960923570337 --- Target= 1.1125139653955785e-07\n",
      "     Training Step: 20 Training Loss: 0.10956946015357971 \n",
      "P: Output= -0.9745895494997567 --- Target= 2.0017560586893524e-07\n",
      "Q: Output= 1.2732697809083904 --- Target= 1.7132133578456887e-07\n",
      "     Training Step: 21 Training Loss: 0.12422235310077667 \n",
      "P: Output= -0.948309137609419 --- Target= -5.179597195592578e-09\n",
      "Q: Output= 1.7555556711060554 --- Target= 2.639692402439664e-07\n",
      "     Training Step: 22 Training Loss: 0.12044692039489746 \n",
      "P: Output= -0.8330062326453174 --- Target= 1.0042763687323486e-07\n",
      "Q: Output= 1.4453966841202188 --- Target= -2.2697150470207816e-07\n",
      "     Training Step: 23 Training Loss: 0.11538082361221313 \n",
      "P: Output= -0.7974373346724848 --- Target= -1.8903377352330608e-07\n",
      "Q: Output= 1.561112156810104 --- Target= -1.4328739972313542e-08\n",
      "     Training Step: 24 Training Loss: 0.1223912239074707 \n",
      "P: Output= -0.9890132089172701 --- Target= -3.0826035057884837e-09\n",
      "Q: Output= 1.7366653705524016 --- Target= -2.622571191324141e-07\n",
      "     Training Step: 25 Training Loss: 0.12734341621398926 \n",
      "P: Output= -1.192745842319347 --- Target= 8.365438475266274e-08\n",
      "Q: Output= 1.644956895148531 --- Target= 9.268469902679044e-08\n",
      "     Training Step: 26 Training Loss: 0.11917911469936371 \n",
      "P: Output= -1.3666305610687486 --- Target= -1.2503434199118146e-07\n",
      "Q: Output= 1.5462108455124204 --- Target= 9.859051619542925e-08\n",
      "     Training Step: 27 Training Loss: 0.11398302018642426 \n",
      "P: Output= -1.5612100194493852 --- Target= 3.1786598242433683e-08\n",
      "Q: Output= 1.5405970097188444 --- Target= -1.8279911540020066e-07\n",
      "     Training Step: 28 Training Loss: 0.11765904724597931 \n",
      "P: Output= -1.8641028040674437 --- Target= -3.2882650824461734e-07\n",
      "Q: Output= -0.35027834200166463 --- Target= 1.8922977140789499e-07\n",
      "     Training Step: 29 Training Loss: 0.11458499729633331 \n",
      "P: Output= -1.6977392677773588 --- Target= -1.3430537837422207e-07\n",
      "Q: Output= 1.2629563829555082 --- Target= -4.899229200105992e-08\n",
      "     Training Step: 30 Training Loss: 0.11527016013860703 \n",
      "P: Output= -1.7631793068195156 --- Target= 6.838634547534639e-09\n",
      "Q: Output= 1.6399299370142195 --- Target= -2.2156927759198197e-07\n",
      "     Training Step: 31 Training Loss: 0.11200302839279175 \n",
      "P: Output= -1.8251459321889696 --- Target= -2.0669487454938462e-07\n",
      "Q: Output= -0.09231335487123449 --- Target= -2.526114277756619e-07\n",
      "     Training Step: 32 Training Loss: 0.11171549558639526 \n",
      "P: Output= -1.6213458288610596 --- Target= -1.5149533005853755e-07\n",
      "Q: Output= 1.8179457129743817 --- Target= 9.25239334037542e-08\n",
      "     Training Step: 33 Training Loss: 0.13563303649425507 \n",
      "P: Output= -1.4374544273721321 --- Target= -4.617224291791899e-08\n",
      "Q: Output= 1.8301879968244004 --- Target= 2.430648136098057e-07\n",
      "     Training Step: 34 Training Loss: 0.14831432700157166 \n",
      "P: Output= -1.538444992224841 --- Target= 1.239808575803636e-07\n",
      "Q: Output= 0.5776051845418566 --- Target= -1.1685591427834652e-07\n",
      "     Training Step: 35 Training Loss: 0.11290857195854187 \n",
      "P: Output= -1.1474054811751682 --- Target= 1.3434094103814687e-07\n",
      "Q: Output= 2.48497541801472 --- Target= 3.515323943048543e-08\n",
      "     Training Step: 36 Training Loss: 0.1192341297864914 \n",
      "P: Output= -1.1313285308228735 --- Target= 7.188702522142876e-08\n",
      "Q: Output= 2.7085832302775383 --- Target= -7.979786786904697e-08\n",
      "     Training Step: 37 Training Loss: 0.13171017169952393 \n",
      "P: Output= -1.3749446843716466 --- Target= 9.865799821540122e-08\n",
      "Q: Output= 0.3723319084852754 --- Target= -2.008277482090648e-07\n",
      "     Training Step: 38 Training Loss: 0.10942213982343674 \n",
      "P: Output= -1.387939818641633 --- Target= 3.7221670989850963e-07\n",
      "Q: Output= -0.3697056515309205 --- Target= 1.2649384828478105e-07\n",
      "     Training Step: 39 Training Loss: 0.12376219034194946 \n",
      "P: Output= -1.6344028424163994 --- Target= -1.9260328887327205e-07\n",
      "Q: Output= -0.8955888838897321 --- Target= -2.2134605526247242e-07\n",
      "     Training Step: 40 Training Loss: 0.11242993175983429 \n",
      "P: Output= -1.7503722601099287 --- Target= -1.2817007455367957e-07\n",
      "Q: Output= -0.8929522487425752 --- Target= 1.353179772678459e-07\n",
      "     Training Step: 41 Training Loss: 0.11404937505722046 \n",
      "P: Output= -1.9748519362182648 --- Target= 3.9508186677750246e-07\n",
      "Q: Output= -1.357292343813186 --- Target= -1.781836616032706e-07\n",
      "     Training Step: 42 Training Loss: 0.11177299916744232 \n",
      "P: Output= -1.8867573094903856 --- Target= -1.917980831933619e-07\n",
      "Q: Output= 0.3070163794186831 --- Target= 8.82911361799188e-08\n",
      "     Training Step: 43 Training Loss: 0.13828115165233612 \n",
      "P: Output= -2.0402267631177997 --- Target= -2.7004509739469995e-08\n",
      "Q: Output= -1.3518458693909077 --- Target= -1.0651923521010076e-07\n",
      "     Training Step: 44 Training Loss: 0.10545296221971512 \n",
      "P: Output= -1.6418931437250448 --- Target= 1.5684695142681448e-07\n",
      "Q: Output= 0.05855040591202698 --- Target= 5.6571213491452e-08\n",
      "     Training Step: 45 Training Loss: 0.14576950669288635 \n",
      "P: Output= -1.722981696304707 --- Target= 1.897620762747465e-07\n",
      "Q: Output= 0.7618055830387256 --- Target= 6.123527995782752e-08\n",
      "     Training Step: 46 Training Loss: 0.12499552965164185 \n",
      "P: Output= -1.729315570595812 --- Target= -6.314223188752521e-08\n",
      "Q: Output= 1.3004181078108923 --- Target= -1.8445520222343248e-07\n",
      "     Training Step: 47 Training Loss: 0.12135772407054901 \n",
      "P: Output= -1.7558243987351396 --- Target= 1.7641259297818124e-07\n",
      "Q: Output= -0.56076532313421 --- Target= -1.4118087143799585e-08\n",
      "     Training Step: 48 Training Loss: 0.11335760354995728 \n",
      "P: Output= -1.6012048845573421 --- Target= 1.652745300617653e-07\n",
      "Q: Output= -0.44015003009300546 --- Target= 2.775136591637306e-07\n",
      "     Training Step: 49 Training Loss: 0.12425339221954346 \n",
      "P: Output= -1.0642482862903258 --- Target= -6.188908585613717e-08\n",
      "Q: Output= 1.6442972161688783 --- Target= -2.0366467889942896e-07\n",
      "     Training Step: 50 Training Loss: 0.12385769188404083 \n",
      "P: Output= -1.308808881088849 --- Target= -1.3720320701793298e-07\n",
      "Q: Output= 0.2487498578569829 --- Target= 1.939360281255631e-07\n",
      "     Training Step: 51 Training Loss: 0.113711416721344 \n",
      "P: Output= -1.2096304957535144 --- Target= -2.741996745214692e-07\n",
      "Q: Output= 0.6202424648901692 --- Target= -5.3869321448019036e-08\n",
      "     Training Step: 52 Training Loss: 0.1087120771408081 \n",
      "P: Output= -1.1186004079355625 --- Target= -1.9671541728882858e-08\n",
      "Q: Output= 2.6758789563777334 --- Target= 2.2100783958478587e-07\n",
      "     Training Step: 53 Training Loss: 0.11938609182834625 \n",
      "P: Output= -0.8050808771354658 --- Target= -1.490649044200154e-07\n",
      "Q: Output= 2.9661420248339 --- Target= -2.1526257754089784e-07\n",
      "     Training Step: 54 Training Loss: 0.13469725847244263 \n",
      "P: Output= -0.9152366764212925 --- Target= 1.7172532196241264e-07\n",
      "Q: Output= 3.386185791355791 --- Target= 1.398118252282643e-07\n",
      "     Training Step: 55 Training Loss: 0.1200072169303894 \n",
      "P: Output= -1.2620252852941638 --- Target= 9.257290134456753e-08\n",
      "Q: Output= 1.1054999745004652 --- Target= 4.880774184812253e-08\n",
      "     Training Step: 56 Training Loss: 0.106834277510643 \n",
      "P: Output= -1.2709341315410336 --- Target= 8.139931839679093e-08\n",
      "Q: Output= 2.8227617954877946 --- Target= 1.0578493547797052e-07\n",
      "     Training Step: 57 Training Loss: 0.12943021953105927 \n",
      "P: Output= -1.2229598069424368 --- Target= 1.2133339666320353e-07\n",
      "Q: Output= 2.696364596097766 --- Target= 1.5734847824688813e-07\n",
      "     Training Step: 58 Training Loss: 0.13519594073295593 \n",
      "P: Output= -1.4049426725936325 --- Target= -1.9746435153678021e-07\n",
      "Q: Output= 2.5836915004664656 --- Target= 1.4522734748112498e-07\n",
      "     Training Step: 59 Training Loss: 0.11596488952636719 \n",
      "P: Output= -1.5866645640587027 --- Target= -1.437938283999074e-07\n",
      "Q: Output= 3.058382368115778 --- Target= -2.3077114619241001e-07\n",
      "     Training Step: 60 Training Loss: 0.11318287253379822 \n",
      "P: Output= -1.7739445773966738 --- Target= 3.0595319699955326e-07\n",
      "Q: Output= 1.7649112165997822 --- Target= -2.2970945234845885e-07\n",
      "     Training Step: 61 Training Loss: 0.11725203692913055 \n",
      "P: Output= -1.7918520148814627 --- Target= 1.0417638929283157e-07\n",
      "Q: Output= 1.7185881261151996 --- Target= 2.73001037776055e-07\n",
      "     Training Step: 62 Training Loss: 0.11074522882699966 \n",
      "P: Output= -1.5624180915205823 --- Target= 1.6034182426238885e-07\n",
      "Q: Output= 3.3334780912244693 --- Target= -7.019280534592554e-08\n",
      "     Training Step: 63 Training Loss: 0.12315672636032104 \n",
      "P: Output= -1.4880018290738466 --- Target= 5.936255753624664e-08\n",
      "Q: Output= 2.9574803409812773 --- Target= -2.0724761817803028e-07\n",
      "     Training Step: 64 Training Loss: 0.12587499618530273 \n",
      "P: Output= -1.6595709007877986 --- Target= 8.498954429114747e-08\n",
      "Q: Output= 1.0219528195013199 --- Target= 1.3056566938729475e-07\n",
      "     Training Step: 65 Training Loss: 0.11216035485267639 \n",
      "P: Output= -1.4487682616644486 --- Target= -8.602393553047705e-08\n",
      "Q: Output= 2.552612897364037 --- Target= -1.8666663681443652e-07\n",
      "     Training Step: 66 Training Loss: 0.13022030889987946 \n",
      "P: Output= -1.3505160888402497 --- Target= 2.601515447508973e-08\n",
      "Q: Output= 1.7935986040862861 --- Target= -1.6964030358224136e-07\n",
      "     Training Step: 67 Training Loss: 0.14059233665466309 \n",
      "P: Output= -1.4737879497254003 --- Target= 6.39805808333449e-09\n",
      "Q: Output= -0.39815235827818896 --- Target= 1.1352544415643706e-07\n",
      "     Training Step: 68 Training Loss: 0.113905169069767 \n",
      "P: Output= -1.2920017760844784 --- Target= -9.128675593217395e-09\n",
      "Q: Output= -1.7022153110244478 --- Target= -6.766692983717348e-09\n",
      "     Training Step: 69 Training Loss: 0.11104056239128113 \n",
      "P: Output= -1.257030378778703 --- Target= -2.8006546326508897e-07\n",
      "Q: Output= -1.9806643136754927 --- Target= -3.4415042371449545e-08\n",
      "     Training Step: 70 Training Loss: 0.10856746137142181 \n",
      "P: Output= -1.1959984665118775 --- Target= -3.576887390721595e-07\n",
      "Q: Output= -2.0688192387878024 --- Target= 2.791940323376707e-07\n",
      "     Training Step: 71 Training Loss: 0.1120242029428482 \n",
      "P: Output= -0.7320719375495113 --- Target= 2.021405629548667e-07\n",
      "Q: Output= -0.021509829327203356 --- Target= 1.6152878767883294e-07\n",
      "     Training Step: 72 Training Loss: 0.1312747299671173 \n",
      "P: Output= -0.8436771540465253 --- Target= 2.011873458940272e-07\n",
      "Q: Output= 0.2291996344245062 --- Target= 5.54184644840916e-08\n",
      "     Training Step: 73 Training Loss: 0.12140439450740814 \n",
      "P: Output= -0.7808112533045852 --- Target= 8.009064700331692e-08\n",
      "Q: Output= 1.1467708976596862 --- Target= -6.505352168062473e-08\n",
      "     Training Step: 74 Training Loss: 0.12529325485229492 \n",
      "P: Output= -1.0683813009523524 --- Target= -3.15492234115311e-07\n",
      "Q: Output= 0.07726729152604594 --- Target= 2.3786578218221166e-07\n",
      "     Training Step: 75 Training Loss: 0.11603164672851562 \n",
      "P: Output= -1.293981661551376 --- Target= -3.3324536996559573e-07\n",
      "Q: Output= 1.3612159204168162 --- Target= 2.826765479468918e-07\n",
      "     Training Step: 76 Training Loss: 0.10817568004131317 \n",
      "P: Output= -1.1863770312729756 --- Target= 4.680602572193493e-09\n",
      "Q: Output= 3.9685697519915557 --- Target= -2.5737304998330046e-07\n",
      "     Training Step: 77 Training Loss: 0.12283594906330109 \n",
      "P: Output= -1.619834196484839 --- Target= 2.4975721579778565e-07\n",
      "Q: Output= 3.148130934714196 --- Target= -5.860388796463667e-08\n",
      "     Training Step: 78 Training Loss: 0.10699755698442459 \n",
      "P: Output= -1.3532994780343106 --- Target= 1.1271428501657965e-07\n",
      "Q: Output= 5.018474869613963 --- Target= 2.9488747443906504e-08\n",
      "     Training Step: 79 Training Loss: 0.14628858864307404 \n",
      "P: Output= -1.7713972825379107 --- Target= 3.9797827877663394e-07\n",
      "Q: Output= 4.25969153456292 --- Target= -1.5470696723696165e-08\n",
      "     Training Step: 80 Training Loss: 0.1127622127532959 \n",
      "P: Output= -1.6585513604254638 --- Target= -1.6945505088017399e-07\n",
      "Q: Output= 4.833464923841483 --- Target= -1.0409703321556663e-07\n",
      "     Training Step: 81 Training Loss: 0.1293182671070099 \n",
      "P: Output= -1.9275799114884826 --- Target= -1.9133547901617476e-07\n",
      "Q: Output= 4.4118321975226 --- Target= 1.1878202421655715e-08\n",
      "     Training Step: 82 Training Loss: 0.11160782724618912 \n",
      "P: Output= -1.9983674298394272 --- Target= -2.1005775963089945e-07\n",
      "Q: Output= 2.2759523844592078 --- Target= -2.3533423387789298e-07\n",
      "     Training Step: 83 Training Loss: 0.11143309623003006 \n",
      "P: Output= -2.0241725554538403 --- Target= -6.683027464760016e-08\n",
      "Q: Output= 1.549785977724647 --- Target= -1.8428631864964018e-07\n",
      "     Training Step: 84 Training Loss: 0.11845837533473969 \n",
      "P: Output= -1.8286153662941178 --- Target= -1.269564124939393e-07\n",
      "Q: Output= 0.17155136415090944 --- Target= -2.394256002347106e-07\n",
      "     Training Step: 85 Training Loss: 0.130270853638649 \n",
      "P: Output= -1.8948201284702382 --- Target= 3.4847553287420396e-07\n",
      "Q: Output= -1.2160633086547907 --- Target= 1.2254336212436101e-08\n",
      "     Training Step: 86 Training Loss: 0.11347577720880508 \n",
      "P: Output= -1.7113373773983875 --- Target= 1.885248828159547e-07\n",
      "Q: Output= -2.8724102643609095 --- Target= 2.0705972936241324e-07\n",
      "     Training Step: 87 Training Loss: 0.11934645473957062 \n",
      "P: Output= -1.3293370157534037 --- Target= 4.8572480260133943e-08\n",
      "Q: Output= -2.215928412169152 --- Target= -1.6697831295431342e-07\n",
      "     Training Step: 88 Training Loss: 0.11961139738559723 \n",
      "P: Output= -1.5692484666514588 --- Target= -2.7064323848691174e-07\n",
      "Q: Output= -3.9294701495320234 --- Target= -2.846272950662865e-07\n",
      "     Training Step: 89 Training Loss: 0.1452237218618393 \n",
      "P: Output= -1.5235490747299272 --- Target= 3.9437001131403804e-07\n",
      "Q: Output= -3.2849496290316833 --- Target= 2.505694487453525e-07\n",
      "     Training Step: 90 Training Loss: 0.10621906816959381 \n",
      "P: Output= -1.0642548800359268 --- Target= -1.5204389924861061e-07\n",
      "Q: Output= -0.3604363888472779 --- Target= -1.9688459396149938e-07\n",
      "     Training Step: 91 Training Loss: 0.11695874482393265 \n",
      "P: Output= -1.0637013522637409 --- Target= -9.425136404672685e-08\n",
      "Q: Output= 0.7644623394313426 --- Target= -9.000420408966647e-08\n",
      "     Training Step: 92 Training Loss: 0.11978738754987717 \n",
      "P: Output= -1.2991265738734725 --- Target= -1.618743441511583e-07\n",
      "Q: Output= 0.21751957846472258 --- Target= -2.0131386513355665e-07\n",
      "     Training Step: 93 Training Loss: 0.10508476197719574 \n",
      "P: Output= -1.0921919082801672 --- Target= -7.359242903959284e-08\n",
      "Q: Output= 2.1602453934840957 --- Target= 1.35686415880798e-07\n",
      "     Training Step: 94 Training Loss: 0.1158517599105835 \n",
      "P: Output= -1.3284881699796065 --- Target= -1.052064861895019e-07\n",
      "Q: Output= 2.468367481361496 --- Target= 2.4939903386922424e-07\n",
      "     Training Step: 95 Training Loss: 0.12351518869400024 \n",
      "P: Output= -1.4788463338328652 --- Target= 2.85940750188729e-07\n",
      "Q: Output= 0.7118222444487969 --- Target= -1.1620162432279812e-07\n",
      "     Training Step: 96 Training Loss: 0.11554738134145737 \n",
      "P: Output= -1.7365236863962705 --- Target= 2.8236644222801033e-07\n",
      "Q: Output= 1.2659240525776783 --- Target= 6.923178474949054e-08\n",
      "     Training Step: 97 Training Loss: 0.11020348966121674 \n",
      "P: Output= -1.699091994746813 --- Target= -3.921259228434337e-08\n",
      "Q: Output= 2.7267904577014814 --- Target= 2.895931370261451e-07\n",
      "     Training Step: 98 Training Loss: 0.11959671974182129 \n",
      "P: Output= -1.9400725113906123 --- Target= 1.586396098929299e-07\n",
      "Q: Output= 0.9309181642313442 --- Target= -2.2882196137885558e-07\n",
      "     Training Step: 99 Training Loss: 0.10624811798334122 \n",
      "P: Output= -2.0152826808753126 --- Target= 1.1845659564357902e-08\n",
      "Q: Output= 0.8343856774389655 --- Target= 3.064811426867209e-08\n",
      "     Training Step: 100 Training Loss: 0.11025308817625046 \n",
      "P: Output= -2.158150430923805 --- Target= 1.04987089244446e-07\n",
      "Q: Output= 0.975907758866386 --- Target= -1.4474719467472141e-07\n",
      "     Training Step: 101 Training Loss: 0.10384127497673035 \n",
      "P: Output= -1.932563467555699 --- Target= 9.813912260625557e-08\n",
      "Q: Output= 0.8405977001914966 --- Target= -1.2775576152534995e-08\n",
      "     Training Step: 102 Training Loss: 0.10575006157159805 \n",
      "P: Output= -1.6199807948244977 --- Target= 2.0769515085561352e-07\n",
      "Q: Output= 2.2221203869200377 --- Target= 3.667751169444955e-08\n",
      "     Training Step: 103 Training Loss: 0.11990360170602798 \n",
      "P: Output= -1.6356952705471635 --- Target= 1.335692196846594e-07\n",
      "Q: Output= 2.209119501612311 --- Target= 6.76467504234779e-08\n",
      "     Training Step: 104 Training Loss: 0.13035549223423004 \n",
      "P: Output= -1.3142809639097104 --- Target= 2.0109617882013708e-07\n",
      "Q: Output= 2.0546329518910555 --- Target= 2.4894947436848724e-07\n",
      "     Training Step: 105 Training Loss: 0.13646183907985687 \n",
      "P: Output= -1.5676202928166374 --- Target= 2.4539921295030354e-07\n",
      "Q: Output= 0.2841932481591041 --- Target= 1.1219656315120119e-07\n",
      "     Training Step: 106 Training Loss: 0.10782915353775024 \n",
      "P: Output= -1.2030014337266888 --- Target= 9.852414084576822e-08\n",
      "Q: Output= 1.6404191625968894 --- Target= 2.0127218824939064e-07\n",
      "     Training Step: 107 Training Loss: 0.11186419427394867 \n",
      "P: Output= -1.1758563407046472 --- Target= -7.718915107091107e-08\n",
      "Q: Output= 1.2160506220912506 --- Target= -1.7780709349324297e-07\n",
      "     Training Step: 108 Training Loss: 0.11568912863731384 \n",
      "P: Output= -1.430473511942977 --- Target= -6.546786224248535e-09\n",
      "Q: Output= -0.31154903552028834 --- Target= -2.6806962605263607e-07\n",
      "     Training Step: 109 Training Loss: 0.10939660668373108 \n",
      "P: Output= -1.5072106402651393 --- Target= 2.1751141954240438e-07\n",
      "Q: Output= -0.2953226748321631 --- Target= -7.359537601558941e-08\n",
      "     Training Step: 110 Training Loss: 0.12336748838424683 \n",
      "P: Output= -1.1124551433961951 --- Target= -6.944598229807752e-08\n",
      "Q: Output= 1.9361993121101326 --- Target= -1.385801686737409e-07\n",
      "     Training Step: 111 Training Loss: 0.12940916419029236 \n",
      "P: Output= -1.1279469444939183 --- Target= 1.72196169323513e-07\n",
      "Q: Output= 2.2568716899236847 --- Target= 1.6635111244056588e-07\n",
      "     Training Step: 112 Training Loss: 0.11589883267879486 \n",
      "P: Output= -1.3115215145451327 --- Target= -8.159233733096016e-08\n",
      "Q: Output= 1.0113469885687856 --- Target= -2.2176131064810534e-07\n",
      "     Training Step: 113 Training Loss: 0.10758441686630249 \n",
      "P: Output= -1.3792030217590154 --- Target= 1.8373910037894348e-07\n",
      "Q: Output= 1.661630464965815 --- Target= -7.410292290899179e-08\n",
      "     Training Step: 114 Training Loss: 0.12090951204299927 \n",
      "P: Output= -1.3126660809106712 --- Target= -1.4516211166437643e-07\n",
      "Q: Output= 2.1279097702618968 --- Target= -2.4019516153828135e-07\n",
      "     Training Step: 115 Training Loss: 0.12764590978622437 \n",
      "P: Output= -0.9825287860768288 --- Target= 1.6340485764487767e-07\n",
      "Q: Output= 3.4702122090170366 --- Target= 1.4550656679546137e-08\n",
      "     Training Step: 116 Training Loss: 0.1257428526878357 \n",
      "P: Output= -1.3516783286920502 --- Target= 9.913016452145484e-08\n",
      "Q: Output= 2.114739622373846 --- Target= -6.271125396750676e-08\n",
      "     Training Step: 117 Training Loss: 0.11646738648414612 \n",
      "P: Output= -1.145303826913139 --- Target= 1.1007516675931583e-08\n",
      "Q: Output= 3.8755581103081163 --- Target= 2.1663688709594453e-07\n",
      "     Training Step: 118 Training Loss: 0.12325743585824966 \n",
      "P: Output= -1.0730520877568637 --- Target= 3.967031148022215e-09\n",
      "Q: Output= 4.101208064933731 --- Target= -2.0165494607482515e-07\n",
      "     Training Step: 119 Training Loss: 0.12060633301734924 \n",
      "P: Output= -1.3005883872235104 --- Target= 2.3693102768618246e-07\n",
      "Q: Output= 2.7165057516647995 --- Target= -6.779994965455671e-08\n",
      "     Training Step: 120 Training Loss: 0.10773395001888275 \n",
      "P: Output= -1.2735821588652083 --- Target= 1.779068545815221e-07\n",
      "Q: Output= 4.714566112998488 --- Target= 1.478247764197249e-07\n",
      "     Training Step: 121 Training Loss: 0.12367172539234161 \n",
      "P: Output= -1.485750199236401 --- Target= 4.168930338721566e-07\n",
      "Q: Output= 3.4485709468812846 --- Target= 5.136099989755394e-08\n",
      "     Training Step: 122 Training Loss: 0.11498197913169861 \n",
      "P: Output= -1.7085374028332758 --- Target= -5.012515025271114e-08\n",
      "Q: Output= 3.279888323622769 --- Target= 2.854986025369044e-08\n",
      "     Training Step: 123 Training Loss: 0.11183483153581619 \n",
      "P: Output= -1.711299009948191 --- Target= -1.695987421612699e-07\n",
      "Q: Output= 2.856803017339745 --- Target= -2.765287883121914e-07\n",
      "     Training Step: 124 Training Loss: 0.1071445494890213 \n",
      "P: Output= -1.7305233603181565 --- Target= 1.1957043355437236e-07\n",
      "Q: Output= 2.2555847494602475 --- Target= -1.5736305947200435e-07\n",
      "     Training Step: 125 Training Loss: 0.10730159282684326 \n",
      "P: Output= -1.4316697757926375 --- Target= -1.9180810006957927e-07\n",
      "Q: Output= 3.584571468367745 --- Target= 2.84598824507043e-07\n",
      "     Training Step: 126 Training Loss: 0.13839098811149597 \n",
      "P: Output= -1.6116042807716928 --- Target= 9.79909344778207e-08\n",
      "Q: Output= 1.3052524332622344 --- Target= -6.17558795212858e-08\n",
      "     Training Step: 127 Training Loss: 0.113824263215065 \n",
      "P: Output= -1.5941601353228165 --- Target= 1.28001061305838e-07\n",
      "Q: Output= 2.3179504555164314 --- Target= 4.1893986946206496e-08\n",
      "     Training Step: 128 Training Loss: 0.13519451022148132 \n",
      "P: Output= -1.4030763950427128 --- Target= -1.40884607091607e-07\n",
      "Q: Output= 1.2451447521151051 --- Target= -2.0053192528735053e-07\n",
      "     Training Step: 129 Training Loss: 0.1187150776386261 \n",
      "P: Output= -1.5297556503134935 --- Target= -1.1836248514640602e-07\n",
      "Q: Output= 0.17621569540248316 --- Target= -1.4082841559570625e-08\n",
      "     Training Step: 130 Training Loss: 0.11642524600028992 \n",
      "P: Output= -1.4116169865718824 --- Target= -1.8051935235519068e-07\n",
      "Q: Output= -1.1356837552016437 --- Target= 1.388802992607907e-07\n",
      "     Training Step: 131 Training Loss: 0.13515357673168182 \n",
      "P: Output= -1.5985658523850166 --- Target= -9.703328984755899e-08\n",
      "Q: Output= -3.779126764458299 --- Target= 1.5876524095403965e-08\n",
      "     Training Step: 132 Training Loss: 0.11205354332923889 \n",
      "P: Output= -1.6506856041870908 --- Target= -3.1682707746227834e-07\n",
      "Q: Output= -4.024838856121825 --- Target= -1.4625804745094229e-07\n",
      "     Training Step: 133 Training Loss: 0.12884455919265747 \n",
      "P: Output= -1.791533500139952 --- Target= -3.1550403445379516e-07\n",
      "Q: Output= -3.6496884966839502 --- Target= -1.9005168994112864e-07\n",
      "     Training Step: 134 Training Loss: 0.1073494702577591 \n",
      "P: Output= -1.8027582889008622 --- Target= -3.730653368450021e-07\n",
      "Q: Output= -3.068588782380562 --- Target= -8.653813399206456e-08\n",
      "     Training Step: 135 Training Loss: 0.10426536202430725 \n",
      "P: Output= -1.471568534246681 --- Target= 6.896767335007326e-08\n",
      "Q: Output= -0.895955951022267 --- Target= -1.5412109366508275e-07\n",
      "     Training Step: 136 Training Loss: 0.12802216410636902 \n",
      "P: Output= -1.7981893752593079 --- Target= 2.191291796904693e-07\n",
      "Q: Output= -2.1479518045165045 --- Target= -5.284382709191959e-08\n",
      "     Training Step: 137 Training Loss: 0.1135890781879425 \n",
      "P: Output= -1.7359372903618677 --- Target= 3.465683571235445e-07\n",
      "Q: Output= -2.0395956208564425 --- Target= -2.3334681387154887e-07\n",
      "     Training Step: 138 Training Loss: 0.10682752728462219 \n",
      "P: Output= -1.8637113542428256 --- Target= 1.0822539486810001e-07\n",
      "Q: Output= -1.0646346939353126 --- Target= -1.6476269237841734e-09\n",
      "     Training Step: 139 Training Loss: 0.11370145529508591 \n",
      "P: Output= -1.752205382192126 --- Target= -8.441947496606872e-08\n",
      "Q: Output= -0.8044226855969843 --- Target= 9.759071772919015e-09\n",
      "     Training Step: 140 Training Loss: 0.11813610792160034 \n",
      "P: Output= -1.415837559662437 --- Target= 6.895684645513711e-08\n",
      "Q: Output= 1.328371743339865 --- Target= -3.219690736244729e-08\n",
      "     Training Step: 141 Training Loss: 0.127668559551239 \n",
      "P: Output= -1.6728812845311012 --- Target= -2.9549783064908297e-08\n",
      "Q: Output= 0.1912217077795848 --- Target= 6.014618847416386e-08\n",
      "     Training Step: 142 Training Loss: 0.11256217956542969 \n",
      "P: Output= -1.5416389046601582 --- Target= -1.4194001707323878e-07\n",
      "Q: Output= 1.83728198121719 --- Target= -2.5577347351202206e-08\n",
      "     Training Step: 143 Training Loss: 0.11505943536758423 \n",
      "P: Output= -1.660065636678862 --- Target= 1.7845506228297836e-07\n",
      "Q: Output= 2.0975826346337545 --- Target= -2.3674996185718555e-07\n",
      "     Training Step: 144 Training Loss: 0.1204453557729721 \n",
      "P: Output= -1.6584887492786686 --- Target= 5.560158822248695e-08\n",
      "Q: Output= 2.1881211230705677 --- Target= 6.327246282467058e-08\n",
      "     Training Step: 145 Training Loss: 0.12307535111904144 \n",
      "P: Output= -1.7609752577343984 --- Target= 1.606130251019522e-08\n",
      "Q: Output= 0.11192639730983434 --- Target= -2.661081417443256e-07\n",
      "     Training Step: 146 Training Loss: 0.10765090584754944 \n",
      "P: Output= -1.6024389685996407 --- Target= -6.915814321217795e-08\n",
      "Q: Output= 2.061640065114916 --- Target= -4.405360432002681e-08\n",
      "     Training Step: 147 Training Loss: 0.13290998339653015 \n",
      "P: Output= -1.5058929844580886 --- Target= 8.384345484557798e-08\n",
      "Q: Output= 2.165109157820634 --- Target= 5.0259417960774044e-08\n",
      "     Training Step: 148 Training Loss: 0.12303688377141953 \n",
      "P: Output= -1.66006075640637 --- Target= 1.9951933261097565e-07\n",
      "Q: Output= 2.5781173060414897 --- Target= 1.3944378185470896e-07\n",
      "     Training Step: 149 Training Loss: 0.12510249018669128 \n",
      "P: Output= -1.7987702946575368 --- Target= 9.307350889997679e-08\n",
      "Q: Output= 3.2035109821802212 --- Target= -1.8867560935831307e-07\n",
      "     Training Step: 150 Training Loss: 0.11834098398685455 \n",
      "P: Output= -1.488657993433641 --- Target= 1.7009500474785e-07\n",
      "Q: Output= 3.8290263893400382 --- Target= 2.3806379889634854e-07\n",
      "     Training Step: 151 Training Loss: 0.12149443477392197 \n",
      "P: Output= -1.5729214814106385 --- Target= 1.726564144988174e-07\n",
      "Q: Output= 2.398440740869632 --- Target= -4.714744150646766e-10\n",
      "     Training Step: 152 Training Loss: 0.114469513297081 \n",
      "P: Output= -1.524151747014801 --- Target= 7.829150838745136e-08\n",
      "Q: Output= 4.352312301984153 --- Target= -1.2868554222222883e-07\n",
      "     Training Step: 153 Training Loss: 0.11171790957450867 \n",
      "P: Output= -1.3770475009326093 --- Target= -1.6889706166978158e-07\n",
      "Q: Output= 4.594272289953935 --- Target= 2.0500012887225694e-07\n",
      "     Training Step: 154 Training Loss: 0.12657706439495087 \n",
      "P: Output= -1.438505703988361 --- Target= -2.3633682033619152e-07\n",
      "Q: Output= 3.0072629673591056 --- Target= 9.600702277623441e-08\n",
      "     Training Step: 155 Training Loss: 0.10934725403785706 \n",
      "P: Output= -1.5832070633652027 --- Target= -3.0343061574455987e-07\n",
      "Q: Output= 2.3212752586089618 --- Target= -2.802038574500898e-07\n",
      "     Training Step: 156 Training Loss: 0.1000453531742096 \n",
      "P: Output= -1.4039055438340018 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 3.0370012252608216 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 157 Training Loss: 0.12776049971580505 \n",
      "P: Output= -1.3269175007020184 --- Target= 2.09855794963687e-07\n",
      "Q: Output= 2.4719329172774023 --- Target= -7.849494387812683e-09\n",
      "     Training Step: 158 Training Loss: 0.12094278633594513 \n",
      "P: Output= -1.3305275620762433 --- Target= 1.0702619945845981e-07\n",
      "Q: Output= 1.1494662254210608 --- Target= 1.69412527561974e-07\n",
      "     Training Step: 159 Training Loss: 0.12431248277425766 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "P: Output= -1.3334862367553777 --- Target= -8.584417265922184e-08\n",
      "Q: Output= 0.4048896157936266 --- Target= -1.0640909042791691e-07\n",
      "     Validation Step: 0 Validation Loss: 0.12311779707670212 \n",
      "P: Output= -1.2409841240515211 --- Target= 5.541669878539324e-08\n",
      "Q: Output= 0.3656136620658499 --- Target= -1.237090909711469e-07\n",
      "     Validation Step: 1 Validation Loss: 0.11018157005310059 \n",
      "P: Output= -1.6032351985316557 --- Target= -2.8967986942518564e-07\n",
      "Q: Output= -1.2899775578858987 --- Target= -2.2856965031792242e-07\n",
      "     Validation Step: 2 Validation Loss: 0.10300341993570328 \n",
      "P: Output= -1.2580918428876062 --- Target= -4.6586865920517084e-08\n",
      "Q: Output= 0.42448263920135343 --- Target= 1.5306391887293103e-07\n",
      "     Validation Step: 3 Validation Loss: 0.12351220846176147 \n",
      "P: Output= -1.264936193859029 --- Target= -1.907385298594022e-07\n",
      "Q: Output= 0.4264507224610252 --- Target= 1.7726091883218942e-07\n",
      "     Validation Step: 4 Validation Loss: 0.1254698634147644 \n",
      "P: Output= -1.497050090105775 --- Target= -1.6173180128475906e-08\n",
      "Q: Output= 0.6424834051978667 --- Target= 2.4296335254803125e-07\n",
      "     Validation Step: 5 Validation Loss: 0.10937049984931946 \n",
      "P: Output= -1.671510012994978 --- Target= 1.1838081448445337e-07\n",
      "Q: Output= -0.8991959202945514 --- Target= 2.47884773152407e-07\n",
      "     Validation Step: 6 Validation Loss: 0.10040365159511566 \n",
      "P: Output= -1.3402381042440856 --- Target= -4.6246403151428694e-08\n",
      "Q: Output= 0.5098209260516882 --- Target= -1.9352377567116719e-07\n",
      "     Validation Step: 7 Validation Loss: 0.11506883054971695 \n",
      "P: Output= -1.2754963065533946 --- Target= 3.1024191216033614e-08\n",
      "Q: Output= 0.41839468638820154 --- Target= 2.564854648667847e-07\n",
      "     Validation Step: 8 Validation Loss: 0.11490678787231445 \n",
      "P: Output= -1.325351421878561 --- Target= -4.7754620702278316e-08\n",
      "Q: Output= 0.4802557345171188 --- Target= -2.550339424089998e-07\n",
      "     Validation Step: 9 Validation Loss: 0.13110394775867462 \n",
      "P: Output= -1.2813528836687613 --- Target= 1.4730581465727255e-07\n",
      "Q: Output= 0.18288353401749458 --- Target= -3.1092168839563783e-09\n",
      "     Validation Step: 10 Validation Loss: 0.11381764709949493 \n",
      "P: Output= -1.5504315232544963 --- Target= -3.8843234761998247e-07\n",
      "Q: Output= -1.190814872679523 --- Target= -1.4123967240209367e-07\n",
      "     Validation Step: 11 Validation Loss: 0.10409557074308395 \n",
      "P: Output= -1.4067293013171343 --- Target= 1.3524267039599636e-08\n",
      "Q: Output= 0.46608012528096765 --- Target= -1.5636400441110254e-07\n",
      "     Validation Step: 12 Validation Loss: 0.12486723065376282 \n",
      "P: Output= -1.2404999220587305 --- Target= 1.3670019605172e-07\n",
      "Q: Output= 0.3709417107743347 --- Target= -9.722997607042316e-08\n",
      "     Validation Step: 13 Validation Loss: 0.11244365572929382 \n",
      "P: Output= -1.4218733207083503 --- Target= 4.268641484728164e-07\n",
      "Q: Output= -1.2558545161603085 --- Target= 2.1356932311533683e-07\n",
      "     Validation Step: 14 Validation Loss: 0.1100359708070755 \n",
      "P: Output= -1.60381924550546 --- Target= 3.239203305582805e-08\n",
      "Q: Output= -1.3392656714779614 --- Target= 1.8079943675530785e-07\n",
      "     Validation Step: 15 Validation Loss: 0.10895092785358429 \n",
      "P: Output= -1.3352764625862772 --- Target= -1.3653431096827262e-07\n",
      "Q: Output= 0.46389550054720186 --- Target= 1.26096413310961e-07\n",
      "     Validation Step: 16 Validation Loss: 0.1161651760339737 \n",
      "P: Output= -1.4765594663416186 --- Target= 2.9464328488160163e-07\n",
      "Q: Output= -1.273626607976266 --- Target= 5.5005182630907257e-08\n",
      "     Validation Step: 17 Validation Loss: 0.1066480502486229 \n",
      "P: Output= -1.6354296443066918 --- Target= 2.911645777814442e-07\n",
      "Q: Output= -1.2016999639879717 --- Target= -1.9078715940423763e-07\n",
      "     Validation Step: 18 Validation Loss: 0.11048667132854462 \n",
      "P: Output= -1.4623099969445361 --- Target= -1.4939428716331804e-07\n",
      "Q: Output= 0.4441592855486469 --- Target= 6.743340996706593e-09\n",
      "     Validation Step: 19 Validation Loss: 0.11887441575527191 \n",
      "P: Output= -1.3232922163290004 --- Target= 2.077415652834702e-07\n",
      "Q: Output= 0.6455215903897358 --- Target= 4.188399493898487e-08\n",
      "     Validation Step: 20 Validation Loss: 0.11721092462539673 \n",
      "P: Output= -1.452674134425866 --- Target= 4.0468087281197995e-07\n",
      "Q: Output= -1.6260169500254054 --- Target= -8.191393341405728e-09\n",
      "     Validation Step: 21 Validation Loss: 0.10731223225593567 \n",
      "P: Output= -1.5655167612083591 --- Target= 3.8293074400286287e-07\n",
      "Q: Output= -1.2225165973948915 --- Target= 5.552556636700956e-08\n",
      "     Validation Step: 22 Validation Loss: 0.1060580387711525 \n",
      "P: Output= -1.3507772043240243 --- Target= 1.3920718000548504e-07\n",
      "Q: Output= 0.44322511753567895 --- Target= 1.0113801796052257e-07\n",
      "     Validation Step: 23 Validation Loss: 0.10911835730075836 \n",
      "P: Output= -1.5749000899698071 --- Target= -4.6241368956145834e-08\n",
      "Q: Output= -1.4347232291129703 --- Target= -2.0840936976185276e-08\n",
      "     Validation Step: 24 Validation Loss: 0.11764256656169891 \n",
      "P: Output= -1.2265808693645317 --- Target= 1.9761130687356854e-07\n",
      "Q: Output= 0.43172785943402037 --- Target= -2.0003887346575766e-07\n",
      "     Validation Step: 25 Validation Loss: 0.11329974234104156 \n",
      "P: Output= -1.2967922582154872 --- Target= 1.4481634469376559e-07\n",
      "Q: Output= 0.14814833979815933 --- Target= -1.5063534064552186e-07\n",
      "     Validation Step: 26 Validation Loss: 0.12124918401241302 \n",
      "P: Output= -1.5392536305425022 --- Target= -2.219336545650208e-07\n",
      "Q: Output= -1.460300425515868 --- Target= 5.520056234331605e-08\n",
      "     Validation Step: 27 Validation Loss: 0.10281387716531754 \n",
      "P: Output= -1.384367287067593 --- Target= 7.48511617132408e-08\n",
      "Q: Output= 0.30960257981223194 --- Target= 1.6603029884265652e-07\n",
      "     Validation Step: 28 Validation Loss: 0.11855857074260712 \n",
      "P: Output= -1.5949711482188835 --- Target= 2.239196774667107e-07\n",
      "Q: Output= -1.3634671638515394 --- Target= -2.0015967905351317e-07\n",
      "     Validation Step: 29 Validation Loss: 0.10704095661640167 \n",
      "Epoch: 11\n",
      "###########################################\n",
      "   TRAINING\n",
      "P: Output= -1.3771441755563503 --- Target= 1.1015588885499028e-07\n",
      "Q: Output= 0.32997626460958074 --- Target= 9.050370408658637e-08\n",
      "     Training Step: 0 Training Loss: 0.12384678423404694 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_10228\\3796889569.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mnum_epochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m30\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mloss_weights\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mtrain_input\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mval_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_validate_ACOPF_chained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedder_model\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mACOPF_optimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mtrain_validate_ACOPF_chained\u001B[1;34m(minimizer_model, enforcer_model, embedder_model, ACOPF_optimizer, train_inputs, val_inputs, loss_weights, start_epoch, num_epochs, return_outputs, save_model)\u001B[0m\n\u001B[0;32m   2497\u001B[0m             \u001B[0mout_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0membedder_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconstraint_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0medge_idx_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0medge_attr_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2498\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2499\u001B[1;33m             \u001B[0mphysics_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2500\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2501\u001B[0m             \u001B[1;31m# Forward pass with enforcer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mself_supervised_embedder_obj_fn\u001B[1;34m(out_dict, powers_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   2182\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2183\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpowers_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2184\u001B[1;33m     \u001B[0mphysics_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_physics_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpowers_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2185\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2186\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mphysics_loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mcalc_physics_loss\u001B[1;34m(x_dict, out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   3070\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3071\u001B[0m                 \u001B[1;31m# ACOPF Equation for P_i\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3072\u001B[1;33m                 \u001B[0mP_ij\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1.0\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_i\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_j\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mG_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mB_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3073\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3074\u001B[0m                 \u001B[0mP\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mformat_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mformat_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mextract_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    198\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m     \u001B[0mstack\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mStackSummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwalk_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m     \u001B[0mstack\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreverse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mstack\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    357\u001B[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001B[0;32m    358\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mfilename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfnames\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 359\u001B[1;33m             \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckcache\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    360\u001B[0m         \u001B[1;31m# If immediate lookup was desired, trigger lookups now.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlookup_lines\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\IPython\\core\\compilerop.py\u001B[0m in \u001B[0;36mcheck_linecache_ipython\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \"\"\"\n\u001B[0;32m    184\u001B[0m     \u001B[1;31m# First call the original checkcache as intended\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 185\u001B[1;33m     \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_checkcache_ori\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    186\u001B[0m     \u001B[1;31m# Then, update back the cache with our data, so that tracebacks related\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m     \u001B[1;31m# to our compiled codes can be produced.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\linecache.py\u001B[0m in \u001B[0;36mcheckcache\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mcontinue\u001B[0m   \u001B[1;31m# no-op for files loaded via a __loader__\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m             \u001B[0mstat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfullname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mcache\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 1\n",
    "num_epochs = 30\n",
    "loss_weights = (0.0, 1.0, 0.0)\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF_chained(None,None, embedder_model,ACOPF_optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 6\n",
    "num_epochs = 30\n",
    "loss_weights = (1.0, 1.0, 1.0)\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF_chained(minimizer_model,enforcer_model, embedder_model,ACOPF_optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(train_output.output[:, 2]), sum(train_output.output[:, 3]))\n",
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(val_output.output[:, 2]), sum(val_output.output[:, 3]))\n",
    "val_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
