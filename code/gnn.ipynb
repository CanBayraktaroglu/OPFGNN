{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from helper import *\n",
    "from heterognn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The State of the nth node is expressed by 4 real scalars:\n",
    "\n",
    "v_n -> the voltage at the node\n",
    "delta_n -> the voltage angle at the node (relative to the slack bus)\n",
    "p_n -> the net active power flowing into the node\n",
    "q_n -> the net reactive power flowing into the node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical characteristics of the network are described by the power flow equations:\n",
    "\n",
    "p = P(v, delta, W)\n",
    "q = Q(v, delta, W)\n",
    "\n",
    "-> Relate local net power generation with the global state\n",
    "-> Depends on the topology W of the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electrical grid => Weighted Graph\n",
    "\n",
    "Nodes in the graph produce/consume power\n",
    "\n",
    "Edges represent electrical connections between nodes\n",
    "\n",
    "State Matrix X element of  R(N x 4) => graph signal with 4 features\n",
    "    => Each row is the state of the corresponding Node\n",
    "\n",
    "Adjacency Matrix A => sparse matrix to represent the connections of each node, element of R(N x N), Aij = 1 if node i is connected to node j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the GNN as a mode phi(X, A, H)\n",
    "\n",
    "We want to imitate the OPF solution p*\n",
    "-> We want to minimize a loss L over a dataset T = {{X, p*}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Function:min arg H of sum over T L(p*,phi(X, A, H)) and we use L = Mean Squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once GNN model phi is trained, we do not need the costly p* from pandapower to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input Data X - R^(Nx4): Uniformly sample p_ref and q_ref of each load L with P_L ~ Uniform(0.9 * p_ref, 1.1 * p_ref) and Q_L ~ Uniform(0.9 * q_ref, 1.1 * q_ref)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pseudocode for X and y in supervised learning:\n",
    "for each P_L and Q_L:\n",
    "    Create X with sub-optimal DCOPF results\n",
    "    Create y with Pandapower calculating p* ACOPF with IPOPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatio-Temporal GNN -> superposition of a gnn with spatial info and a temporal layer (Temporal Conv,LSTM etc.) ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bus => Node in GNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['1-complete_data-mixed-all-0-sw',\n '1-complete_data-mixed-all-1-sw',\n '1-complete_data-mixed-all-2-sw',\n '1-EHVHVMVLV-mixed-all-0-sw',\n '1-EHVHVMVLV-mixed-all-1-sw',\n '1-EHVHVMVLV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-0-sw',\n '1-EHVHV-mixed-all-0-no_sw',\n '1-EHVHV-mixed-all-1-sw',\n '1-EHVHV-mixed-all-1-no_sw',\n '1-EHVHV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-2-no_sw',\n '1-EHVHV-mixed-1-0-sw',\n '1-EHVHV-mixed-1-0-no_sw',\n '1-EHVHV-mixed-1-1-sw',\n '1-EHVHV-mixed-1-1-no_sw',\n '1-EHVHV-mixed-1-2-sw',\n '1-EHVHV-mixed-1-2-no_sw',\n '1-EHVHV-mixed-2-0-sw',\n '1-EHVHV-mixed-2-0-no_sw',\n '1-EHVHV-mixed-2-1-sw',\n '1-EHVHV-mixed-2-1-no_sw',\n '1-EHVHV-mixed-2-2-sw',\n '1-EHVHV-mixed-2-2-no_sw',\n '1-EHV-mixed--0-sw',\n '1-EHV-mixed--0-no_sw',\n '1-EHV-mixed--1-sw',\n '1-EHV-mixed--1-no_sw',\n '1-EHV-mixed--2-sw',\n '1-EHV-mixed--2-no_sw',\n '1-HVMV-mixed-all-0-sw',\n '1-HVMV-mixed-all-0-no_sw',\n '1-HVMV-mixed-all-1-sw',\n '1-HVMV-mixed-all-1-no_sw',\n '1-HVMV-mixed-all-2-sw',\n '1-HVMV-mixed-all-2-no_sw',\n '1-HVMV-mixed-1.105-0-sw',\n '1-HVMV-mixed-1.105-0-no_sw',\n '1-HVMV-mixed-1.105-1-sw',\n '1-HVMV-mixed-1.105-1-no_sw',\n '1-HVMV-mixed-1.105-2-sw',\n '1-HVMV-mixed-1.105-2-no_sw',\n '1-HVMV-mixed-2.102-0-sw',\n '1-HVMV-mixed-2.102-0-no_sw',\n '1-HVMV-mixed-2.102-1-sw',\n '1-HVMV-mixed-2.102-1-no_sw',\n '1-HVMV-mixed-2.102-2-sw',\n '1-HVMV-mixed-2.102-2-no_sw',\n '1-HVMV-mixed-4.101-0-sw',\n '1-HVMV-mixed-4.101-0-no_sw',\n '1-HVMV-mixed-4.101-1-sw',\n '1-HVMV-mixed-4.101-1-no_sw',\n '1-HVMV-mixed-4.101-2-sw',\n '1-HVMV-mixed-4.101-2-no_sw',\n '1-HVMV-urban-all-0-sw',\n '1-HVMV-urban-all-0-no_sw',\n '1-HVMV-urban-all-1-sw',\n '1-HVMV-urban-all-1-no_sw',\n '1-HVMV-urban-all-2-sw',\n '1-HVMV-urban-all-2-no_sw',\n '1-HVMV-urban-2.203-0-sw',\n '1-HVMV-urban-2.203-0-no_sw',\n '1-HVMV-urban-2.203-1-sw',\n '1-HVMV-urban-2.203-1-no_sw',\n '1-HVMV-urban-2.203-2-sw',\n '1-HVMV-urban-2.203-2-no_sw',\n '1-HVMV-urban-3.201-0-sw',\n '1-HVMV-urban-3.201-0-no_sw',\n '1-HVMV-urban-3.201-1-sw',\n '1-HVMV-urban-3.201-1-no_sw',\n '1-HVMV-urban-3.201-2-sw',\n '1-HVMV-urban-3.201-2-no_sw',\n '1-HVMV-urban-4.201-0-sw',\n '1-HVMV-urban-4.201-0-no_sw',\n '1-HVMV-urban-4.201-1-sw',\n '1-HVMV-urban-4.201-1-no_sw',\n '1-HVMV-urban-4.201-2-sw',\n '1-HVMV-urban-4.201-2-no_sw',\n '1-HV-mixed--0-sw',\n '1-HV-mixed--0-no_sw',\n '1-HV-mixed--1-sw',\n '1-HV-mixed--1-no_sw',\n '1-HV-mixed--2-sw',\n '1-HV-mixed--2-no_sw',\n '1-HV-urban--0-sw',\n '1-HV-urban--0-no_sw',\n '1-HV-urban--1-sw',\n '1-HV-urban--1-no_sw',\n '1-HV-urban--2-sw',\n '1-HV-urban--2-no_sw',\n '1-MVLV-rural-all-0-sw',\n '1-MVLV-rural-all-0-no_sw',\n '1-MVLV-rural-all-1-sw',\n '1-MVLV-rural-all-1-no_sw',\n '1-MVLV-rural-all-2-sw',\n '1-MVLV-rural-all-2-no_sw',\n '1-MVLV-rural-1.108-0-sw',\n '1-MVLV-rural-1.108-0-no_sw',\n '1-MVLV-rural-1.108-1-sw',\n '1-MVLV-rural-1.108-1-no_sw',\n '1-MVLV-rural-1.108-2-sw',\n '1-MVLV-rural-1.108-2-no_sw',\n '1-MVLV-rural-2.107-0-sw',\n '1-MVLV-rural-2.107-0-no_sw',\n '1-MVLV-rural-2.107-1-sw',\n '1-MVLV-rural-2.107-1-no_sw',\n '1-MVLV-rural-2.107-2-sw',\n '1-MVLV-rural-2.107-2-no_sw',\n '1-MVLV-rural-4.101-0-sw',\n '1-MVLV-rural-4.101-0-no_sw',\n '1-MVLV-rural-4.101-1-sw',\n '1-MVLV-rural-4.101-1-no_sw',\n '1-MVLV-rural-4.101-2-sw',\n '1-MVLV-rural-4.101-2-no_sw',\n '1-MVLV-semiurb-all-0-sw',\n '1-MVLV-semiurb-all-0-no_sw',\n '1-MVLV-semiurb-all-1-sw',\n '1-MVLV-semiurb-all-1-no_sw',\n '1-MVLV-semiurb-all-2-sw',\n '1-MVLV-semiurb-all-2-no_sw',\n '1-MVLV-semiurb-3.202-0-sw',\n '1-MVLV-semiurb-3.202-0-no_sw',\n '1-MVLV-semiurb-3.202-1-sw',\n '1-MVLV-semiurb-3.202-1-no_sw',\n '1-MVLV-semiurb-3.202-2-sw',\n '1-MVLV-semiurb-3.202-2-no_sw',\n '1-MVLV-semiurb-4.201-0-sw',\n '1-MVLV-semiurb-4.201-0-no_sw',\n '1-MVLV-semiurb-4.201-1-sw',\n '1-MVLV-semiurb-4.201-1-no_sw',\n '1-MVLV-semiurb-4.201-2-sw',\n '1-MVLV-semiurb-4.201-2-no_sw',\n '1-MVLV-semiurb-5.220-0-sw',\n '1-MVLV-semiurb-5.220-0-no_sw',\n '1-MVLV-semiurb-5.220-1-sw',\n '1-MVLV-semiurb-5.220-1-no_sw',\n '1-MVLV-semiurb-5.220-2-sw',\n '1-MVLV-semiurb-5.220-2-no_sw',\n '1-MVLV-urban-all-0-sw',\n '1-MVLV-urban-all-0-no_sw',\n '1-MVLV-urban-all-1-sw',\n '1-MVLV-urban-all-1-no_sw',\n '1-MVLV-urban-all-2-sw',\n '1-MVLV-urban-all-2-no_sw',\n '1-MVLV-urban-5.303-0-sw',\n '1-MVLV-urban-5.303-0-no_sw',\n '1-MVLV-urban-5.303-1-sw',\n '1-MVLV-urban-5.303-1-no_sw',\n '1-MVLV-urban-5.303-2-sw',\n '1-MVLV-urban-5.303-2-no_sw',\n '1-MVLV-urban-6.305-0-sw',\n '1-MVLV-urban-6.305-0-no_sw',\n '1-MVLV-urban-6.305-1-sw',\n '1-MVLV-urban-6.305-1-no_sw',\n '1-MVLV-urban-6.305-2-sw',\n '1-MVLV-urban-6.305-2-no_sw',\n '1-MVLV-urban-6.309-0-sw',\n '1-MVLV-urban-6.309-0-no_sw',\n '1-MVLV-urban-6.309-1-sw',\n '1-MVLV-urban-6.309-1-no_sw',\n '1-MVLV-urban-6.309-2-sw',\n '1-MVLV-urban-6.309-2-no_sw',\n '1-MVLV-comm-all-0-sw',\n '1-MVLV-comm-all-0-no_sw',\n '1-MVLV-comm-all-1-sw',\n '1-MVLV-comm-all-1-no_sw',\n '1-MVLV-comm-all-2-sw',\n '1-MVLV-comm-all-2-no_sw',\n '1-MVLV-comm-3.403-0-sw',\n '1-MVLV-comm-3.403-0-no_sw',\n '1-MVLV-comm-3.403-1-sw',\n '1-MVLV-comm-3.403-1-no_sw',\n '1-MVLV-comm-3.403-2-sw',\n '1-MVLV-comm-3.403-2-no_sw',\n '1-MVLV-comm-4.416-0-sw',\n '1-MVLV-comm-4.416-0-no_sw',\n '1-MVLV-comm-4.416-1-sw',\n '1-MVLV-comm-4.416-1-no_sw',\n '1-MVLV-comm-4.416-2-sw',\n '1-MVLV-comm-4.416-2-no_sw',\n '1-MVLV-comm-5.401-0-sw',\n '1-MVLV-comm-5.401-0-no_sw',\n '1-MVLV-comm-5.401-1-sw',\n '1-MVLV-comm-5.401-1-no_sw',\n '1-MVLV-comm-5.401-2-sw',\n '1-MVLV-comm-5.401-2-no_sw',\n '1-MV-rural--0-sw',\n '1-MV-rural--0-no_sw',\n '1-MV-rural--1-sw',\n '1-MV-rural--1-no_sw',\n '1-MV-rural--2-sw',\n '1-MV-rural--2-no_sw',\n '1-MV-semiurb--0-sw',\n '1-MV-semiurb--0-no_sw',\n '1-MV-semiurb--1-sw',\n '1-MV-semiurb--1-no_sw',\n '1-MV-semiurb--2-sw',\n '1-MV-semiurb--2-no_sw',\n '1-MV-urban--0-sw',\n '1-MV-urban--0-no_sw',\n '1-MV-urban--1-sw',\n '1-MV-urban--1-no_sw',\n '1-MV-urban--2-sw',\n '1-MV-urban--2-no_sw',\n '1-MV-comm--0-sw',\n '1-MV-comm--0-no_sw',\n '1-MV-comm--1-sw',\n '1-MV-comm--1-no_sw',\n '1-MV-comm--2-sw',\n '1-MV-comm--2-no_sw',\n '1-LV-rural1--0-sw',\n '1-LV-rural1--0-no_sw',\n '1-LV-rural1--1-sw',\n '1-LV-rural1--1-no_sw',\n '1-LV-rural1--2-sw',\n '1-LV-rural1--2-no_sw',\n '1-LV-rural2--0-sw',\n '1-LV-rural2--0-no_sw',\n '1-LV-rural2--1-sw',\n '1-LV-rural2--1-no_sw',\n '1-LV-rural2--2-sw',\n '1-LV-rural2--2-no_sw',\n '1-LV-rural3--0-sw',\n '1-LV-rural3--0-no_sw',\n '1-LV-rural3--1-sw',\n '1-LV-rural3--1-no_sw',\n '1-LV-rural3--2-sw',\n '1-LV-rural3--2-no_sw',\n '1-LV-semiurb4--0-sw',\n '1-LV-semiurb4--0-no_sw',\n '1-LV-semiurb4--1-sw',\n '1-LV-semiurb4--1-no_sw',\n '1-LV-semiurb4--2-sw',\n '1-LV-semiurb4--2-no_sw',\n '1-LV-semiurb5--0-sw',\n '1-LV-semiurb5--0-no_sw',\n '1-LV-semiurb5--1-sw',\n '1-LV-semiurb5--1-no_sw',\n '1-LV-semiurb5--2-sw',\n '1-LV-semiurb5--2-no_sw',\n '1-LV-urban6--0-sw',\n '1-LV-urban6--0-no_sw',\n '1-LV-urban6--1-sw',\n '1-LV-urban6--1-no_sw',\n '1-LV-urban6--2-sw',\n '1-LV-urban6--2-no_sw']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get lists of simbench codes\n",
    "all_simbench_codes = sb.collect_all_simbench_codes()\n",
    "all_simbench_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'/'.join(os.path.dirname(os.path.abspath(__file__).split(\"\\\\\"))) + r\"/Models/SelfSupervised/base_model.pt\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net.bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_json('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'\n",
    "#Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "#TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "#TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "#NETWORK CONSTRAINTS\n",
    "\n",
    "#Maximize the branch limits\n",
    "\n",
    "#max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "#for i in range(len(max_i_ka)):\n",
    "# max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "#Maximize line loading percents\n",
    "max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo loading percent\n",
    "max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo3w loading percent\n",
    "max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Cost assignment\n",
    "\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "pp.runopp(net,verbose=True)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = read_unsupervised_dataset('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_data[0].has_isolated_nodes()\n",
    "#train_data[0].has_self_loops()\n",
    "#train_data[0].is_undirected()\n",
    "x_dict = train_data[0].to_dict()\n",
    "#to_json(train_dict)\n",
    "ln = len(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"][0])\n",
    "print(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"])#[:, :int(ln/2)]\n",
    "x_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')\n",
    "idx_mapper, node_types_as_dict = extract_node_types_as_dict(net)\n",
    "for key in node_types_as_dict:\n",
    "    print(f\"Bus Type: {key}\")\n",
    "    for i in range(len(node_types_as_dict[key])):\n",
    "        #node_types_as_dict[key][i] = idx_mapper[node_types_as_dict[key][i]]\n",
    "        print(str(node_types_as_dict[key][i]))\n",
    "    print(\"-------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_dict[\"PQ\"]['x'][2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_available = []\n",
    "for nw_name in all_simbench_codes:\n",
    "        net = sb.get_simbench_net(nw_name)\n",
    "        print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "        #dict_probs = pp.diagnostic(net,report_style='None')\n",
    "        #for bus_num in dict_probs['multiple_voltage_controlling_elements_per_bus']['buses_with_gens_and_ext_grids']:\n",
    "        #    net.gen = net.gen.drop(net.gen[net.gen.bus == bus_num].index)\n",
    "\n",
    "        #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "        #Set upper and lower limits of active-reactive powers of loads\n",
    "        min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "        p_mw = list(net.load.p_mw.values)\n",
    "        q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "        for i in range(len(p_mw)):\n",
    "            min_p_mw_val.append(p_mw[i])\n",
    "            max_p_mw_val.append(p_mw[i])\n",
    "            min_q_mvar_val.append(q_mvar[i])\n",
    "            max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "        net.load.min_p_mw = min_p_mw_val\n",
    "        net.load.max_p_mw = max_p_mw_val\n",
    "        net.load.min_q_mvar = min_q_mvar_val\n",
    "        net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "        #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "        ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "        pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "        #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "        #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "        #NETWORK CONSTRAINTS\n",
    "\n",
    "        #Maximize the branch limits\n",
    "\n",
    "        #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "        #for i in range(len(max_i_ka)):\n",
    "        # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "        #Maximize line loading percents\n",
    "        max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo loading percent\n",
    "        max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo3w loading percent\n",
    "        max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Cost assignment\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "        try:\n",
    "            pp.runpm_dc_opf(net) # Run DCOPP\n",
    "        except pp.OPFNotConverged:\n",
    "            text = \"DC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "            print(text)\n",
    "            continue\n",
    "        print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR DCOPF\")\n",
    "        grids_available.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_available:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) # dcopp on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_ready = []\n",
    "for nw_name in all_simbench_codes[6:]:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "    #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "    #Set upper and lower limits of active-reactive powers of loads\n",
    "    min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "    p_mw = list(net.load.p_mw.values)\n",
    "    q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "    for i in range(len(p_mw)):\n",
    "        min_p_mw_val.append(p_mw[i])\n",
    "        max_p_mw_val.append(p_mw[i])\n",
    "        min_q_mvar_val.append(q_mvar[i])\n",
    "        max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "    net.load.min_p_mw = min_p_mw_val\n",
    "    net.load.max_p_mw = max_p_mw_val\n",
    "    net.load.min_q_mvar = min_q_mvar_val\n",
    "    net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "    #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "    ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "    pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "    #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "    #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "    #NETWORK CONSTRAINTS\n",
    "\n",
    "    #Maximize the branch limits\n",
    "\n",
    "    #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "    #for i in range(len(max_i_ka)):\n",
    "    # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "    #Maximize line loading percents\n",
    "    max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo loading percent\n",
    "    max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo3w loading percent\n",
    "    max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Cost assignment\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "    #ac_converged = True\n",
    "\n",
    "    #start_vec_name = \"\"\n",
    "    #for init in [\"pf\", \"flat\", \"results\"]:\n",
    "    #    try:\n",
    "    #        pp.runopp(net, init=init)  # Calculate ACOPF with IPFOPT\n",
    "    #    except pp.OPFNotConverged:\n",
    "    #        if init == \"results\":\n",
    "    #            text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \". SKIPPING THIS GRID.\"\n",
    "    #            print(text)\n",
    "    #            break\n",
    "    #        continue\n",
    "    #    start_vec_name = init\n",
    "    #    ac_converged = True\n",
    "    #    break\n",
    "    #if ac_converged:\n",
    "    #    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + start_vec_name + \".\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        pp.runpm_ac_opf(net) # Run DCOPP\n",
    "    except pp.OPFNotConverged:\n",
    "        text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "        print(text)\n",
    "        continue\n",
    "    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + \".\")\n",
    "    grids_ready.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_acopf_available_grid_names = [\"1-HV-mixed--0-no_sw\",\"1-HV-urban--0-no_sw\", \"1-MV-comm--0-no_sw\", \"1-MV-semiurb--0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_acopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf_and_acopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_available_grid_names = [\"1-HVMV-mixed-all-0-no_sw\", \"1-HVMV-mixed-1.105-0-no_sw\", \"1-HVMV-mixed-2.102-0-no_sw\",\"1-HVMV-mixed-4.101-0-no_sw\", \"1-HVMV-urban-all-0-no_sw\", \"1-HVMV-urban-2.203-0-no_sw\", \"1-HVMV-urban-3.201-0-no_sw\", \"1-HVMV-urban-4.201-0-no_sw\", \"1-HV-mixed--0-no_sw\", \"1-HV-urban--0-no_sw\", \"1-MVLV-rural-all-0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Unsupervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this revised version of the compute_node_embeddings function, the node features are weighted by the reverse admittance values in the adjacency matrix before they are summed to compute the node embeddings. The resulting node embeddings will reflect the strength of the connections between the nodes.\n",
    "\n",
    "To use the reverse admittance values as the edge weights, you would need to pass the Ybus matrix as the adjacency matrix when calling the compute_node_embeddings function. The Ybus matrix should be converted to a PyTorch tensor before passing it to the function.\n",
    "\n",
    "use the reverse admittance values as edge weights, you can modify the computation of the node embeddings to weight the node features by the reverse admittance values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the node types\n",
    "node_types = ['Slack Node', 'Generator Node', 'Load Node']\n",
    "\n",
    "# Define the number of nodes of each type in the graph\n",
    "num_nodes = {\n",
    "    'Slack Node': 1,\n",
    "    'Generator Node': 20,\n",
    "    'Load Node': 99\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_names = [_ for _ in os.listdir(os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\\")]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphdata_lst = read_multiple_supervised_datasets(grid_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(4, 256, num_layers, 4, dropout=0.0, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=\"last\",layer_type=\"TransConv\", activation=\"elu\")#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_datasets_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "    run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"OPF-GNN-Supervised-Homo\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Homo-GNN\",\n",
    "    \"num_layers\": num_layers,\n",
    "    \"dataset\": grid_names[i],\n",
    "    \"epochs\": 1000,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"output_channels\": out_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"channel type\": layer_type,\n",
    "    \"norm\": \"BatchNorm\",\n",
    "    \"scaler\": scaler\n",
    "\n",
    "    }\n",
    "    )\n",
    "    num_epochs = wandb.run.config.epochs\n",
    "    run.watch(model)\n",
    "    grid_name = graphdata.grid_name\n",
    "    run.config.dataset = grid_name\n",
    "    train_data = graphdata.train_data\n",
    "    run.config[\"number of busses\"] = np.shape(train_data[0].x)[0]\n",
    "    val_data = graphdata.val_data\n",
    "    test_data = graphdata.test_data\n",
    "    test_datasets_lst.append(test_data)\n",
    "    training_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    validation_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "    #test_loader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "    for _ in range(num_epochs):\n",
    "        #train_one_epoch(i, optimizer, training_loader, model, nn.MSELoss(), edge_index, edge_weights)\n",
    "        train_validate_one_epoch(_, grid_name, optimizer, training_loader, validation_loader, model, nn.MSELoss(), scaler)\n",
    "    print(\"Training and Validation finished \" + \"for GraphData \" + str(i) + \".\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader_lst = []\n",
    "val_loader_lst = []\n",
    "test_loader_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "\n",
    "    # Divide training data into chunks, load into Dataloaders and append to the list of training loaders\n",
    "    for train_data in divide_chunks(graphdata.train_data, 5):\n",
    "        train_loader_lst.append(DataLoader(train_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Divide validation data into chunks, load into Dataloaders and append to the list of validation loaders\n",
    "    for val_data in divide_chunks(graphdata.val_data, 5):\n",
    "        val_loader_lst.append(DataLoader(val_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Append the test data to the list\n",
    "    for test_data in divide_chunks(graphdata.test_data, 5):\n",
    "        test_loader_lst.append(DataLoader(test_data, batch_size=1, shuffle=True))\n",
    "\n",
    "print(\"Data Preparation finished.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr/100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"OPF-GNN-Supervised-Homo\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Homo-GNN\",\n",
    "    \"num_layers\": num_layers,\n",
    "    \"epochs\": 2000,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"output_channels\": out_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"channel type\": layer_type,\n",
    "    \"norm\": \"BatchNorm\",\n",
    "    \"scaler\": scaler\n",
    "    }\n",
    "    )\n",
    "\"\"\"\n",
    "for _ in range(wandb.run.config.epochs):\n",
    "    # Training\n",
    "    random.shuffle(train_loader_lst)\n",
    "    print(\"Training the model for epoch \" + str(_))\n",
    "    train_all_one_epoch(_, optimizer, train_loader_lst, model, nn.MSELoss())\n",
    "\n",
    "    # Validation\n",
    "    random.shuffle(val_loader_lst)\n",
    "    print(\"Validating the model for epoch \" + str(_))\n",
    "    validate_all_one_epoch(_, val_loader_lst, model, nn.MSELoss())\n",
    "\n",
    "    outputs = test_all_one_epoch(test_loader_lst, model, nn.MSELoss())\n",
    "print(\"Training and Validation finished.\")\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = test_all_one_epoch(test_loader_lst, model, nn.MSELoss())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output,target = outputs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\" #os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\Models\\\\Supervised\\\\\" + \"basemodel.pt\" #r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\"\n",
    "torch.save(model.state_dict(), \"supervisedmodel.pt\")\n",
    "print(\"done\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'/'.join(os.path.dirname(os.path.abspath(\"gnn.ipynb\")).split(\"\\\\\"))+ r\"/Models/SelfSupervised/base_model.pt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandapower.plotting.simple_plot import simple_plot\n",
    "from pandapower.plotting.plotly.simple_plotly import simple_plotly\n",
    "#simple_plot(net, plot_gens=True, plot_loads=True, plot_sgens=True, library=\"igraph\")\n",
    "simple_plotly(net, map_style=\"satellite\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runopp(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx_mapper, node_types_as_dict = extract_node_types_as_dict(net)\n",
    "node_types_as_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Module.parameters of GNN(4, 4, num_layers=5)>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "net = process_network(grid_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " HETEROGENEOUS GNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "index_mappers,net,data = generate_unsupervised_input('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_ACOPFGNN_model(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "x_dict, constraint_dict, _, _, bus_idx_neighbors_dict,scalers_dict, _ = extract_unsupervised_inputs(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'SB': tensor([[ 0.7618, -1.0000,  0.0000,  0.0000]], requires_grad=True),\n 'PQ': tensor([[-0.9310,  0.0000,  0.0091,  0.0037],\n         [-0.9310, -0.6667,  0.0094,  0.0034],\n         [-0.9310,  0.1667,  0.0538,  0.0260],\n         [-0.9310, -0.5000,  0.1295,  0.0438],\n         [-0.9310,  0.1667,  0.0090,  0.0032],\n         [-0.9310,  0.0000,  0.0083,  0.0033],\n         [-0.9310,  0.5000,  0.1223,  0.0493],\n         [-0.9310, -0.1667,  0.0806,  0.0232],\n         [-0.9310, -0.1667,  0.0093,  0.0031],\n         [-0.9310, -0.3333,  0.0084,  0.0035],\n         [-0.9310,  1.0000,  0.0083,  0.0035],\n         [-0.9310,  0.0000,  0.0562,  0.0209],\n         [-0.9310,  0.0000,  0.0612,  0.0229],\n         [-0.9310,  0.0000,  0.0083,  0.0031],\n         [-0.9310, -0.5000,  0.0085,  0.0036],\n         [-0.9310,  0.3333,  0.0088,  0.0036]], requires_grad=True),\n 'PV': tensor([[ 0.7618, -0.1667, -0.5000, -0.5000],\n         [-0.2414,  0.0000, -0.4286, -0.4286],\n         [-0.9310,  0.6667, -0.0122, -0.0048],\n         [-0.9310,  0.3333, -0.0340, -0.0135],\n         [-0.9310, -0.6667, -0.0182, -0.0072],\n         [-0.9310,  1.0000, -0.2129, -0.0842],\n         [-0.9310,  0.8333, -0.0702, -0.0285],\n         [-0.9310,  0.1667, -0.2750, -0.1078],\n         [-0.9310, -0.6667, -0.0562, -0.0222],\n         [-0.9310,  0.6667, -0.0231, -0.0091],\n         [-0.9310, -0.1667, -0.0781, -0.0318],\n         [-0.9310, -0.8333, -0.0402, -0.0159],\n         [-0.9310,  0.1667, -0.1845, -0.0730],\n         [-0.9310, -0.3333, -0.0703, -0.0275],\n         [-0.9310,  0.8333, -0.0094, -0.0037],\n         [-0.9310,  0.1667, -0.1149, -0.0455],\n         [-0.9310,  0.8333, -0.0154, -0.0062],\n         [-0.9310, -0.6667, -0.0286, -0.0112],\n         [-0.9310,  0.0000, -0.0261, -0.0103],\n         [-0.9310, -0.3333, -0.1225, -0.0490],\n         [-0.9310,  0.1667, -0.0963, -0.0371],\n         [-0.9310,  0.0000, -0.0117, -0.0046],\n         [-0.9310,  0.1667, -0.0187, -0.0073],\n         [-0.9310,  0.1667, -0.0687, -0.0266],\n         [-0.9310, -0.3333, -0.0119, -0.0044],\n         [-0.9310, -0.5000, -0.0121, -0.0048],\n         [-0.9310, -0.3333, -0.0159, -0.0064],\n         [-0.9310, -0.6667, -0.0578, -0.0228],\n         [-0.9310,  1.0000, -0.0752, -0.0297],\n         [-0.9310,  0.1667, -0.0130, -0.0051],\n         [-0.9310, -0.5000, -0.0136, -0.0054],\n         [-0.9310, -0.1667, -0.0263, -0.0103],\n         [-0.9310, -0.8333, -0.0817, -0.0322],\n         [-0.9310, -0.5000, -0.0175, -0.0066],\n         [-0.9310, -0.6667, -0.0810, -0.0324],\n         [-0.9310,  0.5000, -0.0202, -0.0079],\n         [-0.9310, -0.6667, -0.0166, -0.0063],\n         [-0.9310, -0.8333, -0.0130, -0.0053],\n         [-0.9310, -0.8333, -0.0287, -0.0112],\n         [-0.9310, -0.8333, -0.0215, -0.0084],\n         [-0.9310, -0.5000, -0.0371, -0.0147],\n         [-0.9310,  0.5000, -0.0239, -0.0095]], requires_grad=True),\n 'NB': tensor([[-0.9310,  0.8333,  0.0000,  0.0000],\n         [-0.9310, -0.3333,  0.0000,  0.0000],\n         [-0.9310,  0.8333,  0.0000,  0.0000],\n         [-0.9310, -0.5000,  0.0000,  0.0000],\n         [-0.9310, -0.8333,  0.0000,  0.0000]], requires_grad=True)}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "constraint_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ACOPFOutput(x_dict, scalers_dict, None, index_mappers).output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_names = sb.collect_all_simbench_codes()[25:]\n",
    "acopf_ok_grid_names = []\n",
    "\n",
    "for grid_name in grid_names:\n",
    "    print(f\"Trying grid {grid_name}\")\n",
    "    net = process_network(grid_name)\n",
    "\n",
    "    # try:\n",
    "    #     acopf_ok_grid_names.append(grid_name)\n",
    "    #     pp.runpm_ac_opf(net)\n",
    "    #     #print(net.res_bus)\n",
    "    # except:\n",
    "    #     print(f\"Julia didnt converge for {grid_name}\")\n",
    "    #     acopf_ok_grid_names.remove(grid_name)\n",
    "    try:\n",
    "        acopf_ok_grid_names.append(grid_name)\n",
    "        pp.runopp(net)\n",
    "        #print(net.res_bus)\n",
    "    except:\n",
    "        print(f\"OPP didnt converge for {grid_name}\")\n",
    "        acopf_ok_grid_names.remove(grid_name)\n",
    "acopf_ok_grid_names\n",
    "#save_multiple_unsupervised_inputs(grid_names, 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "data": {
      "text/plain": "        vm_pu   va_degree      p_mw     q_mvar     lam_p         lam_q\n0    1.025000    0.000000 -8.057173 -11.835759  1.000000  2.259515e-21\n2    1.001425  209.119600 -1.176327   0.135443  1.000714  1.887024e-03\n4    0.999942  209.185313  0.397800   0.183689  1.002495  4.950700e-03\n5    0.999154  209.227646  0.293150   0.148678  1.003357  6.811706e-03\n6    0.998564  209.265084  0.239906   0.171257  1.003935  8.382988e-03\n..        ...         ...       ...        ...       ...           ...\n112  0.987459  209.245271  0.237492   0.096853  1.020719  1.774877e-02\n113  0.987333  209.251486  0.036984   0.085200  1.020904  1.802879e-02\n114  0.987261  209.254787 -0.074431   0.034476  1.021013  1.818089e-02\n115  0.987149  209.258063  0.118649   0.137547  1.021198  1.835478e-02\n116  0.987060  209.260287  0.159486   0.137561  1.021350  1.848019e-02\n\n[115 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vm_pu</th>\n      <th>va_degree</th>\n      <th>p_mw</th>\n      <th>q_mvar</th>\n      <th>lam_p</th>\n      <th>lam_q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.025000</td>\n      <td>0.000000</td>\n      <td>-8.057173</td>\n      <td>-11.835759</td>\n      <td>1.000000</td>\n      <td>2.259515e-21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.001425</td>\n      <td>209.119600</td>\n      <td>-1.176327</td>\n      <td>0.135443</td>\n      <td>1.000714</td>\n      <td>1.887024e-03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.999942</td>\n      <td>209.185313</td>\n      <td>0.397800</td>\n      <td>0.183689</td>\n      <td>1.002495</td>\n      <td>4.950700e-03</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.999154</td>\n      <td>209.227646</td>\n      <td>0.293150</td>\n      <td>0.148678</td>\n      <td>1.003357</td>\n      <td>6.811706e-03</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.998564</td>\n      <td>209.265084</td>\n      <td>0.239906</td>\n      <td>0.171257</td>\n      <td>1.003935</td>\n      <td>8.382988e-03</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>0.987459</td>\n      <td>209.245271</td>\n      <td>0.237492</td>\n      <td>0.096853</td>\n      <td>1.020719</td>\n      <td>1.774877e-02</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>0.987333</td>\n      <td>209.251486</td>\n      <td>0.036984</td>\n      <td>0.085200</td>\n      <td>1.020904</td>\n      <td>1.802879e-02</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>0.987261</td>\n      <td>209.254787</td>\n      <td>-0.074431</td>\n      <td>0.034476</td>\n      <td>1.021013</td>\n      <td>1.818089e-02</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>0.987149</td>\n      <td>209.258063</td>\n      <td>0.118649</td>\n      <td>0.137547</td>\n      <td>1.021198</td>\n      <td>1.835478e-02</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>0.987060</td>\n      <td>209.260287</td>\n      <td>0.159486</td>\n      <td>0.137561</td>\n      <td>1.021350</td>\n      <td>1.848019e-02</td>\n    </tr>\n  </tbody>\n</table>\n<p>115 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acopf_ok_grid_names = ['1-HV-mixed--1-sw','1-MV-semiurb--0-no_sw','1-MV-comm--0-sw','1-MV-comm--0-no_sw']\n",
    "net = process_network(acopf_ok_grid_names[1])\n",
    "pp.runopp(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acopf_grid_names = ['1-HV-mixed--0-no_sw','1-MV-semiurb--0-no_sw', '1-MV-comm--0-no_sw']\n",
    "save_multiple_unsupervised_inputs(acopf_grid_names, num_samples=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "inputs = load_multiple_unsupervised_inputs()\n",
    "for i,input in enumerate(inputs):\n",
    "    if input.res_bus is None:\n",
    "        print(f\"None at {i}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_unsupervised_inputs('1-HV-mixed--0-no_sw', 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'SB': tensor([[ 1., -1.,  1., -1.]]),\n 'PQ': tensor([[-0.9873,  0.7709, -0.3623, -0.1186],\n         [-0.9394, -0.0805, -0.3617, -0.1180],\n         [-0.9398, -0.0860, -0.3399, -0.1061],\n         [-0.9422, -0.1190, -0.2339, -0.0596],\n         [-0.8992,  0.4338, -0.3625, -0.1181],\n         [-0.9888,  0.5735, -0.3609, -0.1185],\n         [-0.9211,  0.1659, -0.2409, -0.0530],\n         [-0.9576,  0.1665, -0.3611, -0.1181],\n         [-0.9398, -0.0755, -0.3623, -0.1183],\n         [-0.9895,  0.5856, -0.3613, -0.1186],\n         [-0.9341,  0.2072, -0.3541, -0.1106],\n         [-0.9940,  0.7561, -0.3464, -0.1047],\n         [-0.9886,  0.5763, -0.3623, -0.1183],\n         [-0.9395, -0.0816, -0.3612, -0.1184],\n         [-0.9347,  0.7639, -0.3609, -0.1177]]),\n 'PV': tensor([[ 0.5288, -0.2341,  0.6834,  0.5512],\n         [-0.4013,  0.3430,  0.2802,  1.0000],\n         [-1.0000,  0.7024, -0.3831, -0.1285],\n         [-0.9931,  0.7434, -0.4373, -0.1552],\n         [-0.9809,  0.8055, -0.3962, -0.1350],\n         [-0.9984,  0.6872, -0.9070, -0.3857],\n         [-0.9480,  0.1122, -0.4262, -0.1478],\n         [-0.8986,  0.4423, -0.8197, -0.3525],\n         [-0.9185,  0.1989, -0.4910, -0.1819],\n         [-0.9267,  0.8882, -0.4306, -0.1519],\n         [-0.9684,  0.3861, -0.4436, -0.1626],\n         [-0.9866,  0.5918, -0.4526, -0.1626],\n         [-0.9845,  0.8857, -0.8151, -0.3399],\n         [-0.9730,  0.5483, -0.4224, -0.1476],\n         [-0.9751,  0.5201, -0.3960, -0.1350],\n         [-0.9045,  0.3720, -0.5360, -0.2024],\n         [-0.9967,  0.7077, -0.3897, -0.1319],\n         [-0.9340,  0.2081, -0.4233, -0.1488],\n         [-0.9332,  0.7182, -0.3756, -0.1196],\n         [-0.9298,  0.4596, -0.4381, -0.1556],\n         [-0.9866,  0.8909, -0.5575, -0.2149],\n         [-0.9987,  0.7209, -0.4866, -0.1791],\n         [-0.9488,  0.1014, -0.4018, -0.1378],\n         [-0.9871,  0.7736, -0.3985, -0.1360],\n         [-0.9416, -0.1161, -0.4202, -0.1504],\n         [-0.9395, -0.0814, -0.3791, -0.1270],\n         [-0.9981,  0.7278, -0.3824, -0.1282],\n         [-0.9812,  0.8017, -0.3905, -0.1329],\n         [-0.9927,  0.7704, -0.4951, -0.1844],\n         [-0.9179,  1.0000, -0.5613, -0.2161],\n         [-0.9351,  0.7779, -0.3837, -0.1294],\n         [-0.9299,  0.8463, -0.3848, -0.1291],\n         [-0.9297,  0.4796, -0.4155, -0.1449],\n         [-0.9298,  0.6174, -0.4543, -0.1622],\n         [-0.9999,  0.7032, -0.3942, -0.1340],\n         [-0.9762,  0.8207, -0.5558, -0.2136],\n         [-0.9394, -0.0798, -0.4018, -0.1383],\n         [-0.9794,  0.8089, -0.3911, -0.1333],\n         [-0.9296,  0.8495, -0.3844, -0.1289],\n         [-0.9296,  0.6190, -0.4237, -0.1477],\n         [-0.9315,  0.3489, -0.4040, -0.1398],\n         [-0.9933,  0.7568, -0.4450, -0.1592],\n         [-0.9937,  0.7920, -0.4094, -0.1415]]),\n 'NB': tensor([[-0.9779,  0.4835, -0.3725, -0.1234],\n         [-0.9578,  0.1697, -0.3725, -0.1234],\n         [-0.9895,  0.5854, -0.3725, -0.1234],\n         [-0.9646,  0.2744, -0.3725, -0.1234],\n         [-0.9045,  0.3719, -0.3725, -0.1234]])}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = load_unsupervised_inputs('1-HV-mixed--0-no_sw')\n",
    "inputs[0].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs[57].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display Plot of Customized Sigmoid Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as p\n",
    "import torch\n",
    "\n",
    "def custom_tanh(x: torch.Tensor, lower_bound: float, upper_bound: float) -> torch.Tensor:\n",
    "    width = upper_bound - lower_bound\n",
    "    return 0.5 * width * torch.tanh(x) + 0.5 * (upper_bound + lower_bound)\n",
    "\n",
    "\n",
    "lst = []\n",
    "min_val = 0\n",
    "max_val = 0.1\n",
    "range_ = []\n",
    "i = -2\n",
    "while i < 2:\n",
    "    range_.append(i)\n",
    "    i += 0.01\n",
    "\n",
    "for i in range_:\n",
    "    val = custom_tanh(torch.tensor(i), min_val, max_val)\n",
    "    #val = torch.tanh(torch.tensor(i))\n",
    "    lst.append(val)\n",
    "\n",
    "# Now, plot the values in lst\n",
    "p.plot(range_, lst)\n",
    "p.xlabel('Input')\n",
    "p.ylabel('Value')\n",
    "p.title('Plot of Enforcing Activation Function')\n",
    "p.grid(True)\n",
    "p.show()\n",
    "print(range_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Heterogeneous Self Supervised Model and Process Inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "model = load_ACOPFGNN_model(grid_name, \"base_model.pt\", hidden_channels=128, num_layers=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'SB': tensor([[ 1.0410,  0.1195,  0.1117, -0.5527]], grad_fn=<EluBackward0>), 'PQ': tensor([[ 0.7845,  0.5525, -0.3123,  0.9621],\n",
      "        [ 0.9402, -0.0822, -0.5976,  1.0648],\n",
      "        [ 0.7305,  0.6192, -0.2504,  0.9214],\n",
      "        [ 0.6637,  0.6712, -0.1704,  0.8417],\n",
      "        [ 0.9081, -0.0444, -0.5810,  1.0999],\n",
      "        [ 0.3656,  0.6597, -0.0082,  0.7576],\n",
      "        [ 0.7483, -0.3726, -0.6642,  1.1994],\n",
      "        [ 0.8414, -0.0771, -0.5779,  1.1349],\n",
      "        [ 0.3199,  0.6072, -0.0699,  0.8908],\n",
      "        [ 1.1162, -0.1804, -0.5681,  0.6163],\n",
      "        [ 0.4061, -0.5692, -0.7087,  1.3860],\n",
      "        [ 0.6826,  0.6575, -0.1870,  0.8458],\n",
      "        [ 0.8687,  0.3222, -0.4489,  1.0479],\n",
      "        [ 0.0871,  0.1304, -0.2140,  1.0187],\n",
      "        [ 0.9251,  0.5312, -0.3394,  0.9130],\n",
      "        [ 0.7109,  0.6334, -0.2208,  0.8751]], grad_fn=<EluBackward0>), 'PV': tensor([[-0.3689,  0.6374,  0.7594, -0.0317],\n",
      "        [-0.4046,  0.5157,  0.7236, -0.0487],\n",
      "        [-0.6544,  0.7219,  0.9912,  0.2323],\n",
      "        [-0.6278,  0.6986,  0.9464,  0.2188],\n",
      "        [-0.5368,  0.2951,  0.8150, -0.2100],\n",
      "        [-0.7253,  0.7179,  1.0872,  0.2880],\n",
      "        [-0.5308,  0.2922,  0.8074, -0.2117],\n",
      "        [-0.2521, -0.1691,  0.6265, -0.6046],\n",
      "        [-0.3717,  0.0616,  0.6889, -0.4382],\n",
      "        [-0.5362,  0.2948,  0.8142, -0.2101],\n",
      "        [-0.5293,  0.2911,  0.8055, -0.2122],\n",
      "        [-0.7879,  1.2360,  1.1765,  1.0048],\n",
      "        [-0.5165,  0.2857,  0.7897, -0.2149],\n",
      "        [-0.5306,  0.2916,  0.8071, -0.2119],\n",
      "        [-0.4078, -0.2470,  0.6575, -0.6014],\n",
      "        [-0.4701, -0.2458,  0.7886, -0.6506],\n",
      "        [-0.8207,  1.2776,  1.2538,  1.0767],\n",
      "        [-0.6596,  0.8400,  1.0123,  0.3719],\n",
      "        [-0.7186,  0.9673,  1.0604,  0.6131],\n",
      "        [-0.7285,  1.0223,  1.0728,  0.6928],\n",
      "        [-0.5441,  0.5218,  0.8620, -0.0406],\n",
      "        [-0.7288,  1.0785,  1.0829,  0.7430],\n",
      "        [-0.5176,  0.4592,  0.8482, -0.1362],\n",
      "        [-0.3583,  0.0858,  0.6916, -0.4382],\n",
      "        [-0.3900,  0.1262,  0.7043, -0.3960],\n",
      "        [-0.7716,  1.1377,  1.1493,  0.8622],\n",
      "        [-0.5966,  0.6208,  0.9144,  0.1032],\n",
      "        [-0.6080,  0.5093,  0.9383, -0.0458],\n",
      "        [-0.7722,  1.1575,  1.1450,  0.8980],\n",
      "        [-0.3901,  0.1273,  0.7044, -0.3961],\n",
      "        [-0.6148,  0.6633,  0.9333,  0.1659],\n",
      "        [-0.5212,  0.4579,  0.8446, -0.1269],\n",
      "        [-0.7518,  0.9341,  1.1165,  0.5990],\n",
      "        [-0.4756,  0.3755,  0.8063, -0.2248],\n",
      "        [-0.8323,  1.2990,  1.2738,  1.1327],\n",
      "        [-0.4944,  0.4096,  0.8213, -0.1851],\n",
      "        [-0.5373,  0.4850,  0.8606, -0.0944],\n",
      "        [-0.6536,  0.7627,  0.9762,  0.3131],\n",
      "        [-0.4722,  0.3597,  0.8082, -0.2485],\n",
      "        [-0.5364,  0.2949,  0.8145, -0.2101],\n",
      "        [-0.5346,  0.2939,  0.8121, -0.2107],\n",
      "        [-0.5362,  0.2948,  0.8142, -0.2101]], grad_fn=<EluBackward0>), 'NB': tensor([[ 1.2333, -0.3460, -0.5098, -0.1023],\n",
      "        [ 0.0264,  0.4674,  0.4864, -0.1269],\n",
      "        [ 0.6934,  0.0292, -0.2543,  0.1708],\n",
      "        [ 3.1425, -0.9039, -0.7878, -0.9336],\n",
      "        [ 0.1846,  0.2464,  0.5397, -0.4864]], grad_fn=<EluBackward0>)})\n"
     ]
    }
   ],
   "source": [
    "# Create ACOPFGNN model and Optimizer\n",
    "index_mappers,net,data = generate_unsupervised_input(grid_name)\n",
    "model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=32, num_layers=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#lr_per_mil_param = 1e-5/1000000\n",
    "learning_rate = 2.5e-4#(num_parameters/1000000) * lr_per_mil_param\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "23705"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mcanbay96\u001B[0m (\u001B[33mcbml\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\wandb\\run-20231014_163153-kkz7f85e</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/kkz7f85e' target=\"_blank\">restful-bush-146</a></strong> to <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/kkz7f85e' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/kkz7f85e</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_NOTEBOOK_NAME = \"msi\"\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"ACOPF-GNN-SelfSupervised-Hetero\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            #\"learning_rate\": optimizer.,\n",
    "            \"architecture\": \"Hetero-SelfSupervised-GNN\",\n",
    "            \"num_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "            \"num_heads\": model.heads,\n",
    "            \"num_layers\": model.num_layers,\n",
    "            \"grid_name\": grid_name,\n",
    "            \"activation\": model.act_fn,\n",
    "            \"dropout\": model.dropout,\n",
    "            \"in_channels\": model.in_channels,\n",
    "            \"output_channels\": model.out_channels,\n",
    "            \"hidden_channels\": model.hidden_channels,\n",
    "            \"channel type\": \"TransformerConv\",\n",
    "            \"scaler\": \"MinMax\",\n",
    "            \"model\": \"base_model_wired\"\n",
    "        }\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load Inputs\n",
    "inputs = load_unsupervised_inputs(grid_name)\n",
    "#inputs = load_multiple_unsupervised_inputs()\n",
    "n = len(inputs)\n",
    "train_inputs = inputs[:int(n*0.8)]\n",
    "val_inputs = inputs[int(n*0.8): int(n*0.95)]\n",
    "test_inputs = inputs[int(n*0.95):]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------   CURRENT LEARNING RATE: 0.001   ---------------\n",
      "Epoch: 0\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 1.9385786056518555 \n",
      "     Training Step: 1 Training Loss: 1.493556022644043 \n",
      "     Training Step: 2 Training Loss: 1.2006518840789795 \n",
      "     Training Step: 3 Training Loss: 1.0047500133514404 \n",
      "     Training Step: 4 Training Loss: 0.8729242086410522 \n",
      "     Training Step: 5 Training Loss: 0.7745358943939209 \n",
      "     Training Step: 6 Training Loss: 0.7127636671066284 \n",
      "     Training Step: 7 Training Loss: 0.6239981651306152 \n",
      "     Training Step: 8 Training Loss: 0.588146984577179 \n",
      "     Training Step: 9 Training Loss: 0.5374649167060852 \n",
      "     Training Step: 10 Training Loss: 0.47320985794067383 \n",
      "     Training Step: 11 Training Loss: 0.4327434301376343 \n",
      "     Training Step: 12 Training Loss: 0.41743409633636475 \n",
      "     Training Step: 13 Training Loss: 0.3629176616668701 \n",
      "     Training Step: 14 Training Loss: 0.33380356431007385 \n",
      "     Training Step: 15 Training Loss: 0.32277923822402954 \n",
      "     Training Step: 16 Training Loss: 0.28255969285964966 \n",
      "     Training Step: 17 Training Loss: 0.2596317529678345 \n",
      "     Training Step: 18 Training Loss: 0.2390606701374054 \n",
      "     Training Step: 19 Training Loss: 0.22064897418022156 \n",
      "     Training Step: 20 Training Loss: 0.21592992544174194 \n",
      "     Training Step: 21 Training Loss: 0.1883838176727295 \n",
      "     Training Step: 22 Training Loss: 0.17596039175987244 \n",
      "     Training Step: 23 Training Loss: 0.17262029647827148 \n",
      "     Training Step: 24 Training Loss: 0.1545570194721222 \n",
      "     Training Step: 25 Training Loss: 0.15219874680042267 \n",
      "     Training Step: 26 Training Loss: 0.14438936114311218 \n",
      "     Training Step: 27 Training Loss: 0.13040854036808014 \n",
      "     Training Step: 28 Training Loss: 0.12985289096832275 \n",
      "     Training Step: 29 Training Loss: 0.1185126081109047 \n",
      "     Training Step: 30 Training Loss: 0.11329258978366852 \n",
      "     Training Step: 31 Training Loss: 0.10912813246250153 \n",
      "     Training Step: 32 Training Loss: 0.10779570043087006 \n",
      "     Training Step: 33 Training Loss: 0.10400469601154327 \n",
      "     Training Step: 34 Training Loss: 0.10025165975093842 \n",
      "     Training Step: 35 Training Loss: 0.09554905444383621 \n",
      "     Training Step: 36 Training Loss: 0.08900953829288483 \n",
      "     Training Step: 37 Training Loss: 0.08529787510633469 \n",
      "     Training Step: 38 Training Loss: 0.08487334847450256 \n",
      "     Training Step: 39 Training Loss: 0.08149313926696777 \n",
      "     Training Step: 40 Training Loss: 0.07784457504749298 \n",
      "     Training Step: 41 Training Loss: 0.07514746487140656 \n",
      "     Training Step: 42 Training Loss: 0.07012777775526047 \n",
      "     Training Step: 43 Training Loss: 0.06749676167964935 \n",
      "     Training Step: 44 Training Loss: 0.065328449010849 \n",
      "     Training Step: 45 Training Loss: 0.06423118710517883 \n",
      "     Training Step: 46 Training Loss: 0.06094265356659889 \n",
      "     Training Step: 47 Training Loss: 0.05723220854997635 \n",
      "     Training Step: 48 Training Loss: 0.056210216134786606 \n",
      "     Training Step: 49 Training Loss: 0.053159914910793304 \n",
      "     Training Step: 50 Training Loss: 0.05066049098968506 \n",
      "     Training Step: 51 Training Loss: 0.04890943318605423 \n",
      "     Training Step: 52 Training Loss: 0.0477861687541008 \n",
      "     Training Step: 53 Training Loss: 0.04527276009321213 \n",
      "     Training Step: 54 Training Loss: 0.043348900973796844 \n",
      "     Training Step: 55 Training Loss: 0.041980959475040436 \n",
      "     Training Step: 56 Training Loss: 0.04002540558576584 \n",
      "     Training Step: 57 Training Loss: 0.03847840055823326 \n",
      "     Training Step: 58 Training Loss: 0.037188366055488586 \n",
      "     Training Step: 59 Training Loss: 0.03526991605758667 \n",
      "     Training Step: 60 Training Loss: 0.03430523723363876 \n",
      "     Training Step: 61 Training Loss: 0.032731276005506516 \n",
      "     Training Step: 62 Training Loss: 0.03167518973350525 \n",
      "     Training Step: 63 Training Loss: 0.030115965753793716 \n",
      "     Training Step: 64 Training Loss: 0.029000844806432724 \n",
      "     Training Step: 65 Training Loss: 0.028416842222213745 \n",
      "     Training Step: 66 Training Loss: 0.027400974184274673 \n",
      "     Training Step: 67 Training Loss: 0.026446055620908737 \n",
      "     Training Step: 68 Training Loss: 0.02548968978226185 \n",
      "     Training Step: 69 Training Loss: 0.024158863350749016 \n",
      "     Training Step: 70 Training Loss: 0.023784831166267395 \n",
      "     Training Step: 71 Training Loss: 0.022423308342695236 \n",
      "     Training Step: 72 Training Loss: 0.02213328331708908 \n",
      "     Training Step: 73 Training Loss: 0.021442990750074387 \n",
      "     Training Step: 74 Training Loss: 0.020726360380649567 \n",
      "     Training Step: 75 Training Loss: 0.020103957504034042 \n",
      "     Training Step: 76 Training Loss: 0.019510138779878616 \n",
      "     Training Step: 77 Training Loss: 0.01890641823410988 \n",
      "     Training Step: 78 Training Loss: 0.01835409179329872 \n",
      "     Training Step: 79 Training Loss: 0.017181001603603363 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.016699815168976784 \n",
      "     Validation Step: 1 Validation Loss: 0.017223242670297623 \n",
      "     Validation Step: 2 Validation Loss: 0.016678547486662865 \n",
      "     Validation Step: 3 Validation Loss: 0.016683727502822876 \n",
      "     Validation Step: 4 Validation Loss: 0.01662348210811615 \n",
      "     Validation Step: 5 Validation Loss: 0.01723961904644966 \n",
      "     Validation Step: 6 Validation Loss: 0.016640279442071915 \n",
      "     Validation Step: 7 Validation Loss: 0.016668062657117844 \n",
      "     Validation Step: 8 Validation Loss: 0.017243310809135437 \n",
      "     Validation Step: 9 Validation Loss: 0.01725715771317482 \n",
      "     Validation Step: 10 Validation Loss: 0.0166923888027668 \n",
      "     Validation Step: 11 Validation Loss: 0.01723765954375267 \n",
      "     Validation Step: 12 Validation Loss: 0.017224149778485298 \n",
      "     Validation Step: 13 Validation Loss: 0.016635935753583908 \n",
      "     Validation Step: 14 Validation Loss: 0.016580855473876 \n",
      "Epoch: 1\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.017224270850419998 \n",
      "     Training Step: 1 Training Loss: 0.016148054972290993 \n",
      "     Training Step: 2 Training Loss: 0.01628570631146431 \n",
      "     Training Step: 3 Training Loss: 0.015827501192688942 \n",
      "     Training Step: 4 Training Loss: 0.014802331104874611 \n",
      "     Training Step: 5 Training Loss: 0.014974869787693024 \n",
      "     Training Step: 6 Training Loss: 0.014036325737833977 \n",
      "     Training Step: 7 Training Loss: 0.014216805808246136 \n",
      "     Training Step: 8 Training Loss: 0.013841768726706505 \n",
      "     Training Step: 9 Training Loss: 0.012988423928618431 \n",
      "     Training Step: 10 Training Loss: 0.013178225606679916 \n",
      "     Training Step: 11 Training Loss: 0.012861042283475399 \n",
      "     Training Step: 12 Training Loss: 0.012036347761750221 \n",
      "     Training Step: 13 Training Loss: 0.011755636893212795 \n",
      "     Training Step: 14 Training Loss: 0.01149367168545723 \n",
      "     Training Step: 15 Training Loss: 0.011690226383507252 \n",
      "     Training Step: 16 Training Loss: 0.01142866536974907 \n",
      "     Training Step: 17 Training Loss: 0.011155690997838974 \n",
      "     Training Step: 18 Training Loss: 0.01045216340571642 \n",
      "     Training Step: 19 Training Loss: 0.010617312043905258 \n",
      "     Training Step: 20 Training Loss: 0.010356379672884941 \n",
      "     Training Step: 21 Training Loss: 0.010085465386509895 \n",
      "     Training Step: 22 Training Loss: 0.009428969584405422 \n",
      "     Training Step: 23 Training Loss: 0.009175167419016361 \n",
      "     Training Step: 24 Training Loss: 0.009304506704211235 \n",
      "     Training Step: 25 Training Loss: 0.008720554411411285 \n",
      "     Training Step: 26 Training Loss: 0.008835251443088055 \n",
      "     Training Step: 27 Training Loss: 0.008613036014139652 \n",
      "     Training Step: 28 Training Loss: 0.008418738842010498 \n",
      "     Training Step: 29 Training Loss: 0.00789650622755289 \n",
      "     Training Step: 30 Training Loss: 0.00771808996796608 \n",
      "     Training Step: 31 Training Loss: 0.007530765607953072 \n",
      "     Training Step: 32 Training Loss: 0.0074202436953783035 \n",
      "     Training Step: 33 Training Loss: 0.0075039733201265335 \n",
      "     Training Step: 34 Training Loss: 0.007340606767684221 \n",
      "     Training Step: 35 Training Loss: 0.006911564618349075 \n",
      "     Training Step: 36 Training Loss: 0.007006531581282616 \n",
      "     Training Step: 37 Training Loss: 0.006956788711249828 \n",
      "     Training Step: 38 Training Loss: 0.006510389968752861 \n",
      "     Training Step: 39 Training Loss: 0.006628595292568207 \n",
      "     Training Step: 40 Training Loss: 0.006254095584154129 \n",
      "     Training Step: 41 Training Loss: 0.006178181618452072 \n",
      "     Training Step: 42 Training Loss: 0.006014755927026272 \n",
      "     Training Step: 43 Training Loss: 0.005940834991633892 \n",
      "     Training Step: 44 Training Loss: 0.0058052269741892815 \n",
      "     Training Step: 45 Training Loss: 0.005892972461879253 \n",
      "     Training Step: 46 Training Loss: 0.005811755545437336 \n",
      "     Training Step: 47 Training Loss: 0.005692503415048122 \n",
      "     Training Step: 48 Training Loss: 0.005425000097602606 \n",
      "     Training Step: 49 Training Loss: 0.005497106816619635 \n",
      "     Training Step: 50 Training Loss: 0.00533998291939497 \n",
      "     Training Step: 51 Training Loss: 0.005243465770035982 \n",
      "     Training Step: 52 Training Loss: 0.005160384811460972 \n",
      "     Training Step: 53 Training Loss: 0.005074933171272278 \n",
      "     Training Step: 54 Training Loss: 0.004834255203604698 \n",
      "     Training Step: 55 Training Loss: 0.00489400140941143 \n",
      "     Training Step: 56 Training Loss: 0.004672222770750523 \n",
      "     Training Step: 57 Training Loss: 0.004705528728663921 \n",
      "     Training Step: 58 Training Loss: 0.004634463228285313 \n",
      "     Training Step: 59 Training Loss: 0.004437839612364769 \n",
      "     Training Step: 60 Training Loss: 0.004340927582234144 \n",
      "     Training Step: 61 Training Loss: 0.004369885195046663 \n",
      "     Training Step: 62 Training Loss: 0.00427808566018939 \n",
      "     Training Step: 63 Training Loss: 0.004231439903378487 \n",
      "     Training Step: 64 Training Loss: 0.004141533747315407 \n",
      "     Training Step: 65 Training Loss: 0.004092340357601643 \n",
      "     Training Step: 66 Training Loss: 0.004006471484899521 \n",
      "     Training Step: 67 Training Loss: 0.003922521136701107 \n",
      "     Training Step: 68 Training Loss: 0.003765012137591839 \n",
      "     Training Step: 69 Training Loss: 0.003804117674008012 \n",
      "     Training Step: 70 Training Loss: 0.0036581731401383877 \n",
      "     Training Step: 71 Training Loss: 0.003583522979170084 \n",
      "     Training Step: 72 Training Loss: 0.003611853811889887 \n",
      "     Training Step: 73 Training Loss: 0.0034679719246923923 \n",
      "     Training Step: 74 Training Loss: 0.00342229544185102 \n",
      "     Training Step: 75 Training Loss: 0.003364815842360258 \n",
      "     Training Step: 76 Training Loss: 0.003293998073786497 \n",
      "     Training Step: 77 Training Loss: 0.0033345152623951435 \n",
      "     Training Step: 78 Training Loss: 0.003288016654551029 \n",
      "     Training Step: 79 Training Loss: 0.003143864218145609 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.0031092544086277485 \n",
      "     Validation Step: 1 Validation Loss: 0.0031084278598427773 \n",
      "     Validation Step: 2 Validation Loss: 0.0030990634113550186 \n",
      "     Validation Step: 3 Validation Loss: 0.0031860596500337124 \n",
      "     Validation Step: 4 Validation Loss: 0.0031879146117717028 \n",
      "     Validation Step: 5 Validation Loss: 0.0031857462599873543 \n",
      "     Validation Step: 6 Validation Loss: 0.0031164740212261677 \n",
      "     Validation Step: 7 Validation Loss: 0.0031083114445209503 \n",
      "     Validation Step: 8 Validation Loss: 0.0031977295875549316 \n",
      "     Validation Step: 9 Validation Loss: 0.0031037782318890095 \n",
      "     Validation Step: 10 Validation Loss: 0.0031859646551311016 \n",
      "     Validation Step: 11 Validation Loss: 0.0031228652223944664 \n",
      "     Validation Step: 12 Validation Loss: 0.0031848447397351265 \n",
      "     Validation Step: 13 Validation Loss: 0.003112947102636099 \n",
      "     Validation Step: 14 Validation Loss: 0.0031106430105865 \n",
      "Epoch: 2\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.0031903989147394896 \n",
      "     Training Step: 1 Training Loss: 0.0031447228975594044 \n",
      "     Training Step: 2 Training Loss: 0.003098361659795046 \n",
      "     Training Step: 3 Training Loss: 0.003048144280910492 \n",
      "     Training Step: 4 Training Loss: 0.0029948391020298004 \n",
      "     Training Step: 5 Training Loss: 0.0028858566656708717 \n",
      "     Training Step: 6 Training Loss: 0.002847331576049328 \n",
      "     Training Step: 7 Training Loss: 0.002798167522996664 \n",
      "     Training Step: 8 Training Loss: 0.0028284480795264244 \n",
      "     Training Step: 9 Training Loss: 0.0027255089953541756 \n",
      "     Training Step: 10 Training Loss: 0.002752946224063635 \n",
      "     Training Step: 11 Training Loss: 0.002709721215069294 \n",
      "     Training Step: 12 Training Loss: 0.002675373572856188 \n",
      "     Training Step: 13 Training Loss: 0.0025711664929986 \n",
      "     Training Step: 14 Training Loss: 0.0025389278307557106 \n",
      "     Training Step: 15 Training Loss: 0.0024995259009301662 \n",
      "     Training Step: 16 Training Loss: 0.0025298758409917355 \n",
      "     Training Step: 17 Training Loss: 0.0024347715079784393 \n",
      "     Training Step: 18 Training Loss: 0.0024046762846410275 \n",
      "     Training Step: 19 Training Loss: 0.002426860621199012 \n",
      "     Training Step: 20 Training Loss: 0.002335929311811924 \n",
      "     Training Step: 21 Training Loss: 0.002304544672369957 \n",
      "     Training Step: 22 Training Loss: 0.002332631964236498 \n",
      "     Training Step: 23 Training Loss: 0.002301509492099285 \n",
      "     Training Step: 24 Training Loss: 0.0022228960879147053 \n",
      "     Training Step: 25 Training Loss: 0.0021879440173506737 \n",
      "     Training Step: 26 Training Loss: 0.0021615857258439064 \n",
      "     Training Step: 27 Training Loss: 0.0021374179050326347 \n",
      "     Training Step: 28 Training Loss: 0.0021603843197226524 \n",
      "     Training Step: 29 Training Loss: 0.0021308246068656445 \n",
      "     Training Step: 30 Training Loss: 0.00210028188303113 \n",
      "     Training Step: 31 Training Loss: 0.0020737056620419025 \n",
      "     Training Step: 32 Training Loss: 0.002046771813184023 \n",
      "     Training Step: 33 Training Loss: 0.0019763228483498096 \n",
      "     Training Step: 34 Training Loss: 0.0019523256924003363 \n",
      "     Training Step: 35 Training Loss: 0.0019212744664400816 \n",
      "     Training Step: 36 Training Loss: 0.0019028703682124615 \n",
      "     Training Step: 37 Training Loss: 0.001922006020322442 \n",
      "     Training Step: 38 Training Loss: 0.0018999428721144795 \n",
      "     Training Step: 39 Training Loss: 0.0018737604841589928 \n",
      "     Training Step: 40 Training Loss: 0.0018064189935103059 \n",
      "     Training Step: 41 Training Loss: 0.0018357507651671767 \n",
      "     Training Step: 42 Training Loss: 0.0018091436941176653 \n",
      "     Training Step: 43 Training Loss: 0.001744449371472001 \n",
      "     Training Step: 44 Training Loss: 0.0017246529459953308 \n",
      "     Training Step: 45 Training Loss: 0.0017068267334252596 \n",
      "     Training Step: 46 Training Loss: 0.0017239778535440564 \n",
      "     Training Step: 47 Training Loss: 0.0017015819903463125 \n",
      "     Training Step: 48 Training Loss: 0.0016851366963237524 \n",
      "     Training Step: 49 Training Loss: 0.0016604848206043243 \n",
      "     Training Step: 50 Training Loss: 0.0016421254258602858 \n",
      "     Training Step: 51 Training Loss: 0.001625086646527052 \n",
      "     Training Step: 52 Training Loss: 0.0016037907917052507 \n",
      "     Training Step: 53 Training Loss: 0.001558820717036724 \n",
      "     Training Step: 54 Training Loss: 0.001538027892820537 \n",
      "     Training Step: 55 Training Loss: 0.0015490882797166705 \n",
      "     Training Step: 56 Training Loss: 0.001504268147982657 \n",
      "     Training Step: 57 Training Loss: 0.0015172073617577553 \n",
      "     Training Step: 58 Training Loss: 0.0014668029034510255 \n",
      "     Training Step: 59 Training Loss: 0.0014513025525957346 \n",
      "     Training Step: 60 Training Loss: 0.0014688004739582539 \n",
      "     Training Step: 61 Training Loss: 0.001450603362172842 \n",
      "     Training Step: 62 Training Loss: 0.0014339626068249345 \n",
      "     Training Step: 63 Training Loss: 0.0013862294144928455 \n",
      "     Training Step: 64 Training Loss: 0.001374545507133007 \n",
      "     Training Step: 65 Training Loss: 0.0013839517487213016 \n",
      "     Training Step: 66 Training Loss: 0.0013716774992644787 \n",
      "     Training Step: 67 Training Loss: 0.001353780971840024 \n",
      "     Training Step: 68 Training Loss: 0.0013376972638070583 \n",
      "     Training Step: 69 Training Loss: 0.0012975921854376793 \n",
      "     Training Step: 70 Training Loss: 0.0012863336596637964 \n",
      "     Training Step: 71 Training Loss: 0.0012990017421543598 \n",
      "     Training Step: 72 Training Loss: 0.0012836870737373829 \n",
      "     Training Step: 73 Training Loss: 0.001246427884325385 \n",
      "     Training Step: 74 Training Loss: 0.0012586507946252823 \n",
      "     Training Step: 75 Training Loss: 0.0012265121331438422 \n",
      "     Training Step: 76 Training Loss: 0.0012288304278627038 \n",
      "     Training Step: 77 Training Loss: 0.0012153335846960545 \n",
      "     Training Step: 78 Training Loss: 0.001177250873297453 \n",
      "     Training Step: 79 Training Loss: 0.0011951978085562587 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.0011800120119005442 \n",
      "     Validation Step: 1 Validation Loss: 0.0011850501177832484 \n",
      "     Validation Step: 2 Validation Loss: 0.0011593258241191506 \n",
      "     Validation Step: 3 Validation Loss: 0.0011586180189624429 \n",
      "     Validation Step: 4 Validation Loss: 0.0011793849989771843 \n",
      "     Validation Step: 5 Validation Loss: 0.0011589712230488658 \n",
      "     Validation Step: 6 Validation Loss: 0.0011618249118328094 \n",
      "     Validation Step: 7 Validation Loss: 0.001164260320365429 \n",
      "     Validation Step: 8 Validation Loss: 0.0011813633609563112 \n",
      "     Validation Step: 9 Validation Loss: 0.0011661183089017868 \n",
      "     Validation Step: 10 Validation Loss: 0.0011568120680749416 \n",
      "     Validation Step: 11 Validation Loss: 0.001176149700768292 \n",
      "     Validation Step: 12 Validation Loss: 0.0011684568598866463 \n",
      "     Validation Step: 13 Validation Loss: 0.0011821419466286898 \n",
      "     Validation Step: 14 Validation Loss: 0.0011553395306691527 \n",
      "Epoch: 3\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.0011592204682528973 \n",
      "     Training Step: 1 Training Loss: 0.0011493850033730268 \n",
      "     Training Step: 2 Training Loss: 0.0011546257883310318 \n",
      "     Training Step: 3 Training Loss: 0.00114570208825171 \n",
      "     Training Step: 4 Training Loss: 0.0011329954722896218 \n",
      "     Training Step: 5 Training Loss: 0.0011236945865675807 \n",
      "     Training Step: 6 Training Loss: 0.001089730765670538 \n",
      "     Training Step: 7 Training Loss: 0.0010954028693959117 \n",
      "     Training Step: 8 Training Loss: 0.0010702924337238073 \n",
      "     Training Step: 9 Training Loss: 0.0010605899151414633 \n",
      "     Training Step: 10 Training Loss: 0.0010444408981129527 \n",
      "     Training Step: 11 Training Loss: 0.0010353364050388336 \n",
      "     Training Step: 12 Training Loss: 0.001034020446240902 \n",
      "     Training Step: 13 Training Loss: 0.0010362656321376562 \n",
      "     Training Step: 14 Training Loss: 0.0010158646618947387 \n",
      "     Training Step: 15 Training Loss: 0.0010172019246965647 \n",
      "     Training Step: 16 Training Loss: 0.0010156865464523435 \n",
      "     Training Step: 17 Training Loss: 0.0009858743287622929 \n",
      "     Training Step: 18 Training Loss: 0.0009716857457533479 \n",
      "     Training Step: 19 Training Loss: 0.000977586954832077 \n",
      "     Training Step: 20 Training Loss: 0.0009687995770946145 \n",
      "     Training Step: 21 Training Loss: 0.0009472438832744956 \n",
      "     Training Step: 22 Training Loss: 0.000951983209233731 \n",
      "     Training Step: 23 Training Loss: 0.0009431741200387478 \n",
      "     Training Step: 24 Training Loss: 0.0009379595867358148 \n",
      "     Training Step: 25 Training Loss: 0.0009320452809333801 \n",
      "     Training Step: 26 Training Loss: 0.0009124434436671436 \n",
      "     Training Step: 27 Training Loss: 0.0008913782075978816 \n",
      "     Training Step: 28 Training Loss: 0.0008897266816347837 \n",
      "     Training Step: 29 Training Loss: 0.0008813483873382211 \n",
      "     Training Step: 30 Training Loss: 0.0008764298399910331 \n",
      "     Training Step: 31 Training Loss: 0.0008712344570085406 \n",
      "     Training Step: 32 Training Loss: 0.0008726247469894588 \n",
      "     Training Step: 33 Training Loss: 0.0008499724790453911 \n",
      "     Training Step: 34 Training Loss: 0.000858768355101347 \n",
      "     Training Step: 35 Training Loss: 0.00084878591587767 \n",
      "     Training Step: 36 Training Loss: 0.0008480969117954373 \n",
      "     Training Step: 37 Training Loss: 0.0008387214038521051 \n",
      "     Training Step: 38 Training Loss: 0.0008279526955448091 \n",
      "     Training Step: 39 Training Loss: 0.0008236065623350441 \n",
      "     Training Step: 40 Training Loss: 0.0008031112374737859 \n",
      "     Training Step: 41 Training Loss: 0.0008080793777480721 \n",
      "     Training Step: 42 Training Loss: 0.0008063168497756124 \n",
      "     Training Step: 43 Training Loss: 0.0007943939999677241 \n",
      "     Training Step: 44 Training Loss: 0.0007934957975521684 \n",
      "     Training Step: 45 Training Loss: 0.0007861544145271182 \n",
      "     Training Step: 46 Training Loss: 0.0007735984399914742 \n",
      "     Training Step: 47 Training Loss: 0.0007747914060018957 \n",
      "     Training Step: 48 Training Loss: 0.0007584599079564214 \n",
      "     Training Step: 49 Training Loss: 0.000756972236558795 \n",
      "     Training Step: 50 Training Loss: 0.0007540946244262159 \n",
      "     Training Step: 51 Training Loss: 0.0007397481822408736 \n",
      "     Training Step: 52 Training Loss: 0.0007431728299707174 \n",
      "     Training Step: 53 Training Loss: 0.000740912975743413 \n",
      "     Training Step: 54 Training Loss: 0.0007309608045034111 \n",
      "     Training Step: 55 Training Loss: 0.0007142534595914185 \n",
      "     Training Step: 56 Training Loss: 0.000710139109287411 \n",
      "     Training Step: 57 Training Loss: 0.0007178026135079563 \n",
      "     Training Step: 58 Training Loss: 0.0007026151870377362 \n",
      "     Training Step: 59 Training Loss: 0.0007106932462193072 \n",
      "     Training Step: 60 Training Loss: 0.0006971897091716528 \n",
      "     Training Step: 61 Training Loss: 0.0006979652680456638 \n",
      "     Training Step: 62 Training Loss: 0.0006959132733754814 \n",
      "     Training Step: 63 Training Loss: 0.0006867439951747656 \n",
      "     Training Step: 64 Training Loss: 0.0006874239770695567 \n",
      "     Training Step: 65 Training Loss: 0.0006680308724753559 \n",
      "     Training Step: 66 Training Loss: 0.0006665840628556907 \n",
      "     Training Step: 67 Training Loss: 0.0006700551602989435 \n",
      "     Training Step: 68 Training Loss: 0.0006564294453710318 \n",
      "     Training Step: 69 Training Loss: 0.0006520794122479856 \n",
      "     Training Step: 70 Training Loss: 0.0006564460345543921 \n",
      "     Training Step: 71 Training Loss: 0.0006501796888187528 \n",
      "     Training Step: 72 Training Loss: 0.0006468904321081936 \n",
      "     Training Step: 73 Training Loss: 0.000643306178972125 \n",
      "     Training Step: 74 Training Loss: 0.0006378337275236845 \n",
      "     Training Step: 75 Training Loss: 0.0006360533880069852 \n",
      "     Training Step: 76 Training Loss: 0.0006245794938877225 \n",
      "     Training Step: 77 Training Loss: 0.0006304385024122894 \n",
      "     Training Step: 78 Training Loss: 0.0006255918997339904 \n",
      "     Training Step: 79 Training Loss: 0.0006179955089464784 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.0006044692127034068 \n",
      "     Validation Step: 1 Validation Loss: 0.0006193708395585418 \n",
      "     Validation Step: 2 Validation Loss: 0.0006091780960559845 \n",
      "     Validation Step: 3 Validation Loss: 0.0006100577302277088 \n",
      "     Validation Step: 4 Validation Loss: 0.000608944334089756 \n",
      "     Validation Step: 5 Validation Loss: 0.0006169619737192988 \n",
      "     Validation Step: 6 Validation Loss: 0.0006158024771139026 \n",
      "     Validation Step: 7 Validation Loss: 0.0006187057588249445 \n",
      "     Validation Step: 8 Validation Loss: 0.0006119036115705967 \n",
      "     Validation Step: 9 Validation Loss: 0.0006145777879282832 \n",
      "     Validation Step: 10 Validation Loss: 0.000611942436080426 \n",
      "     Validation Step: 11 Validation Loss: 0.0006221546791493893 \n",
      "     Validation Step: 12 Validation Loss: 0.0006179448682814837 \n",
      "     Validation Step: 13 Validation Loss: 0.0006178696639835835 \n",
      "     Validation Step: 14 Validation Loss: 0.0006055735284462571 \n",
      "Epoch: 4\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.000613944313954562 \n",
      "     Training Step: 1 Training Loss: 0.0006096575525589287 \n",
      "     Training Step: 2 Training Loss: 0.000613611307926476 \n",
      "     Training Step: 3 Training Loss: 0.0005995956598781049 \n",
      "     Training Step: 4 Training Loss: 0.0006026310729794204 \n",
      "     Training Step: 5 Training Loss: 0.0005909583414904773 \n",
      "     Training Step: 6 Training Loss: 0.0005944863660261035 \n",
      "     Training Step: 7 Training Loss: 0.0005923105636611581 \n",
      "     Training Step: 8 Training Loss: 0.0005837432108819485 \n",
      "     Training Step: 9 Training Loss: 0.0005818758509121835 \n",
      "     Training Step: 10 Training Loss: 0.0005817721830680966 \n",
      "     Training Step: 11 Training Loss: 0.0005649972590617836 \n",
      "     Training Step: 12 Training Loss: 0.0005673771374858916 \n",
      "     Training Step: 13 Training Loss: 0.0005683279014192522 \n",
      "     Training Step: 14 Training Loss: 0.0005642975447699428 \n",
      "     Training Step: 15 Training Loss: 0.0005627641221508384 \n",
      "     Training Step: 16 Training Loss: 0.0005632074316963553 \n",
      "     Training Step: 17 Training Loss: 0.0005489843315444887 \n",
      "     Training Step: 18 Training Loss: 0.0005510490154847503 \n",
      "     Training Step: 19 Training Loss: 0.0005493247299455106 \n",
      "     Training Step: 20 Training Loss: 0.0005502129206433892 \n",
      "     Training Step: 21 Training Loss: 0.0005387538112699986 \n",
      "     Training Step: 22 Training Loss: 0.0005412271129898727 \n",
      "     Training Step: 23 Training Loss: 0.0005352746229618788 \n",
      "     Training Step: 24 Training Loss: 0.0005346853868104517 \n",
      "     Training Step: 25 Training Loss: 0.000531430880073458 \n",
      "     Training Step: 26 Training Loss: 0.0005325250676833093 \n",
      "     Training Step: 27 Training Loss: 0.0005176261183805764 \n",
      "     Training Step: 28 Training Loss: 0.0005254819989204407 \n",
      "     Training Step: 29 Training Loss: 0.0005202505853958428 \n",
      "     Training Step: 30 Training Loss: 0.0005113395163789392 \n",
      "     Training Step: 31 Training Loss: 0.0005147580523043871 \n",
      "     Training Step: 32 Training Loss: 0.0005184360197745264 \n",
      "     Training Step: 33 Training Loss: 0.000513626029714942 \n",
      "     Training Step: 34 Training Loss: 0.0005055742803961039 \n",
      "     Training Step: 35 Training Loss: 0.000503311341162771 \n",
      "     Training Step: 36 Training Loss: 0.0005021788529120386 \n",
      "     Training Step: 37 Training Loss: 0.0005011461325921118 \n",
      "     Training Step: 38 Training Loss: 0.0004975298652425408 \n",
      "     Training Step: 39 Training Loss: 0.0004932317533530295 \n",
      "     Training Step: 40 Training Loss: 0.0004916329053230584 \n",
      "     Training Step: 41 Training Loss: 0.0004905203240923584 \n",
      "     Training Step: 42 Training Loss: 0.0004890655400231481 \n",
      "     Training Step: 43 Training Loss: 0.0004740522999782115 \n",
      "     Training Step: 44 Training Loss: 0.00047696137335151434 \n",
      "     Training Step: 45 Training Loss: 0.00048147892812266946 \n",
      "     Training Step: 46 Training Loss: 0.00048002906260080636 \n",
      "     Training Step: 47 Training Loss: 0.00047745206393301487 \n",
      "     Training Step: 48 Training Loss: 0.0004722091543953866 \n",
      "     Training Step: 49 Training Loss: 0.0004655118682421744 \n",
      "     Training Step: 50 Training Loss: 0.00046372044016607106 \n",
      "     Training Step: 51 Training Loss: 0.00045867025619372725 \n",
      "     Training Step: 52 Training Loss: 0.0004626510199159384 \n",
      "     Training Step: 53 Training Loss: 0.00046334153739735484 \n",
      "     Training Step: 54 Training Loss: 0.00046137868775986135 \n",
      "     Training Step: 55 Training Loss: 0.0004581717075780034 \n",
      "     Training Step: 56 Training Loss: 0.00045879691606387496 \n",
      "     Training Step: 57 Training Loss: 0.00045431809849105775 \n",
      "     Training Step: 58 Training Loss: 0.0004481604555621743 \n",
      "     Training Step: 59 Training Loss: 0.0004463504010345787 \n",
      "     Training Step: 60 Training Loss: 0.0004452877910807729 \n",
      "     Training Step: 61 Training Loss: 0.00044335643178783357 \n",
      "     Training Step: 62 Training Loss: 0.00045038279495202005 \n",
      "     Training Step: 63 Training Loss: 0.00044142556726001203 \n",
      "     Training Step: 64 Training Loss: 0.00043359340634196997 \n",
      "     Training Step: 65 Training Loss: 0.00044010012061335146 \n",
      "     Training Step: 66 Training Loss: 0.0004255295789334923 \n",
      "     Training Step: 67 Training Loss: 0.0004325410700403154 \n",
      "     Training Step: 68 Training Loss: 0.0004279788990970701 \n",
      "     Training Step: 69 Training Loss: 0.00043375734821893275 \n",
      "     Training Step: 70 Training Loss: 0.0004191047337371856 \n",
      "     Training Step: 71 Training Loss: 0.0004215298977214843 \n",
      "     Training Step: 72 Training Loss: 0.00041841063648462296 \n",
      "     Training Step: 73 Training Loss: 0.0004201927222311497 \n",
      "     Training Step: 74 Training Loss: 0.0004185184952802956 \n",
      "     Training Step: 75 Training Loss: 0.0004181175318080932 \n",
      "     Training Step: 76 Training Loss: 0.0004187571757938713 \n",
      "     Training Step: 77 Training Loss: 0.0004058146441821009 \n",
      "     Training Step: 78 Training Loss: 0.00040713450289331377 \n",
      "     Training Step: 79 Training Loss: 0.0004061058280058205 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.0003997881431132555 \n",
      "     Validation Step: 1 Validation Loss: 0.00041028676787391305 \n",
      "     Validation Step: 2 Validation Loss: 0.00040486198849976063 \n",
      "     Validation Step: 3 Validation Loss: 0.0004110364825464785 \n",
      "     Validation Step: 4 Validation Loss: 0.00040073308628052473 \n",
      "     Validation Step: 5 Validation Loss: 0.00040495162829756737 \n",
      "     Validation Step: 6 Validation Loss: 0.0004075189935974777 \n",
      "     Validation Step: 7 Validation Loss: 0.00041149076423607767 \n",
      "     Validation Step: 8 Validation Loss: 0.0004046152753289789 \n",
      "     Validation Step: 9 Validation Loss: 0.00041368312668055296 \n",
      "     Validation Step: 10 Validation Loss: 0.00040574511513113976 \n",
      "     Validation Step: 11 Validation Loss: 0.0004132028261665255 \n",
      "     Validation Step: 12 Validation Loss: 0.0004094103933311999 \n",
      "     Validation Step: 13 Validation Loss: 0.00040739664109423757 \n",
      "     Validation Step: 14 Validation Loss: 0.0004127249412704259 \n",
      "Epoch: 5\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.00041082935058511794 \n",
      "     Training Step: 1 Training Loss: 0.00040627855923958123 \n",
      "     Training Step: 2 Training Loss: 0.00040609913412481546 \n",
      "     Training Step: 3 Training Loss: 0.0004044276720378548 \n",
      "     Training Step: 4 Training Loss: 0.0003955518186558038 \n",
      "     Training Step: 5 Training Loss: 0.0003904512559529394 \n",
      "     Training Step: 6 Training Loss: 0.00039494765223935246 \n",
      "     Training Step: 7 Training Loss: 0.00039562309393659234 \n",
      "     Training Step: 8 Training Loss: 0.0003977596352342516 \n",
      "     Training Step: 9 Training Loss: 0.0003965322393923998 \n",
      "     Training Step: 10 Training Loss: 0.0003829645866062492 \n",
      "     Training Step: 11 Training Loss: 0.00038819567998871207 \n",
      "     Training Step: 12 Training Loss: 0.0003880635485984385 \n",
      "     Training Step: 13 Training Loss: 0.00038225570460781455 \n",
      "     Training Step: 14 Training Loss: 0.00038319508894346654 \n",
      "     Training Step: 15 Training Loss: 0.0003865346952807158 \n",
      "     Training Step: 16 Training Loss: 0.00038105103885754943 \n",
      "     Training Step: 17 Training Loss: 0.0003677748900372535 \n",
      "     Training Step: 18 Training Loss: 0.0003786349552683532 \n",
      "     Training Step: 19 Training Loss: 0.00037578956107608974 \n",
      "     Training Step: 20 Training Loss: 0.0003709193551912904 \n",
      "     Training Step: 21 Training Loss: 0.0003735680365934968 \n",
      "     Training Step: 22 Training Loss: 0.00037395505933091044 \n",
      "     Training Step: 23 Training Loss: 0.0003739891399163753 \n",
      "     Training Step: 24 Training Loss: 0.0003642347001004964 \n",
      "     Training Step: 25 Training Loss: 0.0003674948529805988 \n",
      "     Training Step: 26 Training Loss: 0.0003620180650614202 \n",
      "     Training Step: 27 Training Loss: 0.000361581624019891 \n",
      "     Training Step: 28 Training Loss: 0.00036402943078428507 \n",
      "     Training Step: 29 Training Loss: 0.0003570126136764884 \n",
      "     Training Step: 30 Training Loss: 0.0003609975101426244 \n",
      "     Training Step: 31 Training Loss: 0.0003581481287255883 \n",
      "     Training Step: 32 Training Loss: 0.00035502322134561837 \n",
      "     Training Step: 33 Training Loss: 0.00036189035745337605 \n",
      "     Training Step: 34 Training Loss: 0.00035373441642150283 \n",
      "     Training Step: 35 Training Loss: 0.0003557204909157008 \n",
      "     Training Step: 36 Training Loss: 0.00035082577960565686 \n",
      "     Training Step: 37 Training Loss: 0.0003412243677303195 \n",
      "     Training Step: 38 Training Loss: 0.0003457223065197468 \n",
      "     Training Step: 39 Training Loss: 0.00034296244848519564 \n",
      "     Training Step: 40 Training Loss: 0.00034524896182119846 \n",
      "     Training Step: 41 Training Loss: 0.00034759240224957466 \n",
      "     Training Step: 42 Training Loss: 0.0003452864184509963 \n",
      "     Training Step: 43 Training Loss: 0.00033590832026675344 \n",
      "     Training Step: 44 Training Loss: 0.00034241043613292277 \n",
      "     Training Step: 45 Training Loss: 0.00034113225410692394 \n",
      "     Training Step: 46 Training Loss: 0.00033785856794565916 \n",
      "     Training Step: 47 Training Loss: 0.0003339183167554438 \n",
      "     Training Step: 48 Training Loss: 0.00033614662243053317 \n",
      "     Training Step: 49 Training Loss: 0.00033457670360803604 \n",
      "     Training Step: 50 Training Loss: 0.00033487798646092415 \n",
      "     Training Step: 51 Training Loss: 0.0003376259992364794 \n",
      "     Training Step: 52 Training Loss: 0.00032290371018461883 \n",
      "     Training Step: 53 Training Loss: 0.0003285574493929744 \n",
      "     Training Step: 54 Training Loss: 0.00033022306161001325 \n",
      "     Training Step: 55 Training Loss: 0.0003248431603424251 \n",
      "     Training Step: 56 Training Loss: 0.0003278255753684789 \n",
      "     Training Step: 57 Training Loss: 0.0003309466119389981 \n",
      "     Training Step: 58 Training Loss: 0.00032875477336347103 \n",
      "     Training Step: 59 Training Loss: 0.00032247434137389064 \n",
      "     Training Step: 60 Training Loss: 0.0003146878443658352 \n",
      "     Training Step: 61 Training Loss: 0.00031908019445836544 \n",
      "     Training Step: 62 Training Loss: 0.0003206537221558392 \n",
      "     Training Step: 63 Training Loss: 0.0003121469053439796 \n",
      "     Training Step: 64 Training Loss: 0.0003130005206912756 \n",
      "     Training Step: 65 Training Loss: 0.00031518973992206156 \n",
      "     Training Step: 66 Training Loss: 0.00031218116055242717 \n",
      "     Training Step: 67 Training Loss: 0.0003157944302074611 \n",
      "     Training Step: 68 Training Loss: 0.00031179215875454247 \n",
      "     Training Step: 69 Training Loss: 0.000312282529193908 \n",
      "     Training Step: 70 Training Loss: 0.0003137621679343283 \n",
      "     Training Step: 71 Training Loss: 0.0003056780551560223 \n",
      "     Training Step: 72 Training Loss: 0.00030728764249943197 \n",
      "     Training Step: 73 Training Loss: 0.0003048201324418187 \n",
      "     Training Step: 74 Training Loss: 0.0003060830058529973 \n",
      "     Training Step: 75 Training Loss: 0.000305972236674279 \n",
      "     Training Step: 76 Training Loss: 0.0003068896767217666 \n",
      "     Training Step: 77 Training Loss: 0.00029946770519018173 \n",
      "     Training Step: 78 Training Loss: 0.0002992427325807512 \n",
      "     Training Step: 79 Training Loss: 0.0003028984647244215 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.0002988574851769954 \n",
      "     Validation Step: 1 Validation Loss: 0.0002984277089126408 \n",
      "     Validation Step: 2 Validation Loss: 0.00030271668219938874 \n",
      "     Validation Step: 3 Validation Loss: 0.0003033170069102198 \n",
      "     Validation Step: 4 Validation Loss: 0.0002930978371296078 \n",
      "     Validation Step: 5 Validation Loss: 0.0003000013530254364 \n",
      "     Validation Step: 6 Validation Loss: 0.00030545180197805166 \n",
      "     Validation Step: 7 Validation Loss: 0.0002938686520792544 \n",
      "     Validation Step: 8 Validation Loss: 0.000302495202049613 \n",
      "     Validation Step: 9 Validation Loss: 0.0003058428701478988 \n",
      "     Validation Step: 10 Validation Loss: 0.0003054787521250546 \n",
      "     Validation Step: 11 Validation Loss: 0.00030374687048606575 \n",
      "     Validation Step: 12 Validation Loss: 0.0002973457449115813 \n",
      "     Validation Step: 13 Validation Loss: 0.00030048051849007607 \n",
      "     Validation Step: 14 Validation Loss: 0.000297820515697822 \n",
      "Epoch: 6\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.0003012866945937276 \n",
      "     Training Step: 1 Training Loss: 0.00029286451172083616 \n",
      "     Training Step: 2 Training Loss: 0.0003040255978703499 \n",
      "     Training Step: 3 Training Loss: 0.0002956269891001284 \n",
      "     Training Step: 4 Training Loss: 0.0002975202223751694 \n",
      "     Training Step: 5 Training Loss: 0.0002945567248389125 \n",
      "     Training Step: 6 Training Loss: 0.0002946921158581972 \n",
      "     Training Step: 7 Training Loss: 0.0002938822144642472 \n",
      "     Training Step: 8 Training Loss: 0.0002923182910308242 \n",
      "     Training Step: 9 Training Loss: 0.00029386713868007064 \n",
      "     Training Step: 10 Training Loss: 0.00028992717852815986 \n",
      "     Training Step: 11 Training Loss: 0.0002899952232837677 \n",
      "     Training Step: 12 Training Loss: 0.00028872606344521046 \n",
      "     Training Step: 13 Training Loss: 0.00028925377409905195 \n",
      "     Training Step: 14 Training Loss: 0.0002896988298743963 \n",
      "     Training Step: 15 Training Loss: 0.00029162265127524734 \n",
      "     Training Step: 16 Training Loss: 0.00028368859784677625 \n",
      "     Training Step: 17 Training Loss: 0.0002857572981156409 \n",
      "     Training Step: 18 Training Loss: 0.00028629854205064476 \n",
      "     Training Step: 19 Training Loss: 0.0002850033633876592 \n",
      "     Training Step: 20 Training Loss: 0.0002824206021614373 \n",
      "     Training Step: 21 Training Loss: 0.00028452411061152816 \n",
      "     Training Step: 22 Training Loss: 0.000277467945124954 \n",
      "     Training Step: 23 Training Loss: 0.00027463570586405694 \n",
      "     Training Step: 24 Training Loss: 0.0002827300922945142 \n",
      "     Training Step: 25 Training Loss: 0.00027476411196403205 \n",
      "     Training Step: 26 Training Loss: 0.0002836953499354422 \n",
      "     Training Step: 27 Training Loss: 0.0002793624298647046 \n",
      "     Training Step: 28 Training Loss: 0.00027705251704901457 \n",
      "     Training Step: 29 Training Loss: 0.0002755350433290005 \n",
      "     Training Step: 30 Training Loss: 0.0002789629506878555 \n",
      "     Training Step: 31 Training Loss: 0.0002729557454586029 \n",
      "     Training Step: 32 Training Loss: 0.00027014946681447327 \n",
      "     Training Step: 33 Training Loss: 0.00026390099083073437 \n",
      "     Training Step: 34 Training Loss: 0.00026466604322195053 \n",
      "     Training Step: 35 Training Loss: 0.00026848173001781106 \n",
      "     Training Step: 36 Training Loss: 0.0002719378098845482 \n",
      "     Training Step: 37 Training Loss: 0.0002698046446312219 \n",
      "     Training Step: 38 Training Loss: 0.00027211004635319114 \n",
      "     Training Step: 39 Training Loss: 0.00027134083211421967 \n",
      "     Training Step: 40 Training Loss: 0.00026933749904856086 \n",
      "     Training Step: 41 Training Loss: 0.0002658049634192139 \n",
      "     Training Step: 42 Training Loss: 0.0002617466379888356 \n",
      "     Training Step: 43 Training Loss: 0.00026644172612577677 \n",
      "     Training Step: 44 Training Loss: 0.0002602780587039888 \n",
      "     Training Step: 45 Training Loss: 0.0002627850917633623 \n",
      "     Training Step: 46 Training Loss: 0.0002666675136424601 \n",
      "     Training Step: 47 Training Loss: 0.00026342301862314343 \n",
      "     Training Step: 48 Training Loss: 0.00026058644289150834 \n",
      "     Training Step: 49 Training Loss: 0.00026345555670559406 \n",
      "     Training Step: 50 Training Loss: 0.0002622557512950152 \n",
      "     Training Step: 51 Training Loss: 0.00025546568213030696 \n",
      "     Training Step: 52 Training Loss: 0.0002601438609417528 \n",
      "     Training Step: 53 Training Loss: 0.0002591008960735053 \n",
      "     Training Step: 54 Training Loss: 0.0002590063086245209 \n",
      "     Training Step: 55 Training Loss: 0.00025844492483884096 \n",
      "     Training Step: 56 Training Loss: 0.00025811215164139867 \n",
      "     Training Step: 57 Training Loss: 0.00025665893917903304 \n",
      "     Training Step: 58 Training Loss: 0.0002510838385205716 \n",
      "     Training Step: 59 Training Loss: 0.00025344701134599745 \n",
      "     Training Step: 60 Training Loss: 0.0002462724514771253 \n",
      "     Training Step: 61 Training Loss: 0.00024664076045155525 \n",
      "     Training Step: 62 Training Loss: 0.00025046925293281674 \n",
      "     Training Step: 63 Training Loss: 0.00024893920635804534 \n",
      "     Training Step: 64 Training Loss: 0.00025167802232317626 \n",
      "     Training Step: 65 Training Loss: 0.00024926415062509477 \n",
      "     Training Step: 66 Training Loss: 0.0002468221646267921 \n",
      "     Training Step: 67 Training Loss: 0.0002502113929949701 \n",
      "     Training Step: 68 Training Loss: 0.0002485025906935334 \n",
      "     Training Step: 69 Training Loss: 0.0002499913680367172 \n",
      "     Training Step: 70 Training Loss: 0.0002446392609272152 \n",
      "     Training Step: 71 Training Loss: 0.00023806675744708627 \n",
      "     Training Step: 72 Training Loss: 0.0002451100153848529 \n",
      "     Training Step: 73 Training Loss: 0.00024423919967375696 \n",
      "     Training Step: 74 Training Loss: 0.000242300215177238 \n",
      "     Training Step: 75 Training Loss: 0.00024099164875224233 \n",
      "     Training Step: 76 Training Loss: 0.00023850257275626063 \n",
      "     Training Step: 77 Training Loss: 0.0002432223263895139 \n",
      "     Training Step: 78 Training Loss: 0.0002449406892992556 \n",
      "     Training Step: 79 Training Loss: 0.00024186880909837782 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.00023957653320394456 \n",
      "     Validation Step: 1 Validation Loss: 0.00024273373128380626 \n",
      "     Validation Step: 2 Validation Loss: 0.00024103448959067464 \n",
      "     Validation Step: 3 Validation Loss: 0.00024321157252416015 \n",
      "     Validation Step: 4 Validation Loss: 0.00023470043379347771 \n",
      "     Validation Step: 5 Validation Loss: 0.00024277875490952283 \n",
      "     Validation Step: 6 Validation Loss: 0.0002380610239924863 \n",
      "     Validation Step: 7 Validation Loss: 0.0002451344334986061 \n",
      "     Validation Step: 8 Validation Loss: 0.00023931296891532838 \n",
      "     Validation Step: 9 Validation Loss: 0.00024482287699356675 \n",
      "     Validation Step: 10 Validation Loss: 0.00024383130948990583 \n",
      "     Validation Step: 11 Validation Loss: 0.0002451991895213723 \n",
      "     Validation Step: 12 Validation Loss: 0.00023888336727395654 \n",
      "     Validation Step: 13 Validation Loss: 0.00023757611052133143 \n",
      "     Validation Step: 14 Validation Loss: 0.00023406470427289605 \n",
      "Epoch: 7\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.0002433718036627397 \n",
      "     Training Step: 1 Training Loss: 0.00023686156782787293 \n",
      "     Training Step: 2 Training Loss: 0.0002408754953648895 \n",
      "     Training Step: 3 Training Loss: 0.0002407583815511316 \n",
      "     Training Step: 4 Training Loss: 0.00023171678185462952 \n",
      "     Training Step: 5 Training Loss: 0.00022886210354045033 \n",
      "     Training Step: 6 Training Loss: 0.00023791806597728282 \n",
      "     Training Step: 7 Training Loss: 0.00023760423937346786 \n",
      "     Training Step: 8 Training Loss: 0.00023151248751673847 \n",
      "     Training Step: 9 Training Loss: 0.0002401061647105962 \n",
      "     Training Step: 10 Training Loss: 0.0002367468987358734 \n",
      "     Training Step: 11 Training Loss: 0.00023450898879673332 \n",
      "     Training Step: 12 Training Loss: 0.0002348074922338128 \n",
      "     Training Step: 13 Training Loss: 0.00023854512255638838 \n",
      "     Training Step: 14 Training Loss: 0.00022661004913970828 \n",
      "     Training Step: 15 Training Loss: 0.00023318832973018289 \n",
      "     Training Step: 16 Training Loss: 0.00023761644843034446 \n",
      "     Training Step: 17 Training Loss: 0.00023169087944552302 \n",
      "     Training Step: 18 Training Loss: 0.00023301172768697143 \n",
      "     Training Step: 19 Training Loss: 0.00023291082470677793 \n",
      "     Training Step: 20 Training Loss: 0.0002260193577967584 \n",
      "     Training Step: 21 Training Loss: 0.00022520982020068914 \n",
      "     Training Step: 22 Training Loss: 0.0002297661849297583 \n",
      "     Training Step: 23 Training Loss: 0.0002308189432369545 \n",
      "     Training Step: 24 Training Loss: 0.00022896815789863467 \n",
      "     Training Step: 25 Training Loss: 0.00022689987963531166 \n",
      "     Training Step: 26 Training Loss: 0.0002262801572214812 \n",
      "     Training Step: 27 Training Loss: 0.00021937908604741096 \n",
      "     Training Step: 28 Training Loss: 0.0002251790720038116 \n",
      "     Training Step: 29 Training Loss: 0.00022753456141799688 \n",
      "     Training Step: 30 Training Loss: 0.000222476024646312 \n",
      "     Training Step: 31 Training Loss: 0.00022270463523454964 \n",
      "     Training Step: 32 Training Loss: 0.00022446890943683684 \n",
      "     Training Step: 33 Training Loss: 0.00021917879348620772 \n",
      "     Training Step: 34 Training Loss: 0.00022171781165525317 \n",
      "     Training Step: 35 Training Loss: 0.00022459361935034394 \n",
      "     Training Step: 36 Training Loss: 0.00021699711214751005 \n",
      "     Training Step: 37 Training Loss: 0.0002204676129622385 \n",
      "     Training Step: 38 Training Loss: 0.00022055732551962137 \n",
      "     Training Step: 39 Training Loss: 0.00021382534760050476 \n",
      "     Training Step: 40 Training Loss: 0.00021470451611094177 \n",
      "     Training Step: 41 Training Loss: 0.00021960449521429837 \n",
      "     Training Step: 42 Training Loss: 0.0002158029528800398 \n",
      "     Training Step: 43 Training Loss: 0.00021976717107463628 \n",
      "     Training Step: 44 Training Loss: 0.0002192684478359297 \n",
      "     Training Step: 45 Training Loss: 0.00021403565187938511 \n",
      "     Training Step: 46 Training Loss: 0.00021884927991777658 \n",
      "     Training Step: 47 Training Loss: 0.00021667740656994283 \n",
      "     Training Step: 48 Training Loss: 0.00021245460084173828 \n",
      "     Training Step: 49 Training Loss: 0.0002140098949894309 \n",
      "     Training Step: 50 Training Loss: 0.00021337514044716954 \n",
      "     Training Step: 51 Training Loss: 0.00021614540310110897 \n",
      "     Training Step: 52 Training Loss: 0.00021582184126600623 \n",
      "     Training Step: 53 Training Loss: 0.0002143158344551921 \n",
      "     Training Step: 54 Training Loss: 0.0002169106446672231 \n",
      "     Training Step: 55 Training Loss: 0.00021533272229135036 \n",
      "     Training Step: 56 Training Loss: 0.00020887631399091333 \n",
      "     Training Step: 57 Training Loss: 0.00021333788754418492 \n",
      "     Training Step: 58 Training Loss: 0.00021328790171537548 \n",
      "     Training Step: 59 Training Loss: 0.00021082584862597287 \n",
      "     Training Step: 60 Training Loss: 0.00020903271797578782 \n",
      "     Training Step: 61 Training Loss: 0.00021073341486044228 \n",
      "     Training Step: 62 Training Loss: 0.00021266809199005365 \n",
      "     Training Step: 63 Training Loss: 0.0002096261887345463 \n",
      "     Training Step: 64 Training Loss: 0.00020431334269233048 \n",
      "     Training Step: 65 Training Loss: 0.00020066826255060732 \n",
      "     Training Step: 66 Training Loss: 0.000203870382392779 \n",
      "     Training Step: 67 Training Loss: 0.0002069720794679597 \n",
      "     Training Step: 68 Training Loss: 0.00020871525339316577 \n",
      "     Training Step: 69 Training Loss: 0.00020944127754773945 \n",
      "     Training Step: 70 Training Loss: 0.0002061663253698498 \n",
      "     Training Step: 71 Training Loss: 0.00020838344062212855 \n",
      "     Training Step: 72 Training Loss: 0.00020329811377450824 \n",
      "     Training Step: 73 Training Loss: 0.00020356784807518125 \n",
      "     Training Step: 74 Training Loss: 0.00020233768736943603 \n",
      "     Training Step: 75 Training Loss: 0.00020487175788730383 \n",
      "     Training Step: 76 Training Loss: 0.00020411223522387445 \n",
      "     Training Step: 77 Training Loss: 0.0002024780260398984 \n",
      "     Training Step: 78 Training Loss: 0.00019967550178989768 \n",
      "     Training Step: 79 Training Loss: 0.00020332414715085179 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.0002042825217358768 \n",
      "     Validation Step: 1 Validation Loss: 0.00020082273113075644 \n",
      "     Validation Step: 2 Validation Loss: 0.00019573424651753157 \n",
      "     Validation Step: 3 Validation Loss: 0.0001991804747376591 \n",
      "     Validation Step: 4 Validation Loss: 0.00020560779375955462 \n",
      "     Validation Step: 5 Validation Loss: 0.00020018441136926413 \n",
      "     Validation Step: 6 Validation Loss: 0.00020384398521855474 \n",
      "     Validation Step: 7 Validation Loss: 0.00020617486734408885 \n",
      "     Validation Step: 8 Validation Loss: 0.00019877494196407497 \n",
      "     Validation Step: 9 Validation Loss: 0.0002025229186983779 \n",
      "     Validation Step: 10 Validation Loss: 0.0002039488754235208 \n",
      "     Validation Step: 11 Validation Loss: 0.0002010034368140623 \n",
      "     Validation Step: 12 Validation Loss: 0.00020558903634082526 \n",
      "     Validation Step: 13 Validation Loss: 0.0001965477567864582 \n",
      "     Validation Step: 14 Validation Loss: 0.00020495429635047913 \n",
      "Epoch: 8\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.00019643624546006322 \n",
      "     Training Step: 1 Training Loss: 0.00019806568161584437 \n",
      "     Training Step: 2 Training Loss: 0.00020146314636804163 \n",
      "     Training Step: 3 Training Loss: 0.0002030212344834581 \n",
      "     Training Step: 4 Training Loss: 0.0001957306230906397 \n",
      "     Training Step: 5 Training Loss: 0.00019401049939915538 \n",
      "     Training Step: 6 Training Loss: 0.00020040484378114343 \n",
      "     Training Step: 7 Training Loss: 0.0001950872829183936 \n",
      "     Training Step: 8 Training Loss: 0.00019937389879487455 \n",
      "     Training Step: 9 Training Loss: 0.0001948420685948804 \n",
      "     Training Step: 10 Training Loss: 0.00019883873756043613 \n",
      "     Training Step: 11 Training Loss: 0.0002020406536757946 \n",
      "     Training Step: 12 Training Loss: 0.00019881437765434384 \n",
      "     Training Step: 13 Training Loss: 0.00019731867359951138 \n",
      "     Training Step: 14 Training Loss: 0.00019364985928405076 \n",
      "     Training Step: 15 Training Loss: 0.00019504220108501613 \n",
      "     Training Step: 16 Training Loss: 0.00019875523867085576 \n",
      "     Training Step: 17 Training Loss: 0.0001931928563863039 \n",
      "     Training Step: 18 Training Loss: 0.0001955506158992648 \n",
      "     Training Step: 19 Training Loss: 0.00019624995184130967 \n",
      "     Training Step: 20 Training Loss: 0.00019559968495741487 \n",
      "     Training Step: 21 Training Loss: 0.00019554933533072472 \n",
      "     Training Step: 22 Training Loss: 0.00019644017447717488 \n",
      "     Training Step: 23 Training Loss: 0.00018908531637862325 \n",
      "     Training Step: 24 Training Loss: 0.00018885319877881557 \n",
      "     Training Step: 25 Training Loss: 0.0001885909296106547 \n",
      "     Training Step: 26 Training Loss: 0.00019571336451917887 \n",
      "     Training Step: 27 Training Loss: 0.00018596525478642434 \n",
      "     Training Step: 28 Training Loss: 0.00018500935402698815 \n",
      "     Training Step: 29 Training Loss: 0.00018800899852067232 \n",
      "     Training Step: 30 Training Loss: 0.0001934120082296431 \n",
      "     Training Step: 31 Training Loss: 0.00019233571947552264 \n",
      "     Training Step: 32 Training Loss: 0.00018738556536845863 \n",
      "     Training Step: 33 Training Loss: 0.0001836615556385368 \n",
      "     Training Step: 34 Training Loss: 0.0001917713088914752 \n",
      "     Training Step: 35 Training Loss: 0.00019064151274506003 \n",
      "     Training Step: 36 Training Loss: 0.00018885795725509524 \n",
      "     Training Step: 37 Training Loss: 0.0001891965657705441 \n",
      "     Training Step: 38 Training Loss: 0.00019103467639070004 \n",
      "     Training Step: 39 Training Loss: 0.00018435483798384666 \n",
      "     Training Step: 40 Training Loss: 0.00018710184667725116 \n",
      "     Training Step: 41 Training Loss: 0.00018778422963805497 \n",
      "     Training Step: 42 Training Loss: 0.00018799054669216275 \n",
      "     Training Step: 43 Training Loss: 0.00018782756524160504 \n",
      "     Training Step: 44 Training Loss: 0.00018626121163833886 \n",
      "     Training Step: 45 Training Loss: 0.00019078011973761022 \n",
      "     Training Step: 46 Training Loss: 0.00018637551693245769 \n",
      "     Training Step: 47 Training Loss: 0.00018635138985700905 \n",
      "     Training Step: 48 Training Loss: 0.00018654770974535495 \n",
      "     Training Step: 49 Training Loss: 0.0001851667184382677 \n",
      "     Training Step: 50 Training Loss: 0.00017810333520174026 \n",
      "     Training Step: 51 Training Loss: 0.00018703422392718494 \n",
      "     Training Step: 52 Training Loss: 0.00017827928240876645 \n",
      "     Training Step: 53 Training Loss: 0.00017627430497668684 \n",
      "     Training Step: 54 Training Loss: 0.00018306783749721944 \n",
      "     Training Step: 55 Training Loss: 0.0001830442633945495 \n",
      "     Training Step: 56 Training Loss: 0.00018501063459552824 \n",
      "     Training Step: 57 Training Loss: 0.00018277883646078408 \n",
      "     Training Step: 58 Training Loss: 0.00018302071839571 \n",
      "     Training Step: 59 Training Loss: 0.0001834691211115569 \n",
      "     Training Step: 60 Training Loss: 0.00017767083772923797 \n",
      "     Training Step: 61 Training Loss: 0.00017971510533243418 \n",
      "     Training Step: 62 Training Loss: 0.00018227112013846636 \n",
      "     Training Step: 63 Training Loss: 0.00017684968770481646 \n",
      "     Training Step: 64 Training Loss: 0.00018065026961266994 \n",
      "     Training Step: 65 Training Loss: 0.00017793654114939272 \n",
      "     Training Step: 66 Training Loss: 0.00018106045899912715 \n",
      "     Training Step: 67 Training Loss: 0.0001751258096192032 \n",
      "     Training Step: 68 Training Loss: 0.0001754453405737877 \n",
      "     Training Step: 69 Training Loss: 0.00018009758787229657 \n",
      "     Training Step: 70 Training Loss: 0.00018014138913713396 \n",
      "     Training Step: 71 Training Loss: 0.00017296650912612677 \n",
      "     Training Step: 72 Training Loss: 0.0001794426643755287 \n",
      "     Training Step: 73 Training Loss: 0.00017757629393599927 \n",
      "     Training Step: 74 Training Loss: 0.00017330984701402485 \n",
      "     Training Step: 75 Training Loss: 0.00017803898663260043 \n",
      "     Training Step: 76 Training Loss: 0.00017853397002909333 \n",
      "     Training Step: 77 Training Loss: 0.00017700820171739906 \n",
      "     Training Step: 78 Training Loss: 0.0001744820037856698 \n",
      "     Training Step: 79 Training Loss: 0.00017757344176061451 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.00017335846496280283 \n",
      "     Validation Step: 1 Validation Loss: 0.0001782284234650433 \n",
      "     Validation Step: 2 Validation Loss: 0.00017007195856422186 \n",
      "     Validation Step: 3 Validation Loss: 0.00017184927128255367 \n",
      "     Validation Step: 4 Validation Loss: 0.00017851150187198073 \n",
      "     Validation Step: 5 Validation Loss: 0.0001772587129380554 \n",
      "     Validation Step: 6 Validation Loss: 0.00017931117326952517 \n",
      "     Validation Step: 7 Validation Loss: 0.00017434850451536477 \n",
      "     Validation Step: 8 Validation Loss: 0.00017799041233956814 \n",
      "     Validation Step: 9 Validation Loss: 0.00017327362729702145 \n",
      "     Validation Step: 10 Validation Loss: 0.00017628961359150708 \n",
      "     Validation Step: 11 Validation Loss: 0.0001686952164163813 \n",
      "     Validation Step: 12 Validation Loss: 0.00017560874402988702 \n",
      "     Validation Step: 13 Validation Loss: 0.00017203138850163668 \n",
      "     Validation Step: 14 Validation Loss: 0.00017722544725984335 \n",
      "Epoch: 9\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.00017162473523057997 \n",
      "     Training Step: 1 Training Loss: 0.00017825287068262696 \n",
      "     Training Step: 2 Training Loss: 0.0001792106486391276 \n",
      "     Training Step: 3 Training Loss: 0.00017504006973467767 \n",
      "     Training Step: 4 Training Loss: 0.00017569190822541714 \n",
      "     Training Step: 5 Training Loss: 0.0001745161134749651 \n",
      "     Training Step: 6 Training Loss: 0.00017242789908777922 \n",
      "     Training Step: 7 Training Loss: 0.00017400638898834586 \n",
      "     Training Step: 8 Training Loss: 0.00017443920660298318 \n",
      "     Training Step: 9 Training Loss: 0.000166284415172413 \n",
      "     Training Step: 10 Training Loss: 0.00017256225692108274 \n",
      "     Training Step: 11 Training Loss: 0.0001752396347001195 \n",
      "     Training Step: 12 Training Loss: 0.00017298420425504446 \n",
      "     Training Step: 13 Training Loss: 0.00016867846716195345 \n",
      "     Training Step: 14 Training Loss: 0.00016780811711214483 \n",
      "     Training Step: 15 Training Loss: 0.00016921377391554415 \n",
      "     Training Step: 16 Training Loss: 0.00017138943076133728 \n",
      "     Training Step: 17 Training Loss: 0.0001724771864246577 \n",
      "     Training Step: 18 Training Loss: 0.00016616200446151197 \n",
      "     Training Step: 19 Training Loss: 0.00017187604680657387 \n",
      "     Training Step: 20 Training Loss: 0.00017061521066352725 \n",
      "     Training Step: 21 Training Loss: 0.00016677485837135464 \n",
      "     Training Step: 22 Training Loss: 0.00016468761896248907 \n",
      "     Training Step: 23 Training Loss: 0.00017123034922406077 \n",
      "     Training Step: 24 Training Loss: 0.00016711675561964512 \n",
      "     Training Step: 25 Training Loss: 0.00016839509771671146 \n",
      "     Training Step: 26 Training Loss: 0.00016609154408797622 \n",
      "     Training Step: 27 Training Loss: 0.00016345105541404337 \n",
      "     Training Step: 28 Training Loss: 0.00016960667562671006 \n",
      "     Training Step: 29 Training Loss: 0.00016750278882682323 \n",
      "     Training Step: 30 Training Loss: 0.0001633362116990611 \n",
      "     Training Step: 31 Training Loss: 0.0001626430166652426 \n",
      "     Training Step: 32 Training Loss: 0.00016744922322686762 \n",
      "     Training Step: 33 Training Loss: 0.00016855946159921587 \n",
      "     Training Step: 34 Training Loss: 0.00016831938410177827 \n",
      "     Training Step: 35 Training Loss: 0.00015922944294288754 \n",
      "     Training Step: 36 Training Loss: 0.000167312246048823 \n",
      "     Training Step: 37 Training Loss: 0.00016063176735769957 \n",
      "     Training Step: 38 Training Loss: 0.00015961482131388038 \n",
      "     Training Step: 39 Training Loss: 0.00016691305791027844 \n",
      "     Training Step: 40 Training Loss: 0.00016559743380639702 \n",
      "     Training Step: 41 Training Loss: 0.0001652026257943362 \n",
      "     Training Step: 42 Training Loss: 0.00015934955445118248 \n",
      "     Training Step: 43 Training Loss: 0.00016423864872194827 \n",
      "     Training Step: 44 Training Loss: 0.00015765819989610463 \n",
      "     Training Step: 45 Training Loss: 0.0001595771755091846 \n",
      "     Training Step: 46 Training Loss: 0.00016501465870533139 \n",
      "     Training Step: 47 Training Loss: 0.0001580486714374274 \n",
      "     Training Step: 48 Training Loss: 0.0001641279668547213 \n",
      "     Training Step: 49 Training Loss: 0.00016409176168963313 \n",
      "     Training Step: 50 Training Loss: 0.00015614459698554128 \n",
      "     Training Step: 51 Training Loss: 0.00016130993026308715 \n",
      "     Training Step: 52 Training Loss: 0.0001637583482079208 \n",
      "     Training Step: 53 Training Loss: 0.00016483053332194686 \n",
      "     Training Step: 54 Training Loss: 0.0001621098635951057 \n",
      "     Training Step: 55 Training Loss: 0.00015475304098799825 \n",
      "     Training Step: 56 Training Loss: 0.00016286598111037165 \n",
      "     Training Step: 57 Training Loss: 0.0001547986757941544 \n",
      "     Training Step: 58 Training Loss: 0.00015686859842389822 \n",
      "     Training Step: 59 Training Loss: 0.0001623839052626863 \n",
      "     Training Step: 60 Training Loss: 0.00016131188021972775 \n",
      "     Training Step: 61 Training Loss: 0.00015598544268868864 \n",
      "     Training Step: 62 Training Loss: 0.00016127992421388626 \n",
      "     Training Step: 63 Training Loss: 0.00015864029410295188 \n",
      "     Training Step: 64 Training Loss: 0.00016010567196644843 \n",
      "     Training Step: 65 Training Loss: 0.00015551986871287227 \n",
      "     Training Step: 66 Training Loss: 0.00015352734772022814 \n",
      "     Training Step: 67 Training Loss: 0.00015469788922928274 \n",
      "     Training Step: 68 Training Loss: 0.00015348651504609734 \n",
      "     Training Step: 69 Training Loss: 0.00016017406596802175 \n",
      "     Training Step: 70 Training Loss: 0.00015989228268153965 \n",
      "     Training Step: 71 Training Loss: 0.00015253252058755606 \n",
      "     Training Step: 72 Training Loss: 0.00015879263810347766 \n",
      "     Training Step: 73 Training Loss: 0.00015822768909856677 \n",
      "     Training Step: 74 Training Loss: 0.00015940718003548682 \n",
      "     Training Step: 75 Training Loss: 0.00015751011960674077 \n",
      "     Training Step: 76 Training Loss: 0.00015351377078332007 \n",
      "     Training Step: 77 Training Loss: 0.0001568490406498313 \n",
      "     Training Step: 78 Training Loss: 0.00015759763482492417 \n",
      "     Training Step: 79 Training Loss: 0.00015660436474718153 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.00015729977167211473 \n",
      "     Validation Step: 1 Validation Loss: 0.0001551572058815509 \n",
      "     Validation Step: 2 Validation Loss: 0.00014943511632736772 \n",
      "     Validation Step: 3 Validation Loss: 0.000156297319335863 \n",
      "     Validation Step: 4 Validation Loss: 0.0001574014313519001 \n",
      "     Validation Step: 5 Validation Loss: 0.00015345601423177868 \n",
      "     Validation Step: 6 Validation Loss: 0.0001572724140714854 \n",
      "     Validation Step: 7 Validation Loss: 0.00015874739619903266 \n",
      "     Validation Step: 8 Validation Loss: 0.00015003568842075765 \n",
      "     Validation Step: 9 Validation Loss: 0.00015644426457583904 \n",
      "     Validation Step: 10 Validation Loss: 0.00015121926844585687 \n",
      "     Validation Step: 11 Validation Loss: 0.00015116571739781648 \n",
      "     Validation Step: 12 Validation Loss: 0.00015216125757433474 \n",
      "     Validation Step: 13 Validation Loss: 0.00015504902694374323 \n",
      "     Validation Step: 14 Validation Loss: 0.0001575023343320936 \n",
      "Epoch: 10\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.00015044848260004073 \n",
      "     Training Step: 1 Training Loss: 0.00015718719805590808 \n",
      "     Training Step: 2 Training Loss: 0.000152879991219379 \n",
      "     Training Step: 3 Training Loss: 0.00015637421165592968 \n",
      "     Training Step: 4 Training Loss: 0.00015301040548365563 \n",
      "     Training Step: 5 Training Loss: 0.0001514236064394936 \n",
      "     Training Step: 6 Training Loss: 0.00015619304031133652 \n",
      "     Training Step: 7 Training Loss: 0.00014945186558179557 \n",
      "     Training Step: 8 Training Loss: 0.00015526915376540273 \n",
      "     Training Step: 9 Training Loss: 0.00015489743964280933 \n",
      "     Training Step: 10 Training Loss: 0.00015061743033584207 \n",
      "     Training Step: 11 Training Loss: 0.0001485594839323312 \n",
      "     Training Step: 12 Training Loss: 0.00015450608043465763 \n",
      "     Training Step: 13 Training Loss: 0.00015517283463850617 \n",
      "     Training Step: 14 Training Loss: 0.0001537567877676338 \n",
      "     Training Step: 15 Training Loss: 0.00014792285219300538 \n",
      "     Training Step: 16 Training Loss: 0.0001530496374471113 \n",
      "     Training Step: 17 Training Loss: 0.00015258448547683656 \n",
      "     Training Step: 18 Training Loss: 0.00015189153782557696 \n",
      "     Training Step: 19 Training Loss: 0.00014799523341935128 \n",
      "     Training Step: 20 Training Loss: 0.00015061732847243547 \n",
      "     Training Step: 21 Training Loss: 0.0001524608233012259 \n",
      "     Training Step: 22 Training Loss: 0.0001515081967227161 \n",
      "     Training Step: 23 Training Loss: 0.00015110234380699694 \n",
      "     Training Step: 24 Training Loss: 0.00014463672414422035 \n",
      "     Training Step: 25 Training Loss: 0.0001453281583962962 \n",
      "     Training Step: 26 Training Loss: 0.00015137242735363543 \n",
      "     Training Step: 27 Training Loss: 0.00015318109944928437 \n",
      "     Training Step: 28 Training Loss: 0.00014525113510899246 \n",
      "     Training Step: 29 Training Loss: 0.00015072028327267617 \n",
      "     Training Step: 30 Training Loss: 0.0001507617998868227 \n",
      "     Training Step: 31 Training Loss: 0.00014902523253113031 \n",
      "     Training Step: 32 Training Loss: 0.00014273203851189464 \n",
      "     Training Step: 33 Training Loss: 0.0001490141439717263 \n",
      "     Training Step: 34 Training Loss: 0.00014635879779234529 \n",
      "     Training Step: 35 Training Loss: 0.00014974713849369437 \n",
      "     Training Step: 36 Training Loss: 0.00014928061864338815 \n",
      "     Training Step: 37 Training Loss: 0.0001498120545875281 \n",
      "     Training Step: 38 Training Loss: 0.00014177928096614778 \n",
      "     Training Step: 39 Training Loss: 0.00014868428115732968 \n",
      "     Training Step: 40 Training Loss: 0.00014942711277399212 \n",
      "     Training Step: 41 Training Loss: 0.00014410448784474283 \n",
      "     Training Step: 42 Training Loss: 0.00014172498777043074 \n",
      "     Training Step: 43 Training Loss: 0.00014834050671197474 \n",
      "     Training Step: 44 Training Loss: 0.00014742405619472265 \n",
      "     Training Step: 45 Training Loss: 0.0001403124915668741 \n",
      "     Training Step: 46 Training Loss: 0.0001416646409779787 \n",
      "     Training Step: 47 Training Loss: 0.00014700862811878324 \n",
      "     Training Step: 48 Training Loss: 0.00014734212891198695 \n",
      "     Training Step: 49 Training Loss: 0.00013879559992346913 \n",
      "     Training Step: 50 Training Loss: 0.0001419599720975384 \n",
      "     Training Step: 51 Training Loss: 0.0001405895163770765 \n",
      "     Training Step: 52 Training Loss: 0.0001403411733917892 \n",
      "     Training Step: 53 Training Loss: 0.00014022114919498563 \n",
      "     Training Step: 54 Training Loss: 0.00014771246060263366 \n",
      "     Training Step: 55 Training Loss: 0.00014576489047612995 \n",
      "     Training Step: 56 Training Loss: 0.00014151765208225697 \n",
      "     Training Step: 57 Training Loss: 0.0001445971429347992 \n",
      "     Training Step: 58 Training Loss: 0.00014528530300594866 \n",
      "     Training Step: 59 Training Loss: 0.000138105358928442 \n",
      "     Training Step: 60 Training Loss: 0.00013895888696424663 \n",
      "     Training Step: 61 Training Loss: 0.00014521687990054488 \n",
      "     Training Step: 62 Training Loss: 0.00013966229744255543 \n",
      "     Training Step: 63 Training Loss: 0.0001444366353098303 \n",
      "     Training Step: 64 Training Loss: 0.00013637306983582675 \n",
      "     Training Step: 65 Training Loss: 0.00014570483472198248 \n",
      "     Training Step: 66 Training Loss: 0.00014440080849453807 \n",
      "     Training Step: 67 Training Loss: 0.00014434302283916622 \n",
      "     Training Step: 68 Training Loss: 0.00013816342107020319 \n",
      "     Training Step: 69 Training Loss: 0.0001437792816432193 \n",
      "     Training Step: 70 Training Loss: 0.0001433542201993987 \n",
      "     Training Step: 71 Training Loss: 0.0001366875076200813 \n",
      "     Training Step: 72 Training Loss: 0.00014256767462939024 \n",
      "     Training Step: 73 Training Loss: 0.0001435478188795969 \n",
      "     Training Step: 74 Training Loss: 0.0001358640001853928 \n",
      "     Training Step: 75 Training Loss: 0.00014171935617923737 \n",
      "     Training Step: 76 Training Loss: 0.00014170650683809072 \n",
      "     Training Step: 77 Training Loss: 0.00014104829460848123 \n",
      "     Training Step: 78 Training Loss: 0.0001352705294266343 \n",
      "     Training Step: 79 Training Loss: 0.00014046335127204657 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.0001415196165908128 \n",
      "     Validation Step: 1 Validation Loss: 0.00014157082478050143 \n",
      "     Validation Step: 2 Validation Loss: 0.00013455163571052253 \n",
      "     Validation Step: 3 Validation Loss: 0.00014117854880169034 \n",
      "     Validation Step: 4 Validation Loss: 0.00014022624236531556 \n",
      "     Validation Step: 5 Validation Loss: 0.000138993957079947 \n",
      "     Validation Step: 6 Validation Loss: 0.0001362926559522748 \n",
      "     Validation Step: 7 Validation Loss: 0.00013593462062999606 \n",
      "     Validation Step: 8 Validation Loss: 0.00013516354374587536 \n",
      "     Validation Step: 9 Validation Loss: 0.00013896416930947453 \n",
      "     Validation Step: 10 Validation Loss: 0.00013878628669772297 \n",
      "     Validation Step: 11 Validation Loss: 0.00014193053357303143 \n",
      "     Validation Step: 12 Validation Loss: 0.00014193516108207405 \n",
      "     Validation Step: 13 Validation Loss: 0.00013477829634211957 \n",
      "     Validation Step: 14 Validation Loss: 0.00013527434202842414 \n",
      "Epoch: 11\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.00013521689106710255 \n",
      "     Training Step: 1 Training Loss: 0.00013502089132089168 \n",
      "     Training Step: 2 Training Loss: 0.00014103793364483863 \n",
      "     Training Step: 3 Training Loss: 0.0001374360581394285 \n",
      "     Training Step: 4 Training Loss: 0.0001401263871230185 \n",
      "     Training Step: 5 Training Loss: 0.00013324996689334512 \n",
      "     Training Step: 6 Training Loss: 0.00014008142170496285 \n",
      "     Training Step: 7 Training Loss: 0.00013978751667309552 \n",
      "     Training Step: 8 Training Loss: 0.00013410529936663806 \n",
      "     Training Step: 9 Training Loss: 0.0001406802039127797 \n",
      "     Training Step: 10 Training Loss: 0.00013394412235356867 \n",
      "     Training Step: 11 Training Loss: 0.00013851805124431849 \n",
      "     Training Step: 12 Training Loss: 0.0001383998605888337 \n",
      "     Training Step: 13 Training Loss: 0.000139528390718624 \n",
      "     Training Step: 14 Training Loss: 0.00013227987801656127 \n",
      "     Training Step: 15 Training Loss: 0.00013896840391680598 \n",
      "     Training Step: 16 Training Loss: 0.00013166884309612215 \n",
      "     Training Step: 17 Training Loss: 0.00013387353101279587 \n",
      "     Training Step: 18 Training Loss: 0.000131960870930925 \n",
      "     Training Step: 19 Training Loss: 0.0001390984107274562 \n",
      "     Training Step: 20 Training Loss: 0.00013820658205077052 \n",
      "     Training Step: 21 Training Loss: 0.00013208910240791738 \n",
      "     Training Step: 22 Training Loss: 0.0001379979366902262 \n",
      "     Training Step: 23 Training Loss: 0.00013755736290477216 \n",
      "     Training Step: 24 Training Loss: 0.00013004506763536483 \n",
      "     Training Step: 25 Training Loss: 0.00013622090045828372 \n",
      "     Training Step: 26 Training Loss: 0.00013293341908138245 \n",
      "     Training Step: 27 Training Loss: 0.00013825044152326882 \n",
      "     Training Step: 28 Training Loss: 0.00013028355897404253 \n",
      "     Training Step: 29 Training Loss: 0.00013566284906119108 \n",
      "     Training Step: 30 Training Loss: 0.00013696725363843143 \n",
      "     Training Step: 31 Training Loss: 0.00012936640996485949 \n",
      "     Training Step: 32 Training Loss: 0.00013593281619250774 \n",
      "     Training Step: 33 Training Loss: 0.00013641853001900017 \n",
      "     Training Step: 34 Training Loss: 0.00013049917470198125 \n",
      "     Training Step: 35 Training Loss: 0.00013655709335580468 \n",
      "     Training Step: 36 Training Loss: 0.0001354781852569431 \n",
      "     Training Step: 37 Training Loss: 0.0001329410879407078 \n",
      "     Training Step: 38 Training Loss: 0.00013013905845582485 \n",
      "     Training Step: 39 Training Loss: 0.00013466685777530074 \n",
      "     Training Step: 40 Training Loss: 0.00012847036123275757 \n",
      "     Training Step: 41 Training Loss: 0.0001344786724075675 \n",
      "     Training Step: 42 Training Loss: 0.00012766294821631163 \n",
      "     Training Step: 43 Training Loss: 0.000134445377625525 \n",
      "     Training Step: 44 Training Loss: 0.00012976555444765836 \n",
      "     Training Step: 45 Training Loss: 0.00013350564404390752 \n",
      "     Training Step: 46 Training Loss: 0.00012681903899647295 \n",
      "     Training Step: 47 Training Loss: 0.0001270850480068475 \n",
      "     Training Step: 48 Training Loss: 0.00013237673556432128 \n",
      "     Training Step: 49 Training Loss: 0.00012587575474753976 \n",
      "     Training Step: 50 Training Loss: 0.00012791129120159894 \n",
      "     Training Step: 51 Training Loss: 0.00013342664169613272 \n",
      "     Training Step: 52 Training Loss: 0.00012734223855659366 \n",
      "     Training Step: 53 Training Loss: 0.00013504642993211746 \n",
      "     Training Step: 54 Training Loss: 0.00012532970868051052 \n",
      "     Training Step: 55 Training Loss: 0.00013250723714008927 \n",
      "     Training Step: 56 Training Loss: 0.0001318584254477173 \n",
      "     Training Step: 57 Training Loss: 0.00013246783055365086 \n",
      "     Training Step: 58 Training Loss: 0.00013126162230037153 \n",
      "     Training Step: 59 Training Loss: 0.0001316238340223208 \n",
      "     Training Step: 60 Training Loss: 0.00013142256648279727 \n",
      "     Training Step: 61 Training Loss: 0.0001251219364348799 \n",
      "     Training Step: 62 Training Loss: 0.00013099458010401577 \n",
      "     Training Step: 63 Training Loss: 0.0001316466077696532 \n",
      "     Training Step: 64 Training Loss: 0.0001272174995392561 \n",
      "     Training Step: 65 Training Loss: 0.00012663993402384222 \n",
      "     Training Step: 66 Training Loss: 0.00013027875684201717 \n",
      "     Training Step: 67 Training Loss: 0.00012965062342118472 \n",
      "     Training Step: 68 Training Loss: 0.000122810757602565 \n",
      "     Training Step: 69 Training Loss: 0.00012984592467546463 \n",
      "     Training Step: 70 Training Loss: 0.00012979649181943387 \n",
      "     Training Step: 71 Training Loss: 0.0001300510484725237 \n",
      "     Training Step: 72 Training Loss: 0.00012550101382657886 \n",
      "     Training Step: 73 Training Loss: 0.00013050563575234264 \n",
      "     Training Step: 74 Training Loss: 0.0001233982911799103 \n",
      "     Training Step: 75 Training Loss: 0.00012431837967596948 \n",
      "     Training Step: 76 Training Loss: 0.00012951268581673503 \n",
      "     Training Step: 77 Training Loss: 0.0001286829065065831 \n",
      "     Training Step: 78 Training Loss: 0.00012231493019498885 \n",
      "     Training Step: 79 Training Loss: 0.0001278974232263863 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.0001230879861395806 \n",
      "     Validation Step: 1 Validation Loss: 0.0001290151703869924 \n",
      "     Validation Step: 2 Validation Loss: 0.00012930759112350643 \n",
      "     Validation Step: 3 Validation Loss: 0.0001222976134158671 \n",
      "     Validation Step: 4 Validation Loss: 0.00012570162652991712 \n",
      "     Validation Step: 5 Validation Loss: 0.00012267376587260514 \n",
      "     Validation Step: 6 Validation Loss: 0.00012231861182954162 \n",
      "     Validation Step: 7 Validation Loss: 0.00012500587035901845 \n",
      "     Validation Step: 8 Validation Loss: 0.00012154881551396102 \n",
      "     Validation Step: 9 Validation Loss: 0.00012790081382263452 \n",
      "     Validation Step: 10 Validation Loss: 0.00012864696327596903 \n",
      "     Validation Step: 11 Validation Loss: 0.0001286858896492049 \n",
      "     Validation Step: 12 Validation Loss: 0.00012835647794418037 \n",
      "     Validation Step: 13 Validation Loss: 0.00012227724073454738 \n",
      "     Validation Step: 14 Validation Loss: 0.0001255382230738178 \n",
      "Epoch: 12\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.0001231332280440256 \n",
      "     Training Step: 1 Training Loss: 0.00012366700684651732 \n",
      "     Training Step: 2 Training Loss: 0.00012771261390298605 \n",
      "     Training Step: 3 Training Loss: 0.00012771296314895153 \n",
      "     Training Step: 4 Training Loss: 0.00012878476991318166 \n",
      "     Training Step: 5 Training Loss: 0.0001269657223019749 \n",
      "     Training Step: 6 Training Loss: 0.0001272260706173256 \n",
      "     Training Step: 7 Training Loss: 0.0001209414258482866 \n",
      "     Training Step: 8 Training Loss: 0.0001213091891258955 \n",
      "     Training Step: 9 Training Loss: 0.00012423429870977998 \n",
      "     Training Step: 10 Training Loss: 0.00011977714893873781 \n",
      "     Training Step: 11 Training Loss: 0.00012159286416135728 \n",
      "     Training Step: 12 Training Loss: 0.00012182520003989339 \n",
      "     Training Step: 13 Training Loss: 0.0001193765492644161 \n",
      "     Training Step: 14 Training Loss: 0.00012695984332822263 \n",
      "     Training Step: 15 Training Loss: 0.00011950163025176153 \n",
      "     Training Step: 16 Training Loss: 0.00012571082334034145 \n",
      "     Training Step: 17 Training Loss: 0.00012675352627411485 \n",
      "     Training Step: 18 Training Loss: 0.00012639113992918283 \n",
      "     Training Step: 19 Training Loss: 0.00012744171544909477 \n",
      "     Training Step: 20 Training Loss: 0.00012791754852514714 \n",
      "     Training Step: 21 Training Loss: 0.00012590836558956653 \n",
      "     Training Step: 22 Training Loss: 0.00012595581938512623 \n",
      "     Training Step: 23 Training Loss: 0.0001189154718304053 \n",
      "     Training Step: 24 Training Loss: 0.00012039526336593553 \n",
      "     Training Step: 25 Training Loss: 0.0001259224081877619 \n",
      "     Training Step: 26 Training Loss: 0.00011802403605543077 \n",
      "     Training Step: 27 Training Loss: 0.00012395434896461666 \n",
      "     Training Step: 28 Training Loss: 0.00012455442629288882 \n",
      "     Training Step: 29 Training Loss: 0.0001179444370791316 \n",
      "     Training Step: 30 Training Loss: 0.00012201059143990278 \n",
      "     Training Step: 31 Training Loss: 0.0001181167463073507 \n",
      "     Training Step: 32 Training Loss: 0.00012499970034696162 \n",
      "     Training Step: 33 Training Loss: 0.0001180663748527877 \n",
      "     Training Step: 34 Training Loss: 0.00012331458856351674 \n",
      "     Training Step: 35 Training Loss: 0.00011780283239204437 \n",
      "     Training Step: 36 Training Loss: 0.00012528226943686604 \n",
      "     Training Step: 37 Training Loss: 0.0001165143185062334 \n",
      "     Training Step: 38 Training Loss: 0.00011664494377328083 \n",
      "     Training Step: 39 Training Loss: 0.00011522351996973157 \n",
      "     Training Step: 40 Training Loss: 0.0001231864735018462 \n",
      "     Training Step: 41 Training Loss: 0.00012590816186275333 \n",
      "     Training Step: 42 Training Loss: 0.0001235423405887559 \n",
      "     Training Step: 43 Training Loss: 0.00012272332969587296 \n",
      "     Training Step: 44 Training Loss: 0.000122578683658503 \n",
      "     Training Step: 45 Training Loss: 0.00012213931768201292 \n",
      "     Training Step: 46 Training Loss: 0.0001214070653077215 \n",
      "     Training Step: 47 Training Loss: 0.00012175984738860279 \n",
      "     Training Step: 48 Training Loss: 0.00011579229612834752 \n",
      "     Training Step: 49 Training Loss: 0.00012235381291247904 \n",
      "     Training Step: 50 Training Loss: 0.0001216399105032906 \n",
      "     Training Step: 51 Training Loss: 0.00012204513768665493 \n",
      "     Training Step: 52 Training Loss: 0.00012104819325031713 \n",
      "     Training Step: 53 Training Loss: 0.00012131309631513432 \n",
      "     Training Step: 54 Training Loss: 0.00012082399916835129 \n",
      "     Training Step: 55 Training Loss: 0.00011966028978349641 \n",
      "     Training Step: 56 Training Loss: 0.00012060503649991006 \n",
      "     Training Step: 57 Training Loss: 0.00012062277528457344 \n",
      "     Training Step: 58 Training Loss: 0.00011503543646540493 \n",
      "     Training Step: 59 Training Loss: 0.00012025890464428812 \n",
      "     Training Step: 60 Training Loss: 0.00011422293027862906 \n",
      "     Training Step: 61 Training Loss: 0.00011438741785241291 \n",
      "     Training Step: 62 Training Loss: 0.00011368192645022646 \n",
      "     Training Step: 63 Training Loss: 0.00011973391519859433 \n",
      "     Training Step: 64 Training Loss: 0.00011407567944843322 \n",
      "     Training Step: 65 Training Loss: 0.00011331094719935209 \n",
      "     Training Step: 66 Training Loss: 0.00011341008939780295 \n",
      "     Training Step: 67 Training Loss: 0.00011354991875123233 \n",
      "     Training Step: 68 Training Loss: 0.00011404375254642218 \n",
      "     Training Step: 69 Training Loss: 0.00011377237387932837 \n",
      "     Training Step: 70 Training Loss: 0.00011263969645369798 \n",
      "     Training Step: 71 Training Loss: 0.00012265110854059458 \n",
      "     Training Step: 72 Training Loss: 0.0001180791441584006 \n",
      "     Training Step: 73 Training Loss: 0.00012061864254064858 \n",
      "     Training Step: 74 Training Loss: 0.0001112886966438964 \n",
      "     Training Step: 75 Training Loss: 0.00011936372902709991 \n",
      "     Training Step: 76 Training Loss: 0.00011892744078068063 \n",
      "     Training Step: 77 Training Loss: 0.00011724178330041468 \n",
      "     Training Step: 78 Training Loss: 0.00011146379983983934 \n",
      "     Training Step: 79 Training Loss: 0.00011774487211368978 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.0001190880429930985 \n",
      "     Validation Step: 1 Validation Loss: 0.00011243100743740797 \n",
      "     Validation Step: 2 Validation Loss: 0.00011087796156061813 \n",
      "     Validation Step: 3 Validation Loss: 0.0001151769029092975 \n",
      "     Validation Step: 4 Validation Loss: 0.00011726945376722142 \n",
      "     Validation Step: 5 Validation Loss: 0.00011201137385796756 \n",
      "     Validation Step: 6 Validation Loss: 0.000118466661660932 \n",
      "     Validation Step: 7 Validation Loss: 0.00011794354941230267 \n",
      "     Validation Step: 8 Validation Loss: 0.00011823303066194057 \n",
      "     Validation Step: 9 Validation Loss: 0.00011139922571601346 \n",
      "     Validation Step: 10 Validation Loss: 0.00011138442641822621 \n",
      "     Validation Step: 11 Validation Loss: 0.00011436497152317315 \n",
      "     Validation Step: 12 Validation Loss: 0.00011476495274109766 \n",
      "     Validation Step: 13 Validation Loss: 0.00011802701919805259 \n",
      "     Validation Step: 14 Validation Loss: 0.00011101725976914167 \n",
      "Epoch: 13\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.00011755432933568954 \n",
      "     Training Step: 1 Training Loss: 0.00011064882710343227 \n",
      "     Training Step: 2 Training Loss: 0.00011757419269997627 \n",
      "     Training Step: 3 Training Loss: 0.00011752557475119829 \n",
      "     Training Step: 4 Training Loss: 0.00011321814963594079 \n",
      "     Training Step: 5 Training Loss: 0.00011693188571371138 \n",
      "     Training Step: 6 Training Loss: 0.00011431679013185203 \n",
      "     Training Step: 7 Training Loss: 0.00011133016232633963 \n",
      "     Training Step: 8 Training Loss: 0.00011712957348208874 \n",
      "     Training Step: 9 Training Loss: 0.00011121614807052538 \n",
      "     Training Step: 10 Training Loss: 0.00011026469292119145 \n",
      "     Training Step: 11 Training Loss: 0.00011786249524448067 \n",
      "     Training Step: 12 Training Loss: 0.00011782822548411787 \n",
      "     Training Step: 13 Training Loss: 0.00011652569810394198 \n",
      "     Training Step: 14 Training Loss: 0.00011050733155570924 \n",
      "     Training Step: 15 Training Loss: 0.00011527953029144555 \n",
      "     Training Step: 16 Training Loss: 0.00011527893366292119 \n",
      "     Training Step: 17 Training Loss: 0.00011549037299118936 \n",
      "     Training Step: 18 Training Loss: 0.00011091057967860252 \n",
      "     Training Step: 19 Training Loss: 0.00011508922034408897 \n",
      "     Training Step: 20 Training Loss: 0.00011516227095853537 \n",
      "     Training Step: 21 Training Loss: 0.00011618265125434846 \n",
      "     Training Step: 22 Training Loss: 0.00010955515608657151 \n",
      "     Training Step: 23 Training Loss: 0.00011602348240558058 \n",
      "     Training Step: 24 Training Loss: 0.00010793709952849895 \n",
      "     Training Step: 25 Training Loss: 0.00010945938993245363 \n",
      "     Training Step: 26 Training Loss: 0.00011020444799214602 \n",
      "     Training Step: 27 Training Loss: 0.00011440931120887399 \n",
      "     Training Step: 28 Training Loss: 0.00011390153667889535 \n",
      "     Training Step: 29 Training Loss: 0.00011711598199326545 \n",
      "     Training Step: 30 Training Loss: 0.00010852054401766509 \n",
      "     Training Step: 31 Training Loss: 0.00010621290857670829 \n",
      "     Training Step: 32 Training Loss: 0.00010779334115795791 \n",
      "     Training Step: 33 Training Loss: 0.00010765374463517219 \n",
      "     Training Step: 34 Training Loss: 0.00010657166421879083 \n",
      "     Training Step: 35 Training Loss: 0.00011326217645546421 \n",
      "     Training Step: 36 Training Loss: 0.00011436286877142265 \n",
      "     Training Step: 37 Training Loss: 0.00011401320080040023 \n",
      "     Training Step: 38 Training Loss: 0.00011373477900633588 \n",
      "     Training Step: 39 Training Loss: 0.00011365664977347478 \n",
      "     Training Step: 40 Training Loss: 0.00010554958862485364 \n",
      "     Training Step: 41 Training Loss: 0.00011301302583888173 \n",
      "     Training Step: 42 Training Loss: 0.00011195820843568072 \n",
      "     Training Step: 43 Training Loss: 0.00010630845645209774 \n",
      "     Training Step: 44 Training Loss: 0.00011265440116403624 \n",
      "     Training Step: 45 Training Loss: 0.00010570409358479083 \n",
      "     Training Step: 46 Training Loss: 0.00010501522046979517 \n",
      "     Training Step: 47 Training Loss: 0.00011130837083328515 \n",
      "     Training Step: 48 Training Loss: 0.00010619899694574997 \n",
      "     Training Step: 49 Training Loss: 0.00011167669435963035 \n",
      "     Training Step: 50 Training Loss: 0.0001119447624660097 \n",
      "     Training Step: 51 Training Loss: 0.00011189236829522997 \n",
      "     Training Step: 52 Training Loss: 0.00010536084300838411 \n",
      "     Training Step: 53 Training Loss: 0.00010461996134836227 \n",
      "     Training Step: 54 Training Loss: 0.00011284125503152609 \n",
      "     Training Step: 55 Training Loss: 0.00011196313425898552 \n",
      "     Training Step: 56 Training Loss: 0.00011140992864966393 \n",
      "     Training Step: 57 Training Loss: 0.00010408269008621573 \n",
      "     Training Step: 58 Training Loss: 0.00010997858771588653 \n",
      "     Training Step: 59 Training Loss: 0.00010437074524816126 \n",
      "     Training Step: 60 Training Loss: 0.0001067511475412175 \n",
      "     Training Step: 61 Training Loss: 0.00010929273412330076 \n",
      "     Training Step: 62 Training Loss: 0.00010501292126718909 \n",
      "     Training Step: 63 Training Loss: 0.0001111784513341263 \n",
      "     Training Step: 64 Training Loss: 0.00010984386608470231 \n",
      "     Training Step: 65 Training Loss: 0.00010871814447455108 \n",
      "     Training Step: 66 Training Loss: 0.00010863639181479812 \n",
      "     Training Step: 67 Training Loss: 0.00010330021905247122 \n",
      "     Training Step: 68 Training Loss: 0.00010930876305792481 \n",
      "     Training Step: 69 Training Loss: 0.00010903864313149825 \n",
      "     Training Step: 70 Training Loss: 0.00010493613808648661 \n",
      "     Training Step: 71 Training Loss: 0.00010253587970510125 \n",
      "     Training Step: 72 Training Loss: 0.00010874519648496062 \n",
      "     Training Step: 73 Training Loss: 0.00011113930668216199 \n",
      "     Training Step: 74 Training Loss: 0.00010205923899775371 \n",
      "     Training Step: 75 Training Loss: 0.00010836944420589134 \n",
      "     Training Step: 76 Training Loss: 0.00010199306416325271 \n",
      "     Training Step: 77 Training Loss: 0.00010300803114660084 \n",
      "     Training Step: 78 Training Loss: 0.00010722150909714401 \n",
      "     Training Step: 79 Training Loss: 0.00010180076060350984 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.00010830147948581725 \n",
      "     Validation Step: 1 Validation Loss: 0.00010345032205805182 \n",
      "     Validation Step: 2 Validation Loss: 0.00010146303247893229 \n",
      "     Validation Step: 3 Validation Loss: 0.00010354920959798619 \n",
      "     Validation Step: 4 Validation Loss: 0.00010154744813917205 \n",
      "     Validation Step: 5 Validation Loss: 0.00010153751645702869 \n",
      "     Validation Step: 6 Validation Loss: 0.00010774507245514542 \n",
      "     Validation Step: 7 Validation Loss: 0.00010788087820401415 \n",
      "     Validation Step: 8 Validation Loss: 0.0001033041044138372 \n",
      "     Validation Step: 9 Validation Loss: 0.00011021926184184849 \n",
      "     Validation Step: 10 Validation Loss: 0.00010772093082778156 \n",
      "     Validation Step: 11 Validation Loss: 0.00010296111577190459 \n",
      "     Validation Step: 12 Validation Loss: 0.00010834629938472062 \n",
      "     Validation Step: 13 Validation Loss: 0.00010191558976657689 \n",
      "     Validation Step: 14 Validation Loss: 0.00010169135930482298 \n",
      "Epoch: 14\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.00010714597010519356 \n",
      "     Training Step: 1 Training Loss: 0.00010780750017147511 \n",
      "     Training Step: 2 Training Loss: 0.00010680206469260156 \n",
      "     Training Step: 3 Training Loss: 0.00010127780842594802 \n",
      "     Training Step: 4 Training Loss: 0.00010173628834309056 \n",
      "     Training Step: 5 Training Loss: 0.00010714537347666919 \n",
      "     Training Step: 6 Training Loss: 0.00010074950114358217 \n",
      "     Training Step: 7 Training Loss: 0.00010991965245921165 \n",
      "     Training Step: 8 Training Loss: 0.00010842978372238576 \n",
      "     Training Step: 9 Training Loss: 0.00010225392179563642 \n",
      "     Training Step: 10 Training Loss: 0.00010684805602068081 \n",
      "     Training Step: 11 Training Loss: 0.0001032156142173335 \n",
      "     Training Step: 12 Training Loss: 0.00010725665924837813 \n",
      "     Training Step: 13 Training Loss: 0.00010044161172118038 \n",
      "     Training Step: 14 Training Loss: 0.00010089058196172118 \n",
      "     Training Step: 15 Training Loss: 9.984054486267269e-05 \n",
      "     Training Step: 16 Training Loss: 0.00010769948130473495 \n",
      "     Training Step: 17 Training Loss: 0.0001004343357635662 \n",
      "     Training Step: 18 Training Loss: 0.00010668246250133961 \n",
      "     Training Step: 19 Training Loss: 0.00010588609438855201 \n",
      "     Training Step: 20 Training Loss: 0.00010671030031517148 \n",
      "     Training Step: 21 Training Loss: 0.00010598069638945162 \n",
      "     Training Step: 22 Training Loss: 0.00010478714830242097 \n",
      "     Training Step: 23 Training Loss: 9.937652794178575e-05 \n",
      "     Training Step: 24 Training Loss: 0.00010523200035095215 \n",
      "     Training Step: 25 Training Loss: 9.832014620769769e-05 \n",
      "     Training Step: 26 Training Loss: 9.87773819360882e-05 \n",
      "     Training Step: 27 Training Loss: 0.00010530219879001379 \n",
      "     Training Step: 28 Training Loss: 0.00010510953143239021 \n",
      "     Training Step: 29 Training Loss: 9.884267637971789e-05 \n",
      "     Training Step: 30 Training Loss: 0.00010712098446674645 \n",
      "     Training Step: 31 Training Loss: 9.813641372602433e-05 \n",
      "     Training Step: 32 Training Loss: 0.00010578864021226764 \n",
      "     Training Step: 33 Training Loss: 9.89816035144031e-05 \n",
      "     Training Step: 34 Training Loss: 9.752064943313599e-05 \n",
      "     Training Step: 35 Training Loss: 0.00010523779201321304 \n",
      "     Training Step: 36 Training Loss: 0.00010459701297804713 \n",
      "     Training Step: 37 Training Loss: 0.00010527866834308952 \n",
      "     Training Step: 38 Training Loss: 0.00010378594015492126 \n",
      "     Training Step: 39 Training Loss: 9.780495020095259e-05 \n",
      "     Training Step: 40 Training Loss: 0.0001035956374835223 \n",
      "     Training Step: 41 Training Loss: 0.00010455750452820212 \n",
      "     Training Step: 42 Training Loss: 0.00010464330262038857 \n",
      "     Training Step: 43 Training Loss: 0.00010343236499466002 \n",
      "     Training Step: 44 Training Loss: 0.00010091955482494086 \n",
      "     Training Step: 45 Training Loss: 9.778613457456231e-05 \n",
      "     Training Step: 46 Training Loss: 0.00010352076787967235 \n",
      "     Training Step: 47 Training Loss: 9.795903315534815e-05 \n",
      "     Training Step: 48 Training Loss: 9.626348037272692e-05 \n",
      "     Training Step: 49 Training Loss: 9.653054439695552e-05 \n",
      "     Training Step: 50 Training Loss: 0.00010237496462650597 \n",
      "     Training Step: 51 Training Loss: 9.611033601686358e-05 \n",
      "     Training Step: 52 Training Loss: 9.61748301051557e-05 \n",
      "     Training Step: 53 Training Loss: 0.0001021634234348312 \n",
      "     Training Step: 54 Training Loss: 0.00010239327093586326 \n",
      "     Training Step: 55 Training Loss: 9.482483437750489e-05 \n",
      "     Training Step: 56 Training Loss: 0.0001025296951411292 \n",
      "     Training Step: 57 Training Loss: 9.575371950631961e-05 \n",
      "     Training Step: 58 Training Loss: 9.624650556361303e-05 \n",
      "     Training Step: 59 Training Loss: 9.640974894864485e-05 \n",
      "     Training Step: 60 Training Loss: 0.00010148435831069946 \n",
      "     Training Step: 61 Training Loss: 9.540849714539945e-05 \n",
      "     Training Step: 62 Training Loss: 9.486889757681638e-05 \n",
      "     Training Step: 63 Training Loss: 9.517086436972022e-05 \n",
      "     Training Step: 64 Training Loss: 0.0001022908472805284 \n",
      "     Training Step: 65 Training Loss: 0.0001004742516670376 \n",
      "     Training Step: 66 Training Loss: 9.361829870613292e-05 \n",
      "     Training Step: 67 Training Loss: 0.0001001314667519182 \n",
      "     Training Step: 68 Training Loss: 0.00010209003085037693 \n",
      "     Training Step: 69 Training Loss: 9.815449448069558e-05 \n",
      "     Training Step: 70 Training Loss: 0.00010087552072945982 \n",
      "     Training Step: 71 Training Loss: 0.00010091076546814293 \n",
      "     Training Step: 72 Training Loss: 9.342176781501621e-05 \n",
      "     Training Step: 73 Training Loss: 0.00010189058230025694 \n",
      "     Training Step: 74 Training Loss: 0.00010042708890978247 \n",
      "     Training Step: 75 Training Loss: 9.366706945002079e-05 \n",
      "     Training Step: 76 Training Loss: 9.933332330547273e-05 \n",
      "     Training Step: 77 Training Loss: 9.9532502645161e-05 \n",
      "     Training Step: 78 Training Loss: 0.00010011985432356596 \n",
      "     Training Step: 79 Training Loss: 9.992545528803021e-05 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 9.515591955278069e-05 \n",
      "     Validation Step: 1 Validation Loss: 9.899838187266141e-05 \n",
      "     Validation Step: 2 Validation Loss: 9.807827154872939e-05 \n",
      "     Validation Step: 3 Validation Loss: 9.411529754288495e-05 \n",
      "     Validation Step: 4 Validation Loss: 9.382464486407116e-05 \n",
      "     Validation Step: 5 Validation Loss: 0.00010047039540950209 \n",
      "     Validation Step: 6 Validation Loss: 9.487799252383411e-05 \n",
      "     Validation Step: 7 Validation Loss: 9.405896707903594e-05 \n",
      "     Validation Step: 8 Validation Loss: 0.00010003821807913482 \n",
      "     Validation Step: 9 Validation Loss: 9.944803605321795e-05 \n",
      "     Validation Step: 10 Validation Loss: 9.978890011552721e-05 \n",
      "     Validation Step: 11 Validation Loss: 9.71497647697106e-05 \n",
      "     Validation Step: 12 Validation Loss: 9.370986663270742e-05 \n",
      "     Validation Step: 13 Validation Loss: 9.766255971044302e-05 \n",
      "     Validation Step: 14 Validation Loss: 9.99103212961927e-05 \n",
      "Epoch: 15\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.00010017232125392184 \n",
      "     Training Step: 1 Training Loss: 9.3232469225768e-05 \n",
      "     Training Step: 2 Training Loss: 9.298040822613984e-05 \n",
      "     Training Step: 3 Training Loss: 9.327513544121757e-05 \n",
      "     Training Step: 4 Training Loss: 9.815511293709278e-05 \n",
      "     Training Step: 5 Training Loss: 9.920456795953214e-05 \n",
      "     Training Step: 6 Training Loss: 9.830048657022417e-05 \n",
      "     Training Step: 7 Training Loss: 9.959815361071378e-05 \n",
      "     Training Step: 8 Training Loss: 9.821583807934076e-05 \n",
      "     Training Step: 9 Training Loss: 9.909579966915771e-05 \n",
      "     Training Step: 10 Training Loss: 9.431048965780064e-05 \n",
      "     Training Step: 11 Training Loss: 9.38173834583722e-05 \n",
      "     Training Step: 12 Training Loss: 9.530028910376132e-05 \n",
      "     Training Step: 13 Training Loss: 9.516272257314995e-05 \n",
      "     Training Step: 14 Training Loss: 9.766231232788414e-05 \n",
      "     Training Step: 15 Training Loss: 9.788630995899439e-05 \n",
      "     Training Step: 16 Training Loss: 9.791030606720597e-05 \n",
      "     Training Step: 17 Training Loss: 9.140744805335999e-05 \n",
      "     Training Step: 18 Training Loss: 9.65778308454901e-05 \n",
      "     Training Step: 19 Training Loss: 9.110644168686122e-05 \n",
      "     Training Step: 20 Training Loss: 9.809723997022957e-05 \n",
      "     Training Step: 21 Training Loss: 9.136740118265152e-05 \n",
      "     Training Step: 22 Training Loss: 9.067221253644675e-05 \n",
      "     Training Step: 23 Training Loss: 9.908956417348236e-05 \n",
      "     Training Step: 24 Training Loss: 9.076220158021897e-05 \n",
      "     Training Step: 25 Training Loss: 9.164653602056205e-05 \n",
      "     Training Step: 26 Training Loss: 9.945705824065953e-05 \n",
      "     Training Step: 27 Training Loss: 9.648424747865647e-05 \n",
      "     Training Step: 28 Training Loss: 9.682735981186852e-05 \n",
      "     Training Step: 29 Training Loss: 9.680492803454399e-05 \n",
      "     Training Step: 30 Training Loss: 9.662196680437773e-05 \n",
      "     Training Step: 31 Training Loss: 9.670162398833781e-05 \n",
      "     Training Step: 32 Training Loss: 9.006056643556803e-05 \n",
      "     Training Step: 33 Training Loss: 9.620469063520432e-05 \n",
      "     Training Step: 34 Training Loss: 8.91827221494168e-05 \n",
      "     Training Step: 35 Training Loss: 9.030100773088634e-05 \n",
      "     Training Step: 36 Training Loss: 9.61500991252251e-05 \n",
      "     Training Step: 37 Training Loss: 9.760607645148411e-05 \n",
      "     Training Step: 38 Training Loss: 9.134803258348256e-05 \n",
      "     Training Step: 39 Training Loss: 9.611520363250747e-05 \n",
      "     Training Step: 40 Training Loss: 8.944195724325255e-05 \n",
      "     Training Step: 41 Training Loss: 9.55486175371334e-05 \n",
      "     Training Step: 42 Training Loss: 9.178064647130668e-05 \n",
      "     Training Step: 43 Training Loss: 9.511179814580828e-05 \n",
      "     Training Step: 44 Training Loss: 9.578245953889564e-05 \n",
      "     Training Step: 45 Training Loss: 9.504621266387403e-05 \n",
      "     Training Step: 46 Training Loss: 8.994106610771269e-05 \n",
      "     Training Step: 47 Training Loss: 9.695958578959107e-05 \n",
      "     Training Step: 48 Training Loss: 9.48687520576641e-05 \n",
      "     Training Step: 49 Training Loss: 9.025995677802712e-05 \n",
      "     Training Step: 50 Training Loss: 9.482620225753635e-05 \n",
      "     Training Step: 51 Training Loss: 8.842660463415086e-05 \n",
      "     Training Step: 52 Training Loss: 8.929379691835493e-05 \n",
      "     Training Step: 53 Training Loss: 9.422528091818094e-05 \n",
      "     Training Step: 54 Training Loss: 9.488181967753917e-05 \n",
      "     Training Step: 55 Training Loss: 8.894516213331372e-05 \n",
      "     Training Step: 56 Training Loss: 8.86005291249603e-05 \n",
      "     Training Step: 57 Training Loss: 9.399031841894612e-05 \n",
      "     Training Step: 58 Training Loss: 9.299210796598345e-05 \n",
      "     Training Step: 59 Training Loss: 9.427995246369392e-05 \n",
      "     Training Step: 60 Training Loss: 8.787523256614804e-05 \n",
      "     Training Step: 61 Training Loss: 8.740370685700327e-05 \n",
      "     Training Step: 62 Training Loss: 9.38342564040795e-05 \n",
      "     Training Step: 63 Training Loss: 9.309274901170284e-05 \n",
      "     Training Step: 64 Training Loss: 8.73853568919003e-05 \n",
      "     Training Step: 65 Training Loss: 8.85346089489758e-05 \n",
      "     Training Step: 66 Training Loss: 9.359769319416955e-05 \n",
      "     Training Step: 67 Training Loss: 9.231670264853165e-05 \n",
      "     Training Step: 68 Training Loss: 9.210453572450206e-05 \n",
      "     Training Step: 69 Training Loss: 8.717608579900116e-05 \n",
      "     Training Step: 70 Training Loss: 8.829214493744075e-05 \n",
      "     Training Step: 71 Training Loss: 9.418358968105167e-05 \n",
      "     Training Step: 72 Training Loss: 9.196356404572725e-05 \n",
      "     Training Step: 73 Training Loss: 8.666333451401442e-05 \n",
      "     Training Step: 74 Training Loss: 8.600477303843945e-05 \n",
      "     Training Step: 75 Training Loss: 9.60773613769561e-05 \n",
      "     Training Step: 76 Training Loss: 9.175179729936644e-05 \n",
      "     Training Step: 77 Training Loss: 8.720512414583936e-05 \n",
      "     Training Step: 78 Training Loss: 9.318908996647224e-05 \n",
      "     Training Step: 79 Training Loss: 8.605718903709203e-05 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 9.228818817064166e-05 \n",
      "     Validation Step: 1 Validation Loss: 8.634400001028553e-05 \n",
      "     Validation Step: 2 Validation Loss: 8.964361040852964e-05 \n",
      "     Validation Step: 3 Validation Loss: 8.929815521696582e-05 \n",
      "     Validation Step: 4 Validation Loss: 8.654135308461264e-05 \n",
      "     Validation Step: 5 Validation Loss: 8.607743075117469e-05 \n",
      "     Validation Step: 6 Validation Loss: 8.823744428809732e-05 \n",
      "     Validation Step: 7 Validation Loss: 8.642264583613724e-05 \n",
      "     Validation Step: 8 Validation Loss: 9.199932537740096e-05 \n",
      "     Validation Step: 9 Validation Loss: 9.189006232190877e-05 \n",
      "     Validation Step: 10 Validation Loss: 9.323861740995198e-05 \n",
      "     Validation Step: 11 Validation Loss: 9.191481512971222e-05 \n",
      "     Validation Step: 12 Validation Loss: 9.248541027773172e-05 \n",
      "     Validation Step: 13 Validation Loss: 8.569293277105317e-05 \n",
      "     Validation Step: 14 Validation Loss: 8.635748235974461e-05 \n",
      "Epoch: 16\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 9.213255543727428e-05 \n",
      "     Training Step: 1 Training Loss: 8.596935367677361e-05 \n",
      "     Training Step: 2 Training Loss: 9.348330786451697e-05 \n",
      "     Training Step: 3 Training Loss: 9.20870661502704e-05 \n",
      "     Training Step: 4 Training Loss: 8.746667299419641e-05 \n",
      "     Training Step: 5 Training Loss: 8.653027907712385e-05 \n",
      "     Training Step: 6 Training Loss: 8.559328853152692e-05 \n",
      "     Training Step: 7 Training Loss: 8.485330181429163e-05 \n",
      "     Training Step: 8 Training Loss: 8.519332914147526e-05 \n",
      "     Training Step: 9 Training Loss: 9.25369604374282e-05 \n",
      "     Training Step: 10 Training Loss: 9.090478852158412e-05 \n",
      "     Training Step: 11 Training Loss: 9.209025301970541e-05 \n",
      "     Training Step: 12 Training Loss: 8.4510407759808e-05 \n",
      "     Training Step: 13 Training Loss: 9.106249490287155e-05 \n",
      "     Training Step: 14 Training Loss: 9.074367699213326e-05 \n",
      "     Training Step: 15 Training Loss: 9.034752292791381e-05 \n",
      "     Training Step: 16 Training Loss: 8.433550829067826e-05 \n",
      "     Training Step: 17 Training Loss: 9.108378435485065e-05 \n",
      "     Training Step: 18 Training Loss: 9.014968236442655e-05 \n",
      "     Training Step: 19 Training Loss: 8.528043690603226e-05 \n",
      "     Training Step: 20 Training Loss: 9.12812101887539e-05 \n",
      "     Training Step: 21 Training Loss: 8.509145845891908e-05 \n",
      "     Training Step: 22 Training Loss: 8.92856769496575e-05 \n",
      "     Training Step: 23 Training Loss: 8.54960162541829e-05 \n",
      "     Training Step: 24 Training Loss: 8.980558777693659e-05 \n",
      "     Training Step: 25 Training Loss: 8.837750647217035e-05 \n",
      "     Training Step: 26 Training Loss: 8.911889744922519e-05 \n",
      "     Training Step: 27 Training Loss: 8.410216105403379e-05 \n",
      "     Training Step: 28 Training Loss: 9.112566476687789e-05 \n",
      "     Training Step: 29 Training Loss: 9.132764535024762e-05 \n",
      "     Training Step: 30 Training Loss: 8.312472346005961e-05 \n",
      "     Training Step: 31 Training Loss: 8.514148066751659e-05 \n",
      "     Training Step: 32 Training Loss: 8.467728912364691e-05 \n",
      "     Training Step: 33 Training Loss: 8.297905151266605e-05 \n",
      "     Training Step: 34 Training Loss: 8.375149627681822e-05 \n",
      "     Training Step: 35 Training Loss: 8.895659993868321e-05 \n",
      "     Training Step: 36 Training Loss: 8.403579704463482e-05 \n",
      "     Training Step: 37 Training Loss: 8.34304082673043e-05 \n",
      "     Training Step: 38 Training Loss: 8.871044701663777e-05 \n",
      "     Training Step: 39 Training Loss: 8.235662971856073e-05 \n",
      "     Training Step: 40 Training Loss: 8.893857011571527e-05 \n",
      "     Training Step: 41 Training Loss: 8.817171328701079e-05 \n",
      "     Training Step: 42 Training Loss: 8.340214844793081e-05 \n",
      "     Training Step: 43 Training Loss: 8.925687870942056e-05 \n",
      "     Training Step: 44 Training Loss: 8.180616714525968e-05 \n",
      "     Training Step: 45 Training Loss: 8.12771322671324e-05 \n",
      "     Training Step: 46 Training Loss: 8.135873213177547e-05 \n",
      "     Training Step: 47 Training Loss: 8.91594827407971e-05 \n",
      "     Training Step: 48 Training Loss: 9.130893158726394e-05 \n",
      "     Training Step: 49 Training Loss: 8.129460911732167e-05 \n",
      "     Training Step: 50 Training Loss: 8.814131433609873e-05 \n",
      "     Training Step: 51 Training Loss: 8.10495694167912e-05 \n",
      "     Training Step: 52 Training Loss: 8.757548494031653e-05 \n",
      "     Training Step: 53 Training Loss: 8.231576066464186e-05 \n",
      "     Training Step: 54 Training Loss: 8.747341053094715e-05 \n",
      "     Training Step: 55 Training Loss: 8.3660583186429e-05 \n",
      "     Training Step: 56 Training Loss: 8.713805436855182e-05 \n",
      "     Training Step: 57 Training Loss: 8.187220373656601e-05 \n",
      "     Training Step: 58 Training Loss: 8.61864973558113e-05 \n",
      "     Training Step: 59 Training Loss: 9.133068670053035e-05 \n",
      "     Training Step: 60 Training Loss: 8.099294791463763e-05 \n",
      "     Training Step: 61 Training Loss: 8.647853246657178e-05 \n",
      "     Training Step: 62 Training Loss: 8.617520506959409e-05 \n",
      "     Training Step: 63 Training Loss: 8.715363219380379e-05 \n",
      "     Training Step: 64 Training Loss: 8.650109521113336e-05 \n",
      "     Training Step: 65 Training Loss: 8.552192593924701e-05 \n",
      "     Training Step: 66 Training Loss: 8.681622421136126e-05 \n",
      "     Training Step: 67 Training Loss: 8.652283577248454e-05 \n",
      "     Training Step: 68 Training Loss: 8.653987606521696e-05 \n",
      "     Training Step: 69 Training Loss: 8.235780842369422e-05 \n",
      "     Training Step: 70 Training Loss: 8.547115430701524e-05 \n",
      "     Training Step: 71 Training Loss: 8.274256833828986e-05 \n",
      "     Training Step: 72 Training Loss: 8.115755917970091e-05 \n",
      "     Training Step: 73 Training Loss: 8.511733904015273e-05 \n",
      "     Training Step: 74 Training Loss: 8.531678759027272e-05 \n",
      "     Training Step: 75 Training Loss: 8.08148251962848e-05 \n",
      "     Training Step: 76 Training Loss: 8.557637920603156e-05 \n",
      "     Training Step: 77 Training Loss: 7.9749857832212e-05 \n",
      "     Training Step: 78 Training Loss: 8.659102604724467e-05 \n",
      "     Training Step: 79 Training Loss: 8.490252366755158e-05 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 8.524117583874613e-05 \n",
      "     Validation Step: 1 Validation Loss: 7.988676225068048e-05 \n",
      "     Validation Step: 2 Validation Loss: 7.966614793986082e-05 \n",
      "     Validation Step: 3 Validation Loss: 8.161563891917467e-05 \n",
      "     Validation Step: 4 Validation Loss: 8.08320619398728e-05 \n",
      "     Validation Step: 5 Validation Loss: 8.687603985890746e-05 \n",
      "     Validation Step: 6 Validation Loss: 8.057944069150835e-05 \n",
      "     Validation Step: 7 Validation Loss: 8.50279611768201e-05 \n",
      "     Validation Step: 8 Validation Loss: 7.933894812595099e-05 \n",
      "     Validation Step: 9 Validation Loss: 8.481381519231945e-05 \n",
      "     Validation Step: 10 Validation Loss: 8.131787762977183e-05 \n",
      "     Validation Step: 11 Validation Loss: 7.969416765263304e-05 \n",
      "     Validation Step: 12 Validation Loss: 8.49016651045531e-05 \n",
      "     Validation Step: 13 Validation Loss: 8.564282325096428e-05 \n",
      "     Validation Step: 14 Validation Loss: 7.920375355752185e-05 \n",
      "Epoch: 17\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 8.501768024871126e-05 \n",
      "     Training Step: 1 Training Loss: 8.50361684570089e-05 \n",
      "     Training Step: 2 Training Loss: 8.380808867514133e-05 \n",
      "     Training Step: 3 Training Loss: 7.947064295876771e-05 \n",
      "     Training Step: 4 Training Loss: 7.972037565195933e-05 \n",
      "     Training Step: 5 Training Loss: 8.580493158660829e-05 \n",
      "     Training Step: 6 Training Loss: 8.51759104989469e-05 \n",
      "     Training Step: 7 Training Loss: 8.35741957416758e-05 \n",
      "     Training Step: 8 Training Loss: 7.920668576844037e-05 \n",
      "     Training Step: 9 Training Loss: 7.902394281700253e-05 \n",
      "     Training Step: 10 Training Loss: 7.827118679415435e-05 \n",
      "     Training Step: 11 Training Loss: 8.415358024649322e-05 \n",
      "     Training Step: 12 Training Loss: 8.396248449571431e-05 \n",
      "     Training Step: 13 Training Loss: 8.396891644224524e-05 \n",
      "     Training Step: 14 Training Loss: 8.563451410736889e-05 \n",
      "     Training Step: 15 Training Loss: 7.943090167827904e-05 \n",
      "     Training Step: 16 Training Loss: 7.90791236795485e-05 \n",
      "     Training Step: 17 Training Loss: 8.262939081760123e-05 \n",
      "     Training Step: 18 Training Loss: 8.303273352794349e-05 \n",
      "     Training Step: 19 Training Loss: 7.823493797332048e-05 \n",
      "     Training Step: 20 Training Loss: 7.81434791861102e-05 \n",
      "     Training Step: 21 Training Loss: 7.757538696750998e-05 \n",
      "     Training Step: 22 Training Loss: 8.389088907279074e-05 \n",
      "     Training Step: 23 Training Loss: 7.714018283877522e-05 \n",
      "     Training Step: 24 Training Loss: 8.420174708589911e-05 \n",
      "     Training Step: 25 Training Loss: 8.205871563404799e-05 \n",
      "     Training Step: 26 Training Loss: 8.498925308231264e-05 \n",
      "     Training Step: 27 Training Loss: 8.180397708201781e-05 \n",
      "     Training Step: 28 Training Loss: 7.744559843558818e-05 \n",
      "     Training Step: 29 Training Loss: 8.355974568985403e-05 \n",
      "     Training Step: 30 Training Loss: 8.258565503638238e-05 \n",
      "     Training Step: 31 Training Loss: 8.239320595748723e-05 \n",
      "     Training Step: 32 Training Loss: 7.862488564569503e-05 \n",
      "     Training Step: 33 Training Loss: 7.904980157036334e-05 \n",
      "     Training Step: 34 Training Loss: 7.992702740011737e-05 \n",
      "     Training Step: 35 Training Loss: 7.711572106927633e-05 \n",
      "     Training Step: 36 Training Loss: 8.226007776102051e-05 \n",
      "     Training Step: 37 Training Loss: 8.202200115192682e-05 \n",
      "     Training Step: 38 Training Loss: 8.234671986429021e-05 \n",
      "     Training Step: 39 Training Loss: 8.218802395276725e-05 \n",
      "     Training Step: 40 Training Loss: 7.620552787557244e-05 \n",
      "     Training Step: 41 Training Loss: 8.15046951174736e-05 \n",
      "     Training Step: 42 Training Loss: 7.573018956463784e-05 \n",
      "     Training Step: 43 Training Loss: 7.648101018276066e-05 \n",
      "     Training Step: 44 Training Loss: 8.177034032996744e-05 \n",
      "     Training Step: 45 Training Loss: 8.060650725383312e-05 \n",
      "     Training Step: 46 Training Loss: 7.563435065094382e-05 \n",
      "     Training Step: 47 Training Loss: 7.552017632406205e-05 \n",
      "     Training Step: 48 Training Loss: 7.64156284276396e-05 \n",
      "     Training Step: 49 Training Loss: 8.065093425102532e-05 \n",
      "     Training Step: 50 Training Loss: 8.045047434279695e-05 \n",
      "     Training Step: 51 Training Loss: 8.326566603500396e-05 \n",
      "     Training Step: 52 Training Loss: 8.115198579616845e-05 \n",
      "     Training Step: 53 Training Loss: 7.50671315472573e-05 \n",
      "     Training Step: 54 Training Loss: 7.461915811290964e-05 \n",
      "     Training Step: 55 Training Loss: 8.142116712406278e-05 \n",
      "     Training Step: 56 Training Loss: 7.555732008768246e-05 \n",
      "     Training Step: 57 Training Loss: 8.224901102948934e-05 \n",
      "     Training Step: 58 Training Loss: 8.066085865721107e-05 \n",
      "     Training Step: 59 Training Loss: 7.46894656913355e-05 \n",
      "     Training Step: 60 Training Loss: 7.514280878240243e-05 \n",
      "     Training Step: 61 Training Loss: 8.16764950286597e-05 \n",
      "     Training Step: 62 Training Loss: 8.066515147220343e-05 \n",
      "     Training Step: 63 Training Loss: 8.00084526417777e-05 \n",
      "     Training Step: 64 Training Loss: 8.060005347942933e-05 \n",
      "     Training Step: 65 Training Loss: 7.763131725369021e-05 \n",
      "     Training Step: 66 Training Loss: 7.425720104947686e-05 \n",
      "     Training Step: 67 Training Loss: 7.964995893416926e-05 \n",
      "     Training Step: 68 Training Loss: 8.162363519659266e-05 \n",
      "     Training Step: 69 Training Loss: 7.520714279962704e-05 \n",
      "     Training Step: 70 Training Loss: 7.41508265491575e-05 \n",
      "     Training Step: 71 Training Loss: 7.603297126479447e-05 \n",
      "     Training Step: 72 Training Loss: 8.3261271356605e-05 \n",
      "     Training Step: 73 Training Loss: 7.910266140243039e-05 \n",
      "     Training Step: 74 Training Loss: 7.473157893400639e-05 \n",
      "     Training Step: 75 Training Loss: 7.611456385347992e-05 \n",
      "     Training Step: 76 Training Loss: 7.964902033563703e-05 \n",
      "     Training Step: 77 Training Loss: 7.90750709711574e-05 \n",
      "     Training Step: 78 Training Loss: 7.848785753594711e-05 \n",
      "     Training Step: 79 Training Loss: 7.444093353115022e-05 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 7.866798841860145e-05 \n",
      "     Validation Step: 1 Validation Loss: 7.943339733174071e-05 \n",
      "     Validation Step: 2 Validation Loss: 7.467168325092643e-05 \n",
      "     Validation Step: 3 Validation Loss: 7.40505347494036e-05 \n",
      "     Validation Step: 4 Validation Loss: 7.51484913052991e-05 \n",
      "     Validation Step: 5 Validation Loss: 7.403525523841381e-05 \n",
      "     Validation Step: 6 Validation Loss: 7.70947226556018e-05 \n",
      "     Validation Step: 7 Validation Loss: 7.945125980768353e-05 \n",
      "     Validation Step: 8 Validation Loss: 7.868763350415975e-05 \n",
      "     Validation Step: 9 Validation Loss: 7.623490819241852e-05 \n",
      "     Validation Step: 10 Validation Loss: 7.640453986823559e-05 \n",
      "     Validation Step: 11 Validation Loss: 7.892157736932859e-05 \n",
      "     Validation Step: 12 Validation Loss: 7.368382648564875e-05 \n",
      "     Validation Step: 13 Validation Loss: 7.358871516771615e-05 \n",
      "     Validation Step: 14 Validation Loss: 7.970674050739035e-05 \n",
      "Epoch: 18\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 7.886021921876818e-05 \n",
      "     Training Step: 1 Training Loss: 7.861341873649508e-05 \n",
      "     Training Step: 2 Training Loss: 7.858824392315e-05 \n",
      "     Training Step: 3 Training Loss: 7.947540143504739e-05 \n",
      "     Training Step: 4 Training Loss: 7.837164594093338e-05 \n",
      "     Training Step: 5 Training Loss: 7.880094926804304e-05 \n",
      "     Training Step: 6 Training Loss: 7.403272320516407e-05 \n",
      "     Training Step: 7 Training Loss: 7.555281626991928e-05 \n",
      "     Training Step: 8 Training Loss: 7.842843479011208e-05 \n",
      "     Training Step: 9 Training Loss: 7.832670235075057e-05 \n",
      "     Training Step: 10 Training Loss: 7.710275531280786e-05 \n",
      "     Training Step: 11 Training Loss: 7.475921302102506e-05 \n",
      "     Training Step: 12 Training Loss: 7.741548324702308e-05 \n",
      "     Training Step: 13 Training Loss: 7.360820018220693e-05 \n",
      "     Training Step: 14 Training Loss: 7.248869223985821e-05 \n",
      "     Training Step: 15 Training Loss: 7.36481451895088e-05 \n",
      "     Training Step: 16 Training Loss: 7.741593435639516e-05 \n",
      "     Training Step: 17 Training Loss: 7.767094939481467e-05 \n",
      "     Training Step: 18 Training Loss: 7.73837382439524e-05 \n",
      "     Training Step: 19 Training Loss: 7.303241727640852e-05 \n",
      "     Training Step: 20 Training Loss: 7.139440276660025e-05 \n",
      "     Training Step: 21 Training Loss: 7.815616118023172e-05 \n",
      "     Training Step: 22 Training Loss: 7.212120544863865e-05 \n",
      "     Training Step: 23 Training Loss: 7.159069355111569e-05 \n",
      "     Training Step: 24 Training Loss: 7.959145295899361e-05 \n",
      "     Training Step: 25 Training Loss: 7.924875535536557e-05 \n",
      "     Training Step: 26 Training Loss: 7.157039362937212e-05 \n",
      "     Training Step: 27 Training Loss: 7.697568798903376e-05 \n",
      "     Training Step: 28 Training Loss: 7.700586138525978e-05 \n",
      "     Training Step: 29 Training Loss: 7.210931653389707e-05 \n",
      "     Training Step: 30 Training Loss: 7.643528806511313e-05 \n",
      "     Training Step: 31 Training Loss: 7.807921792846173e-05 \n",
      "     Training Step: 32 Training Loss: 7.743389141978696e-05 \n",
      "     Training Step: 33 Training Loss: 7.192266639322042e-05 \n",
      "     Training Step: 34 Training Loss: 7.085882680257782e-05 \n",
      "     Training Step: 35 Training Loss: 7.646688027307391e-05 \n",
      "     Training Step: 36 Training Loss: 7.546492997789755e-05 \n",
      "     Training Step: 37 Training Loss: 7.597838703077286e-05 \n",
      "     Training Step: 38 Training Loss: 7.061313954181969e-05 \n",
      "     Training Step: 39 Training Loss: 7.689345511607826e-05 \n",
      "     Training Step: 40 Training Loss: 7.121632370399311e-05 \n",
      "     Training Step: 41 Training Loss: 7.539209036622196e-05 \n",
      "     Training Step: 42 Training Loss: 7.090019062161446e-05 \n",
      "     Training Step: 43 Training Loss: 7.608825399074703e-05 \n",
      "     Training Step: 44 Training Loss: 7.06656719557941e-05 \n",
      "     Training Step: 45 Training Loss: 7.041197386570275e-05 \n",
      "     Training Step: 46 Training Loss: 7.532432209700346e-05 \n",
      "     Training Step: 47 Training Loss: 7.042541983537376e-05 \n",
      "     Training Step: 48 Training Loss: 7.016792369540781e-05 \n",
      "     Training Step: 49 Training Loss: 6.98176008882001e-05 \n",
      "     Training Step: 50 Training Loss: 7.447332609444857e-05 \n",
      "     Training Step: 51 Training Loss: 7.500105130020529e-05 \n",
      "     Training Step: 52 Training Loss: 7.332168752327561e-05 \n",
      "     Training Step: 53 Training Loss: 7.502194785047323e-05 \n",
      "     Training Step: 54 Training Loss: 6.973514973651618e-05 \n",
      "     Training Step: 55 Training Loss: 7.039449701551348e-05 \n",
      "     Training Step: 56 Training Loss: 6.96769347996451e-05 \n",
      "     Training Step: 57 Training Loss: 7.522520900238305e-05 \n",
      "     Training Step: 58 Training Loss: 6.999712786637247e-05 \n",
      "     Training Step: 59 Training Loss: 7.046914106467739e-05 \n",
      "     Training Step: 60 Training Loss: 7.172152982093394e-05 \n",
      "     Training Step: 61 Training Loss: 6.940818275325e-05 \n",
      "     Training Step: 62 Training Loss: 7.603033736813813e-05 \n",
      "     Training Step: 63 Training Loss: 7.500280480599031e-05 \n",
      "     Training Step: 64 Training Loss: 6.871347432024777e-05 \n",
      "     Training Step: 65 Training Loss: 6.869246135465801e-05 \n",
      "     Training Step: 66 Training Loss: 7.466207898687571e-05 \n",
      "     Training Step: 67 Training Loss: 7.381387695204467e-05 \n",
      "     Training Step: 68 Training Loss: 7.793081749696285e-05 \n",
      "     Training Step: 69 Training Loss: 6.827355537097901e-05 \n",
      "     Training Step: 70 Training Loss: 7.581841782666743e-05 \n",
      "     Training Step: 71 Training Loss: 6.805107113905251e-05 \n",
      "     Training Step: 72 Training Loss: 7.300838478840888e-05 \n",
      "     Training Step: 73 Training Loss: 6.827409379184246e-05 \n",
      "     Training Step: 74 Training Loss: 7.360927702393383e-05 \n",
      "     Training Step: 75 Training Loss: 7.30609244783409e-05 \n",
      "     Training Step: 76 Training Loss: 7.250856288010255e-05 \n",
      "     Training Step: 77 Training Loss: 7.267545151989907e-05 \n",
      "     Training Step: 78 Training Loss: 7.43658747524023e-05 \n",
      "     Training Step: 79 Training Loss: 7.414288120344281e-05 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 6.840606511104852e-05 \n",
      "     Validation Step: 1 Validation Loss: 7.28469603927806e-05 \n",
      "     Validation Step: 2 Validation Loss: 6.855110405012965e-05 \n",
      "     Validation Step: 3 Validation Loss: 7.447692041750997e-05 \n",
      "     Validation Step: 4 Validation Loss: 7.008633110672235e-05 \n",
      "     Validation Step: 5 Validation Loss: 6.801524432376027e-05 \n",
      "     Validation Step: 6 Validation Loss: 7.353779801633209e-05 \n",
      "     Validation Step: 7 Validation Loss: 7.349444786086679e-05 \n",
      "     Validation Step: 8 Validation Loss: 6.900102744111791e-05 \n",
      "     Validation Step: 9 Validation Loss: 7.307415216928348e-05 \n",
      "     Validation Step: 10 Validation Loss: 6.800123810535297e-05 \n",
      "     Validation Step: 11 Validation Loss: 6.955553544685245e-05 \n",
      "     Validation Step: 12 Validation Loss: 7.039970660116524e-05 \n",
      "     Validation Step: 13 Validation Loss: 7.001357153058052e-05 \n",
      "     Validation Step: 14 Validation Loss: 7.275972166098654e-05 \n",
      "Epoch: 19\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 6.802026473451406e-05 \n",
      "     Training Step: 1 Training Loss: 6.905551708769053e-05 \n",
      "     Training Step: 2 Training Loss: 6.776101508876309e-05 \n",
      "     Training Step: 3 Training Loss: 6.910132651682943e-05 \n",
      "     Training Step: 4 Training Loss: 6.82273821439594e-05 \n",
      "     Training Step: 5 Training Loss: 7.305625331355259e-05 \n",
      "     Training Step: 6 Training Loss: 6.761288386769593e-05 \n",
      "     Training Step: 7 Training Loss: 6.779850809834898e-05 \n",
      "     Training Step: 8 Training Loss: 6.704102997900918e-05 \n",
      "     Training Step: 9 Training Loss: 7.27774458937347e-05 \n",
      "     Training Step: 10 Training Loss: 7.25992867955938e-05 \n",
      "     Training Step: 11 Training Loss: 6.705662235617638e-05 \n",
      "     Training Step: 12 Training Loss: 6.70201116008684e-05 \n",
      "     Training Step: 13 Training Loss: 6.729093729518354e-05 \n",
      "     Training Step: 14 Training Loss: 7.349133375100791e-05 \n",
      "     Training Step: 15 Training Loss: 7.326574268518016e-05 \n",
      "     Training Step: 16 Training Loss: 7.236532110255212e-05 \n",
      "     Training Step: 17 Training Loss: 7.201250991784036e-05 \n",
      "     Training Step: 18 Training Loss: 7.344101322814822e-05 \n",
      "     Training Step: 19 Training Loss: 7.197090599220246e-05 \n",
      "     Training Step: 20 Training Loss: 7.126833952497691e-05 \n",
      "     Training Step: 21 Training Loss: 7.193369674496353e-05 \n",
      "     Training Step: 22 Training Loss: 7.09735177224502e-05 \n",
      "     Training Step: 23 Training Loss: 6.744684651494026e-05 \n",
      "     Training Step: 24 Training Loss: 6.854649109300226e-05 \n",
      "     Training Step: 25 Training Loss: 6.896934064570814e-05 \n",
      "     Training Step: 26 Training Loss: 7.194095815066248e-05 \n",
      "     Training Step: 27 Training Loss: 7.167833973653615e-05 \n",
      "     Training Step: 28 Training Loss: 6.707447755616158e-05 \n",
      "     Training Step: 29 Training Loss: 7.242796709761024e-05 \n",
      "     Training Step: 30 Training Loss: 6.753924390068278e-05 \n",
      "     Training Step: 31 Training Loss: 6.889297219458967e-05 \n",
      "     Training Step: 32 Training Loss: 7.172572077251971e-05 \n",
      "     Training Step: 33 Training Loss: 7.173256744863465e-05 \n",
      "     Training Step: 34 Training Loss: 7.035049929982051e-05 \n",
      "     Training Step: 35 Training Loss: 7.320944860111922e-05 \n",
      "     Training Step: 36 Training Loss: 7.0600988692604e-05 \n",
      "     Training Step: 37 Training Loss: 7.001639460213482e-05 \n",
      "     Training Step: 38 Training Loss: 6.61554659018293e-05 \n",
      "     Training Step: 39 Training Loss: 7.063233351800591e-05 \n",
      "     Training Step: 40 Training Loss: 7.030468987068161e-05 \n",
      "     Training Step: 41 Training Loss: 7.259498670464382e-05 \n",
      "     Training Step: 42 Training Loss: 7.037328032311052e-05 \n",
      "     Training Step: 43 Training Loss: 7.065841055009514e-05 \n",
      "     Training Step: 44 Training Loss: 6.525529897771776e-05 \n",
      "     Training Step: 45 Training Loss: 7.012406422290951e-05 \n",
      "     Training Step: 46 Training Loss: 6.831692735431716e-05 \n",
      "     Training Step: 47 Training Loss: 6.999747711233795e-05 \n",
      "     Training Step: 48 Training Loss: 6.99643132975325e-05 \n",
      "     Training Step: 49 Training Loss: 6.53130337013863e-05 \n",
      "     Training Step: 50 Training Loss: 6.528558151330799e-05 \n",
      "     Training Step: 51 Training Loss: 7.139081571949646e-05 \n",
      "     Training Step: 52 Training Loss: 6.515558925457299e-05 \n",
      "     Training Step: 53 Training Loss: 7.159133383538574e-05 \n",
      "     Training Step: 54 Training Loss: 6.470491643995047e-05 \n",
      "     Training Step: 55 Training Loss: 6.996348383836448e-05 \n",
      "     Training Step: 56 Training Loss: 6.535306602017954e-05 \n",
      "     Training Step: 57 Training Loss: 6.862296140752733e-05 \n",
      "     Training Step: 58 Training Loss: 6.478715658886358e-05 \n",
      "     Training Step: 59 Training Loss: 6.897994899190962e-05 \n",
      "     Training Step: 60 Training Loss: 6.400891288649291e-05 \n",
      "     Training Step: 61 Training Loss: 6.600191409233958e-05 \n",
      "     Training Step: 62 Training Loss: 7.016924791969359e-05 \n",
      "     Training Step: 63 Training Loss: 6.920564192114398e-05 \n",
      "     Training Step: 64 Training Loss: 6.88521540723741e-05 \n",
      "     Training Step: 65 Training Loss: 6.388234032783657e-05 \n",
      "     Training Step: 66 Training Loss: 6.403157021850348e-05 \n",
      "     Training Step: 67 Training Loss: 6.881941226311028e-05 \n",
      "     Training Step: 68 Training Loss: 6.980765465414152e-05 \n",
      "     Training Step: 69 Training Loss: 6.845071038696915e-05 \n",
      "     Training Step: 70 Training Loss: 6.842859875177965e-05 \n",
      "     Training Step: 71 Training Loss: 6.337240483844653e-05 \n",
      "     Training Step: 72 Training Loss: 6.34010648354888e-05 \n",
      "     Training Step: 73 Training Loss: 6.322501576505601e-05 \n",
      "     Training Step: 74 Training Loss: 6.341361586237326e-05 \n",
      "     Training Step: 75 Training Loss: 6.351193587761372e-05 \n",
      "     Training Step: 76 Training Loss: 7.059217750793323e-05 \n",
      "     Training Step: 77 Training Loss: 6.749189924448729e-05 \n",
      "     Training Step: 78 Training Loss: 6.801307608839124e-05 \n",
      "     Training Step: 79 Training Loss: 7.171618199208751e-05 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 6.898658466525376e-05 \n",
      "     Validation Step: 1 Validation Loss: 6.416243559215218e-05 \n",
      "     Validation Step: 2 Validation Loss: 6.814490188844502e-05 \n",
      "     Validation Step: 3 Validation Loss: 6.350537296384573e-05 \n",
      "     Validation Step: 4 Validation Loss: 6.432425288949162e-05 \n",
      "     Validation Step: 5 Validation Loss: 6.42392915324308e-05 \n",
      "     Validation Step: 6 Validation Loss: 6.856332765892148e-05 \n",
      "     Validation Step: 7 Validation Loss: 6.836484681116417e-05 \n",
      "     Validation Step: 8 Validation Loss: 6.316414510365576e-05 \n",
      "     Validation Step: 9 Validation Loss: 6.554566061822698e-05 \n",
      "     Validation Step: 10 Validation Loss: 6.32263399893418e-05 \n",
      "     Validation Step: 11 Validation Loss: 6.765733269276097e-05 \n",
      "     Validation Step: 12 Validation Loss: 6.477223359979689e-05 \n",
      "     Validation Step: 13 Validation Loss: 6.478687282651663e-05 \n",
      "     Validation Step: 14 Validation Loss: 6.763655255781487e-05 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and Validate Model\n",
    "start_epoch = 1\n",
    "num_epochs = 20\n",
    "loss_weights = (0.0, 1.0, 0.0)\n",
    "model_name = \"base_model_physics.pt\"\n",
    "train_output, val_output = train_validate_ACOPF(model, model_name, optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------   CURRENT LEARNING RATE: 0.001   ---------------\n",
      "Epoch: 0\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2294.640380859375 \n",
      "     Training Step: 1 Training Loss: 1887.6961669921875 \n",
      "     Training Step: 2 Training Loss: 1599.8555908203125 \n",
      "     Training Step: 3 Training Loss: 1371.1243896484375 \n",
      "     Training Step: 4 Training Loss: 1237.79150390625 \n",
      "     Training Step: 5 Training Loss: 1108.6446533203125 \n",
      "     Training Step: 6 Training Loss: 981.4968872070312 \n",
      "     Training Step: 7 Training Loss: 887.6901245117188 \n",
      "     Training Step: 8 Training Loss: 803.025390625 \n",
      "     Training Step: 9 Training Loss: 737.6721801757812 \n",
      "     Training Step: 10 Training Loss: 655.5064697265625 \n",
      "     Training Step: 11 Training Loss: 602.1118774414062 \n",
      "     Training Step: 12 Training Loss: 544.0517578125 \n",
      "     Training Step: 13 Training Loss: 482.4895935058594 \n",
      "     Training Step: 14 Training Loss: 444.9730224609375 \n",
      "     Training Step: 15 Training Loss: 392.84576416015625 \n",
      "     Training Step: 16 Training Loss: 353.1507873535156 \n",
      "     Training Step: 17 Training Loss: 322.8008728027344 \n",
      "     Training Step: 18 Training Loss: 286.1127014160156 \n",
      "     Training Step: 19 Training Loss: 256.2944641113281 \n",
      "     Training Step: 20 Training Loss: 228.37493896484375 \n",
      "     Training Step: 21 Training Loss: 205.20765686035156 \n",
      "     Training Step: 22 Training Loss: 187.38851928710938 \n",
      "     Training Step: 23 Training Loss: 167.18495178222656 \n",
      "     Training Step: 24 Training Loss: 149.28787231445312 \n",
      "     Training Step: 25 Training Loss: 129.71058654785156 \n",
      "     Training Step: 26 Training Loss: 114.50099182128906 \n",
      "     Training Step: 27 Training Loss: 101.52147674560547 \n",
      "     Training Step: 28 Training Loss: 90.1070785522461 \n",
      "     Training Step: 29 Training Loss: 84.62589263916016 \n",
      "     Training Step: 30 Training Loss: 73.49290466308594 \n",
      "     Training Step: 31 Training Loss: 62.306514739990234 \n",
      "     Training Step: 32 Training Loss: 54.31977462768555 \n",
      "     Training Step: 33 Training Loss: 47.970516204833984 \n",
      "     Training Step: 34 Training Loss: 43.16768264770508 \n",
      "     Training Step: 35 Training Loss: 38.31758499145508 \n",
      "     Training Step: 36 Training Loss: 31.805517196655273 \n",
      "     Training Step: 37 Training Loss: 28.031564712524414 \n",
      "     Training Step: 38 Training Loss: 25.522705078125 \n",
      "     Training Step: 39 Training Loss: 22.518526077270508 \n",
      "     Training Step: 40 Training Loss: 19.315868377685547 \n",
      "     Training Step: 41 Training Loss: 17.343162536621094 \n",
      "     Training Step: 42 Training Loss: 13.29598331451416 \n",
      "     Training Step: 43 Training Loss: 11.420294761657715 \n",
      "     Training Step: 44 Training Loss: 10.675018310546875 \n",
      "     Training Step: 45 Training Loss: 8.017541885375977 \n",
      "     Training Step: 46 Training Loss: 7.751132011413574 \n",
      "     Training Step: 47 Training Loss: 6.371396064758301 \n",
      "     Training Step: 48 Training Loss: 4.515458583831787 \n",
      "     Training Step: 49 Training Loss: 4.028995990753174 \n",
      "     Training Step: 50 Training Loss: 3.039557695388794 \n",
      "     Training Step: 51 Training Loss: 2.538457155227661 \n",
      "     Training Step: 52 Training Loss: 2.0867714881896973 \n",
      "     Training Step: 53 Training Loss: 1.9552264213562012 \n",
      "     Training Step: 54 Training Loss: 1.4957261085510254 \n",
      "     Training Step: 55 Training Loss: 1.4552586078643799 \n",
      "     Training Step: 56 Training Loss: 1.1989611387252808 \n",
      "     Training Step: 57 Training Loss: 1.0858253240585327 \n",
      "     Training Step: 58 Training Loss: 1.000603199005127 \n",
      "     Training Step: 59 Training Loss: 0.9243078231811523 \n",
      "     Training Step: 60 Training Loss: 0.7817825078964233 \n",
      "     Training Step: 61 Training Loss: 0.7250499725341797 \n",
      "     Training Step: 62 Training Loss: 0.8094843626022339 \n",
      "     Training Step: 63 Training Loss: 0.7565147876739502 \n",
      "     Training Step: 64 Training Loss: 0.7660280466079712 \n",
      "     Training Step: 65 Training Loss: 0.6878975629806519 \n",
      "     Training Step: 66 Training Loss: 0.7016205787658691 \n",
      "     Training Step: 67 Training Loss: 0.710737943649292 \n",
      "     Training Step: 68 Training Loss: 0.7525254487991333 \n",
      "     Training Step: 69 Training Loss: 0.7733415365219116 \n",
      "     Training Step: 70 Training Loss: 0.7513517141342163 \n",
      "     Training Step: 71 Training Loss: 0.7574950456619263 \n",
      "     Training Step: 72 Training Loss: 0.7552388310432434 \n",
      "     Training Step: 73 Training Loss: 0.8055821657180786 \n",
      "     Training Step: 74 Training Loss: 0.7560763359069824 \n",
      "     Training Step: 75 Training Loss: 0.7836915254592896 \n",
      "     Training Step: 76 Training Loss: 0.7311053276062012 \n",
      "     Training Step: 77 Training Loss: 0.737268328666687 \n",
      "     Training Step: 78 Training Loss: 0.7025116682052612 \n",
      "     Training Step: 79 Training Loss: 0.7507617473602295 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.7071242332458496 \n",
      "     Validation Step: 1 Validation Loss: 0.6778011322021484 \n",
      "     Validation Step: 2 Validation Loss: 0.6761914491653442 \n",
      "     Validation Step: 3 Validation Loss: 0.6859470009803772 \n",
      "     Validation Step: 4 Validation Loss: 0.6746519804000854 \n",
      "     Validation Step: 5 Validation Loss: 0.7279586791992188 \n",
      "     Validation Step: 6 Validation Loss: 0.6753629446029663 \n",
      "     Validation Step: 7 Validation Loss: 0.6679938435554504 \n",
      "     Validation Step: 8 Validation Loss: 0.704288125038147 \n",
      "     Validation Step: 9 Validation Loss: 0.692679762840271 \n",
      "     Validation Step: 10 Validation Loss: 0.6685858368873596 \n",
      "     Validation Step: 11 Validation Loss: 0.6708089113235474 \n",
      "     Validation Step: 12 Validation Loss: 0.6697698831558228 \n",
      "     Validation Step: 13 Validation Loss: 0.6631495952606201 \n",
      "     Validation Step: 14 Validation Loss: 0.6700431108474731 \n",
      "Epoch: 1\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6648045778274536 \n",
      "     Training Step: 1 Training Loss: 0.6597054600715637 \n",
      "     Training Step: 2 Training Loss: 0.6415492296218872 \n",
      "     Training Step: 3 Training Loss: 0.6420385241508484 \n",
      "     Training Step: 4 Training Loss: 0.6718930006027222 \n",
      "     Training Step: 5 Training Loss: 0.5985406637191772 \n",
      "     Training Step: 6 Training Loss: 0.6321591734886169 \n",
      "     Training Step: 7 Training Loss: 0.6071100831031799 \n",
      "     Training Step: 8 Training Loss: 0.6267126798629761 \n",
      "     Training Step: 9 Training Loss: 0.627135694026947 \n",
      "     Training Step: 10 Training Loss: 0.5556914806365967 \n",
      "     Training Step: 11 Training Loss: 0.5508233308792114 \n",
      "     Training Step: 12 Training Loss: 0.5947889089584351 \n",
      "     Training Step: 13 Training Loss: 0.5752748847007751 \n",
      "     Training Step: 14 Training Loss: 0.5457703471183777 \n",
      "     Training Step: 15 Training Loss: 0.5899003744125366 \n",
      "     Training Step: 16 Training Loss: 0.5453945994377136 \n",
      "     Training Step: 17 Training Loss: 0.5321208238601685 \n",
      "     Training Step: 18 Training Loss: 0.5286446809768677 \n",
      "     Training Step: 19 Training Loss: 0.5197516679763794 \n",
      "     Training Step: 20 Training Loss: 0.5279541611671448 \n",
      "     Training Step: 21 Training Loss: 0.5298702120780945 \n",
      "     Training Step: 22 Training Loss: 0.5335500240325928 \n",
      "     Training Step: 23 Training Loss: 0.5341119766235352 \n",
      "     Training Step: 24 Training Loss: 0.5343015193939209 \n",
      "     Training Step: 25 Training Loss: 0.49424707889556885 \n",
      "     Training Step: 26 Training Loss: 0.4985094666481018 \n",
      "     Training Step: 27 Training Loss: 0.5170645713806152 \n",
      "     Training Step: 28 Training Loss: 0.506914496421814 \n",
      "     Training Step: 29 Training Loss: 0.48227906227111816 \n",
      "     Training Step: 30 Training Loss: 0.5051085948944092 \n",
      "     Training Step: 31 Training Loss: 0.5207173824310303 \n",
      "     Training Step: 32 Training Loss: 0.4902644157409668 \n",
      "     Training Step: 33 Training Loss: 0.5003694295883179 \n",
      "     Training Step: 34 Training Loss: 0.5016605854034424 \n",
      "     Training Step: 35 Training Loss: 0.476457417011261 \n",
      "     Training Step: 36 Training Loss: 0.4843365550041199 \n",
      "     Training Step: 37 Training Loss: 0.47280406951904297 \n",
      "     Training Step: 38 Training Loss: 0.4710875451564789 \n",
      "     Training Step: 39 Training Loss: 0.4736517071723938 \n",
      "     Training Step: 40 Training Loss: 0.4605744183063507 \n",
      "     Training Step: 41 Training Loss: 0.5055280923843384 \n",
      "     Training Step: 42 Training Loss: 0.4554944336414337 \n",
      "     Training Step: 43 Training Loss: 0.45064419507980347 \n",
      "     Training Step: 44 Training Loss: 0.4960920214653015 \n",
      "     Training Step: 45 Training Loss: 0.4940482974052429 \n",
      "     Training Step: 46 Training Loss: 0.47914332151412964 \n",
      "     Training Step: 47 Training Loss: 0.46452128887176514 \n",
      "     Training Step: 48 Training Loss: 0.4526839256286621 \n",
      "     Training Step: 49 Training Loss: 0.48033297061920166 \n",
      "     Training Step: 50 Training Loss: 0.4941049814224243 \n",
      "     Training Step: 51 Training Loss: 0.43938931822776794 \n",
      "     Training Step: 52 Training Loss: 0.4976992607116699 \n",
      "     Training Step: 53 Training Loss: 0.4898005723953247 \n",
      "     Training Step: 54 Training Loss: 0.4918863773345947 \n",
      "     Training Step: 55 Training Loss: 0.468169629573822 \n",
      "     Training Step: 56 Training Loss: 0.4397384226322174 \n",
      "     Training Step: 57 Training Loss: 0.4613266587257385 \n",
      "     Training Step: 58 Training Loss: 0.4412727355957031 \n",
      "     Training Step: 59 Training Loss: 0.4343862533569336 \n",
      "     Training Step: 60 Training Loss: 0.5148745775222778 \n",
      "     Training Step: 61 Training Loss: 0.4315553903579712 \n",
      "     Training Step: 62 Training Loss: 0.42730963230133057 \n",
      "     Training Step: 63 Training Loss: 0.4219103455543518 \n",
      "     Training Step: 64 Training Loss: 0.42289167642593384 \n",
      "     Training Step: 65 Training Loss: 0.4912745952606201 \n",
      "     Training Step: 66 Training Loss: 0.47088855504989624 \n",
      "     Training Step: 67 Training Loss: 0.41759541630744934 \n",
      "     Training Step: 68 Training Loss: 0.47623908519744873 \n",
      "     Training Step: 69 Training Loss: 0.4460437297821045 \n",
      "     Training Step: 70 Training Loss: 0.45175546407699585 \n",
      "     Training Step: 71 Training Loss: 0.43503493070602417 \n",
      "     Training Step: 72 Training Loss: 0.4873974919319153 \n",
      "     Training Step: 73 Training Loss: 0.4670628309249878 \n",
      "     Training Step: 74 Training Loss: 0.43565496802330017 \n",
      "     Training Step: 75 Training Loss: 0.46796727180480957 \n",
      "     Training Step: 76 Training Loss: 0.41336876153945923 \n",
      "     Training Step: 77 Training Loss: 0.447663813829422 \n",
      "     Training Step: 78 Training Loss: 0.41307565569877625 \n",
      "     Training Step: 79 Training Loss: 0.4083150029182434 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.4108813405036926 \n",
      "     Validation Step: 1 Validation Loss: 0.4031607210636139 \n",
      "     Validation Step: 2 Validation Loss: 0.4090464115142822 \n",
      "     Validation Step: 3 Validation Loss: 0.4036160707473755 \n",
      "     Validation Step: 4 Validation Loss: 0.40916353464126587 \n",
      "     Validation Step: 5 Validation Loss: 0.4368893504142761 \n",
      "     Validation Step: 6 Validation Loss: 0.413404256105423 \n",
      "     Validation Step: 7 Validation Loss: 0.4288328289985657 \n",
      "     Validation Step: 8 Validation Loss: 0.4418419599533081 \n",
      "     Validation Step: 9 Validation Loss: 0.4313843548297882 \n",
      "     Validation Step: 10 Validation Loss: 0.40538835525512695 \n",
      "     Validation Step: 11 Validation Loss: 0.4465082287788391 \n",
      "     Validation Step: 12 Validation Loss: 0.4519311785697937 \n",
      "     Validation Step: 13 Validation Loss: 0.4262821078300476 \n",
      "     Validation Step: 14 Validation Loss: 0.4289821982383728 \n",
      "Epoch: 2\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.4410616457462311 \n",
      "     Training Step: 1 Training Loss: 0.42485493421554565 \n",
      "     Training Step: 2 Training Loss: 0.4223223328590393 \n",
      "     Training Step: 3 Training Loss: 0.4105764627456665 \n",
      "     Training Step: 4 Training Loss: 0.4203838109970093 \n",
      "     Training Step: 5 Training Loss: 0.40795254707336426 \n",
      "     Training Step: 6 Training Loss: 0.45707187056541443 \n",
      "     Training Step: 7 Training Loss: 0.4283832907676697 \n",
      "     Training Step: 8 Training Loss: 0.40926119685173035 \n",
      "     Training Step: 9 Training Loss: 0.40322229266166687 \n",
      "     Training Step: 10 Training Loss: 0.43368327617645264 \n",
      "     Training Step: 11 Training Loss: 0.39976343512535095 \n",
      "     Training Step: 12 Training Loss: 0.4349791705608368 \n",
      "     Training Step: 13 Training Loss: 0.4205373525619507 \n",
      "     Training Step: 14 Training Loss: 0.4254342317581177 \n",
      "     Training Step: 15 Training Loss: 0.394918292760849 \n",
      "     Training Step: 16 Training Loss: 0.41909968852996826 \n",
      "     Training Step: 17 Training Loss: 0.4422115087509155 \n",
      "     Training Step: 18 Training Loss: 0.4522063136100769 \n",
      "     Training Step: 19 Training Loss: 0.44135957956314087 \n",
      "     Training Step: 20 Training Loss: 0.4186219274997711 \n",
      "     Training Step: 21 Training Loss: 0.39954185485839844 \n",
      "     Training Step: 22 Training Loss: 0.4158644676208496 \n",
      "     Training Step: 23 Training Loss: 0.4137510657310486 \n",
      "     Training Step: 24 Training Loss: 0.3992740213871002 \n",
      "     Training Step: 25 Training Loss: 0.4189158082008362 \n",
      "     Training Step: 26 Training Loss: 0.3993869423866272 \n",
      "     Training Step: 27 Training Loss: 0.4231928586959839 \n",
      "     Training Step: 28 Training Loss: 0.3909015953540802 \n",
      "     Training Step: 29 Training Loss: 0.4345092177391052 \n",
      "     Training Step: 30 Training Loss: 0.3958991467952728 \n",
      "     Training Step: 31 Training Loss: 0.4087180197238922 \n",
      "     Training Step: 32 Training Loss: 0.3978222608566284 \n",
      "     Training Step: 33 Training Loss: 0.40513449907302856 \n",
      "     Training Step: 34 Training Loss: 0.413398802280426 \n",
      "     Training Step: 35 Training Loss: 0.41693639755249023 \n",
      "     Training Step: 36 Training Loss: 0.3860563039779663 \n",
      "     Training Step: 37 Training Loss: 0.4162675440311432 \n",
      "     Training Step: 38 Training Loss: 0.3858838379383087 \n",
      "     Training Step: 39 Training Loss: 0.41773721575737 \n",
      "     Training Step: 40 Training Loss: 0.42099636793136597 \n",
      "     Training Step: 41 Training Loss: 0.4012032747268677 \n",
      "     Training Step: 42 Training Loss: 0.3972654938697815 \n",
      "     Training Step: 43 Training Loss: 0.3966110944747925 \n",
      "     Training Step: 44 Training Loss: 0.4182165861129761 \n",
      "     Training Step: 45 Training Loss: 0.42006921768188477 \n",
      "     Training Step: 46 Training Loss: 0.3999888300895691 \n",
      "     Training Step: 47 Training Loss: 0.38040104508399963 \n",
      "     Training Step: 48 Training Loss: 0.3835579752922058 \n",
      "     Training Step: 49 Training Loss: 0.4236500859260559 \n",
      "     Training Step: 50 Training Loss: 0.38060158491134644 \n",
      "     Training Step: 51 Training Loss: 0.4404371380805969 \n",
      "     Training Step: 52 Training Loss: 0.4259505867958069 \n",
      "     Training Step: 53 Training Loss: 0.3801689147949219 \n",
      "     Training Step: 54 Training Loss: 0.4005069434642792 \n",
      "     Training Step: 55 Training Loss: 0.3977963626384735 \n",
      "     Training Step: 56 Training Loss: 0.4031502902507782 \n",
      "     Training Step: 57 Training Loss: 0.4169866740703583 \n",
      "     Training Step: 58 Training Loss: 0.4241975247859955 \n",
      "     Training Step: 59 Training Loss: 0.38119521737098694 \n",
      "     Training Step: 60 Training Loss: 0.398667573928833 \n",
      "     Training Step: 61 Training Loss: 0.38248151540756226 \n",
      "     Training Step: 62 Training Loss: 0.4089664816856384 \n",
      "     Training Step: 63 Training Loss: 0.3726726472377777 \n",
      "     Training Step: 64 Training Loss: 0.40990501642227173 \n",
      "     Training Step: 65 Training Loss: 0.3822208642959595 \n",
      "     Training Step: 66 Training Loss: 0.41330933570861816 \n",
      "     Training Step: 67 Training Loss: 0.3739485442638397 \n",
      "     Training Step: 68 Training Loss: 0.40310490131378174 \n",
      "     Training Step: 69 Training Loss: 0.3776644170284271 \n",
      "     Training Step: 70 Training Loss: 0.38005802035331726 \n",
      "     Training Step: 71 Training Loss: 0.41703033447265625 \n",
      "     Training Step: 72 Training Loss: 0.43616101145744324 \n",
      "     Training Step: 73 Training Loss: 0.38997188210487366 \n",
      "     Training Step: 74 Training Loss: 0.3949107229709625 \n",
      "     Training Step: 75 Training Loss: 0.37023138999938965 \n",
      "     Training Step: 76 Training Loss: 0.4337158501148224 \n",
      "     Training Step: 77 Training Loss: 0.4508751630783081 \n",
      "     Training Step: 78 Training Loss: 0.41878458857536316 \n",
      "     Training Step: 79 Training Loss: 0.36821985244750977 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.36281779408454895 \n",
      "     Validation Step: 1 Validation Loss: 0.3620009124279022 \n",
      "     Validation Step: 2 Validation Loss: 0.40201336145401 \n",
      "     Validation Step: 3 Validation Loss: 0.36885446310043335 \n",
      "     Validation Step: 4 Validation Loss: 0.3635804355144501 \n",
      "     Validation Step: 5 Validation Loss: 0.4122430682182312 \n",
      "     Validation Step: 6 Validation Loss: 0.3668878972530365 \n",
      "     Validation Step: 7 Validation Loss: 0.4063091576099396 \n",
      "     Validation Step: 8 Validation Loss: 0.39172983169555664 \n",
      "     Validation Step: 9 Validation Loss: 0.3666241466999054 \n",
      "     Validation Step: 10 Validation Loss: 0.38839736580848694 \n",
      "     Validation Step: 11 Validation Loss: 0.3914339542388916 \n",
      "     Validation Step: 12 Validation Loss: 0.38859498500823975 \n",
      "     Validation Step: 13 Validation Loss: 0.3813425898551941 \n",
      "     Validation Step: 14 Validation Loss: 0.3699900209903717 \n",
      "Epoch: 3\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.36482420563697815 \n",
      "     Training Step: 1 Training Loss: 0.3831697702407837 \n",
      "     Training Step: 2 Training Loss: 0.38002482056617737 \n",
      "     Training Step: 3 Training Loss: 0.3727613687515259 \n",
      "     Training Step: 4 Training Loss: 0.3703160881996155 \n",
      "     Training Step: 5 Training Loss: 0.4108768105506897 \n",
      "     Training Step: 6 Training Loss: 0.3680473566055298 \n",
      "     Training Step: 7 Training Loss: 0.44883379340171814 \n",
      "     Training Step: 8 Training Loss: 0.36803188920021057 \n",
      "     Training Step: 9 Training Loss: 0.3976014256477356 \n",
      "     Training Step: 10 Training Loss: 0.3866056799888611 \n",
      "     Training Step: 11 Training Loss: 0.38137027621269226 \n",
      "     Training Step: 12 Training Loss: 0.38524144887924194 \n",
      "     Training Step: 13 Training Loss: 0.4260442852973938 \n",
      "     Training Step: 14 Training Loss: 0.3603951036930084 \n",
      "     Training Step: 15 Training Loss: 0.4240099787712097 \n",
      "     Training Step: 16 Training Loss: 0.3575211465358734 \n",
      "     Training Step: 17 Training Loss: 0.42543143033981323 \n",
      "     Training Step: 18 Training Loss: 0.36494380235671997 \n",
      "     Training Step: 19 Training Loss: 0.36387330293655396 \n",
      "     Training Step: 20 Training Loss: 0.41255155205726624 \n",
      "     Training Step: 21 Training Loss: 0.3771814703941345 \n",
      "     Training Step: 22 Training Loss: 0.3810011148452759 \n",
      "     Training Step: 23 Training Loss: 0.37160754203796387 \n",
      "     Training Step: 24 Training Loss: 0.3854394555091858 \n",
      "     Training Step: 25 Training Loss: 0.39783936738967896 \n",
      "     Training Step: 26 Training Loss: 0.36375540494918823 \n",
      "     Training Step: 27 Training Loss: 0.3586937189102173 \n",
      "     Training Step: 28 Training Loss: 0.3941131830215454 \n",
      "     Training Step: 29 Training Loss: 0.3963356614112854 \n",
      "     Training Step: 30 Training Loss: 0.3636110723018646 \n",
      "     Training Step: 31 Training Loss: 0.36420324444770813 \n",
      "     Training Step: 32 Training Loss: 0.4132488965988159 \n",
      "     Training Step: 33 Training Loss: 0.3587360978126526 \n",
      "     Training Step: 34 Training Loss: 0.43114936351776123 \n",
      "     Training Step: 35 Training Loss: 0.42381250858306885 \n",
      "     Training Step: 36 Training Loss: 0.38842064142227173 \n",
      "     Training Step: 37 Training Loss: 0.3678351938724518 \n",
      "     Training Step: 38 Training Loss: 0.3559679090976715 \n",
      "     Training Step: 39 Training Loss: 0.38124915957450867 \n",
      "     Training Step: 40 Training Loss: 0.3690584897994995 \n",
      "     Training Step: 41 Training Loss: 0.3670477569103241 \n",
      "     Training Step: 42 Training Loss: 0.47268232703208923 \n",
      "     Training Step: 43 Training Loss: 0.3562670350074768 \n",
      "     Training Step: 44 Training Loss: 0.35370996594429016 \n",
      "     Training Step: 45 Training Loss: 0.4112911522388458 \n",
      "     Training Step: 46 Training Loss: 0.3572704493999481 \n",
      "     Training Step: 47 Training Loss: 0.4103601574897766 \n",
      "     Training Step: 48 Training Loss: 0.4069538116455078 \n",
      "     Training Step: 49 Training Loss: 0.36857378482818604 \n",
      "     Training Step: 50 Training Loss: 0.3962124288082123 \n",
      "     Training Step: 51 Training Loss: 0.39150556921958923 \n",
      "     Training Step: 52 Training Loss: 0.3469697833061218 \n",
      "     Training Step: 53 Training Loss: 0.41663694381713867 \n",
      "     Training Step: 54 Training Loss: 0.348328173160553 \n",
      "     Training Step: 55 Training Loss: 0.3898715674877167 \n",
      "     Training Step: 56 Training Loss: 0.35560351610183716 \n",
      "     Training Step: 57 Training Loss: 0.35923269391059875 \n",
      "     Training Step: 58 Training Loss: 0.3860926628112793 \n",
      "     Training Step: 59 Training Loss: 0.3831144869327545 \n",
      "     Training Step: 60 Training Loss: 0.36834272742271423 \n",
      "     Training Step: 61 Training Loss: 0.35214710235595703 \n",
      "     Training Step: 62 Training Loss: 0.4075492024421692 \n",
      "     Training Step: 63 Training Loss: 0.3537535071372986 \n",
      "     Training Step: 64 Training Loss: 0.3513813316822052 \n",
      "     Training Step: 65 Training Loss: 0.3518965244293213 \n",
      "     Training Step: 66 Training Loss: 0.3521782159805298 \n",
      "     Training Step: 67 Training Loss: 0.4311100244522095 \n",
      "     Training Step: 68 Training Loss: 0.3502178192138672 \n",
      "     Training Step: 69 Training Loss: 0.3477502167224884 \n",
      "     Training Step: 70 Training Loss: 0.3470703065395355 \n",
      "     Training Step: 71 Training Loss: 0.4228406548500061 \n",
      "     Training Step: 72 Training Loss: 0.3793378472328186 \n",
      "     Training Step: 73 Training Loss: 0.3479404151439667 \n",
      "     Training Step: 74 Training Loss: 0.3827356696128845 \n",
      "     Training Step: 75 Training Loss: 0.4101865887641907 \n",
      "     Training Step: 76 Training Loss: 0.4043980836868286 \n",
      "     Training Step: 77 Training Loss: 0.4107934832572937 \n",
      "     Training Step: 78 Training Loss: 0.3693646788597107 \n",
      "     Training Step: 79 Training Loss: 0.35809046030044556 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.3651677370071411 \n",
      "     Validation Step: 1 Validation Loss: 0.3484345078468323 \n",
      "     Validation Step: 2 Validation Loss: 0.3476528525352478 \n",
      "     Validation Step: 3 Validation Loss: 0.4225400686264038 \n",
      "     Validation Step: 4 Validation Loss: 0.3794272243976593 \n",
      "     Validation Step: 5 Validation Loss: 0.3804532587528229 \n",
      "     Validation Step: 6 Validation Loss: 0.3517606854438782 \n",
      "     Validation Step: 7 Validation Loss: 0.4102139472961426 \n",
      "     Validation Step: 8 Validation Loss: 0.354000449180603 \n",
      "     Validation Step: 9 Validation Loss: 0.37445902824401855 \n",
      "     Validation Step: 10 Validation Loss: 0.3668028712272644 \n",
      "     Validation Step: 11 Validation Loss: 0.38672924041748047 \n",
      "     Validation Step: 12 Validation Loss: 0.346530944108963 \n",
      "     Validation Step: 13 Validation Loss: 0.37805813550949097 \n",
      "     Validation Step: 14 Validation Loss: 0.3563149571418762 \n",
      "Epoch: 4\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.37498044967651367 \n",
      "     Training Step: 1 Training Loss: 0.4057254195213318 \n",
      "     Training Step: 2 Training Loss: 0.3494477868080139 \n",
      "     Training Step: 3 Training Loss: 0.40789711475372314 \n",
      "     Training Step: 4 Training Loss: 0.4094735383987427 \n",
      "     Training Step: 5 Training Loss: 0.4019552171230316 \n",
      "     Training Step: 6 Training Loss: 0.3492045998573303 \n",
      "     Training Step: 7 Training Loss: 0.38168883323669434 \n",
      "     Training Step: 8 Training Loss: 0.38896724581718445 \n",
      "     Training Step: 9 Training Loss: 0.34268441796302795 \n",
      "     Training Step: 10 Training Loss: 0.39178505539894104 \n",
      "     Training Step: 11 Training Loss: 0.4188097417354584 \n",
      "     Training Step: 12 Training Loss: 0.3774530589580536 \n",
      "     Training Step: 13 Training Loss: 0.3701367974281311 \n",
      "     Training Step: 14 Training Loss: 0.3489760756492615 \n",
      "     Training Step: 15 Training Loss: 0.38300561904907227 \n",
      "     Training Step: 16 Training Loss: 0.3523128032684326 \n",
      "     Training Step: 17 Training Loss: 0.3847671151161194 \n",
      "     Training Step: 18 Training Loss: 0.4272489547729492 \n",
      "     Training Step: 19 Training Loss: 0.3510758578777313 \n",
      "     Training Step: 20 Training Loss: 0.3562939763069153 \n",
      "     Training Step: 21 Training Loss: 0.3703622817993164 \n",
      "     Training Step: 22 Training Loss: 0.35473740100860596 \n",
      "     Training Step: 23 Training Loss: 0.35426944494247437 \n",
      "     Training Step: 24 Training Loss: 0.341584175825119 \n",
      "     Training Step: 25 Training Loss: 0.4175700843334198 \n",
      "     Training Step: 26 Training Loss: 0.33987414836883545 \n",
      "     Training Step: 27 Training Loss: 0.39540091156959534 \n",
      "     Training Step: 28 Training Loss: 0.3924378752708435 \n",
      "     Training Step: 29 Training Loss: 0.3463907241821289 \n",
      "     Training Step: 30 Training Loss: 0.3528505861759186 \n",
      "     Training Step: 31 Training Loss: 0.3731248378753662 \n",
      "     Training Step: 32 Training Loss: 0.3608291447162628 \n",
      "     Training Step: 33 Training Loss: 0.3557204604148865 \n",
      "     Training Step: 34 Training Loss: 0.35320985317230225 \n",
      "     Training Step: 35 Training Loss: 0.34443241357803345 \n",
      "     Training Step: 36 Training Loss: 0.34248608350753784 \n",
      "     Training Step: 37 Training Loss: 0.34492790699005127 \n",
      "     Training Step: 38 Training Loss: 0.3463086485862732 \n",
      "     Training Step: 39 Training Loss: 0.3461928367614746 \n",
      "     Training Step: 40 Training Loss: 0.4463942050933838 \n",
      "     Training Step: 41 Training Loss: 0.34247881174087524 \n",
      "     Training Step: 42 Training Loss: 0.3436465859413147 \n",
      "     Training Step: 43 Training Loss: 0.34148910641670227 \n",
      "     Training Step: 44 Training Loss: 0.349126398563385 \n",
      "     Training Step: 45 Training Loss: 0.3474010229110718 \n",
      "     Training Step: 46 Training Loss: 0.3756917715072632 \n",
      "     Training Step: 47 Training Loss: 0.34861990809440613 \n",
      "     Training Step: 48 Training Loss: 0.36459529399871826 \n",
      "     Training Step: 49 Training Loss: 0.3643619120121002 \n",
      "     Training Step: 50 Training Loss: 0.3635950982570648 \n",
      "     Training Step: 51 Training Loss: 0.3559851050376892 \n",
      "     Training Step: 52 Training Loss: 0.37865328788757324 \n",
      "     Training Step: 53 Training Loss: 0.36745133996009827 \n",
      "     Training Step: 54 Training Loss: 0.3501967489719391 \n",
      "     Training Step: 55 Training Loss: 0.3456984758377075 \n",
      "     Training Step: 56 Training Loss: 0.3851393759250641 \n",
      "     Training Step: 57 Training Loss: 0.3415752351284027 \n",
      "     Training Step: 58 Training Loss: 0.37254947423934937 \n",
      "     Training Step: 59 Training Loss: 0.34036725759506226 \n",
      "     Training Step: 60 Training Loss: 0.3608422875404358 \n",
      "     Training Step: 61 Training Loss: 0.35733580589294434 \n",
      "     Training Step: 62 Training Loss: 0.36435946822166443 \n",
      "     Training Step: 63 Training Loss: 0.34206530451774597 \n",
      "     Training Step: 64 Training Loss: 0.3602859675884247 \n",
      "     Training Step: 65 Training Loss: 0.3401874899864197 \n",
      "     Training Step: 66 Training Loss: 0.3515249192714691 \n",
      "     Training Step: 67 Training Loss: 0.37687110900878906 \n",
      "     Training Step: 68 Training Loss: 0.40910422801971436 \n",
      "     Training Step: 69 Training Loss: 0.35050806403160095 \n",
      "     Training Step: 70 Training Loss: 0.35358524322509766 \n",
      "     Training Step: 71 Training Loss: 0.41088539361953735 \n",
      "     Training Step: 72 Training Loss: 0.34968987107276917 \n",
      "     Training Step: 73 Training Loss: 0.3684280216693878 \n",
      "     Training Step: 74 Training Loss: 0.3696416914463043 \n",
      "     Training Step: 75 Training Loss: 0.343793123960495 \n",
      "     Training Step: 76 Training Loss: 0.3924001455307007 \n",
      "     Training Step: 77 Training Loss: 0.37836623191833496 \n",
      "     Training Step: 78 Training Loss: 0.3591410517692566 \n",
      "     Training Step: 79 Training Loss: 0.34698253870010376 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.3631514310836792 \n",
      "     Validation Step: 1 Validation Loss: 0.38274621963500977 \n",
      "     Validation Step: 2 Validation Loss: 0.4183686673641205 \n",
      "     Validation Step: 3 Validation Loss: 0.338009774684906 \n",
      "     Validation Step: 4 Validation Loss: 0.37042462825775146 \n",
      "     Validation Step: 5 Validation Loss: 0.3363986611366272 \n",
      "     Validation Step: 6 Validation Loss: 0.34573665261268616 \n",
      "     Validation Step: 7 Validation Loss: 0.4054476022720337 \n",
      "     Validation Step: 8 Validation Loss: 0.3433355689048767 \n",
      "     Validation Step: 9 Validation Loss: 0.374836802482605 \n",
      "     Validation Step: 10 Validation Loss: 0.33742913603782654 \n",
      "     Validation Step: 11 Validation Loss: 0.34138667583465576 \n",
      "     Validation Step: 12 Validation Loss: 0.3730615973472595 \n",
      "     Validation Step: 13 Validation Loss: 0.375431627035141 \n",
      "     Validation Step: 14 Validation Loss: 0.360373854637146 \n",
      "Epoch: 5\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.34345534443855286 \n",
      "     Training Step: 1 Training Loss: 0.3393111824989319 \n",
      "     Training Step: 2 Training Loss: 0.42824625968933105 \n",
      "     Training Step: 3 Training Loss: 0.5348871946334839 \n",
      "     Training Step: 4 Training Loss: 0.3395196497440338 \n",
      "     Training Step: 5 Training Loss: 0.33863142132759094 \n",
      "     Training Step: 6 Training Loss: 0.3455348014831543 \n",
      "     Training Step: 7 Training Loss: 0.41264235973358154 \n",
      "     Training Step: 8 Training Loss: 0.36657944321632385 \n",
      "     Training Step: 9 Training Loss: 0.3591337203979492 \n",
      "     Training Step: 10 Training Loss: 0.33783745765686035 \n",
      "     Training Step: 11 Training Loss: 0.3387908637523651 \n",
      "     Training Step: 12 Training Loss: 0.43067270517349243 \n",
      "     Training Step: 13 Training Loss: 0.3402547240257263 \n",
      "     Training Step: 14 Training Loss: 0.4241308271884918 \n",
      "     Training Step: 15 Training Loss: 0.3372410237789154 \n",
      "     Training Step: 16 Training Loss: 0.37082380056381226 \n",
      "     Training Step: 17 Training Loss: 0.3471015989780426 \n",
      "     Training Step: 18 Training Loss: 0.36506855487823486 \n",
      "     Training Step: 19 Training Loss: 0.3355289399623871 \n",
      "     Training Step: 20 Training Loss: 0.42488765716552734 \n",
      "     Training Step: 21 Training Loss: 0.4032610058784485 \n",
      "     Training Step: 22 Training Loss: 0.3443298935890198 \n",
      "     Training Step: 23 Training Loss: 0.34236788749694824 \n",
      "     Training Step: 24 Training Loss: 0.3362019658088684 \n",
      "     Training Step: 25 Training Loss: 0.3714560866355896 \n",
      "     Training Step: 26 Training Loss: 0.36991074681282043 \n",
      "     Training Step: 27 Training Loss: 0.3417101502418518 \n",
      "     Training Step: 28 Training Loss: 0.380094438791275 \n",
      "     Training Step: 29 Training Loss: 0.3913800120353699 \n",
      "     Training Step: 30 Training Loss: 0.38370805978775024 \n",
      "     Training Step: 31 Training Loss: 0.3619498610496521 \n",
      "     Training Step: 32 Training Loss: 0.36052706837654114 \n",
      "     Training Step: 33 Training Loss: 0.3594082295894623 \n",
      "     Training Step: 34 Training Loss: 0.3696746230125427 \n",
      "     Training Step: 35 Training Loss: 0.3396395742893219 \n",
      "     Training Step: 36 Training Loss: 0.3366427421569824 \n",
      "     Training Step: 37 Training Loss: 0.3363349735736847 \n",
      "     Training Step: 38 Training Loss: 0.3360152542591095 \n",
      "     Training Step: 39 Training Loss: 0.3359062969684601 \n",
      "     Training Step: 40 Training Loss: 0.43149545788764954 \n",
      "     Training Step: 41 Training Loss: 0.39066463708877563 \n",
      "     Training Step: 42 Training Loss: 0.3566358983516693 \n",
      "     Training Step: 43 Training Loss: 0.3348405361175537 \n",
      "     Training Step: 44 Training Loss: 0.3897210359573364 \n",
      "     Training Step: 45 Training Loss: 0.33497294783592224 \n",
      "     Training Step: 46 Training Loss: 0.4285751283168793 \n",
      "     Training Step: 47 Training Loss: 0.3335663676261902 \n",
      "     Training Step: 48 Training Loss: 0.40019920468330383 \n",
      "     Training Step: 49 Training Loss: 0.32877129316329956 \n",
      "     Training Step: 50 Training Loss: 0.3351123631000519 \n",
      "     Training Step: 51 Training Loss: 0.3656874895095825 \n",
      "     Training Step: 52 Training Loss: 0.3392500877380371 \n",
      "     Training Step: 53 Training Loss: 0.3577174246311188 \n",
      "     Training Step: 54 Training Loss: 0.3349819481372833 \n",
      "     Training Step: 55 Training Loss: 0.35735762119293213 \n",
      "     Training Step: 56 Training Loss: 0.33251750469207764 \n",
      "     Training Step: 57 Training Loss: 0.37801259756088257 \n",
      "     Training Step: 58 Training Loss: 0.33387336134910583 \n",
      "     Training Step: 59 Training Loss: 0.3771316111087799 \n",
      "     Training Step: 60 Training Loss: 0.33884838223457336 \n",
      "     Training Step: 61 Training Loss: 0.3404143452644348 \n",
      "     Training Step: 62 Training Loss: 0.3483425974845886 \n",
      "     Training Step: 63 Training Loss: 0.34724271297454834 \n",
      "     Training Step: 64 Training Loss: 0.3690611720085144 \n",
      "     Training Step: 65 Training Loss: 0.3396399915218353 \n",
      "     Training Step: 66 Training Loss: 0.3658197224140167 \n",
      "     Training Step: 67 Training Loss: 0.3353749215602875 \n",
      "     Training Step: 68 Training Loss: 0.35804563760757446 \n",
      "     Training Step: 69 Training Loss: 0.3519645035266876 \n",
      "     Training Step: 70 Training Loss: 0.3391953408718109 \n",
      "     Training Step: 71 Training Loss: 0.3681642711162567 \n",
      "     Training Step: 72 Training Loss: 0.3430795669555664 \n",
      "     Training Step: 73 Training Loss: 0.3387388288974762 \n",
      "     Training Step: 74 Training Loss: 0.34159791469573975 \n",
      "     Training Step: 75 Training Loss: 0.34547799825668335 \n",
      "     Training Step: 76 Training Loss: 0.33474159240722656 \n",
      "     Training Step: 77 Training Loss: 0.33255016803741455 \n",
      "     Training Step: 78 Training Loss: 0.3384367525577545 \n",
      "     Training Step: 79 Training Loss: 0.33344271779060364 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.38566139340400696 \n",
      "     Validation Step: 1 Validation Loss: 0.33272671699523926 \n",
      "     Validation Step: 2 Validation Loss: 0.3296114206314087 \n",
      "     Validation Step: 3 Validation Loss: 0.3696770966053009 \n",
      "     Validation Step: 4 Validation Loss: 0.42100703716278076 \n",
      "     Validation Step: 5 Validation Loss: 0.38254791498184204 \n",
      "     Validation Step: 6 Validation Loss: 0.3332027792930603 \n",
      "     Validation Step: 7 Validation Loss: 0.36598441004753113 \n",
      "     Validation Step: 8 Validation Loss: 0.33424532413482666 \n",
      "     Validation Step: 9 Validation Loss: 0.3924834132194519 \n",
      "     Validation Step: 10 Validation Loss: 0.384757936000824 \n",
      "     Validation Step: 11 Validation Loss: 0.33229008316993713 \n",
      "     Validation Step: 12 Validation Loss: 0.37904560565948486 \n",
      "     Validation Step: 13 Validation Loss: 0.43240898847579956 \n",
      "     Validation Step: 14 Validation Loss: 0.3323458433151245 \n",
      "Epoch: 6\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.33259162306785583 \n",
      "     Training Step: 1 Training Loss: 0.422850638628006 \n",
      "     Training Step: 2 Training Loss: 0.3840526044368744 \n",
      "     Training Step: 3 Training Loss: 0.37081852555274963 \n",
      "     Training Step: 4 Training Loss: 0.3480812609195709 \n",
      "     Training Step: 5 Training Loss: 0.360500693321228 \n",
      "     Training Step: 6 Training Loss: 0.3685290515422821 \n",
      "     Training Step: 7 Training Loss: 0.34063464403152466 \n",
      "     Training Step: 8 Training Loss: 0.3293718993663788 \n",
      "     Training Step: 9 Training Loss: 0.3538132905960083 \n",
      "     Training Step: 10 Training Loss: 0.3361400365829468 \n",
      "     Training Step: 11 Training Loss: 0.37794655561447144 \n",
      "     Training Step: 12 Training Loss: 0.4052395224571228 \n",
      "     Training Step: 13 Training Loss: 0.33196398615837097 \n",
      "     Training Step: 14 Training Loss: 0.33140039443969727 \n",
      "     Training Step: 15 Training Loss: 0.3610302805900574 \n",
      "     Training Step: 16 Training Loss: 0.34073442220687866 \n",
      "     Training Step: 17 Training Loss: 0.3623864948749542 \n",
      "     Training Step: 18 Training Loss: 0.35274994373321533 \n",
      "     Training Step: 19 Training Loss: 0.3556462526321411 \n",
      "     Training Step: 20 Training Loss: 0.3372417092323303 \n",
      "     Training Step: 21 Training Loss: 0.3445560932159424 \n",
      "     Training Step: 22 Training Loss: 0.3585079312324524 \n",
      "     Training Step: 23 Training Loss: 0.3532807528972626 \n",
      "     Training Step: 24 Training Loss: 0.33097416162490845 \n",
      "     Training Step: 25 Training Loss: 0.3421243727207184 \n",
      "     Training Step: 26 Training Loss: 0.3536354899406433 \n",
      "     Training Step: 27 Training Loss: 0.33299702405929565 \n",
      "     Training Step: 28 Training Loss: 0.3332938849925995 \n",
      "     Training Step: 29 Training Loss: 0.32826435565948486 \n",
      "     Training Step: 30 Training Loss: 0.3338237702846527 \n",
      "     Training Step: 31 Training Loss: 0.4164223074913025 \n",
      "     Training Step: 32 Training Loss: 0.3951115608215332 \n",
      "     Training Step: 33 Training Loss: 0.3323116600513458 \n",
      "     Training Step: 34 Training Loss: 0.3855981230735779 \n",
      "     Training Step: 35 Training Loss: 0.34885120391845703 \n",
      "     Training Step: 36 Training Loss: 0.3414415419101715 \n",
      "     Training Step: 37 Training Loss: 0.37660184502601624 \n",
      "     Training Step: 38 Training Loss: 0.3667415678501129 \n",
      "     Training Step: 39 Training Loss: 0.3685307204723358 \n",
      "     Training Step: 40 Training Loss: 0.33210301399230957 \n",
      "     Training Step: 41 Training Loss: 0.36258670687675476 \n",
      "     Training Step: 42 Training Loss: 0.33713436126708984 \n",
      "     Training Step: 43 Training Loss: 0.39210397005081177 \n",
      "     Training Step: 44 Training Loss: 0.376961886882782 \n",
      "     Training Step: 45 Training Loss: 0.3667777478694916 \n",
      "     Training Step: 46 Training Loss: 0.3383946120738983 \n",
      "     Training Step: 47 Training Loss: 0.34372678399086 \n",
      "     Training Step: 48 Training Loss: 0.3476831912994385 \n",
      "     Training Step: 49 Training Loss: 0.3372920751571655 \n",
      "     Training Step: 50 Training Loss: 0.33866703510284424 \n",
      "     Training Step: 51 Training Loss: 0.34092140197753906 \n",
      "     Training Step: 52 Training Loss: 0.34499406814575195 \n",
      "     Training Step: 53 Training Loss: 0.3477182686328888 \n",
      "     Training Step: 54 Training Loss: 0.32868507504463196 \n",
      "     Training Step: 55 Training Loss: 0.3486314117908478 \n",
      "     Training Step: 56 Training Loss: 0.3301573097705841 \n",
      "     Training Step: 57 Training Loss: 0.3284715712070465 \n",
      "     Training Step: 58 Training Loss: 0.3316458761692047 \n",
      "     Training Step: 59 Training Loss: 0.36304140090942383 \n",
      "     Training Step: 60 Training Loss: 0.3333410620689392 \n",
      "     Training Step: 61 Training Loss: 0.33749619126319885 \n",
      "     Training Step: 62 Training Loss: 0.34518668055534363 \n",
      "     Training Step: 63 Training Loss: 0.336455374956131 \n",
      "     Training Step: 64 Training Loss: 0.38741713762283325 \n",
      "     Training Step: 65 Training Loss: 0.3578840494155884 \n",
      "     Training Step: 66 Training Loss: 0.32647576928138733 \n",
      "     Training Step: 67 Training Loss: 0.38708603382110596 \n",
      "     Training Step: 68 Training Loss: 0.38083162903785706 \n",
      "     Training Step: 69 Training Loss: 0.33065637946128845 \n",
      "     Training Step: 70 Training Loss: 0.34330374002456665 \n",
      "     Training Step: 71 Training Loss: 0.3337339162826538 \n",
      "     Training Step: 72 Training Loss: 0.3311045169830322 \n",
      "     Training Step: 73 Training Loss: 0.34386366605758667 \n",
      "     Training Step: 74 Training Loss: 0.3284231424331665 \n",
      "     Training Step: 75 Training Loss: 0.39476239681243896 \n",
      "     Training Step: 76 Training Loss: 0.39074328541755676 \n",
      "     Training Step: 77 Training Loss: 0.33254027366638184 \n",
      "     Training Step: 78 Training Loss: 0.32898232340812683 \n",
      "     Training Step: 79 Training Loss: 0.3594376742839813 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.3333146572113037 \n",
      "     Validation Step: 1 Validation Loss: 0.34260839223861694 \n",
      "     Validation Step: 2 Validation Loss: 0.345456600189209 \n",
      "     Validation Step: 3 Validation Loss: 0.33225947618484497 \n",
      "     Validation Step: 4 Validation Loss: 0.3292159140110016 \n",
      "     Validation Step: 5 Validation Loss: 0.3389435112476349 \n",
      "     Validation Step: 6 Validation Loss: 0.3420208692550659 \n",
      "     Validation Step: 7 Validation Loss: 0.36925262212753296 \n",
      "     Validation Step: 8 Validation Loss: 0.33189645409584045 \n",
      "     Validation Step: 9 Validation Loss: 0.3288242816925049 \n",
      "     Validation Step: 10 Validation Loss: 0.35809803009033203 \n",
      "     Validation Step: 11 Validation Loss: 0.3323076367378235 \n",
      "     Validation Step: 12 Validation Loss: 0.3361055850982666 \n",
      "     Validation Step: 13 Validation Loss: 0.33857694268226624 \n",
      "     Validation Step: 14 Validation Loss: 0.33671876788139343 \n",
      "Epoch: 7\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.34347110986709595 \n",
      "     Training Step: 1 Training Loss: 0.3235700726509094 \n",
      "     Training Step: 2 Training Loss: 0.36422497034072876 \n",
      "     Training Step: 3 Training Loss: 0.3917187452316284 \n",
      "     Training Step: 4 Training Loss: 0.37009501457214355 \n",
      "     Training Step: 5 Training Loss: 0.3643963932991028 \n",
      "     Training Step: 6 Training Loss: 0.33827224373817444 \n",
      "     Training Step: 7 Training Loss: 0.32710474729537964 \n",
      "     Training Step: 8 Training Loss: 0.3990939259529114 \n",
      "     Training Step: 9 Training Loss: 0.4030188322067261 \n",
      "     Training Step: 10 Training Loss: 0.3262218236923218 \n",
      "     Training Step: 11 Training Loss: 0.3277730941772461 \n",
      "     Training Step: 12 Training Loss: 0.3328512907028198 \n",
      "     Training Step: 13 Training Loss: 0.39514920115470886 \n",
      "     Training Step: 14 Training Loss: 0.3295050263404846 \n",
      "     Training Step: 15 Training Loss: 0.34900277853012085 \n",
      "     Training Step: 16 Training Loss: 0.34395742416381836 \n",
      "     Training Step: 17 Training Loss: 0.33828142285346985 \n",
      "     Training Step: 18 Training Loss: 0.3326488435268402 \n",
      "     Training Step: 19 Training Loss: 0.330393522977829 \n",
      "     Training Step: 20 Training Loss: 0.33545273542404175 \n",
      "     Training Step: 21 Training Loss: 0.3293650150299072 \n",
      "     Training Step: 22 Training Loss: 0.3747091591358185 \n",
      "     Training Step: 23 Training Loss: 0.3261844217777252 \n",
      "     Training Step: 24 Training Loss: 0.3885824978351593 \n",
      "     Training Step: 25 Training Loss: 0.3241163194179535 \n",
      "     Training Step: 26 Training Loss: 0.3407416343688965 \n",
      "     Training Step: 27 Training Loss: 0.3285343647003174 \n",
      "     Training Step: 28 Training Loss: 0.3295145034790039 \n",
      "     Training Step: 29 Training Loss: 0.33087795972824097 \n",
      "     Training Step: 30 Training Loss: 0.3957546353340149 \n",
      "     Training Step: 31 Training Loss: 0.33486616611480713 \n",
      "     Training Step: 32 Training Loss: 0.3307476043701172 \n",
      "     Training Step: 33 Training Loss: 0.3300142288208008 \n",
      "     Training Step: 34 Training Loss: 0.3719903826713562 \n",
      "     Training Step: 35 Training Loss: 0.3271445035934448 \n",
      "     Training Step: 36 Training Loss: 0.32631853222846985 \n",
      "     Training Step: 37 Training Loss: 0.34670090675354004 \n",
      "     Training Step: 38 Training Loss: 0.33544766902923584 \n",
      "     Training Step: 39 Training Loss: 0.3301224410533905 \n",
      "     Training Step: 40 Training Loss: 0.32948586344718933 \n",
      "     Training Step: 41 Training Loss: 0.37887728214263916 \n",
      "     Training Step: 42 Training Loss: 0.3602249026298523 \n",
      "     Training Step: 43 Training Loss: 0.3434607684612274 \n",
      "     Training Step: 44 Training Loss: 0.34250420331954956 \n",
      "     Training Step: 45 Training Loss: 0.3441568613052368 \n",
      "     Training Step: 46 Training Loss: 0.3287007212638855 \n",
      "     Training Step: 47 Training Loss: 0.34472566843032837 \n",
      "     Training Step: 48 Training Loss: 0.32655835151672363 \n",
      "     Training Step: 49 Training Loss: 0.36156436800956726 \n",
      "     Training Step: 50 Training Loss: 0.3443942666053772 \n",
      "     Training Step: 51 Training Loss: 0.35377973318099976 \n",
      "     Training Step: 52 Training Loss: 0.35365161299705505 \n",
      "     Training Step: 53 Training Loss: 0.3368455767631531 \n",
      "     Training Step: 54 Training Loss: 0.35362911224365234 \n",
      "     Training Step: 55 Training Loss: 0.32696565985679626 \n",
      "     Training Step: 56 Training Loss: 0.3243371844291687 \n",
      "     Training Step: 57 Training Loss: 0.3256526291370392 \n",
      "     Training Step: 58 Training Loss: 0.39395976066589355 \n",
      "     Training Step: 59 Training Loss: 0.40768492221832275 \n",
      "     Training Step: 60 Training Loss: 0.34262484312057495 \n",
      "     Training Step: 61 Training Loss: 0.3489952087402344 \n",
      "     Training Step: 62 Training Loss: 0.32012227177619934 \n",
      "     Training Step: 63 Training Loss: 0.3328007757663727 \n",
      "     Training Step: 64 Training Loss: 0.3266467750072479 \n",
      "     Training Step: 65 Training Loss: 0.4571208953857422 \n",
      "     Training Step: 66 Training Loss: 0.42318084836006165 \n",
      "     Training Step: 67 Training Loss: 0.36503756046295166 \n",
      "     Training Step: 68 Training Loss: 0.3334641456604004 \n",
      "     Training Step: 69 Training Loss: 0.3642721176147461 \n",
      "     Training Step: 70 Training Loss: 0.3954007625579834 \n",
      "     Training Step: 71 Training Loss: 0.37454691529273987 \n",
      "     Training Step: 72 Training Loss: 0.32620465755462646 \n",
      "     Training Step: 73 Training Loss: 0.3767608106136322 \n",
      "     Training Step: 74 Training Loss: 0.32722020149230957 \n",
      "     Training Step: 75 Training Loss: 0.34268876910209656 \n",
      "     Training Step: 76 Training Loss: 0.3376806974411011 \n",
      "     Training Step: 77 Training Loss: 0.33710017800331116 \n",
      "     Training Step: 78 Training Loss: 0.37421056628227234 \n",
      "     Training Step: 79 Training Loss: 0.3301907777786255 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.32181304693222046 \n",
      "     Validation Step: 1 Validation Loss: 0.3546774387359619 \n",
      "     Validation Step: 2 Validation Loss: 0.3419003486633301 \n",
      "     Validation Step: 3 Validation Loss: 0.32276469469070435 \n",
      "     Validation Step: 4 Validation Loss: 0.3330550193786621 \n",
      "     Validation Step: 5 Validation Loss: 0.32488736510276794 \n",
      "     Validation Step: 6 Validation Loss: 0.3252079486846924 \n",
      "     Validation Step: 7 Validation Loss: 0.31937137246131897 \n",
      "     Validation Step: 8 Validation Loss: 0.36488956212997437 \n",
      "     Validation Step: 9 Validation Loss: 0.32211846113204956 \n",
      "     Validation Step: 10 Validation Loss: 0.34430867433547974 \n",
      "     Validation Step: 11 Validation Loss: 0.3420383930206299 \n",
      "     Validation Step: 12 Validation Loss: 0.3199545443058014 \n",
      "     Validation Step: 13 Validation Loss: 0.34223881363868713 \n",
      "     Validation Step: 14 Validation Loss: 0.3598635792732239 \n",
      "Epoch: 8\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.3582601249217987 \n",
      "     Training Step: 1 Training Loss: 0.35258933901786804 \n",
      "     Training Step: 2 Training Loss: 0.3492453694343567 \n",
      "     Training Step: 3 Training Loss: 0.34302833676338196 \n",
      "     Training Step: 4 Training Loss: 0.32883259654045105 \n",
      "     Training Step: 5 Training Loss: 0.32705438137054443 \n",
      "     Training Step: 6 Training Loss: 0.326373428106308 \n",
      "     Training Step: 7 Training Loss: 0.40396255254745483 \n",
      "     Training Step: 8 Training Loss: 0.36913907527923584 \n",
      "     Training Step: 9 Training Loss: 0.3706044554710388 \n",
      "     Training Step: 10 Training Loss: 0.3458442986011505 \n",
      "     Training Step: 11 Training Loss: 0.32197439670562744 \n",
      "     Training Step: 12 Training Loss: 0.31982487440109253 \n",
      "     Training Step: 13 Training Loss: 0.4138498604297638 \n",
      "     Training Step: 14 Training Loss: 0.3277442157268524 \n",
      "     Training Step: 15 Training Loss: 0.32353848218917847 \n",
      "     Training Step: 16 Training Loss: 0.37969860434532166 \n",
      "     Training Step: 17 Training Loss: 0.33943450450897217 \n",
      "     Training Step: 18 Training Loss: 0.3451513350009918 \n",
      "     Training Step: 19 Training Loss: 0.37075209617614746 \n",
      "     Training Step: 20 Training Loss: 0.4397101104259491 \n",
      "     Training Step: 21 Training Loss: 0.36835551261901855 \n",
      "     Training Step: 22 Training Loss: 0.3319227397441864 \n",
      "     Training Step: 23 Training Loss: 0.3417621850967407 \n",
      "     Training Step: 24 Training Loss: 0.345730721950531 \n",
      "     Training Step: 25 Training Loss: 0.32117992639541626 \n",
      "     Training Step: 26 Training Loss: 0.3472263813018799 \n",
      "     Training Step: 27 Training Loss: 0.3205615282058716 \n",
      "     Training Step: 28 Training Loss: 0.34495896100997925 \n",
      "     Training Step: 29 Training Loss: 0.3294421434402466 \n",
      "     Training Step: 30 Training Loss: 0.3446725308895111 \n",
      "     Training Step: 31 Training Loss: 0.34979724884033203 \n",
      "     Training Step: 32 Training Loss: 0.3217191696166992 \n",
      "     Training Step: 33 Training Loss: 0.3662358224391937 \n",
      "     Training Step: 34 Training Loss: 0.38293904066085815 \n",
      "     Training Step: 35 Training Loss: 0.3239698112010956 \n",
      "     Training Step: 36 Training Loss: 0.3248126804828644 \n",
      "     Training Step: 37 Training Loss: 0.3275013267993927 \n",
      "     Training Step: 38 Training Loss: 0.3303838074207306 \n",
      "     Training Step: 39 Training Loss: 0.35563287138938904 \n",
      "     Training Step: 40 Training Loss: 0.3370220363140106 \n",
      "     Training Step: 41 Training Loss: 0.336249440908432 \n",
      "     Training Step: 42 Training Loss: 0.34810107946395874 \n",
      "     Training Step: 43 Training Loss: 0.3199988603591919 \n",
      "     Training Step: 44 Training Loss: 0.3455919325351715 \n",
      "     Training Step: 45 Training Loss: 0.34581300616264343 \n",
      "     Training Step: 46 Training Loss: 0.333925724029541 \n",
      "     Training Step: 47 Training Loss: 0.33808764815330505 \n",
      "     Training Step: 48 Training Loss: 0.33324921131134033 \n",
      "     Training Step: 49 Training Loss: 0.3236117660999298 \n",
      "     Training Step: 50 Training Loss: 0.3856569528579712 \n",
      "     Training Step: 51 Training Loss: 0.3264024257659912 \n",
      "     Training Step: 52 Training Loss: 0.39195117354393005 \n",
      "     Training Step: 53 Training Loss: 0.3224920332431793 \n",
      "     Training Step: 54 Training Loss: 0.32152029871940613 \n",
      "     Training Step: 55 Training Loss: 0.3328457474708557 \n",
      "     Training Step: 56 Training Loss: 0.32854920625686646 \n",
      "     Training Step: 57 Training Loss: 0.35283440351486206 \n",
      "     Training Step: 58 Training Loss: 0.32193660736083984 \n",
      "     Training Step: 59 Training Loss: 0.32522445917129517 \n",
      "     Training Step: 60 Training Loss: 0.322097510099411 \n",
      "     Training Step: 61 Training Loss: 0.33214667439460754 \n",
      "     Training Step: 62 Training Loss: 0.4034994840621948 \n",
      "     Training Step: 63 Training Loss: 0.3699062168598175 \n",
      "     Training Step: 64 Training Loss: 0.3227103352546692 \n",
      "     Training Step: 65 Training Loss: 0.3334658741950989 \n",
      "     Training Step: 66 Training Loss: 0.35541045665740967 \n",
      "     Training Step: 67 Training Loss: 0.32358139753341675 \n",
      "     Training Step: 68 Training Loss: 0.32528746128082275 \n",
      "     Training Step: 69 Training Loss: 0.3682142496109009 \n",
      "     Training Step: 70 Training Loss: 0.3249955475330353 \n",
      "     Training Step: 71 Training Loss: 0.36374402046203613 \n",
      "     Training Step: 72 Training Loss: 0.3367000222206116 \n",
      "     Training Step: 73 Training Loss: 0.3433806896209717 \n",
      "     Training Step: 74 Training Loss: 0.3207145035266876 \n",
      "     Training Step: 75 Training Loss: 0.33858034014701843 \n",
      "     Training Step: 76 Training Loss: 0.34928515553474426 \n",
      "     Training Step: 77 Training Loss: 0.32988858222961426 \n",
      "     Training Step: 78 Training Loss: 0.32813987135887146 \n",
      "     Training Step: 79 Training Loss: 0.32633495330810547 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.38665735721588135 \n",
      "     Validation Step: 1 Validation Loss: 0.3333892226219177 \n",
      "     Validation Step: 2 Validation Loss: 0.31891193985939026 \n",
      "     Validation Step: 3 Validation Loss: 0.3208967447280884 \n",
      "     Validation Step: 4 Validation Loss: 0.35191237926483154 \n",
      "     Validation Step: 5 Validation Loss: 0.3345291316509247 \n",
      "     Validation Step: 6 Validation Loss: 0.3244737982749939 \n",
      "     Validation Step: 7 Validation Loss: 0.3265441358089447 \n",
      "     Validation Step: 8 Validation Loss: 0.34455448389053345 \n",
      "     Validation Step: 9 Validation Loss: 0.3733970820903778 \n",
      "     Validation Step: 10 Validation Loss: 0.345340758562088 \n",
      "     Validation Step: 11 Validation Loss: 0.346680223941803 \n",
      "     Validation Step: 12 Validation Loss: 0.3230959475040436 \n",
      "     Validation Step: 13 Validation Loss: 0.34080106019973755 \n",
      "     Validation Step: 14 Validation Loss: 0.32179322838783264 \n",
      "Epoch: 9\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.32173317670822144 \n",
      "     Training Step: 1 Training Loss: 0.3426201641559601 \n",
      "     Training Step: 2 Training Loss: 0.3642513155937195 \n",
      "     Training Step: 3 Training Loss: 0.3222414255142212 \n",
      "     Training Step: 4 Training Loss: 0.32026952505111694 \n",
      "     Training Step: 5 Training Loss: 0.32338836789131165 \n",
      "     Training Step: 6 Training Loss: 0.3288145959377289 \n",
      "     Training Step: 7 Training Loss: 0.3462369441986084 \n",
      "     Training Step: 8 Training Loss: 0.3442828059196472 \n",
      "     Training Step: 9 Training Loss: 0.331972599029541 \n",
      "     Training Step: 10 Training Loss: 0.3415464758872986 \n",
      "     Training Step: 11 Training Loss: 0.3206949532032013 \n",
      "     Training Step: 12 Training Loss: 0.32014161348342896 \n",
      "     Training Step: 13 Training Loss: 0.37466609477996826 \n",
      "     Training Step: 14 Training Loss: 0.35681411623954773 \n",
      "     Training Step: 15 Training Loss: 0.3551330864429474 \n",
      "     Training Step: 16 Training Loss: 0.32776200771331787 \n",
      "     Training Step: 17 Training Loss: 0.4080556333065033 \n",
      "     Training Step: 18 Training Loss: 0.3836120367050171 \n",
      "     Training Step: 19 Training Loss: 0.32416823506355286 \n",
      "     Training Step: 20 Training Loss: 0.3217966854572296 \n",
      "     Training Step: 21 Training Loss: 0.3583797812461853 \n",
      "     Training Step: 22 Training Loss: 0.3210972547531128 \n",
      "     Training Step: 23 Training Loss: 0.33039650321006775 \n",
      "     Training Step: 24 Training Loss: 0.33877095580101013 \n",
      "     Training Step: 25 Training Loss: 0.3336385488510132 \n",
      "     Training Step: 26 Training Loss: 0.3476349115371704 \n",
      "     Training Step: 27 Training Loss: 0.33725646138191223 \n",
      "     Training Step: 28 Training Loss: 0.34731119871139526 \n",
      "     Training Step: 29 Training Loss: 0.34119656682014465 \n",
      "     Training Step: 30 Training Loss: 0.36578911542892456 \n",
      "     Training Step: 31 Training Loss: 0.32463133335113525 \n",
      "     Training Step: 32 Training Loss: 0.32281798124313354 \n",
      "     Training Step: 33 Training Loss: 0.3833371102809906 \n",
      "     Training Step: 34 Training Loss: 0.33111700415611267 \n",
      "     Training Step: 35 Training Loss: 0.32217371463775635 \n",
      "     Training Step: 36 Training Loss: 0.381020724773407 \n",
      "     Training Step: 37 Training Loss: 0.3184362053871155 \n",
      "     Training Step: 38 Training Loss: 0.3491705656051636 \n",
      "     Training Step: 39 Training Loss: 0.36165833473205566 \n",
      "     Training Step: 40 Training Loss: 0.3287438154220581 \n",
      "     Training Step: 41 Training Loss: 0.3505876660346985 \n",
      "     Training Step: 42 Training Loss: 0.3273557424545288 \n",
      "     Training Step: 43 Training Loss: 0.3878075182437897 \n",
      "     Training Step: 44 Training Loss: 0.3198753893375397 \n",
      "     Training Step: 45 Training Loss: 0.32189273834228516 \n",
      "     Training Step: 46 Training Loss: 0.3220568299293518 \n",
      "     Training Step: 47 Training Loss: 0.3400169909000397 \n",
      "     Training Step: 48 Training Loss: 0.34299716353416443 \n",
      "     Training Step: 49 Training Loss: 0.3275422751903534 \n",
      "     Training Step: 50 Training Loss: 0.3459538221359253 \n",
      "     Training Step: 51 Training Loss: 0.3722487688064575 \n",
      "     Training Step: 52 Training Loss: 0.3202071487903595 \n",
      "     Training Step: 53 Training Loss: 0.33474525809288025 \n",
      "     Training Step: 54 Training Loss: 0.3225570321083069 \n",
      "     Training Step: 55 Training Loss: 0.3298304080963135 \n",
      "     Training Step: 56 Training Loss: 0.3244840204715729 \n",
      "     Training Step: 57 Training Loss: 0.32505032420158386 \n",
      "     Training Step: 58 Training Loss: 0.36691176891326904 \n",
      "     Training Step: 59 Training Loss: 0.32284680008888245 \n",
      "     Training Step: 60 Training Loss: 0.36927998065948486 \n",
      "     Training Step: 61 Training Loss: 0.32153546810150146 \n",
      "     Training Step: 62 Training Loss: 0.32597336173057556 \n",
      "     Training Step: 63 Training Loss: 0.3395978510379791 \n",
      "     Training Step: 64 Training Loss: 0.323378324508667 \n",
      "     Training Step: 65 Training Loss: 0.3399934768676758 \n",
      "     Training Step: 66 Training Loss: 0.3180006146430969 \n",
      "     Training Step: 67 Training Loss: 0.31733816862106323 \n",
      "     Training Step: 68 Training Loss: 0.3211613595485687 \n",
      "     Training Step: 69 Training Loss: 0.35390904545783997 \n",
      "     Training Step: 70 Training Loss: 0.3517036736011505 \n",
      "     Training Step: 71 Training Loss: 0.34706956148147583 \n",
      "     Training Step: 72 Training Loss: 0.32211461663246155 \n",
      "     Training Step: 73 Training Loss: 0.34026437997817993 \n",
      "     Training Step: 74 Training Loss: 0.3293720483779907 \n",
      "     Training Step: 75 Training Loss: 0.33099621534347534 \n",
      "     Training Step: 76 Training Loss: 0.3270112872123718 \n",
      "     Training Step: 77 Training Loss: 0.34286239743232727 \n",
      "     Training Step: 78 Training Loss: 0.348688006401062 \n",
      "     Training Step: 79 Training Loss: 0.3248736262321472 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.3341291546821594 \n",
      "     Validation Step: 1 Validation Loss: 0.33109262585639954 \n",
      "     Validation Step: 2 Validation Loss: 0.33990174531936646 \n",
      "     Validation Step: 3 Validation Loss: 0.3249301314353943 \n",
      "     Validation Step: 4 Validation Loss: 0.3201075494289398 \n",
      "     Validation Step: 5 Validation Loss: 0.3207406997680664 \n",
      "     Validation Step: 6 Validation Loss: 0.33424946665763855 \n",
      "     Validation Step: 7 Validation Loss: 0.335807740688324 \n",
      "     Validation Step: 8 Validation Loss: 0.31995344161987305 \n",
      "     Validation Step: 9 Validation Loss: 0.3278972804546356 \n",
      "     Validation Step: 10 Validation Loss: 0.3263067901134491 \n",
      "     Validation Step: 11 Validation Loss: 0.36672288179397583 \n",
      "     Validation Step: 12 Validation Loss: 0.33094167709350586 \n",
      "     Validation Step: 13 Validation Loss: 0.3560214638710022 \n",
      "     Validation Step: 14 Validation Loss: 0.32662758231163025 \n",
      "Epoch: 10\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.34161072969436646 \n",
      "     Training Step: 1 Training Loss: 0.32995498180389404 \n",
      "     Training Step: 2 Training Loss: 0.34517326951026917 \n",
      "     Training Step: 3 Training Loss: 0.31597745418548584 \n",
      "     Training Step: 4 Training Loss: 0.3146187365055084 \n",
      "     Training Step: 5 Training Loss: 0.3546144366264343 \n",
      "     Training Step: 6 Training Loss: 0.3516696095466614 \n",
      "     Training Step: 7 Training Loss: 0.3227311074733734 \n",
      "     Training Step: 8 Training Loss: 0.3184935450553894 \n",
      "     Training Step: 9 Training Loss: 0.3396994471549988 \n",
      "     Training Step: 10 Training Loss: 0.33303385972976685 \n",
      "     Training Step: 11 Training Loss: 0.32634586095809937 \n",
      "     Training Step: 12 Training Loss: 0.32202866673469543 \n",
      "     Training Step: 13 Training Loss: 0.3249955475330353 \n",
      "     Training Step: 14 Training Loss: 0.3253955841064453 \n",
      "     Training Step: 15 Training Loss: 0.32044097781181335 \n",
      "     Training Step: 16 Training Loss: 0.3222404718399048 \n",
      "     Training Step: 17 Training Loss: 0.31917282938957214 \n",
      "     Training Step: 18 Training Loss: 0.3673349916934967 \n",
      "     Training Step: 19 Training Loss: 0.32458558678627014 \n",
      "     Training Step: 20 Training Loss: 0.3180883824825287 \n",
      "     Training Step: 21 Training Loss: 0.3225071430206299 \n",
      "     Training Step: 22 Training Loss: 0.32007741928100586 \n",
      "     Training Step: 23 Training Loss: 0.3215755820274353 \n",
      "     Training Step: 24 Training Loss: 0.3263746500015259 \n",
      "     Training Step: 25 Training Loss: 0.33281952142715454 \n",
      "     Training Step: 26 Training Loss: 0.3193485736846924 \n",
      "     Training Step: 27 Training Loss: 0.3246506452560425 \n",
      "     Training Step: 28 Training Loss: 0.32059013843536377 \n",
      "     Training Step: 29 Training Loss: 0.3600273132324219 \n",
      "     Training Step: 30 Training Loss: 0.3476852774620056 \n",
      "     Training Step: 31 Training Loss: 0.3541988134384155 \n",
      "     Training Step: 32 Training Loss: 0.32415497303009033 \n",
      "     Training Step: 33 Training Loss: 0.3570890426635742 \n",
      "     Training Step: 34 Training Loss: 0.36112597584724426 \n",
      "     Training Step: 35 Training Loss: 0.3667494058609009 \n",
      "     Training Step: 36 Training Loss: 0.3205389976501465 \n",
      "     Training Step: 37 Training Loss: 0.32218265533447266 \n",
      "     Training Step: 38 Training Loss: 0.3363545536994934 \n",
      "     Training Step: 39 Training Loss: 0.33648037910461426 \n",
      "     Training Step: 40 Training Loss: 0.3378588855266571 \n",
      "     Training Step: 41 Training Loss: 0.32054516673088074 \n",
      "     Training Step: 42 Training Loss: 0.3270604908466339 \n",
      "     Training Step: 43 Training Loss: 0.3371926248073578 \n",
      "     Training Step: 44 Training Loss: 0.32702210545539856 \n",
      "     Training Step: 45 Training Loss: 0.33197659254074097 \n",
      "     Training Step: 46 Training Loss: 0.34251996874809265 \n",
      "     Training Step: 47 Training Loss: 0.32732468843460083 \n",
      "     Training Step: 48 Training Loss: 0.3189179301261902 \n",
      "     Training Step: 49 Training Loss: 0.31562668085098267 \n",
      "     Training Step: 50 Training Loss: 0.3438168168067932 \n",
      "     Training Step: 51 Training Loss: 0.36415380239486694 \n",
      "     Training Step: 52 Training Loss: 0.3713100552558899 \n",
      "     Training Step: 53 Training Loss: 0.3575993776321411 \n",
      "     Training Step: 54 Training Loss: 0.33440154790878296 \n",
      "     Training Step: 55 Training Loss: 0.36708080768585205 \n",
      "     Training Step: 56 Training Loss: 0.3766888678073883 \n",
      "     Training Step: 57 Training Loss: 0.35907667875289917 \n",
      "     Training Step: 58 Training Loss: 0.3174947500228882 \n",
      "     Training Step: 59 Training Loss: 0.32645297050476074 \n",
      "     Training Step: 60 Training Loss: 0.3265209197998047 \n",
      "     Training Step: 61 Training Loss: 0.33606287837028503 \n",
      "     Training Step: 62 Training Loss: 0.3297596275806427 \n",
      "     Training Step: 63 Training Loss: 0.31739577651023865 \n",
      "     Training Step: 64 Training Loss: 0.31895196437835693 \n",
      "     Training Step: 65 Training Loss: 0.3208823502063751 \n",
      "     Training Step: 66 Training Loss: 0.4132426381111145 \n",
      "     Training Step: 67 Training Loss: 0.35344281792640686 \n",
      "     Training Step: 68 Training Loss: 0.32972535490989685 \n",
      "     Training Step: 69 Training Loss: 0.3320483863353729 \n",
      "     Training Step: 70 Training Loss: 0.36734506487846375 \n",
      "     Training Step: 71 Training Loss: 0.347772479057312 \n",
      "     Training Step: 72 Training Loss: 0.3290058672428131 \n",
      "     Training Step: 73 Training Loss: 0.3135654330253601 \n",
      "     Training Step: 74 Training Loss: 0.34491628408432007 \n",
      "     Training Step: 75 Training Loss: 0.3176248073577881 \n",
      "     Training Step: 76 Training Loss: 0.32565823197364807 \n",
      "     Training Step: 77 Training Loss: 0.34267550706863403 \n",
      "     Training Step: 78 Training Loss: 0.324772447347641 \n",
      "     Training Step: 79 Training Loss: 0.31707149744033813 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.3251310884952545 \n",
      "     Validation Step: 1 Validation Loss: 0.31421956419944763 \n",
      "     Validation Step: 2 Validation Loss: 0.3238421082496643 \n",
      "     Validation Step: 3 Validation Loss: 0.31434547901153564 \n",
      "     Validation Step: 4 Validation Loss: 0.3371351361274719 \n",
      "     Validation Step: 5 Validation Loss: 0.31718310713768005 \n",
      "     Validation Step: 6 Validation Loss: 0.3177143335342407 \n",
      "     Validation Step: 7 Validation Loss: 0.3301684856414795 \n",
      "     Validation Step: 8 Validation Loss: 0.31975990533828735 \n",
      "     Validation Step: 9 Validation Loss: 0.33349302411079407 \n",
      "     Validation Step: 10 Validation Loss: 0.3417419493198395 \n",
      "     Validation Step: 11 Validation Loss: 0.3243999183177948 \n",
      "     Validation Step: 12 Validation Loss: 0.32045307755470276 \n",
      "     Validation Step: 13 Validation Loss: 0.3387640118598938 \n",
      "     Validation Step: 14 Validation Loss: 0.31578031182289124 \n",
      "Epoch: 11\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.3263636827468872 \n",
      "     Training Step: 1 Training Loss: 0.3165488839149475 \n",
      "     Training Step: 2 Training Loss: 0.333469957113266 \n",
      "     Training Step: 3 Training Loss: 0.32586729526519775 \n",
      "     Training Step: 4 Training Loss: 0.3498113751411438 \n",
      "     Training Step: 5 Training Loss: 0.32468467950820923 \n",
      "     Training Step: 6 Training Loss: 0.33253172039985657 \n",
      "     Training Step: 7 Training Loss: 0.3325880765914917 \n",
      "     Training Step: 8 Training Loss: 0.32278311252593994 \n",
      "     Training Step: 9 Training Loss: 0.3163038492202759 \n",
      "     Training Step: 10 Training Loss: 0.3385734260082245 \n",
      "     Training Step: 11 Training Loss: 0.3520088195800781 \n",
      "     Training Step: 12 Training Loss: 0.32437315583229065 \n",
      "     Training Step: 13 Training Loss: 0.3421051502227783 \n",
      "     Training Step: 14 Training Loss: 0.3394125998020172 \n",
      "     Training Step: 15 Training Loss: 0.3248213827610016 \n",
      "     Training Step: 16 Training Loss: 0.3460359275341034 \n",
      "     Training Step: 17 Training Loss: 0.3210761249065399 \n",
      "     Training Step: 18 Training Loss: 0.3510521650314331 \n",
      "     Training Step: 19 Training Loss: 0.3189307451248169 \n",
      "     Training Step: 20 Training Loss: 0.3201366662979126 \n",
      "     Training Step: 21 Training Loss: 0.3203631043434143 \n",
      "     Training Step: 22 Training Loss: 0.3251303434371948 \n",
      "     Training Step: 23 Training Loss: 0.31761234998703003 \n",
      "     Training Step: 24 Training Loss: 0.31578096747398376 \n",
      "     Training Step: 25 Training Loss: 0.3286568820476532 \n",
      "     Training Step: 26 Training Loss: 0.3176455795764923 \n",
      "     Training Step: 27 Training Loss: 0.34966403245925903 \n",
      "     Training Step: 28 Training Loss: 0.3211556375026703 \n",
      "     Training Step: 29 Training Loss: 0.35036003589630127 \n",
      "     Training Step: 30 Training Loss: 0.3401068150997162 \n",
      "     Training Step: 31 Training Loss: 0.3296779990196228 \n",
      "     Training Step: 32 Training Loss: 0.3318261206150055 \n",
      "     Training Step: 33 Training Loss: 0.3378051519393921 \n",
      "     Training Step: 34 Training Loss: 0.31827402114868164 \n",
      "     Training Step: 35 Training Loss: 0.3234013020992279 \n",
      "     Training Step: 36 Training Loss: 0.3368995487689972 \n",
      "     Training Step: 37 Training Loss: 0.31821292638778687 \n",
      "     Training Step: 38 Training Loss: 0.31573694944381714 \n",
      "     Training Step: 39 Training Loss: 0.418405681848526 \n",
      "     Training Step: 40 Training Loss: 0.3471395969390869 \n",
      "     Training Step: 41 Training Loss: 0.32119178771972656 \n",
      "     Training Step: 42 Training Loss: 0.3333985507488251 \n",
      "     Training Step: 43 Training Loss: 0.3599673807621002 \n",
      "     Training Step: 44 Training Loss: 0.4057181477546692 \n",
      "     Training Step: 45 Training Loss: 0.3666875660419464 \n",
      "     Training Step: 46 Training Loss: 0.3546784222126007 \n",
      "     Training Step: 47 Training Loss: 0.3230316936969757 \n",
      "     Training Step: 48 Training Loss: 0.3511514663696289 \n",
      "     Training Step: 49 Training Loss: 0.35997313261032104 \n",
      "     Training Step: 50 Training Loss: 0.3788575530052185 \n",
      "     Training Step: 51 Training Loss: 0.3200691044330597 \n",
      "     Training Step: 52 Training Loss: 0.33863240480422974 \n",
      "     Training Step: 53 Training Loss: 0.3188096880912781 \n",
      "     Training Step: 54 Training Loss: 0.31802821159362793 \n",
      "     Training Step: 55 Training Loss: 0.3163318336009979 \n",
      "     Training Step: 56 Training Loss: 0.4052070677280426 \n",
      "     Training Step: 57 Training Loss: 0.37355637550354004 \n",
      "     Training Step: 58 Training Loss: 0.31680572032928467 \n",
      "     Training Step: 59 Training Loss: 0.34124812483787537 \n",
      "     Training Step: 60 Training Loss: 0.32551905512809753 \n",
      "     Training Step: 61 Training Loss: 0.3176948130130768 \n",
      "     Training Step: 62 Training Loss: 0.33341413736343384 \n",
      "     Training Step: 63 Training Loss: 0.412707656621933 \n",
      "     Training Step: 64 Training Loss: 0.3372807204723358 \n",
      "     Training Step: 65 Training Loss: 0.31911224126815796 \n",
      "     Training Step: 66 Training Loss: 0.31578487157821655 \n",
      "     Training Step: 67 Training Loss: 0.329279363155365 \n",
      "     Training Step: 68 Training Loss: 0.3280319273471832 \n",
      "     Training Step: 69 Training Loss: 0.3137606680393219 \n",
      "     Training Step: 70 Training Loss: 0.3236541748046875 \n",
      "     Training Step: 71 Training Loss: 0.36522573232650757 \n",
      "     Training Step: 72 Training Loss: 0.31810855865478516 \n",
      "     Training Step: 73 Training Loss: 0.3177269399166107 \n",
      "     Training Step: 74 Training Loss: 0.3509673774242401 \n",
      "     Training Step: 75 Training Loss: 0.3241238296031952 \n",
      "     Training Step: 76 Training Loss: 0.3277631998062134 \n",
      "     Training Step: 77 Training Loss: 0.31475016474723816 \n",
      "     Training Step: 78 Training Loss: 0.3187253773212433 \n",
      "     Training Step: 79 Training Loss: 0.3207843601703644 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.3233855366706848 \n",
      "     Validation Step: 1 Validation Loss: 0.40303418040275574 \n",
      "     Validation Step: 2 Validation Loss: 0.3194923996925354 \n",
      "     Validation Step: 3 Validation Loss: 0.35526248812675476 \n",
      "     Validation Step: 4 Validation Loss: 0.364226371049881 \n",
      "     Validation Step: 5 Validation Loss: 0.3521651029586792 \n",
      "     Validation Step: 6 Validation Loss: 0.3241433799266815 \n",
      "     Validation Step: 7 Validation Loss: 0.3891129791736603 \n",
      "     Validation Step: 8 Validation Loss: 0.35702407360076904 \n",
      "     Validation Step: 9 Validation Loss: 0.3410162031650543 \n",
      "     Validation Step: 10 Validation Loss: 0.3437749743461609 \n",
      "     Validation Step: 11 Validation Loss: 0.3206384778022766 \n",
      "     Validation Step: 12 Validation Loss: 0.35753998160362244 \n",
      "     Validation Step: 13 Validation Loss: 0.3201294243335724 \n",
      "     Validation Step: 14 Validation Loss: 0.32018494606018066 \n",
      "Epoch: 12\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.3612068295478821 \n",
      "     Training Step: 1 Training Loss: 0.3442964553833008 \n",
      "     Training Step: 2 Training Loss: 0.3306143879890442 \n",
      "     Training Step: 3 Training Loss: 0.31656306982040405 \n",
      "     Training Step: 4 Training Loss: 0.3597925007343292 \n",
      "     Training Step: 5 Training Loss: 0.3939087986946106 \n",
      "     Training Step: 6 Training Loss: 0.32273322343826294 \n",
      "     Training Step: 7 Training Loss: 0.3667054772377014 \n",
      "     Training Step: 8 Training Loss: 0.33906257152557373 \n",
      "     Training Step: 9 Training Loss: 0.31493258476257324 \n",
      "     Training Step: 10 Training Loss: 0.35513588786125183 \n",
      "     Training Step: 11 Training Loss: 0.330485999584198 \n",
      "     Training Step: 12 Training Loss: 0.3853234350681305 \n",
      "     Training Step: 13 Training Loss: 0.3686712682247162 \n",
      "     Training Step: 14 Training Loss: 0.31738099455833435 \n",
      "     Training Step: 15 Training Loss: 0.3176560401916504 \n",
      "     Training Step: 16 Training Loss: 0.32264506816864014 \n",
      "     Training Step: 17 Training Loss: 0.39534831047058105 \n",
      "     Training Step: 18 Training Loss: 0.40811920166015625 \n",
      "     Training Step: 19 Training Loss: 0.31835174560546875 \n",
      "     Training Step: 20 Training Loss: 0.31748828291893005 \n",
      "     Training Step: 21 Training Loss: 0.326389878988266 \n",
      "     Training Step: 22 Training Loss: 0.31666049361228943 \n",
      "     Training Step: 23 Training Loss: 0.3170120120048523 \n",
      "     Training Step: 24 Training Loss: 0.3582448959350586 \n",
      "     Training Step: 25 Training Loss: 0.3631918430328369 \n",
      "     Training Step: 26 Training Loss: 0.31620946526527405 \n",
      "     Training Step: 27 Training Loss: 0.34181779623031616 \n",
      "     Training Step: 28 Training Loss: 0.3305794894695282 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_7004\\550524826.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mnum_epochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m20\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mmodel_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"base_model_wired_alt.pt\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0mtrain_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_validate_ACOPF\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"base_model.pt\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mtrain_validate_ACOPF\u001B[1;34m(model, model_name, optimizer, train_inputs, val_inputs, start_epoch, num_epochs, return_outputs, save_model)\u001B[0m\n\u001B[0;32m   2422\u001B[0m             \u001B[1;31m# Compute the loss and its gradients for RMSE loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2423\u001B[0m             physics_loss, mre_loss, loss, penalty_loss = self_supervised_hetero_obj_fn(out_dict, bus_idx_neighbors_dict,\n\u001B[1;32m-> 2424\u001B[1;33m                                                                                        res_bus, constraint_dict, beta)\n\u001B[0m\u001B[0;32m   2425\u001B[0m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2426\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mself_supervised_hetero_obj_fn\u001B[1;34m(out_dict, bus_idx_neighbors_dict, res_bus_dict, constraints_dict, beta)\u001B[0m\n\u001B[0;32m   2229\u001B[0m     \u001B[1;31m# wired_losses = loss(torch.stack(output), torch.stack(wired))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2230\u001B[0m     \u001B[1;31m# wired_mre_losses = mre_loss_fn(torch.stack(output), torch.stack(wired))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2231\u001B[1;33m     \u001B[0mphysics_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_physics_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2232\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2233\u001B[0m     \u001B[0munsupervised_loss_P\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_supplied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequires_grad\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mcalc_physics_loss\u001B[1;34m(out_dict, bus_idx_neighbors_dict)\u001B[0m\n\u001B[0;32m   2866\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2867\u001B[0m                 \u001B[0mP\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2868\u001B[1;33m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2869\u001B[0m                 \u001B[1;31m# ACOPF Equation for Q_i\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2870\u001B[0m                 \u001B[0mQ_ij\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_i\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_j\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mG_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mB_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mformat_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mformat_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mextract_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    198\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m     \u001B[0mstack\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mStackSummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwalk_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m     \u001B[0mstack\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreverse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mstack\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    357\u001B[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001B[0;32m    358\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mfilename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfnames\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 359\u001B[1;33m             \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckcache\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    360\u001B[0m         \u001B[1;31m# If immediate lookup was desired, trigger lookups now.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlookup_lines\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\IPython\\core\\compilerop.py\u001B[0m in \u001B[0;36mcheck_linecache_ipython\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \"\"\"\n\u001B[0;32m    184\u001B[0m     \u001B[1;31m# First call the original checkcache as intended\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 185\u001B[1;33m     \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_checkcache_ori\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    186\u001B[0m     \u001B[1;31m# Then, update back the cache with our data, so that tracebacks related\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m     \u001B[1;31m# to our compiled codes can be produced.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\linecache.py\u001B[0m in \u001B[0;36mcheckcache\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mcontinue\u001B[0m   \u001B[1;31m# no-op for files loaded via a __loader__\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m             \u001B[0mstat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfullname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mcache\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, 100, 20) :\n",
    "    start_epoch = i\n",
    "    num_epochs = 20 + i\n",
    "    model_name = \"base_model_wired_alt.pt\"\n",
    "    train_output, val_output = train_validate_ACOPF(model, \"base_model.pt\", optimizer, train_inputs, val_inputs, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  0.65535082,   0.85065937, 101.1341095 ,  30.60231972],\n       [  0.87852446,   0.        ,  97.71369934,  26.49389076],\n       [  1.17149561,   0.52058458,  91.36586761,  25.2800045 ],\n       [  2.35825972,   1.26082039,  98.54285431,  28.40731621],\n       [  2.47807118,   1.90889263,  96.10548401,  26.99606323],\n       [  2.32928439,   2.10406399,  98.14118958,  18.67367935],\n       [  2.29653556,   0.48540926,  95.84825897,  23.06588745],\n       [  2.27876434,   0.65570354,  97.94632721,  28.5507164 ],\n       [  2.23215776,   0.57320786,  98.65805054,  28.91212845],\n       [  2.49235646,   1.88243008,  96.47057343,  26.99268532],\n       [  2.54828547,   0.45856953,  97.71361542,  26.83773232],\n       [  2.24220165,   0.53578043,  97.63864136,  27.81454277],\n       [  2.4400496 ,   0.45287943,  95.73828125,  27.18199539],\n       [  2.33850514,   0.54629946,  97.04912567,  28.78719521],\n       [  2.36899636,   1.25119305,  97.65576935,  27.05794525],\n       [  2.50876187,   1.84869766,  97.14277649,  26.57588196],\n       [  2.23311795,   0.56961346,  98.56024933,  28.80678368],\n       [  2.24454429,   0.52710056,  97.39527893,  27.57834816],\n       [  2.27802443,   0.62805939,  97.30225372,  27.64239883],\n       [  2.2646766 ,   0.45337009,  95.36643982,  25.47357368],\n       [  2.38372886,   2.0842247 ,  98.04393005,  27.97853851],\n       [  2.33677202,   1.33852005,  99.00449371,  28.96012306],\n       [  2.52265902,   1.78650284, 100.3764267 ,  25.21249771],\n       [  2.24220858,   0.53574991,  97.62985992,  27.83494186],\n       [  2.23583887,   0.79436779,  96.89859772,  28.43802071],\n       [  3.01405113,  -1.52978754,  45.17313385, -42.8144722 ],\n       [  2.74578746,   0.91350269,  96.96837616,  27.43000603],\n       [  2.40798284,   0.71419048,  98.42508698,  29.17367744],\n       [  2.2848132 ,   0.53534508,  95.91337585,  25.07572365],\n       [  2.45653437,   0.51820707,  97.37248993,  24.92024422],\n       [  2.51877719,   1.7276907 , 101.95704651,  29.93992043],\n       [  2.27600542,   0.64974594,  97.85959625,  28.5065136 ],\n       [  2.49157521,   1.86581135,  97.81864929,  27.11807823],\n       [  2.9116358 ,   0.16260576,  89.95698547,  19.8903904 ],\n       [  2.34837175,   1.8966732 ,  97.3908844 ,  28.41923714],\n       [  2.29526978,   0.57758093,  95.8325119 ,  26.50872993],\n       [  2.36796681,   1.28250122,  98.77541351,  28.83038712],\n       [  2.29248657,   0.61613464,  96.8038559 ,  27.34645081],\n       [  2.41396956,   0.76245975, 100.02166748,  29.69691086],\n       [  2.37758068,   0.6415658 ,  96.41993713,  28.28798294],\n       [  2.34518738,   0.49126387,  95.81318665,  26.62782669],\n       [  2.31688413,   0.54849672,  96.5474472 ,  28.17289543],\n       [  2.27821794,   0.61421108,  97.34073639,  27.37156868],\n       [  2.27637163,   0.66863632,  98.32279968,  28.82246017],\n       [  2.42341309,   0.56487656,  96.00868225,  26.17300987],\n       [  2.28358459,   0.60640049,  96.67209625,  27.45202255],\n       [  2.31617251,   0.54507971,  96.33098602,  27.99449539],\n       [  2.50382552,   1.85614014,  96.7508316 ,  26.43014336],\n       [  2.27546623,   0.66940975,  98.35060883,  28.87668991],\n       [  2.44558411,   1.97788429,  96.34515381,  28.1722393 ],\n       [  2.68185869,   1.09617996,  98.09493256,  28.38990021],\n       [  2.53463995,   1.78114223,  97.91098022,  27.22012711],\n       [  2.28026692,   0.66045332,  98.13483429,  28.50179482],\n       [  2.42163419,   0.64123678,  97.75920868,  29.81824112],\n       [  2.27783411,   0.67588997,  98.47927856,  28.94283485],\n       [  2.2752404 ,   0.57965326,  95.92424011,  27.41014671],\n       [  2.27806785,   0.67343044,  98.40755463,  28.89763069],\n       [  2.28025638,   0.65575933,  98.13538361,  28.27817917],\n       [  2.49507557,   1.87091923,  96.47299194,  26.27114105],\n       [  2.27428284,   0.66831589,  98.31065369,  28.946558  ],\n       [  2.28665466,   0.6329999 ,  97.68325043,  27.45923805],\n       [  2.23278503,   0.57085943,  98.59484863,  28.84143448],\n       [  2.23580128,   0.55958605,  98.28643036,  28.51516151],\n       [  2.23318925,   0.5693469 ,  98.55321503,  28.79833794]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  0.65395636,   0.91567945, 113.43726349,  29.42810631],\n       [  0.87851281,   0.        , 108.61277008,  25.59692192],\n       [  1.16987846,   0.55724096, 103.16665649,  23.53556824],\n       [  2.36202032,   1.31174994, 109.1506958 ,  26.64596558],\n       [  2.47899891,   2.01026392, 107.45316315,  25.36096764],\n       [  2.32930631,   2.2199378 , 109.58473206,  16.79294968],\n       [  2.29702703,   0.51201105, 107.33950043,  21.24329758],\n       [  2.2801909 ,   0.68483496, 109.45345306,  26.64138985],\n       [  2.23273204,   0.60182142, 110.2062912 ,  27.19800758],\n       [  2.49354581,   1.98105192, 107.84737396,  25.28386307],\n       [  2.54720293,   0.48814821, 109.29827118,  25.25607681],\n       [  2.24280396,   0.56217146, 109.1360321 ,  26.07968903],\n       [  2.43797025,   0.48188353, 107.30545044,  25.60661697],\n       [  2.33934659,   0.57242441, 108.4782486 ,  26.97213173],\n       [  2.37285434,   1.30195093, 108.24641418,  25.27768898],\n       [  2.50968989,   1.94699907, 108.55965424,  24.96804428],\n       [  2.23375092,   0.59779215, 110.09792328,  27.08373642],\n       [  2.24415575,   0.55688429, 108.99532318,  25.92480659],\n       [  2.27932892,   0.65637827, 108.79974365,  25.73417473],\n       [  2.26415197,   0.47944498, 106.89300537,  23.77368927],\n       [  2.38547474,   2.19479132, 109.30426025,  26.46444702],\n       [  2.34027294,   1.39488459, 109.62480164,  27.27622414],\n       [  2.52381841,   1.88049364, 111.84564209,  23.51050377],\n       [  2.24262404,   0.56287432, 109.15218353,  26.10723305],\n       [  2.23642037,   0.8272357 , 108.38642883,  26.40777779],\n       [  3.01753679,  -1.62946749,  53.76194763, -46.65253448],\n       [  2.74569536,   0.96157694, 108.01644135,  25.75670624],\n       [  2.40656211,   0.7527442 , 110.03723145,  27.49531174],\n       [  2.28575148,   0.56111288, 107.36577606,  23.18917274],\n       [  2.45834378,   0.53913164, 108.72255707,  22.92404938],\n       [  2.51945079,   1.81954622, 113.17687988,  28.43504333],\n       [  2.27747539,   0.6784234 , 109.35964966,  26.58939171],\n       [  2.49133661,   1.96743536, 109.22169495,  25.53074646],\n       [  2.91394126,   0.15020752, 100.76760864,  17.43444633],\n       [  2.34848383,   2.00116682, 108.82382965,  26.8883934 ],\n       [  2.29560713,   0.60633707, 107.36978912,  24.64925766],\n       [  2.3712075 ,   1.33818293, 109.52745819,  27.17893219],\n       [  2.29303145,   0.6463027 , 108.36722565,  25.46832085],\n       [  2.41559531,   0.79488802, 111.5145874 ,  27.74601173],\n       [  2.37919034,   0.66740561, 107.78098297,  26.24003029],\n       [  2.34575667,   0.51357126, 107.19521332,  24.72506142],\n       [  2.31770824,   0.57212305, 107.89693451,  26.2684803 ],\n       [  2.27934862,   0.64313745, 108.8474884 ,  25.50373077],\n       [  2.27793898,   0.69808149, 109.83203125,  26.90636253],\n       [  2.42395935,   0.59205198, 107.43308258,  24.2896595 ],\n       [  2.28475564,   0.63365412, 108.16054535,  25.52520752],\n       [  2.31717307,   0.56846189, 107.67804718,  26.09079361],\n       [  2.50431297,   1.95602369, 108.13848114,  24.84480667],\n       [  2.27711043,   0.6984849 , 109.85176086,  26.9520359 ],\n       [  2.44677041,   2.08346224, 107.66533661,  26.64993477],\n       [  2.68207758,   1.1547246 , 109.3758316 ,  26.8391304 ],\n       [  2.53610729,   1.87415838, 109.2827301 ,  25.57441521],\n       [  2.28168377,   0.69005537, 109.64746094,  26.60095596],\n       [  2.4230036 ,   0.66952276, 109.19352722,  27.9182415 ],\n       [  2.27937053,   0.70581579, 109.99255371,  27.04252434],\n       [  2.27662423,   0.60411596, 107.37216187,  25.42849731],\n       [  2.27961662,   0.70320177, 109.91832733,  26.99278259],\n       [  2.28167586,   0.68523073, 109.64398956,  26.38047981],\n       [  2.49594005,   1.97017241, 107.86252594,  24.60523605],\n       [  2.27593134,   0.69728518, 109.80974579,  27.02291107],\n       [  2.28766688,   0.66319513, 109.20350647,  25.60612869],\n       [  2.23340676,   0.59915209, 110.13419342,  27.12315941],\n       [  2.23642439,   0.58723974, 109.81358337,  26.78518295],\n       [  2.23381445,   0.59754324, 110.09201813,  27.07432365]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5966.011412382126 1892.8524532318115\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 7.15626927e-01,  1.60224915e-01,  1.15818100e+02,\n         3.00056400e+01],\n       [ 9.07988217e-01,  0.00000000e+00,  9.94839325e+01,\n        -1.02789154e+02],\n       [ 1.23604334e+00,  1.46118164e-01,  1.15757896e+02,\n         3.01666393e+01],\n       [ 2.55935558e+00, -2.05223799e+00,  3.12666154e+00,\n         2.69802341e+01],\n       [ 2.44898793e+00, -3.17946029e+00,  4.55197945e+01,\n         1.82022038e+01],\n       [ 2.69901456e+00, -3.27771020e+00,  1.27916498e+01,\n         1.49252577e+01],\n       [ 2.45637790e+00,  6.56132698e-02,  1.17606575e+02,\n         3.24012260e+01],\n       [ 2.46242343e+00,  1.08196735e-01,  1.16958267e+02,\n         3.13654480e+01],\n       [ 2.45159524e+00,  6.78076744e-02,  1.19253365e+02,\n         3.20844154e+01],\n       [ 2.44778803e+00, -3.13636684e+00,  4.60496750e+01,\n         1.50414839e+01],\n       [ 2.33369640e+00, -6.41553402e-01,  1.32012894e+02,\n         5.11921272e+01],\n       [ 2.45274991e+00,  6.70785904e-02,  1.19010818e+02,\n         3.20621796e+01],\n       [ 2.42495145e+00, -9.32188034e-02,  1.21564041e+02,\n         3.67479897e+01],\n       [ 2.45438343e+00,  6.93492889e-02,  1.18829224e+02,\n         3.19055595e+01],\n       [ 2.49719710e+00, -2.45745659e+00, -8.02969456e+00,\n         4.38502159e+01],\n       [ 2.45614957e+00, -3.05748367e+00,  4.59105949e+01,\n         1.02308979e+01],\n       [ 2.45170760e+00,  6.77385330e-02,  1.19229813e+02,\n         3.20822105e+01],\n       [ 2.45296686e+00,  6.71949387e-02,  1.18968681e+02,\n         3.20531654e+01],\n       [ 2.45081232e+00,  1.44658089e-02,  1.18100395e+02,\n         3.35891113e+01],\n       [ 2.45514887e+00,  6.68792725e-02,  1.18522560e+02,\n         3.19907322e+01],\n       [ 2.32551977e+00, -3.28094697e+00,  6.75904922e+01,\n         4.14650345e+01],\n       [ 2.44387346e+00, -2.45563698e+00,  2.52547121e+00,\n         4.65119209e+01],\n       [ 2.40882624e+00, -3.15034270e+00,  5.20908241e+01,\n         1.80408249e+01],\n       [ 2.45279458e+00,  6.70833588e-02,  1.19001877e+02,\n         3.20606956e+01],\n       [ 2.40706759e+00, -2.70880222e-01,  1.23191292e+02,\n         4.05364342e+01],\n       [ 2.45501959e+00, -2.55241060e+00, -9.93712139e+00,\n         5.01007462e+01],\n       [ 2.18693515e+00, -3.17354965e+00,  8.74710007e+01,\n         3.00765438e+01],\n       [ 2.39118763e+00, -3.54922295e-01,  1.25100365e+02,\n         4.28268394e+01],\n       [ 2.44483115e+00, -2.49276161e-02,  1.18775795e+02,\n         3.45731049e+01],\n       [ 2.45855907e+00,  7.08780289e-02,  1.17239632e+02,\n         3.22674713e+01],\n       [ 2.45479126e+00,  6.11405373e-02,  1.18545563e+02,\n         3.21127014e+01],\n       [ 2.45967907e+00,  8.36267471e-02,  1.17165802e+02,\n         3.19663868e+01],\n       [ 2.44070851e+00, -3.17557335e+00,  4.82995872e+01,\n         1.96277733e+01],\n       [ 2.13762637e+00, -3.13082504e+00,  9.54478378e+01,\n         2.56336155e+01],\n       [ 2.32989668e+00, -3.43030858e+00,  7.67517395e+01,\n         7.45539627e+01],\n       [ 2.46078380e+00,  8.02674294e-02,  1.16982124e+02,\n         3.19419613e+01],\n       [ 2.54108609e+00, -2.39391565e+00, -1.24553061e+01,\n         3.92970963e+01],\n       [ 2.46507013e+00,  1.22999191e-01,  1.16735237e+02,\n         3.09032860e+01],\n       [ 2.39045882e+00, -3.48005295e-01,  1.25223640e+02,\n         4.28319664e+01],\n       [ 2.46440652e+00,  1.10797405e-01,  1.16696724e+02,\n         3.11910496e+01],\n       [ 2.46060819e+00,  9.04932022e-02,  1.18212143e+02,\n         3.10552368e+01],\n       [ 2.46246920e+00,  1.03769302e-01,  1.18263069e+02,\n         3.05707150e+01],\n       [ 2.45582497e+00,  5.23657799e-02,  1.17552055e+02,\n         3.27011108e+01],\n       [ 2.46273110e+00,  1.13848209e-01,  1.16986336e+02,\n         3.12292480e+01],\n       [ 2.45344738e+00,  4.09188271e-02,  1.17869324e+02,\n         3.30013390e+01],\n       [ 2.45758917e+00,  6.01315498e-02,  1.17330627e+02,\n         3.24726639e+01],\n       [ 2.46294361e+00,  1.04624748e-01,  1.18206924e+02,\n         3.05204678e+01],\n       [ 2.46641624e+00, -3.07379341e+00,  4.42435989e+01,\n         1.05698471e+01],\n       [ 2.46228166e+00,  1.10126019e-01,  1.17009720e+02,\n         3.13314838e+01],\n       [ 2.38323031e+00, -3.23680329e+00,  5.60065842e+01,\n         2.88161278e+01],\n       [ 2.60287559e+00, -3.30192661e+00,  2.89807568e+01,\n         3.18125992e+01],\n       [ 2.49000827e+00, -3.14425516e+00,  3.82415161e+01,\n         1.15234604e+01],\n       [ 2.46467063e+00,  1.30635738e-01,  1.16902069e+02,\n         3.07701015e+01],\n       [ 2.45628912e+00,  4.91909981e-02,  1.17467987e+02,\n         3.27159119e+01],\n       [ 2.46574513e+00,  1.41992569e-01,  1.16915657e+02,\n         3.04540958e+01],\n       [ 2.44200606e+00, -4.17814255e-02,  1.19092796e+02,\n         3.50359612e+01],\n       [ 2.46533952e+00,  1.37900352e-01,  1.16906853e+02,\n         3.05739594e+01],\n       [ 2.46395236e+00,  1.24944210e-01,  1.16947678e+02,\n         3.09221058e+01],\n       [ 2.47333208e+00, -3.16588640e+00,  4.23744583e+01,\n         1.58103561e+01],\n       [ 2.46147489e+00,  1.02510929e-01,  1.17045876e+02,\n         3.15305233e+01],\n       [ 2.46541609e+00,  1.35637283e-01,  1.16900055e+02,\n         3.05866241e+01],\n       [ 2.45167014e+00,  6.77580833e-02,  1.19237633e+02,\n         3.20830040e+01],\n       [ 2.45202831e+00,  6.74991608e-02,  1.19162010e+02,\n         3.20767517e+01],\n       [ 2.45171786e+00,  6.77452087e-02,  1.19227867e+02,\n         3.20817604e+01]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(train_output.output[:, 2]), sum(train_output.output[:, 3]))\n",
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6574.148982524872 1890.5028705596924\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 7.15729081e-01,  1.77783489e-01,  1.26331802e+02,\n         3.00175591e+01],\n       [ 9.08043390e-01,  0.00000000e+00,  1.09311668e+02,\n        -1.04568199e+02],\n       [ 1.23621632e+00,  1.62799835e-01,  1.26268112e+02,\n         3.01858730e+01],\n       [ 2.55875272e+00, -2.13988113e+00,  9.35225296e+00,\n         2.72275620e+01],\n       [ 2.45310530e+00, -3.31060219e+00,  5.29221764e+01,\n         1.79087620e+01],\n       [ 2.70498879e+00, -3.41292596e+00,  1.88522415e+01,\n         1.44304237e+01],\n       [ 2.45709201e+00,  7.98630714e-02,  1.28134369e+02,\n         3.24117546e+01],\n       [ 2.46299605e+00,  1.22897625e-01,  1.27475647e+02,\n         3.13910370e+01],\n       [ 2.45226995e+00,  8.12582970e-02,  1.29818588e+02,\n         3.21212082e+01],\n       [ 2.45450107e+00, -3.26728702e+00,  5.30583572e+01,\n         1.46673098e+01],\n       [ 2.33528609e+00, -6.52376175e-01,  1.42961456e+02,\n         5.13160248e+01],\n       [ 2.45337885e+00,  8.06016922e-02,  1.29578094e+02,\n         3.20981407e+01],\n       [ 2.42604564e+00, -8.36496353e-02,  1.32187164e+02,\n         3.67649384e+01],\n       [ 2.45508950e+00,  8.29997063e-02,  1.29369476e+02,\n         3.19389248e+01],\n       [ 2.49755332e+00, -2.55487013e+00, -2.01143217e+00,\n         4.40402298e+01],\n       [ 2.46111644e+00, -3.18477702e+00,  5.31764755e+01,\n         9.84519958e+00],\n       [ 2.45238703e+00,  8.11843872e-02,  1.29793213e+02,\n         3.21188622e+01],\n       [ 2.45359109e+00,  8.07590485e-02,  1.29535889e+02,\n         3.20884705e+01],\n       [ 2.45146595e+00,  2.65874863e-02,  1.28658386e+02,\n         3.36160545e+01],\n       [ 2.45570041e+00,  8.07037354e-02,  1.29092407e+02,\n         3.20222778e+01],\n       [ 2.32960066e+00, -3.41543317e+00,  7.57463303e+01,\n         4.14701118e+01],\n       [ 2.44406988e+00, -2.55518436e+00,  8.84015083e+00,\n         4.68282738e+01],\n       [ 2.41242981e+00, -3.28037667e+00,  5.97961502e+01,\n         1.78154545e+01],\n       [ 2.45342962e+00,  8.07566643e-02,  1.29569641e+02,\n         3.20936546e+01],\n       [ 2.40758501e+00, -2.70685673e-01,  1.33955582e+02,\n         4.06677208e+01],\n       [ 2.45540910e+00, -2.65380144e+00, -3.98336554e+00,\n         5.03708305e+01],\n       [ 2.19226116e+00, -3.30328655e+00,  9.59547653e+01,\n         2.98100147e+01],\n       [ 2.39204878e+00, -3.56551170e-01,  1.35891403e+02,\n         4.29405212e+01],\n       [ 2.44569036e+00, -1.29694939e-02,  1.29334778e+02,\n         3.45781860e+01],\n       [ 2.45945351e+00,  8.58407021e-02,  1.27732582e+02,\n         3.22518883e+01],\n       [ 2.45550842e+00,  7.49487877e-02,  1.29090134e+02,\n         3.21334381e+01],\n       [ 2.46029053e+00,  9.78288651e-02,  1.27690590e+02,\n         3.19874134e+01],\n       [ 2.44391868e+00, -3.30660772e+00,  5.59528542e+01,\n         1.94616013e+01],\n       [ 2.13811007e+00, -3.25789142e+00,  1.05253998e+02,\n         2.56132488e+01],\n       [ 2.33373108e+00, -3.57100272e+00,  8.52515030e+01,\n         7.50147705e+01],\n       [ 2.46130565e+00,  9.45534706e-02,  1.27514748e+02,\n         3.19661522e+01],\n       [ 2.54139321e+00, -2.48881578e+00, -6.59576559e+00,\n         3.94313202e+01],\n       [ 2.46554454e+00,  1.38218403e-01,  1.27255966e+02,\n         3.09331760e+01],\n       [ 2.39165927e+00, -3.48001003e-01,  1.35974945e+02,\n         4.29017029e+01],\n       [ 2.46502380e+00,  1.25713825e-01,  1.27197281e+02,\n         3.12107754e+01],\n       [ 2.46122714e+00,  1.05084419e-01,  1.28747498e+02,\n         3.10803432e+01],\n       [ 2.46307012e+00,  1.18020535e-01,  1.28783646e+02,\n         3.06130695e+01],\n       [ 2.45652022e+00,  6.61120415e-02,  1.28083496e+02,\n         3.27133827e+01],\n       [ 2.46329651e+00,  1.28585339e-01,  1.27502960e+02,\n         3.12576942e+01],\n       [ 2.45426469e+00,  5.45663834e-02,  1.28394211e+02,\n         3.30064011e+01],\n       [ 2.45819258e+00,  7.37838745e-02,  1.27865868e+02,\n         3.24938850e+01],\n       [ 2.46354065e+00,  1.19104862e-01,  1.28728531e+02,\n         3.05584812e+01],\n       [ 2.47445318e+00, -3.20333600e+00,  5.09741898e+01,\n         1.01188374e+01],\n       [ 2.46286371e+00,  1.24756336e-01,  1.27525902e+02,\n         3.13587132e+01],\n       [ 2.38767950e+00, -3.36979651e+00,  6.37261353e+01,\n         2.86442928e+01],\n       [ 2.60631020e+00, -3.43874741e+00,  3.59059143e+01,\n         3.17512188e+01],\n       [ 2.49448769e+00, -3.27497125e+00,  4.53107719e+01,\n         1.11220741e+01],\n       [ 2.46519331e+00,  1.45728111e-01,  1.27415810e+02,\n         3.08035545e+01],\n       [ 2.45691584e+00,  6.27074242e-02,  1.28006226e+02,\n         3.27345695e+01],\n       [ 2.46624229e+00,  1.57244682e-01,  1.27427658e+02,\n         3.04926128e+01],\n       [ 2.44277732e+00, -3.11455727e-02,  1.29674591e+02,\n         3.50603676e+01],\n       [ 2.46584473e+00,  1.53061867e-01,  1.27419495e+02,\n         3.06111698e+01],\n       [ 2.46449807e+00,  1.39932632e-01,  1.27461548e+02,\n         3.09533520e+01],\n       [ 2.47788086e+00, -3.29674459e+00,  4.96050453e+01,\n         1.54789190e+01],\n       [ 2.46206915e+00,  1.17027283e-01,  1.27564384e+02,\n         3.15553513e+01],\n       [ 2.46592463e+00,  1.50904655e-01,  1.27412910e+02,\n         3.06211777e+01],\n       [ 2.45234652e+00,  8.12101364e-02,  1.29801971e+02,\n         3.21196671e+01],\n       [ 2.45269276e+00,  8.10160637e-02,  1.29727066e+02,\n         3.21122780e+01],\n       [ 2.45239008e+00,  8.11824799e-02,  1.29792542e+02,\n         3.21187897e+01]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(val_output.output[:, 2]), sum(val_output.output[:, 3]))\n",
    "val_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_output.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def save_ACOPFGNN_model(model: ACOPFGNN):\n",
    "    path = r\"./Models/SelfSupervised/base_model_physics.pt\"\n",
    "\n",
    "    torch.save(model.state_dict(), path)\n",
    "save_ACOPFGNN_model(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "1e-05"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr'] /= 2.5\n",
    "optimizer.param_groups[0]['lr']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b5c3b491a67410a92324c0e5ce791ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">still-spaceship-78</strong> at: <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/dedc7nw8' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/dedc7nw8</a><br/> View job at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNDkyMTA0OA==/version_details/v3' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNDkyMTA0OA==/version_details/v3</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20231010_183350-dedc7nw8\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 101\n",
    "num_epochs = 150\n",
    "\n",
    "train_output, val_output = train_validate_ACOPF(model, optimizer, train_inputs, val_inputs, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
