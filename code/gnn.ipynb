{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from helper import *\n",
    "from heterognn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The State of the nth node is expressed by 4 real scalars:\n",
    "\n",
    "v_n -> the voltage at the node\n",
    "delta_n -> the voltage angle at the node (relative to the slack bus)\n",
    "p_n -> the net active power flowing into the node\n",
    "q_n -> the net reactive power flowing into the node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical characteristics of the network are described by the power flow equations:\n",
    "\n",
    "p = P(v, delta, W)\n",
    "q = Q(v, delta, W)\n",
    "\n",
    "-> Relate local net power generation with the global state\n",
    "-> Depends on the topology W of the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electrical grid => Weighted Graph\n",
    "\n",
    "Nodes in the graph produce/consume power\n",
    "\n",
    "Edges represent electrical connections between nodes\n",
    "\n",
    "State Matrix X element of  R(N x 4) => graph signal with 4 features\n",
    "    => Each row is the state of the corresponding Node\n",
    "\n",
    "Adjacency Matrix A => sparse matrix to represent the connections of each node, element of R(N x N), Aij = 1 if node i is connected to node j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the GNN as a mode phi(X, A, H)\n",
    "\n",
    "We want to imitate the OPF solution p*\n",
    "-> We want to minimize a loss L over a dataset T = {{X, p*}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Function:min arg H of sum over T L(p*,phi(X, A, H)) and we use L = Mean Squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once GNN model phi is trained, we do not need the costly p* from pandapower to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input Data X - R^(Nx4): Uniformly sample p_ref and q_ref of each load L with P_L ~ Uniform(0.9 * p_ref, 1.1 * p_ref) and Q_L ~ Uniform(0.9 * q_ref, 1.1 * q_ref)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pseudocode for X and y in supervised learning:\n",
    "for each P_L and Q_L:\n",
    "    Create X with sub-optimal DCOPF results\n",
    "    Create y with Pandapower calculating p* ACOPF with IPOPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatio-Temporal GNN -> superposition of a gnn with spatial info and a temporal layer (Temporal Conv,LSTM etc.) ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bus => Node in GNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['1-complete_data-mixed-all-0-sw',\n '1-complete_data-mixed-all-1-sw',\n '1-complete_data-mixed-all-2-sw',\n '1-EHVHVMVLV-mixed-all-0-sw',\n '1-EHVHVMVLV-mixed-all-1-sw',\n '1-EHVHVMVLV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-0-sw',\n '1-EHVHV-mixed-all-0-no_sw',\n '1-EHVHV-mixed-all-1-sw',\n '1-EHVHV-mixed-all-1-no_sw',\n '1-EHVHV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-2-no_sw',\n '1-EHVHV-mixed-1-0-sw',\n '1-EHVHV-mixed-1-0-no_sw',\n '1-EHVHV-mixed-1-1-sw',\n '1-EHVHV-mixed-1-1-no_sw',\n '1-EHVHV-mixed-1-2-sw',\n '1-EHVHV-mixed-1-2-no_sw',\n '1-EHVHV-mixed-2-0-sw',\n '1-EHVHV-mixed-2-0-no_sw',\n '1-EHVHV-mixed-2-1-sw',\n '1-EHVHV-mixed-2-1-no_sw',\n '1-EHVHV-mixed-2-2-sw',\n '1-EHVHV-mixed-2-2-no_sw',\n '1-EHV-mixed--0-sw',\n '1-EHV-mixed--0-no_sw',\n '1-EHV-mixed--1-sw',\n '1-EHV-mixed--1-no_sw',\n '1-EHV-mixed--2-sw',\n '1-EHV-mixed--2-no_sw',\n '1-HVMV-mixed-all-0-sw',\n '1-HVMV-mixed-all-0-no_sw',\n '1-HVMV-mixed-all-1-sw',\n '1-HVMV-mixed-all-1-no_sw',\n '1-HVMV-mixed-all-2-sw',\n '1-HVMV-mixed-all-2-no_sw',\n '1-HVMV-mixed-1.105-0-sw',\n '1-HVMV-mixed-1.105-0-no_sw',\n '1-HVMV-mixed-1.105-1-sw',\n '1-HVMV-mixed-1.105-1-no_sw',\n '1-HVMV-mixed-1.105-2-sw',\n '1-HVMV-mixed-1.105-2-no_sw',\n '1-HVMV-mixed-2.102-0-sw',\n '1-HVMV-mixed-2.102-0-no_sw',\n '1-HVMV-mixed-2.102-1-sw',\n '1-HVMV-mixed-2.102-1-no_sw',\n '1-HVMV-mixed-2.102-2-sw',\n '1-HVMV-mixed-2.102-2-no_sw',\n '1-HVMV-mixed-4.101-0-sw',\n '1-HVMV-mixed-4.101-0-no_sw',\n '1-HVMV-mixed-4.101-1-sw',\n '1-HVMV-mixed-4.101-1-no_sw',\n '1-HVMV-mixed-4.101-2-sw',\n '1-HVMV-mixed-4.101-2-no_sw',\n '1-HVMV-urban-all-0-sw',\n '1-HVMV-urban-all-0-no_sw',\n '1-HVMV-urban-all-1-sw',\n '1-HVMV-urban-all-1-no_sw',\n '1-HVMV-urban-all-2-sw',\n '1-HVMV-urban-all-2-no_sw',\n '1-HVMV-urban-2.203-0-sw',\n '1-HVMV-urban-2.203-0-no_sw',\n '1-HVMV-urban-2.203-1-sw',\n '1-HVMV-urban-2.203-1-no_sw',\n '1-HVMV-urban-2.203-2-sw',\n '1-HVMV-urban-2.203-2-no_sw',\n '1-HVMV-urban-3.201-0-sw',\n '1-HVMV-urban-3.201-0-no_sw',\n '1-HVMV-urban-3.201-1-sw',\n '1-HVMV-urban-3.201-1-no_sw',\n '1-HVMV-urban-3.201-2-sw',\n '1-HVMV-urban-3.201-2-no_sw',\n '1-HVMV-urban-4.201-0-sw',\n '1-HVMV-urban-4.201-0-no_sw',\n '1-HVMV-urban-4.201-1-sw',\n '1-HVMV-urban-4.201-1-no_sw',\n '1-HVMV-urban-4.201-2-sw',\n '1-HVMV-urban-4.201-2-no_sw',\n '1-HV-mixed--0-sw',\n '1-HV-mixed--0-no_sw',\n '1-HV-mixed--1-sw',\n '1-HV-mixed--1-no_sw',\n '1-HV-mixed--2-sw',\n '1-HV-mixed--2-no_sw',\n '1-HV-urban--0-sw',\n '1-HV-urban--0-no_sw',\n '1-HV-urban--1-sw',\n '1-HV-urban--1-no_sw',\n '1-HV-urban--2-sw',\n '1-HV-urban--2-no_sw',\n '1-MVLV-rural-all-0-sw',\n '1-MVLV-rural-all-0-no_sw',\n '1-MVLV-rural-all-1-sw',\n '1-MVLV-rural-all-1-no_sw',\n '1-MVLV-rural-all-2-sw',\n '1-MVLV-rural-all-2-no_sw',\n '1-MVLV-rural-1.108-0-sw',\n '1-MVLV-rural-1.108-0-no_sw',\n '1-MVLV-rural-1.108-1-sw',\n '1-MVLV-rural-1.108-1-no_sw',\n '1-MVLV-rural-1.108-2-sw',\n '1-MVLV-rural-1.108-2-no_sw',\n '1-MVLV-rural-2.107-0-sw',\n '1-MVLV-rural-2.107-0-no_sw',\n '1-MVLV-rural-2.107-1-sw',\n '1-MVLV-rural-2.107-1-no_sw',\n '1-MVLV-rural-2.107-2-sw',\n '1-MVLV-rural-2.107-2-no_sw',\n '1-MVLV-rural-4.101-0-sw',\n '1-MVLV-rural-4.101-0-no_sw',\n '1-MVLV-rural-4.101-1-sw',\n '1-MVLV-rural-4.101-1-no_sw',\n '1-MVLV-rural-4.101-2-sw',\n '1-MVLV-rural-4.101-2-no_sw',\n '1-MVLV-semiurb-all-0-sw',\n '1-MVLV-semiurb-all-0-no_sw',\n '1-MVLV-semiurb-all-1-sw',\n '1-MVLV-semiurb-all-1-no_sw',\n '1-MVLV-semiurb-all-2-sw',\n '1-MVLV-semiurb-all-2-no_sw',\n '1-MVLV-semiurb-3.202-0-sw',\n '1-MVLV-semiurb-3.202-0-no_sw',\n '1-MVLV-semiurb-3.202-1-sw',\n '1-MVLV-semiurb-3.202-1-no_sw',\n '1-MVLV-semiurb-3.202-2-sw',\n '1-MVLV-semiurb-3.202-2-no_sw',\n '1-MVLV-semiurb-4.201-0-sw',\n '1-MVLV-semiurb-4.201-0-no_sw',\n '1-MVLV-semiurb-4.201-1-sw',\n '1-MVLV-semiurb-4.201-1-no_sw',\n '1-MVLV-semiurb-4.201-2-sw',\n '1-MVLV-semiurb-4.201-2-no_sw',\n '1-MVLV-semiurb-5.220-0-sw',\n '1-MVLV-semiurb-5.220-0-no_sw',\n '1-MVLV-semiurb-5.220-1-sw',\n '1-MVLV-semiurb-5.220-1-no_sw',\n '1-MVLV-semiurb-5.220-2-sw',\n '1-MVLV-semiurb-5.220-2-no_sw',\n '1-MVLV-urban-all-0-sw',\n '1-MVLV-urban-all-0-no_sw',\n '1-MVLV-urban-all-1-sw',\n '1-MVLV-urban-all-1-no_sw',\n '1-MVLV-urban-all-2-sw',\n '1-MVLV-urban-all-2-no_sw',\n '1-MVLV-urban-5.303-0-sw',\n '1-MVLV-urban-5.303-0-no_sw',\n '1-MVLV-urban-5.303-1-sw',\n '1-MVLV-urban-5.303-1-no_sw',\n '1-MVLV-urban-5.303-2-sw',\n '1-MVLV-urban-5.303-2-no_sw',\n '1-MVLV-urban-6.305-0-sw',\n '1-MVLV-urban-6.305-0-no_sw',\n '1-MVLV-urban-6.305-1-sw',\n '1-MVLV-urban-6.305-1-no_sw',\n '1-MVLV-urban-6.305-2-sw',\n '1-MVLV-urban-6.305-2-no_sw',\n '1-MVLV-urban-6.309-0-sw',\n '1-MVLV-urban-6.309-0-no_sw',\n '1-MVLV-urban-6.309-1-sw',\n '1-MVLV-urban-6.309-1-no_sw',\n '1-MVLV-urban-6.309-2-sw',\n '1-MVLV-urban-6.309-2-no_sw',\n '1-MVLV-comm-all-0-sw',\n '1-MVLV-comm-all-0-no_sw',\n '1-MVLV-comm-all-1-sw',\n '1-MVLV-comm-all-1-no_sw',\n '1-MVLV-comm-all-2-sw',\n '1-MVLV-comm-all-2-no_sw',\n '1-MVLV-comm-3.403-0-sw',\n '1-MVLV-comm-3.403-0-no_sw',\n '1-MVLV-comm-3.403-1-sw',\n '1-MVLV-comm-3.403-1-no_sw',\n '1-MVLV-comm-3.403-2-sw',\n '1-MVLV-comm-3.403-2-no_sw',\n '1-MVLV-comm-4.416-0-sw',\n '1-MVLV-comm-4.416-0-no_sw',\n '1-MVLV-comm-4.416-1-sw',\n '1-MVLV-comm-4.416-1-no_sw',\n '1-MVLV-comm-4.416-2-sw',\n '1-MVLV-comm-4.416-2-no_sw',\n '1-MVLV-comm-5.401-0-sw',\n '1-MVLV-comm-5.401-0-no_sw',\n '1-MVLV-comm-5.401-1-sw',\n '1-MVLV-comm-5.401-1-no_sw',\n '1-MVLV-comm-5.401-2-sw',\n '1-MVLV-comm-5.401-2-no_sw',\n '1-MV-rural--0-sw',\n '1-MV-rural--0-no_sw',\n '1-MV-rural--1-sw',\n '1-MV-rural--1-no_sw',\n '1-MV-rural--2-sw',\n '1-MV-rural--2-no_sw',\n '1-MV-semiurb--0-sw',\n '1-MV-semiurb--0-no_sw',\n '1-MV-semiurb--1-sw',\n '1-MV-semiurb--1-no_sw',\n '1-MV-semiurb--2-sw',\n '1-MV-semiurb--2-no_sw',\n '1-MV-urban--0-sw',\n '1-MV-urban--0-no_sw',\n '1-MV-urban--1-sw',\n '1-MV-urban--1-no_sw',\n '1-MV-urban--2-sw',\n '1-MV-urban--2-no_sw',\n '1-MV-comm--0-sw',\n '1-MV-comm--0-no_sw',\n '1-MV-comm--1-sw',\n '1-MV-comm--1-no_sw',\n '1-MV-comm--2-sw',\n '1-MV-comm--2-no_sw',\n '1-LV-rural1--0-sw',\n '1-LV-rural1--0-no_sw',\n '1-LV-rural1--1-sw',\n '1-LV-rural1--1-no_sw',\n '1-LV-rural1--2-sw',\n '1-LV-rural1--2-no_sw',\n '1-LV-rural2--0-sw',\n '1-LV-rural2--0-no_sw',\n '1-LV-rural2--1-sw',\n '1-LV-rural2--1-no_sw',\n '1-LV-rural2--2-sw',\n '1-LV-rural2--2-no_sw',\n '1-LV-rural3--0-sw',\n '1-LV-rural3--0-no_sw',\n '1-LV-rural3--1-sw',\n '1-LV-rural3--1-no_sw',\n '1-LV-rural3--2-sw',\n '1-LV-rural3--2-no_sw',\n '1-LV-semiurb4--0-sw',\n '1-LV-semiurb4--0-no_sw',\n '1-LV-semiurb4--1-sw',\n '1-LV-semiurb4--1-no_sw',\n '1-LV-semiurb4--2-sw',\n '1-LV-semiurb4--2-no_sw',\n '1-LV-semiurb5--0-sw',\n '1-LV-semiurb5--0-no_sw',\n '1-LV-semiurb5--1-sw',\n '1-LV-semiurb5--1-no_sw',\n '1-LV-semiurb5--2-sw',\n '1-LV-semiurb5--2-no_sw',\n '1-LV-urban6--0-sw',\n '1-LV-urban6--0-no_sw',\n '1-LV-urban6--1-sw',\n '1-LV-urban6--1-no_sw',\n '1-LV-urban6--2-sw',\n '1-LV-urban6--2-no_sw']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get lists of simbench codes\n",
    "all_simbench_codes = sb.collect_all_simbench_codes()\n",
    "all_simbench_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Supervised Grids:\n",
    "\n",
    "grid_1 = \"1-HV-mixed--0-no_sw\"\n",
    "grid_2 = \"1-HV-urban--0-no_sw\"\n",
    "grid_3 = \"1-MV-comm--0-no_sw\"\n",
    "grid_4 = \"1-MV-semiurb--0-no_sw\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "             name  vn_kv type zone  in_service  max_vm_pu voltLvl  \\\n0      EHV Bus 57  380.0   db  NaN        True        1.1       1   \n2     EHV Bus 143  380.0   db  NaN        True        1.1       1   \n4    EHV Bus 1649  220.0   db  NaN        True        1.1       1   \n12      HV1 Bus 1  110.0   db  NaN        True        1.1       3   \n14      HV1 Bus 3  110.0   db  NaN        True        1.1       3   \n..            ...    ...  ...  ...         ...        ...     ...   \n124   HV1 Bus 113  110.0   db  NaN        True        1.1       3   \n126   HV1 Bus 115  110.0   db  NaN        True        1.1       3   \n128   HV1 Bus 117  110.0   db  NaN        True        1.1       3   \n130   HV1 Bus 119  110.0   db  NaN        True        1.1       3   \n132   HV1 Bus 121  110.0   db  NaN        True        1.1       3   \n\n              substation    subnet  min_vm_pu  \n0    EHV_HV_substation_1  EHV1_HV1        0.9  \n2    EHV_HV_substation_2  EHV1_HV1        0.9  \n4    EHV_HV_substation_3  EHV1_HV1        0.9  \n12                   NaN       HV1        0.9  \n14                   NaN       HV1        0.9  \n..                   ...       ...        ...  \n124                  NaN       HV1        0.9  \n126                  NaN       HV1        0.9  \n128                  NaN       HV1        0.9  \n130                  NaN       HV1        0.9  \n132                  NaN       HV1        0.9  \n\n[64 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>vn_kv</th>\n      <th>type</th>\n      <th>zone</th>\n      <th>in_service</th>\n      <th>max_vm_pu</th>\n      <th>voltLvl</th>\n      <th>substation</th>\n      <th>subnet</th>\n      <th>min_vm_pu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EHV Bus 57</td>\n      <td>380.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>1</td>\n      <td>EHV_HV_substation_1</td>\n      <td>EHV1_HV1</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EHV Bus 143</td>\n      <td>380.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>1</td>\n      <td>EHV_HV_substation_2</td>\n      <td>EHV1_HV1</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EHV Bus 1649</td>\n      <td>220.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>1</td>\n      <td>EHV_HV_substation_3</td>\n      <td>EHV1_HV1</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>HV1 Bus 1</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV1</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>HV1 Bus 3</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV1</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>HV1 Bus 113</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV1</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>HV1 Bus 115</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV1</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>HV1 Bus 117</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV1</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>HV1 Bus 119</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV1</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>HV1 Bus 121</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV1</td>\n      <td>0.9</td>\n    </tr>\n  </tbody>\n</table>\n<p>64 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = process_network(grid_1)\n",
    "net.bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "             name  bus  vm_pu  va_degree  slack_weight  in_service sn_mva  \\\n0  EHV Ext_grid 8    2  1.092        0.0           1.0        True    NaN   \n\n  max_p_mw profile voltLvl type       subnet  p_disp_mw max_q_mvar min_q_mvar  \\\n0      NaN     NaN       1  NaN  HV1_EHV1_eq        NaN        NaN        NaN   \n\n  min_p_mw    phys_type  \n0      NaN  ExternalNet  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>bus</th>\n      <th>vm_pu</th>\n      <th>va_degree</th>\n      <th>slack_weight</th>\n      <th>in_service</th>\n      <th>sn_mva</th>\n      <th>max_p_mw</th>\n      <th>profile</th>\n      <th>voltLvl</th>\n      <th>type</th>\n      <th>subnet</th>\n      <th>p_disp_mw</th>\n      <th>max_q_mvar</th>\n      <th>min_q_mvar</th>\n      <th>min_p_mw</th>\n      <th>phys_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EHV Ext_grid 8</td>\n      <td>2</td>\n      <td>1.092</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>HV1_EHV1_eq</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ExternalNet</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.ext_grid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "             name  vn_kv type zone  in_service  max_vm_pu voltLvl  \\\n0    EHV Bus 1865  220.0   db  NaN        True        1.1       1   \n5       HV2 Bus 1  110.0   db  NaN        True        1.1       3   \n11      HV2 Bus 7  110.0   db  NaN        True        1.1       3   \n27     HV2 Bus 23  110.0   db  NaN        True        1.1       3   \n31     HV2 Bus 27  110.0   db  NaN        True        1.1       3   \n..            ...    ...  ...  ...         ...        ...     ...   \n351   HV2 Bus 347  110.0    b  NaN        True        1.1       3   \n353   HV2 Bus 349  110.0   db  NaN        True        1.1       3   \n358   HV2 Bus 354  110.0   db  NaN        True        1.1       3   \n361   HV2 Bus 357  110.0   db  NaN        True        1.1       3   \n365   HV2 Bus 361  110.0   db  NaN        True        1.1       3   \n\n                 substation       subnet  min_vm_pu  \n0       EHV_HV_substation_4     EHV1_HV2        0.9  \n5                       NaN          HV2        0.9  \n11                      NaN          HV2        0.9  \n27                      NaN          HV2        0.9  \n31                      NaN          HV2        0.9  \n..                      ...          ...        ...  \n351  HV2_MV1.205_Substation  HV2_MV1.205        0.9  \n353                     NaN          HV2        0.9  \n358                     NaN          HV2        0.9  \n361                     NaN          HV2        0.9  \n365                     NaN          HV2        0.9  \n\n[82 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>vn_kv</th>\n      <th>type</th>\n      <th>zone</th>\n      <th>in_service</th>\n      <th>max_vm_pu</th>\n      <th>voltLvl</th>\n      <th>substation</th>\n      <th>subnet</th>\n      <th>min_vm_pu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EHV Bus 1865</td>\n      <td>220.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>1</td>\n      <td>EHV_HV_substation_4</td>\n      <td>EHV1_HV2</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>HV2 Bus 1</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV2</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>HV2 Bus 7</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV2</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>HV2 Bus 23</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV2</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>HV2 Bus 27</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV2</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>351</th>\n      <td>HV2 Bus 347</td>\n      <td>110.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>HV2_MV1.205_Substation</td>\n      <td>HV2_MV1.205</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>353</th>\n      <td>HV2 Bus 349</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV2</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>358</th>\n      <td>HV2 Bus 354</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV2</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>HV2 Bus 357</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV2</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <th>365</th>\n      <td>HV2 Bus 361</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.1</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>HV2</td>\n      <td>0.9</td>\n    </tr>\n  </tbody>\n</table>\n<p>82 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = process_network(grid_2)\n",
    "net.bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                    name  vn_kv type zone  in_service  max_vm_pu voltLvl  \\\n0             HV1 Bus 13  110.0    b  NaN        True      1.100       3   \n2      MV4.101 busbar1.1   20.0    b  NaN        True      1.055       5   \n5    MV4.101 RS busbar1A   20.0    b  NaN        True      1.055       5   \n7          MV4.101 Bus 7   20.0    b  NaN        True      1.055       5   \n8          MV4.101 Bus 8   20.0    b  NaN        True      1.055       5   \n..                   ...    ...  ...  ...         ...        ...     ...   \n102      MV4.101 Bus 102   20.0    n  NaN        True      1.055       5   \n103      MV4.101 Bus 103   20.0    b  NaN        True      1.055       5   \n104      MV4.101 Bus 104   20.0    b  NaN        True      1.055       5   \n105      MV4.101 Bus 105   20.0    b  NaN        True      1.055       5   \n106      MV4.101 Bus 106   20.0    b  NaN        True      1.055       5   \n\n                 substation                   subnet  min_vm_pu  \n0    HV1_MV4.101_Substation              HV1_MV4.101      0.900  \n2    HV1_MV4.101_Substation                  MV4.101      0.965  \n5    MV4.101_Remote_Station                  MV4.101      0.965  \n7                       NaN  MV4.101_LV4.401_Feeder1      0.965  \n8                       NaN  MV4.101_LV2.401_Feeder1      0.965  \n..                      ...                      ...        ...  \n102                     NaN          MV4.101_Feeder9      0.965  \n103                     NaN  MV4.101_LV5.420_Feeder9      0.965  \n104                     NaN  MV4.101_LV5.421_Feeder9      0.965  \n105                     NaN  MV4.101_LV5.422_Feeder9      0.965  \n106                     NaN  MV4.101_LV4.423_Feeder9      0.965  \n\n[103 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>vn_kv</th>\n      <th>type</th>\n      <th>zone</th>\n      <th>in_service</th>\n      <th>max_vm_pu</th>\n      <th>voltLvl</th>\n      <th>substation</th>\n      <th>subnet</th>\n      <th>min_vm_pu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HV1 Bus 13</td>\n      <td>110.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.100</td>\n      <td>3</td>\n      <td>HV1_MV4.101_Substation</td>\n      <td>HV1_MV4.101</td>\n      <td>0.900</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MV4.101 busbar1.1</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>HV1_MV4.101_Substation</td>\n      <td>MV4.101</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>MV4.101 RS busbar1A</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>MV4.101_Remote_Station</td>\n      <td>MV4.101</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>MV4.101 Bus 7</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV4.101_LV4.401_Feeder1</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>MV4.101 Bus 8</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV4.101_LV2.401_Feeder1</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>MV4.101 Bus 102</td>\n      <td>20.0</td>\n      <td>n</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV4.101_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>MV4.101 Bus 103</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV4.101_LV5.420_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>MV4.101 Bus 104</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV4.101_LV5.421_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>MV4.101 Bus 105</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV4.101_LV5.422_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>MV4.101 Bus 106</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV4.101_LV4.423_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n  </tbody>\n</table>\n<p>103 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = process_network(grid_3)\n",
    "net.bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                  name  vn_kv type zone  in_service  max_vm_pu voltLvl  \\\n0           HV1 Bus 19  110.0   db  NaN        True      1.100       3   \n2    MV2.101 busbar1.1   20.0    b  NaN        True      1.055       5   \n4        MV2.101 Bus 4   20.0    b  NaN        True      1.055       5   \n5        MV2.101 Bus 5   20.0    b  NaN        True      1.055       5   \n6        MV2.101 Bus 6   20.0    b  NaN        True      1.055       5   \n..                 ...    ...  ...  ...         ...        ...     ...   \n112    MV2.101 Bus 112   20.0    b  NaN        True      1.055       5   \n113    MV2.101 Bus 113   20.0    b  NaN        True      1.055       5   \n114    MV2.101 Bus 114   20.0    b  NaN        True      1.055       5   \n115    MV2.101 Bus 115   20.0    b  NaN        True      1.055       5   \n116    MV2.101 Bus 116   20.0    b  NaN        True      1.055       5   \n\n                 substation                   subnet  min_vm_pu  \n0    HV1_MV2.101_Substation              HV1_MV2.101      0.900  \n2    HV1_MV2.101_Substation                  MV2.101      0.965  \n4                       NaN  MV2.101_LV6.201_Feeder1      0.965  \n5                       NaN  MV2.101_LV5.201_Feeder1      0.965  \n6                       NaN  MV2.101_LV5.202_Feeder1      0.965  \n..                      ...                      ...        ...  \n112                     NaN  MV2.101_LV4.236_Feeder9      0.965  \n113                     NaN  MV2.101_LV2.225_Feeder9      0.965  \n114                     NaN  MV2.101_LV1.206_Feeder9      0.965  \n115                     NaN  MV2.101_LV3.217_Feeder9      0.965  \n116                     NaN  MV2.101_LV3.218_Feeder9      0.965  \n\n[115 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>vn_kv</th>\n      <th>type</th>\n      <th>zone</th>\n      <th>in_service</th>\n      <th>max_vm_pu</th>\n      <th>voltLvl</th>\n      <th>substation</th>\n      <th>subnet</th>\n      <th>min_vm_pu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HV1 Bus 19</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.100</td>\n      <td>3</td>\n      <td>HV1_MV2.101_Substation</td>\n      <td>HV1_MV2.101</td>\n      <td>0.900</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MV2.101 busbar1.1</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>HV1_MV2.101_Substation</td>\n      <td>MV2.101</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MV2.101 Bus 4</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV2.101_LV6.201_Feeder1</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>MV2.101 Bus 5</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV2.101_LV5.201_Feeder1</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>MV2.101 Bus 6</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV2.101_LV5.202_Feeder1</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>MV2.101 Bus 112</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV2.101_LV4.236_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>MV2.101 Bus 113</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV2.101_LV2.225_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>MV2.101 Bus 114</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV2.101_LV1.206_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>MV2.101 Bus 115</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV2.101_LV3.217_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>MV2.101 Bus 116</td>\n      <td>20.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>1.055</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>MV2.101_LV3.218_Feeder9</td>\n      <td>0.965</td>\n    </tr>\n  </tbody>\n</table>\n<p>115 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = process_network(grid_4)\n",
    "net.bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'/'.join(os.path.dirname(os.path.abspath(__file__).split(\"\\\\\"))) + r\"/Models/SelfSupervised/base_model.pt\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net.bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_json('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'\n",
    "#Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "#TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "#TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "#NETWORK CONSTRAINTS\n",
    "\n",
    "#Maximize the branch limits\n",
    "\n",
    "#max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "#for i in range(len(max_i_ka)):\n",
    "# max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "#Maximize line loading percents\n",
    "max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo loading percent\n",
    "max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo3w loading percent\n",
    "max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Cost assignment\n",
    "\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "pp.runopp(net,verbose=True)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = read_unsupervised_dataset('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_data[0].has_isolated_nodes()\n",
    "#train_data[0].has_self_loops()\n",
    "#train_data[0].is_undirected()\n",
    "x_dict = train_data[0].to_dict()\n",
    "#to_json(train_dict)\n",
    "ln = len(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"][0])\n",
    "print(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"])#[:, :int(ln/2)]\n",
    "x_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')\n",
    "idx_mapper, node_types_as_dict = extract_node_types_as_dict(net)\n",
    "for key in node_types_as_dict:\n",
    "    print(f\"Bus Type: {key}\")\n",
    "    for i in range(len(node_types_as_dict[key])):\n",
    "        #node_types_as_dict[key][i] = idx_mapper[node_types_as_dict[key][i]]\n",
    "        print(str(node_types_as_dict[key][i]))\n",
    "    print(\"-------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_dict[\"PQ\"]['x'][2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_available = []\n",
    "for nw_name in all_simbench_codes:\n",
    "        net = sb.get_simbench_net(nw_name)\n",
    "        print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "        #dict_probs = pp.diagnostic(net,report_style='None')\n",
    "        #for bus_num in dict_probs['multiple_voltage_controlling_elements_per_bus']['buses_with_gens_and_ext_grids']:\n",
    "        #    net.gen = net.gen.drop(net.gen[net.gen.bus == bus_num].index)\n",
    "\n",
    "        #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "        #Set upper and lower limits of active-reactive powers of loads\n",
    "        min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "        p_mw = list(net.load.p_mw.values)\n",
    "        q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "        for i in range(len(p_mw)):\n",
    "            min_p_mw_val.append(p_mw[i])\n",
    "            max_p_mw_val.append(p_mw[i])\n",
    "            min_q_mvar_val.append(q_mvar[i])\n",
    "            max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "        net.load.min_p_mw = min_p_mw_val\n",
    "        net.load.max_p_mw = max_p_mw_val\n",
    "        net.load.min_q_mvar = min_q_mvar_val\n",
    "        net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "        #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "        ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "        pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "        #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "        #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "        #NETWORK CONSTRAINTS\n",
    "\n",
    "        #Maximize the branch limits\n",
    "\n",
    "        #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "        #for i in range(len(max_i_ka)):\n",
    "        # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "        #Maximize line loading percents\n",
    "        max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo loading percent\n",
    "        max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo3w loading percent\n",
    "        max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Cost assignment\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "        try:\n",
    "            pp.runpm_dc_opf(net) # Run DCOPP\n",
    "        except pp.OPFNotConverged:\n",
    "            text = \"DC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "            print(text)\n",
    "            continue\n",
    "        print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR DCOPF\")\n",
    "        grids_available.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_available:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) # dcopp on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_ready = []\n",
    "for nw_name in all_simbench_codes[6:]:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "    #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "    #Set upper and lower limits of active-reactive powers of loads\n",
    "    min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "    p_mw = list(net.load.p_mw.values)\n",
    "    q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "    for i in range(len(p_mw)):\n",
    "        min_p_mw_val.append(p_mw[i])\n",
    "        max_p_mw_val.append(p_mw[i])\n",
    "        min_q_mvar_val.append(q_mvar[i])\n",
    "        max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "    net.load.min_p_mw = min_p_mw_val\n",
    "    net.load.max_p_mw = max_p_mw_val\n",
    "    net.load.min_q_mvar = min_q_mvar_val\n",
    "    net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "    #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "    ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "    pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "    #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "    #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "    #NETWORK CONSTRAINTS\n",
    "\n",
    "    #Maximize the branch limits\n",
    "\n",
    "    #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "    #for i in range(len(max_i_ka)):\n",
    "    # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "    #Maximize line loading percents\n",
    "    max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo loading percent\n",
    "    max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo3w loading percent\n",
    "    max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Cost assignment\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "    #ac_converged = True\n",
    "\n",
    "    #start_vec_name = \"\"\n",
    "    #for init in [\"pf\", \"flat\", \"results\"]:\n",
    "    #    try:\n",
    "    #        pp.runopp(net, init=init)  # Calculate ACOPF with IPFOPT\n",
    "    #    except pp.OPFNotConverged:\n",
    "    #        if init == \"results\":\n",
    "    #            text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \". SKIPPING THIS GRID.\"\n",
    "    #            print(text)\n",
    "    #            break\n",
    "    #        continue\n",
    "    #    start_vec_name = init\n",
    "    #    ac_converged = True\n",
    "    #    break\n",
    "    #if ac_converged:\n",
    "    #    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + start_vec_name + \".\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        pp.runpm_ac_opf(net) # Run DCOPP\n",
    "    except pp.OPFNotConverged:\n",
    "        text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "        print(text)\n",
    "        continue\n",
    "    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + \".\")\n",
    "    grids_ready.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_acopf_available_grid_names = [\"1-HV-mixed--0-no_sw\",\"1-HV-urban--0-no_sw\", \"1-MV-comm--0-no_sw\", \"1-MV-semiurb--0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_acopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf_and_acopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_available_grid_names = [\"1-HVMV-mixed-all-0-no_sw\", \"1-HVMV-mixed-1.105-0-no_sw\", \"1-HVMV-mixed-2.102-0-no_sw\",\"1-HVMV-mixed-4.101-0-no_sw\", \"1-HVMV-urban-all-0-no_sw\", \"1-HVMV-urban-2.203-0-no_sw\", \"1-HVMV-urban-3.201-0-no_sw\", \"1-HVMV-urban-4.201-0-no_sw\", \"1-HV-mixed--0-no_sw\", \"1-HV-urban--0-no_sw\", \"1-MVLV-rural-all-0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Unsupervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this revised version of the compute_node_embeddings function, the node features are weighted by the reverse admittance values in the adjacency matrix before they are summed to compute the node embeddings. The resulting node embeddings will reflect the strength of the connections between the nodes.\n",
    "\n",
    "To use the reverse admittance values as the edge weights, you would need to pass the Ybus matrix as the adjacency matrix when calling the compute_node_embeddings function. The Ybus matrix should be converted to a PyTorch tensor before passing it to the function.\n",
    "\n",
    "use the reverse admittance values as edge weights, you can modify the computation of the node embeddings to weight the node features by the reverse admittance values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the node types\n",
    "node_types = ['Slack Node', 'Generator Node', 'Load Node']\n",
    "\n",
    "# Define the number of nodes of each type in the graph\n",
    "num_nodes = {\n",
    "    'Slack Node': 1,\n",
    "    'Generator Node': 20,\n",
    "    'Load Node': 99\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "grid_names = [_ for _ in os.listdir(os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\\")]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating edge index and edge attributes for the grid 1-HV-mixed--0-no_sw ...\n",
      "Reading all of the .csv files from the directory of 1-HV-mixed--0-no_sw ...\n",
      "Processing Training Data for 1-HV-mixed--0-no_sw ...\n",
      "Processing Validation Data for 1-HV-mixed--0-no_sw ...\n",
      "Processing Test Data for 1-HV-mixed--0-no_sw ...\n",
      "Processing complete.\n",
      "Calculating edge index and edge attributes for the grid 1-HV-urban--0-no_sw ...\n",
      "Reading all of the .csv files from the directory of 1-HV-urban--0-no_sw ...\n",
      "Processing Training Data for 1-HV-urban--0-no_sw ...\n",
      "Processing Validation Data for 1-HV-urban--0-no_sw ...\n",
      "Processing Test Data for 1-HV-urban--0-no_sw ...\n",
      "Processing complete.\n",
      "Calculating edge index and edge attributes for the grid 1-MV-comm--0-no_sw ...\n",
      "Reading all of the .csv files from the directory of 1-MV-comm--0-no_sw ...\n",
      "Processing Training Data for 1-MV-comm--0-no_sw ...\n",
      "Processing Validation Data for 1-MV-comm--0-no_sw ...\n",
      "Processing Test Data for 1-MV-comm--0-no_sw ...\n",
      "Processing complete.\n",
      "Calculating edge index and edge attributes for the grid 1-MV-semiurb--0-no_sw ...\n",
      "Reading all of the .csv files from the directory of 1-MV-semiurb--0-no_sw ...\n",
      "Processing Training Data for 1-MV-semiurb--0-no_sw ...\n",
      "Processing Validation Data for 1-MV-semiurb--0-no_sw ...\n",
      "Processing Test Data for 1-MV-semiurb--0-no_sw ...\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "graphdata_lst = read_multiple_supervised_datasets(grid_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Name: 1-HV-mixed--0-no_sw, Len_Train: 85,Len_Val: 10,Len_Test: 5\n",
      "Grid Name: 1-HV-urban--0-no_sw, Len_Train: 85,Len_Val: 10,Len_Test: 5\n",
      "Grid Name: 1-MV-comm--0-no_sw, Len_Train: 85,Len_Val: 10,Len_Test: 5\n",
      "Grid Name: 1-MV-semiurb--0-no_sw, Len_Train: 85,Len_Val: 10,Len_Test: 5\n"
     ]
    }
   ],
   "source": [
    "for graph_data in graphdata_lst:\n",
    "    grid_name = graph_data.grid_name\n",
    "    ln_train_dataset = len(graph_data.train_data)\n",
    "    ln_val_dataset = len(graph_data.val_data)\n",
    "    ln_test_dataset = len(graph_data.test_data)\n",
    "\n",
    "    print (f\"Grid Name: {grid_name}, Len_Train: {ln_train_dataset},Len_Val: {ln_val_dataset},Len_Test: {ln_test_dataset}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(4, 256, num_layers, 4, dropout=0.0, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=\"last\",layer_type=\"TransConv\", activation=\"elu\")#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_datasets_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "    run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"OPF-GNN-Supervised-Homo\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Homo-GNN\",\n",
    "    \"num_layers\": num_layers,\n",
    "    \"dataset\": grid_names[i],\n",
    "    \"epochs\": 1000,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"output_channels\": out_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"channel type\": layer_type,\n",
    "    \"norm\": \"BatchNorm\",\n",
    "    \"scaler\": scaler\n",
    "\n",
    "    }\n",
    "    )\n",
    "    num_epochs = wandb.run.config.epochs\n",
    "    run.watch(model)\n",
    "    grid_name = graphdata.grid_name\n",
    "    run.config.dataset = grid_name\n",
    "    train_data = graphdata.train_data\n",
    "    run.config[\"number of busses\"] = np.shape(train_data[0].x)[0]\n",
    "    val_data = graphdata.val_data\n",
    "    test_data = graphdata.test_data\n",
    "    test_datasets_lst.append(test_data)\n",
    "    training_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    validation_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "    #test_loader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "    for _ in range(num_epochs):\n",
    "        #train_one_epoch(i, optimizer, training_loader, model, nn.MSELoss(), edge_index, edge_weights)\n",
    "        train_validate_one_epoch(_, grid_name, optimizer, training_loader, validation_loader, model, nn.MSELoss(), scaler)\n",
    "    print(\"Training and Validation finished \" + \"for GraphData \" + str(i) + \".\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader_lst = []\n",
    "val_loader_lst = []\n",
    "test_loader_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "\n",
    "    # Divide training data into chunks, load into Dataloaders and append to the list of training loaders\n",
    "    for train_data in divide_chunks(graphdata.train_data, 5):\n",
    "        train_loader_lst.append(DataLoader(train_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Divide validation data into chunks, load into Dataloaders and append to the list of validation loaders\n",
    "    for val_data in divide_chunks(graphdata.val_data, 5):\n",
    "        val_loader_lst.append(DataLoader(val_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Append the test data to the list\n",
    "    for test_data in divide_chunks(graphdata.test_data, 5):\n",
    "        test_loader_lst.append(DataLoader(test_data, batch_size=1, shuffle=True))\n",
    "\n",
    "print(\"Data Preparation finished.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = test_all_one_epoch(test_loader_lst, model, nn.MSELoss())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output,target = outputs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandapower.plotting.simple_plot import simple_plot\n",
    "from pandapower.plotting.plotly.simple_plotly import simple_plotly\n",
    "#simple_plot(net, plot_gens=True, plot_loads=True, plot_sgens=True, library=\"igraph\")\n",
    "simple_plotly(net, map_style=\"satellite\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Module.parameters of GNN(4, 4, num_layers=5)>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "net = process_network(grid_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " HETEROGENEOUS GNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "index_mappers,net,data = generate_unsupervised_input('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    }
   ],
   "source": [
    "x_dict, constraint_dict, edge_idx_dict, edge_attr_dict, bus_idx_neighbors_dict, scaler, angle_params, res_bus_dict = extract_unsupervised_inputs(data, net, index_mappers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "for from_bus, val in bus_idx_neighbors_dict.items():\n",
    "    # Sorting the dictionary by its keys\n",
    "    sorted_data = {k: val[k] for k in sorted(val.keys())}\n",
    "    bus_idx_neighbors_dict[from_bus] = sorted_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: [('PV', 5, tensor([7.3469e-06, 1.7959e+00]))],\n 1: [('PV', 2, tensor([4.2778e-06, 1.3333e+00]))],\n 2: [('PV', 33, tensor([0.0719, 0.1944])),\n  ('PV', 20, tensor([0.2999, 0.8108])),\n  ('PV', 3, tensor([0.5696, 1.5398])),\n  ('PV', 41, tensor([0.8170, 2.2086])),\n  ('PV', 11, tensor([1.3666, 3.6942])),\n  ('PV', 1, tensor([4.2778e-06, 1.3333e+00]))],\n 3: [('PV', 22, tensor([0.5483, 1.4821])),\n  ('PV', 2, tensor([0.5696, 1.5398]))],\n 4: [('PV', 26, tensor([0.3375, 0.9123]))],\n 5: [('PQ', 10, tensor([ 3.7512, 10.1401])),\n  ('PV', 27, tensor([0.6237, 1.6859])),\n  ('PV', 16, tensor([2.4927, 6.7384])),\n  ('PV', 40, tensor([1.9532, 5.2799])),\n  ('PV', 0, tensor([7.3469e-06, 1.7959e+00]))],\n 6: [('PV', 21, tensor([0.4586, 1.2397]))],\n 7: [('PQ', 4, tensor([2.1370, 5.7768])),\n  ('PV', 15, tensor([0.8511, 2.3007]))],\n 8: [('PQ', 6, tensor([0.6969, 1.8839]))],\n 9: [('PV', 28, tensor([1.4714, 3.9775])),\n  ('PV', 30, tensor([0.4157, 1.1238]))],\n 10: [('PV', 21, tensor([2.6916, 7.2760])),\n  ('PV', 11, tensor([2.1478, 5.8060]))],\n 11: [('PV', 10, tensor([2.1478, 5.8060])),\n  ('PV', 2, tensor([1.3666, 3.6942]))],\n 12: [('PV', 27, tensor([1.0758, 2.9081]))],\n 13: [('PV', 14, tensor([1.2198, 3.2974]))],\n 14: [('NB', 0, tensor([1.0761, 2.9089])),\n  ('PV', 13, tensor([1.2198, 3.2974]))],\n 15: [('PQ', 6, tensor([1.7621, 4.7632])),\n  ('NB', 4, tensor([0.5852, 1.5820])),\n  ('PV', 7, tensor([0.8511, 2.3007]))],\n 16: [('PV', 5, tensor([2.4927, 6.7384]))],\n 17: [('PQ', 11, tensor([0.1192, 0.3221])),\n  ('PQ', 1, tensor([2.2874, 6.1833])),\n  ('PV', 39, tensor([1.1915, 3.2209]))],\n 18: [('PV', 31, tensor([0.2041, 0.5518])),\n  ('PV', 39, tensor([0.9938, 2.6863]))],\n 19: [('PV', 41, tensor([1.1007, 2.9754]))],\n 20: [('PV', 2, tensor([0.2999, 0.8108])),\n  ('PV', 25, tensor([1.4386, 3.8887]))],\n 21: [('PQ', 1, tensor([1.5287, 4.1324])),\n  ('PV', 10, tensor([2.6916, 7.2760])),\n  ('PV', 6, tensor([0.4586, 1.2397]))],\n 22: [('PQ', 0, tensor([0.5434, 1.4688])),\n  ('PV', 26, tensor([0.5569, 1.5055])),\n  ('PV', 3, tensor([0.5483, 1.4821]))],\n 23: [('PQ', 1, tensor([0.8772, 2.3713])),\n  ('PQ', 3, tensor([0.1755, 0.1763]))],\n 24: [('PQ', 14, tensor([0.1643, 0.4440])),\n  ('PQ', 2, tensor([0.3369, 0.9108])),\n  ('PV', 35, tensor([0.1239, 0.3348]))],\n 25: [('PV', 20, tensor([1.4386, 3.8887]))],\n 26: [('PV', 22, tensor([0.5569, 1.5055])),\n  ('PV', 4, tensor([0.3375, 0.9123])),\n  ('PV', 36, tensor([0.1787, 0.4829]))],\n 27: [('PQ', 12, tensor([1.1997, 3.2430])),\n  ('PV', 12, tensor([1.0758, 2.9081])),\n  ('PV', 5, tensor([0.6237, 1.6859]))],\n 28: [('PV', 9, tensor([1.4714, 3.9775]))],\n 29: [('PQ', 15, tensor([0.2171, 0.5869])),\n  ('PV', 30, tensor([0.6109, 1.6514])),\n  ('PV', 34, tensor([3.2143, 8.6889]))],\n 30: [('PV', 29, tensor([0.6109, 1.6514])),\n  ('PV', 9, tensor([0.4157, 1.1238])),\n  ('PV', 37, tensor([0.6679, 1.8055]))],\n 31: [('PV', 18, tensor([0.2041, 0.5518])),\n  ('PV', 32, tensor([1.5459, 4.1788]))],\n 32: [('PQ', 7, tensor([1.6194, 4.3776])),\n  ('PV', 38, tensor([0.0741, 0.2003])),\n  ('PV', 31, tensor([1.5459, 4.1788]))],\n 33: [('PV', 2, tensor([0.0719, 0.1944]))],\n 34: [('PV', 36, tensor([0.3279, 0.8865])),\n  ('PV', 29, tensor([3.2143, 8.6889]))],\n 35: [('PV', 24, tensor([0.1239, 0.3348]))],\n 36: [('PV', 34, tensor([0.3279, 0.8865])),\n  ('PV', 26, tensor([0.1787, 0.4829]))],\n 37: [('PV', 30, tensor([0.6679, 1.8055]))],\n 38: [('PV', 32, tensor([0.0741, 0.2003]))],\n 39: [('PV', 17, tensor([1.1915, 3.2209])),\n  ('PV', 18, tensor([0.9938, 2.6863]))],\n 40: [('PV', 5, tensor([1.9532, 5.2799]))],\n 41: [('PV', 19, tensor([1.1007, 2.9754])),\n  ('PV', 2, tensor([0.8170, 2.2086]))]}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_idx_neighbors_dict[\"PV\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SB', 'isConnected', 'PQ') : 1\n",
      "('PV', 'isConnected', 'PQ') : 15\n",
      "('PV', 'isConnected', 'NB') : 2\n",
      "('PQ', 'isConnected', 'NB') : 3\n",
      "('PQ', 'isConnected', 'SB') : 1\n",
      "('PQ', 'isConnected', 'PV') : 15\n",
      "('NB', 'isConnected', 'PV') : 2\n",
      "('NB', 'isConnected', 'PQ') : 3\n",
      "('PV', 'isConnected', 'PV') : 36\n",
      "('PQ', 'isConnected', 'PQ') : 7\n",
      "('NB', 'isConnected', 'NB') : 2\n"
     ]
    }
   ],
   "source": [
    "for edge_type in edge_idx_dict:\n",
    "    print(f\"{edge_type} : {len(edge_idx_dict[edge_type][0])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SB', 'isConnected', 'PQ'):tensor([[0],\n",
      "        [0]])\n",
      "('PV', 'isConnected', 'PQ'):tensor([[ 0,  1,  2,  3,  4,  5,  6,  6,  8,  9, 10, 11,  5,  1, 14],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  5,  9, 10, 11, 12,  5,  0]])\n",
      "('PV', 'isConnected', 'NB'):tensor([[0, 1],\n",
      "        [0, 1]])\n",
      "('PQ', 'isConnected', 'NB'):tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n",
      "('PQ', 'isConnected', 'SB'):tensor([[0],\n",
      "        [0]])\n",
      "('PQ', 'isConnected', 'PV'):tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  5,  9, 10, 11, 12,  5,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  6,  8,  9, 10, 11,  5,  1, 14]])\n",
      "('NB', 'isConnected', 'PV'):tensor([[0, 1],\n",
      "        [0, 1]])\n",
      "('NB', 'isConnected', 'PQ'):tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n",
      "('PV', 'isConnected', 'PV'):tensor([[ 0,  1,  2,  3,  2,  5,  6,  5,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 17,  0, 23, 23,  5, 26,  6, 28,  5, 18, 31, 32,  5, 34, 23],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  0,  9, 10, 11, 12, 13,  9, 10, 16, 17,\n",
      "         18, 19, 20, 18, 22, 16, 24, 25, 26, 27, 28, 22, 30, 31, 27, 33, 28, 35]])\n",
      "('PQ', 'isConnected', 'PQ'):tensor([[0, 1, 2, 3, 1, 1, 6],\n",
      "        [0, 1, 2, 2, 0, 5, 6]])\n",
      "('NB', 'isConnected', 'NB'):tensor([[0, 1],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for edge_type, edge_index in edge_idx_dict.items():\n",
    "    src, rel, dst = edge_type\n",
    "    edge_attr = edge_attr_dict[edge_type]\n",
    "\n",
    "    mapper_from = dict()\n",
    "    mapper_to = dict()\n",
    "\n",
    "    for j in range(len(edge_index[0])):\n",
    "        from_idx = edge_index[0][j].item()\n",
    "\n",
    "        if from_idx not in mapper_from:\n",
    "            mapper_from[from_idx] = j\n",
    "\n",
    "\n",
    "    for k in range(len(edge_index[1])):\n",
    "        to_idx = edge_index[1][k].item()\n",
    "\n",
    "        if to_idx not in mapper_to:\n",
    "            mapper_to[to_idx] = k\n",
    "\n",
    "    from_idx = [mapper_from[key.item()] for key in edge_index[0]]\n",
    "    to_idx = [mapper_to[key.item()] for key in edge_index[1]]\n",
    "\n",
    "    print(f\"{edge_type}:{torch.tensor([from_idx, to_idx])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  2,  2, 31],\n        [ 1,  2, 25, 31]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1,2,2,31],[1,2,25,31],[11,2,32,531],[1,22,2,311]]\n",
    "#y = list(reversed(x))\n",
    "t = torch.tensor(x)\n",
    "t[torch.tensor([0,1])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "{4: [('PV', 15, tensor([0.5852, 1.5820]))],\n 0: [('PV', 14, tensor([1.0761, 2.9089])),\n  ('PQ', 12, tensor([2.1526, 5.8188])),\n  ('NB', 3, tensor([3.6198, 9.7850]))],\n 2: [('PQ', 9, tensor([1.3074, 3.5341]))],\n 1: [('PQ', 7, tensor([0.0587, 0.1587])), ('NB', 3, tensor([1.8857, 5.0973]))],\n 3: [('NB', 1, tensor([1.8857, 5.0973])), ('NB', 0, tensor([3.6198, 9.7850]))]}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_idx_neighbors_dict[\"NB\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(torch.Tensor,\n            {'PQ': tensor([[-0.9727,  0.4975,  0.1855,  0.1735],\n                     [-0.9881, -0.3441,  0.1876,  0.1740],\n                     [-0.9433, -0.1555,  0.5085,  0.3566],\n                     [-0.8736, -0.6474,  0.8787,  0.5507],\n                     [-0.9996,  2.1459,  0.1823,  0.1737],\n                     [-0.7947, -0.3202,  0.1851,  0.1765],\n                     [-1.0000,  6.4361,  0.7988,  0.5809],\n                     [-0.8129, -0.4006,  0.1863,  0.1734],\n                     [-0.9954,  4.7195,  0.1867,  0.1750],\n                     [-1.0000,  3.5074,  0.1859,  0.1759],\n                     [-0.9077, -0.3457,  0.5017,  0.3234],\n                     [-0.9938,  0.8982,  0.4428,  0.3478],\n                     [ 0.1649,  0.0599,  0.1850,  0.1719],\n                     [-0.9517,  1.1979,  0.1877,  0.1745],\n                     [-0.9716,  0.3346,  0.1855,  0.1764]], grad_fn=<CatBackward0>),\n             'NB': tensor([[-0.5042, -0.9679,  0.1318,  0.1443],\n                     [-0.4228, -0.5739,  0.1318,  0.1443],\n                     [-0.0776, -0.9174,  0.1318,  0.1443],\n                     [-0.9629, -0.8821,  0.1318,  0.1443],\n                     [-0.7104, -0.5152,  0.1318,  0.1443]], grad_fn=<CatBackward0>),\n             'SB': tensor([[-0.7722,  0.6120,  0.1318,  0.1443]], grad_fn=<CatBackward0>),\n             'PV': tensor([[-9.7774e-01, -8.5157e-01, -2.8817e+00, -4.3336e+00],\n                     [-4.5090e-01, -7.3275e-01, -2.4512e+00, -3.6939e+00],\n                     [-8.8442e-01,  2.3983e+00,  5.7386e-02,  1.0016e-01],\n                     [-2.7672e-01,  9.2331e-01, -7.4906e-02,  2.2673e-02],\n                     [-1.6263e-01,  8.0254e-01,  2.2714e-02,  8.2273e-02],\n                     [-9.9844e-01,  1.2012e+01, -1.1516e+00, -6.0946e-01],\n                     [-4.7029e-02,  7.0152e-01, -2.9466e-01, -1.0793e-01],\n                     [-8.0817e-01,  5.4861e+00, -1.4915e+00, -8.1052e-01],\n                     [-6.1014e-01,  2.7211e+00, -2.0738e-01, -5.5428e-02],\n                     [-1.5199e-01,  7.9273e-01, -7.6784e-03,  6.2356e-02],\n                     [-2.5229e-02,  6.8396e-01, -3.4897e-01, -1.4393e-01],\n                     [-8.9097e-01,  2.1765e+00, -1.1272e-01,  1.8092e-03],\n                     [ 2.3027e-01,  4.8371e-01, -9.8136e-01, -5.0980e-01],\n                     [-4.4762e-02,  6.9927e-01, -3.0475e-01, -1.0172e-01],\n                     [-4.4939e-01, -9.5437e-02,  7.5322e-02,  1.1110e-01],\n                     [-9.5586e-01,  5.3520e+00, -5.7378e-01, -2.6986e-01],\n                     [-9.7663e-01,  3.5803e+00,  3.7141e-02,  8.9300e-02],\n                     [-8.7474e-01,  4.7665e+00, -4.2391e-02,  4.2921e-02],\n                     [-4.3252e-01,  2.7053e+00, -6.9198e-01, -3.4512e-01],\n                     [-6.1359e-01,  1.3603e+00, -2.5587e-02,  5.1840e-02],\n                     [-5.7900e-01,  1.2790e+00, -6.1336e-01, -2.8357e-01],\n                     [ 1.8788e-01,  5.6472e-01, -4.4179e-01, -1.9509e-01],\n                     [-9.8367e-01,  6.0517e+00,  6.1374e-02,  1.0291e-01],\n                     [-5.9596e-01,  2.7398e+00,  2.2123e-02,  7.7446e-02],\n                     [-4.3141e-01,  2.3136e+00, -2.9866e-01, -9.7611e-02],\n                     [-3.5326e-01,  1.8666e+00,  6.1353e-02,  1.0200e-01],\n                     [-8.4697e-01,  2.1156e+00,  5.5587e-02,  9.9328e-02],\n                     [-2.0836e-01,  8.7470e-01,  3.4563e-02,  8.8339e-02],\n                     [-9.2869e-01,  5.0721e+00, -2.1648e-01, -6.0706e-02],\n                     [-7.7225e-01,  1.7022e+00, -3.2151e-01, -1.2195e-01],\n                     [-2.9127e-01,  1.8119e+00,  5.1375e-02,  9.8303e-02],\n                     [-2.7762e-01,  9.3781e-01,  5.2180e-02,  9.6424e-02],\n                     [ 9.2224e-02,  6.7261e-01, -2.4821e-02,  5.3573e-02],\n                     [-8.4763e-01,  2.0518e+00, -3.4189e-01, -1.3095e-01],\n                     [ 3.0258e-01,  5.1016e-01,  3.0583e-02,  8.4228e-02],\n                     [-9.7691e-01,  3.2098e+00, -3.5932e-01, -1.4345e-01],\n                     [ 2.4285e-01,  5.4805e-01,  1.1315e-02,  7.3480e-02],\n                     [-1.0202e-01,  8.3838e-01,  3.2260e-02,  8.5170e-02],\n                     [-4.0031e-01,  1.0560e+00,  5.2399e-02,  9.6106e-02],\n                     [ 6.5637e-02,  7.8332e-01, -4.1604e-02,  4.3883e-02],\n                     [-1.5517e-01,  7.9563e-01,  1.1319e-03,  6.8834e-02],\n                     [-1.2226e-01,  7.6581e-01, -9.2208e-02,  1.1550e-02],\n                     [-1.5127e-01,  7.9204e-01, -1.0091e-02,  6.1869e-02]],\n                    grad_fn=<CatBackward0>)})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder_model(x_dict, constraint_dict, edge_idx_dict, edge_attr_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0.23723669614964715"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.167**2 + 0.1685**2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 342.       418.       350.      -350.       350.      -350.\n",
      "   350.00003]]\n",
      "[[ 99.        121.          3.1069336   2.8781586   2.8781586   1.1700897\n",
      "    1.1700897]\n",
      " [ 99.        121.          3.257019    3.0235138   3.0235138   1.2110443\n",
      "    1.2110443]\n",
      " [ 99.        121.         37.16533     8.023071   34.423065    3.5772247\n",
      "   14.011185 ]\n",
      " [ 99.        121.         51.020508   33.0876     46.6476     15.307114\n",
      "   20.66623  ]\n",
      " [ 99.        121.          3.0006409   2.704483    2.704483    1.2998047\n",
      "    1.2998047]\n",
      " [ 99.        121.          3.2549133   3.059372    3.059372    1.1112213\n",
      "    1.1112213]\n",
      " [ 99.        121.         49.814194   31.622696   45.182693   15.616501\n",
      "   20.975632 ]\n",
      " [ 99.        121.          3.423462    3.208374    3.208374    1.1942902\n",
      "    1.1942902]\n",
      " [ 99.        121.          3.0209045   2.8138428   2.8138428   1.0991058\n",
      "    1.0991058]\n",
      " [ 99.        121.          3.4769897   3.2351685   3.2351685   1.2740631\n",
      "    1.2740631]\n",
      " [ 99.        121.         31.884735    5.5887756  29.388763    2.9603271\n",
      "   12.366684 ]\n",
      " [ 99.        121.         32.33777     6.388916   30.18892     2.185028\n",
      "   11.5914   ]\n",
      " [ 99.        121.          3.4465942   3.223648    3.223648    1.2194519\n",
      "    1.2194519]\n",
      " [ 99.        121.          2.9874878   2.776535    2.776535    1.1026306\n",
      "    1.1026306]\n",
      " [ 99.        121.          2.9378662   2.7017365   2.7017365   1.1539612\n",
      "    1.1539612]]\n",
      "[[ 3.42000000e+02  4.18000000e+02  3.50000000e+02 -3.50000000e+02\n",
      "  -1.52587891e-05 -3.50000000e+02  1.52587891e-05]\n",
      " [ 1.98000000e+02  2.42000000e+02  3.00000000e+02 -3.00000000e+02\n",
      "  -1.52587891e-05 -3.00000000e+02  1.52587891e-05]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.10754395e+00 -5.67999268e+00\n",
      "  -2.70428467e+00 -1.25527954e+00 -2.24487305e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  2.23441010e+01 -2.07800140e+01\n",
      "  -2.90278625e+00 -1.30058289e+00 -8.21281433e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.02581177e+01 -9.54000854e+00\n",
      "  -3.05505371e+00 -1.07026672e+00 -3.77052307e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.60280014e+02 -1.49059998e+02\n",
      "  -1.52587891e-05  1.52587891e-05 -5.89134369e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.39892426e+01 -3.16100006e+01\n",
      "  -1.89137115e+01 -6.66284180e+00 -1.24929810e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.70806427e+02 -1.58850006e+02\n",
      "  -3.03179932e+01 -1.35118561e+01 -6.27813721e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.90108032e+01 -3.62799988e+01\n",
      "  -3.11334229e+00 -1.28262329e+00 -1.43388977e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.74194183e+01 -1.61999969e+01\n",
      "  -1.52587891e-05  1.52587891e-05 -6.40275574e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  4.15376129e+01 -3.86300049e+01\n",
      "  -1.68571930e+01 -7.40226746e+00 -1.52674561e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  2.70538025e+01 -2.51600037e+01\n",
      "  -3.01489258e+00 -1.22120667e+00 -9.94395447e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.35645020e+02 -1.26150009e+02\n",
      "  -3.19332886e+00 -1.18431091e+00 -4.98572235e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.43118134e+01 -3.19100037e+01\n",
      "  -1.65105896e+01 -6.15379333e+00 -1.26115570e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  7.05377197e+00 -6.56001282e+00\n",
      "  -1.52587891e-05  1.52587891e-05 -2.59266663e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.78279114e+01 -6.30800095e+01\n",
      "  -1.76498260e+01 -6.88507080e+00 -2.49306335e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  8.46238708e+00 -7.87001038e+00\n",
      "  -3.05123901e+00 -1.14131165e+00 -3.11041260e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.84516144e+01 -1.71600037e+01\n",
      "  -3.27821350e+00 -1.17807007e+00 -6.78202820e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.36668549e+01 -4.92400055e+01\n",
      "  -4.84744797e+01 -2.13436737e+01 -1.94607849e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.96558990e+01 -1.82799988e+01\n",
      "  -1.52587891e-05  1.52587891e-05 -7.22467041e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  7.37203979e+01 -6.85599976e+01\n",
      "  -1.61134033e+01 -7.38481140e+00 -2.70965576e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.32795105e+01 -4.95500031e+01\n",
      "  -1.73746796e+01 -6.95779419e+00 -1.95832214e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  8.79570007e+00 -8.18000793e+00\n",
      "  -1.52587891e-05  1.52587891e-05 -3.23294067e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.07419128e+01 -9.99002075e+00\n",
      "  -2.93855286e+00 -1.27900696e+00 -3.94818115e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.36128845e+01 -3.12600098e+01\n",
      "  -1.58906250e+01 -6.95913696e+00 -1.23546753e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.44085693e+00 -5.05999756e+00\n",
      "  -2.73703003e+00 -1.09313965e+00 -1.99983215e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.18280029e+00 -5.75000000e+00\n",
      "  -3.27565002e+00 -1.08125305e+00 -2.27253723e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  8.92474365e+00 -8.30001831e+00\n",
      "  -3.24577332e+00 -1.19587708e+00 -3.28036499e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  4.01828156e+01 -3.73700104e+01\n",
      "  -2.94935608e+00 -1.19841003e+00 -1.47695618e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.66129150e+01 -5.26500092e+01\n",
      "  -1.52587891e-05  1.52587891e-05 -2.08085938e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.80645752e+00 -6.33000183e+00\n",
      "  -2.77929688e+00 -1.22323608e+00 -2.50175476e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.93548584e+00 -6.44999695e+00\n",
      "  -2.98042297e+00 -1.17948914e+00 -2.54917908e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.62688141e+01 -1.51300049e+01\n",
      "  -3.23254395e+00 -1.09085083e+00 -5.97970581e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  4.12580109e+01 -3.83700104e+01\n",
      "  -1.73695374e+01 -6.78463745e+00 -1.51645966e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  9.63442993e+00 -8.96000671e+00\n",
      "  -3.09129333e+00 -1.14685059e+00 -3.54119873e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.80753021e+01 -5.40100021e+01\n",
      "  -3.20039368e+00 -1.29483032e+00 -2.13461914e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.20108032e+01 -1.11700134e+01\n",
      "  -3.07546997e+00 -1.19583130e+00 -4.41477966e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  9.04301453e+00 -8.41000366e+00\n",
      "  -2.99108887e+00 -1.26058960e+00 -3.32382202e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.84945679e+00 -6.36999512e+00\n",
      "  -3.14657593e+00 -1.17970276e+00 -2.51757812e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.82687988e+01 -1.69900055e+01\n",
      "  -3.12025452e+00 -1.22277832e+00 -6.71482849e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.29140167e+01 -1.20100098e+01\n",
      "  -2.92842102e+00 -1.26052856e+00 -4.74670410e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  2.47419128e+01 -2.30100098e+01\n",
      "  -2.88661194e+00 -1.24166870e+00 -9.09402466e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.44839020e+01 -1.34700165e+01\n",
      "  -3.08052063e+00 -1.30276489e+00 -5.32376099e+00]]\n",
      "[[ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]]\n"
     ]
    }
   ],
   "source": [
    "for node_type in constraint_dict:\n",
    "    print(custom_standard_inverse_transform(scaler, constraint_dict[node_type].detach().numpy()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ACOPFOutput(x_dict, scalers_dict, None, index_mappers).output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_names = sb.collect_all_simbench_codes()[25:]\n",
    "acopf_ok_grid_names = []\n",
    "\n",
    "for grid_name in grid_names:\n",
    "    print(f\"Trying grid {grid_name}\")\n",
    "    net = process_network(grid_name)\n",
    "\n",
    "    # try:\n",
    "    #     acopf_ok_grid_names.append(grid_name)\n",
    "    #     pp.runpm_ac_opf(net)\n",
    "    #     #print(net.res_bus)\n",
    "    # except:\n",
    "    #     print(f\"Julia didnt converge for {grid_name}\")\n",
    "    #     acopf_ok_grid_names.remove(grid_name)\n",
    "    try:\n",
    "        acopf_ok_grid_names.append(grid_name)\n",
    "        pp.runopp(net)\n",
    "        #print(net.res_bus)\n",
    "    except:\n",
    "        print(f\"OPP didnt converge for {grid_name}\")\n",
    "        acopf_ok_grid_names.remove(grid_name)\n",
    "acopf_ok_grid_names\n",
    "#save_multiple_unsupervised_inputs(grid_names, 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "data": {
      "text/plain": "        vm_pu   va_degree      p_mw     q_mvar     lam_p         lam_q\n0    1.025000    0.000000 -8.057173 -11.835759  1.000000  2.259515e-21\n2    1.001425  209.119600 -1.176327   0.135443  1.000714  1.887024e-03\n4    0.999942  209.185313  0.397800   0.183689  1.002495  4.950700e-03\n5    0.999154  209.227646  0.293150   0.148678  1.003357  6.811706e-03\n6    0.998564  209.265084  0.239906   0.171257  1.003935  8.382988e-03\n..        ...         ...       ...        ...       ...           ...\n112  0.987459  209.245271  0.237492   0.096853  1.020719  1.774877e-02\n113  0.987333  209.251486  0.036984   0.085200  1.020904  1.802879e-02\n114  0.987261  209.254787 -0.074431   0.034476  1.021013  1.818089e-02\n115  0.987149  209.258063  0.118649   0.137547  1.021198  1.835478e-02\n116  0.987060  209.260287  0.159486   0.137561  1.021350  1.848019e-02\n\n[115 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vm_pu</th>\n      <th>va_degree</th>\n      <th>p_mw</th>\n      <th>q_mvar</th>\n      <th>lam_p</th>\n      <th>lam_q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.025000</td>\n      <td>0.000000</td>\n      <td>-8.057173</td>\n      <td>-11.835759</td>\n      <td>1.000000</td>\n      <td>2.259515e-21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.001425</td>\n      <td>209.119600</td>\n      <td>-1.176327</td>\n      <td>0.135443</td>\n      <td>1.000714</td>\n      <td>1.887024e-03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.999942</td>\n      <td>209.185313</td>\n      <td>0.397800</td>\n      <td>0.183689</td>\n      <td>1.002495</td>\n      <td>4.950700e-03</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.999154</td>\n      <td>209.227646</td>\n      <td>0.293150</td>\n      <td>0.148678</td>\n      <td>1.003357</td>\n      <td>6.811706e-03</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.998564</td>\n      <td>209.265084</td>\n      <td>0.239906</td>\n      <td>0.171257</td>\n      <td>1.003935</td>\n      <td>8.382988e-03</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>0.987459</td>\n      <td>209.245271</td>\n      <td>0.237492</td>\n      <td>0.096853</td>\n      <td>1.020719</td>\n      <td>1.774877e-02</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>0.987333</td>\n      <td>209.251486</td>\n      <td>0.036984</td>\n      <td>0.085200</td>\n      <td>1.020904</td>\n      <td>1.802879e-02</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>0.987261</td>\n      <td>209.254787</td>\n      <td>-0.074431</td>\n      <td>0.034476</td>\n      <td>1.021013</td>\n      <td>1.818089e-02</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>0.987149</td>\n      <td>209.258063</td>\n      <td>0.118649</td>\n      <td>0.137547</td>\n      <td>1.021198</td>\n      <td>1.835478e-02</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>0.987060</td>\n      <td>209.260287</td>\n      <td>0.159486</td>\n      <td>0.137561</td>\n      <td>1.021350</td>\n      <td>1.848019e-02</td>\n    </tr>\n  </tbody>\n</table>\n<p>115 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acopf_ok_grid_names = ['1-HV-mixed--1-sw','1-MV-semiurb--0-no_sw','1-MV-comm--0-sw','1-MV-comm--0-no_sw']\n",
    "net = process_network(acopf_ok_grid_names[1])\n",
    "pp.runopp(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acopf_grid_names = ['1-HV-mixed--0-no_sw','1-MV-semiurb--0-no_sw', '1-MV-comm--0-no_sw']\n",
    "save_multiple_unsupervised_inputs(acopf_grid_names, num_samples=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "inputs = load_multiple_unsupervised_inputs()\n",
    "for i,input in enumerate(inputs):\n",
    "    if input.res_bus is None:\n",
    "        print(f\"None at {i}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_unsupervised_inputs('1-HV-mixed--0-no_sw', 300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = load_unsupervised_inputs('1-HV-mixed--0-no_sw')\n",
    "inputs[0].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs[57].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display Plot of Customized Sigmoid Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as p\n",
    "import torch\n",
    "\n",
    "def custom_tanh(x: torch.Tensor, lower_bound: float, upper_bound: float) -> torch.Tensor:\n",
    "    width = upper_bound - lower_bound\n",
    "    return 0.5 * width * torch.tanh(x) + 0.5 * (upper_bound + lower_bound)\n",
    "\n",
    "\n",
    "lst = []\n",
    "min_val = 0\n",
    "max_val = 0.1\n",
    "range_ = []\n",
    "i = -2\n",
    "while i < 2:\n",
    "    range_.append(i)\n",
    "    i += 0.01\n",
    "\n",
    "for i in range_:\n",
    "    val = custom_tanh(torch.tensor(i), min_val, max_val)\n",
    "    #val = torch.tanh(torch.tensor(i))\n",
    "    lst.append(val)\n",
    "\n",
    "# Now, plot the values in lst\n",
    "p.plot(range_, lst)\n",
    "p.xlabel('Input')\n",
    "p.ylabel('Value')\n",
    "p.title('Plot of Enforcing Activation Function')\n",
    "p.grid(True)\n",
    "p.show()\n",
    "print(range_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Heterogeneous Self Supervised Model and Process Inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "embedder_model = load_ACOPFGNN_model(grid_name, \"embedder_model.pt\", hidden_channels=4, num_layers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create ACOPFGNN model and Optimizer\n",
    "index_mappers,net,data = generate_unsupervised_input(grid_name)\n",
    "model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "learning_rate = 2.5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "2321"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mcanbay96\u001B[0m (\u001B[33mcbml\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\wandb\\run-20231019_023541-i8m08wvo</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/i8m08wvo' target=\"_blank\">generous-salad-227</a></strong> to <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/i8m08wvo' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/i8m08wvo</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_NOTEBOOK_NAME = \"msi\"\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"ACOPF-GNN-SelfSupervised-Hetero\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            #\"learning_rate\": optimizer.,\n",
    "            \"architecture\": \"Hetero-SelfSupervised-GNN\",\n",
    "            \"num_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "            \"num_heads\": model.heads,\n",
    "            \"num_layers\": model.num_layers,\n",
    "            \"grid_name\": grid_name,\n",
    "            \"activation\": model.act_fn,\n",
    "            \"dropout\": model.dropout,\n",
    "            \"in_channels\": model.in_channels,\n",
    "            \"output_channels\": model.out_channels,\n",
    "            \"hidden_channels\": model.hidden_channels,\n",
    "            \"channel type\": \"TransformerConv\",\n",
    "            \"scaler\": \"MinMax\",\n",
    "            \"model\": \"base_model_wired\"\n",
    "        }\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load Inputs\n",
    "inputs = load_unsupervised_inputs(grid_name)\n",
    "#inputs = load_multiple_unsupervised_inputs()\n",
    "n = len(inputs)\n",
    "train_inputs = inputs[:int(n*0.8)]\n",
    "val_inputs = inputs[int(n*0.8): int(n*0.95)]\n",
    "test_inputs = inputs[int(n*0.95):]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'SB': tensor([[ 6.0855, -1.1532,  6.4307, -4.8642]]),\n 'PQ': tensor([[-0.2825,  1.3591,  0.1810,  0.1701],\n         [-0.1253,  0.1567,  0.1793,  0.1708],\n         [-0.1267,  0.1450,  0.3101,  0.2293],\n         [-0.1348,  0.1020,  0.7808,  0.5394],\n         [ 0.0020,  0.9540,  0.1793,  0.1688],\n         [-0.2854,  1.0763,  0.1809,  0.1702],\n         [-0.0670,  0.5362,  0.8061,  0.5207],\n         [-0.1101,  1.3001,  0.1545,  0.1648],\n         [-0.1840,  0.5053,  0.1789,  0.1704],\n         [-0.1266,  0.1638,  0.1796,  0.1689],\n         [-0.2876,  1.0932,  0.1815,  0.1698],\n         [-0.1107,  0.5591,  0.3072,  0.2274],\n         [-0.3032,  1.3241,  0.3103,  0.2452],\n         [-0.2848,  1.0805,  0.1771,  0.1708],\n         [-0.1255,  0.1539,  0.1796,  0.1717],\n         [-0.1149,  1.3686,  0.1814,  0.1689]]),\n 'PV': tensor([[ 4.5760e+00, -1.8228e-01,  5.3563e+00,  4.0652e+00],\n         [ 1.5969e+00,  6.8250e-01,  3.4548e+00,  6.6139e+00],\n         [-3.2272e-01,  1.2525e+00,  7.9409e-02,  1.1184e-01],\n         [-3.0089e-01,  1.3164e+00, -1.8874e-01, -4.3694e-02],\n         [-2.6203e-01,  1.4144e+00,  8.1061e-03,  7.0635e-02],\n         [-3.1635e-01,  1.2346e+00, -2.5190e+00, -1.4007e+00],\n         [-1.5385e-01,  4.2675e-01, -1.0354e-01,  4.7541e-03],\n         [ 3.9337e-03,  9.6895e-01, -2.1284e+00, -1.1557e+00],\n         [-5.8643e-02,  5.8700e-01, -4.6419e-01, -2.0291e-01],\n         [-8.9029e-02,  1.5592e+00, -1.6361e-01, -2.9483e-02],\n         [-2.2024e-01,  8.1573e-01, -2.3017e-01, -7.1563e-02],\n         [-2.7928e-01,  1.1016e+00, -2.6452e-01, -9.0405e-02],\n         [-2.7206e-01,  1.5350e+00, -2.0580e+00, -1.1318e+00],\n         [-2.3360e-01,  1.0583e+00, -1.3468e-01, -7.6614e-03],\n         [-2.4053e-01,  1.0129e+00,  7.2963e-03,  7.0013e-02],\n         [-1.4721e-02,  8.5832e-01, -6.8502e-01, -3.1884e-01],\n         [-3.1111e-01,  1.2670e+00,  3.7510e-02,  9.0090e-02],\n         [-1.1028e-01,  5.6204e-01, -1.2300e-01, -7.2779e-03],\n         [-9.8230e-02,  9.2833e-01, -2.0048e-01, -5.0946e-02],\n         [-2.8116e-01,  1.5333e+00, -7.6083e-01, -3.8149e-01],\n         [-3.1849e-01,  1.2810e+00, -4.4203e-01, -1.8570e-01],\n         [-1.5609e-01,  4.1282e-01, -2.1424e-02,  5.3293e-02],\n         [-2.8183e-01,  1.3638e+00,  1.7864e-03,  6.7814e-02],\n         [-1.3315e-01,  1.0595e-01, -9.7209e-02,  1.0052e-02],\n         [-1.2550e-01,  1.5349e-01,  8.8421e-02,  1.1652e-01],\n         [-3.1680e-01,  1.2909e+00,  7.7380e-02,  1.1036e-01],\n         [-2.6291e-01,  1.4087e+00,  3.0625e-02,  8.3932e-02],\n         [-2.9826e-01,  1.3586e+00, -4.8129e-01, -2.1678e-01],\n         [-6.0923e-02,  1.7311e+00, -8.0982e-01, -4.0567e-01],\n         [-1.1620e-01,  1.3897e+00,  6.8115e-02,  1.0496e-01],\n         [-9.9423e-02,  1.4947e+00,  6.2627e-02,  1.0222e-01],\n         [-9.7926e-02,  9.5710e-01, -8.9095e-02,  1.4997e-02],\n         [-9.8945e-02,  1.1545e+00, -2.3634e-01, -6.3489e-02],\n         [-3.2255e-01,  1.2537e+00,  2.1368e-02,  7.7986e-02],\n         [-2.4725e-01,  1.4391e+00, -7.7966e-01, -3.8715e-01],\n         [-1.2513e-01,  1.5574e-01, -1.5959e-02,  5.5402e-02],\n         [-2.5727e-01,  1.4202e+00,  3.2090e-02,  8.3587e-02],\n         [-9.8596e-02,  1.4997e+00,  6.5025e-02,  1.0559e-01],\n         [-9.8568e-02,  1.1568e+00, -1.1960e-01, -5.1343e-03],\n         [-1.0295e-01,  7.6752e-01, -3.4208e-02,  4.7358e-02],\n         [-3.0041e-01,  1.3403e+00, -2.3056e-01, -6.8620e-02],\n         [-3.0313e-01,  1.3864e+00, -5.8311e-02,  3.1730e-02]]),\n 'NB': tensor([[-0.2496,  0.9548,  0.1236,  0.1377],\n         [-0.1846,  0.5098,  0.1236,  0.1377],\n         [-0.2875,  1.0929,  0.1236,  0.1377],\n         [-0.2066,  0.6582,  0.1236,  0.1377],\n         [-0.0147,  0.8583,  0.1236,  0.1377]])}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------   CURRENT LEARNING RATE: 0.001   ---------------\n",
      "Epoch: 0\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.0 \n",
      "     Training Step: 1 Training Loss: 0.0 \n",
      "     Training Step: 2 Training Loss: 0.0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17588\\836682791.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mloss_weights\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mmodel_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"embedder_model_specialized.pt\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mtrain_input\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mval_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_validate_ACOPF\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mtrain_validate_ACOPF\u001B[1;34m(model, model_name, optimizer, train_inputs, val_inputs, loss_weights, start_epoch, num_epochs, return_outputs, save_model)\u001B[0m\n\u001B[0;32m   2287\u001B[0m             \u001B[1;31m#physics_loss, mre_loss, loss, penalty_loss, unsupervised_loss = self_supervised_hetero_obj_fn(out_dict,bus_idx_neighbors_dict,constraint_dict,scaler, alpha,beta, gamma)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2288\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2289\u001B[1;33m             \u001B[0mphysics_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmre_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpenalty_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munsupervised_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2290\u001B[0m             \u001B[0mphysics_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2291\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mself_supervised_embedder_obj_fn\u001B[1;34m(out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   2182\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2183\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2184\u001B[1;33m     \u001B[0mphysics_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_physics_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2185\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2186\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mphysics_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mcalc_physics_loss\u001B[1;34m(out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   3058\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3059\u001B[0m                 \u001B[1;31m# ACOPF Equation for P_i\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3060\u001B[1;33m                 \u001B[0mP_ij\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1.0\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_i\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_j\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mG_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mB_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3061\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3062\u001B[0m                 \u001B[0mP\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mformat_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mformat_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mextract_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    198\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m     \u001B[0mstack\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mStackSummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwalk_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m     \u001B[0mstack\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreverse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mstack\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    357\u001B[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001B[0;32m    358\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mfilename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfnames\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 359\u001B[1;33m             \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckcache\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    360\u001B[0m         \u001B[1;31m# If immediate lookup was desired, trigger lookups now.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlookup_lines\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\IPython\\core\\compilerop.py\u001B[0m in \u001B[0;36mcheck_linecache_ipython\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \"\"\"\n\u001B[0;32m    184\u001B[0m     \u001B[1;31m# First call the original checkcache as intended\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 185\u001B[1;33m     \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_checkcache_ori\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    186\u001B[0m     \u001B[1;31m# Then, update back the cache with our data, so that tracebacks related\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m     \u001B[1;31m# to our compiled codes can be produced.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\linecache.py\u001B[0m in \u001B[0;36mcheckcache\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mcontinue\u001B[0m   \u001B[1;31m# no-op for files loaded via a __loader__\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m             \u001B[0mstat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfullname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mcache\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 1\n",
    "num_epochs = 100\n",
    "loss_weights = (0.0, 1.0, 0.0)\n",
    "model_name = \"embedder_model_specialized.pt\"\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF(model, model_name, optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------   CURRENT LEARNING RATE: 4e-05   ---------------\n",
      "Epoch: 60\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 6.849725723266602 \n",
      "     Training Step: 1 Training Loss: 7.994863033294678 \n",
      "     Training Step: 2 Training Loss: 7.418996334075928 \n",
      "     Training Step: 3 Training Loss: 4.887266635894775 \n",
      "     Training Step: 4 Training Loss: 5.012649059295654 \n",
      "     Training Step: 5 Training Loss: 3.0778064727783203 \n",
      "     Training Step: 6 Training Loss: 11.675814628601074 \n",
      "     Training Step: 7 Training Loss: 10.785894393920898 \n",
      "     Training Step: 8 Training Loss: 8.542866706848145 \n",
      "     Training Step: 9 Training Loss: 9.035882949829102 \n",
      "     Training Step: 10 Training Loss: 9.545356750488281 \n",
      "     Training Step: 11 Training Loss: 8.43463134765625 \n",
      "     Training Step: 12 Training Loss: 12.55498218536377 \n",
      "     Training Step: 13 Training Loss: 5.320827484130859 \n",
      "     Training Step: 14 Training Loss: 6.62136173248291 \n",
      "     Training Step: 15 Training Loss: 5.096195220947266 \n",
      "     Training Step: 16 Training Loss: 9.50904655456543 \n",
      "     Training Step: 17 Training Loss: 7.444781303405762 \n",
      "     Training Step: 18 Training Loss: 6.758533477783203 \n",
      "     Training Step: 19 Training Loss: 6.758355617523193 \n",
      "     Training Step: 20 Training Loss: 7.661477565765381 \n",
      "     Training Step: 21 Training Loss: 6.79921293258667 \n",
      "     Training Step: 22 Training Loss: 10.037893295288086 \n",
      "     Training Step: 23 Training Loss: 9.075201034545898 \n",
      "     Training Step: 24 Training Loss: 4.894169807434082 \n",
      "     Training Step: 25 Training Loss: 7.776320934295654 \n",
      "     Training Step: 26 Training Loss: 3.6629114151000977 \n",
      "     Training Step: 27 Training Loss: 7.650761127471924 \n",
      "     Training Step: 28 Training Loss: 7.431985378265381 \n",
      "     Training Step: 29 Training Loss: 7.586144924163818 \n",
      "     Training Step: 30 Training Loss: 12.974316596984863 \n",
      "     Training Step: 31 Training Loss: 12.894868850708008 \n",
      "     Training Step: 32 Training Loss: 6.032436370849609 \n",
      "     Training Step: 33 Training Loss: 4.2507405281066895 \n",
      "     Training Step: 34 Training Loss: 5.452780723571777 \n",
      "     Training Step: 35 Training Loss: 13.459784507751465 \n",
      "     Training Step: 36 Training Loss: 5.302128314971924 \n",
      "     Training Step: 37 Training Loss: 13.050894737243652 \n",
      "     Training Step: 38 Training Loss: 13.304935455322266 \n",
      "     Training Step: 39 Training Loss: 3.919915199279785 \n",
      "     Training Step: 40 Training Loss: 5.36727237701416 \n",
      "     Training Step: 41 Training Loss: 5.129926681518555 \n",
      "     Training Step: 42 Training Loss: 5.51865291595459 \n",
      "     Training Step: 43 Training Loss: 7.853332042694092 \n",
      "     Training Step: 44 Training Loss: 5.049275875091553 \n",
      "     Training Step: 45 Training Loss: 5.308444499969482 \n",
      "     Training Step: 46 Training Loss: 6.933781623840332 \n",
      "     Training Step: 47 Training Loss: 10.68270206451416 \n",
      "     Training Step: 48 Training Loss: 3.287910223007202 \n",
      "     Training Step: 49 Training Loss: 6.718997955322266 \n",
      "     Training Step: 50 Training Loss: 7.86416482925415 \n",
      "     Training Step: 51 Training Loss: 4.65550422668457 \n",
      "     Training Step: 52 Training Loss: 7.46273136138916 \n",
      "     Training Step: 53 Training Loss: 4.859919548034668 \n",
      "     Training Step: 54 Training Loss: 4.085713863372803 \n",
      "     Training Step: 55 Training Loss: 7.330260753631592 \n",
      "     Training Step: 56 Training Loss: 5.473433494567871 \n",
      "     Training Step: 57 Training Loss: 4.964493274688721 \n",
      "     Training Step: 58 Training Loss: 11.09634780883789 \n",
      "     Training Step: 59 Training Loss: 5.558187484741211 \n",
      "     Training Step: 60 Training Loss: 10.589357376098633 \n",
      "     Training Step: 61 Training Loss: 5.367259502410889 \n",
      "     Training Step: 62 Training Loss: 14.799457550048828 \n",
      "     Training Step: 63 Training Loss: 13.711862564086914 \n",
      "     Training Step: 64 Training Loss: 3.6749696731567383 \n",
      "     Training Step: 65 Training Loss: 9.222043991088867 \n",
      "     Training Step: 66 Training Loss: 6.734905242919922 \n",
      "     Training Step: 67 Training Loss: 8.710442543029785 \n",
      "     Training Step: 68 Training Loss: 7.278345584869385 \n",
      "     Training Step: 69 Training Loss: 8.809966087341309 \n",
      "     Training Step: 70 Training Loss: 8.195501327514648 \n",
      "     Training Step: 71 Training Loss: 8.988158226013184 \n",
      "     Training Step: 72 Training Loss: 9.117876052856445 \n",
      "     Training Step: 73 Training Loss: 5.26828670501709 \n",
      "     Training Step: 74 Training Loss: 5.45643424987793 \n",
      "     Training Step: 75 Training Loss: 3.749128818511963 \n",
      "     Training Step: 76 Training Loss: 10.256917953491211 \n",
      "     Training Step: 77 Training Loss: 12.184932708740234 \n",
      "     Training Step: 78 Training Loss: 12.044828414916992 \n",
      "     Training Step: 79 Training Loss: 8.22641372680664 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 6.2922682762146 \n",
      "     Validation Step: 1 Validation Loss: 7.970943450927734 \n",
      "     Validation Step: 2 Validation Loss: 9.62479305267334 \n",
      "     Validation Step: 3 Validation Loss: 8.7261381149292 \n",
      "     Validation Step: 4 Validation Loss: 12.534525871276855 \n",
      "     Validation Step: 5 Validation Loss: 11.159050941467285 \n",
      "     Validation Step: 6 Validation Loss: 3.3455405235290527 \n",
      "     Validation Step: 7 Validation Loss: 4.4155097007751465 \n",
      "     Validation Step: 8 Validation Loss: 5.8844828605651855 \n",
      "     Validation Step: 9 Validation Loss: 4.783188819885254 \n",
      "     Validation Step: 10 Validation Loss: 9.90269947052002 \n",
      "     Validation Step: 11 Validation Loss: 6.927859783172607 \n",
      "     Validation Step: 12 Validation Loss: 13.698420524597168 \n",
      "     Validation Step: 13 Validation Loss: 12.103291511535645 \n",
      "     Validation Step: 14 Validation Loss: 5.034355640411377 \n",
      "Epoch: 61\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.6355698108673096 \n",
      "     Training Step: 1 Training Loss: 3.2790489196777344 \n",
      "     Training Step: 2 Training Loss: 2.0579960346221924 \n",
      "     Training Step: 3 Training Loss: 3.4389989376068115 \n",
      "     Training Step: 4 Training Loss: 2.5847768783569336 \n",
      "     Training Step: 5 Training Loss: 2.554276704788208 \n",
      "     Training Step: 6 Training Loss: 2.485879898071289 \n",
      "     Training Step: 7 Training Loss: 2.446885585784912 \n",
      "     Training Step: 8 Training Loss: 2.324148654937744 \n",
      "     Training Step: 9 Training Loss: 2.551927089691162 \n",
      "     Training Step: 10 Training Loss: 2.7412290573120117 \n",
      "     Training Step: 11 Training Loss: 2.4219207763671875 \n",
      "     Training Step: 12 Training Loss: 2.387845516204834 \n",
      "     Training Step: 13 Training Loss: 2.9680519104003906 \n",
      "     Training Step: 14 Training Loss: 2.352520227432251 \n",
      "     Training Step: 15 Training Loss: 2.8653669357299805 \n",
      "     Training Step: 16 Training Loss: 3.28800106048584 \n",
      "     Training Step: 17 Training Loss: 2.507077693939209 \n",
      "     Training Step: 18 Training Loss: 3.054689884185791 \n",
      "     Training Step: 19 Training Loss: 2.3753955364227295 \n",
      "     Training Step: 20 Training Loss: 3.495718479156494 \n",
      "     Training Step: 21 Training Loss: 2.7977917194366455 \n",
      "     Training Step: 22 Training Loss: 2.6439530849456787 \n",
      "     Training Step: 23 Training Loss: 3.3722167015075684 \n",
      "     Training Step: 24 Training Loss: 2.768357276916504 \n",
      "     Training Step: 25 Training Loss: 2.9859795570373535 \n",
      "     Training Step: 26 Training Loss: 3.1551802158355713 \n",
      "     Training Step: 27 Training Loss: 2.31691837310791 \n",
      "     Training Step: 28 Training Loss: 3.2671611309051514 \n",
      "     Training Step: 29 Training Loss: 2.217538833618164 \n",
      "     Training Step: 30 Training Loss: 2.7339463233947754 \n",
      "     Training Step: 31 Training Loss: 2.6649608612060547 \n",
      "     Training Step: 32 Training Loss: 2.751533031463623 \n",
      "     Training Step: 33 Training Loss: 2.944531202316284 \n",
      "     Training Step: 34 Training Loss: 2.8072686195373535 \n",
      "     Training Step: 35 Training Loss: 2.9901790618896484 \n",
      "     Training Step: 36 Training Loss: 2.7854702472686768 \n",
      "     Training Step: 37 Training Loss: 2.4883780479431152 \n",
      "     Training Step: 38 Training Loss: 2.213292121887207 \n",
      "     Training Step: 39 Training Loss: 2.548321008682251 \n",
      "     Training Step: 40 Training Loss: 3.0088748931884766 \n",
      "     Training Step: 41 Training Loss: 3.368178129196167 \n",
      "     Training Step: 42 Training Loss: 2.822603225708008 \n",
      "     Training Step: 43 Training Loss: 3.030764102935791 \n",
      "     Training Step: 44 Training Loss: 2.229597568511963 \n",
      "     Training Step: 45 Training Loss: 2.8533804416656494 \n",
      "     Training Step: 46 Training Loss: 2.0107736587524414 \n",
      "     Training Step: 47 Training Loss: 2.209465980529785 \n",
      "     Training Step: 48 Training Loss: 2.3026845455169678 \n",
      "     Training Step: 49 Training Loss: 2.301964044570923 \n",
      "     Training Step: 50 Training Loss: 3.3419573307037354 \n",
      "     Training Step: 51 Training Loss: 3.6319408416748047 \n",
      "     Training Step: 52 Training Loss: 3.3818068504333496 \n",
      "     Training Step: 53 Training Loss: 2.5126912593841553 \n",
      "     Training Step: 54 Training Loss: 3.7466955184936523 \n",
      "     Training Step: 55 Training Loss: 2.8762738704681396 \n",
      "     Training Step: 56 Training Loss: 3.5802860260009766 \n",
      "     Training Step: 57 Training Loss: 3.4996771812438965 \n",
      "     Training Step: 58 Training Loss: 2.536621570587158 \n",
      "     Training Step: 59 Training Loss: 2.3883039951324463 \n",
      "     Training Step: 60 Training Loss: 3.328050136566162 \n",
      "     Training Step: 61 Training Loss: 2.908033609390259 \n",
      "     Training Step: 62 Training Loss: 2.9240691661834717 \n",
      "     Training Step: 63 Training Loss: 2.533116102218628 \n",
      "     Training Step: 64 Training Loss: 2.4196269512176514 \n",
      "     Training Step: 65 Training Loss: 2.5194225311279297 \n",
      "     Training Step: 66 Training Loss: 2.468540668487549 \n",
      "     Training Step: 67 Training Loss: 2.6171860694885254 \n",
      "     Training Step: 68 Training Loss: 2.3061156272888184 \n",
      "     Training Step: 69 Training Loss: 2.3685994148254395 \n",
      "     Training Step: 70 Training Loss: 2.282291889190674 \n",
      "     Training Step: 71 Training Loss: 3.3871171474456787 \n",
      "     Training Step: 72 Training Loss: 3.9170775413513184 \n",
      "     Training Step: 73 Training Loss: 3.755721092224121 \n",
      "     Training Step: 74 Training Loss: 2.3278894424438477 \n",
      "     Training Step: 75 Training Loss: 3.6431431770324707 \n",
      "     Training Step: 76 Training Loss: 2.7319881916046143 \n",
      "     Training Step: 77 Training Loss: 2.9789109230041504 \n",
      "     Training Step: 78 Training Loss: 2.6027445793151855 \n",
      "     Training Step: 79 Training Loss: 3.4189162254333496 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.321836471557617 \n",
      "     Validation Step: 1 Validation Loss: 2.7259020805358887 \n",
      "     Validation Step: 2 Validation Loss: 3.4820737838745117 \n",
      "     Validation Step: 3 Validation Loss: 2.551522731781006 \n",
      "     Validation Step: 4 Validation Loss: 3.259505271911621 \n",
      "     Validation Step: 5 Validation Loss: 2.785046100616455 \n",
      "     Validation Step: 6 Validation Loss: 2.381053924560547 \n",
      "     Validation Step: 7 Validation Loss: 3.0370004177093506 \n",
      "     Validation Step: 8 Validation Loss: 3.1503448486328125 \n",
      "     Validation Step: 9 Validation Loss: 3.0699212551116943 \n",
      "     Validation Step: 10 Validation Loss: 2.853329658508301 \n",
      "     Validation Step: 11 Validation Loss: 2.3067870140075684 \n",
      "     Validation Step: 12 Validation Loss: 2.680511951446533 \n",
      "     Validation Step: 13 Validation Loss: 3.0116653442382812 \n",
      "     Validation Step: 14 Validation Loss: 2.2327804565429688 \n",
      "Epoch: 62\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.0609235763549805 \n",
      "     Training Step: 1 Training Loss: 3.17659592628479 \n",
      "     Training Step: 2 Training Loss: 2.1783833503723145 \n",
      "     Training Step: 3 Training Loss: 2.174607515335083 \n",
      "     Training Step: 4 Training Loss: 2.0741963386535645 \n",
      "     Training Step: 5 Training Loss: 3.1043784618377686 \n",
      "     Training Step: 6 Training Loss: 3.13986873626709 \n",
      "     Training Step: 7 Training Loss: 2.9091439247131348 \n",
      "     Training Step: 8 Training Loss: 3.4113197326660156 \n",
      "     Training Step: 9 Training Loss: 2.7322945594787598 \n",
      "     Training Step: 10 Training Loss: 2.6169681549072266 \n",
      "     Training Step: 11 Training Loss: 2.9376585483551025 \n",
      "     Training Step: 12 Training Loss: 3.0303378105163574 \n",
      "     Training Step: 13 Training Loss: 2.6589856147766113 \n",
      "     Training Step: 14 Training Loss: 2.191561698913574 \n",
      "     Training Step: 15 Training Loss: 2.44677472114563 \n",
      "     Training Step: 16 Training Loss: 2.3249590396881104 \n",
      "     Training Step: 17 Training Loss: 2.220390796661377 \n",
      "     Training Step: 18 Training Loss: 3.325751781463623 \n",
      "     Training Step: 19 Training Loss: 2.368943929672241 \n",
      "     Training Step: 20 Training Loss: 3.3433055877685547 \n",
      "     Training Step: 21 Training Loss: 2.7937192916870117 \n",
      "     Training Step: 22 Training Loss: 3.1934733390808105 \n",
      "     Training Step: 23 Training Loss: 3.539168357849121 \n",
      "     Training Step: 24 Training Loss: 4.160991191864014 \n",
      "     Training Step: 25 Training Loss: 2.595266819000244 \n",
      "     Training Step: 26 Training Loss: 2.7317471504211426 \n",
      "     Training Step: 27 Training Loss: 2.6427972316741943 \n",
      "     Training Step: 28 Training Loss: 2.339905261993408 \n",
      "     Training Step: 29 Training Loss: 2.443847894668579 \n",
      "     Training Step: 30 Training Loss: 2.3639633655548096 \n",
      "     Training Step: 31 Training Loss: 2.397139549255371 \n",
      "     Training Step: 32 Training Loss: 3.443136692047119 \n",
      "     Training Step: 33 Training Loss: 2.488542079925537 \n",
      "     Training Step: 34 Training Loss: 3.13734769821167 \n",
      "     Training Step: 35 Training Loss: 3.054051160812378 \n",
      "     Training Step: 36 Training Loss: 2.646345615386963 \n",
      "     Training Step: 37 Training Loss: 2.6646361351013184 \n",
      "     Training Step: 38 Training Loss: 2.7199504375457764 \n",
      "     Training Step: 39 Training Loss: 2.992504119873047 \n",
      "     Training Step: 40 Training Loss: 3.421041488647461 \n",
      "     Training Step: 41 Training Loss: 3.175034284591675 \n",
      "     Training Step: 42 Training Loss: 2.222857713699341 \n",
      "     Training Step: 43 Training Loss: 3.186616897583008 \n",
      "     Training Step: 44 Training Loss: 3.084123134613037 \n",
      "     Training Step: 45 Training Loss: 3.214345693588257 \n",
      "     Training Step: 46 Training Loss: 3.729248523712158 \n",
      "     Training Step: 47 Training Loss: 2.296299695968628 \n",
      "     Training Step: 48 Training Loss: 2.6865251064300537 \n",
      "     Training Step: 49 Training Loss: 2.6053147315979004 \n",
      "     Training Step: 50 Training Loss: 2.660512924194336 \n",
      "     Training Step: 51 Training Loss: 2.4678382873535156 \n",
      "     Training Step: 52 Training Loss: 2.597283363342285 \n",
      "     Training Step: 53 Training Loss: 2.315915822982788 \n",
      "     Training Step: 54 Training Loss: 3.7120602130889893 \n",
      "     Training Step: 55 Training Loss: 4.433568954467773 \n",
      "     Training Step: 56 Training Loss: 4.211912155151367 \n",
      "     Training Step: 57 Training Loss: 4.363589286804199 \n",
      "     Training Step: 58 Training Loss: 2.1148648262023926 \n",
      "     Training Step: 59 Training Loss: 2.2037737369537354 \n",
      "     Training Step: 60 Training Loss: 2.6702089309692383 \n",
      "     Training Step: 61 Training Loss: 2.6055774688720703 \n",
      "     Training Step: 62 Training Loss: 3.20627498626709 \n",
      "     Training Step: 63 Training Loss: 3.055093765258789 \n",
      "     Training Step: 64 Training Loss: 3.664633274078369 \n",
      "     Training Step: 65 Training Loss: 3.743229866027832 \n",
      "     Training Step: 66 Training Loss: 3.6341564655303955 \n",
      "     Training Step: 67 Training Loss: 2.6190099716186523 \n",
      "     Training Step: 68 Training Loss: 3.978950023651123 \n",
      "     Training Step: 69 Training Loss: 3.9750943183898926 \n",
      "     Training Step: 70 Training Loss: 2.541748046875 \n",
      "     Training Step: 71 Training Loss: 3.103971481323242 \n",
      "     Training Step: 72 Training Loss: 3.233975410461426 \n",
      "     Training Step: 73 Training Loss: 2.6943447589874268 \n",
      "     Training Step: 74 Training Loss: 3.0929341316223145 \n",
      "     Training Step: 75 Training Loss: 3.601835250854492 \n",
      "     Training Step: 76 Training Loss: 3.197295665740967 \n",
      "     Training Step: 77 Training Loss: 3.3342061042785645 \n",
      "     Training Step: 78 Training Loss: 2.585078477859497 \n",
      "     Training Step: 79 Training Loss: 2.988157033920288 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.6997427940368652 \n",
      "     Validation Step: 1 Validation Loss: 2.272503137588501 \n",
      "     Validation Step: 2 Validation Loss: 2.965146064758301 \n",
      "     Validation Step: 3 Validation Loss: 3.2630581855773926 \n",
      "     Validation Step: 4 Validation Loss: 2.7251014709472656 \n",
      "     Validation Step: 5 Validation Loss: 3.580699920654297 \n",
      "     Validation Step: 6 Validation Loss: 3.2723846435546875 \n",
      "     Validation Step: 7 Validation Loss: 2.634354591369629 \n",
      "     Validation Step: 8 Validation Loss: 3.1506710052490234 \n",
      "     Validation Step: 9 Validation Loss: 2.446561813354492 \n",
      "     Validation Step: 10 Validation Loss: 3.5477006435394287 \n",
      "     Validation Step: 11 Validation Loss: 3.0839805603027344 \n",
      "     Validation Step: 12 Validation Loss: 2.4819352626800537 \n",
      "     Validation Step: 13 Validation Loss: 2.7969632148742676 \n",
      "     Validation Step: 14 Validation Loss: 3.4660515785217285 \n",
      "Epoch: 63\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.350492477416992 \n",
      "     Training Step: 1 Training Loss: 2.7761542797088623 \n",
      "     Training Step: 2 Training Loss: 3.0742547512054443 \n",
      "     Training Step: 3 Training Loss: 3.4398179054260254 \n",
      "     Training Step: 4 Training Loss: 3.5629544258117676 \n",
      "     Training Step: 5 Training Loss: 3.8351540565490723 \n",
      "     Training Step: 6 Training Loss: 2.3371591567993164 \n",
      "     Training Step: 7 Training Loss: 2.1466307640075684 \n",
      "     Training Step: 8 Training Loss: 3.4183337688446045 \n",
      "     Training Step: 9 Training Loss: 3.439323663711548 \n",
      "     Training Step: 10 Training Loss: 3.4735283851623535 \n",
      "     Training Step: 11 Training Loss: 2.482759714126587 \n",
      "     Training Step: 12 Training Loss: 2.5249476432800293 \n",
      "     Training Step: 13 Training Loss: 3.8131227493286133 \n",
      "     Training Step: 14 Training Loss: 2.285106658935547 \n",
      "     Training Step: 15 Training Loss: 2.5044875144958496 \n",
      "     Training Step: 16 Training Loss: 2.2803661823272705 \n",
      "     Training Step: 17 Training Loss: 2.891028881072998 \n",
      "     Training Step: 18 Training Loss: 2.4770584106445312 \n",
      "     Training Step: 19 Training Loss: 2.6894140243530273 \n",
      "     Training Step: 20 Training Loss: 2.416494369506836 \n",
      "     Training Step: 21 Training Loss: 2.9659314155578613 \n",
      "     Training Step: 22 Training Loss: 2.8433446884155273 \n",
      "     Training Step: 23 Training Loss: 2.8131070137023926 \n",
      "     Training Step: 24 Training Loss: 2.9125990867614746 \n",
      "     Training Step: 25 Training Loss: 2.824246644973755 \n",
      "     Training Step: 26 Training Loss: 3.9514894485473633 \n",
      "     Training Step: 27 Training Loss: 4.339582920074463 \n",
      "     Training Step: 28 Training Loss: 2.129723072052002 \n",
      "     Training Step: 29 Training Loss: 3.8834619522094727 \n",
      "     Training Step: 30 Training Loss: 3.5999560356140137 \n",
      "     Training Step: 31 Training Loss: 2.211543083190918 \n",
      "     Training Step: 32 Training Loss: 3.1559441089630127 \n",
      "     Training Step: 33 Training Loss: 3.321014642715454 \n",
      "     Training Step: 34 Training Loss: 2.520155668258667 \n",
      "     Training Step: 35 Training Loss: 2.9687461853027344 \n",
      "     Training Step: 36 Training Loss: 2.6559042930603027 \n",
      "     Training Step: 37 Training Loss: 3.444615602493286 \n",
      "     Training Step: 38 Training Loss: 2.644380807876587 \n",
      "     Training Step: 39 Training Loss: 3.680511474609375 \n",
      "     Training Step: 40 Training Loss: 2.8726015090942383 \n",
      "     Training Step: 41 Training Loss: 2.729069709777832 \n",
      "     Training Step: 42 Training Loss: 2.273205041885376 \n",
      "     Training Step: 43 Training Loss: 2.4821088314056396 \n",
      "     Training Step: 44 Training Loss: 2.5093166828155518 \n",
      "     Training Step: 45 Training Loss: 3.199690580368042 \n",
      "     Training Step: 46 Training Loss: 2.885711669921875 \n",
      "     Training Step: 47 Training Loss: 2.489046812057495 \n",
      "     Training Step: 48 Training Loss: 2.793769598007202 \n",
      "     Training Step: 49 Training Loss: 3.263688802719116 \n",
      "     Training Step: 50 Training Loss: 2.9064230918884277 \n",
      "     Training Step: 51 Training Loss: 2.7272562980651855 \n",
      "     Training Step: 52 Training Loss: 4.3142218589782715 \n",
      "     Training Step: 53 Training Loss: 3.3402628898620605 \n",
      "     Training Step: 54 Training Loss: 2.689541816711426 \n",
      "     Training Step: 55 Training Loss: 3.015148878097534 \n",
      "     Training Step: 56 Training Loss: 2.851558208465576 \n",
      "     Training Step: 57 Training Loss: 2.5671162605285645 \n",
      "     Training Step: 58 Training Loss: 2.8845326900482178 \n",
      "     Training Step: 59 Training Loss: 3.1151041984558105 \n",
      "     Training Step: 60 Training Loss: 3.166029214859009 \n",
      "     Training Step: 61 Training Loss: 2.046379804611206 \n",
      "     Training Step: 62 Training Loss: 3.0691781044006348 \n",
      "     Training Step: 63 Training Loss: 3.0757110118865967 \n",
      "     Training Step: 64 Training Loss: 2.6397342681884766 \n",
      "     Training Step: 65 Training Loss: 2.235316276550293 \n",
      "     Training Step: 66 Training Loss: 2.7301552295684814 \n",
      "     Training Step: 67 Training Loss: 2.3605520725250244 \n",
      "     Training Step: 68 Training Loss: 4.518928527832031 \n",
      "     Training Step: 69 Training Loss: 2.4143919944763184 \n",
      "     Training Step: 70 Training Loss: 2.604386806488037 \n",
      "     Training Step: 71 Training Loss: 4.575492858886719 \n",
      "     Training Step: 72 Training Loss: 2.312904119491577 \n",
      "     Training Step: 73 Training Loss: 2.4002461433410645 \n",
      "     Training Step: 74 Training Loss: 3.2159948348999023 \n",
      "     Training Step: 75 Training Loss: 2.4122750759124756 \n",
      "     Training Step: 76 Training Loss: 2.3115389347076416 \n",
      "     Training Step: 77 Training Loss: 2.249664068222046 \n",
      "     Training Step: 78 Training Loss: 2.3040928840637207 \n",
      "     Training Step: 79 Training Loss: 2.7971134185791016 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.239210844039917 \n",
      "     Validation Step: 1 Validation Loss: 2.3554508686065674 \n",
      "     Validation Step: 2 Validation Loss: 3.5278046131134033 \n",
      "     Validation Step: 3 Validation Loss: 3.0248594284057617 \n",
      "     Validation Step: 4 Validation Loss: 2.554131507873535 \n",
      "     Validation Step: 5 Validation Loss: 2.6708338260650635 \n",
      "     Validation Step: 6 Validation Loss: 3.3792877197265625 \n",
      "     Validation Step: 7 Validation Loss: 3.4421427249908447 \n",
      "     Validation Step: 8 Validation Loss: 3.602975368499756 \n",
      "     Validation Step: 9 Validation Loss: 3.724350690841675 \n",
      "     Validation Step: 10 Validation Loss: 2.8725876808166504 \n",
      "     Validation Step: 11 Validation Loss: 2.9635519981384277 \n",
      "     Validation Step: 12 Validation Loss: 2.778569221496582 \n",
      "     Validation Step: 13 Validation Loss: 2.422628879547119 \n",
      "     Validation Step: 14 Validation Loss: 3.1461822986602783 \n",
      "Epoch: 64\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.8166370391845703 \n",
      "     Training Step: 1 Training Loss: 3.3043031692504883 \n",
      "     Training Step: 2 Training Loss: 2.995069742202759 \n",
      "     Training Step: 3 Training Loss: 2.9556267261505127 \n",
      "     Training Step: 4 Training Loss: 4.579167366027832 \n",
      "     Training Step: 5 Training Loss: 2.63482666015625 \n",
      "     Training Step: 6 Training Loss: 3.885169506072998 \n",
      "     Training Step: 7 Training Loss: 3.189358711242676 \n",
      "     Training Step: 8 Training Loss: 2.163419485092163 \n",
      "     Training Step: 9 Training Loss: 2.4347879886627197 \n",
      "     Training Step: 10 Training Loss: 3.018167018890381 \n",
      "     Training Step: 11 Training Loss: 3.377235174179077 \n",
      "     Training Step: 12 Training Loss: 2.4053196907043457 \n",
      "     Training Step: 13 Training Loss: 2.1560871601104736 \n",
      "     Training Step: 14 Training Loss: 2.700906753540039 \n",
      "     Training Step: 15 Training Loss: 3.9529075622558594 \n",
      "     Training Step: 16 Training Loss: 3.056607246398926 \n",
      "     Training Step: 17 Training Loss: 3.8545994758605957 \n",
      "     Training Step: 18 Training Loss: 3.3856749534606934 \n",
      "     Training Step: 19 Training Loss: 3.9476754665374756 \n",
      "     Training Step: 20 Training Loss: 2.929025411605835 \n",
      "     Training Step: 21 Training Loss: 2.7837371826171875 \n",
      "     Training Step: 22 Training Loss: 2.6997005939483643 \n",
      "     Training Step: 23 Training Loss: 2.4101946353912354 \n",
      "     Training Step: 24 Training Loss: 4.0705037117004395 \n",
      "     Training Step: 25 Training Loss: 2.2036521434783936 \n",
      "     Training Step: 26 Training Loss: 4.069825172424316 \n",
      "     Training Step: 27 Training Loss: 2.2287724018096924 \n",
      "     Training Step: 28 Training Loss: 3.7236714363098145 \n",
      "     Training Step: 29 Training Loss: 3.2921931743621826 \n",
      "     Training Step: 30 Training Loss: 4.292503356933594 \n",
      "     Training Step: 31 Training Loss: 3.0457520484924316 \n",
      "     Training Step: 32 Training Loss: 3.6483469009399414 \n",
      "     Training Step: 33 Training Loss: 2.474405288696289 \n",
      "     Training Step: 34 Training Loss: 3.161085605621338 \n",
      "     Training Step: 35 Training Loss: 3.968397617340088 \n",
      "     Training Step: 36 Training Loss: 2.4151289463043213 \n",
      "     Training Step: 37 Training Loss: 3.075357437133789 \n",
      "     Training Step: 38 Training Loss: 2.1406006813049316 \n",
      "     Training Step: 39 Training Loss: 3.006072521209717 \n",
      "     Training Step: 40 Training Loss: 3.510308027267456 \n",
      "     Training Step: 41 Training Loss: 3.576425075531006 \n",
      "     Training Step: 42 Training Loss: 2.5647549629211426 \n",
      "     Training Step: 43 Training Loss: 2.3023178577423096 \n",
      "     Training Step: 44 Training Loss: 2.391667127609253 \n",
      "     Training Step: 45 Training Loss: 2.377190113067627 \n",
      "     Training Step: 46 Training Loss: 2.3370378017425537 \n",
      "     Training Step: 47 Training Loss: 2.341648817062378 \n",
      "     Training Step: 48 Training Loss: 2.265803337097168 \n",
      "     Training Step: 49 Training Loss: 2.798945665359497 \n",
      "     Training Step: 50 Training Loss: 2.6837716102600098 \n",
      "     Training Step: 51 Training Loss: 3.2978055477142334 \n",
      "     Training Step: 52 Training Loss: 2.975473165512085 \n",
      "     Training Step: 53 Training Loss: 3.124541997909546 \n",
      "     Training Step: 54 Training Loss: 3.680816173553467 \n",
      "     Training Step: 55 Training Loss: 4.100606918334961 \n",
      "     Training Step: 56 Training Loss: 2.8304805755615234 \n",
      "     Training Step: 57 Training Loss: 3.435638427734375 \n",
      "     Training Step: 58 Training Loss: 3.1693568229675293 \n",
      "     Training Step: 59 Training Loss: 3.089113235473633 \n",
      "     Training Step: 60 Training Loss: 3.567826509475708 \n",
      "     Training Step: 61 Training Loss: 3.1940441131591797 \n",
      "     Training Step: 62 Training Loss: 3.0819849967956543 \n",
      "     Training Step: 63 Training Loss: 2.978015661239624 \n",
      "     Training Step: 64 Training Loss: 2.742379665374756 \n",
      "     Training Step: 65 Training Loss: 2.5247392654418945 \n",
      "     Training Step: 66 Training Loss: 3.7250044345855713 \n",
      "     Training Step: 67 Training Loss: 2.448546886444092 \n",
      "     Training Step: 68 Training Loss: 3.9751317501068115 \n",
      "     Training Step: 69 Training Loss: 2.4564361572265625 \n",
      "     Training Step: 70 Training Loss: 2.963017463684082 \n",
      "     Training Step: 71 Training Loss: 3.46236515045166 \n",
      "     Training Step: 72 Training Loss: 2.551771879196167 \n",
      "     Training Step: 73 Training Loss: 2.571284294128418 \n",
      "     Training Step: 74 Training Loss: 2.9263148307800293 \n",
      "     Training Step: 75 Training Loss: 3.1761281490325928 \n",
      "     Training Step: 76 Training Loss: 2.353303909301758 \n",
      "     Training Step: 77 Training Loss: 3.092446804046631 \n",
      "     Training Step: 78 Training Loss: 2.3967509269714355 \n",
      "     Training Step: 79 Training Loss: 2.622293472290039 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.6334776878356934 \n",
      "     Validation Step: 1 Validation Loss: 3.3547286987304688 \n",
      "     Validation Step: 2 Validation Loss: 2.700427770614624 \n",
      "     Validation Step: 3 Validation Loss: 2.5452282428741455 \n",
      "     Validation Step: 4 Validation Loss: 2.4194741249084473 \n",
      "     Validation Step: 5 Validation Loss: 3.811025619506836 \n",
      "     Validation Step: 6 Validation Loss: 3.8318610191345215 \n",
      "     Validation Step: 7 Validation Loss: 2.7649917602539062 \n",
      "     Validation Step: 8 Validation Loss: 3.5420398712158203 \n",
      "     Validation Step: 9 Validation Loss: 2.287759304046631 \n",
      "     Validation Step: 10 Validation Loss: 3.3077077865600586 \n",
      "     Validation Step: 11 Validation Loss: 3.264392375946045 \n",
      "     Validation Step: 12 Validation Loss: 3.200045108795166 \n",
      "     Validation Step: 13 Validation Loss: 2.9103503227233887 \n",
      "     Validation Step: 14 Validation Loss: 3.4357707500457764 \n",
      "Epoch: 65\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.3412466049194336 \n",
      "     Training Step: 1 Training Loss: 2.4244332313537598 \n",
      "     Training Step: 2 Training Loss: 3.6601033210754395 \n",
      "     Training Step: 3 Training Loss: 2.4363794326782227 \n",
      "     Training Step: 4 Training Loss: 3.0832571983337402 \n",
      "     Training Step: 5 Training Loss: 2.882855176925659 \n",
      "     Training Step: 6 Training Loss: 2.520778179168701 \n",
      "     Training Step: 7 Training Loss: 3.821288585662842 \n",
      "     Training Step: 8 Training Loss: 2.736907482147217 \n",
      "     Training Step: 9 Training Loss: 3.9853460788726807 \n",
      "     Training Step: 10 Training Loss: 3.300044059753418 \n",
      "     Training Step: 11 Training Loss: 2.325242757797241 \n",
      "     Training Step: 12 Training Loss: 3.136544704437256 \n",
      "     Training Step: 13 Training Loss: 2.4410109519958496 \n",
      "     Training Step: 14 Training Loss: 2.2346086502075195 \n",
      "     Training Step: 15 Training Loss: 3.0691933631896973 \n",
      "     Training Step: 16 Training Loss: 3.402808904647827 \n",
      "     Training Step: 17 Training Loss: 2.297043561935425 \n",
      "     Training Step: 18 Training Loss: 3.5698938369750977 \n",
      "     Training Step: 19 Training Loss: 2.8692615032196045 \n",
      "     Training Step: 20 Training Loss: 3.5164237022399902 \n",
      "     Training Step: 21 Training Loss: 3.423877239227295 \n",
      "     Training Step: 22 Training Loss: 2.6200037002563477 \n",
      "     Training Step: 23 Training Loss: 2.2629904747009277 \n",
      "     Training Step: 24 Training Loss: 2.506802558898926 \n",
      "     Training Step: 25 Training Loss: 3.1912684440612793 \n",
      "     Training Step: 26 Training Loss: 3.4551334381103516 \n",
      "     Training Step: 27 Training Loss: 2.1556410789489746 \n",
      "     Training Step: 28 Training Loss: 3.0390005111694336 \n",
      "     Training Step: 29 Training Loss: 3.740813970565796 \n",
      "     Training Step: 30 Training Loss: 2.935110092163086 \n",
      "     Training Step: 31 Training Loss: 3.4247875213623047 \n",
      "     Training Step: 32 Training Loss: 3.8621678352355957 \n",
      "     Training Step: 33 Training Loss: 3.6848859786987305 \n",
      "     Training Step: 34 Training Loss: 2.7361297607421875 \n",
      "     Training Step: 35 Training Loss: 2.7050461769104004 \n",
      "     Training Step: 36 Training Loss: 2.411414384841919 \n",
      "     Training Step: 37 Training Loss: 3.1775684356689453 \n",
      "     Training Step: 38 Training Loss: 2.3705430030822754 \n",
      "     Training Step: 39 Training Loss: 2.130967140197754 \n",
      "     Training Step: 40 Training Loss: 2.2590174674987793 \n",
      "     Training Step: 41 Training Loss: 2.489447593688965 \n",
      "     Training Step: 42 Training Loss: 2.657982587814331 \n",
      "     Training Step: 43 Training Loss: 2.8750996589660645 \n",
      "     Training Step: 44 Training Loss: 3.3974289894104004 \n",
      "     Training Step: 45 Training Loss: 3.55523419380188 \n",
      "     Training Step: 46 Training Loss: 2.7299013137817383 \n",
      "     Training Step: 47 Training Loss: 2.7185142040252686 \n",
      "     Training Step: 48 Training Loss: 3.033860683441162 \n",
      "     Training Step: 49 Training Loss: 3.5523123741149902 \n",
      "     Training Step: 50 Training Loss: 2.528726100921631 \n",
      "     Training Step: 51 Training Loss: 2.5876331329345703 \n",
      "     Training Step: 52 Training Loss: 3.132133960723877 \n",
      "     Training Step: 53 Training Loss: 2.201655149459839 \n",
      "     Training Step: 54 Training Loss: 2.3190412521362305 \n",
      "     Training Step: 55 Training Loss: 3.4573025703430176 \n",
      "     Training Step: 56 Training Loss: 2.556990623474121 \n",
      "     Training Step: 57 Training Loss: 2.734179735183716 \n",
      "     Training Step: 58 Training Loss: 2.9428014755249023 \n",
      "     Training Step: 59 Training Loss: 2.816711902618408 \n",
      "     Training Step: 60 Training Loss: 2.7931809425354004 \n",
      "     Training Step: 61 Training Loss: 3.556525707244873 \n",
      "     Training Step: 62 Training Loss: 5.194555282592773 \n",
      "     Training Step: 63 Training Loss: 3.178161144256592 \n",
      "     Training Step: 64 Training Loss: 2.440124988555908 \n",
      "     Training Step: 65 Training Loss: 3.2478036880493164 \n",
      "     Training Step: 66 Training Loss: 2.6442463397979736 \n",
      "     Training Step: 67 Training Loss: 2.8246521949768066 \n",
      "     Training Step: 68 Training Loss: 4.507791519165039 \n",
      "     Training Step: 69 Training Loss: 2.335753917694092 \n",
      "     Training Step: 70 Training Loss: 3.1959481239318848 \n",
      "     Training Step: 71 Training Loss: 3.152040958404541 \n",
      "     Training Step: 72 Training Loss: 2.980825185775757 \n",
      "     Training Step: 73 Training Loss: 2.6741528511047363 \n",
      "     Training Step: 74 Training Loss: 3.1745309829711914 \n",
      "     Training Step: 75 Training Loss: 2.3682074546813965 \n",
      "     Training Step: 76 Training Loss: 4.582268714904785 \n",
      "     Training Step: 77 Training Loss: 3.6945347785949707 \n",
      "     Training Step: 78 Training Loss: 3.4372501373291016 \n",
      "     Training Step: 79 Training Loss: 2.620643138885498 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.034815549850464 \n",
      "     Validation Step: 1 Validation Loss: 3.5666956901550293 \n",
      "     Validation Step: 2 Validation Loss: 2.7859580516815186 \n",
      "     Validation Step: 3 Validation Loss: 3.471325635910034 \n",
      "     Validation Step: 4 Validation Loss: 3.576883316040039 \n",
      "     Validation Step: 5 Validation Loss: 4.05910587310791 \n",
      "     Validation Step: 6 Validation Loss: 3.9909300804138184 \n",
      "     Validation Step: 7 Validation Loss: 4.253793716430664 \n",
      "     Validation Step: 8 Validation Loss: 2.90686297416687 \n",
      "     Validation Step: 9 Validation Loss: 2.7579565048217773 \n",
      "     Validation Step: 10 Validation Loss: 2.430593967437744 \n",
      "     Validation Step: 11 Validation Loss: 3.007106304168701 \n",
      "     Validation Step: 12 Validation Loss: 2.9753942489624023 \n",
      "     Validation Step: 13 Validation Loss: 3.0002613067626953 \n",
      "     Validation Step: 14 Validation Loss: 3.536858081817627 \n",
      "Epoch: 66\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.757416248321533 \n",
      "     Training Step: 1 Training Loss: 2.7919888496398926 \n",
      "     Training Step: 2 Training Loss: 2.768770217895508 \n",
      "     Training Step: 3 Training Loss: 3.4729301929473877 \n",
      "     Training Step: 4 Training Loss: 3.3766136169433594 \n",
      "     Training Step: 5 Training Loss: 2.8275256156921387 \n",
      "     Training Step: 6 Training Loss: 2.5979864597320557 \n",
      "     Training Step: 7 Training Loss: 2.731060743331909 \n",
      "     Training Step: 8 Training Loss: 4.051044464111328 \n",
      "     Training Step: 9 Training Loss: 3.178060293197632 \n",
      "     Training Step: 10 Training Loss: 3.6726932525634766 \n",
      "     Training Step: 11 Training Loss: 4.170749187469482 \n",
      "     Training Step: 12 Training Loss: 3.206249713897705 \n",
      "     Training Step: 13 Training Loss: 2.413889169692993 \n",
      "     Training Step: 14 Training Loss: 3.694615125656128 \n",
      "     Training Step: 15 Training Loss: 3.8220558166503906 \n",
      "     Training Step: 16 Training Loss: 2.7899537086486816 \n",
      "     Training Step: 17 Training Loss: 2.8846139907836914 \n",
      "     Training Step: 18 Training Loss: 2.8885769844055176 \n",
      "     Training Step: 19 Training Loss: 3.6845200061798096 \n",
      "     Training Step: 20 Training Loss: 2.6363768577575684 \n",
      "     Training Step: 21 Training Loss: 2.3099935054779053 \n",
      "     Training Step: 22 Training Loss: 2.7882025241851807 \n",
      "     Training Step: 23 Training Loss: 3.1490566730499268 \n",
      "     Training Step: 24 Training Loss: 2.84049654006958 \n",
      "     Training Step: 25 Training Loss: 3.1084470748901367 \n",
      "     Training Step: 26 Training Loss: 2.650998115539551 \n",
      "     Training Step: 27 Training Loss: 2.5145320892333984 \n",
      "     Training Step: 28 Training Loss: 3.225149154663086 \n",
      "     Training Step: 29 Training Loss: 2.3865747451782227 \n",
      "     Training Step: 30 Training Loss: 3.2863335609436035 \n",
      "     Training Step: 31 Training Loss: 2.550858497619629 \n",
      "     Training Step: 32 Training Loss: 4.010443210601807 \n",
      "     Training Step: 33 Training Loss: 2.6733767986297607 \n",
      "     Training Step: 34 Training Loss: 2.793221950531006 \n",
      "     Training Step: 35 Training Loss: 3.3221027851104736 \n",
      "     Training Step: 36 Training Loss: 2.415220260620117 \n",
      "     Training Step: 37 Training Loss: 2.185811996459961 \n",
      "     Training Step: 38 Training Loss: 2.963890552520752 \n",
      "     Training Step: 39 Training Loss: 2.699791431427002 \n",
      "     Training Step: 40 Training Loss: 2.819584369659424 \n",
      "     Training Step: 41 Training Loss: 4.393510818481445 \n",
      "     Training Step: 42 Training Loss: 2.8043837547302246 \n",
      "     Training Step: 43 Training Loss: 2.8925158977508545 \n",
      "     Training Step: 44 Training Loss: 2.8223044872283936 \n",
      "     Training Step: 45 Training Loss: 2.4004061222076416 \n",
      "     Training Step: 46 Training Loss: 2.777791976928711 \n",
      "     Training Step: 47 Training Loss: 2.7563300132751465 \n",
      "     Training Step: 48 Training Loss: 3.068240165710449 \n",
      "     Training Step: 49 Training Loss: 2.8629727363586426 \n",
      "     Training Step: 50 Training Loss: 4.65223503112793 \n",
      "     Training Step: 51 Training Loss: 3.260021686553955 \n",
      "     Training Step: 52 Training Loss: 2.828136444091797 \n",
      "     Training Step: 53 Training Loss: 3.045400619506836 \n",
      "     Training Step: 54 Training Loss: 2.4707388877868652 \n",
      "     Training Step: 55 Training Loss: 3.355945110321045 \n",
      "     Training Step: 56 Training Loss: 3.379423141479492 \n",
      "     Training Step: 57 Training Loss: 3.264540195465088 \n",
      "     Training Step: 58 Training Loss: 3.2019405364990234 \n",
      "     Training Step: 59 Training Loss: 3.252058506011963 \n",
      "     Training Step: 60 Training Loss: 3.6671204566955566 \n",
      "     Training Step: 61 Training Loss: 2.8773813247680664 \n",
      "     Training Step: 62 Training Loss: 3.275329828262329 \n",
      "     Training Step: 63 Training Loss: 2.8429057598114014 \n",
      "     Training Step: 64 Training Loss: 4.12801456451416 \n",
      "     Training Step: 65 Training Loss: 2.37164568901062 \n",
      "     Training Step: 66 Training Loss: 4.199367046356201 \n",
      "     Training Step: 67 Training Loss: 2.6401526927948 \n",
      "     Training Step: 68 Training Loss: 2.4857144355773926 \n",
      "     Training Step: 69 Training Loss: 2.565760612487793 \n",
      "     Training Step: 70 Training Loss: 2.721193552017212 \n",
      "     Training Step: 71 Training Loss: 3.8316667079925537 \n",
      "     Training Step: 72 Training Loss: 3.205169439315796 \n",
      "     Training Step: 73 Training Loss: 3.3253824710845947 \n",
      "     Training Step: 74 Training Loss: 2.7015182971954346 \n",
      "     Training Step: 75 Training Loss: 2.681657552719116 \n",
      "     Training Step: 76 Training Loss: 2.801240921020508 \n",
      "     Training Step: 77 Training Loss: 3.0749926567077637 \n",
      "     Training Step: 78 Training Loss: 2.207120895385742 \n",
      "     Training Step: 79 Training Loss: 4.0178303718566895 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.828429698944092 \n",
      "     Validation Step: 1 Validation Loss: 4.197119235992432 \n",
      "     Validation Step: 2 Validation Loss: 3.5465245246887207 \n",
      "     Validation Step: 3 Validation Loss: 3.6235432624816895 \n",
      "     Validation Step: 4 Validation Loss: 2.683095693588257 \n",
      "     Validation Step: 5 Validation Loss: 4.505313873291016 \n",
      "     Validation Step: 6 Validation Loss: 3.656599283218384 \n",
      "     Validation Step: 7 Validation Loss: 4.333347320556641 \n",
      "     Validation Step: 8 Validation Loss: 3.793977737426758 \n",
      "     Validation Step: 9 Validation Loss: 3.3418149948120117 \n",
      "     Validation Step: 10 Validation Loss: 2.5750203132629395 \n",
      "     Validation Step: 11 Validation Loss: 3.6512160301208496 \n",
      "     Validation Step: 12 Validation Loss: 2.595144510269165 \n",
      "     Validation Step: 13 Validation Loss: 4.450430870056152 \n",
      "     Validation Step: 14 Validation Loss: 3.0166468620300293 \n",
      "Epoch: 67\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.9800796508789062 \n",
      "     Training Step: 1 Training Loss: 2.404939651489258 \n",
      "     Training Step: 2 Training Loss: 2.968441963195801 \n",
      "     Training Step: 3 Training Loss: 3.6953983306884766 \n",
      "     Training Step: 4 Training Loss: 3.8599987030029297 \n",
      "     Training Step: 5 Training Loss: 3.1228983402252197 \n",
      "     Training Step: 6 Training Loss: 2.333810329437256 \n",
      "     Training Step: 7 Training Loss: 3.110743522644043 \n",
      "     Training Step: 8 Training Loss: 3.337332248687744 \n",
      "     Training Step: 9 Training Loss: 2.82924222946167 \n",
      "     Training Step: 10 Training Loss: 2.4106948375701904 \n",
      "     Training Step: 11 Training Loss: 3.095079183578491 \n",
      "     Training Step: 12 Training Loss: 2.444843053817749 \n",
      "     Training Step: 13 Training Loss: 4.1123199462890625 \n",
      "     Training Step: 14 Training Loss: 3.220590829849243 \n",
      "     Training Step: 15 Training Loss: 2.131993532180786 \n",
      "     Training Step: 16 Training Loss: 2.8694229125976562 \n",
      "     Training Step: 17 Training Loss: 4.137569904327393 \n",
      "     Training Step: 18 Training Loss: 2.2587108612060547 \n",
      "     Training Step: 19 Training Loss: 3.3002805709838867 \n",
      "     Training Step: 20 Training Loss: 3.308314561843872 \n",
      "     Training Step: 21 Training Loss: 2.6860976219177246 \n",
      "     Training Step: 22 Training Loss: 2.951270341873169 \n",
      "     Training Step: 23 Training Loss: 3.1124143600463867 \n",
      "     Training Step: 24 Training Loss: 3.300198554992676 \n",
      "     Training Step: 25 Training Loss: 3.1401734352111816 \n",
      "     Training Step: 26 Training Loss: 3.372130870819092 \n",
      "     Training Step: 27 Training Loss: 2.4699552059173584 \n",
      "     Training Step: 28 Training Loss: 3.412726879119873 \n",
      "     Training Step: 29 Training Loss: 3.546781539916992 \n",
      "     Training Step: 30 Training Loss: 2.3688292503356934 \n",
      "     Training Step: 31 Training Loss: 3.266605854034424 \n",
      "     Training Step: 32 Training Loss: 2.29292893409729 \n",
      "     Training Step: 33 Training Loss: 2.540274143218994 \n",
      "     Training Step: 34 Training Loss: 3.1423349380493164 \n",
      "     Training Step: 35 Training Loss: 2.8153107166290283 \n",
      "     Training Step: 36 Training Loss: 2.2511684894561768 \n",
      "     Training Step: 37 Training Loss: 2.931042194366455 \n",
      "     Training Step: 38 Training Loss: 3.5365376472473145 \n",
      "     Training Step: 39 Training Loss: 2.0464277267456055 \n",
      "     Training Step: 40 Training Loss: 2.648054599761963 \n",
      "     Training Step: 41 Training Loss: 3.1463382244110107 \n",
      "     Training Step: 42 Training Loss: 3.9189534187316895 \n",
      "     Training Step: 43 Training Loss: 2.12688946723938 \n",
      "     Training Step: 44 Training Loss: 2.584662675857544 \n",
      "     Training Step: 45 Training Loss: 2.915128231048584 \n",
      "     Training Step: 46 Training Loss: 3.7320687770843506 \n",
      "     Training Step: 47 Training Loss: 3.1199893951416016 \n",
      "     Training Step: 48 Training Loss: 2.626598834991455 \n",
      "     Training Step: 49 Training Loss: 3.555039882659912 \n",
      "     Training Step: 50 Training Loss: 3.595332622528076 \n",
      "     Training Step: 51 Training Loss: 3.6844005584716797 \n",
      "     Training Step: 52 Training Loss: 3.7816548347473145 \n",
      "     Training Step: 53 Training Loss: 2.503875255584717 \n",
      "     Training Step: 54 Training Loss: 2.49776029586792 \n",
      "     Training Step: 55 Training Loss: 4.026512622833252 \n",
      "     Training Step: 56 Training Loss: 4.050997257232666 \n",
      "     Training Step: 57 Training Loss: 2.4530670642852783 \n",
      "     Training Step: 58 Training Loss: 3.1619176864624023 \n",
      "     Training Step: 59 Training Loss: 2.446878671646118 \n",
      "     Training Step: 60 Training Loss: 3.659313201904297 \n",
      "     Training Step: 61 Training Loss: 3.094566822052002 \n",
      "     Training Step: 62 Training Loss: 2.3307206630706787 \n",
      "     Training Step: 63 Training Loss: 2.230520248413086 \n",
      "     Training Step: 64 Training Loss: 2.4143435955047607 \n",
      "     Training Step: 65 Training Loss: 2.4176065921783447 \n",
      "     Training Step: 66 Training Loss: 2.1693615913391113 \n",
      "     Training Step: 67 Training Loss: 2.4591567516326904 \n",
      "     Training Step: 68 Training Loss: 2.457047939300537 \n",
      "     Training Step: 69 Training Loss: 3.645663261413574 \n",
      "     Training Step: 70 Training Loss: 2.6148295402526855 \n",
      "     Training Step: 71 Training Loss: 3.6586437225341797 \n",
      "     Training Step: 72 Training Loss: 3.2987351417541504 \n",
      "     Training Step: 73 Training Loss: 2.7996246814727783 \n",
      "     Training Step: 74 Training Loss: 3.1399145126342773 \n",
      "     Training Step: 75 Training Loss: 3.702789306640625 \n",
      "     Training Step: 76 Training Loss: 2.5563578605651855 \n",
      "     Training Step: 77 Training Loss: 3.049520492553711 \n",
      "     Training Step: 78 Training Loss: 3.159393310546875 \n",
      "     Training Step: 79 Training Loss: 2.6528210639953613 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.245046854019165 \n",
      "     Validation Step: 1 Validation Loss: 2.7663326263427734 \n",
      "     Validation Step: 2 Validation Loss: 2.2890074253082275 \n",
      "     Validation Step: 3 Validation Loss: 3.5367319583892822 \n",
      "     Validation Step: 4 Validation Loss: 3.752258777618408 \n",
      "     Validation Step: 5 Validation Loss: 3.3443093299865723 \n",
      "     Validation Step: 6 Validation Loss: 2.5215110778808594 \n",
      "     Validation Step: 7 Validation Loss: 3.1630852222442627 \n",
      "     Validation Step: 8 Validation Loss: 2.254012107849121 \n",
      "     Validation Step: 9 Validation Loss: 2.578977584838867 \n",
      "     Validation Step: 10 Validation Loss: 2.926823139190674 \n",
      "     Validation Step: 11 Validation Loss: 2.5314865112304688 \n",
      "     Validation Step: 12 Validation Loss: 2.9408857822418213 \n",
      "     Validation Step: 13 Validation Loss: 2.9075193405151367 \n",
      "     Validation Step: 14 Validation Loss: 3.76043701171875 \n",
      "Epoch: 68\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.92633056640625 \n",
      "     Training Step: 1 Training Loss: 3.888920783996582 \n",
      "     Training Step: 2 Training Loss: 4.255284786224365 \n",
      "     Training Step: 3 Training Loss: 2.32720685005188 \n",
      "     Training Step: 4 Training Loss: 3.0711750984191895 \n",
      "     Training Step: 5 Training Loss: 3.0128839015960693 \n",
      "     Training Step: 6 Training Loss: 3.2994914054870605 \n",
      "     Training Step: 7 Training Loss: 4.189970970153809 \n",
      "     Training Step: 8 Training Loss: 3.446687698364258 \n",
      "     Training Step: 9 Training Loss: 3.4511466026306152 \n",
      "     Training Step: 10 Training Loss: 2.2584075927734375 \n",
      "     Training Step: 11 Training Loss: 2.229454278945923 \n",
      "     Training Step: 12 Training Loss: 3.8915820121765137 \n",
      "     Training Step: 13 Training Loss: 2.9185805320739746 \n",
      "     Training Step: 14 Training Loss: 3.120506763458252 \n",
      "     Training Step: 15 Training Loss: 3.2052130699157715 \n",
      "     Training Step: 16 Training Loss: 2.2704856395721436 \n",
      "     Training Step: 17 Training Loss: 2.7452425956726074 \n",
      "     Training Step: 18 Training Loss: 2.514890432357788 \n",
      "     Training Step: 19 Training Loss: 2.288527727127075 \n",
      "     Training Step: 20 Training Loss: 2.964277744293213 \n",
      "     Training Step: 21 Training Loss: 2.6581850051879883 \n",
      "     Training Step: 22 Training Loss: 2.586643695831299 \n",
      "     Training Step: 23 Training Loss: 3.4925711154937744 \n",
      "     Training Step: 24 Training Loss: 3.0198493003845215 \n",
      "     Training Step: 25 Training Loss: 3.110100269317627 \n",
      "     Training Step: 26 Training Loss: 3.5676517486572266 \n",
      "     Training Step: 27 Training Loss: 3.1490564346313477 \n",
      "     Training Step: 28 Training Loss: 2.0429420471191406 \n",
      "     Training Step: 29 Training Loss: 2.8243188858032227 \n",
      "     Training Step: 30 Training Loss: 2.632249355316162 \n",
      "     Training Step: 31 Training Loss: 2.1323814392089844 \n",
      "     Training Step: 32 Training Loss: 3.132200241088867 \n",
      "     Training Step: 33 Training Loss: 3.4895029067993164 \n",
      "     Training Step: 34 Training Loss: 3.348206043243408 \n",
      "     Training Step: 35 Training Loss: 2.472032308578491 \n",
      "     Training Step: 36 Training Loss: 2.5806922912597656 \n",
      "     Training Step: 37 Training Loss: 2.9500184059143066 \n",
      "     Training Step: 38 Training Loss: 3.2351291179656982 \n",
      "     Training Step: 39 Training Loss: 2.5871729850769043 \n",
      "     Training Step: 40 Training Loss: 3.031161308288574 \n",
      "     Training Step: 41 Training Loss: 2.394306182861328 \n",
      "     Training Step: 42 Training Loss: 3.5740151405334473 \n",
      "     Training Step: 43 Training Loss: 3.485480785369873 \n",
      "     Training Step: 44 Training Loss: 2.946516513824463 \n",
      "     Training Step: 45 Training Loss: 3.057931423187256 \n",
      "     Training Step: 46 Training Loss: 2.6497881412506104 \n",
      "     Training Step: 47 Training Loss: 3.3137545585632324 \n",
      "     Training Step: 48 Training Loss: 2.2678942680358887 \n",
      "     Training Step: 49 Training Loss: 3.773848533630371 \n",
      "     Training Step: 50 Training Loss: 2.6342127323150635 \n",
      "     Training Step: 51 Training Loss: 4.194113731384277 \n",
      "     Training Step: 52 Training Loss: 3.086387872695923 \n",
      "     Training Step: 53 Training Loss: 2.8747386932373047 \n",
      "     Training Step: 54 Training Loss: 2.9177238941192627 \n",
      "     Training Step: 55 Training Loss: 3.993530750274658 \n",
      "     Training Step: 56 Training Loss: 3.256727695465088 \n",
      "     Training Step: 57 Training Loss: 2.7596187591552734 \n",
      "     Training Step: 58 Training Loss: 2.404909610748291 \n",
      "     Training Step: 59 Training Loss: 2.506186008453369 \n",
      "     Training Step: 60 Training Loss: 3.189793586730957 \n",
      "     Training Step: 61 Training Loss: 2.401590585708618 \n",
      "     Training Step: 62 Training Loss: 2.659193992614746 \n",
      "     Training Step: 63 Training Loss: 3.0411224365234375 \n",
      "     Training Step: 64 Training Loss: 2.4407236576080322 \n",
      "     Training Step: 65 Training Loss: 2.955921173095703 \n",
      "     Training Step: 66 Training Loss: 2.8083810806274414 \n",
      "     Training Step: 67 Training Loss: 3.055178165435791 \n",
      "     Training Step: 68 Training Loss: 3.1392412185668945 \n",
      "     Training Step: 69 Training Loss: 2.0809638500213623 \n",
      "     Training Step: 70 Training Loss: 2.4605884552001953 \n",
      "     Training Step: 71 Training Loss: 3.4012644290924072 \n",
      "     Training Step: 72 Training Loss: 2.8914644718170166 \n",
      "     Training Step: 73 Training Loss: 3.0658347606658936 \n",
      "     Training Step: 74 Training Loss: 3.0409350395202637 \n",
      "     Training Step: 75 Training Loss: 3.206784248352051 \n",
      "     Training Step: 76 Training Loss: 3.6537787914276123 \n",
      "     Training Step: 77 Training Loss: 3.303849697113037 \n",
      "     Training Step: 78 Training Loss: 2.143547534942627 \n",
      "     Training Step: 79 Training Loss: 2.591796398162842 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1738250255584717 \n",
      "     Validation Step: 1 Validation Loss: 2.976388692855835 \n",
      "     Validation Step: 2 Validation Loss: 2.900782823562622 \n",
      "     Validation Step: 3 Validation Loss: 2.64689040184021 \n",
      "     Validation Step: 4 Validation Loss: 3.5986595153808594 \n",
      "     Validation Step: 5 Validation Loss: 2.8018083572387695 \n",
      "     Validation Step: 6 Validation Loss: 3.9189863204956055 \n",
      "     Validation Step: 7 Validation Loss: 2.3344802856445312 \n",
      "     Validation Step: 8 Validation Loss: 3.2001137733459473 \n",
      "     Validation Step: 9 Validation Loss: 3.630046844482422 \n",
      "     Validation Step: 10 Validation Loss: 2.641007423400879 \n",
      "     Validation Step: 11 Validation Loss: 3.10603666305542 \n",
      "     Validation Step: 12 Validation Loss: 3.5892157554626465 \n",
      "     Validation Step: 13 Validation Loss: 3.2980189323425293 \n",
      "     Validation Step: 14 Validation Loss: 2.805398464202881 \n",
      "Epoch: 69\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.4569618701934814 \n",
      "     Training Step: 1 Training Loss: 3.118788480758667 \n",
      "     Training Step: 2 Training Loss: 2.4244837760925293 \n",
      "     Training Step: 3 Training Loss: 3.1173019409179688 \n",
      "     Training Step: 4 Training Loss: 2.9197497367858887 \n",
      "     Training Step: 5 Training Loss: 3.452955722808838 \n",
      "     Training Step: 6 Training Loss: 2.28100323677063 \n",
      "     Training Step: 7 Training Loss: 3.7038564682006836 \n",
      "     Training Step: 8 Training Loss: 3.1633975505828857 \n",
      "     Training Step: 9 Training Loss: 2.5192508697509766 \n",
      "     Training Step: 10 Training Loss: 3.100217819213867 \n",
      "     Training Step: 11 Training Loss: 2.558281421661377 \n",
      "     Training Step: 12 Training Loss: 2.336671829223633 \n",
      "     Training Step: 13 Training Loss: 2.6309447288513184 \n",
      "     Training Step: 14 Training Loss: 3.7643752098083496 \n",
      "     Training Step: 15 Training Loss: 3.329288959503174 \n",
      "     Training Step: 16 Training Loss: 2.8244190216064453 \n",
      "     Training Step: 17 Training Loss: 3.1597585678100586 \n",
      "     Training Step: 18 Training Loss: 2.4332666397094727 \n",
      "     Training Step: 19 Training Loss: 3.504819393157959 \n",
      "     Training Step: 20 Training Loss: 2.9182848930358887 \n",
      "     Training Step: 21 Training Loss: 3.1757771968841553 \n",
      "     Training Step: 22 Training Loss: 3.2260148525238037 \n",
      "     Training Step: 23 Training Loss: 2.7132151126861572 \n",
      "     Training Step: 24 Training Loss: 2.0834109783172607 \n",
      "     Training Step: 25 Training Loss: 2.6420178413391113 \n",
      "     Training Step: 26 Training Loss: 3.084001302719116 \n",
      "     Training Step: 27 Training Loss: 2.719355583190918 \n",
      "     Training Step: 28 Training Loss: 2.2316341400146484 \n",
      "     Training Step: 29 Training Loss: 2.5876529216766357 \n",
      "     Training Step: 30 Training Loss: 2.870124101638794 \n",
      "     Training Step: 31 Training Loss: 4.296316146850586 \n",
      "     Training Step: 32 Training Loss: 3.0355005264282227 \n",
      "     Training Step: 33 Training Loss: 3.12387752532959 \n",
      "     Training Step: 34 Training Loss: 2.9071879386901855 \n",
      "     Training Step: 35 Training Loss: 3.0135061740875244 \n",
      "     Training Step: 36 Training Loss: 3.276231288909912 \n",
      "     Training Step: 37 Training Loss: 3.941862106323242 \n",
      "     Training Step: 38 Training Loss: 3.645176410675049 \n",
      "     Training Step: 39 Training Loss: 2.52834415435791 \n",
      "     Training Step: 40 Training Loss: 2.8216867446899414 \n",
      "     Training Step: 41 Training Loss: 2.282526969909668 \n",
      "     Training Step: 42 Training Loss: 2.809553623199463 \n",
      "     Training Step: 43 Training Loss: 3.769899368286133 \n",
      "     Training Step: 44 Training Loss: 2.608783721923828 \n",
      "     Training Step: 45 Training Loss: 2.8472492694854736 \n",
      "     Training Step: 46 Training Loss: 3.091465950012207 \n",
      "     Training Step: 47 Training Loss: 3.4579672813415527 \n",
      "     Training Step: 48 Training Loss: 3.446979284286499 \n",
      "     Training Step: 49 Training Loss: 2.7618069648742676 \n",
      "     Training Step: 50 Training Loss: 4.327534198760986 \n",
      "     Training Step: 51 Training Loss: 2.286712169647217 \n",
      "     Training Step: 52 Training Loss: 3.675461530685425 \n",
      "     Training Step: 53 Training Loss: 2.493455410003662 \n",
      "     Training Step: 54 Training Loss: 2.844625949859619 \n",
      "     Training Step: 55 Training Loss: 2.324678421020508 \n",
      "     Training Step: 56 Training Loss: 2.46878719329834 \n",
      "     Training Step: 57 Training Loss: 3.6717286109924316 \n",
      "     Training Step: 58 Training Loss: 2.870272159576416 \n",
      "     Training Step: 59 Training Loss: 2.6702914237976074 \n",
      "     Training Step: 60 Training Loss: 3.5796926021575928 \n",
      "     Training Step: 61 Training Loss: 3.891183376312256 \n",
      "     Training Step: 62 Training Loss: 3.489076614379883 \n",
      "     Training Step: 63 Training Loss: 2.6481752395629883 \n",
      "     Training Step: 64 Training Loss: 4.025807857513428 \n",
      "     Training Step: 65 Training Loss: 3.1932716369628906 \n",
      "     Training Step: 66 Training Loss: 2.8548648357391357 \n",
      "     Training Step: 67 Training Loss: 2.895651340484619 \n",
      "     Training Step: 68 Training Loss: 3.1967365741729736 \n",
      "     Training Step: 69 Training Loss: 3.0581817626953125 \n",
      "     Training Step: 70 Training Loss: 3.4135994911193848 \n",
      "     Training Step: 71 Training Loss: 3.4416608810424805 \n",
      "     Training Step: 72 Training Loss: 3.0653493404388428 \n",
      "     Training Step: 73 Training Loss: 2.87869930267334 \n",
      "     Training Step: 74 Training Loss: 2.4079794883728027 \n",
      "     Training Step: 75 Training Loss: 3.728273868560791 \n",
      "     Training Step: 76 Training Loss: 2.7098488807678223 \n",
      "     Training Step: 77 Training Loss: 2.8159189224243164 \n",
      "     Training Step: 78 Training Loss: 2.86495304107666 \n",
      "     Training Step: 79 Training Loss: 2.741276264190674 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.4629292488098145 \n",
      "     Validation Step: 1 Validation Loss: 3.186652660369873 \n",
      "     Validation Step: 2 Validation Loss: 3.0839099884033203 \n",
      "     Validation Step: 3 Validation Loss: 3.952679395675659 \n",
      "     Validation Step: 4 Validation Loss: 2.7806310653686523 \n",
      "     Validation Step: 5 Validation Loss: 3.3821873664855957 \n",
      "     Validation Step: 6 Validation Loss: 3.904107093811035 \n",
      "     Validation Step: 7 Validation Loss: 4.131795883178711 \n",
      "     Validation Step: 8 Validation Loss: 3.0460362434387207 \n",
      "     Validation Step: 9 Validation Loss: 3.387998104095459 \n",
      "     Validation Step: 10 Validation Loss: 3.5851681232452393 \n",
      "     Validation Step: 11 Validation Loss: 3.3068745136260986 \n",
      "     Validation Step: 12 Validation Loss: 2.8595941066741943 \n",
      "     Validation Step: 13 Validation Loss: 2.758230686187744 \n",
      "     Validation Step: 14 Validation Loss: 3.1515703201293945 \n",
      "Epoch: 70\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.092508554458618 \n",
      "     Training Step: 1 Training Loss: 2.3504714965820312 \n",
      "     Training Step: 2 Training Loss: 3.7740707397460938 \n",
      "     Training Step: 3 Training Loss: 3.581231117248535 \n",
      "     Training Step: 4 Training Loss: 2.3271303176879883 \n",
      "     Training Step: 5 Training Loss: 2.433692216873169 \n",
      "     Training Step: 6 Training Loss: 2.3223042488098145 \n",
      "     Training Step: 7 Training Loss: 3.330448865890503 \n",
      "     Training Step: 8 Training Loss: 2.610761880874634 \n",
      "     Training Step: 9 Training Loss: 3.693258047103882 \n",
      "     Training Step: 10 Training Loss: 3.0851614475250244 \n",
      "     Training Step: 11 Training Loss: 3.4503250122070312 \n",
      "     Training Step: 12 Training Loss: 2.4077529907226562 \n",
      "     Training Step: 13 Training Loss: 2.4707589149475098 \n",
      "     Training Step: 14 Training Loss: 2.511570692062378 \n",
      "     Training Step: 15 Training Loss: 2.7364187240600586 \n",
      "     Training Step: 16 Training Loss: 2.5472476482391357 \n",
      "     Training Step: 17 Training Loss: 2.656698703765869 \n",
      "     Training Step: 18 Training Loss: 2.339851140975952 \n",
      "     Training Step: 19 Training Loss: 3.4255573749542236 \n",
      "     Training Step: 20 Training Loss: 3.1218056678771973 \n",
      "     Training Step: 21 Training Loss: 2.902228832244873 \n",
      "     Training Step: 22 Training Loss: 2.4731690883636475 \n",
      "     Training Step: 23 Training Loss: 2.4494121074676514 \n",
      "     Training Step: 24 Training Loss: 2.1316781044006348 \n",
      "     Training Step: 25 Training Loss: 2.2452292442321777 \n",
      "     Training Step: 26 Training Loss: 3.619837522506714 \n",
      "     Training Step: 27 Training Loss: 2.962334632873535 \n",
      "     Training Step: 28 Training Loss: 2.156996488571167 \n",
      "     Training Step: 29 Training Loss: 2.778700113296509 \n",
      "     Training Step: 30 Training Loss: 2.9550037384033203 \n",
      "     Training Step: 31 Training Loss: 2.5129175186157227 \n",
      "     Training Step: 32 Training Loss: 4.55842399597168 \n",
      "     Training Step: 33 Training Loss: 3.26843523979187 \n",
      "     Training Step: 34 Training Loss: 3.4783873558044434 \n",
      "     Training Step: 35 Training Loss: 2.663784980773926 \n",
      "     Training Step: 36 Training Loss: 4.338017463684082 \n",
      "     Training Step: 37 Training Loss: 2.3144054412841797 \n",
      "     Training Step: 38 Training Loss: 2.8360068798065186 \n",
      "     Training Step: 39 Training Loss: 2.4673337936401367 \n",
      "     Training Step: 40 Training Loss: 2.619072914123535 \n",
      "     Training Step: 41 Training Loss: 2.926816940307617 \n",
      "     Training Step: 42 Training Loss: 3.1883339881896973 \n",
      "     Training Step: 43 Training Loss: 2.591238498687744 \n",
      "     Training Step: 44 Training Loss: 4.169231414794922 \n",
      "     Training Step: 45 Training Loss: 3.551642894744873 \n",
      "     Training Step: 46 Training Loss: 2.6757893562316895 \n",
      "     Training Step: 47 Training Loss: 2.7308573722839355 \n",
      "     Training Step: 48 Training Loss: 3.8725790977478027 \n",
      "     Training Step: 49 Training Loss: 2.725267171859741 \n",
      "     Training Step: 50 Training Loss: 2.5984106063842773 \n",
      "     Training Step: 51 Training Loss: 4.013891220092773 \n",
      "     Training Step: 52 Training Loss: 2.3238179683685303 \n",
      "     Training Step: 53 Training Loss: 2.9592602252960205 \n",
      "     Training Step: 54 Training Loss: 3.466717004776001 \n",
      "     Training Step: 55 Training Loss: 2.2259469032287598 \n",
      "     Training Step: 56 Training Loss: 2.4811506271362305 \n",
      "     Training Step: 57 Training Loss: 2.326690673828125 \n",
      "     Training Step: 58 Training Loss: 3.901884078979492 \n",
      "     Training Step: 59 Training Loss: 2.9461183547973633 \n",
      "     Training Step: 60 Training Loss: 3.6660494804382324 \n",
      "     Training Step: 61 Training Loss: 2.8513550758361816 \n",
      "     Training Step: 62 Training Loss: 3.0426859855651855 \n",
      "     Training Step: 63 Training Loss: 3.4102654457092285 \n",
      "     Training Step: 64 Training Loss: 3.62593936920166 \n",
      "     Training Step: 65 Training Loss: 2.2639670372009277 \n",
      "     Training Step: 66 Training Loss: 2.8863027095794678 \n",
      "     Training Step: 67 Training Loss: 2.6659774780273438 \n",
      "     Training Step: 68 Training Loss: 3.024656295776367 \n",
      "     Training Step: 69 Training Loss: 2.8956503868103027 \n",
      "     Training Step: 70 Training Loss: 2.6805779933929443 \n",
      "     Training Step: 71 Training Loss: 3.0748329162597656 \n",
      "     Training Step: 72 Training Loss: 3.0382089614868164 \n",
      "     Training Step: 73 Training Loss: 3.0359930992126465 \n",
      "     Training Step: 74 Training Loss: 4.26733922958374 \n",
      "     Training Step: 75 Training Loss: 3.740424156188965 \n",
      "     Training Step: 76 Training Loss: 3.260685443878174 \n",
      "     Training Step: 77 Training Loss: 2.789181709289551 \n",
      "     Training Step: 78 Training Loss: 2.4530529975891113 \n",
      "     Training Step: 79 Training Loss: 2.1625802516937256 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.839892864227295 \n",
      "     Validation Step: 1 Validation Loss: 3.652275800704956 \n",
      "     Validation Step: 2 Validation Loss: 3.140718460083008 \n",
      "     Validation Step: 3 Validation Loss: 3.0058722496032715 \n",
      "     Validation Step: 4 Validation Loss: 3.616770029067993 \n",
      "     Validation Step: 5 Validation Loss: 3.241367816925049 \n",
      "     Validation Step: 6 Validation Loss: 2.8264873027801514 \n",
      "     Validation Step: 7 Validation Loss: 3.220012903213501 \n",
      "     Validation Step: 8 Validation Loss: 3.7282748222351074 \n",
      "     Validation Step: 9 Validation Loss: 3.033780813217163 \n",
      "     Validation Step: 10 Validation Loss: 2.0371928215026855 \n",
      "     Validation Step: 11 Validation Loss: 3.1720359325408936 \n",
      "     Validation Step: 12 Validation Loss: 3.2643210887908936 \n",
      "     Validation Step: 13 Validation Loss: 3.8247547149658203 \n",
      "     Validation Step: 14 Validation Loss: 2.994004487991333 \n",
      "Epoch: 71\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.2144975662231445 \n",
      "     Training Step: 1 Training Loss: 3.11665678024292 \n",
      "     Training Step: 2 Training Loss: 3.1273820400238037 \n",
      "     Training Step: 3 Training Loss: 2.870293140411377 \n",
      "     Training Step: 4 Training Loss: 2.6075549125671387 \n",
      "     Training Step: 5 Training Loss: 3.7189786434173584 \n",
      "     Training Step: 6 Training Loss: 3.311227321624756 \n",
      "     Training Step: 7 Training Loss: 3.038839817047119 \n",
      "     Training Step: 8 Training Loss: 3.7889065742492676 \n",
      "     Training Step: 9 Training Loss: 2.5586297512054443 \n",
      "     Training Step: 10 Training Loss: 2.867814064025879 \n",
      "     Training Step: 11 Training Loss: 2.6700611114501953 \n",
      "     Training Step: 12 Training Loss: 2.5887975692749023 \n",
      "     Training Step: 13 Training Loss: 2.653841972351074 \n",
      "     Training Step: 14 Training Loss: 3.4666523933410645 \n",
      "     Training Step: 15 Training Loss: 2.835458517074585 \n",
      "     Training Step: 16 Training Loss: 2.85123348236084 \n",
      "     Training Step: 17 Training Loss: 2.133788585662842 \n",
      "     Training Step: 18 Training Loss: 3.039170026779175 \n",
      "     Training Step: 19 Training Loss: 2.2482900619506836 \n",
      "     Training Step: 20 Training Loss: 2.7020068168640137 \n",
      "     Training Step: 21 Training Loss: 2.0196237564086914 \n",
      "     Training Step: 22 Training Loss: 3.631206750869751 \n",
      "     Training Step: 23 Training Loss: 2.964311122894287 \n",
      "     Training Step: 24 Training Loss: 2.4331789016723633 \n",
      "     Training Step: 25 Training Loss: 3.993518352508545 \n",
      "     Training Step: 26 Training Loss: 3.9214391708374023 \n",
      "     Training Step: 27 Training Loss: 4.110648155212402 \n",
      "     Training Step: 28 Training Loss: 2.516742706298828 \n",
      "     Training Step: 29 Training Loss: 4.3122029304504395 \n",
      "     Training Step: 30 Training Loss: 3.9425315856933594 \n",
      "     Training Step: 31 Training Loss: 3.1601760387420654 \n",
      "     Training Step: 32 Training Loss: 3.585658073425293 \n",
      "     Training Step: 33 Training Loss: 3.7493906021118164 \n",
      "     Training Step: 34 Training Loss: 3.2103147506713867 \n",
      "     Training Step: 35 Training Loss: 2.6787304878234863 \n",
      "     Training Step: 36 Training Loss: 2.920753240585327 \n",
      "     Training Step: 37 Training Loss: 3.4049129486083984 \n",
      "     Training Step: 38 Training Loss: 3.2271852493286133 \n",
      "     Training Step: 39 Training Loss: 3.935506820678711 \n",
      "     Training Step: 40 Training Loss: 3.004455089569092 \n",
      "     Training Step: 41 Training Loss: 2.689412832260132 \n",
      "     Training Step: 42 Training Loss: 2.643517255783081 \n",
      "     Training Step: 43 Training Loss: 3.3351805210113525 \n",
      "     Training Step: 44 Training Loss: 2.72222900390625 \n",
      "     Training Step: 45 Training Loss: 3.2947134971618652 \n",
      "     Training Step: 46 Training Loss: 2.178083896636963 \n",
      "     Training Step: 47 Training Loss: 3.057300090789795 \n",
      "     Training Step: 48 Training Loss: 3.946108341217041 \n",
      "     Training Step: 49 Training Loss: 2.2175238132476807 \n",
      "     Training Step: 50 Training Loss: 2.203476905822754 \n",
      "     Training Step: 51 Training Loss: 2.7790305614471436 \n",
      "     Training Step: 52 Training Loss: 2.2491555213928223 \n",
      "     Training Step: 53 Training Loss: 3.3149826526641846 \n",
      "     Training Step: 54 Training Loss: 3.2599215507507324 \n",
      "     Training Step: 55 Training Loss: 4.5201568603515625 \n",
      "     Training Step: 56 Training Loss: 2.9347825050354004 \n",
      "     Training Step: 57 Training Loss: 3.4656643867492676 \n",
      "     Training Step: 58 Training Loss: 3.0578501224517822 \n",
      "     Training Step: 59 Training Loss: 3.6030285358428955 \n",
      "     Training Step: 60 Training Loss: 2.422071933746338 \n",
      "     Training Step: 61 Training Loss: 3.132904529571533 \n",
      "     Training Step: 62 Training Loss: 3.4488561153411865 \n",
      "     Training Step: 63 Training Loss: 2.3296589851379395 \n",
      "     Training Step: 64 Training Loss: 2.7239973545074463 \n",
      "     Training Step: 65 Training Loss: 2.173391819000244 \n",
      "     Training Step: 66 Training Loss: 3.9351296424865723 \n",
      "     Training Step: 67 Training Loss: 3.5744895935058594 \n",
      "     Training Step: 68 Training Loss: 2.530240058898926 \n",
      "     Training Step: 69 Training Loss: 3.099202871322632 \n",
      "     Training Step: 70 Training Loss: 2.7750301361083984 \n",
      "     Training Step: 71 Training Loss: 3.0274832248687744 \n",
      "     Training Step: 72 Training Loss: 2.688926935195923 \n",
      "     Training Step: 73 Training Loss: 2.340763568878174 \n",
      "     Training Step: 74 Training Loss: 2.9146933555603027 \n",
      "     Training Step: 75 Training Loss: 3.010559558868408 \n",
      "     Training Step: 76 Training Loss: 3.3687686920166016 \n",
      "     Training Step: 77 Training Loss: 2.8069818019866943 \n",
      "     Training Step: 78 Training Loss: 3.9904985427856445 \n",
      "     Training Step: 79 Training Loss: 3.0871338844299316 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9581360816955566 \n",
      "     Validation Step: 1 Validation Loss: 3.515228271484375 \n",
      "     Validation Step: 2 Validation Loss: 3.146388530731201 \n",
      "     Validation Step: 3 Validation Loss: 3.9771599769592285 \n",
      "     Validation Step: 4 Validation Loss: 2.77168869972229 \n",
      "     Validation Step: 5 Validation Loss: 3.3409769535064697 \n",
      "     Validation Step: 6 Validation Loss: 4.107226371765137 \n",
      "     Validation Step: 7 Validation Loss: 2.9078633785247803 \n",
      "     Validation Step: 8 Validation Loss: 3.814162015914917 \n",
      "     Validation Step: 9 Validation Loss: 3.182302474975586 \n",
      "     Validation Step: 10 Validation Loss: 2.985215663909912 \n",
      "     Validation Step: 11 Validation Loss: 2.8925960063934326 \n",
      "     Validation Step: 12 Validation Loss: 3.254363536834717 \n",
      "     Validation Step: 13 Validation Loss: 3.6353023052215576 \n",
      "     Validation Step: 14 Validation Loss: 2.3038971424102783 \n",
      "Epoch: 72\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.2357876300811768 \n",
      "     Training Step: 1 Training Loss: 3.831292152404785 \n",
      "     Training Step: 2 Training Loss: 2.3062307834625244 \n",
      "     Training Step: 3 Training Loss: 3.4777581691741943 \n",
      "     Training Step: 4 Training Loss: 3.375150203704834 \n",
      "     Training Step: 5 Training Loss: 2.6379878520965576 \n",
      "     Training Step: 6 Training Loss: 2.910569190979004 \n",
      "     Training Step: 7 Training Loss: 3.5169525146484375 \n",
      "     Training Step: 8 Training Loss: 3.017564296722412 \n",
      "     Training Step: 9 Training Loss: 2.569835662841797 \n",
      "     Training Step: 10 Training Loss: 3.069490432739258 \n",
      "     Training Step: 11 Training Loss: 2.8517491817474365 \n",
      "     Training Step: 12 Training Loss: 2.9531140327453613 \n",
      "     Training Step: 13 Training Loss: 2.6418185234069824 \n",
      "     Training Step: 14 Training Loss: 4.1732025146484375 \n",
      "     Training Step: 15 Training Loss: 2.8207523822784424 \n",
      "     Training Step: 16 Training Loss: 2.351916790008545 \n",
      "     Training Step: 17 Training Loss: 2.1709036827087402 \n",
      "     Training Step: 18 Training Loss: 3.387037992477417 \n",
      "     Training Step: 19 Training Loss: 2.5357494354248047 \n",
      "     Training Step: 20 Training Loss: 3.3668394088745117 \n",
      "     Training Step: 21 Training Loss: 2.658308744430542 \n",
      "     Training Step: 22 Training Loss: 3.1958506107330322 \n",
      "     Training Step: 23 Training Loss: 3.3137245178222656 \n",
      "     Training Step: 24 Training Loss: 2.7852232456207275 \n",
      "     Training Step: 25 Training Loss: 3.22206449508667 \n",
      "     Training Step: 26 Training Loss: 2.6629831790924072 \n",
      "     Training Step: 27 Training Loss: 2.731424331665039 \n",
      "     Training Step: 28 Training Loss: 3.108336925506592 \n",
      "     Training Step: 29 Training Loss: 2.4564194679260254 \n",
      "     Training Step: 30 Training Loss: 3.2378435134887695 \n",
      "     Training Step: 31 Training Loss: 3.816606044769287 \n",
      "     Training Step: 32 Training Loss: 3.4593095779418945 \n",
      "     Training Step: 33 Training Loss: 3.0671849250793457 \n",
      "     Training Step: 34 Training Loss: 3.0405023097991943 \n",
      "     Training Step: 35 Training Loss: 2.400937557220459 \n",
      "     Training Step: 36 Training Loss: 3.151981830596924 \n",
      "     Training Step: 37 Training Loss: 2.869431257247925 \n",
      "     Training Step: 38 Training Loss: 3.344102621078491 \n",
      "     Training Step: 39 Training Loss: 3.0962142944335938 \n",
      "     Training Step: 40 Training Loss: 3.543107032775879 \n",
      "     Training Step: 41 Training Loss: 2.6534597873687744 \n",
      "     Training Step: 42 Training Loss: 2.943896770477295 \n",
      "     Training Step: 43 Training Loss: 3.830364465713501 \n",
      "     Training Step: 44 Training Loss: 2.929915189743042 \n",
      "     Training Step: 45 Training Loss: 3.6600940227508545 \n",
      "     Training Step: 46 Training Loss: 3.2512903213500977 \n",
      "     Training Step: 47 Training Loss: 2.7292771339416504 \n",
      "     Training Step: 48 Training Loss: 2.690009832382202 \n",
      "     Training Step: 49 Training Loss: 3.6105761528015137 \n",
      "     Training Step: 50 Training Loss: 2.111645221710205 \n",
      "     Training Step: 51 Training Loss: 2.672157049179077 \n",
      "     Training Step: 52 Training Loss: 2.1913962364196777 \n",
      "     Training Step: 53 Training Loss: 3.032698154449463 \n",
      "     Training Step: 54 Training Loss: 3.598752498626709 \n",
      "     Training Step: 55 Training Loss: 2.919260025024414 \n",
      "     Training Step: 56 Training Loss: 2.6615428924560547 \n",
      "     Training Step: 57 Training Loss: 3.5123515129089355 \n",
      "     Training Step: 58 Training Loss: 2.9950942993164062 \n",
      "     Training Step: 59 Training Loss: 3.5167832374572754 \n",
      "     Training Step: 60 Training Loss: 2.7507050037384033 \n",
      "     Training Step: 61 Training Loss: 3.177415370941162 \n",
      "     Training Step: 62 Training Loss: 3.277305841445923 \n",
      "     Training Step: 63 Training Loss: 2.6854164600372314 \n",
      "     Training Step: 64 Training Loss: 2.4980900287628174 \n",
      "     Training Step: 65 Training Loss: 3.2005231380462646 \n",
      "     Training Step: 66 Training Loss: 3.3334035873413086 \n",
      "     Training Step: 67 Training Loss: 2.148317337036133 \n",
      "     Training Step: 68 Training Loss: 2.682422161102295 \n",
      "     Training Step: 69 Training Loss: 2.5581765174865723 \n",
      "     Training Step: 70 Training Loss: 4.279880523681641 \n",
      "     Training Step: 71 Training Loss: 2.411449909210205 \n",
      "     Training Step: 72 Training Loss: 3.9151601791381836 \n",
      "     Training Step: 73 Training Loss: 3.0481910705566406 \n",
      "     Training Step: 74 Training Loss: 2.3934521675109863 \n",
      "     Training Step: 75 Training Loss: 3.141209363937378 \n",
      "     Training Step: 76 Training Loss: 3.080069065093994 \n",
      "     Training Step: 77 Training Loss: 2.8056023120880127 \n",
      "     Training Step: 78 Training Loss: 2.5628015995025635 \n",
      "     Training Step: 79 Training Loss: 2.6715610027313232 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.650590419769287 \n",
      "     Validation Step: 1 Validation Loss: 3.399604320526123 \n",
      "     Validation Step: 2 Validation Loss: 3.1098127365112305 \n",
      "     Validation Step: 3 Validation Loss: 2.6424341201782227 \n",
      "     Validation Step: 4 Validation Loss: 3.693294048309326 \n",
      "     Validation Step: 5 Validation Loss: 2.7313904762268066 \n",
      "     Validation Step: 6 Validation Loss: 3.2968146800994873 \n",
      "     Validation Step: 7 Validation Loss: 3.377579689025879 \n",
      "     Validation Step: 8 Validation Loss: 3.16683292388916 \n",
      "     Validation Step: 9 Validation Loss: 2.9194869995117188 \n",
      "     Validation Step: 10 Validation Loss: 2.903313398361206 \n",
      "     Validation Step: 11 Validation Loss: 3.196204900741577 \n",
      "     Validation Step: 12 Validation Loss: 3.4085075855255127 \n",
      "     Validation Step: 13 Validation Loss: 2.658904790878296 \n",
      "     Validation Step: 14 Validation Loss: 2.350834846496582 \n",
      "Epoch: 73\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.6072278022766113 \n",
      "     Training Step: 1 Training Loss: 2.474151134490967 \n",
      "     Training Step: 2 Training Loss: 2.723017692565918 \n",
      "     Training Step: 3 Training Loss: 3.2742176055908203 \n",
      "     Training Step: 4 Training Loss: 3.094348192214966 \n",
      "     Training Step: 5 Training Loss: 3.2585525512695312 \n",
      "     Training Step: 6 Training Loss: 3.94815993309021 \n",
      "     Training Step: 7 Training Loss: 3.2425780296325684 \n",
      "     Training Step: 8 Training Loss: 3.633665084838867 \n",
      "     Training Step: 9 Training Loss: 2.936885356903076 \n",
      "     Training Step: 10 Training Loss: 2.7462894916534424 \n",
      "     Training Step: 11 Training Loss: 2.3317325115203857 \n",
      "     Training Step: 12 Training Loss: 3.3927252292633057 \n",
      "     Training Step: 13 Training Loss: 2.7892704010009766 \n",
      "     Training Step: 14 Training Loss: 3.242896795272827 \n",
      "     Training Step: 15 Training Loss: 3.456852436065674 \n",
      "     Training Step: 16 Training Loss: 2.1038448810577393 \n",
      "     Training Step: 17 Training Loss: 2.8094582557678223 \n",
      "     Training Step: 18 Training Loss: 2.6828455924987793 \n",
      "     Training Step: 19 Training Loss: 2.973184823989868 \n",
      "     Training Step: 20 Training Loss: 2.243375301361084 \n",
      "     Training Step: 21 Training Loss: 4.42046594619751 \n",
      "     Training Step: 22 Training Loss: 3.538667917251587 \n",
      "     Training Step: 23 Training Loss: 3.1004042625427246 \n",
      "     Training Step: 24 Training Loss: 2.643651247024536 \n",
      "     Training Step: 25 Training Loss: 3.6242311000823975 \n",
      "     Training Step: 26 Training Loss: 2.1702218055725098 \n",
      "     Training Step: 27 Training Loss: 2.2070789337158203 \n",
      "     Training Step: 28 Training Loss: 2.1011810302734375 \n",
      "     Training Step: 29 Training Loss: 2.9630744457244873 \n",
      "     Training Step: 30 Training Loss: 2.9432194232940674 \n",
      "     Training Step: 31 Training Loss: 2.7028679847717285 \n",
      "     Training Step: 32 Training Loss: 2.56927227973938 \n",
      "     Training Step: 33 Training Loss: 2.7826180458068848 \n",
      "     Training Step: 34 Training Loss: 3.1068992614746094 \n",
      "     Training Step: 35 Training Loss: 3.715306043624878 \n",
      "     Training Step: 36 Training Loss: 3.334745407104492 \n",
      "     Training Step: 37 Training Loss: 3.111304521560669 \n",
      "     Training Step: 38 Training Loss: 2.5773258209228516 \n",
      "     Training Step: 39 Training Loss: 2.8552913665771484 \n",
      "     Training Step: 40 Training Loss: 2.808370590209961 \n",
      "     Training Step: 41 Training Loss: 3.2276172637939453 \n",
      "     Training Step: 42 Training Loss: 2.372655153274536 \n",
      "     Training Step: 43 Training Loss: 3.2118000984191895 \n",
      "     Training Step: 44 Training Loss: 3.524588108062744 \n",
      "     Training Step: 45 Training Loss: 3.010941743850708 \n",
      "     Training Step: 46 Training Loss: 3.201352596282959 \n",
      "     Training Step: 47 Training Loss: 2.309192180633545 \n",
      "     Training Step: 48 Training Loss: 4.142763137817383 \n",
      "     Training Step: 49 Training Loss: 2.677260398864746 \n",
      "     Training Step: 50 Training Loss: 2.428391456604004 \n",
      "     Training Step: 51 Training Loss: 2.9517111778259277 \n",
      "     Training Step: 52 Training Loss: 3.3177807331085205 \n",
      "     Training Step: 53 Training Loss: 3.4611868858337402 \n",
      "     Training Step: 54 Training Loss: 2.7352242469787598 \n",
      "     Training Step: 55 Training Loss: 2.6941545009613037 \n",
      "     Training Step: 56 Training Loss: 2.6906094551086426 \n",
      "     Training Step: 57 Training Loss: 2.672316551208496 \n",
      "     Training Step: 58 Training Loss: 3.070032835006714 \n",
      "     Training Step: 59 Training Loss: 2.4976673126220703 \n",
      "     Training Step: 60 Training Loss: 2.725794792175293 \n",
      "     Training Step: 61 Training Loss: 3.648561477661133 \n",
      "     Training Step: 62 Training Loss: 4.511281967163086 \n",
      "     Training Step: 63 Training Loss: 2.9737601280212402 \n",
      "     Training Step: 64 Training Loss: 2.4902820587158203 \n",
      "     Training Step: 65 Training Loss: 3.118831157684326 \n",
      "     Training Step: 66 Training Loss: 2.783702850341797 \n",
      "     Training Step: 67 Training Loss: 2.789210796356201 \n",
      "     Training Step: 68 Training Loss: 3.61007022857666 \n",
      "     Training Step: 69 Training Loss: 4.237696647644043 \n",
      "     Training Step: 70 Training Loss: 2.967020034790039 \n",
      "     Training Step: 71 Training Loss: 2.2867612838745117 \n",
      "     Training Step: 72 Training Loss: 3.621962547302246 \n",
      "     Training Step: 73 Training Loss: 3.668031692504883 \n",
      "     Training Step: 74 Training Loss: 2.463397979736328 \n",
      "     Training Step: 75 Training Loss: 2.4368886947631836 \n",
      "     Training Step: 76 Training Loss: 3.248748540878296 \n",
      "     Training Step: 77 Training Loss: 2.149308681488037 \n",
      "     Training Step: 78 Training Loss: 2.6298398971557617 \n",
      "     Training Step: 79 Training Loss: 2.9218690395355225 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.9335880279541016 \n",
      "     Validation Step: 1 Validation Loss: 2.983700752258301 \n",
      "     Validation Step: 2 Validation Loss: 3.4396328926086426 \n",
      "     Validation Step: 3 Validation Loss: 2.907719612121582 \n",
      "     Validation Step: 4 Validation Loss: 3.122973680496216 \n",
      "     Validation Step: 5 Validation Loss: 2.828704595565796 \n",
      "     Validation Step: 6 Validation Loss: 3.3585364818573 \n",
      "     Validation Step: 7 Validation Loss: 3.3923885822296143 \n",
      "     Validation Step: 8 Validation Loss: 2.3764214515686035 \n",
      "     Validation Step: 9 Validation Loss: 3.5788984298706055 \n",
      "     Validation Step: 10 Validation Loss: 3.9011011123657227 \n",
      "     Validation Step: 11 Validation Loss: 2.6351089477539062 \n",
      "     Validation Step: 12 Validation Loss: 2.8502767086029053 \n",
      "     Validation Step: 13 Validation Loss: 4.010265827178955 \n",
      "     Validation Step: 14 Validation Loss: 2.8790955543518066 \n",
      "Epoch: 74\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.36830997467041 \n",
      "     Training Step: 1 Training Loss: 2.213787794113159 \n",
      "     Training Step: 2 Training Loss: 2.9758782386779785 \n",
      "     Training Step: 3 Training Loss: 2.676769733428955 \n",
      "     Training Step: 4 Training Loss: 3.296267509460449 \n",
      "     Training Step: 5 Training Loss: 3.3955254554748535 \n",
      "     Training Step: 6 Training Loss: 2.9434192180633545 \n",
      "     Training Step: 7 Training Loss: 2.2294187545776367 \n",
      "     Training Step: 8 Training Loss: 2.2586917877197266 \n",
      "     Training Step: 9 Training Loss: 3.054830551147461 \n",
      "     Training Step: 10 Training Loss: 3.0235915184020996 \n",
      "     Training Step: 11 Training Loss: 2.8315305709838867 \n",
      "     Training Step: 12 Training Loss: 3.228639841079712 \n",
      "     Training Step: 13 Training Loss: 4.023061752319336 \n",
      "     Training Step: 14 Training Loss: 2.2308502197265625 \n",
      "     Training Step: 15 Training Loss: 3.9324965476989746 \n",
      "     Training Step: 16 Training Loss: 2.5270309448242188 \n",
      "     Training Step: 17 Training Loss: 3.0235469341278076 \n",
      "     Training Step: 18 Training Loss: 3.983541488647461 \n",
      "     Training Step: 19 Training Loss: 3.286546468734741 \n",
      "     Training Step: 20 Training Loss: 3.3651742935180664 \n",
      "     Training Step: 21 Training Loss: 4.387045860290527 \n",
      "     Training Step: 22 Training Loss: 2.789139747619629 \n",
      "     Training Step: 23 Training Loss: 3.3941264152526855 \n",
      "     Training Step: 24 Training Loss: 2.4398069381713867 \n",
      "     Training Step: 25 Training Loss: 3.480649471282959 \n",
      "     Training Step: 26 Training Loss: 2.7214252948760986 \n",
      "     Training Step: 27 Training Loss: 4.423488140106201 \n",
      "     Training Step: 28 Training Loss: 3.700007438659668 \n",
      "     Training Step: 29 Training Loss: 3.362921953201294 \n",
      "     Training Step: 30 Training Loss: 2.658388376235962 \n",
      "     Training Step: 31 Training Loss: 2.8448829650878906 \n",
      "     Training Step: 32 Training Loss: 2.5693845748901367 \n",
      "     Training Step: 33 Training Loss: 2.2262120246887207 \n",
      "     Training Step: 34 Training Loss: 3.0490190982818604 \n",
      "     Training Step: 35 Training Loss: 2.8120126724243164 \n",
      "     Training Step: 36 Training Loss: 2.8118057250976562 \n",
      "     Training Step: 37 Training Loss: 2.747690200805664 \n",
      "     Training Step: 38 Training Loss: 2.432555675506592 \n",
      "     Training Step: 39 Training Loss: 3.4359848499298096 \n",
      "     Training Step: 40 Training Loss: 2.967259407043457 \n",
      "     Training Step: 41 Training Loss: 3.0997462272644043 \n",
      "     Training Step: 42 Training Loss: 3.1353912353515625 \n",
      "     Training Step: 43 Training Loss: 2.6590702533721924 \n",
      "     Training Step: 44 Training Loss: 2.7202694416046143 \n",
      "     Training Step: 45 Training Loss: 3.5267374515533447 \n",
      "     Training Step: 46 Training Loss: 3.0902862548828125 \n",
      "     Training Step: 47 Training Loss: 4.272139072418213 \n",
      "     Training Step: 48 Training Loss: 4.03371524810791 \n",
      "     Training Step: 49 Training Loss: 3.0557732582092285 \n",
      "     Training Step: 50 Training Loss: 3.224226474761963 \n",
      "     Training Step: 51 Training Loss: 3.438476085662842 \n",
      "     Training Step: 52 Training Loss: 3.2036120891571045 \n",
      "     Training Step: 53 Training Loss: 2.09706449508667 \n",
      "     Training Step: 54 Training Loss: 3.5103085041046143 \n",
      "     Training Step: 55 Training Loss: 2.3200836181640625 \n",
      "     Training Step: 56 Training Loss: 2.0889973640441895 \n",
      "     Training Step: 57 Training Loss: 3.0868663787841797 \n",
      "     Training Step: 58 Training Loss: 3.371553659439087 \n",
      "     Training Step: 59 Training Loss: 2.5582587718963623 \n",
      "     Training Step: 60 Training Loss: 3.028298854827881 \n",
      "     Training Step: 61 Training Loss: 2.476532220840454 \n",
      "     Training Step: 62 Training Loss: 3.2174906730651855 \n",
      "     Training Step: 63 Training Loss: 3.177546739578247 \n",
      "     Training Step: 64 Training Loss: 4.2251386642456055 \n",
      "     Training Step: 65 Training Loss: 2.56494402885437 \n",
      "     Training Step: 66 Training Loss: 2.633150577545166 \n",
      "     Training Step: 67 Training Loss: 3.1078600883483887 \n",
      "     Training Step: 68 Training Loss: 2.27175235748291 \n",
      "     Training Step: 69 Training Loss: 2.3254826068878174 \n",
      "     Training Step: 70 Training Loss: 2.4935810565948486 \n",
      "     Training Step: 71 Training Loss: 3.3230721950531006 \n",
      "     Training Step: 72 Training Loss: 3.488147258758545 \n",
      "     Training Step: 73 Training Loss: 3.819162368774414 \n",
      "     Training Step: 74 Training Loss: 3.361384391784668 \n",
      "     Training Step: 75 Training Loss: 2.508281707763672 \n",
      "     Training Step: 76 Training Loss: 2.9857022762298584 \n",
      "     Training Step: 77 Training Loss: 3.1332077980041504 \n",
      "     Training Step: 78 Training Loss: 2.670274257659912 \n",
      "     Training Step: 79 Training Loss: 2.7195205688476562 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1401309967041016 \n",
      "     Validation Step: 1 Validation Loss: 3.1364247798919678 \n",
      "     Validation Step: 2 Validation Loss: 2.894442558288574 \n",
      "     Validation Step: 3 Validation Loss: 2.9268288612365723 \n",
      "     Validation Step: 4 Validation Loss: 2.9046664237976074 \n",
      "     Validation Step: 5 Validation Loss: 3.997066020965576 \n",
      "     Validation Step: 6 Validation Loss: 3.7985336780548096 \n",
      "     Validation Step: 7 Validation Loss: 3.666475772857666 \n",
      "     Validation Step: 8 Validation Loss: 3.213355541229248 \n",
      "     Validation Step: 9 Validation Loss: 3.0626416206359863 \n",
      "     Validation Step: 10 Validation Loss: 3.2968697547912598 \n",
      "     Validation Step: 11 Validation Loss: 3.0547640323638916 \n",
      "     Validation Step: 12 Validation Loss: 3.48704195022583 \n",
      "     Validation Step: 13 Validation Loss: 3.177858352661133 \n",
      "     Validation Step: 14 Validation Loss: 2.029367446899414 \n",
      "Epoch: 75\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.180116653442383 \n",
      "     Training Step: 1 Training Loss: 3.9012348651885986 \n",
      "     Training Step: 2 Training Loss: 2.827202081680298 \n",
      "     Training Step: 3 Training Loss: 2.695744037628174 \n",
      "     Training Step: 4 Training Loss: 2.5722551345825195 \n",
      "     Training Step: 5 Training Loss: 2.210800886154175 \n",
      "     Training Step: 6 Training Loss: 3.189393997192383 \n",
      "     Training Step: 7 Training Loss: 2.2418363094329834 \n",
      "     Training Step: 8 Training Loss: 2.4260473251342773 \n",
      "     Training Step: 9 Training Loss: 3.291635274887085 \n",
      "     Training Step: 10 Training Loss: 2.347437858581543 \n",
      "     Training Step: 11 Training Loss: 3.3452117443084717 \n",
      "     Training Step: 12 Training Loss: 3.9449121952056885 \n",
      "     Training Step: 13 Training Loss: 3.6541993618011475 \n",
      "     Training Step: 14 Training Loss: 3.1219892501831055 \n",
      "     Training Step: 15 Training Loss: 3.824615478515625 \n",
      "     Training Step: 16 Training Loss: 2.2590880393981934 \n",
      "     Training Step: 17 Training Loss: 2.7911434173583984 \n",
      "     Training Step: 18 Training Loss: 3.167377471923828 \n",
      "     Training Step: 19 Training Loss: 2.387650966644287 \n",
      "     Training Step: 20 Training Loss: 2.8902769088745117 \n",
      "     Training Step: 21 Training Loss: 2.8234500885009766 \n",
      "     Training Step: 22 Training Loss: 3.236628770828247 \n",
      "     Training Step: 23 Training Loss: 3.565706491470337 \n",
      "     Training Step: 24 Training Loss: 2.7676615715026855 \n",
      "     Training Step: 25 Training Loss: 2.7174487113952637 \n",
      "     Training Step: 26 Training Loss: 3.7677183151245117 \n",
      "     Training Step: 27 Training Loss: 3.4326610565185547 \n",
      "     Training Step: 28 Training Loss: 4.003658294677734 \n",
      "     Training Step: 29 Training Loss: 2.820348024368286 \n",
      "     Training Step: 30 Training Loss: 2.9434118270874023 \n",
      "     Training Step: 31 Training Loss: 2.2808871269226074 \n",
      "     Training Step: 32 Training Loss: 2.4419260025024414 \n",
      "     Training Step: 33 Training Loss: 3.484832286834717 \n",
      "     Training Step: 34 Training Loss: 2.172816038131714 \n",
      "     Training Step: 35 Training Loss: 2.866917848587036 \n",
      "     Training Step: 36 Training Loss: 3.321173906326294 \n",
      "     Training Step: 37 Training Loss: 3.5436339378356934 \n",
      "     Training Step: 38 Training Loss: 2.7170791625976562 \n",
      "     Training Step: 39 Training Loss: 3.1505866050720215 \n",
      "     Training Step: 40 Training Loss: 3.0826239585876465 \n",
      "     Training Step: 41 Training Loss: 2.9940643310546875 \n",
      "     Training Step: 42 Training Loss: 3.561765432357788 \n",
      "     Training Step: 43 Training Loss: 3.8895013332366943 \n",
      "     Training Step: 44 Training Loss: 3.0317587852478027 \n",
      "     Training Step: 45 Training Loss: 3.3417458534240723 \n",
      "     Training Step: 46 Training Loss: 3.0824801921844482 \n",
      "     Training Step: 47 Training Loss: 3.825640916824341 \n",
      "     Training Step: 48 Training Loss: 2.5748307704925537 \n",
      "     Training Step: 49 Training Loss: 3.140195369720459 \n",
      "     Training Step: 50 Training Loss: 3.0125348567962646 \n",
      "     Training Step: 51 Training Loss: 2.219028949737549 \n",
      "     Training Step: 52 Training Loss: 2.611403226852417 \n",
      "     Training Step: 53 Training Loss: 3.430675745010376 \n",
      "     Training Step: 54 Training Loss: 2.69344425201416 \n",
      "     Training Step: 55 Training Loss: 2.807689666748047 \n",
      "     Training Step: 56 Training Loss: 3.147580862045288 \n",
      "     Training Step: 57 Training Loss: 2.386720895767212 \n",
      "     Training Step: 58 Training Loss: 3.8042032718658447 \n",
      "     Training Step: 59 Training Loss: 2.2433595657348633 \n",
      "     Training Step: 60 Training Loss: 2.6262779235839844 \n",
      "     Training Step: 61 Training Loss: 3.601726531982422 \n",
      "     Training Step: 62 Training Loss: 2.287811040878296 \n",
      "     Training Step: 63 Training Loss: 3.4251346588134766 \n",
      "     Training Step: 64 Training Loss: 2.7931101322174072 \n",
      "     Training Step: 65 Training Loss: 2.4542887210845947 \n",
      "     Training Step: 66 Training Loss: 2.7031123638153076 \n",
      "     Training Step: 67 Training Loss: 2.9929275512695312 \n",
      "     Training Step: 68 Training Loss: 2.906451463699341 \n",
      "     Training Step: 69 Training Loss: 3.181394100189209 \n",
      "     Training Step: 70 Training Loss: 3.754263162612915 \n",
      "     Training Step: 71 Training Loss: 2.765831232070923 \n",
      "     Training Step: 72 Training Loss: 2.624915838241577 \n",
      "     Training Step: 73 Training Loss: 2.8048057556152344 \n",
      "     Training Step: 74 Training Loss: 2.336503028869629 \n",
      "     Training Step: 75 Training Loss: 3.0114965438842773 \n",
      "     Training Step: 76 Training Loss: 2.679286241531372 \n",
      "     Training Step: 77 Training Loss: 2.847494602203369 \n",
      "     Training Step: 78 Training Loss: 2.7680935859680176 \n",
      "     Training Step: 79 Training Loss: 3.508425235748291 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.106451988220215 \n",
      "     Validation Step: 1 Validation Loss: 3.509291410446167 \n",
      "     Validation Step: 2 Validation Loss: 3.256592035293579 \n",
      "     Validation Step: 3 Validation Loss: 2.9855923652648926 \n",
      "     Validation Step: 4 Validation Loss: 3.0498852729797363 \n",
      "     Validation Step: 5 Validation Loss: 3.3161821365356445 \n",
      "     Validation Step: 6 Validation Loss: 2.307995080947876 \n",
      "     Validation Step: 7 Validation Loss: 3.9152331352233887 \n",
      "     Validation Step: 8 Validation Loss: 2.6748526096343994 \n",
      "     Validation Step: 9 Validation Loss: 3.4143712520599365 \n",
      "     Validation Step: 10 Validation Loss: 3.756159782409668 \n",
      "     Validation Step: 11 Validation Loss: 3.707510232925415 \n",
      "     Validation Step: 12 Validation Loss: 2.9449567794799805 \n",
      "     Validation Step: 13 Validation Loss: 3.2289469242095947 \n",
      "     Validation Step: 14 Validation Loss: 3.0439116954803467 \n",
      "Epoch: 76\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.6906557083129883 \n",
      "     Training Step: 1 Training Loss: 3.1448099613189697 \n",
      "     Training Step: 2 Training Loss: 4.483408451080322 \n",
      "     Training Step: 3 Training Loss: 2.3854570388793945 \n",
      "     Training Step: 4 Training Loss: 2.222639560699463 \n",
      "     Training Step: 5 Training Loss: 2.705263614654541 \n",
      "     Training Step: 6 Training Loss: 3.184998035430908 \n",
      "     Training Step: 7 Training Loss: 2.9107205867767334 \n",
      "     Training Step: 8 Training Loss: 2.396313190460205 \n",
      "     Training Step: 9 Training Loss: 3.1088781356811523 \n",
      "     Training Step: 10 Training Loss: 3.1708521842956543 \n",
      "     Training Step: 11 Training Loss: 3.082336664199829 \n",
      "     Training Step: 12 Training Loss: 2.517308235168457 \n",
      "     Training Step: 13 Training Loss: 2.4434762001037598 \n",
      "     Training Step: 14 Training Loss: 2.692354917526245 \n",
      "     Training Step: 15 Training Loss: 3.6212515830993652 \n",
      "     Training Step: 16 Training Loss: 2.3145158290863037 \n",
      "     Training Step: 17 Training Loss: 3.366025447845459 \n",
      "     Training Step: 18 Training Loss: 2.7297699451446533 \n",
      "     Training Step: 19 Training Loss: 2.904444456100464 \n",
      "     Training Step: 20 Training Loss: 2.6992673873901367 \n",
      "     Training Step: 21 Training Loss: 2.8550591468811035 \n",
      "     Training Step: 22 Training Loss: 2.536966323852539 \n",
      "     Training Step: 23 Training Loss: 3.3366246223449707 \n",
      "     Training Step: 24 Training Loss: 2.860541343688965 \n",
      "     Training Step: 25 Training Loss: 3.010401725769043 \n",
      "     Training Step: 26 Training Loss: 2.7450568675994873 \n",
      "     Training Step: 27 Training Loss: 3.541822671890259 \n",
      "     Training Step: 28 Training Loss: 4.118041038513184 \n",
      "     Training Step: 29 Training Loss: 3.4197750091552734 \n",
      "     Training Step: 30 Training Loss: 2.663890838623047 \n",
      "     Training Step: 31 Training Loss: 2.3978466987609863 \n",
      "     Training Step: 32 Training Loss: 2.707815647125244 \n",
      "     Training Step: 33 Training Loss: 2.744950771331787 \n",
      "     Training Step: 34 Training Loss: 3.0873847007751465 \n",
      "     Training Step: 35 Training Loss: 2.700430393218994 \n",
      "     Training Step: 36 Training Loss: 2.7610743045806885 \n",
      "     Training Step: 37 Training Loss: 2.987617015838623 \n",
      "     Training Step: 38 Training Loss: 2.1622142791748047 \n",
      "     Training Step: 39 Training Loss: 2.994565010070801 \n",
      "     Training Step: 40 Training Loss: 2.267965316772461 \n",
      "     Training Step: 41 Training Loss: 2.2554855346679688 \n",
      "     Training Step: 42 Training Loss: 4.490623474121094 \n",
      "     Training Step: 43 Training Loss: 2.6334242820739746 \n",
      "     Training Step: 44 Training Loss: 2.922849178314209 \n",
      "     Training Step: 45 Training Loss: 3.262566566467285 \n",
      "     Training Step: 46 Training Loss: 2.6787071228027344 \n",
      "     Training Step: 47 Training Loss: 3.0300092697143555 \n",
      "     Training Step: 48 Training Loss: 2.250009536743164 \n",
      "     Training Step: 49 Training Loss: 3.321133613586426 \n",
      "     Training Step: 50 Training Loss: 3.0122904777526855 \n",
      "     Training Step: 51 Training Loss: 2.62545108795166 \n",
      "     Training Step: 52 Training Loss: 4.007596492767334 \n",
      "     Training Step: 53 Training Loss: 2.9905593395233154 \n",
      "     Training Step: 54 Training Loss: 2.5315494537353516 \n",
      "     Training Step: 55 Training Loss: 2.7027535438537598 \n",
      "     Training Step: 56 Training Loss: 2.3913321495056152 \n",
      "     Training Step: 57 Training Loss: 3.249868631362915 \n",
      "     Training Step: 58 Training Loss: 3.0146424770355225 \n",
      "     Training Step: 59 Training Loss: 2.7205405235290527 \n",
      "     Training Step: 60 Training Loss: 2.3377246856689453 \n",
      "     Training Step: 61 Training Loss: 3.7590861320495605 \n",
      "     Training Step: 62 Training Loss: 3.201151132583618 \n",
      "     Training Step: 63 Training Loss: 2.7590160369873047 \n",
      "     Training Step: 64 Training Loss: 2.578536033630371 \n",
      "     Training Step: 65 Training Loss: 2.600534677505493 \n",
      "     Training Step: 66 Training Loss: 2.4302966594696045 \n",
      "     Training Step: 67 Training Loss: 3.155036211013794 \n",
      "     Training Step: 68 Training Loss: 3.7380762100219727 \n",
      "     Training Step: 69 Training Loss: 2.666973352432251 \n",
      "     Training Step: 70 Training Loss: 3.13130784034729 \n",
      "     Training Step: 71 Training Loss: 2.881897449493408 \n",
      "     Training Step: 72 Training Loss: 2.5852389335632324 \n",
      "     Training Step: 73 Training Loss: 2.80853271484375 \n",
      "     Training Step: 74 Training Loss: 2.5448126792907715 \n",
      "     Training Step: 75 Training Loss: 2.7864789962768555 \n",
      "     Training Step: 76 Training Loss: 3.2834455966949463 \n",
      "     Training Step: 77 Training Loss: 3.495901107788086 \n",
      "     Training Step: 78 Training Loss: 2.8513426780700684 \n",
      "     Training Step: 79 Training Loss: 3.3656275272369385 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9917943477630615 \n",
      "     Validation Step: 1 Validation Loss: 3.208130359649658 \n",
      "     Validation Step: 2 Validation Loss: 3.4326629638671875 \n",
      "     Validation Step: 3 Validation Loss: 3.3852386474609375 \n",
      "     Validation Step: 4 Validation Loss: 2.5970935821533203 \n",
      "     Validation Step: 5 Validation Loss: 2.7846333980560303 \n",
      "     Validation Step: 6 Validation Loss: 2.938565731048584 \n",
      "     Validation Step: 7 Validation Loss: 2.823119878768921 \n",
      "     Validation Step: 8 Validation Loss: 2.510255813598633 \n",
      "     Validation Step: 9 Validation Loss: 2.8588528633117676 \n",
      "     Validation Step: 10 Validation Loss: 2.566067695617676 \n",
      "     Validation Step: 11 Validation Loss: 3.6342761516571045 \n",
      "     Validation Step: 12 Validation Loss: 2.4093048572540283 \n",
      "     Validation Step: 13 Validation Loss: 3.237705707550049 \n",
      "     Validation Step: 14 Validation Loss: 3.5911431312561035 \n",
      "Epoch: 77\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.253333806991577 \n",
      "     Training Step: 1 Training Loss: 3.611982822418213 \n",
      "     Training Step: 2 Training Loss: 2.967888355255127 \n",
      "     Training Step: 3 Training Loss: 2.2910187244415283 \n",
      "     Training Step: 4 Training Loss: 4.393502235412598 \n",
      "     Training Step: 5 Training Loss: 2.5497779846191406 \n",
      "     Training Step: 6 Training Loss: 3.0745229721069336 \n",
      "     Training Step: 7 Training Loss: 2.463163375854492 \n",
      "     Training Step: 8 Training Loss: 2.4339473247528076 \n",
      "     Training Step: 9 Training Loss: 2.9367291927337646 \n",
      "     Training Step: 10 Training Loss: 3.0262460708618164 \n",
      "     Training Step: 11 Training Loss: 3.447087049484253 \n",
      "     Training Step: 12 Training Loss: 2.23892879486084 \n",
      "     Training Step: 13 Training Loss: 2.5888073444366455 \n",
      "     Training Step: 14 Training Loss: 3.396327257156372 \n",
      "     Training Step: 15 Training Loss: 2.33026385307312 \n",
      "     Training Step: 16 Training Loss: 3.006371021270752 \n",
      "     Training Step: 17 Training Loss: 2.3793210983276367 \n",
      "     Training Step: 18 Training Loss: 2.9919979572296143 \n",
      "     Training Step: 19 Training Loss: 3.6892409324645996 \n",
      "     Training Step: 20 Training Loss: 3.182088613510132 \n",
      "     Training Step: 21 Training Loss: 3.919311285018921 \n",
      "     Training Step: 22 Training Loss: 2.602883815765381 \n",
      "     Training Step: 23 Training Loss: 2.816218852996826 \n",
      "     Training Step: 24 Training Loss: 3.097594738006592 \n",
      "     Training Step: 25 Training Loss: 3.2296059131622314 \n",
      "     Training Step: 26 Training Loss: 3.267519235610962 \n",
      "     Training Step: 27 Training Loss: 3.6589126586914062 \n",
      "     Training Step: 28 Training Loss: 3.335725784301758 \n",
      "     Training Step: 29 Training Loss: 2.7696285247802734 \n",
      "     Training Step: 30 Training Loss: 2.4296469688415527 \n",
      "     Training Step: 31 Training Loss: 2.2349541187286377 \n",
      "     Training Step: 32 Training Loss: 2.3866941928863525 \n",
      "     Training Step: 33 Training Loss: 2.6908345222473145 \n",
      "     Training Step: 34 Training Loss: 3.1795053482055664 \n",
      "     Training Step: 35 Training Loss: 3.519381284713745 \n",
      "     Training Step: 36 Training Loss: 2.1940736770629883 \n",
      "     Training Step: 37 Training Loss: 4.428057670593262 \n",
      "     Training Step: 38 Training Loss: 2.3858790397644043 \n",
      "     Training Step: 39 Training Loss: 3.17104172706604 \n",
      "     Training Step: 40 Training Loss: 3.304830312728882 \n",
      "     Training Step: 41 Training Loss: 2.8035504817962646 \n",
      "     Training Step: 42 Training Loss: 2.7132768630981445 \n",
      "     Training Step: 43 Training Loss: 3.212916374206543 \n",
      "     Training Step: 44 Training Loss: 3.0459532737731934 \n",
      "     Training Step: 45 Training Loss: 2.6921491622924805 \n",
      "     Training Step: 46 Training Loss: 3.2373132705688477 \n",
      "     Training Step: 47 Training Loss: 3.177863597869873 \n",
      "     Training Step: 48 Training Loss: 3.143580675125122 \n",
      "     Training Step: 49 Training Loss: 2.246386766433716 \n",
      "     Training Step: 50 Training Loss: 3.373112916946411 \n",
      "     Training Step: 51 Training Loss: 2.733898162841797 \n",
      "     Training Step: 52 Training Loss: 2.5535707473754883 \n",
      "     Training Step: 53 Training Loss: 3.745854139328003 \n",
      "     Training Step: 54 Training Loss: 2.762432098388672 \n",
      "     Training Step: 55 Training Loss: 3.077760696411133 \n",
      "     Training Step: 56 Training Loss: 2.1482934951782227 \n",
      "     Training Step: 57 Training Loss: 3.751736640930176 \n",
      "     Training Step: 58 Training Loss: 2.700411558151245 \n",
      "     Training Step: 59 Training Loss: 2.872659683227539 \n",
      "     Training Step: 60 Training Loss: 2.5108916759490967 \n",
      "     Training Step: 61 Training Loss: 2.9822235107421875 \n",
      "     Training Step: 62 Training Loss: 4.022724151611328 \n",
      "     Training Step: 63 Training Loss: 2.493865966796875 \n",
      "     Training Step: 64 Training Loss: 3.7419655323028564 \n",
      "     Training Step: 65 Training Loss: 2.351465940475464 \n",
      "     Training Step: 66 Training Loss: 3.917543888092041 \n",
      "     Training Step: 67 Training Loss: 3.3788070678710938 \n",
      "     Training Step: 68 Training Loss: 3.3501710891723633 \n",
      "     Training Step: 69 Training Loss: 2.137472629547119 \n",
      "     Training Step: 70 Training Loss: 3.3238914012908936 \n",
      "     Training Step: 71 Training Loss: 3.053617000579834 \n",
      "     Training Step: 72 Training Loss: 2.247985601425171 \n",
      "     Training Step: 73 Training Loss: 2.5122203826904297 \n",
      "     Training Step: 74 Training Loss: 3.496245861053467 \n",
      "     Training Step: 75 Training Loss: 2.6178674697875977 \n",
      "     Training Step: 76 Training Loss: 2.603571891784668 \n",
      "     Training Step: 77 Training Loss: 2.5116868019104004 \n",
      "     Training Step: 78 Training Loss: 2.4991352558135986 \n",
      "     Training Step: 79 Training Loss: 3.6932373046875 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.588927984237671 \n",
      "     Validation Step: 1 Validation Loss: 2.319812297821045 \n",
      "     Validation Step: 2 Validation Loss: 2.3365395069122314 \n",
      "     Validation Step: 3 Validation Loss: 2.6731886863708496 \n",
      "     Validation Step: 4 Validation Loss: 2.900085926055908 \n",
      "     Validation Step: 5 Validation Loss: 4.268548488616943 \n",
      "     Validation Step: 6 Validation Loss: 3.07485032081604 \n",
      "     Validation Step: 7 Validation Loss: 3.9524197578430176 \n",
      "     Validation Step: 8 Validation Loss: 3.2198009490966797 \n",
      "     Validation Step: 9 Validation Loss: 2.6315245628356934 \n",
      "     Validation Step: 10 Validation Loss: 3.6886258125305176 \n",
      "     Validation Step: 11 Validation Loss: 4.037630081176758 \n",
      "     Validation Step: 12 Validation Loss: 3.0970702171325684 \n",
      "     Validation Step: 13 Validation Loss: 2.4332892894744873 \n",
      "     Validation Step: 14 Validation Loss: 2.7937655448913574 \n",
      "---------------   LEARNING RATE HALVING DOWN TO: 8.000000000000001e-06   ---------------\n",
      "---------------   CURRENT LEARNING RATE: 8.000000000000001e-06   ---------------\n",
      "Epoch: 78\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.8465168476104736 \n",
      "     Training Step: 1 Training Loss: 3.2983169555664062 \n",
      "     Training Step: 2 Training Loss: 3.8158044815063477 \n",
      "     Training Step: 3 Training Loss: 3.83827543258667 \n",
      "     Training Step: 4 Training Loss: 3.498547077178955 \n",
      "     Training Step: 5 Training Loss: 2.681875228881836 \n",
      "     Training Step: 6 Training Loss: 2.158905506134033 \n",
      "     Training Step: 7 Training Loss: 3.473330020904541 \n",
      "     Training Step: 8 Training Loss: 2.7305426597595215 \n",
      "     Training Step: 9 Training Loss: 3.1231517791748047 \n",
      "     Training Step: 10 Training Loss: 3.3123342990875244 \n",
      "     Training Step: 11 Training Loss: 2.800264596939087 \n",
      "     Training Step: 12 Training Loss: 2.0819578170776367 \n",
      "     Training Step: 13 Training Loss: 3.3878846168518066 \n",
      "     Training Step: 14 Training Loss: 3.1154916286468506 \n",
      "     Training Step: 15 Training Loss: 3.700608253479004 \n",
      "     Training Step: 16 Training Loss: 2.6553831100463867 \n",
      "     Training Step: 17 Training Loss: 2.804182529449463 \n",
      "     Training Step: 18 Training Loss: 3.0499918460845947 \n",
      "     Training Step: 19 Training Loss: 2.495487689971924 \n",
      "     Training Step: 20 Training Loss: 3.2302772998809814 \n",
      "     Training Step: 21 Training Loss: 3.631469488143921 \n",
      "     Training Step: 22 Training Loss: 2.1254262924194336 \n",
      "     Training Step: 23 Training Loss: 2.456265687942505 \n",
      "     Training Step: 24 Training Loss: 3.616889476776123 \n",
      "     Training Step: 25 Training Loss: 3.616501569747925 \n",
      "     Training Step: 26 Training Loss: 2.5304174423217773 \n",
      "     Training Step: 27 Training Loss: 2.351551055908203 \n",
      "     Training Step: 28 Training Loss: 2.712475299835205 \n",
      "     Training Step: 29 Training Loss: 2.6418566703796387 \n",
      "     Training Step: 30 Training Loss: 2.1195802688598633 \n",
      "     Training Step: 31 Training Loss: 3.531841278076172 \n",
      "     Training Step: 32 Training Loss: 3.546875476837158 \n",
      "     Training Step: 33 Training Loss: 2.978020668029785 \n",
      "     Training Step: 34 Training Loss: 2.9578938484191895 \n",
      "     Training Step: 35 Training Loss: 2.893446922302246 \n",
      "     Training Step: 36 Training Loss: 3.117391347885132 \n",
      "     Training Step: 37 Training Loss: 3.1512279510498047 \n",
      "     Training Step: 38 Training Loss: 2.407409906387329 \n",
      "     Training Step: 39 Training Loss: 3.326674461364746 \n",
      "     Training Step: 40 Training Loss: 3.1475136280059814 \n",
      "     Training Step: 41 Training Loss: 2.3428897857666016 \n",
      "     Training Step: 42 Training Loss: 3.4315273761749268 \n",
      "     Training Step: 43 Training Loss: 2.613539218902588 \n",
      "     Training Step: 44 Training Loss: 3.9765663146972656 \n",
      "     Training Step: 45 Training Loss: 3.6021087169647217 \n",
      "     Training Step: 46 Training Loss: 3.109848976135254 \n",
      "     Training Step: 47 Training Loss: 3.1973462104797363 \n",
      "     Training Step: 48 Training Loss: 3.425299644470215 \n",
      "     Training Step: 49 Training Loss: 3.7161810398101807 \n",
      "     Training Step: 50 Training Loss: 2.304710865020752 \n",
      "     Training Step: 51 Training Loss: 3.9968349933624268 \n",
      "     Training Step: 52 Training Loss: 3.053882122039795 \n",
      "     Training Step: 53 Training Loss: 3.0132970809936523 \n",
      "     Training Step: 54 Training Loss: 2.3319778442382812 \n",
      "     Training Step: 55 Training Loss: 2.3858938217163086 \n",
      "     Training Step: 56 Training Loss: 3.19389009475708 \n",
      "     Training Step: 57 Training Loss: 2.9416627883911133 \n",
      "     Training Step: 58 Training Loss: 3.274895668029785 \n",
      "     Training Step: 59 Training Loss: 2.556326150894165 \n",
      "     Training Step: 60 Training Loss: 2.37503981590271 \n",
      "     Training Step: 61 Training Loss: 2.6359591484069824 \n",
      "     Training Step: 62 Training Loss: 2.3616862297058105 \n",
      "     Training Step: 63 Training Loss: 2.412137508392334 \n",
      "     Training Step: 64 Training Loss: 4.094822406768799 \n",
      "     Training Step: 65 Training Loss: 3.3870444297790527 \n",
      "     Training Step: 66 Training Loss: 2.8077621459960938 \n",
      "     Training Step: 67 Training Loss: 3.5541934967041016 \n",
      "     Training Step: 68 Training Loss: 3.5502703189849854 \n",
      "     Training Step: 69 Training Loss: 3.598839521408081 \n",
      "     Training Step: 70 Training Loss: 2.941814422607422 \n",
      "     Training Step: 71 Training Loss: 2.563458204269409 \n",
      "     Training Step: 72 Training Loss: 3.486198902130127 \n",
      "     Training Step: 73 Training Loss: 2.807600975036621 \n",
      "     Training Step: 74 Training Loss: 2.393235445022583 \n",
      "     Training Step: 75 Training Loss: 2.8345518112182617 \n",
      "     Training Step: 76 Training Loss: 4.4990234375 \n",
      "     Training Step: 77 Training Loss: 2.2257485389709473 \n",
      "     Training Step: 78 Training Loss: 3.9385738372802734 \n",
      "     Training Step: 79 Training Loss: 3.047114372253418 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.505666971206665 \n",
      "     Validation Step: 1 Validation Loss: 3.086466073989868 \n",
      "     Validation Step: 2 Validation Loss: 3.5051286220550537 \n",
      "     Validation Step: 3 Validation Loss: 3.6440563201904297 \n",
      "     Validation Step: 4 Validation Loss: 3.3745131492614746 \n",
      "     Validation Step: 5 Validation Loss: 2.872417449951172 \n",
      "     Validation Step: 6 Validation Loss: 3.328947067260742 \n",
      "     Validation Step: 7 Validation Loss: 3.7055747509002686 \n",
      "     Validation Step: 8 Validation Loss: 2.758960723876953 \n",
      "     Validation Step: 9 Validation Loss: 2.4656853675842285 \n",
      "     Validation Step: 10 Validation Loss: 3.3248109817504883 \n",
      "     Validation Step: 11 Validation Loss: 2.360898971557617 \n",
      "     Validation Step: 12 Validation Loss: 3.001498222351074 \n",
      "     Validation Step: 13 Validation Loss: 3.562925338745117 \n",
      "     Validation Step: 14 Validation Loss: 2.7631828784942627 \n",
      "Epoch: 79\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.4428610801696777 \n",
      "     Training Step: 1 Training Loss: 2.43595552444458 \n",
      "     Training Step: 2 Training Loss: 2.5482499599456787 \n",
      "     Training Step: 3 Training Loss: 4.053919315338135 \n",
      "     Training Step: 4 Training Loss: 3.187623977661133 \n",
      "     Training Step: 5 Training Loss: 2.5454044342041016 \n",
      "     Training Step: 6 Training Loss: 2.9727704524993896 \n",
      "     Training Step: 7 Training Loss: 3.0815649032592773 \n",
      "     Training Step: 8 Training Loss: 3.0299458503723145 \n",
      "     Training Step: 9 Training Loss: 2.915590286254883 \n",
      "     Training Step: 10 Training Loss: 2.139655590057373 \n",
      "     Training Step: 11 Training Loss: 2.743821144104004 \n",
      "     Training Step: 12 Training Loss: 2.2150959968566895 \n",
      "     Training Step: 13 Training Loss: 2.410475730895996 \n",
      "     Training Step: 14 Training Loss: 3.6081955432891846 \n",
      "     Training Step: 15 Training Loss: 2.9336228370666504 \n",
      "     Training Step: 16 Training Loss: 3.3451614379882812 \n",
      "     Training Step: 17 Training Loss: 2.590536594390869 \n",
      "     Training Step: 18 Training Loss: 2.382948637008667 \n",
      "     Training Step: 19 Training Loss: 2.5863213539123535 \n",
      "     Training Step: 20 Training Loss: 2.5216660499572754 \n",
      "     Training Step: 21 Training Loss: 3.9700989723205566 \n",
      "     Training Step: 22 Training Loss: 3.1373391151428223 \n",
      "     Training Step: 23 Training Loss: 2.0943644046783447 \n",
      "     Training Step: 24 Training Loss: 4.555177211761475 \n",
      "     Training Step: 25 Training Loss: 3.3272223472595215 \n",
      "     Training Step: 26 Training Loss: 2.48223876953125 \n",
      "     Training Step: 27 Training Loss: 2.878681182861328 \n",
      "     Training Step: 28 Training Loss: 2.507866859436035 \n",
      "     Training Step: 29 Training Loss: 3.5600907802581787 \n",
      "     Training Step: 30 Training Loss: 2.94711971282959 \n",
      "     Training Step: 31 Training Loss: 2.5485939979553223 \n",
      "     Training Step: 32 Training Loss: 3.174128532409668 \n",
      "     Training Step: 33 Training Loss: 3.1950631141662598 \n",
      "     Training Step: 34 Training Loss: 2.705336570739746 \n",
      "     Training Step: 35 Training Loss: 2.4566173553466797 \n",
      "     Training Step: 36 Training Loss: 2.8718881607055664 \n",
      "     Training Step: 37 Training Loss: 2.763195514678955 \n",
      "     Training Step: 38 Training Loss: 3.1134090423583984 \n",
      "     Training Step: 39 Training Loss: 3.159371852874756 \n",
      "     Training Step: 40 Training Loss: 2.9173154830932617 \n",
      "     Training Step: 41 Training Loss: 4.041712284088135 \n",
      "     Training Step: 42 Training Loss: 3.2261650562286377 \n",
      "     Training Step: 43 Training Loss: 3.422537326812744 \n",
      "     Training Step: 44 Training Loss: 3.5806033611297607 \n",
      "     Training Step: 45 Training Loss: 2.5699801445007324 \n",
      "     Training Step: 46 Training Loss: 2.1046414375305176 \n",
      "     Training Step: 47 Training Loss: 3.0386922359466553 \n",
      "     Training Step: 48 Training Loss: 4.135958671569824 \n",
      "     Training Step: 49 Training Loss: 2.477879047393799 \n",
      "     Training Step: 50 Training Loss: 2.6239142417907715 \n",
      "     Training Step: 51 Training Loss: 2.4046874046325684 \n",
      "     Training Step: 52 Training Loss: 3.0605971813201904 \n",
      "     Training Step: 53 Training Loss: 3.300178289413452 \n",
      "     Training Step: 54 Training Loss: 2.539167642593384 \n",
      "     Training Step: 55 Training Loss: 3.7343649864196777 \n",
      "     Training Step: 56 Training Loss: 3.382639169692993 \n",
      "     Training Step: 57 Training Loss: 3.126925230026245 \n",
      "     Training Step: 58 Training Loss: 2.9336581230163574 \n",
      "     Training Step: 59 Training Loss: 3.126133680343628 \n",
      "     Training Step: 60 Training Loss: 2.9480576515197754 \n",
      "     Training Step: 61 Training Loss: 3.146925449371338 \n",
      "     Training Step: 62 Training Loss: 2.449462890625 \n",
      "     Training Step: 63 Training Loss: 2.6353559494018555 \n",
      "     Training Step: 64 Training Loss: 2.500396251678467 \n",
      "     Training Step: 65 Training Loss: 3.419274091720581 \n",
      "     Training Step: 66 Training Loss: 3.7351455688476562 \n",
      "     Training Step: 67 Training Loss: 3.1139512062072754 \n",
      "     Training Step: 68 Training Loss: 2.442211151123047 \n",
      "     Training Step: 69 Training Loss: 3.2096290588378906 \n",
      "     Training Step: 70 Training Loss: 2.0993080139160156 \n",
      "     Training Step: 71 Training Loss: 3.1943435668945312 \n",
      "     Training Step: 72 Training Loss: 3.221226215362549 \n",
      "     Training Step: 73 Training Loss: 2.7548716068267822 \n",
      "     Training Step: 74 Training Loss: 2.7282285690307617 \n",
      "     Training Step: 75 Training Loss: 2.9285836219787598 \n",
      "     Training Step: 76 Training Loss: 2.36222767829895 \n",
      "     Training Step: 77 Training Loss: 2.4731523990631104 \n",
      "     Training Step: 78 Training Loss: 2.9850025177001953 \n",
      "     Training Step: 79 Training Loss: 3.2627859115600586 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.5505378246307373 \n",
      "     Validation Step: 1 Validation Loss: 2.200185775756836 \n",
      "     Validation Step: 2 Validation Loss: 3.482311725616455 \n",
      "     Validation Step: 3 Validation Loss: 3.1316211223602295 \n",
      "     Validation Step: 4 Validation Loss: 2.8587546348571777 \n",
      "     Validation Step: 5 Validation Loss: 3.5185394287109375 \n",
      "     Validation Step: 6 Validation Loss: 3.3684372901916504 \n",
      "     Validation Step: 7 Validation Loss: 2.9385013580322266 \n",
      "     Validation Step: 8 Validation Loss: 2.9692955017089844 \n",
      "     Validation Step: 9 Validation Loss: 2.6158688068389893 \n",
      "     Validation Step: 10 Validation Loss: 3.061396360397339 \n",
      "     Validation Step: 11 Validation Loss: 3.5351338386535645 \n",
      "     Validation Step: 12 Validation Loss: 3.074495315551758 \n",
      "     Validation Step: 13 Validation Loss: 2.7067956924438477 \n",
      "     Validation Step: 14 Validation Loss: 3.8193655014038086 \n",
      "Epoch: 80\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.1053011417388916 \n",
      "     Training Step: 1 Training Loss: 2.047422170639038 \n",
      "     Training Step: 2 Training Loss: 3.2456345558166504 \n",
      "     Training Step: 3 Training Loss: 4.762136459350586 \n",
      "     Training Step: 4 Training Loss: 2.3856003284454346 \n",
      "     Training Step: 5 Training Loss: 2.501915216445923 \n",
      "     Training Step: 6 Training Loss: 3.045243263244629 \n",
      "     Training Step: 7 Training Loss: 2.082569122314453 \n",
      "     Training Step: 8 Training Loss: 2.701554775238037 \n",
      "     Training Step: 9 Training Loss: 3.3373026847839355 \n",
      "     Training Step: 10 Training Loss: 3.5481371879577637 \n",
      "     Training Step: 11 Training Loss: 3.0162253379821777 \n",
      "     Training Step: 12 Training Loss: 2.960237503051758 \n",
      "     Training Step: 13 Training Loss: 3.632028341293335 \n",
      "     Training Step: 14 Training Loss: 3.027132034301758 \n",
      "     Training Step: 15 Training Loss: 3.0979084968566895 \n",
      "     Training Step: 16 Training Loss: 3.8975019454956055 \n",
      "     Training Step: 17 Training Loss: 2.5876898765563965 \n",
      "     Training Step: 18 Training Loss: 2.4830422401428223 \n",
      "     Training Step: 19 Training Loss: 3.1082205772399902 \n",
      "     Training Step: 20 Training Loss: 3.3334035873413086 \n",
      "     Training Step: 21 Training Loss: 3.507380247116089 \n",
      "     Training Step: 22 Training Loss: 2.863346576690674 \n",
      "     Training Step: 23 Training Loss: 2.218003749847412 \n",
      "     Training Step: 24 Training Loss: 2.9287567138671875 \n",
      "     Training Step: 25 Training Loss: 2.5128684043884277 \n",
      "     Training Step: 26 Training Loss: 3.710806369781494 \n",
      "     Training Step: 27 Training Loss: 3.721517562866211 \n",
      "     Training Step: 28 Training Loss: 2.617638111114502 \n",
      "     Training Step: 29 Training Loss: 2.2387800216674805 \n",
      "     Training Step: 30 Training Loss: 2.4110424518585205 \n",
      "     Training Step: 31 Training Loss: 3.185530185699463 \n",
      "     Training Step: 32 Training Loss: 2.1952123641967773 \n",
      "     Training Step: 33 Training Loss: 2.400541305541992 \n",
      "     Training Step: 34 Training Loss: 4.086997985839844 \n",
      "     Training Step: 35 Training Loss: 3.673971176147461 \n",
      "     Training Step: 36 Training Loss: 2.960618734359741 \n",
      "     Training Step: 37 Training Loss: 3.8210291862487793 \n",
      "     Training Step: 38 Training Loss: 3.9791665077209473 \n",
      "     Training Step: 39 Training Loss: 2.6183812618255615 \n",
      "     Training Step: 40 Training Loss: 2.4447174072265625 \n",
      "     Training Step: 41 Training Loss: 3.168586492538452 \n",
      "     Training Step: 42 Training Loss: 3.612621307373047 \n",
      "     Training Step: 43 Training Loss: 3.4541077613830566 \n",
      "     Training Step: 44 Training Loss: 2.4829816818237305 \n",
      "     Training Step: 45 Training Loss: 2.6817357540130615 \n",
      "     Training Step: 46 Training Loss: 3.0894904136657715 \n",
      "     Training Step: 47 Training Loss: 3.358582019805908 \n",
      "     Training Step: 48 Training Loss: 2.8841171264648438 \n",
      "     Training Step: 49 Training Loss: 3.586322784423828 \n",
      "     Training Step: 50 Training Loss: 2.7681946754455566 \n",
      "     Training Step: 51 Training Loss: 3.0360727310180664 \n",
      "     Training Step: 52 Training Loss: 4.32019567489624 \n",
      "     Training Step: 53 Training Loss: 3.21964168548584 \n",
      "     Training Step: 54 Training Loss: 2.850090742111206 \n",
      "     Training Step: 55 Training Loss: 2.6580073833465576 \n",
      "     Training Step: 56 Training Loss: 3.265004873275757 \n",
      "     Training Step: 57 Training Loss: 2.2537033557891846 \n",
      "     Training Step: 58 Training Loss: 2.4816579818725586 \n",
      "     Training Step: 59 Training Loss: 2.8058362007141113 \n",
      "     Training Step: 60 Training Loss: 2.818178653717041 \n",
      "     Training Step: 61 Training Loss: 3.720198631286621 \n",
      "     Training Step: 62 Training Loss: 3.2439093589782715 \n",
      "     Training Step: 63 Training Loss: 4.0763983726501465 \n",
      "     Training Step: 64 Training Loss: 2.6007275581359863 \n",
      "     Training Step: 65 Training Loss: 2.949584484100342 \n",
      "     Training Step: 66 Training Loss: 2.248359441757202 \n",
      "     Training Step: 67 Training Loss: 3.49746036529541 \n",
      "     Training Step: 68 Training Loss: 3.3443603515625 \n",
      "     Training Step: 69 Training Loss: 2.975252151489258 \n",
      "     Training Step: 70 Training Loss: 3.161510467529297 \n",
      "     Training Step: 71 Training Loss: 2.9075334072113037 \n",
      "     Training Step: 72 Training Loss: 2.583110809326172 \n",
      "     Training Step: 73 Training Loss: 2.39263916015625 \n",
      "     Training Step: 74 Training Loss: 3.6690282821655273 \n",
      "     Training Step: 75 Training Loss: 3.076624870300293 \n",
      "     Training Step: 76 Training Loss: 2.8659088611602783 \n",
      "     Training Step: 77 Training Loss: 2.7498583793640137 \n",
      "     Training Step: 78 Training Loss: 2.7592711448669434 \n",
      "     Training Step: 79 Training Loss: 3.0359745025634766 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.005242347717285 \n",
      "     Validation Step: 1 Validation Loss: 3.5008604526519775 \n",
      "     Validation Step: 2 Validation Loss: 3.0489578247070312 \n",
      "     Validation Step: 3 Validation Loss: 3.2635321617126465 \n",
      "     Validation Step: 4 Validation Loss: 2.842008113861084 \n",
      "     Validation Step: 5 Validation Loss: 3.3503973484039307 \n",
      "     Validation Step: 6 Validation Loss: 3.3997604846954346 \n",
      "     Validation Step: 7 Validation Loss: 3.541621685028076 \n",
      "     Validation Step: 8 Validation Loss: 3.517094612121582 \n",
      "     Validation Step: 9 Validation Loss: 2.663109302520752 \n",
      "     Validation Step: 10 Validation Loss: 3.084322214126587 \n",
      "     Validation Step: 11 Validation Loss: 3.2294657230377197 \n",
      "     Validation Step: 12 Validation Loss: 2.1533703804016113 \n",
      "     Validation Step: 13 Validation Loss: 3.8317036628723145 \n",
      "     Validation Step: 14 Validation Loss: 3.818826913833618 \n",
      "---------------   CURRENT LEARNING RATE: 8.000000000000001e-06   ---------------\n",
      "Epoch: 80\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 5.708128452301025 \n",
      "     Training Step: 1 Training Loss: 5.0616044998168945 \n",
      "     Training Step: 2 Training Loss: 6.351646423339844 \n",
      "     Training Step: 3 Training Loss: 7.297306060791016 \n",
      "     Training Step: 4 Training Loss: 4.279818534851074 \n",
      "     Training Step: 5 Training Loss: 3.6887454986572266 \n",
      "     Training Step: 6 Training Loss: 6.1683030128479 \n",
      "     Training Step: 7 Training Loss: 2.831423282623291 \n",
      "     Training Step: 8 Training Loss: 5.634777545928955 \n",
      "     Training Step: 9 Training Loss: 5.386972427368164 \n",
      "     Training Step: 10 Training Loss: 7.844235897064209 \n",
      "     Training Step: 11 Training Loss: 3.690275192260742 \n",
      "     Training Step: 12 Training Loss: 7.334329605102539 \n",
      "     Training Step: 13 Training Loss: 6.757225036621094 \n",
      "     Training Step: 14 Training Loss: 5.898207187652588 \n",
      "     Training Step: 15 Training Loss: 6.673759937286377 \n",
      "     Training Step: 16 Training Loss: 7.907372951507568 \n",
      "     Training Step: 17 Training Loss: 4.030007362365723 \n",
      "     Training Step: 18 Training Loss: 6.641307830810547 \n",
      "     Training Step: 19 Training Loss: 7.4419074058532715 \n",
      "     Training Step: 20 Training Loss: 4.1867475509643555 \n",
      "     Training Step: 21 Training Loss: 5.800421714782715 \n",
      "     Training Step: 22 Training Loss: 9.041643142700195 \n",
      "     Training Step: 23 Training Loss: 8.33658504486084 \n",
      "     Training Step: 24 Training Loss: 7.482402324676514 \n",
      "     Training Step: 25 Training Loss: 6.579489707946777 \n",
      "     Training Step: 26 Training Loss: 5.233977317810059 \n",
      "     Training Step: 27 Training Loss: 5.413455963134766 \n",
      "     Training Step: 28 Training Loss: 6.951939105987549 \n",
      "     Training Step: 29 Training Loss: 8.413948059082031 \n",
      "     Training Step: 30 Training Loss: 4.569578170776367 \n",
      "     Training Step: 31 Training Loss: 6.206502437591553 \n",
      "     Training Step: 32 Training Loss: 5.134590148925781 \n",
      "     Training Step: 33 Training Loss: 3.3431272506713867 \n",
      "     Training Step: 34 Training Loss: 5.097894668579102 \n",
      "     Training Step: 35 Training Loss: 2.6871330738067627 \n",
      "     Training Step: 36 Training Loss: 6.048884391784668 \n",
      "     Training Step: 37 Training Loss: 6.670421600341797 \n",
      "     Training Step: 38 Training Loss: 3.05116605758667 \n",
      "     Training Step: 39 Training Loss: 9.6887845993042 \n",
      "     Training Step: 40 Training Loss: 5.908835411071777 \n",
      "     Training Step: 41 Training Loss: 10.079747200012207 \n",
      "     Training Step: 42 Training Loss: 3.647167682647705 \n",
      "     Training Step: 43 Training Loss: 6.850049018859863 \n",
      "     Training Step: 44 Training Loss: 11.411752700805664 \n",
      "     Training Step: 45 Training Loss: 11.921300888061523 \n",
      "     Training Step: 46 Training Loss: 13.086312294006348 \n",
      "     Training Step: 47 Training Loss: 4.610090255737305 \n",
      "     Training Step: 48 Training Loss: 4.974365711212158 \n",
      "     Training Step: 49 Training Loss: 5.103796482086182 \n",
      "     Training Step: 50 Training Loss: 5.729461193084717 \n",
      "     Training Step: 51 Training Loss: 6.642345905303955 \n",
      "     Training Step: 52 Training Loss: 3.602734327316284 \n",
      "     Training Step: 53 Training Loss: 8.085168838500977 \n",
      "     Training Step: 54 Training Loss: 5.44284725189209 \n",
      "     Training Step: 55 Training Loss: 5.706419944763184 \n",
      "     Training Step: 56 Training Loss: 8.380205154418945 \n",
      "     Training Step: 57 Training Loss: 6.340789794921875 \n",
      "     Training Step: 58 Training Loss: 3.7628843784332275 \n",
      "     Training Step: 59 Training Loss: 9.587419509887695 \n",
      "     Training Step: 60 Training Loss: 5.441897392272949 \n",
      "     Training Step: 61 Training Loss: 4.853376388549805 \n",
      "     Training Step: 62 Training Loss: 5.480179786682129 \n",
      "     Training Step: 63 Training Loss: 4.741135597229004 \n",
      "     Training Step: 64 Training Loss: 11.849407196044922 \n",
      "     Training Step: 65 Training Loss: 6.118941307067871 \n",
      "     Training Step: 66 Training Loss: 5.6376800537109375 \n",
      "     Training Step: 67 Training Loss: 6.668901443481445 \n",
      "     Training Step: 68 Training Loss: 5.393822193145752 \n",
      "     Training Step: 69 Training Loss: 7.164230823516846 \n",
      "     Training Step: 70 Training Loss: 4.940639972686768 \n",
      "     Training Step: 71 Training Loss: 5.406057834625244 \n",
      "     Training Step: 72 Training Loss: 6.0453972816467285 \n",
      "     Training Step: 73 Training Loss: 5.252697944641113 \n",
      "     Training Step: 74 Training Loss: 6.14147424697876 \n",
      "     Training Step: 75 Training Loss: 8.027912139892578 \n",
      "     Training Step: 76 Training Loss: 6.9921464920043945 \n",
      "     Training Step: 77 Training Loss: 4.940079212188721 \n",
      "     Training Step: 78 Training Loss: 7.112635135650635 \n",
      "     Training Step: 79 Training Loss: 5.864377021789551 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 8.939371109008789 \n",
      "     Validation Step: 1 Validation Loss: 6.873490333557129 \n",
      "     Validation Step: 2 Validation Loss: 3.310596466064453 \n",
      "     Validation Step: 3 Validation Loss: 5.768277168273926 \n",
      "     Validation Step: 4 Validation Loss: 5.6228928565979 \n",
      "     Validation Step: 5 Validation Loss: 6.3052778244018555 \n",
      "     Validation Step: 6 Validation Loss: 10.116703033447266 \n",
      "     Validation Step: 7 Validation Loss: 8.134273529052734 \n",
      "     Validation Step: 8 Validation Loss: 6.651108741760254 \n",
      "     Validation Step: 9 Validation Loss: 5.687870025634766 \n",
      "     Validation Step: 10 Validation Loss: 6.628545761108398 \n",
      "     Validation Step: 11 Validation Loss: 8.442007064819336 \n",
      "     Validation Step: 12 Validation Loss: 8.13235092163086 \n",
      "     Validation Step: 13 Validation Loss: 6.307669639587402 \n",
      "     Validation Step: 14 Validation Loss: 6.4690728187561035 \n",
      "Epoch: 81\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.086559772491455 \n",
      "     Training Step: 1 Training Loss: 2.6032207012176514 \n",
      "     Training Step: 2 Training Loss: 2.8844566345214844 \n",
      "     Training Step: 3 Training Loss: 2.698699951171875 \n",
      "     Training Step: 4 Training Loss: 2.7250454425811768 \n",
      "     Training Step: 5 Training Loss: 2.3071517944335938 \n",
      "     Training Step: 6 Training Loss: 3.4518611431121826 \n",
      "     Training Step: 7 Training Loss: 2.3750720024108887 \n",
      "     Training Step: 8 Training Loss: 2.770284414291382 \n",
      "     Training Step: 9 Training Loss: 3.2188706398010254 \n",
      "     Training Step: 10 Training Loss: 2.6442368030548096 \n",
      "     Training Step: 11 Training Loss: 3.5284266471862793 \n",
      "     Training Step: 12 Training Loss: 2.63737154006958 \n",
      "     Training Step: 13 Training Loss: 2.839735269546509 \n",
      "     Training Step: 14 Training Loss: 3.8272385597229004 \n",
      "     Training Step: 15 Training Loss: 3.2846412658691406 \n",
      "     Training Step: 16 Training Loss: 4.690441131591797 \n",
      "     Training Step: 17 Training Loss: 2.752335548400879 \n",
      "     Training Step: 18 Training Loss: 2.7858242988586426 \n",
      "     Training Step: 19 Training Loss: 2.4743103981018066 \n",
      "     Training Step: 20 Training Loss: 3.0812809467315674 \n",
      "     Training Step: 21 Training Loss: 2.8036718368530273 \n",
      "     Training Step: 22 Training Loss: 3.13495135307312 \n",
      "     Training Step: 23 Training Loss: 2.781329870223999 \n",
      "     Training Step: 24 Training Loss: 2.109100580215454 \n",
      "     Training Step: 25 Training Loss: 2.607881784439087 \n",
      "     Training Step: 26 Training Loss: 2.3975229263305664 \n",
      "     Training Step: 27 Training Loss: 3.647634506225586 \n",
      "     Training Step: 28 Training Loss: 2.5144546031951904 \n",
      "     Training Step: 29 Training Loss: 2.6480438709259033 \n",
      "     Training Step: 30 Training Loss: 3.8623335361480713 \n",
      "     Training Step: 31 Training Loss: 2.264103412628174 \n",
      "     Training Step: 32 Training Loss: 2.8324875831604004 \n",
      "     Training Step: 33 Training Loss: 2.837465286254883 \n",
      "     Training Step: 34 Training Loss: 3.1614983081817627 \n",
      "     Training Step: 35 Training Loss: 2.7472715377807617 \n",
      "     Training Step: 36 Training Loss: 2.4263548851013184 \n",
      "     Training Step: 37 Training Loss: 2.1645007133483887 \n",
      "     Training Step: 38 Training Loss: 3.49346923828125 \n",
      "     Training Step: 39 Training Loss: 3.5307531356811523 \n",
      "     Training Step: 40 Training Loss: 3.1451799869537354 \n",
      "     Training Step: 41 Training Loss: 2.6909878253936768 \n",
      "     Training Step: 42 Training Loss: 2.428027868270874 \n",
      "     Training Step: 43 Training Loss: 2.4473180770874023 \n",
      "     Training Step: 44 Training Loss: 3.4027352333068848 \n",
      "     Training Step: 45 Training Loss: 2.6311416625976562 \n",
      "     Training Step: 46 Training Loss: 2.7158641815185547 \n",
      "     Training Step: 47 Training Loss: 2.9303901195526123 \n",
      "     Training Step: 48 Training Loss: 3.2809700965881348 \n",
      "     Training Step: 49 Training Loss: 2.973729372024536 \n",
      "     Training Step: 50 Training Loss: 3.5920233726501465 \n",
      "     Training Step: 51 Training Loss: 2.9725379943847656 \n",
      "     Training Step: 52 Training Loss: 3.703645706176758 \n",
      "     Training Step: 53 Training Loss: 3.2726693153381348 \n",
      "     Training Step: 54 Training Loss: 3.8442864418029785 \n",
      "     Training Step: 55 Training Loss: 3.673194169998169 \n",
      "     Training Step: 56 Training Loss: 2.7781097888946533 \n",
      "     Training Step: 57 Training Loss: 2.578400135040283 \n",
      "     Training Step: 58 Training Loss: 3.0522279739379883 \n",
      "     Training Step: 59 Training Loss: 3.805231809616089 \n",
      "     Training Step: 60 Training Loss: 3.233771562576294 \n",
      "     Training Step: 61 Training Loss: 2.675901174545288 \n",
      "     Training Step: 62 Training Loss: 2.3373446464538574 \n",
      "     Training Step: 63 Training Loss: 3.0575108528137207 \n",
      "     Training Step: 64 Training Loss: 3.3774733543395996 \n",
      "     Training Step: 65 Training Loss: 3.04549503326416 \n",
      "     Training Step: 66 Training Loss: 2.2598183155059814 \n",
      "     Training Step: 67 Training Loss: 2.3789844512939453 \n",
      "     Training Step: 68 Training Loss: 3.669931411743164 \n",
      "     Training Step: 69 Training Loss: 2.9435977935791016 \n",
      "     Training Step: 70 Training Loss: 3.9075326919555664 \n",
      "     Training Step: 71 Training Loss: 2.544290781021118 \n",
      "     Training Step: 72 Training Loss: 3.448119878768921 \n",
      "     Training Step: 73 Training Loss: 2.0231308937072754 \n",
      "     Training Step: 74 Training Loss: 3.095055103302002 \n",
      "     Training Step: 75 Training Loss: 3.3098113536834717 \n",
      "     Training Step: 76 Training Loss: 3.5548038482666016 \n",
      "     Training Step: 77 Training Loss: 3.0559029579162598 \n",
      "     Training Step: 78 Training Loss: 2.4447126388549805 \n",
      "     Training Step: 79 Training Loss: 2.5189476013183594 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9866862297058105 \n",
      "     Validation Step: 1 Validation Loss: 3.010317325592041 \n",
      "     Validation Step: 2 Validation Loss: 2.7734875679016113 \n",
      "     Validation Step: 3 Validation Loss: 2.998879909515381 \n",
      "     Validation Step: 4 Validation Loss: 2.9110989570617676 \n",
      "     Validation Step: 5 Validation Loss: 3.375516653060913 \n",
      "     Validation Step: 6 Validation Loss: 2.2739477157592773 \n",
      "     Validation Step: 7 Validation Loss: 3.079072952270508 \n",
      "     Validation Step: 8 Validation Loss: 3.5197818279266357 \n",
      "     Validation Step: 9 Validation Loss: 2.6084208488464355 \n",
      "     Validation Step: 10 Validation Loss: 3.7785234451293945 \n",
      "     Validation Step: 11 Validation Loss: 2.6990315914154053 \n",
      "     Validation Step: 12 Validation Loss: 3.5027804374694824 \n",
      "     Validation Step: 13 Validation Loss: 3.5302255153656006 \n",
      "     Validation Step: 14 Validation Loss: 3.5408215522766113 \n",
      "Epoch: 82\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.217957019805908 \n",
      "     Training Step: 1 Training Loss: 2.6543519496917725 \n",
      "     Training Step: 2 Training Loss: 3.4251625537872314 \n",
      "     Training Step: 3 Training Loss: 2.886777877807617 \n",
      "     Training Step: 4 Training Loss: 2.600367546081543 \n",
      "     Training Step: 5 Training Loss: 3.57438325881958 \n",
      "     Training Step: 6 Training Loss: 2.376802444458008 \n",
      "     Training Step: 7 Training Loss: 2.7816805839538574 \n",
      "     Training Step: 8 Training Loss: 2.3087077140808105 \n",
      "     Training Step: 9 Training Loss: 3.134568452835083 \n",
      "     Training Step: 10 Training Loss: 3.2983570098876953 \n",
      "     Training Step: 11 Training Loss: 2.8590731620788574 \n",
      "     Training Step: 12 Training Loss: 3.1525025367736816 \n",
      "     Training Step: 13 Training Loss: 3.601762294769287 \n",
      "     Training Step: 14 Training Loss: 2.2896034717559814 \n",
      "     Training Step: 15 Training Loss: 3.1149544715881348 \n",
      "     Training Step: 16 Training Loss: 2.7201621532440186 \n",
      "     Training Step: 17 Training Loss: 2.424678087234497 \n",
      "     Training Step: 18 Training Loss: 2.9927988052368164 \n",
      "     Training Step: 19 Training Loss: 4.754209518432617 \n",
      "     Training Step: 20 Training Loss: 2.5750064849853516 \n",
      "     Training Step: 21 Training Loss: 3.1400856971740723 \n",
      "     Training Step: 22 Training Loss: 4.397074222564697 \n",
      "     Training Step: 23 Training Loss: 2.547865390777588 \n",
      "     Training Step: 24 Training Loss: 2.745936632156372 \n",
      "     Training Step: 25 Training Loss: 2.810767650604248 \n",
      "     Training Step: 26 Training Loss: 2.640425682067871 \n",
      "     Training Step: 27 Training Loss: 3.2430970668792725 \n",
      "     Training Step: 28 Training Loss: 3.1366002559661865 \n",
      "     Training Step: 29 Training Loss: 2.4214515686035156 \n",
      "     Training Step: 30 Training Loss: 4.032630443572998 \n",
      "     Training Step: 31 Training Loss: 2.418978691101074 \n",
      "     Training Step: 32 Training Loss: 3.517320156097412 \n",
      "     Training Step: 33 Training Loss: 2.9707932472229004 \n",
      "     Training Step: 34 Training Loss: 2.8194613456726074 \n",
      "     Training Step: 35 Training Loss: 2.6662447452545166 \n",
      "     Training Step: 36 Training Loss: 2.8752946853637695 \n",
      "     Training Step: 37 Training Loss: 2.8364453315734863 \n",
      "     Training Step: 38 Training Loss: 3.0270256996154785 \n",
      "     Training Step: 39 Training Loss: 2.765998601913452 \n",
      "     Training Step: 40 Training Loss: 2.834052324295044 \n",
      "     Training Step: 41 Training Loss: 3.04887056350708 \n",
      "     Training Step: 42 Training Loss: 2.912754774093628 \n",
      "     Training Step: 43 Training Loss: 3.108745813369751 \n",
      "     Training Step: 44 Training Loss: 2.7887613773345947 \n",
      "     Training Step: 45 Training Loss: 3.0303235054016113 \n",
      "     Training Step: 46 Training Loss: 2.2843830585479736 \n",
      "     Training Step: 47 Training Loss: 3.3314671516418457 \n",
      "     Training Step: 48 Training Loss: 2.990368366241455 \n",
      "     Training Step: 49 Training Loss: 3.1822383403778076 \n",
      "     Training Step: 50 Training Loss: 2.045358657836914 \n",
      "     Training Step: 51 Training Loss: 4.184443950653076 \n",
      "     Training Step: 52 Training Loss: 3.2367660999298096 \n",
      "     Training Step: 53 Training Loss: 3.330759286880493 \n",
      "     Training Step: 54 Training Loss: 2.103499412536621 \n",
      "     Training Step: 55 Training Loss: 2.2948684692382812 \n",
      "     Training Step: 56 Training Loss: 3.757002115249634 \n",
      "     Training Step: 57 Training Loss: 3.0203380584716797 \n",
      "     Training Step: 58 Training Loss: 2.047933578491211 \n",
      "     Training Step: 59 Training Loss: 3.7297606468200684 \n",
      "     Training Step: 60 Training Loss: 2.7907397747039795 \n",
      "     Training Step: 61 Training Loss: 2.712275266647339 \n",
      "     Training Step: 62 Training Loss: 2.7345776557922363 \n",
      "     Training Step: 63 Training Loss: 2.125119209289551 \n",
      "     Training Step: 64 Training Loss: 2.245864152908325 \n",
      "     Training Step: 65 Training Loss: 3.395695686340332 \n",
      "     Training Step: 66 Training Loss: 2.6921215057373047 \n",
      "     Training Step: 67 Training Loss: 2.9338831901550293 \n",
      "     Training Step: 68 Training Loss: 2.6114554405212402 \n",
      "     Training Step: 69 Training Loss: 2.3860597610473633 \n",
      "     Training Step: 70 Training Loss: 2.211223602294922 \n",
      "     Training Step: 71 Training Loss: 3.20210862159729 \n",
      "     Training Step: 72 Training Loss: 3.1447858810424805 \n",
      "     Training Step: 73 Training Loss: 2.5607175827026367 \n",
      "     Training Step: 74 Training Loss: 2.629347562789917 \n",
      "     Training Step: 75 Training Loss: 3.0870532989501953 \n",
      "     Training Step: 76 Training Loss: 4.2584733963012695 \n",
      "     Training Step: 77 Training Loss: 3.7181615829467773 \n",
      "     Training Step: 78 Training Loss: 3.140690326690674 \n",
      "     Training Step: 79 Training Loss: 2.3366973400115967 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.4476161003112793 \n",
      "     Validation Step: 1 Validation Loss: 3.675938129425049 \n",
      "     Validation Step: 2 Validation Loss: 2.745270252227783 \n",
      "     Validation Step: 3 Validation Loss: 2.347513198852539 \n",
      "     Validation Step: 4 Validation Loss: 2.925234794616699 \n",
      "     Validation Step: 5 Validation Loss: 3.4181244373321533 \n",
      "     Validation Step: 6 Validation Loss: 2.5165815353393555 \n",
      "     Validation Step: 7 Validation Loss: 2.8816349506378174 \n",
      "     Validation Step: 8 Validation Loss: 3.4058310985565186 \n",
      "     Validation Step: 9 Validation Loss: 3.538769006729126 \n",
      "     Validation Step: 10 Validation Loss: 3.2505245208740234 \n",
      "     Validation Step: 11 Validation Loss: 2.6454224586486816 \n",
      "     Validation Step: 12 Validation Loss: 3.7935171127319336 \n",
      "     Validation Step: 13 Validation Loss: 3.0302605628967285 \n",
      "     Validation Step: 14 Validation Loss: 2.9759395122528076 \n",
      "Epoch: 83\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.518609046936035 \n",
      "     Training Step: 1 Training Loss: 2.9087748527526855 \n",
      "     Training Step: 2 Training Loss: 3.41182804107666 \n",
      "     Training Step: 3 Training Loss: 4.675167560577393 \n",
      "     Training Step: 4 Training Loss: 3.3233213424682617 \n",
      "     Training Step: 5 Training Loss: 3.438235282897949 \n",
      "     Training Step: 6 Training Loss: 2.9404735565185547 \n",
      "     Training Step: 7 Training Loss: 2.5120959281921387 \n",
      "     Training Step: 8 Training Loss: 2.9856019020080566 \n",
      "     Training Step: 9 Training Loss: 2.370168924331665 \n",
      "     Training Step: 10 Training Loss: 2.218284845352173 \n",
      "     Training Step: 11 Training Loss: 2.7955238819122314 \n",
      "     Training Step: 12 Training Loss: 3.5405614376068115 \n",
      "     Training Step: 13 Training Loss: 2.8733768463134766 \n",
      "     Training Step: 14 Training Loss: 2.5358452796936035 \n",
      "     Training Step: 15 Training Loss: 2.9243569374084473 \n",
      "     Training Step: 16 Training Loss: 3.2161359786987305 \n",
      "     Training Step: 17 Training Loss: 3.103351354598999 \n",
      "     Training Step: 18 Training Loss: 2.2321441173553467 \n",
      "     Training Step: 19 Training Loss: 2.4087255001068115 \n",
      "     Training Step: 20 Training Loss: 3.2018496990203857 \n",
      "     Training Step: 21 Training Loss: 3.39274263381958 \n",
      "     Training Step: 22 Training Loss: 2.2617015838623047 \n",
      "     Training Step: 23 Training Loss: 2.1513142585754395 \n",
      "     Training Step: 24 Training Loss: 3.0081608295440674 \n",
      "     Training Step: 25 Training Loss: 2.720475673675537 \n",
      "     Training Step: 26 Training Loss: 3.517817258834839 \n",
      "     Training Step: 27 Training Loss: 2.9935760498046875 \n",
      "     Training Step: 28 Training Loss: 3.7686927318573 \n",
      "     Training Step: 29 Training Loss: 3.5994808673858643 \n",
      "     Training Step: 30 Training Loss: 3.3750782012939453 \n",
      "     Training Step: 31 Training Loss: 2.12869930267334 \n",
      "     Training Step: 32 Training Loss: 3.1339540481567383 \n",
      "     Training Step: 33 Training Loss: 3.135340690612793 \n",
      "     Training Step: 34 Training Loss: 2.5547561645507812 \n",
      "     Training Step: 35 Training Loss: 2.2397708892822266 \n",
      "     Training Step: 36 Training Loss: 3.053311347961426 \n",
      "     Training Step: 37 Training Loss: 2.3429341316223145 \n",
      "     Training Step: 38 Training Loss: 3.7658936977386475 \n",
      "     Training Step: 39 Training Loss: 4.331502437591553 \n",
      "     Training Step: 40 Training Loss: 2.254896402359009 \n",
      "     Training Step: 41 Training Loss: 2.709838390350342 \n",
      "     Training Step: 42 Training Loss: 2.5059914588928223 \n",
      "     Training Step: 43 Training Loss: 3.2229342460632324 \n",
      "     Training Step: 44 Training Loss: 3.745121955871582 \n",
      "     Training Step: 45 Training Loss: 2.1831231117248535 \n",
      "     Training Step: 46 Training Loss: 2.887815475463867 \n",
      "     Training Step: 47 Training Loss: 4.226601600646973 \n",
      "     Training Step: 48 Training Loss: 3.3141632080078125 \n",
      "     Training Step: 49 Training Loss: 3.6704258918762207 \n",
      "     Training Step: 50 Training Loss: 2.963486671447754 \n",
      "     Training Step: 51 Training Loss: 2.350727081298828 \n",
      "     Training Step: 52 Training Loss: 3.695646286010742 \n",
      "     Training Step: 53 Training Loss: 4.081585884094238 \n",
      "     Training Step: 54 Training Loss: 3.0897555351257324 \n",
      "     Training Step: 55 Training Loss: 3.2786262035369873 \n",
      "     Training Step: 56 Training Loss: 3.239119052886963 \n",
      "     Training Step: 57 Training Loss: 3.447690486907959 \n",
      "     Training Step: 58 Training Loss: 2.864189624786377 \n",
      "     Training Step: 59 Training Loss: 2.933520555496216 \n",
      "     Training Step: 60 Training Loss: 3.6089649200439453 \n",
      "     Training Step: 61 Training Loss: 3.22349214553833 \n",
      "     Training Step: 62 Training Loss: 2.2455716133117676 \n",
      "     Training Step: 63 Training Loss: 2.6375536918640137 \n",
      "     Training Step: 64 Training Loss: 2.5401315689086914 \n",
      "     Training Step: 65 Training Loss: 2.789238929748535 \n",
      "     Training Step: 66 Training Loss: 2.5678601264953613 \n",
      "     Training Step: 67 Training Loss: 2.467031478881836 \n",
      "     Training Step: 68 Training Loss: 2.539113998413086 \n",
      "     Training Step: 69 Training Loss: 3.1953444480895996 \n",
      "     Training Step: 70 Training Loss: 2.7810111045837402 \n",
      "     Training Step: 71 Training Loss: 2.4906094074249268 \n",
      "     Training Step: 72 Training Loss: 3.5963706970214844 \n",
      "     Training Step: 73 Training Loss: 2.8969216346740723 \n",
      "     Training Step: 74 Training Loss: 2.895810127258301 \n",
      "     Training Step: 75 Training Loss: 3.012211322784424 \n",
      "     Training Step: 76 Training Loss: 4.113494873046875 \n",
      "     Training Step: 77 Training Loss: 3.611851692199707 \n",
      "     Training Step: 78 Training Loss: 3.2607884407043457 \n",
      "     Training Step: 79 Training Loss: 3.1315221786499023 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.439946174621582 \n",
      "     Validation Step: 1 Validation Loss: 2.936877727508545 \n",
      "     Validation Step: 2 Validation Loss: 2.6656341552734375 \n",
      "     Validation Step: 3 Validation Loss: 3.1389780044555664 \n",
      "     Validation Step: 4 Validation Loss: 2.536752462387085 \n",
      "     Validation Step: 5 Validation Loss: 3.8935165405273438 \n",
      "     Validation Step: 6 Validation Loss: 3.6038947105407715 \n",
      "     Validation Step: 7 Validation Loss: 3.316214084625244 \n",
      "     Validation Step: 8 Validation Loss: 3.517888307571411 \n",
      "     Validation Step: 9 Validation Loss: 2.9546091556549072 \n",
      "     Validation Step: 10 Validation Loss: 2.4665098190307617 \n",
      "     Validation Step: 11 Validation Loss: 3.813391923904419 \n",
      "     Validation Step: 12 Validation Loss: 2.8785815238952637 \n",
      "     Validation Step: 13 Validation Loss: 2.4916083812713623 \n",
      "     Validation Step: 14 Validation Loss: 3.4663760662078857 \n",
      "Epoch: 84\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.38728404045105 \n",
      "     Training Step: 1 Training Loss: 3.1741981506347656 \n",
      "     Training Step: 2 Training Loss: 3.525338888168335 \n",
      "     Training Step: 3 Training Loss: 2.6523959636688232 \n",
      "     Training Step: 4 Training Loss: 3.225336790084839 \n",
      "     Training Step: 5 Training Loss: 2.076201915740967 \n",
      "     Training Step: 6 Training Loss: 3.128523826599121 \n",
      "     Training Step: 7 Training Loss: 3.2845053672790527 \n",
      "     Training Step: 8 Training Loss: 3.1131067276000977 \n",
      "     Training Step: 9 Training Loss: 2.1884140968322754 \n",
      "     Training Step: 10 Training Loss: 2.7116241455078125 \n",
      "     Training Step: 11 Training Loss: 3.16505765914917 \n",
      "     Training Step: 12 Training Loss: 3.5473155975341797 \n",
      "     Training Step: 13 Training Loss: 2.7740306854248047 \n",
      "     Training Step: 14 Training Loss: 3.502828598022461 \n",
      "     Training Step: 15 Training Loss: 3.6237149238586426 \n",
      "     Training Step: 16 Training Loss: 4.013299465179443 \n",
      "     Training Step: 17 Training Loss: 3.3610262870788574 \n",
      "     Training Step: 18 Training Loss: 3.0156168937683105 \n",
      "     Training Step: 19 Training Loss: 4.172622203826904 \n",
      "     Training Step: 20 Training Loss: 2.9073867797851562 \n",
      "     Training Step: 21 Training Loss: 3.851027011871338 \n",
      "     Training Step: 22 Training Loss: 3.321139097213745 \n",
      "     Training Step: 23 Training Loss: 2.4062411785125732 \n",
      "     Training Step: 24 Training Loss: 3.21982479095459 \n",
      "     Training Step: 25 Training Loss: 2.771190881729126 \n",
      "     Training Step: 26 Training Loss: 2.4972565174102783 \n",
      "     Training Step: 27 Training Loss: 3.5654966831207275 \n",
      "     Training Step: 28 Training Loss: 3.160788059234619 \n",
      "     Training Step: 29 Training Loss: 3.2022571563720703 \n",
      "     Training Step: 30 Training Loss: 3.5427823066711426 \n",
      "     Training Step: 31 Training Loss: 2.2561752796173096 \n",
      "     Training Step: 32 Training Loss: 2.3752987384796143 \n",
      "     Training Step: 33 Training Loss: 3.6131303310394287 \n",
      "     Training Step: 34 Training Loss: 2.9422290325164795 \n",
      "     Training Step: 35 Training Loss: 2.941422462463379 \n",
      "     Training Step: 36 Training Loss: 3.2885079383850098 \n",
      "     Training Step: 37 Training Loss: 2.4878501892089844 \n",
      "     Training Step: 38 Training Loss: 3.153085231781006 \n",
      "     Training Step: 39 Training Loss: 2.8327066898345947 \n",
      "     Training Step: 40 Training Loss: 2.9410758018493652 \n",
      "     Training Step: 41 Training Loss: 2.4702653884887695 \n",
      "     Training Step: 42 Training Loss: 2.417506694793701 \n",
      "     Training Step: 43 Training Loss: 3.3215763568878174 \n",
      "     Training Step: 44 Training Loss: 2.4295270442962646 \n",
      "     Training Step: 45 Training Loss: 2.853627920150757 \n",
      "     Training Step: 46 Training Loss: 2.917360305786133 \n",
      "     Training Step: 47 Training Loss: 3.3668341636657715 \n",
      "     Training Step: 48 Training Loss: 2.6600341796875 \n",
      "     Training Step: 49 Training Loss: 3.193892240524292 \n",
      "     Training Step: 50 Training Loss: 2.3059463500976562 \n",
      "     Training Step: 51 Training Loss: 4.245398998260498 \n",
      "     Training Step: 52 Training Loss: 2.4777026176452637 \n",
      "     Training Step: 53 Training Loss: 2.1574440002441406 \n",
      "     Training Step: 54 Training Loss: 2.2494468688964844 \n",
      "     Training Step: 55 Training Loss: 2.0846996307373047 \n",
      "     Training Step: 56 Training Loss: 3.187422752380371 \n",
      "     Training Step: 57 Training Loss: 2.8199336528778076 \n",
      "     Training Step: 58 Training Loss: 3.23756742477417 \n",
      "     Training Step: 59 Training Loss: 2.454979419708252 \n",
      "     Training Step: 60 Training Loss: 2.125231981277466 \n",
      "     Training Step: 61 Training Loss: 2.5891623497009277 \n",
      "     Training Step: 62 Training Loss: 3.268108367919922 \n",
      "     Training Step: 63 Training Loss: 2.814229726791382 \n",
      "     Training Step: 64 Training Loss: 2.5525217056274414 \n",
      "     Training Step: 65 Training Loss: 3.0107879638671875 \n",
      "     Training Step: 66 Training Loss: 3.839345932006836 \n",
      "     Training Step: 67 Training Loss: 3.194847583770752 \n",
      "     Training Step: 68 Training Loss: 2.6320526599884033 \n",
      "     Training Step: 69 Training Loss: 4.349871635437012 \n",
      "     Training Step: 70 Training Loss: 2.3838508129119873 \n",
      "     Training Step: 71 Training Loss: 2.572817802429199 \n",
      "     Training Step: 72 Training Loss: 2.3561010360717773 \n",
      "     Training Step: 73 Training Loss: 2.1318366527557373 \n",
      "     Training Step: 74 Training Loss: 2.886932373046875 \n",
      "     Training Step: 75 Training Loss: 4.665594100952148 \n",
      "     Training Step: 76 Training Loss: 3.535118341445923 \n",
      "     Training Step: 77 Training Loss: 3.0864248275756836 \n",
      "     Training Step: 78 Training Loss: 3.1318140029907227 \n",
      "     Training Step: 79 Training Loss: 2.2319788932800293 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9864466190338135 \n",
      "     Validation Step: 1 Validation Loss: 3.5210254192352295 \n",
      "     Validation Step: 2 Validation Loss: 2.8180460929870605 \n",
      "     Validation Step: 3 Validation Loss: 3.3701045513153076 \n",
      "     Validation Step: 4 Validation Loss: 3.0676605701446533 \n",
      "     Validation Step: 5 Validation Loss: 3.3449041843414307 \n",
      "     Validation Step: 6 Validation Loss: 3.5838003158569336 \n",
      "     Validation Step: 7 Validation Loss: 2.294213056564331 \n",
      "     Validation Step: 8 Validation Loss: 3.8158645629882812 \n",
      "     Validation Step: 9 Validation Loss: 3.532930612564087 \n",
      "     Validation Step: 10 Validation Loss: 2.6032323837280273 \n",
      "     Validation Step: 11 Validation Loss: 2.7069320678710938 \n",
      "     Validation Step: 12 Validation Loss: 3.0803539752960205 \n",
      "     Validation Step: 13 Validation Loss: 3.1353132724761963 \n",
      "     Validation Step: 14 Validation Loss: 3.649554967880249 \n",
      "Epoch: 85\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.091872215270996 \n",
      "     Training Step: 1 Training Loss: 3.25300931930542 \n",
      "     Training Step: 2 Training Loss: 2.8835887908935547 \n",
      "     Training Step: 3 Training Loss: 3.021296977996826 \n",
      "     Training Step: 4 Training Loss: 2.9260478019714355 \n",
      "     Training Step: 5 Training Loss: 2.6895155906677246 \n",
      "     Training Step: 6 Training Loss: 2.974738597869873 \n",
      "     Training Step: 7 Training Loss: 2.4053540229797363 \n",
      "     Training Step: 8 Training Loss: 2.801892042160034 \n",
      "     Training Step: 9 Training Loss: 3.390512466430664 \n",
      "     Training Step: 10 Training Loss: 3.3344790935516357 \n",
      "     Training Step: 11 Training Loss: 3.656019926071167 \n",
      "     Training Step: 12 Training Loss: 2.8586156368255615 \n",
      "     Training Step: 13 Training Loss: 3.5995054244995117 \n",
      "     Training Step: 14 Training Loss: 2.303649425506592 \n",
      "     Training Step: 15 Training Loss: 2.5525312423706055 \n",
      "     Training Step: 16 Training Loss: 3.1913931369781494 \n",
      "     Training Step: 17 Training Loss: 3.205334186553955 \n",
      "     Training Step: 18 Training Loss: 3.1050515174865723 \n",
      "     Training Step: 19 Training Loss: 2.176088809967041 \n",
      "     Training Step: 20 Training Loss: 3.4001386165618896 \n",
      "     Training Step: 21 Training Loss: 2.8457069396972656 \n",
      "     Training Step: 22 Training Loss: 3.272106409072876 \n",
      "     Training Step: 23 Training Loss: 3.3476076126098633 \n",
      "     Training Step: 24 Training Loss: 2.138439655303955 \n",
      "     Training Step: 25 Training Loss: 2.7616934776306152 \n",
      "     Training Step: 26 Training Loss: 3.775212526321411 \n",
      "     Training Step: 27 Training Loss: 3.5504112243652344 \n",
      "     Training Step: 28 Training Loss: 2.7775216102600098 \n",
      "     Training Step: 29 Training Loss: 3.544908046722412 \n",
      "     Training Step: 30 Training Loss: 2.067155599594116 \n",
      "     Training Step: 31 Training Loss: 3.7705116271972656 \n",
      "     Training Step: 32 Training Loss: 2.546492576599121 \n",
      "     Training Step: 33 Training Loss: 3.827967882156372 \n",
      "     Training Step: 34 Training Loss: 3.3220973014831543 \n",
      "     Training Step: 35 Training Loss: 2.781067371368408 \n",
      "     Training Step: 36 Training Loss: 2.9459569454193115 \n",
      "     Training Step: 37 Training Loss: 2.547894239425659 \n",
      "     Training Step: 38 Training Loss: 2.3619585037231445 \n",
      "     Training Step: 39 Training Loss: 4.388194561004639 \n",
      "     Training Step: 40 Training Loss: 2.278374671936035 \n",
      "     Training Step: 41 Training Loss: 3.351933479309082 \n",
      "     Training Step: 42 Training Loss: 3.153779983520508 \n",
      "     Training Step: 43 Training Loss: 3.189206600189209 \n",
      "     Training Step: 44 Training Loss: 3.263881206512451 \n",
      "     Training Step: 45 Training Loss: 4.306209564208984 \n",
      "     Training Step: 46 Training Loss: 3.147155284881592 \n",
      "     Training Step: 47 Training Loss: 3.7056961059570312 \n",
      "     Training Step: 48 Training Loss: 2.654726505279541 \n",
      "     Training Step: 49 Training Loss: 2.4235119819641113 \n",
      "     Training Step: 50 Training Loss: 3.083383560180664 \n",
      "     Training Step: 51 Training Loss: 3.286625385284424 \n",
      "     Training Step: 52 Training Loss: 3.0780386924743652 \n",
      "     Training Step: 53 Training Loss: 2.5783023834228516 \n",
      "     Training Step: 54 Training Loss: 2.5459556579589844 \n",
      "     Training Step: 55 Training Loss: 3.735978603363037 \n",
      "     Training Step: 56 Training Loss: 2.315192699432373 \n",
      "     Training Step: 57 Training Loss: 2.8051376342773438 \n",
      "     Training Step: 58 Training Loss: 2.888334274291992 \n",
      "     Training Step: 59 Training Loss: 2.1620235443115234 \n",
      "     Training Step: 60 Training Loss: 3.591176748275757 \n",
      "     Training Step: 61 Training Loss: 2.6902503967285156 \n",
      "     Training Step: 62 Training Loss: 2.4199085235595703 \n",
      "     Training Step: 63 Training Loss: 2.9160618782043457 \n",
      "     Training Step: 64 Training Loss: 2.8181943893432617 \n",
      "     Training Step: 65 Training Loss: 3.022254228591919 \n",
      "     Training Step: 66 Training Loss: 2.708348512649536 \n",
      "     Training Step: 67 Training Loss: 4.193424701690674 \n",
      "     Training Step: 68 Training Loss: 3.676685333251953 \n",
      "     Training Step: 69 Training Loss: 4.676959991455078 \n",
      "     Training Step: 70 Training Loss: 2.546271562576294 \n",
      "     Training Step: 71 Training Loss: 3.9400644302368164 \n",
      "     Training Step: 72 Training Loss: 2.181351661682129 \n",
      "     Training Step: 73 Training Loss: 2.2509055137634277 \n",
      "     Training Step: 74 Training Loss: 3.3700947761535645 \n",
      "     Training Step: 75 Training Loss: 2.7915310859680176 \n",
      "     Training Step: 76 Training Loss: 2.5491061210632324 \n",
      "     Training Step: 77 Training Loss: 2.2379684448242188 \n",
      "     Training Step: 78 Training Loss: 2.5590994358062744 \n",
      "     Training Step: 79 Training Loss: 2.6066455841064453 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.612302303314209 \n",
      "     Validation Step: 1 Validation Loss: 2.795193672180176 \n",
      "     Validation Step: 2 Validation Loss: 2.9524128437042236 \n",
      "     Validation Step: 3 Validation Loss: 3.5017666816711426 \n",
      "     Validation Step: 4 Validation Loss: 2.337484836578369 \n",
      "     Validation Step: 5 Validation Loss: 3.173386573791504 \n",
      "     Validation Step: 6 Validation Loss: 3.136164903640747 \n",
      "     Validation Step: 7 Validation Loss: 3.7089147567749023 \n",
      "     Validation Step: 8 Validation Loss: 3.6887660026550293 \n",
      "     Validation Step: 9 Validation Loss: 2.7029404640197754 \n",
      "     Validation Step: 10 Validation Loss: 3.101780891418457 \n",
      "     Validation Step: 11 Validation Loss: 3.046189785003662 \n",
      "     Validation Step: 12 Validation Loss: 2.7491936683654785 \n",
      "     Validation Step: 13 Validation Loss: 3.95540189743042 \n",
      "     Validation Step: 14 Validation Loss: 3.5894627571105957 \n",
      "Epoch: 86\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.3255105018615723 \n",
      "     Training Step: 1 Training Loss: 2.8626692295074463 \n",
      "     Training Step: 2 Training Loss: 2.449577808380127 \n",
      "     Training Step: 3 Training Loss: 3.263604164123535 \n",
      "     Training Step: 4 Training Loss: 3.3550057411193848 \n",
      "     Training Step: 5 Training Loss: 4.2283501625061035 \n",
      "     Training Step: 6 Training Loss: 3.643430233001709 \n",
      "     Training Step: 7 Training Loss: 2.447567939758301 \n",
      "     Training Step: 8 Training Loss: 2.883234977722168 \n",
      "     Training Step: 9 Training Loss: 3.8454086780548096 \n",
      "     Training Step: 10 Training Loss: 3.17038631439209 \n",
      "     Training Step: 11 Training Loss: 2.3422203063964844 \n",
      "     Training Step: 12 Training Loss: 2.924044609069824 \n",
      "     Training Step: 13 Training Loss: 3.588984966278076 \n",
      "     Training Step: 14 Training Loss: 2.858341693878174 \n",
      "     Training Step: 15 Training Loss: 3.3558571338653564 \n",
      "     Training Step: 16 Training Loss: 2.449436664581299 \n",
      "     Training Step: 17 Training Loss: 4.433424949645996 \n",
      "     Training Step: 18 Training Loss: 3.5092239379882812 \n",
      "     Training Step: 19 Training Loss: 4.032950401306152 \n",
      "     Training Step: 20 Training Loss: 3.505547046661377 \n",
      "     Training Step: 21 Training Loss: 2.643428325653076 \n",
      "     Training Step: 22 Training Loss: 2.913130283355713 \n",
      "     Training Step: 23 Training Loss: 3.322554588317871 \n",
      "     Training Step: 24 Training Loss: 2.884974241256714 \n",
      "     Training Step: 25 Training Loss: 2.7687127590179443 \n",
      "     Training Step: 26 Training Loss: 4.0425944328308105 \n",
      "     Training Step: 27 Training Loss: 2.4328341484069824 \n",
      "     Training Step: 28 Training Loss: 3.041455030441284 \n",
      "     Training Step: 29 Training Loss: 3.113090991973877 \n",
      "     Training Step: 30 Training Loss: 2.858217239379883 \n",
      "     Training Step: 31 Training Loss: 3.334868907928467 \n",
      "     Training Step: 32 Training Loss: 2.5075924396514893 \n",
      "     Training Step: 33 Training Loss: 3.145880699157715 \n",
      "     Training Step: 34 Training Loss: 3.5430705547332764 \n",
      "     Training Step: 35 Training Loss: 3.5137081146240234 \n",
      "     Training Step: 36 Training Loss: 3.4810707569122314 \n",
      "     Training Step: 37 Training Loss: 2.511800527572632 \n",
      "     Training Step: 38 Training Loss: 3.595231056213379 \n",
      "     Training Step: 39 Training Loss: 2.719109058380127 \n",
      "     Training Step: 40 Training Loss: 3.149237632751465 \n",
      "     Training Step: 41 Training Loss: 3.068948268890381 \n",
      "     Training Step: 42 Training Loss: 2.5792593955993652 \n",
      "     Training Step: 43 Training Loss: 2.3914949893951416 \n",
      "     Training Step: 44 Training Loss: 2.5089831352233887 \n",
      "     Training Step: 45 Training Loss: 2.2132856845855713 \n",
      "     Training Step: 46 Training Loss: 3.3819360733032227 \n",
      "     Training Step: 47 Training Loss: 2.8015241622924805 \n",
      "     Training Step: 48 Training Loss: 2.283967971801758 \n",
      "     Training Step: 49 Training Loss: 3.3370397090911865 \n",
      "     Training Step: 50 Training Loss: 2.9781360626220703 \n",
      "     Training Step: 51 Training Loss: 2.3080296516418457 \n",
      "     Training Step: 52 Training Loss: 2.988764762878418 \n",
      "     Training Step: 53 Training Loss: 2.5692691802978516 \n",
      "     Training Step: 54 Training Loss: 3.040128469467163 \n",
      "     Training Step: 55 Training Loss: 2.251573085784912 \n",
      "     Training Step: 56 Training Loss: 2.6372179985046387 \n",
      "     Training Step: 57 Training Loss: 3.376347541809082 \n",
      "     Training Step: 58 Training Loss: 3.4681694507598877 \n",
      "     Training Step: 59 Training Loss: 2.470069646835327 \n",
      "     Training Step: 60 Training Loss: 3.342589855194092 \n",
      "     Training Step: 61 Training Loss: 3.219027042388916 \n",
      "     Training Step: 62 Training Loss: 3.6005196571350098 \n",
      "     Training Step: 63 Training Loss: 2.2954635620117188 \n",
      "     Training Step: 64 Training Loss: 3.387645721435547 \n",
      "     Training Step: 65 Training Loss: 2.3325748443603516 \n",
      "     Training Step: 66 Training Loss: 3.5368988513946533 \n",
      "     Training Step: 67 Training Loss: 2.144184112548828 \n",
      "     Training Step: 68 Training Loss: 2.263021945953369 \n",
      "     Training Step: 69 Training Loss: 2.449587345123291 \n",
      "     Training Step: 70 Training Loss: 2.385305166244507 \n",
      "     Training Step: 71 Training Loss: 2.727949619293213 \n",
      "     Training Step: 72 Training Loss: 4.239418029785156 \n",
      "     Training Step: 73 Training Loss: 2.1363537311553955 \n",
      "     Training Step: 74 Training Loss: 2.7418413162231445 \n",
      "     Training Step: 75 Training Loss: 2.5991499423980713 \n",
      "     Training Step: 76 Training Loss: 2.7097535133361816 \n",
      "     Training Step: 77 Training Loss: 2.7397823333740234 \n",
      "     Training Step: 78 Training Loss: 2.7124691009521484 \n",
      "     Training Step: 79 Training Loss: 2.2739005088806152 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.793488025665283 \n",
      "     Validation Step: 1 Validation Loss: 3.17032790184021 \n",
      "     Validation Step: 2 Validation Loss: 3.0180840492248535 \n",
      "     Validation Step: 3 Validation Loss: 3.6177215576171875 \n",
      "     Validation Step: 4 Validation Loss: 2.95013689994812 \n",
      "     Validation Step: 5 Validation Loss: 3.2886240482330322 \n",
      "     Validation Step: 6 Validation Loss: 2.2323057651519775 \n",
      "     Validation Step: 7 Validation Loss: 3.7893505096435547 \n",
      "     Validation Step: 8 Validation Loss: 4.018948554992676 \n",
      "     Validation Step: 9 Validation Loss: 3.5942862033843994 \n",
      "     Validation Step: 10 Validation Loss: 2.841087579727173 \n",
      "     Validation Step: 11 Validation Loss: 2.726402521133423 \n",
      "     Validation Step: 12 Validation Loss: 2.8361544609069824 \n",
      "     Validation Step: 13 Validation Loss: 3.2694146633148193 \n",
      "     Validation Step: 14 Validation Loss: 3.1145646572113037 \n",
      "Epoch: 87\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.7150373458862305 \n",
      "     Training Step: 1 Training Loss: 2.2198479175567627 \n",
      "     Training Step: 2 Training Loss: 3.1893959045410156 \n",
      "     Training Step: 3 Training Loss: 2.4008288383483887 \n",
      "     Training Step: 4 Training Loss: 3.3093252182006836 \n",
      "     Training Step: 5 Training Loss: 2.6786646842956543 \n",
      "     Training Step: 6 Training Loss: 2.8016884326934814 \n",
      "     Training Step: 7 Training Loss: 3.3465306758880615 \n",
      "     Training Step: 8 Training Loss: 3.2154245376586914 \n",
      "     Training Step: 9 Training Loss: 2.4015047550201416 \n",
      "     Training Step: 10 Training Loss: 3.138758420944214 \n",
      "     Training Step: 11 Training Loss: 3.151937484741211 \n",
      "     Training Step: 12 Training Loss: 2.5708117485046387 \n",
      "     Training Step: 13 Training Loss: 2.4368667602539062 \n",
      "     Training Step: 14 Training Loss: 3.526869773864746 \n",
      "     Training Step: 15 Training Loss: 2.6356186866760254 \n",
      "     Training Step: 16 Training Loss: 2.6402788162231445 \n",
      "     Training Step: 17 Training Loss: 2.166137933731079 \n",
      "     Training Step: 18 Training Loss: 4.646235466003418 \n",
      "     Training Step: 19 Training Loss: 4.101316452026367 \n",
      "     Training Step: 20 Training Loss: 3.443274974822998 \n",
      "     Training Step: 21 Training Loss: 2.361635684967041 \n",
      "     Training Step: 22 Training Loss: 3.302861213684082 \n",
      "     Training Step: 23 Training Loss: 2.951455593109131 \n",
      "     Training Step: 24 Training Loss: 2.9583005905151367 \n",
      "     Training Step: 25 Training Loss: 2.4004931449890137 \n",
      "     Training Step: 26 Training Loss: 3.132235527038574 \n",
      "     Training Step: 27 Training Loss: 2.4752798080444336 \n",
      "     Training Step: 28 Training Loss: 3.0251784324645996 \n",
      "     Training Step: 29 Training Loss: 2.3990237712860107 \n",
      "     Training Step: 30 Training Loss: 3.563828468322754 \n",
      "     Training Step: 31 Training Loss: 3.133082866668701 \n",
      "     Training Step: 32 Training Loss: 3.597074508666992 \n",
      "     Training Step: 33 Training Loss: 2.32191801071167 \n",
      "     Training Step: 34 Training Loss: 2.6684587001800537 \n",
      "     Training Step: 35 Training Loss: 4.005645275115967 \n",
      "     Training Step: 36 Training Loss: 3.2065558433532715 \n",
      "     Training Step: 37 Training Loss: 2.5772836208343506 \n",
      "     Training Step: 38 Training Loss: 3.590231418609619 \n",
      "     Training Step: 39 Training Loss: 2.895346164703369 \n",
      "     Training Step: 40 Training Loss: 3.1115076541900635 \n",
      "     Training Step: 41 Training Loss: 2.585206985473633 \n",
      "     Training Step: 42 Training Loss: 3.577181339263916 \n",
      "     Training Step: 43 Training Loss: 2.540447473526001 \n",
      "     Training Step: 44 Training Loss: 3.20686936378479 \n",
      "     Training Step: 45 Training Loss: 2.390407085418701 \n",
      "     Training Step: 46 Training Loss: 2.9117813110351562 \n",
      "     Training Step: 47 Training Loss: 3.1079843044281006 \n",
      "     Training Step: 48 Training Loss: 2.8766837120056152 \n",
      "     Training Step: 49 Training Loss: 3.68471097946167 \n",
      "     Training Step: 50 Training Loss: 2.2864437103271484 \n",
      "     Training Step: 51 Training Loss: 4.252653121948242 \n",
      "     Training Step: 52 Training Loss: 2.265568733215332 \n",
      "     Training Step: 53 Training Loss: 2.8175692558288574 \n",
      "     Training Step: 54 Training Loss: 3.4350688457489014 \n",
      "     Training Step: 55 Training Loss: 3.40364146232605 \n",
      "     Training Step: 56 Training Loss: 2.910163402557373 \n",
      "     Training Step: 57 Training Loss: 3.0154523849487305 \n",
      "     Training Step: 58 Training Loss: 3.8376710414886475 \n",
      "     Training Step: 59 Training Loss: 2.9092564582824707 \n",
      "     Training Step: 60 Training Loss: 2.101503849029541 \n",
      "     Training Step: 61 Training Loss: 3.8880653381347656 \n",
      "     Training Step: 62 Training Loss: 2.0911107063293457 \n",
      "     Training Step: 63 Training Loss: 3.2845423221588135 \n",
      "     Training Step: 64 Training Loss: 2.63049054145813 \n",
      "     Training Step: 65 Training Loss: 3.1731033325195312 \n",
      "     Training Step: 66 Training Loss: 3.3332858085632324 \n",
      "     Training Step: 67 Training Loss: 2.537311553955078 \n",
      "     Training Step: 68 Training Loss: 2.9684982299804688 \n",
      "     Training Step: 69 Training Loss: 2.8253397941589355 \n",
      "     Training Step: 70 Training Loss: 2.614457130432129 \n",
      "     Training Step: 71 Training Loss: 4.5815582275390625 \n",
      "     Training Step: 72 Training Loss: 3.290616512298584 \n",
      "     Training Step: 73 Training Loss: 3.8493595123291016 \n",
      "     Training Step: 74 Training Loss: 2.494840383529663 \n",
      "     Training Step: 75 Training Loss: 3.143540859222412 \n",
      "     Training Step: 76 Training Loss: 2.805475950241089 \n",
      "     Training Step: 77 Training Loss: 2.3246989250183105 \n",
      "     Training Step: 78 Training Loss: 3.698075294494629 \n",
      "     Training Step: 79 Training Loss: 2.8909494876861572 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.2078216075897217 \n",
      "     Validation Step: 1 Validation Loss: 3.688000440597534 \n",
      "     Validation Step: 2 Validation Loss: 3.3806185722351074 \n",
      "     Validation Step: 3 Validation Loss: 3.784027099609375 \n",
      "     Validation Step: 4 Validation Loss: 3.018496036529541 \n",
      "     Validation Step: 5 Validation Loss: 3.89376163482666 \n",
      "     Validation Step: 6 Validation Loss: 2.74837589263916 \n",
      "     Validation Step: 7 Validation Loss: 3.5197973251342773 \n",
      "     Validation Step: 8 Validation Loss: 3.6542398929595947 \n",
      "     Validation Step: 9 Validation Loss: 2.7580032348632812 \n",
      "     Validation Step: 10 Validation Loss: 3.0644168853759766 \n",
      "     Validation Step: 11 Validation Loss: 3.2672390937805176 \n",
      "     Validation Step: 12 Validation Loss: 2.136946678161621 \n",
      "     Validation Step: 13 Validation Loss: 3.1080169677734375 \n",
      "     Validation Step: 14 Validation Loss: 3.1324942111968994 \n",
      "Epoch: 88\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.7150704860687256 \n",
      "     Training Step: 1 Training Loss: 3.4704365730285645 \n",
      "     Training Step: 2 Training Loss: 2.576143980026245 \n",
      "     Training Step: 3 Training Loss: 2.330226421356201 \n",
      "     Training Step: 4 Training Loss: 3.001173496246338 \n",
      "     Training Step: 5 Training Loss: 3.1167964935302734 \n",
      "     Training Step: 6 Training Loss: 2.726775646209717 \n",
      "     Training Step: 7 Training Loss: 3.069571018218994 \n",
      "     Training Step: 8 Training Loss: 2.7892210483551025 \n",
      "     Training Step: 9 Training Loss: 2.971072196960449 \n",
      "     Training Step: 10 Training Loss: 2.3231260776519775 \n",
      "     Training Step: 11 Training Loss: 2.1011996269226074 \n",
      "     Training Step: 12 Training Loss: 2.891474723815918 \n",
      "     Training Step: 13 Training Loss: 4.694244861602783 \n",
      "     Training Step: 14 Training Loss: 3.0829498767852783 \n",
      "     Training Step: 15 Training Loss: 2.213449001312256 \n",
      "     Training Step: 16 Training Loss: 3.487527370452881 \n",
      "     Training Step: 17 Training Loss: 2.4228453636169434 \n",
      "     Training Step: 18 Training Loss: 2.593773365020752 \n",
      "     Training Step: 19 Training Loss: 2.912029266357422 \n",
      "     Training Step: 20 Training Loss: 3.3726792335510254 \n",
      "     Training Step: 21 Training Loss: 2.7822117805480957 \n",
      "     Training Step: 22 Training Loss: 2.379072427749634 \n",
      "     Training Step: 23 Training Loss: 3.4669113159179688 \n",
      "     Training Step: 24 Training Loss: 2.086198329925537 \n",
      "     Training Step: 25 Training Loss: 3.8421053886413574 \n",
      "     Training Step: 26 Training Loss: 3.4474070072174072 \n",
      "     Training Step: 27 Training Loss: 3.111692428588867 \n",
      "     Training Step: 28 Training Loss: 3.1604197025299072 \n",
      "     Training Step: 29 Training Loss: 3.7488393783569336 \n",
      "     Training Step: 30 Training Loss: 3.819148063659668 \n",
      "     Training Step: 31 Training Loss: 2.741431713104248 \n",
      "     Training Step: 32 Training Loss: 2.752406597137451 \n",
      "     Training Step: 33 Training Loss: 2.6067559719085693 \n",
      "     Training Step: 34 Training Loss: 3.4501752853393555 \n",
      "     Training Step: 35 Training Loss: 2.2457194328308105 \n",
      "     Training Step: 36 Training Loss: 2.016894817352295 \n",
      "     Training Step: 37 Training Loss: 2.756019115447998 \n",
      "     Training Step: 38 Training Loss: 3.419651985168457 \n",
      "     Training Step: 39 Training Loss: 4.317755222320557 \n",
      "     Training Step: 40 Training Loss: 3.2624218463897705 \n",
      "     Training Step: 41 Training Loss: 2.2272658348083496 \n",
      "     Training Step: 42 Training Loss: 2.3267390727996826 \n",
      "     Training Step: 43 Training Loss: 2.5512993335723877 \n",
      "     Training Step: 44 Training Loss: 4.274260997772217 \n",
      "     Training Step: 45 Training Loss: 3.1310625076293945 \n",
      "     Training Step: 46 Training Loss: 2.2463128566741943 \n",
      "     Training Step: 47 Training Loss: 2.3484554290771484 \n",
      "     Training Step: 48 Training Loss: 2.91024112701416 \n",
      "     Training Step: 49 Training Loss: 2.6797356605529785 \n",
      "     Training Step: 50 Training Loss: 3.3492093086242676 \n",
      "     Training Step: 51 Training Loss: 3.415419578552246 \n",
      "     Training Step: 52 Training Loss: 3.3697328567504883 \n",
      "     Training Step: 53 Training Loss: 3.7331180572509766 \n",
      "     Training Step: 54 Training Loss: 4.126865863800049 \n",
      "     Training Step: 55 Training Loss: 3.0637869834899902 \n",
      "     Training Step: 56 Training Loss: 3.061859130859375 \n",
      "     Training Step: 57 Training Loss: 3.016322612762451 \n",
      "     Training Step: 58 Training Loss: 2.108281373977661 \n",
      "     Training Step: 59 Training Loss: 2.480276107788086 \n",
      "     Training Step: 60 Training Loss: 3.2515711784362793 \n",
      "     Training Step: 61 Training Loss: 3.14566969871521 \n",
      "     Training Step: 62 Training Loss: 3.2411980628967285 \n",
      "     Training Step: 63 Training Loss: 2.517873764038086 \n",
      "     Training Step: 64 Training Loss: 2.40559983253479 \n",
      "     Training Step: 65 Training Loss: 2.626087188720703 \n",
      "     Training Step: 66 Training Loss: 2.9322268962860107 \n",
      "     Training Step: 67 Training Loss: 3.6437370777130127 \n",
      "     Training Step: 68 Training Loss: 2.42643404006958 \n",
      "     Training Step: 69 Training Loss: 3.318863868713379 \n",
      "     Training Step: 70 Training Loss: 3.086365222930908 \n",
      "     Training Step: 71 Training Loss: 2.748363971710205 \n",
      "     Training Step: 72 Training Loss: 2.247922420501709 \n",
      "     Training Step: 73 Training Loss: 3.2798662185668945 \n",
      "     Training Step: 74 Training Loss: 2.886857271194458 \n",
      "     Training Step: 75 Training Loss: 4.321902275085449 \n",
      "     Training Step: 76 Training Loss: 2.7416975498199463 \n",
      "     Training Step: 77 Training Loss: 3.0019726753234863 \n",
      "     Training Step: 78 Training Loss: 3.4357175827026367 \n",
      "     Training Step: 79 Training Loss: 2.4774856567382812 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.545858860015869 \n",
      "     Validation Step: 1 Validation Loss: 3.2608470916748047 \n",
      "     Validation Step: 2 Validation Loss: 2.922305107116699 \n",
      "     Validation Step: 3 Validation Loss: 2.629533290863037 \n",
      "     Validation Step: 4 Validation Loss: 2.6337528228759766 \n",
      "     Validation Step: 5 Validation Loss: 2.236945152282715 \n",
      "     Validation Step: 6 Validation Loss: 3.531165838241577 \n",
      "     Validation Step: 7 Validation Loss: 3.4877564907073975 \n",
      "     Validation Step: 8 Validation Loss: 3.1007323265075684 \n",
      "     Validation Step: 9 Validation Loss: 3.6592249870300293 \n",
      "     Validation Step: 10 Validation Loss: 2.9570930004119873 \n",
      "     Validation Step: 11 Validation Loss: 3.7629802227020264 \n",
      "     Validation Step: 12 Validation Loss: 3.597275733947754 \n",
      "     Validation Step: 13 Validation Loss: 3.0210633277893066 \n",
      "     Validation Step: 14 Validation Loss: 2.8538804054260254 \n",
      "Epoch: 89\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.4896774291992188 \n",
      "     Training Step: 1 Training Loss: 3.4474825859069824 \n",
      "     Training Step: 2 Training Loss: 2.7035231590270996 \n",
      "     Training Step: 3 Training Loss: 2.2351791858673096 \n",
      "     Training Step: 4 Training Loss: 2.9535393714904785 \n",
      "     Training Step: 5 Training Loss: 4.231462001800537 \n",
      "     Training Step: 6 Training Loss: 2.5712900161743164 \n",
      "     Training Step: 7 Training Loss: 2.700444221496582 \n",
      "     Training Step: 8 Training Loss: 3.4919145107269287 \n",
      "     Training Step: 9 Training Loss: 2.5210938453674316 \n",
      "     Training Step: 10 Training Loss: 2.7468056678771973 \n",
      "     Training Step: 11 Training Loss: 3.7284929752349854 \n",
      "     Training Step: 12 Training Loss: 3.133463144302368 \n",
      "     Training Step: 13 Training Loss: 2.4511566162109375 \n",
      "     Training Step: 14 Training Loss: 3.9244422912597656 \n",
      "     Training Step: 15 Training Loss: 3.5796947479248047 \n",
      "     Training Step: 16 Training Loss: 3.3360953330993652 \n",
      "     Training Step: 17 Training Loss: 2.3276681900024414 \n",
      "     Training Step: 18 Training Loss: 3.580852508544922 \n",
      "     Training Step: 19 Training Loss: 2.879276752471924 \n",
      "     Training Step: 20 Training Loss: 2.268873929977417 \n",
      "     Training Step: 21 Training Loss: 2.5897746086120605 \n",
      "     Training Step: 22 Training Loss: 3.54268741607666 \n",
      "     Training Step: 23 Training Loss: 3.0259134769439697 \n",
      "     Training Step: 24 Training Loss: 2.922410011291504 \n",
      "     Training Step: 25 Training Loss: 2.986604690551758 \n",
      "     Training Step: 26 Training Loss: 2.295498847961426 \n",
      "     Training Step: 27 Training Loss: 2.399272918701172 \n",
      "     Training Step: 28 Training Loss: 2.8694820404052734 \n",
      "     Training Step: 29 Training Loss: 3.2559471130371094 \n",
      "     Training Step: 30 Training Loss: 3.050182580947876 \n",
      "     Training Step: 31 Training Loss: 3.246593952178955 \n",
      "     Training Step: 32 Training Loss: 3.2903506755828857 \n",
      "     Training Step: 33 Training Loss: 2.8741092681884766 \n",
      "     Training Step: 34 Training Loss: 2.3991923332214355 \n",
      "     Training Step: 35 Training Loss: 3.6318554878234863 \n",
      "     Training Step: 36 Training Loss: 3.4287867546081543 \n",
      "     Training Step: 37 Training Loss: 2.7999684810638428 \n",
      "     Training Step: 38 Training Loss: 2.8091044425964355 \n",
      "     Training Step: 39 Training Loss: 2.8794472217559814 \n",
      "     Training Step: 40 Training Loss: 2.022498607635498 \n",
      "     Training Step: 41 Training Loss: 2.6528162956237793 \n",
      "     Training Step: 42 Training Loss: 2.387523651123047 \n",
      "     Training Step: 43 Training Loss: 3.087002992630005 \n",
      "     Training Step: 44 Training Loss: 2.2946383953094482 \n",
      "     Training Step: 45 Training Loss: 3.6076064109802246 \n",
      "     Training Step: 46 Training Loss: 3.094944715499878 \n",
      "     Training Step: 47 Training Loss: 2.084029197692871 \n",
      "     Training Step: 48 Training Loss: 3.957095146179199 \n",
      "     Training Step: 49 Training Loss: 2.837617874145508 \n",
      "     Training Step: 50 Training Loss: 2.1626181602478027 \n",
      "     Training Step: 51 Training Loss: 2.6228318214416504 \n",
      "     Training Step: 52 Training Loss: 3.639329433441162 \n",
      "     Training Step: 53 Training Loss: 2.461021661758423 \n",
      "     Training Step: 54 Training Loss: 2.4162817001342773 \n",
      "     Training Step: 55 Training Loss: 2.9307010173797607 \n",
      "     Training Step: 56 Training Loss: 2.982326030731201 \n",
      "     Training Step: 57 Training Loss: 3.4354066848754883 \n",
      "     Training Step: 58 Training Loss: 2.604248046875 \n",
      "     Training Step: 59 Training Loss: 3.8915185928344727 \n",
      "     Training Step: 60 Training Loss: 2.7444143295288086 \n",
      "     Training Step: 61 Training Loss: 4.150150299072266 \n",
      "     Training Step: 62 Training Loss: 3.0524516105651855 \n",
      "     Training Step: 63 Training Loss: 3.0738461017608643 \n",
      "     Training Step: 64 Training Loss: 3.728804349899292 \n",
      "     Training Step: 65 Training Loss: 4.301446437835693 \n",
      "     Training Step: 66 Training Loss: 2.3892955780029297 \n",
      "     Training Step: 67 Training Loss: 4.970714569091797 \n",
      "     Training Step: 68 Training Loss: 2.3421578407287598 \n",
      "     Training Step: 69 Training Loss: 3.1208295822143555 \n",
      "     Training Step: 70 Training Loss: 2.8445889949798584 \n",
      "     Training Step: 71 Training Loss: 2.710920572280884 \n",
      "     Training Step: 72 Training Loss: 2.30183744430542 \n",
      "     Training Step: 73 Training Loss: 2.4997925758361816 \n",
      "     Training Step: 74 Training Loss: 2.20389986038208 \n",
      "     Training Step: 75 Training Loss: 4.370721340179443 \n",
      "     Training Step: 76 Training Loss: 2.613894462585449 \n",
      "     Training Step: 77 Training Loss: 2.4739809036254883 \n",
      "     Training Step: 78 Training Loss: 3.8901612758636475 \n",
      "     Training Step: 79 Training Loss: 3.2167181968688965 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.3656461238861084 \n",
      "     Validation Step: 1 Validation Loss: 3.8279683589935303 \n",
      "     Validation Step: 2 Validation Loss: 3.5350427627563477 \n",
      "     Validation Step: 3 Validation Loss: 3.570497512817383 \n",
      "     Validation Step: 4 Validation Loss: 3.2054190635681152 \n",
      "     Validation Step: 5 Validation Loss: 3.0311803817749023 \n",
      "     Validation Step: 6 Validation Loss: 3.067458391189575 \n",
      "     Validation Step: 7 Validation Loss: 3.0877323150634766 \n",
      "     Validation Step: 8 Validation Loss: 2.734236240386963 \n",
      "     Validation Step: 9 Validation Loss: 3.636996269226074 \n",
      "     Validation Step: 10 Validation Loss: 3.7235381603240967 \n",
      "     Validation Step: 11 Validation Loss: 2.725142478942871 \n",
      "     Validation Step: 12 Validation Loss: 3.648669719696045 \n",
      "     Validation Step: 13 Validation Loss: 2.8229589462280273 \n",
      "     Validation Step: 14 Validation Loss: 2.9804320335388184 \n",
      "Epoch: 90\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.4094128608703613 \n",
      "     Training Step: 1 Training Loss: 3.048114776611328 \n",
      "     Training Step: 2 Training Loss: 2.137676477432251 \n",
      "     Training Step: 3 Training Loss: 2.775991678237915 \n",
      "     Training Step: 4 Training Loss: 2.2191545963287354 \n",
      "     Training Step: 5 Training Loss: 3.197131872177124 \n",
      "     Training Step: 6 Training Loss: 2.8422505855560303 \n",
      "     Training Step: 7 Training Loss: 2.3635597229003906 \n",
      "     Training Step: 8 Training Loss: 3.0276365280151367 \n",
      "     Training Step: 9 Training Loss: 2.2365708351135254 \n",
      "     Training Step: 10 Training Loss: 4.224048614501953 \n",
      "     Training Step: 11 Training Loss: 3.2518157958984375 \n",
      "     Training Step: 12 Training Loss: 2.1617298126220703 \n",
      "     Training Step: 13 Training Loss: 3.3616716861724854 \n",
      "     Training Step: 14 Training Loss: 3.402005910873413 \n",
      "     Training Step: 15 Training Loss: 2.6649386882781982 \n",
      "     Training Step: 16 Training Loss: 2.4457342624664307 \n",
      "     Training Step: 17 Training Loss: 2.5285842418670654 \n",
      "     Training Step: 18 Training Loss: 2.776412010192871 \n",
      "     Training Step: 19 Training Loss: 3.419966697692871 \n",
      "     Training Step: 20 Training Loss: 4.165882587432861 \n",
      "     Training Step: 21 Training Loss: 2.365170955657959 \n",
      "     Training Step: 22 Training Loss: 4.261285781860352 \n",
      "     Training Step: 23 Training Loss: 2.1185760498046875 \n",
      "     Training Step: 24 Training Loss: 3.342085838317871 \n",
      "     Training Step: 25 Training Loss: 2.168840169906616 \n",
      "     Training Step: 26 Training Loss: 4.664874076843262 \n",
      "     Training Step: 27 Training Loss: 3.133126735687256 \n",
      "     Training Step: 28 Training Loss: 2.880192756652832 \n",
      "     Training Step: 29 Training Loss: 2.8660597801208496 \n",
      "     Training Step: 30 Training Loss: 2.5050578117370605 \n",
      "     Training Step: 31 Training Loss: 2.2820606231689453 \n",
      "     Training Step: 32 Training Loss: 3.5859932899475098 \n",
      "     Training Step: 33 Training Loss: 3.575594425201416 \n",
      "     Training Step: 34 Training Loss: 2.984313488006592 \n",
      "     Training Step: 35 Training Loss: 2.9088973999023438 \n",
      "     Training Step: 36 Training Loss: 2.6866683959960938 \n",
      "     Training Step: 37 Training Loss: 2.994209051132202 \n",
      "     Training Step: 38 Training Loss: 2.844332456588745 \n",
      "     Training Step: 39 Training Loss: 3.6513352394104004 \n",
      "     Training Step: 40 Training Loss: 3.6788296699523926 \n",
      "     Training Step: 41 Training Loss: 3.586456775665283 \n",
      "     Training Step: 42 Training Loss: 3.3184475898742676 \n",
      "     Training Step: 43 Training Loss: 3.035761833190918 \n",
      "     Training Step: 44 Training Loss: 2.777871608734131 \n",
      "     Training Step: 45 Training Loss: 3.0831613540649414 \n",
      "     Training Step: 46 Training Loss: 2.8974204063415527 \n",
      "     Training Step: 47 Training Loss: 4.020031929016113 \n",
      "     Training Step: 48 Training Loss: 2.2223153114318848 \n",
      "     Training Step: 49 Training Loss: 3.4182662963867188 \n",
      "     Training Step: 50 Training Loss: 3.397616147994995 \n",
      "     Training Step: 51 Training Loss: 3.2123894691467285 \n",
      "     Training Step: 52 Training Loss: 3.9827446937561035 \n",
      "     Training Step: 53 Training Loss: 2.9330763816833496 \n",
      "     Training Step: 54 Training Loss: 2.8129820823669434 \n",
      "     Training Step: 55 Training Loss: 3.117760419845581 \n",
      "     Training Step: 56 Training Loss: 2.3970704078674316 \n",
      "     Training Step: 57 Training Loss: 2.1628010272979736 \n",
      "     Training Step: 58 Training Loss: 2.5652554035186768 \n",
      "     Training Step: 59 Training Loss: 3.4399328231811523 \n",
      "     Training Step: 60 Training Loss: 2.856962203979492 \n",
      "     Training Step: 61 Training Loss: 2.69626522064209 \n",
      "     Training Step: 62 Training Loss: 3.5806102752685547 \n",
      "     Training Step: 63 Training Loss: 3.666116952896118 \n",
      "     Training Step: 64 Training Loss: 2.7009310722351074 \n",
      "     Training Step: 65 Training Loss: 2.6141676902770996 \n",
      "     Training Step: 66 Training Loss: 2.8449771404266357 \n",
      "     Training Step: 67 Training Loss: 2.692082166671753 \n",
      "     Training Step: 68 Training Loss: 3.338902711868286 \n",
      "     Training Step: 69 Training Loss: 2.452305793762207 \n",
      "     Training Step: 70 Training Loss: 2.737590789794922 \n",
      "     Training Step: 71 Training Loss: 2.5259454250335693 \n",
      "     Training Step: 72 Training Loss: 3.2756340503692627 \n",
      "     Training Step: 73 Training Loss: 2.9825754165649414 \n",
      "     Training Step: 74 Training Loss: 2.6073455810546875 \n",
      "     Training Step: 75 Training Loss: 3.455657958984375 \n",
      "     Training Step: 76 Training Loss: 3.249573230743408 \n",
      "     Training Step: 77 Training Loss: 2.7145633697509766 \n",
      "     Training Step: 78 Training Loss: 3.8696792125701904 \n",
      "     Training Step: 79 Training Loss: 2.660336494445801 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1312599182128906 \n",
      "     Validation Step: 1 Validation Loss: 2.910292625427246 \n",
      "     Validation Step: 2 Validation Loss: 3.200751304626465 \n",
      "     Validation Step: 3 Validation Loss: 3.698988914489746 \n",
      "     Validation Step: 4 Validation Loss: 3.6525702476501465 \n",
      "     Validation Step: 5 Validation Loss: 3.613636016845703 \n",
      "     Validation Step: 6 Validation Loss: 2.7382125854492188 \n",
      "     Validation Step: 7 Validation Loss: 3.3476197719573975 \n",
      "     Validation Step: 8 Validation Loss: 3.201974868774414 \n",
      "     Validation Step: 9 Validation Loss: 3.9522387981414795 \n",
      "     Validation Step: 10 Validation Loss: 2.908823013305664 \n",
      "     Validation Step: 11 Validation Loss: 3.649005174636841 \n",
      "     Validation Step: 12 Validation Loss: 3.0539817810058594 \n",
      "     Validation Step: 13 Validation Loss: 2.235006809234619 \n",
      "     Validation Step: 14 Validation Loss: 3.140752077102661 \n",
      "Epoch: 91\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.029672145843506 \n",
      "     Training Step: 1 Training Loss: 3.2876136302948 \n",
      "     Training Step: 2 Training Loss: 3.7960824966430664 \n",
      "     Training Step: 3 Training Loss: 2.4352777004241943 \n",
      "     Training Step: 4 Training Loss: 3.1337027549743652 \n",
      "     Training Step: 5 Training Loss: 3.245396375656128 \n",
      "     Training Step: 6 Training Loss: 2.377049207687378 \n",
      "     Training Step: 7 Training Loss: 2.7101686000823975 \n",
      "     Training Step: 8 Training Loss: 2.738079071044922 \n",
      "     Training Step: 9 Training Loss: 2.7537598609924316 \n",
      "     Training Step: 10 Training Loss: 2.070563316345215 \n",
      "     Training Step: 11 Training Loss: 2.1815764904022217 \n",
      "     Training Step: 12 Training Loss: 3.5093390941619873 \n",
      "     Training Step: 13 Training Loss: 3.07993221282959 \n",
      "     Training Step: 14 Training Loss: 3.0622448921203613 \n",
      "     Training Step: 15 Training Loss: 3.5635266304016113 \n",
      "     Training Step: 16 Training Loss: 3.5513885021209717 \n",
      "     Training Step: 17 Training Loss: 3.462193489074707 \n",
      "     Training Step: 18 Training Loss: 3.8501667976379395 \n",
      "     Training Step: 19 Training Loss: 2.8139073848724365 \n",
      "     Training Step: 20 Training Loss: 3.805854320526123 \n",
      "     Training Step: 21 Training Loss: 4.01821231842041 \n",
      "     Training Step: 22 Training Loss: 3.0212039947509766 \n",
      "     Training Step: 23 Training Loss: 3.470486640930176 \n",
      "     Training Step: 24 Training Loss: 2.297631025314331 \n",
      "     Training Step: 25 Training Loss: 3.3223819732666016 \n",
      "     Training Step: 26 Training Loss: 3.2508084774017334 \n",
      "     Training Step: 27 Training Loss: 2.197683811187744 \n",
      "     Training Step: 28 Training Loss: 2.5467026233673096 \n",
      "     Training Step: 29 Training Loss: 3.2455873489379883 \n",
      "     Training Step: 30 Training Loss: 3.101336717605591 \n",
      "     Training Step: 31 Training Loss: 3.7264842987060547 \n",
      "     Training Step: 32 Training Loss: 2.6748363971710205 \n",
      "     Training Step: 33 Training Loss: 2.9894895553588867 \n",
      "     Training Step: 34 Training Loss: 2.205472469329834 \n",
      "     Training Step: 35 Training Loss: 4.200348854064941 \n",
      "     Training Step: 36 Training Loss: 2.4944310188293457 \n",
      "     Training Step: 37 Training Loss: 2.734151601791382 \n",
      "     Training Step: 38 Training Loss: 3.216517448425293 \n",
      "     Training Step: 39 Training Loss: 2.5204596519470215 \n",
      "     Training Step: 40 Training Loss: 3.1186957359313965 \n",
      "     Training Step: 41 Training Loss: 3.953965187072754 \n",
      "     Training Step: 42 Training Loss: 2.342527151107788 \n",
      "     Training Step: 43 Training Loss: 2.475182056427002 \n",
      "     Training Step: 44 Training Loss: 2.5645642280578613 \n",
      "     Training Step: 45 Training Loss: 2.6347603797912598 \n",
      "     Training Step: 46 Training Loss: 3.364511013031006 \n",
      "     Training Step: 47 Training Loss: 3.3551902770996094 \n",
      "     Training Step: 48 Training Loss: 3.19913911819458 \n",
      "     Training Step: 49 Training Loss: 2.5226945877075195 \n",
      "     Training Step: 50 Training Loss: 3.4496641159057617 \n",
      "     Training Step: 51 Training Loss: 2.2533090114593506 \n",
      "     Training Step: 52 Training Loss: 2.3915600776672363 \n",
      "     Training Step: 53 Training Loss: 2.857853889465332 \n",
      "     Training Step: 54 Training Loss: 2.515749931335449 \n",
      "     Training Step: 55 Training Loss: 3.073017120361328 \n",
      "     Training Step: 56 Training Loss: 2.8472301959991455 \n",
      "     Training Step: 57 Training Loss: 2.9923501014709473 \n",
      "     Training Step: 58 Training Loss: 3.4097211360931396 \n",
      "     Training Step: 59 Training Loss: 2.4780004024505615 \n",
      "     Training Step: 60 Training Loss: 3.204059362411499 \n",
      "     Training Step: 61 Training Loss: 2.2219786643981934 \n",
      "     Training Step: 62 Training Loss: 2.7445623874664307 \n",
      "     Training Step: 63 Training Loss: 2.7366490364074707 \n",
      "     Training Step: 64 Training Loss: 3.1149532794952393 \n",
      "     Training Step: 65 Training Loss: 4.407847881317139 \n",
      "     Training Step: 66 Training Loss: 3.253920078277588 \n",
      "     Training Step: 67 Training Loss: 2.9683828353881836 \n",
      "     Training Step: 68 Training Loss: 2.7156312465667725 \n",
      "     Training Step: 69 Training Loss: 4.861326217651367 \n",
      "     Training Step: 70 Training Loss: 2.391031503677368 \n",
      "     Training Step: 71 Training Loss: 2.294001579284668 \n",
      "     Training Step: 72 Training Loss: 3.4922871589660645 \n",
      "     Training Step: 73 Training Loss: 3.081552028656006 \n",
      "     Training Step: 74 Training Loss: 2.644550323486328 \n",
      "     Training Step: 75 Training Loss: 2.4502274990081787 \n",
      "     Training Step: 76 Training Loss: 2.5879578590393066 \n",
      "     Training Step: 77 Training Loss: 3.1472907066345215 \n",
      "     Training Step: 78 Training Loss: 3.388946533203125 \n",
      "     Training Step: 79 Training Loss: 2.050861358642578 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1239171028137207 \n",
      "     Validation Step: 1 Validation Loss: 3.730982780456543 \n",
      "     Validation Step: 2 Validation Loss: 2.1557955741882324 \n",
      "     Validation Step: 3 Validation Loss: 3.0776944160461426 \n",
      "     Validation Step: 4 Validation Loss: 3.6106858253479004 \n",
      "     Validation Step: 5 Validation Loss: 2.7023298740386963 \n",
      "     Validation Step: 6 Validation Loss: 3.895204782485962 \n",
      "     Validation Step: 7 Validation Loss: 3.1668126583099365 \n",
      "     Validation Step: 8 Validation Loss: 3.1383001804351807 \n",
      "     Validation Step: 9 Validation Loss: 2.969607353210449 \n",
      "     Validation Step: 10 Validation Loss: 3.2103636264801025 \n",
      "     Validation Step: 11 Validation Loss: 3.5771279335021973 \n",
      "     Validation Step: 12 Validation Loss: 2.763946771621704 \n",
      "     Validation Step: 13 Validation Loss: 3.36226749420166 \n",
      "     Validation Step: 14 Validation Loss: 3.627530574798584 \n",
      "Epoch: 92\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.144289255142212 \n",
      "     Training Step: 1 Training Loss: 3.51198673248291 \n",
      "     Training Step: 2 Training Loss: 2.522665023803711 \n",
      "     Training Step: 3 Training Loss: 2.771652936935425 \n",
      "     Training Step: 4 Training Loss: 2.151346206665039 \n",
      "     Training Step: 5 Training Loss: 2.820138454437256 \n",
      "     Training Step: 6 Training Loss: 2.7578189373016357 \n",
      "     Training Step: 7 Training Loss: 2.7575597763061523 \n",
      "     Training Step: 8 Training Loss: 4.801661491394043 \n",
      "     Training Step: 9 Training Loss: 2.7499523162841797 \n",
      "     Training Step: 10 Training Loss: 2.295717716217041 \n",
      "     Training Step: 11 Training Loss: 2.4068727493286133 \n",
      "     Training Step: 12 Training Loss: 2.478588581085205 \n",
      "     Training Step: 13 Training Loss: 3.0789248943328857 \n",
      "     Training Step: 14 Training Loss: 2.97831392288208 \n",
      "     Training Step: 15 Training Loss: 2.8641278743743896 \n",
      "     Training Step: 16 Training Loss: 2.543696403503418 \n",
      "     Training Step: 17 Training Loss: 4.2708258628845215 \n",
      "     Training Step: 18 Training Loss: 3.6369104385375977 \n",
      "     Training Step: 19 Training Loss: 2.6657519340515137 \n",
      "     Training Step: 20 Training Loss: 2.6176087856292725 \n",
      "     Training Step: 21 Training Loss: 3.137373208999634 \n",
      "     Training Step: 22 Training Loss: 3.1721138954162598 \n",
      "     Training Step: 23 Training Loss: 2.784409999847412 \n",
      "     Training Step: 24 Training Loss: 3.0956602096557617 \n",
      "     Training Step: 25 Training Loss: 3.507070541381836 \n",
      "     Training Step: 26 Training Loss: 3.442574977874756 \n",
      "     Training Step: 27 Training Loss: 2.5795862674713135 \n",
      "     Training Step: 28 Training Loss: 3.419154167175293 \n",
      "     Training Step: 29 Training Loss: 2.391674041748047 \n",
      "     Training Step: 30 Training Loss: 2.373014450073242 \n",
      "     Training Step: 31 Training Loss: 3.870110511779785 \n",
      "     Training Step: 32 Training Loss: 2.9875361919403076 \n",
      "     Training Step: 33 Training Loss: 3.8568296432495117 \n",
      "     Training Step: 34 Training Loss: 2.5918331146240234 \n",
      "     Training Step: 35 Training Loss: 3.5127930641174316 \n",
      "     Training Step: 36 Training Loss: 3.2652487754821777 \n",
      "     Training Step: 37 Training Loss: 3.1832923889160156 \n",
      "     Training Step: 38 Training Loss: 2.9729530811309814 \n",
      "     Training Step: 39 Training Loss: 2.0757017135620117 \n",
      "     Training Step: 40 Training Loss: 2.3455281257629395 \n",
      "     Training Step: 41 Training Loss: 2.76314115524292 \n",
      "     Training Step: 42 Training Loss: 3.0752780437469482 \n",
      "     Training Step: 43 Training Loss: 3.5174241065979004 \n",
      "     Training Step: 44 Training Loss: 2.2107551097869873 \n",
      "     Training Step: 45 Training Loss: 3.7393908500671387 \n",
      "     Training Step: 46 Training Loss: 2.9216079711914062 \n",
      "     Training Step: 47 Training Loss: 3.423491954803467 \n",
      "     Training Step: 48 Training Loss: 3.7606401443481445 \n",
      "     Training Step: 49 Training Loss: 2.1467418670654297 \n",
      "     Training Step: 50 Training Loss: 3.2660720348358154 \n",
      "     Training Step: 51 Training Loss: 3.4552149772644043 \n",
      "     Training Step: 52 Training Loss: 3.383545398712158 \n",
      "     Training Step: 53 Training Loss: 2.856365203857422 \n",
      "     Training Step: 54 Training Loss: 2.0971035957336426 \n",
      "     Training Step: 55 Training Loss: 4.255249500274658 \n",
      "     Training Step: 56 Training Loss: 3.6452908515930176 \n",
      "     Training Step: 57 Training Loss: 2.912602424621582 \n",
      "     Training Step: 58 Training Loss: 3.4586377143859863 \n",
      "     Training Step: 59 Training Loss: 3.035710573196411 \n",
      "     Training Step: 60 Training Loss: 3.03918194770813 \n",
      "     Training Step: 61 Training Loss: 2.956879138946533 \n",
      "     Training Step: 62 Training Loss: 2.9084155559539795 \n",
      "     Training Step: 63 Training Loss: 2.5603485107421875 \n",
      "     Training Step: 64 Training Loss: 2.386723518371582 \n",
      "     Training Step: 65 Training Loss: 3.0747532844543457 \n",
      "     Training Step: 66 Training Loss: 3.7679994106292725 \n",
      "     Training Step: 67 Training Loss: 3.1243739128112793 \n",
      "     Training Step: 68 Training Loss: 2.5502262115478516 \n",
      "     Training Step: 69 Training Loss: 2.484179735183716 \n",
      "     Training Step: 70 Training Loss: 3.082000255584717 \n",
      "     Training Step: 71 Training Loss: 4.414720058441162 \n",
      "     Training Step: 72 Training Loss: 3.4877452850341797 \n",
      "     Training Step: 73 Training Loss: 3.921194553375244 \n",
      "     Training Step: 74 Training Loss: 3.0089588165283203 \n",
      "     Training Step: 75 Training Loss: 3.074615478515625 \n",
      "     Training Step: 76 Training Loss: 2.9552552700042725 \n",
      "     Training Step: 77 Training Loss: 3.334071159362793 \n",
      "     Training Step: 78 Training Loss: 2.0345211029052734 \n",
      "     Training Step: 79 Training Loss: 2.5237512588500977 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.2679333686828613 \n",
      "     Validation Step: 1 Validation Loss: 2.8829739093780518 \n",
      "     Validation Step: 2 Validation Loss: 2.9843480587005615 \n",
      "     Validation Step: 3 Validation Loss: 3.3269217014312744 \n",
      "     Validation Step: 4 Validation Loss: 3.7905521392822266 \n",
      "     Validation Step: 5 Validation Loss: 3.7847371101379395 \n",
      "     Validation Step: 6 Validation Loss: 3.117025136947632 \n",
      "     Validation Step: 7 Validation Loss: 3.5869083404541016 \n",
      "     Validation Step: 8 Validation Loss: 2.8452723026275635 \n",
      "     Validation Step: 9 Validation Loss: 3.0112905502319336 \n",
      "     Validation Step: 10 Validation Loss: 3.1604223251342773 \n",
      "     Validation Step: 11 Validation Loss: 3.6508336067199707 \n",
      "     Validation Step: 12 Validation Loss: 2.188167095184326 \n",
      "     Validation Step: 13 Validation Loss: 3.1145284175872803 \n",
      "     Validation Step: 14 Validation Loss: 3.9410974979400635 \n",
      "Epoch: 93\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.0471529960632324 \n",
      "     Training Step: 1 Training Loss: 2.983989715576172 \n",
      "     Training Step: 2 Training Loss: 2.4976611137390137 \n",
      "     Training Step: 3 Training Loss: 2.798600196838379 \n",
      "     Training Step: 4 Training Loss: 2.63261079788208 \n",
      "     Training Step: 5 Training Loss: 3.7388672828674316 \n",
      "     Training Step: 6 Training Loss: 3.4831905364990234 \n",
      "     Training Step: 7 Training Loss: 3.9031267166137695 \n",
      "     Training Step: 8 Training Loss: 3.754131317138672 \n",
      "     Training Step: 9 Training Loss: 2.219414710998535 \n",
      "     Training Step: 10 Training Loss: 2.3000073432922363 \n",
      "     Training Step: 11 Training Loss: 3.392242908477783 \n",
      "     Training Step: 12 Training Loss: 3.468581199645996 \n",
      "     Training Step: 13 Training Loss: 2.7719650268554688 \n",
      "     Training Step: 14 Training Loss: 2.6266613006591797 \n",
      "     Training Step: 15 Training Loss: 2.3738622665405273 \n",
      "     Training Step: 16 Training Loss: 3.091411590576172 \n",
      "     Training Step: 17 Training Loss: 2.7690205574035645 \n",
      "     Training Step: 18 Training Loss: 4.012923240661621 \n",
      "     Training Step: 19 Training Loss: 2.941812515258789 \n",
      "     Training Step: 20 Training Loss: 2.6749765872955322 \n",
      "     Training Step: 21 Training Loss: 2.8623995780944824 \n",
      "     Training Step: 22 Training Loss: 2.5986106395721436 \n",
      "     Training Step: 23 Training Loss: 2.503997802734375 \n",
      "     Training Step: 24 Training Loss: 2.599747657775879 \n",
      "     Training Step: 25 Training Loss: 2.4378957748413086 \n",
      "     Training Step: 26 Training Loss: 2.158644199371338 \n",
      "     Training Step: 27 Training Loss: 3.543285369873047 \n",
      "     Training Step: 28 Training Loss: 4.0711894035339355 \n",
      "     Training Step: 29 Training Loss: 3.3700294494628906 \n",
      "     Training Step: 30 Training Loss: 4.6336259841918945 \n",
      "     Training Step: 31 Training Loss: 3.3354527950286865 \n",
      "     Training Step: 32 Training Loss: 2.8491289615631104 \n",
      "     Training Step: 33 Training Loss: 3.518646240234375 \n",
      "     Training Step: 34 Training Loss: 3.1786370277404785 \n",
      "     Training Step: 35 Training Loss: 3.2034850120544434 \n",
      "     Training Step: 36 Training Loss: 3.330155372619629 \n",
      "     Training Step: 37 Training Loss: 2.236557960510254 \n",
      "     Training Step: 38 Training Loss: 3.6649527549743652 \n",
      "     Training Step: 39 Training Loss: 2.8270535469055176 \n",
      "     Training Step: 40 Training Loss: 2.935959815979004 \n",
      "     Training Step: 41 Training Loss: 2.1568174362182617 \n",
      "     Training Step: 42 Training Loss: 3.53836989402771 \n",
      "     Training Step: 43 Training Loss: 3.117283821105957 \n",
      "     Training Step: 44 Training Loss: 3.2995429039001465 \n",
      "     Training Step: 45 Training Loss: 2.6591944694519043 \n",
      "     Training Step: 46 Training Loss: 4.455589294433594 \n",
      "     Training Step: 47 Training Loss: 2.591536045074463 \n",
      "     Training Step: 48 Training Loss: 2.675752639770508 \n",
      "     Training Step: 49 Training Loss: 2.8767037391662598 \n",
      "     Training Step: 50 Training Loss: 2.9782843589782715 \n",
      "     Training Step: 51 Training Loss: 2.739121913909912 \n",
      "     Training Step: 52 Training Loss: 3.267369031906128 \n",
      "     Training Step: 53 Training Loss: 3.2163238525390625 \n",
      "     Training Step: 54 Training Loss: 2.3039259910583496 \n",
      "     Training Step: 55 Training Loss: 2.3013124465942383 \n",
      "     Training Step: 56 Training Loss: 2.3767664432525635 \n",
      "     Training Step: 57 Training Loss: 3.0753636360168457 \n",
      "     Training Step: 58 Training Loss: 4.376291751861572 \n",
      "     Training Step: 59 Training Loss: 2.3059329986572266 \n",
      "     Training Step: 60 Training Loss: 3.3048148155212402 \n",
      "     Training Step: 61 Training Loss: 2.602348804473877 \n",
      "     Training Step: 62 Training Loss: 2.8361477851867676 \n",
      "     Training Step: 63 Training Loss: 2.3813648223876953 \n",
      "     Training Step: 64 Training Loss: 2.870296001434326 \n",
      "     Training Step: 65 Training Loss: 2.9352962970733643 \n",
      "     Training Step: 66 Training Loss: 3.288163185119629 \n",
      "     Training Step: 67 Training Loss: 2.855195999145508 \n",
      "     Training Step: 68 Training Loss: 2.8288474082946777 \n",
      "     Training Step: 69 Training Loss: 2.258194923400879 \n",
      "     Training Step: 70 Training Loss: 2.8165688514709473 \n",
      "     Training Step: 71 Training Loss: 2.094914436340332 \n",
      "     Training Step: 72 Training Loss: 2.457568407058716 \n",
      "     Training Step: 73 Training Loss: 3.874701976776123 \n",
      "     Training Step: 74 Training Loss: 3.496281623840332 \n",
      "     Training Step: 75 Training Loss: 3.001044511795044 \n",
      "     Training Step: 76 Training Loss: 2.809234142303467 \n",
      "     Training Step: 77 Training Loss: 3.2202353477478027 \n",
      "     Training Step: 78 Training Loss: 3.0168557167053223 \n",
      "     Training Step: 79 Training Loss: 3.1127381324768066 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.077273368835449 \n",
      "     Validation Step: 1 Validation Loss: 3.0265166759490967 \n",
      "     Validation Step: 2 Validation Loss: 3.0707626342773438 \n",
      "     Validation Step: 3 Validation Loss: 3.3024797439575195 \n",
      "     Validation Step: 4 Validation Loss: 4.126578330993652 \n",
      "     Validation Step: 5 Validation Loss: 2.953603744506836 \n",
      "     Validation Step: 6 Validation Loss: 3.4864113330841064 \n",
      "     Validation Step: 7 Validation Loss: 2.2104203701019287 \n",
      "     Validation Step: 8 Validation Loss: 3.8009068965911865 \n",
      "     Validation Step: 9 Validation Loss: 3.213028907775879 \n",
      "     Validation Step: 10 Validation Loss: 3.8550689220428467 \n",
      "     Validation Step: 11 Validation Loss: 3.293156623840332 \n",
      "     Validation Step: 12 Validation Loss: 3.7870893478393555 \n",
      "     Validation Step: 13 Validation Loss: 3.138817310333252 \n",
      "     Validation Step: 14 Validation Loss: 3.0465493202209473 \n",
      "Epoch: 94\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.9670426845550537 \n",
      "     Training Step: 1 Training Loss: 2.7002851963043213 \n",
      "     Training Step: 2 Training Loss: 2.884587526321411 \n",
      "     Training Step: 3 Training Loss: 2.969695568084717 \n",
      "     Training Step: 4 Training Loss: 3.8109147548675537 \n",
      "     Training Step: 5 Training Loss: 3.292884349822998 \n",
      "     Training Step: 6 Training Loss: 2.957505464553833 \n",
      "     Training Step: 7 Training Loss: 2.7427570819854736 \n",
      "     Training Step: 8 Training Loss: 3.714500665664673 \n",
      "     Training Step: 9 Training Loss: 2.713592052459717 \n",
      "     Training Step: 10 Training Loss: 2.569615364074707 \n",
      "     Training Step: 11 Training Loss: 2.6112117767333984 \n",
      "     Training Step: 12 Training Loss: 2.598848581314087 \n",
      "     Training Step: 13 Training Loss: 3.5122106075286865 \n",
      "     Training Step: 14 Training Loss: 3.274158477783203 \n",
      "     Training Step: 15 Training Loss: 3.510654926300049 \n",
      "     Training Step: 16 Training Loss: 2.197030544281006 \n",
      "     Training Step: 17 Training Loss: 2.8900578022003174 \n",
      "     Training Step: 18 Training Loss: 2.5531387329101562 \n",
      "     Training Step: 19 Training Loss: 2.671408176422119 \n",
      "     Training Step: 20 Training Loss: 2.9169771671295166 \n",
      "     Training Step: 21 Training Loss: 4.254810810089111 \n",
      "     Training Step: 22 Training Loss: 3.578528881072998 \n",
      "     Training Step: 23 Training Loss: 3.5517826080322266 \n",
      "     Training Step: 24 Training Loss: 3.4034316539764404 \n",
      "     Training Step: 25 Training Loss: 2.518969774246216 \n",
      "     Training Step: 26 Training Loss: 3.301757574081421 \n",
      "     Training Step: 27 Training Loss: 3.4059624671936035 \n",
      "     Training Step: 28 Training Loss: 2.1404895782470703 \n",
      "     Training Step: 29 Training Loss: 3.3725879192352295 \n",
      "     Training Step: 30 Training Loss: 3.10347843170166 \n",
      "     Training Step: 31 Training Loss: 2.863105297088623 \n",
      "     Training Step: 32 Training Loss: 2.561826467514038 \n",
      "     Training Step: 33 Training Loss: 2.511902332305908 \n",
      "     Training Step: 34 Training Loss: 2.8780243396759033 \n",
      "     Training Step: 35 Training Loss: 2.853890895843506 \n",
      "     Training Step: 36 Training Loss: 2.9024460315704346 \n",
      "     Training Step: 37 Training Loss: 3.3248353004455566 \n",
      "     Training Step: 38 Training Loss: 2.1493310928344727 \n",
      "     Training Step: 39 Training Loss: 2.411881446838379 \n",
      "     Training Step: 40 Training Loss: 2.8805348873138428 \n",
      "     Training Step: 41 Training Loss: 3.3364267349243164 \n",
      "     Training Step: 42 Training Loss: 2.1958730220794678 \n",
      "     Training Step: 43 Training Loss: 2.4041285514831543 \n",
      "     Training Step: 44 Training Loss: 2.3913965225219727 \n",
      "     Training Step: 45 Training Loss: 4.313570976257324 \n",
      "     Training Step: 46 Training Loss: 2.2303309440612793 \n",
      "     Training Step: 47 Training Loss: 2.1565632820129395 \n",
      "     Training Step: 48 Training Loss: 3.3235881328582764 \n",
      "     Training Step: 49 Training Loss: 2.734705924987793 \n",
      "     Training Step: 50 Training Loss: 2.4405150413513184 \n",
      "     Training Step: 51 Training Loss: 3.6935620307922363 \n",
      "     Training Step: 52 Training Loss: 3.1352906227111816 \n",
      "     Training Step: 53 Training Loss: 2.5078606605529785 \n",
      "     Training Step: 54 Training Loss: 2.1333327293395996 \n",
      "     Training Step: 55 Training Loss: 2.227288246154785 \n",
      "     Training Step: 56 Training Loss: 2.7495532035827637 \n",
      "     Training Step: 57 Training Loss: 3.7949774265289307 \n",
      "     Training Step: 58 Training Loss: 3.154949426651001 \n",
      "     Training Step: 59 Training Loss: 4.333450794219971 \n",
      "     Training Step: 60 Training Loss: 4.659541130065918 \n",
      "     Training Step: 61 Training Loss: 3.658644676208496 \n",
      "     Training Step: 62 Training Loss: 2.850323438644409 \n",
      "     Training Step: 63 Training Loss: 3.443662643432617 \n",
      "     Training Step: 64 Training Loss: 2.9152631759643555 \n",
      "     Training Step: 65 Training Loss: 2.350405216217041 \n",
      "     Training Step: 66 Training Loss: 2.4379186630249023 \n",
      "     Training Step: 67 Training Loss: 3.865067481994629 \n",
      "     Training Step: 68 Training Loss: 3.799654960632324 \n",
      "     Training Step: 69 Training Loss: 3.01965069770813 \n",
      "     Training Step: 70 Training Loss: 2.79600191116333 \n",
      "     Training Step: 71 Training Loss: 3.5627098083496094 \n",
      "     Training Step: 72 Training Loss: 2.9297661781311035 \n",
      "     Training Step: 73 Training Loss: 2.900482177734375 \n",
      "     Training Step: 74 Training Loss: 3.0525569915771484 \n",
      "     Training Step: 75 Training Loss: 3.1590216159820557 \n",
      "     Training Step: 76 Training Loss: 3.76373291015625 \n",
      "     Training Step: 77 Training Loss: 2.2688355445861816 \n",
      "     Training Step: 78 Training Loss: 2.96988844871521 \n",
      "     Training Step: 79 Training Loss: 2.9551801681518555 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6528444290161133 \n",
      "     Validation Step: 1 Validation Loss: 3.154562473297119 \n",
      "     Validation Step: 2 Validation Loss: 3.2980234622955322 \n",
      "     Validation Step: 3 Validation Loss: 3.0490777492523193 \n",
      "     Validation Step: 4 Validation Loss: 3.705639123916626 \n",
      "     Validation Step: 5 Validation Loss: 3.0770559310913086 \n",
      "     Validation Step: 6 Validation Loss: 2.2623448371887207 \n",
      "     Validation Step: 7 Validation Loss: 2.868234634399414 \n",
      "     Validation Step: 8 Validation Loss: 3.675847291946411 \n",
      "     Validation Step: 9 Validation Loss: 2.775637626647949 \n",
      "     Validation Step: 10 Validation Loss: 4.001750946044922 \n",
      "     Validation Step: 11 Validation Loss: 3.079986333847046 \n",
      "     Validation Step: 12 Validation Loss: 3.1032891273498535 \n",
      "     Validation Step: 13 Validation Loss: 3.6073131561279297 \n",
      "     Validation Step: 14 Validation Loss: 2.7147130966186523 \n",
      "Epoch: 95\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.8583121299743652 \n",
      "     Training Step: 1 Training Loss: 3.295095205307007 \n",
      "     Training Step: 2 Training Loss: 2.862745761871338 \n",
      "     Training Step: 3 Training Loss: 3.5557150840759277 \n",
      "     Training Step: 4 Training Loss: 2.464712142944336 \n",
      "     Training Step: 5 Training Loss: 2.177156686782837 \n",
      "     Training Step: 6 Training Loss: 4.315672397613525 \n",
      "     Training Step: 7 Training Loss: 3.3713631629943848 \n",
      "     Training Step: 8 Training Loss: 2.8379945755004883 \n",
      "     Training Step: 9 Training Loss: 3.0145883560180664 \n",
      "     Training Step: 10 Training Loss: 3.7620458602905273 \n",
      "     Training Step: 11 Training Loss: 2.5046887397766113 \n",
      "     Training Step: 12 Training Loss: 3.012058973312378 \n",
      "     Training Step: 13 Training Loss: 2.4222989082336426 \n",
      "     Training Step: 14 Training Loss: 2.977379322052002 \n",
      "     Training Step: 15 Training Loss: 2.372685432434082 \n",
      "     Training Step: 16 Training Loss: 2.609159469604492 \n",
      "     Training Step: 17 Training Loss: 3.6116795539855957 \n",
      "     Training Step: 18 Training Loss: 2.8499772548675537 \n",
      "     Training Step: 19 Training Loss: 3.299973726272583 \n",
      "     Training Step: 20 Training Loss: 3.6393661499023438 \n",
      "     Training Step: 21 Training Loss: 3.791935682296753 \n",
      "     Training Step: 22 Training Loss: 2.1753053665161133 \n",
      "     Training Step: 23 Training Loss: 2.6070032119750977 \n",
      "     Training Step: 24 Training Loss: 3.263322114944458 \n",
      "     Training Step: 25 Training Loss: 2.4951846599578857 \n",
      "     Training Step: 26 Training Loss: 3.0768537521362305 \n",
      "     Training Step: 27 Training Loss: 3.182544708251953 \n",
      "     Training Step: 28 Training Loss: 2.1218910217285156 \n",
      "     Training Step: 29 Training Loss: 2.952756643295288 \n",
      "     Training Step: 30 Training Loss: 3.2013494968414307 \n",
      "     Training Step: 31 Training Loss: 2.2699193954467773 \n",
      "     Training Step: 32 Training Loss: 4.03354549407959 \n",
      "     Training Step: 33 Training Loss: 3.279359817504883 \n",
      "     Training Step: 34 Training Loss: 2.605349540710449 \n",
      "     Training Step: 35 Training Loss: 3.933833122253418 \n",
      "     Training Step: 36 Training Loss: 2.554924964904785 \n",
      "     Training Step: 37 Training Loss: 3.597170829772949 \n",
      "     Training Step: 38 Training Loss: 4.542297840118408 \n",
      "     Training Step: 39 Training Loss: 2.385375738143921 \n",
      "     Training Step: 40 Training Loss: 3.1412456035614014 \n",
      "     Training Step: 41 Training Loss: 2.257488250732422 \n",
      "     Training Step: 42 Training Loss: 2.5134968757629395 \n",
      "     Training Step: 43 Training Loss: 3.362363815307617 \n",
      "     Training Step: 44 Training Loss: 3.353365421295166 \n",
      "     Training Step: 45 Training Loss: 2.870673418045044 \n",
      "     Training Step: 46 Training Loss: 2.9096415042877197 \n",
      "     Training Step: 47 Training Loss: 3.3750338554382324 \n",
      "     Training Step: 48 Training Loss: 2.389126777648926 \n",
      "     Training Step: 49 Training Loss: 3.2287817001342773 \n",
      "     Training Step: 50 Training Loss: 2.484628677368164 \n",
      "     Training Step: 51 Training Loss: 3.0429000854492188 \n",
      "     Training Step: 52 Training Loss: 2.546217441558838 \n",
      "     Training Step: 53 Training Loss: 3.392007827758789 \n",
      "     Training Step: 54 Training Loss: 2.397698402404785 \n",
      "     Training Step: 55 Training Loss: 3.2518975734710693 \n",
      "     Training Step: 56 Training Loss: 2.4193530082702637 \n",
      "     Training Step: 57 Training Loss: 3.1101319789886475 \n",
      "     Training Step: 58 Training Loss: 2.635831832885742 \n",
      "     Training Step: 59 Training Loss: 3.7047510147094727 \n",
      "     Training Step: 60 Training Loss: 2.4561104774475098 \n",
      "     Training Step: 61 Training Loss: 4.1928300857543945 \n",
      "     Training Step: 62 Training Loss: 3.446777820587158 \n",
      "     Training Step: 63 Training Loss: 2.701181411743164 \n",
      "     Training Step: 64 Training Loss: 3.0227956771850586 \n",
      "     Training Step: 65 Training Loss: 2.505579948425293 \n",
      "     Training Step: 66 Training Loss: 3.0744972229003906 \n",
      "     Training Step: 67 Training Loss: 2.162123680114746 \n",
      "     Training Step: 68 Training Loss: 3.296544075012207 \n",
      "     Training Step: 69 Training Loss: 2.768488883972168 \n",
      "     Training Step: 70 Training Loss: 2.4275803565979004 \n",
      "     Training Step: 71 Training Loss: 2.835064172744751 \n",
      "     Training Step: 72 Training Loss: 3.3759846687316895 \n",
      "     Training Step: 73 Training Loss: 2.5910439491271973 \n",
      "     Training Step: 74 Training Loss: 2.733579158782959 \n",
      "     Training Step: 75 Training Loss: 2.718569755554199 \n",
      "     Training Step: 76 Training Loss: 2.3161282539367676 \n",
      "     Training Step: 77 Training Loss: 3.5690832138061523 \n",
      "     Training Step: 78 Training Loss: 2.9154417514801025 \n",
      "     Training Step: 79 Training Loss: 3.151583671569824 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.257290363311768 \n",
      "     Validation Step: 1 Validation Loss: 3.7372124195098877 \n",
      "     Validation Step: 2 Validation Loss: 3.2424964904785156 \n",
      "     Validation Step: 3 Validation Loss: 3.282809019088745 \n",
      "     Validation Step: 4 Validation Loss: 2.975864887237549 \n",
      "     Validation Step: 5 Validation Loss: 3.821629524230957 \n",
      "     Validation Step: 6 Validation Loss: 3.208620071411133 \n",
      "     Validation Step: 7 Validation Loss: 2.931774377822876 \n",
      "     Validation Step: 8 Validation Loss: 3.5545334815979004 \n",
      "     Validation Step: 9 Validation Loss: 3.849673271179199 \n",
      "     Validation Step: 10 Validation Loss: 2.924466371536255 \n",
      "     Validation Step: 11 Validation Loss: 2.817079544067383 \n",
      "     Validation Step: 12 Validation Loss: 3.086731433868408 \n",
      "     Validation Step: 13 Validation Loss: 2.1978683471679688 \n",
      "     Validation Step: 14 Validation Loss: 3.27976655960083 \n",
      "Epoch: 96\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.1758575439453125 \n",
      "     Training Step: 1 Training Loss: 4.284124851226807 \n",
      "     Training Step: 2 Training Loss: 2.6659374237060547 \n",
      "     Training Step: 3 Training Loss: 2.3084375858306885 \n",
      "     Training Step: 4 Training Loss: 3.096527576446533 \n",
      "     Training Step: 5 Training Loss: 2.6934657096862793 \n",
      "     Training Step: 6 Training Loss: 2.4071359634399414 \n",
      "     Training Step: 7 Training Loss: 2.6044068336486816 \n",
      "     Training Step: 8 Training Loss: 2.2021291255950928 \n",
      "     Training Step: 9 Training Loss: 3.578951358795166 \n",
      "     Training Step: 10 Training Loss: 2.1252593994140625 \n",
      "     Training Step: 11 Training Loss: 2.7547709941864014 \n",
      "     Training Step: 12 Training Loss: 2.135944366455078 \n",
      "     Training Step: 13 Training Loss: 3.1926052570343018 \n",
      "     Training Step: 14 Training Loss: 3.679718017578125 \n",
      "     Training Step: 15 Training Loss: 3.030738353729248 \n",
      "     Training Step: 16 Training Loss: 3.414719581604004 \n",
      "     Training Step: 17 Training Loss: 2.3570032119750977 \n",
      "     Training Step: 18 Training Loss: 4.139590740203857 \n",
      "     Training Step: 19 Training Loss: 4.075984477996826 \n",
      "     Training Step: 20 Training Loss: 2.4463725090026855 \n",
      "     Training Step: 21 Training Loss: 4.15069055557251 \n",
      "     Training Step: 22 Training Loss: 3.198460817337036 \n",
      "     Training Step: 23 Training Loss: 4.526805400848389 \n",
      "     Training Step: 24 Training Loss: 3.0606436729431152 \n",
      "     Training Step: 25 Training Loss: 3.30251407623291 \n",
      "     Training Step: 26 Training Loss: 2.9501514434814453 \n",
      "     Training Step: 27 Training Loss: 3.453403949737549 \n",
      "     Training Step: 28 Training Loss: 2.9376566410064697 \n",
      "     Training Step: 29 Training Loss: 2.5296339988708496 \n",
      "     Training Step: 30 Training Loss: 2.867239236831665 \n",
      "     Training Step: 31 Training Loss: 3.2762131690979004 \n",
      "     Training Step: 32 Training Loss: 3.6220033168792725 \n",
      "     Training Step: 33 Training Loss: 2.9234824180603027 \n",
      "     Training Step: 34 Training Loss: 2.952936887741089 \n",
      "     Training Step: 35 Training Loss: 3.33876895904541 \n",
      "     Training Step: 36 Training Loss: 2.399492025375366 \n",
      "     Training Step: 37 Training Loss: 3.0928232669830322 \n",
      "     Training Step: 38 Training Loss: 2.5671205520629883 \n",
      "     Training Step: 39 Training Loss: 2.584242582321167 \n",
      "     Training Step: 40 Training Loss: 3.219764232635498 \n",
      "     Training Step: 41 Training Loss: 2.3149404525756836 \n",
      "     Training Step: 42 Training Loss: 2.4791226387023926 \n",
      "     Training Step: 43 Training Loss: 3.499415397644043 \n",
      "     Training Step: 44 Training Loss: 2.822922706604004 \n",
      "     Training Step: 45 Training Loss: 3.3246803283691406 \n",
      "     Training Step: 46 Training Loss: 3.484299659729004 \n",
      "     Training Step: 47 Training Loss: 3.0602645874023438 \n",
      "     Training Step: 48 Training Loss: 3.5649166107177734 \n",
      "     Training Step: 49 Training Loss: 2.500967264175415 \n",
      "     Training Step: 50 Training Loss: 3.0460660457611084 \n",
      "     Training Step: 51 Training Loss: 2.835379123687744 \n",
      "     Training Step: 52 Training Loss: 2.872720241546631 \n",
      "     Training Step: 53 Training Loss: 2.34902024269104 \n",
      "     Training Step: 54 Training Loss: 3.209944248199463 \n",
      "     Training Step: 55 Training Loss: 3.4260239601135254 \n",
      "     Training Step: 56 Training Loss: 3.202538013458252 \n",
      "     Training Step: 57 Training Loss: 4.595612525939941 \n",
      "     Training Step: 58 Training Loss: 2.737382411956787 \n",
      "     Training Step: 59 Training Loss: 3.121201515197754 \n",
      "     Training Step: 60 Training Loss: 2.3748767375946045 \n",
      "     Training Step: 61 Training Loss: 2.2261834144592285 \n",
      "     Training Step: 62 Training Loss: 2.1634671688079834 \n",
      "     Training Step: 63 Training Loss: 3.514111280441284 \n",
      "     Training Step: 64 Training Loss: 2.892747402191162 \n",
      "     Training Step: 65 Training Loss: 2.9569931030273438 \n",
      "     Training Step: 66 Training Loss: 2.743119955062866 \n",
      "     Training Step: 67 Training Loss: 3.946836471557617 \n",
      "     Training Step: 68 Training Loss: 3.710261106491089 \n",
      "     Training Step: 69 Training Loss: 2.8060967922210693 \n",
      "     Training Step: 70 Training Loss: 3.2899818420410156 \n",
      "     Training Step: 71 Training Loss: 3.5331950187683105 \n",
      "     Training Step: 72 Training Loss: 2.735991954803467 \n",
      "     Training Step: 73 Training Loss: 2.991211414337158 \n",
      "     Training Step: 74 Training Loss: 3.905003309249878 \n",
      "     Training Step: 75 Training Loss: 3.156360626220703 \n",
      "     Training Step: 76 Training Loss: 2.2835655212402344 \n",
      "     Training Step: 77 Training Loss: 3.0408923625946045 \n",
      "     Training Step: 78 Training Loss: 2.094468116760254 \n",
      "     Training Step: 79 Training Loss: 3.3355913162231445 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.629755973815918 \n",
      "     Validation Step: 1 Validation Loss: 3.0910205841064453 \n",
      "     Validation Step: 2 Validation Loss: 2.762714385986328 \n",
      "     Validation Step: 3 Validation Loss: 3.6323137283325195 \n",
      "     Validation Step: 4 Validation Loss: 2.925386428833008 \n",
      "     Validation Step: 5 Validation Loss: 3.521636962890625 \n",
      "     Validation Step: 6 Validation Loss: 3.553925037384033 \n",
      "     Validation Step: 7 Validation Loss: 3.8088316917419434 \n",
      "     Validation Step: 8 Validation Loss: 3.6608424186706543 \n",
      "     Validation Step: 9 Validation Loss: 2.9250786304473877 \n",
      "     Validation Step: 10 Validation Loss: 3.266768455505371 \n",
      "     Validation Step: 11 Validation Loss: 2.671116828918457 \n",
      "     Validation Step: 12 Validation Loss: 2.2161459922790527 \n",
      "     Validation Step: 13 Validation Loss: 3.2083332538604736 \n",
      "     Validation Step: 14 Validation Loss: 3.1086018085479736 \n",
      "Epoch: 97\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.246814727783203 \n",
      "     Training Step: 1 Training Loss: 3.272238254547119 \n",
      "     Training Step: 2 Training Loss: 2.279426097869873 \n",
      "     Training Step: 3 Training Loss: 2.3315892219543457 \n",
      "     Training Step: 4 Training Loss: 2.2442901134490967 \n",
      "     Training Step: 5 Training Loss: 3.149986505508423 \n",
      "     Training Step: 6 Training Loss: 3.669562339782715 \n",
      "     Training Step: 7 Training Loss: 2.352847099304199 \n",
      "     Training Step: 8 Training Loss: 2.9220893383026123 \n",
      "     Training Step: 9 Training Loss: 3.1691155433654785 \n",
      "     Training Step: 10 Training Loss: 2.5120580196380615 \n",
      "     Training Step: 11 Training Loss: 2.969611644744873 \n",
      "     Training Step: 12 Training Loss: 2.253098964691162 \n",
      "     Training Step: 13 Training Loss: 3.134154796600342 \n",
      "     Training Step: 14 Training Loss: 3.185368061065674 \n",
      "     Training Step: 15 Training Loss: 2.365828275680542 \n",
      "     Training Step: 16 Training Loss: 3.4464943408966064 \n",
      "     Training Step: 17 Training Loss: 3.1904854774475098 \n",
      "     Training Step: 18 Training Loss: 2.4177446365356445 \n",
      "     Training Step: 19 Training Loss: 3.1759872436523438 \n",
      "     Training Step: 20 Training Loss: 2.0460774898529053 \n",
      "     Training Step: 21 Training Loss: 4.080564975738525 \n",
      "     Training Step: 22 Training Loss: 2.9302821159362793 \n",
      "     Training Step: 23 Training Loss: 3.3286757469177246 \n",
      "     Training Step: 24 Training Loss: 4.2854509353637695 \n",
      "     Training Step: 25 Training Loss: 2.4850125312805176 \n",
      "     Training Step: 26 Training Loss: 4.031179428100586 \n",
      "     Training Step: 27 Training Loss: 2.484126329421997 \n",
      "     Training Step: 28 Training Loss: 3.1442651748657227 \n",
      "     Training Step: 29 Training Loss: 3.930412530899048 \n",
      "     Training Step: 30 Training Loss: 2.5568180084228516 \n",
      "     Training Step: 31 Training Loss: 2.172818660736084 \n",
      "     Training Step: 32 Training Loss: 2.973607063293457 \n",
      "     Training Step: 33 Training Loss: 2.8013994693756104 \n",
      "     Training Step: 34 Training Loss: 2.985851287841797 \n",
      "     Training Step: 35 Training Loss: 2.5748181343078613 \n",
      "     Training Step: 36 Training Loss: 2.8826608657836914 \n",
      "     Training Step: 37 Training Loss: 3.0155258178710938 \n",
      "     Training Step: 38 Training Loss: 2.9275832176208496 \n",
      "     Training Step: 39 Training Loss: 3.397418737411499 \n",
      "     Training Step: 40 Training Loss: 4.59384298324585 \n",
      "     Training Step: 41 Training Loss: 2.7990071773529053 \n",
      "     Training Step: 42 Training Loss: 2.6060588359832764 \n",
      "     Training Step: 43 Training Loss: 3.3015689849853516 \n",
      "     Training Step: 44 Training Loss: 2.562263250350952 \n",
      "     Training Step: 45 Training Loss: 2.3964319229125977 \n",
      "     Training Step: 46 Training Loss: 2.7565078735351562 \n",
      "     Training Step: 47 Training Loss: 2.721717357635498 \n",
      "     Training Step: 48 Training Loss: 3.4330811500549316 \n",
      "     Training Step: 49 Training Loss: 3.721797227859497 \n",
      "     Training Step: 50 Training Loss: 2.7389798164367676 \n",
      "     Training Step: 51 Training Loss: 2.178673028945923 \n",
      "     Training Step: 52 Training Loss: 3.309818744659424 \n",
      "     Training Step: 53 Training Loss: 3.249119997024536 \n",
      "     Training Step: 54 Training Loss: 3.312403678894043 \n",
      "     Training Step: 55 Training Loss: 3.12839412689209 \n",
      "     Training Step: 56 Training Loss: 2.519084930419922 \n",
      "     Training Step: 57 Training Loss: 3.585444450378418 \n",
      "     Training Step: 58 Training Loss: 2.7213943004608154 \n",
      "     Training Step: 59 Training Loss: 3.7406017780303955 \n",
      "     Training Step: 60 Training Loss: 2.793750762939453 \n",
      "     Training Step: 61 Training Loss: 2.390522003173828 \n",
      "     Training Step: 62 Training Loss: 3.254948377609253 \n",
      "     Training Step: 63 Training Loss: 3.0974299907684326 \n",
      "     Training Step: 64 Training Loss: 3.4352593421936035 \n",
      "     Training Step: 65 Training Loss: 2.6905336380004883 \n",
      "     Training Step: 66 Training Loss: 3.144760847091675 \n",
      "     Training Step: 67 Training Loss: 4.368508815765381 \n",
      "     Training Step: 68 Training Loss: 3.0772337913513184 \n",
      "     Training Step: 69 Training Loss: 3.524649143218994 \n",
      "     Training Step: 70 Training Loss: 3.1486244201660156 \n",
      "     Training Step: 71 Training Loss: 2.5672764778137207 \n",
      "     Training Step: 72 Training Loss: 3.635768413543701 \n",
      "     Training Step: 73 Training Loss: 2.6763389110565186 \n",
      "     Training Step: 74 Training Loss: 3.046825885772705 \n",
      "     Training Step: 75 Training Loss: 3.4799134731292725 \n",
      "     Training Step: 76 Training Loss: 2.784715175628662 \n",
      "     Training Step: 77 Training Loss: 2.073082447052002 \n",
      "     Training Step: 78 Training Loss: 2.4835476875305176 \n",
      "     Training Step: 79 Training Loss: 2.2348101139068604 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.5835700035095215 \n",
      "     Validation Step: 1 Validation Loss: 2.942688465118408 \n",
      "     Validation Step: 2 Validation Loss: 3.0685393810272217 \n",
      "     Validation Step: 3 Validation Loss: 3.654419183731079 \n",
      "     Validation Step: 4 Validation Loss: 2.8900294303894043 \n",
      "     Validation Step: 5 Validation Loss: 2.6563587188720703 \n",
      "     Validation Step: 6 Validation Loss: 2.893819570541382 \n",
      "     Validation Step: 7 Validation Loss: 3.128994941711426 \n",
      "     Validation Step: 8 Validation Loss: 3.3399057388305664 \n",
      "     Validation Step: 9 Validation Loss: 3.5879364013671875 \n",
      "     Validation Step: 10 Validation Loss: 2.354341983795166 \n",
      "     Validation Step: 11 Validation Loss: 3.2836475372314453 \n",
      "     Validation Step: 12 Validation Loss: 3.4041690826416016 \n",
      "     Validation Step: 13 Validation Loss: 3.621255397796631 \n",
      "     Validation Step: 14 Validation Loss: 2.5139718055725098 \n",
      "---------------   LEARNING RATE HALVING DOWN TO: 1.6000000000000004e-06   ---------------\n",
      "---------------   CURRENT LEARNING RATE: 1.6000000000000004e-06   ---------------\n",
      "Epoch: 98\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.4236338138580322 \n",
      "     Training Step: 1 Training Loss: 3.143789291381836 \n",
      "     Training Step: 2 Training Loss: 3.2711963653564453 \n",
      "     Training Step: 3 Training Loss: 2.865234851837158 \n",
      "     Training Step: 4 Training Loss: 2.8679311275482178 \n",
      "     Training Step: 5 Training Loss: 3.8365113735198975 \n",
      "     Training Step: 6 Training Loss: 2.526322841644287 \n",
      "     Training Step: 7 Training Loss: 4.090980529785156 \n",
      "     Training Step: 8 Training Loss: 2.4858005046844482 \n",
      "     Training Step: 9 Training Loss: 2.515143871307373 \n",
      "     Training Step: 10 Training Loss: 4.2565178871154785 \n",
      "     Training Step: 11 Training Loss: 3.291633129119873 \n",
      "     Training Step: 12 Training Loss: 3.1797170639038086 \n",
      "     Training Step: 13 Training Loss: 3.6083452701568604 \n",
      "     Training Step: 14 Training Loss: 2.137549638748169 \n",
      "     Training Step: 15 Training Loss: 2.765690803527832 \n",
      "     Training Step: 16 Training Loss: 2.3060436248779297 \n",
      "     Training Step: 17 Training Loss: 3.195580005645752 \n",
      "     Training Step: 18 Training Loss: 3.8886706829071045 \n",
      "     Training Step: 19 Training Loss: 2.854132890701294 \n",
      "     Training Step: 20 Training Loss: 2.619267463684082 \n",
      "     Training Step: 21 Training Loss: 3.2590270042419434 \n",
      "     Training Step: 22 Training Loss: 3.7726383209228516 \n",
      "     Training Step: 23 Training Loss: 3.7494616508483887 \n",
      "     Training Step: 24 Training Loss: 2.5857319831848145 \n",
      "     Training Step: 25 Training Loss: 2.6635653972625732 \n",
      "     Training Step: 26 Training Loss: 2.768526792526245 \n",
      "     Training Step: 27 Training Loss: 3.5535919666290283 \n",
      "     Training Step: 28 Training Loss: 2.425311326980591 \n",
      "     Training Step: 29 Training Loss: 3.7307281494140625 \n",
      "     Training Step: 30 Training Loss: 3.315551280975342 \n",
      "     Training Step: 31 Training Loss: 3.739380359649658 \n",
      "     Training Step: 32 Training Loss: 3.48099422454834 \n",
      "     Training Step: 33 Training Loss: 3.389674425125122 \n",
      "     Training Step: 34 Training Loss: 3.250063180923462 \n",
      "     Training Step: 35 Training Loss: 3.0135576725006104 \n",
      "     Training Step: 36 Training Loss: 2.794208526611328 \n",
      "     Training Step: 37 Training Loss: 3.0352742671966553 \n",
      "     Training Step: 38 Training Loss: 2.8439931869506836 \n",
      "     Training Step: 39 Training Loss: 4.754212379455566 \n",
      "     Training Step: 40 Training Loss: 2.970381736755371 \n",
      "     Training Step: 41 Training Loss: 3.3939337730407715 \n",
      "     Training Step: 42 Training Loss: 3.043278217315674 \n",
      "     Training Step: 43 Training Loss: 2.589749813079834 \n",
      "     Training Step: 44 Training Loss: 3.233120918273926 \n",
      "     Training Step: 45 Training Loss: 2.5819966793060303 \n",
      "     Training Step: 46 Training Loss: 2.9306414127349854 \n",
      "     Training Step: 47 Training Loss: 3.1567816734313965 \n",
      "     Training Step: 48 Training Loss: 2.552032947540283 \n",
      "     Training Step: 49 Training Loss: 2.9822137355804443 \n",
      "     Training Step: 50 Training Loss: 2.300781488418579 \n",
      "     Training Step: 51 Training Loss: 2.9807422161102295 \n",
      "     Training Step: 52 Training Loss: 2.898364305496216 \n",
      "     Training Step: 53 Training Loss: 3.0361528396606445 \n",
      "     Training Step: 54 Training Loss: 2.1364307403564453 \n",
      "     Training Step: 55 Training Loss: 3.1080803871154785 \n",
      "     Training Step: 56 Training Loss: 2.3438358306884766 \n",
      "     Training Step: 57 Training Loss: 2.6514174938201904 \n",
      "     Training Step: 58 Training Loss: 3.7840828895568848 \n",
      "     Training Step: 59 Training Loss: 2.4267897605895996 \n",
      "     Training Step: 60 Training Loss: 4.292131423950195 \n",
      "     Training Step: 61 Training Loss: 2.384554624557495 \n",
      "     Training Step: 62 Training Loss: 2.2435684204101562 \n",
      "     Training Step: 63 Training Loss: 2.2246885299682617 \n",
      "     Training Step: 64 Training Loss: 2.8096959590911865 \n",
      "     Training Step: 65 Training Loss: 2.7126312255859375 \n",
      "     Training Step: 66 Training Loss: 2.7508060932159424 \n",
      "     Training Step: 67 Training Loss: 3.2378430366516113 \n",
      "     Training Step: 68 Training Loss: 3.159191608428955 \n",
      "     Training Step: 69 Training Loss: 2.686997175216675 \n",
      "     Training Step: 70 Training Loss: 2.3977935314178467 \n",
      "     Training Step: 71 Training Loss: 2.8787424564361572 \n",
      "     Training Step: 72 Training Loss: 2.1733646392822266 \n",
      "     Training Step: 73 Training Loss: 3.550889492034912 \n",
      "     Training Step: 74 Training Loss: 2.269408702850342 \n",
      "     Training Step: 75 Training Loss: 2.028555154800415 \n",
      "     Training Step: 76 Training Loss: 2.482964038848877 \n",
      "     Training Step: 77 Training Loss: 2.5568182468414307 \n",
      "     Training Step: 78 Training Loss: 2.6702656745910645 \n",
      "     Training Step: 79 Training Loss: 2.9927186965942383 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.7072651386260986 \n",
      "     Validation Step: 1 Validation Loss: 2.7829554080963135 \n",
      "     Validation Step: 2 Validation Loss: 3.031236410140991 \n",
      "     Validation Step: 3 Validation Loss: 3.136042594909668 \n",
      "     Validation Step: 4 Validation Loss: 3.332934617996216 \n",
      "     Validation Step: 5 Validation Loss: 3.149477005004883 \n",
      "     Validation Step: 6 Validation Loss: 3.9500021934509277 \n",
      "     Validation Step: 7 Validation Loss: 3.1915624141693115 \n",
      "     Validation Step: 8 Validation Loss: 2.754042863845825 \n",
      "     Validation Step: 9 Validation Loss: 2.194363594055176 \n",
      "     Validation Step: 10 Validation Loss: 3.6274030208587646 \n",
      "     Validation Step: 11 Validation Loss: 3.5822763442993164 \n",
      "     Validation Step: 12 Validation Loss: 2.8847689628601074 \n",
      "     Validation Step: 13 Validation Loss: 3.648491621017456 \n",
      "     Validation Step: 14 Validation Loss: 3.059950351715088 \n",
      "Epoch: 99\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.74811053276062 \n",
      "     Training Step: 1 Training Loss: 3.464482545852661 \n",
      "     Training Step: 2 Training Loss: 2.6394898891448975 \n",
      "     Training Step: 3 Training Loss: 2.4765892028808594 \n",
      "     Training Step: 4 Training Loss: 2.7711377143859863 \n",
      "     Training Step: 5 Training Loss: 2.3915605545043945 \n",
      "     Training Step: 6 Training Loss: 3.5166666507720947 \n",
      "     Training Step: 7 Training Loss: 2.692929983139038 \n",
      "     Training Step: 8 Training Loss: 2.3649678230285645 \n",
      "     Training Step: 9 Training Loss: 2.687767267227173 \n",
      "     Training Step: 10 Training Loss: 2.2096519470214844 \n",
      "     Training Step: 11 Training Loss: 3.8083622455596924 \n",
      "     Training Step: 12 Training Loss: 2.2440192699432373 \n",
      "     Training Step: 13 Training Loss: 3.5663790702819824 \n",
      "     Training Step: 14 Training Loss: 2.4548587799072266 \n",
      "     Training Step: 15 Training Loss: 3.014925956726074 \n",
      "     Training Step: 16 Training Loss: 2.557511806488037 \n",
      "     Training Step: 17 Training Loss: 3.859846591949463 \n",
      "     Training Step: 18 Training Loss: 2.975245237350464 \n",
      "     Training Step: 19 Training Loss: 3.4168589115142822 \n",
      "     Training Step: 20 Training Loss: 3.096932888031006 \n",
      "     Training Step: 21 Training Loss: 3.819176435470581 \n",
      "     Training Step: 22 Training Loss: 2.5503759384155273 \n",
      "     Training Step: 23 Training Loss: 3.082725763320923 \n",
      "     Training Step: 24 Training Loss: 3.0812859535217285 \n",
      "     Training Step: 25 Training Loss: 3.037140130996704 \n",
      "     Training Step: 26 Training Loss: 3.5017402172088623 \n",
      "     Training Step: 27 Training Loss: 2.822924852371216 \n",
      "     Training Step: 28 Training Loss: 3.1222071647644043 \n",
      "     Training Step: 29 Training Loss: 2.9808571338653564 \n",
      "     Training Step: 30 Training Loss: 2.8266427516937256 \n",
      "     Training Step: 31 Training Loss: 3.5660252571105957 \n",
      "     Training Step: 32 Training Loss: 2.6878480911254883 \n",
      "     Training Step: 33 Training Loss: 2.621269941329956 \n",
      "     Training Step: 34 Training Loss: 2.330484390258789 \n",
      "     Training Step: 35 Training Loss: 4.1772780418396 \n",
      "     Training Step: 36 Training Loss: 3.850109577178955 \n",
      "     Training Step: 37 Training Loss: 3.1611084938049316 \n",
      "     Training Step: 38 Training Loss: 2.82487154006958 \n",
      "     Training Step: 39 Training Loss: 4.378697395324707 \n",
      "     Training Step: 40 Training Loss: 4.199602127075195 \n",
      "     Training Step: 41 Training Loss: 2.8836746215820312 \n",
      "     Training Step: 42 Training Loss: 2.1583542823791504 \n",
      "     Training Step: 43 Training Loss: 4.730981826782227 \n",
      "     Training Step: 44 Training Loss: 3.318988800048828 \n",
      "     Training Step: 45 Training Loss: 3.0501627922058105 \n",
      "     Training Step: 46 Training Loss: 3.1153178215026855 \n",
      "     Training Step: 47 Training Loss: 2.9472055435180664 \n",
      "     Training Step: 48 Training Loss: 3.5440025329589844 \n",
      "     Training Step: 49 Training Loss: 2.589965343475342 \n",
      "     Training Step: 50 Training Loss: 3.4472007751464844 \n",
      "     Training Step: 51 Training Loss: 2.555180788040161 \n",
      "     Training Step: 52 Training Loss: 2.227257013320923 \n",
      "     Training Step: 53 Training Loss: 2.159507989883423 \n",
      "     Training Step: 54 Training Loss: 2.742863178253174 \n",
      "     Training Step: 55 Training Loss: 3.0904226303100586 \n",
      "     Training Step: 56 Training Loss: 4.344768524169922 \n",
      "     Training Step: 57 Training Loss: 3.049825668334961 \n",
      "     Training Step: 58 Training Loss: 2.750709056854248 \n",
      "     Training Step: 59 Training Loss: 2.8026392459869385 \n",
      "     Training Step: 60 Training Loss: 2.751774787902832 \n",
      "     Training Step: 61 Training Loss: 2.3469491004943848 \n",
      "     Training Step: 62 Training Loss: 3.125153064727783 \n",
      "     Training Step: 63 Training Loss: 2.4209868907928467 \n",
      "     Training Step: 64 Training Loss: 2.8027830123901367 \n",
      "     Training Step: 65 Training Loss: 2.5554232597351074 \n",
      "     Training Step: 66 Training Loss: 3.2398557662963867 \n",
      "     Training Step: 67 Training Loss: 2.3904590606689453 \n",
      "     Training Step: 68 Training Loss: 3.4842991828918457 \n",
      "     Training Step: 69 Training Loss: 2.0853757858276367 \n",
      "     Training Step: 70 Training Loss: 3.2486419677734375 \n",
      "     Training Step: 71 Training Loss: 3.5110580921173096 \n",
      "     Training Step: 72 Training Loss: 2.618227005004883 \n",
      "     Training Step: 73 Training Loss: 2.0181989669799805 \n",
      "     Training Step: 74 Training Loss: 2.867245674133301 \n",
      "     Training Step: 75 Training Loss: 3.526066303253174 \n",
      "     Training Step: 76 Training Loss: 3.46686053276062 \n",
      "     Training Step: 77 Training Loss: 3.3358659744262695 \n",
      "     Training Step: 78 Training Loss: 3.338503360748291 \n",
      "     Training Step: 79 Training Loss: 2.461160898208618 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.8833868503570557 \n",
      "     Validation Step: 1 Validation Loss: 3.6867835521698 \n",
      "     Validation Step: 2 Validation Loss: 3.15159010887146 \n",
      "     Validation Step: 3 Validation Loss: 2.647331714630127 \n",
      "     Validation Step: 4 Validation Loss: 3.550321578979492 \n",
      "     Validation Step: 5 Validation Loss: 3.5120882987976074 \n",
      "     Validation Step: 6 Validation Loss: 3.020254611968994 \n",
      "     Validation Step: 7 Validation Loss: 3.0539143085479736 \n",
      "     Validation Step: 8 Validation Loss: 2.986816167831421 \n",
      "     Validation Step: 9 Validation Loss: 3.5336341857910156 \n",
      "     Validation Step: 10 Validation Loss: 2.6395719051361084 \n",
      "     Validation Step: 11 Validation Loss: 3.207979679107666 \n",
      "     Validation Step: 12 Validation Loss: 2.247368812561035 \n",
      "     Validation Step: 13 Validation Loss: 3.680840492248535 \n",
      "     Validation Step: 14 Validation Loss: 3.941417932510376 \n",
      "Epoch: 100\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.5835375785827637 \n",
      "     Training Step: 1 Training Loss: 3.4471583366394043 \n",
      "     Training Step: 2 Training Loss: 4.800356864929199 \n",
      "     Training Step: 3 Training Loss: 2.6118311882019043 \n",
      "     Training Step: 4 Training Loss: 2.4562840461730957 \n",
      "     Training Step: 5 Training Loss: 4.344081401824951 \n",
      "     Training Step: 6 Training Loss: 2.9438095092773438 \n",
      "     Training Step: 7 Training Loss: 2.305685520172119 \n",
      "     Training Step: 8 Training Loss: 2.974033832550049 \n",
      "     Training Step: 9 Training Loss: 2.698215961456299 \n",
      "     Training Step: 10 Training Loss: 3.2736847400665283 \n",
      "     Training Step: 11 Training Loss: 2.7612359523773193 \n",
      "     Training Step: 12 Training Loss: 2.732560634613037 \n",
      "     Training Step: 13 Training Loss: 2.8962814807891846 \n",
      "     Training Step: 14 Training Loss: 2.04600191116333 \n",
      "     Training Step: 15 Training Loss: 2.744941473007202 \n",
      "     Training Step: 16 Training Loss: 3.775704860687256 \n",
      "     Training Step: 17 Training Loss: 3.086117744445801 \n",
      "     Training Step: 18 Training Loss: 2.756448984146118 \n",
      "     Training Step: 19 Training Loss: 2.919142007827759 \n",
      "     Training Step: 20 Training Loss: 3.4886584281921387 \n",
      "     Training Step: 21 Training Loss: 3.1544992923736572 \n",
      "     Training Step: 22 Training Loss: 2.9154212474823 \n",
      "     Training Step: 23 Training Loss: 2.8137714862823486 \n",
      "     Training Step: 24 Training Loss: 3.3432974815368652 \n",
      "     Training Step: 25 Training Loss: 2.1451807022094727 \n",
      "     Training Step: 26 Training Loss: 4.025269985198975 \n",
      "     Training Step: 27 Training Loss: 3.0415704250335693 \n",
      "     Training Step: 28 Training Loss: 3.3463568687438965 \n",
      "     Training Step: 29 Training Loss: 2.221501111984253 \n",
      "     Training Step: 30 Training Loss: 3.203817844390869 \n",
      "     Training Step: 31 Training Loss: 2.884207248687744 \n",
      "     Training Step: 32 Training Loss: 3.028228759765625 \n",
      "     Training Step: 33 Training Loss: 2.579409122467041 \n",
      "     Training Step: 34 Training Loss: 3.6385560035705566 \n",
      "     Training Step: 35 Training Loss: 3.2481393814086914 \n",
      "     Training Step: 36 Training Loss: 2.0631613731384277 \n",
      "     Training Step: 37 Training Loss: 2.2290329933166504 \n",
      "     Training Step: 38 Training Loss: 2.1401662826538086 \n",
      "     Training Step: 39 Training Loss: 3.3932137489318848 \n",
      "     Training Step: 40 Training Loss: 4.361369609832764 \n",
      "     Training Step: 41 Training Loss: 2.191401958465576 \n",
      "     Training Step: 42 Training Loss: 2.3766977787017822 \n",
      "     Training Step: 43 Training Loss: 2.5316851139068604 \n",
      "     Training Step: 44 Training Loss: 2.6997766494750977 \n",
      "     Training Step: 45 Training Loss: 2.3866257667541504 \n",
      "     Training Step: 46 Training Loss: 2.251783847808838 \n",
      "     Training Step: 47 Training Loss: 2.995832681655884 \n",
      "     Training Step: 48 Training Loss: 3.476628303527832 \n",
      "     Training Step: 49 Training Loss: 2.2925732135772705 \n",
      "     Training Step: 50 Training Loss: 3.12672758102417 \n",
      "     Training Step: 51 Training Loss: 3.740662097930908 \n",
      "     Training Step: 52 Training Loss: 3.3709635734558105 \n",
      "     Training Step: 53 Training Loss: 3.3591904640197754 \n",
      "     Training Step: 54 Training Loss: 2.7097959518432617 \n",
      "     Training Step: 55 Training Loss: 3.897789478302002 \n",
      "     Training Step: 56 Training Loss: 2.451742172241211 \n",
      "     Training Step: 57 Training Loss: 2.4043540954589844 \n",
      "     Training Step: 58 Training Loss: 3.7503743171691895 \n",
      "     Training Step: 59 Training Loss: 3.273486375808716 \n",
      "     Training Step: 60 Training Loss: 3.3819451332092285 \n",
      "     Training Step: 61 Training Loss: 2.8759663105010986 \n",
      "     Training Step: 62 Training Loss: 3.375065326690674 \n",
      "     Training Step: 63 Training Loss: 3.0590999126434326 \n",
      "     Training Step: 64 Training Loss: 2.711000442504883 \n",
      "     Training Step: 65 Training Loss: 2.7533254623413086 \n",
      "     Training Step: 66 Training Loss: 3.105516195297241 \n",
      "     Training Step: 67 Training Loss: 3.3076038360595703 \n",
      "     Training Step: 68 Training Loss: 2.39894437789917 \n",
      "     Training Step: 69 Training Loss: 2.455385684967041 \n",
      "     Training Step: 70 Training Loss: 2.697624683380127 \n",
      "     Training Step: 71 Training Loss: 4.334470272064209 \n",
      "     Training Step: 72 Training Loss: 2.6160073280334473 \n",
      "     Training Step: 73 Training Loss: 2.3698480129241943 \n",
      "     Training Step: 74 Training Loss: 3.324705123901367 \n",
      "     Training Step: 75 Training Loss: 3.2095189094543457 \n",
      "     Training Step: 76 Training Loss: 3.1453146934509277 \n",
      "     Training Step: 77 Training Loss: 3.06174635887146 \n",
      "     Training Step: 78 Training Loss: 2.726444959640503 \n",
      "     Training Step: 79 Training Loss: 2.2966089248657227 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.108457088470459 \n",
      "     Validation Step: 1 Validation Loss: 2.976858377456665 \n",
      "     Validation Step: 2 Validation Loss: 3.2247462272644043 \n",
      "     Validation Step: 3 Validation Loss: 2.9186785221099854 \n",
      "     Validation Step: 4 Validation Loss: 3.6619794368743896 \n",
      "     Validation Step: 5 Validation Loss: 3.6651554107666016 \n",
      "     Validation Step: 6 Validation Loss: 2.776916980743408 \n",
      "     Validation Step: 7 Validation Loss: 3.1787269115448 \n",
      "     Validation Step: 8 Validation Loss: 2.842000961303711 \n",
      "     Validation Step: 9 Validation Loss: 3.6679060459136963 \n",
      "     Validation Step: 10 Validation Loss: 2.2333202362060547 \n",
      "     Validation Step: 11 Validation Loss: 3.0850014686584473 \n",
      "     Validation Step: 12 Validation Loss: 3.1831634044647217 \n",
      "     Validation Step: 13 Validation Loss: 3.7202131748199463 \n",
      "     Validation Step: 14 Validation Loss: 3.0660383701324463 \n",
      "---------------   CURRENT LEARNING RATE: 1.6000000000000004e-06   ---------------\n",
      "Epoch: 100\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 6.315705299377441 \n",
      "     Training Step: 1 Training Loss: 7.244068145751953 \n",
      "     Training Step: 2 Training Loss: 4.454128265380859 \n",
      "     Training Step: 3 Training Loss: 10.347792625427246 \n",
      "     Training Step: 4 Training Loss: 6.638555526733398 \n",
      "     Training Step: 5 Training Loss: 3.349789619445801 \n",
      "     Training Step: 6 Training Loss: 6.631289958953857 \n",
      "     Training Step: 7 Training Loss: 6.665380477905273 \n",
      "     Training Step: 8 Training Loss: 5.485795497894287 \n",
      "     Training Step: 9 Training Loss: 8.762036323547363 \n",
      "     Training Step: 10 Training Loss: 6.621796131134033 \n",
      "     Training Step: 11 Training Loss: 6.96030330657959 \n",
      "     Training Step: 12 Training Loss: 7.20860481262207 \n",
      "     Training Step: 13 Training Loss: 3.2659196853637695 \n",
      "     Training Step: 14 Training Loss: 4.065299034118652 \n",
      "     Training Step: 15 Training Loss: 5.064201354980469 \n",
      "     Training Step: 16 Training Loss: 5.191128253936768 \n",
      "     Training Step: 17 Training Loss: 8.967781066894531 \n",
      "     Training Step: 18 Training Loss: 3.3221254348754883 \n",
      "     Training Step: 19 Training Loss: 4.9340410232543945 \n",
      "     Training Step: 20 Training Loss: 4.848502159118652 \n",
      "     Training Step: 21 Training Loss: 8.221604347229004 \n",
      "     Training Step: 22 Training Loss: 5.564946174621582 \n",
      "     Training Step: 23 Training Loss: 4.55942440032959 \n",
      "     Training Step: 24 Training Loss: 12.346108436584473 \n",
      "     Training Step: 25 Training Loss: 4.906598091125488 \n",
      "     Training Step: 26 Training Loss: 10.074295997619629 \n",
      "     Training Step: 27 Training Loss: 8.8071870803833 \n",
      "     Training Step: 28 Training Loss: 4.3252458572387695 \n",
      "     Training Step: 29 Training Loss: 3.5359907150268555 \n",
      "     Training Step: 30 Training Loss: 7.300327301025391 \n",
      "     Training Step: 31 Training Loss: 2.978018045425415 \n",
      "     Training Step: 32 Training Loss: 5.47663688659668 \n",
      "     Training Step: 33 Training Loss: 6.132676124572754 \n",
      "     Training Step: 34 Training Loss: 6.874701023101807 \n",
      "     Training Step: 35 Training Loss: 4.960198879241943 \n",
      "     Training Step: 36 Training Loss: 6.147006511688232 \n",
      "     Training Step: 37 Training Loss: 5.99641752243042 \n",
      "     Training Step: 38 Training Loss: 4.395364284515381 \n",
      "     Training Step: 39 Training Loss: 7.417538642883301 \n",
      "     Training Step: 40 Training Loss: 10.200090408325195 \n",
      "     Training Step: 41 Training Loss: 8.067131996154785 \n",
      "     Training Step: 42 Training Loss: 3.6617894172668457 \n",
      "     Training Step: 43 Training Loss: 4.162550926208496 \n",
      "     Training Step: 44 Training Loss: 3.2720437049865723 \n",
      "     Training Step: 45 Training Loss: 6.642420768737793 \n",
      "     Training Step: 46 Training Loss: 8.452583312988281 \n",
      "     Training Step: 47 Training Loss: 6.987141132354736 \n",
      "     Training Step: 48 Training Loss: 4.296846866607666 \n",
      "     Training Step: 49 Training Loss: 6.07908821105957 \n",
      "     Training Step: 50 Training Loss: 10.636251449584961 \n",
      "     Training Step: 51 Training Loss: 5.307145118713379 \n",
      "     Training Step: 52 Training Loss: 3.6311068534851074 \n",
      "     Training Step: 53 Training Loss: 5.267079830169678 \n",
      "     Training Step: 54 Training Loss: 5.431302070617676 \n",
      "     Training Step: 55 Training Loss: 5.9543137550354 \n",
      "     Training Step: 56 Training Loss: 7.426760673522949 \n",
      "     Training Step: 57 Training Loss: 2.9321086406707764 \n",
      "     Training Step: 58 Training Loss: 2.755427360534668 \n",
      "     Training Step: 59 Training Loss: 3.95790433883667 \n",
      "     Training Step: 60 Training Loss: 6.133782863616943 \n",
      "     Training Step: 61 Training Loss: 7.361504554748535 \n",
      "     Training Step: 62 Training Loss: 6.340631484985352 \n",
      "     Training Step: 63 Training Loss: 5.9662699699401855 \n",
      "     Training Step: 64 Training Loss: 6.799368381500244 \n",
      "     Training Step: 65 Training Loss: 4.095131874084473 \n",
      "     Training Step: 66 Training Loss: 7.424806118011475 \n",
      "     Training Step: 67 Training Loss: 5.375712871551514 \n",
      "     Training Step: 68 Training Loss: 5.1369524002075195 \n",
      "     Training Step: 69 Training Loss: 6.192532539367676 \n",
      "     Training Step: 70 Training Loss: 7.568655967712402 \n",
      "     Training Step: 71 Training Loss: 3.5338616371154785 \n",
      "     Training Step: 72 Training Loss: 4.455545425415039 \n",
      "     Training Step: 73 Training Loss: 8.642576217651367 \n",
      "     Training Step: 74 Training Loss: 3.289175033569336 \n",
      "     Training Step: 75 Training Loss: 7.617958068847656 \n",
      "     Training Step: 76 Training Loss: 4.900948524475098 \n",
      "     Training Step: 77 Training Loss: 4.854245185852051 \n",
      "     Training Step: 78 Training Loss: 6.955386161804199 \n",
      "     Training Step: 79 Training Loss: 4.712984561920166 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 9.501187324523926 \n",
      "     Validation Step: 1 Validation Loss: 5.192089080810547 \n",
      "     Validation Step: 2 Validation Loss: 8.034553527832031 \n",
      "     Validation Step: 3 Validation Loss: 8.444432258605957 \n",
      "     Validation Step: 4 Validation Loss: 6.351536750793457 \n",
      "     Validation Step: 5 Validation Loss: 6.719906330108643 \n",
      "     Validation Step: 6 Validation Loss: 7.245396614074707 \n",
      "     Validation Step: 7 Validation Loss: 6.216196537017822 \n",
      "     Validation Step: 8 Validation Loss: 6.304421424865723 \n",
      "     Validation Step: 9 Validation Loss: 3.230677843093872 \n",
      "     Validation Step: 10 Validation Loss: 6.328394889831543 \n",
      "     Validation Step: 11 Validation Loss: 5.379700660705566 \n",
      "     Validation Step: 12 Validation Loss: 5.8099799156188965 \n",
      "     Validation Step: 13 Validation Loss: 8.407064437866211 \n",
      "     Validation Step: 14 Validation Loss: 8.230209350585938 \n",
      "Epoch: 101\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.534174919128418 \n",
      "     Training Step: 1 Training Loss: 3.339362144470215 \n",
      "     Training Step: 2 Training Loss: 2.730725049972534 \n",
      "     Training Step: 3 Training Loss: 2.190870523452759 \n",
      "     Training Step: 4 Training Loss: 3.310800552368164 \n",
      "     Training Step: 5 Training Loss: 3.065293788909912 \n",
      "     Training Step: 6 Training Loss: 2.996281623840332 \n",
      "     Training Step: 7 Training Loss: 3.3730907440185547 \n",
      "     Training Step: 8 Training Loss: 3.1550793647766113 \n",
      "     Training Step: 9 Training Loss: 2.75626802444458 \n",
      "     Training Step: 10 Training Loss: 2.6282081604003906 \n",
      "     Training Step: 11 Training Loss: 3.7943506240844727 \n",
      "     Training Step: 12 Training Loss: 4.2643914222717285 \n",
      "     Training Step: 13 Training Loss: 2.139247417449951 \n",
      "     Training Step: 14 Training Loss: 2.940213918685913 \n",
      "     Training Step: 15 Training Loss: 4.006777286529541 \n",
      "     Training Step: 16 Training Loss: 3.7020339965820312 \n",
      "     Training Step: 17 Training Loss: 3.127699375152588 \n",
      "     Training Step: 18 Training Loss: 3.300632953643799 \n",
      "     Training Step: 19 Training Loss: 2.923668622970581 \n",
      "     Training Step: 20 Training Loss: 2.470276117324829 \n",
      "     Training Step: 21 Training Loss: 3.3048155307769775 \n",
      "     Training Step: 22 Training Loss: 2.3307511806488037 \n",
      "     Training Step: 23 Training Loss: 2.912848472595215 \n",
      "     Training Step: 24 Training Loss: 3.153958797454834 \n",
      "     Training Step: 25 Training Loss: 2.685420036315918 \n",
      "     Training Step: 26 Training Loss: 3.3972012996673584 \n",
      "     Training Step: 27 Training Loss: 3.7390170097351074 \n",
      "     Training Step: 28 Training Loss: 3.2946677207946777 \n",
      "     Training Step: 29 Training Loss: 2.318368434906006 \n",
      "     Training Step: 30 Training Loss: 2.444420337677002 \n",
      "     Training Step: 31 Training Loss: 4.2265520095825195 \n",
      "     Training Step: 32 Training Loss: 2.6179676055908203 \n",
      "     Training Step: 33 Training Loss: 2.1637020111083984 \n",
      "     Training Step: 34 Training Loss: 3.727903366088867 \n",
      "     Training Step: 35 Training Loss: 2.9115748405456543 \n",
      "     Training Step: 36 Training Loss: 3.537853717803955 \n",
      "     Training Step: 37 Training Loss: 2.753843069076538 \n",
      "     Training Step: 38 Training Loss: 2.579254627227783 \n",
      "     Training Step: 39 Training Loss: 2.9746971130371094 \n",
      "     Training Step: 40 Training Loss: 2.471539258956909 \n",
      "     Training Step: 41 Training Loss: 3.6275429725646973 \n",
      "     Training Step: 42 Training Loss: 2.1791162490844727 \n",
      "     Training Step: 43 Training Loss: 2.740347146987915 \n",
      "     Training Step: 44 Training Loss: 3.341982841491699 \n",
      "     Training Step: 45 Training Loss: 2.8703856468200684 \n",
      "     Training Step: 46 Training Loss: 2.5244555473327637 \n",
      "     Training Step: 47 Training Loss: 2.670755386352539 \n",
      "     Training Step: 48 Training Loss: 3.126176118850708 \n",
      "     Training Step: 49 Training Loss: 2.3964037895202637 \n",
      "     Training Step: 50 Training Loss: 2.8114373683929443 \n",
      "     Training Step: 51 Training Loss: 2.6529159545898438 \n",
      "     Training Step: 52 Training Loss: 3.077800750732422 \n",
      "     Training Step: 53 Training Loss: 3.076714038848877 \n",
      "     Training Step: 54 Training Loss: 2.1965322494506836 \n",
      "     Training Step: 55 Training Loss: 3.7502732276916504 \n",
      "     Training Step: 56 Training Loss: 2.8922383785247803 \n",
      "     Training Step: 57 Training Loss: 2.3608968257904053 \n",
      "     Training Step: 58 Training Loss: 3.252342700958252 \n",
      "     Training Step: 59 Training Loss: 2.5100765228271484 \n",
      "     Training Step: 60 Training Loss: 3.207132339477539 \n",
      "     Training Step: 61 Training Loss: 2.1232049465179443 \n",
      "     Training Step: 62 Training Loss: 2.47808837890625 \n",
      "     Training Step: 63 Training Loss: 2.8137013912200928 \n",
      "     Training Step: 64 Training Loss: 3.195737838745117 \n",
      "     Training Step: 65 Training Loss: 2.6408843994140625 \n",
      "     Training Step: 66 Training Loss: 3.10884428024292 \n",
      "     Training Step: 67 Training Loss: 3.5987586975097656 \n",
      "     Training Step: 68 Training Loss: 3.174217700958252 \n",
      "     Training Step: 69 Training Loss: 2.611884355545044 \n",
      "     Training Step: 70 Training Loss: 2.613624095916748 \n",
      "     Training Step: 71 Training Loss: 2.8439886569976807 \n",
      "     Training Step: 72 Training Loss: 3.2982940673828125 \n",
      "     Training Step: 73 Training Loss: 2.520148754119873 \n",
      "     Training Step: 74 Training Loss: 4.813880443572998 \n",
      "     Training Step: 75 Training Loss: 4.412202835083008 \n",
      "     Training Step: 76 Training Loss: 2.324972629547119 \n",
      "     Training Step: 77 Training Loss: 3.3207430839538574 \n",
      "     Training Step: 78 Training Loss: 2.287454128265381 \n",
      "     Training Step: 79 Training Loss: 2.5182623863220215 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9079763889312744 \n",
      "     Validation Step: 1 Validation Loss: 3.043985366821289 \n",
      "     Validation Step: 2 Validation Loss: 3.864452838897705 \n",
      "     Validation Step: 3 Validation Loss: 3.0553600788116455 \n",
      "     Validation Step: 4 Validation Loss: 3.4146182537078857 \n",
      "     Validation Step: 5 Validation Loss: 3.568972587585449 \n",
      "     Validation Step: 6 Validation Loss: 2.6605892181396484 \n",
      "     Validation Step: 7 Validation Loss: 3.6585352420806885 \n",
      "     Validation Step: 8 Validation Loss: 2.2314600944519043 \n",
      "     Validation Step: 9 Validation Loss: 3.093435525894165 \n",
      "     Validation Step: 10 Validation Loss: 2.8367409706115723 \n",
      "     Validation Step: 11 Validation Loss: 3.7165117263793945 \n",
      "     Validation Step: 12 Validation Loss: 3.0662384033203125 \n",
      "     Validation Step: 13 Validation Loss: 3.170640468597412 \n",
      "     Validation Step: 14 Validation Loss: 3.6239943504333496 \n",
      "Epoch: 102\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.410940408706665 \n",
      "     Training Step: 1 Training Loss: 3.532611608505249 \n",
      "     Training Step: 2 Training Loss: 3.311034679412842 \n",
      "     Training Step: 3 Training Loss: 3.047023296356201 \n",
      "     Training Step: 4 Training Loss: 3.065577983856201 \n",
      "     Training Step: 5 Training Loss: 3.336151599884033 \n",
      "     Training Step: 6 Training Loss: 2.8117899894714355 \n",
      "     Training Step: 7 Training Loss: 2.755643129348755 \n",
      "     Training Step: 8 Training Loss: 3.401139497756958 \n",
      "     Training Step: 9 Training Loss: 2.760625123977661 \n",
      "     Training Step: 10 Training Loss: 2.675493001937866 \n",
      "     Training Step: 11 Training Loss: 2.5800113677978516 \n",
      "     Training Step: 12 Training Loss: 3.247086524963379 \n",
      "     Training Step: 13 Training Loss: 2.4009439945220947 \n",
      "     Training Step: 14 Training Loss: 2.4703874588012695 \n",
      "     Training Step: 15 Training Loss: 2.548342227935791 \n",
      "     Training Step: 16 Training Loss: 2.68371844291687 \n",
      "     Training Step: 17 Training Loss: 2.3128602504730225 \n",
      "     Training Step: 18 Training Loss: 3.1249144077301025 \n",
      "     Training Step: 19 Training Loss: 4.88879919052124 \n",
      "     Training Step: 20 Training Loss: 2.279970169067383 \n",
      "     Training Step: 21 Training Loss: 2.11140775680542 \n",
      "     Training Step: 22 Training Loss: 3.869757652282715 \n",
      "     Training Step: 23 Training Loss: 3.8850865364074707 \n",
      "     Training Step: 24 Training Loss: 3.128032684326172 \n",
      "     Training Step: 25 Training Loss: 2.760316848754883 \n",
      "     Training Step: 26 Training Loss: 2.686063528060913 \n",
      "     Training Step: 27 Training Loss: 3.2947463989257812 \n",
      "     Training Step: 28 Training Loss: 2.019613742828369 \n",
      "     Training Step: 29 Training Loss: 3.0977962017059326 \n",
      "     Training Step: 30 Training Loss: 2.797518253326416 \n",
      "     Training Step: 31 Training Loss: 2.8776278495788574 \n",
      "     Training Step: 32 Training Loss: 3.264803409576416 \n",
      "     Training Step: 33 Training Loss: 4.304278373718262 \n",
      "     Training Step: 34 Training Loss: 3.322181224822998 \n",
      "     Training Step: 35 Training Loss: 3.5724940299987793 \n",
      "     Training Step: 36 Training Loss: 2.603191375732422 \n",
      "     Training Step: 37 Training Loss: 3.1316518783569336 \n",
      "     Training Step: 38 Training Loss: 2.8618762493133545 \n",
      "     Training Step: 39 Training Loss: 4.385018825531006 \n",
      "     Training Step: 40 Training Loss: 3.1509640216827393 \n",
      "     Training Step: 41 Training Loss: 3.4047012329101562 \n",
      "     Training Step: 42 Training Loss: 4.123673915863037 \n",
      "     Training Step: 43 Training Loss: 2.2597222328186035 \n",
      "     Training Step: 44 Training Loss: 2.168398380279541 \n",
      "     Training Step: 45 Training Loss: 2.7288177013397217 \n",
      "     Training Step: 46 Training Loss: 2.353954315185547 \n",
      "     Training Step: 47 Training Loss: 3.8118669986724854 \n",
      "     Training Step: 48 Training Loss: 2.590787887573242 \n",
      "     Training Step: 49 Training Loss: 2.9156930446624756 \n",
      "     Training Step: 50 Training Loss: 3.4370412826538086 \n",
      "     Training Step: 51 Training Loss: 3.1468663215637207 \n",
      "     Training Step: 52 Training Loss: 2.4026145935058594 \n",
      "     Training Step: 53 Training Loss: 2.386634349822998 \n",
      "     Training Step: 54 Training Loss: 3.3301401138305664 \n",
      "     Training Step: 55 Training Loss: 2.6038742065429688 \n",
      "     Training Step: 56 Training Loss: 2.6732845306396484 \n",
      "     Training Step: 57 Training Loss: 2.912163257598877 \n",
      "     Training Step: 58 Training Loss: 3.314389228820801 \n",
      "     Training Step: 59 Training Loss: 2.6630613803863525 \n",
      "     Training Step: 60 Training Loss: 4.229770660400391 \n",
      "     Training Step: 61 Training Loss: 3.732722520828247 \n",
      "     Training Step: 62 Training Loss: 4.008252143859863 \n",
      "     Training Step: 63 Training Loss: 3.2810702323913574 \n",
      "     Training Step: 64 Training Loss: 2.9044346809387207 \n",
      "     Training Step: 65 Training Loss: 2.511885643005371 \n",
      "     Training Step: 66 Training Loss: 2.227639675140381 \n",
      "     Training Step: 67 Training Loss: 2.861402988433838 \n",
      "     Training Step: 68 Training Loss: 3.497581958770752 \n",
      "     Training Step: 69 Training Loss: 3.1221320629119873 \n",
      "     Training Step: 70 Training Loss: 3.264446258544922 \n",
      "     Training Step: 71 Training Loss: 2.9446182250976562 \n",
      "     Training Step: 72 Training Loss: 3.601438522338867 \n",
      "     Training Step: 73 Training Loss: 2.5485610961914062 \n",
      "     Training Step: 74 Training Loss: 2.1035518646240234 \n",
      "     Training Step: 75 Training Loss: 2.3459856510162354 \n",
      "     Training Step: 76 Training Loss: 2.505093574523926 \n",
      "     Training Step: 77 Training Loss: 2.9928526878356934 \n",
      "     Training Step: 78 Training Loss: 3.43483304977417 \n",
      "     Training Step: 79 Training Loss: 3.210428237915039 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6165685653686523 \n",
      "     Validation Step: 1 Validation Loss: 3.5164248943328857 \n",
      "     Validation Step: 2 Validation Loss: 3.2221481800079346 \n",
      "     Validation Step: 3 Validation Loss: 2.7527668476104736 \n",
      "     Validation Step: 4 Validation Loss: 3.156486988067627 \n",
      "     Validation Step: 5 Validation Loss: 3.1023974418640137 \n",
      "     Validation Step: 6 Validation Loss: 3.6144471168518066 \n",
      "     Validation Step: 7 Validation Loss: 2.652003288269043 \n",
      "     Validation Step: 8 Validation Loss: 2.8518929481506348 \n",
      "     Validation Step: 9 Validation Loss: 3.0081958770751953 \n",
      "     Validation Step: 10 Validation Loss: 2.3142411708831787 \n",
      "     Validation Step: 11 Validation Loss: 3.893293619155884 \n",
      "     Validation Step: 12 Validation Loss: 2.9595751762390137 \n",
      "     Validation Step: 13 Validation Loss: 3.746692180633545 \n",
      "     Validation Step: 14 Validation Loss: 3.5386345386505127 \n",
      "Epoch: 103\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.648138999938965 \n",
      "     Training Step: 1 Training Loss: 2.9892947673797607 \n",
      "     Training Step: 2 Training Loss: 2.2768237590789795 \n",
      "     Training Step: 3 Training Loss: 2.799866199493408 \n",
      "     Training Step: 4 Training Loss: 2.4189629554748535 \n",
      "     Training Step: 5 Training Loss: 2.8882827758789062 \n",
      "     Training Step: 6 Training Loss: 3.9585225582122803 \n",
      "     Training Step: 7 Training Loss: 4.269007682800293 \n",
      "     Training Step: 8 Training Loss: 2.4144952297210693 \n",
      "     Training Step: 9 Training Loss: 4.6563310623168945 \n",
      "     Training Step: 10 Training Loss: 3.4763317108154297 \n",
      "     Training Step: 11 Training Loss: 3.3468780517578125 \n",
      "     Training Step: 12 Training Loss: 3.3446762561798096 \n",
      "     Training Step: 13 Training Loss: 3.1695046424865723 \n",
      "     Training Step: 14 Training Loss: 3.0171847343444824 \n",
      "     Training Step: 15 Training Loss: 3.352200508117676 \n",
      "     Training Step: 16 Training Loss: 2.998873472213745 \n",
      "     Training Step: 17 Training Loss: 3.386735439300537 \n",
      "     Training Step: 18 Training Loss: 2.8948752880096436 \n",
      "     Training Step: 19 Training Loss: 3.4816155433654785 \n",
      "     Training Step: 20 Training Loss: 2.2031679153442383 \n",
      "     Training Step: 21 Training Loss: 2.8191657066345215 \n",
      "     Training Step: 22 Training Loss: 3.3855624198913574 \n",
      "     Training Step: 23 Training Loss: 2.6728579998016357 \n",
      "     Training Step: 24 Training Loss: 2.7884268760681152 \n",
      "     Training Step: 25 Training Loss: 2.5616390705108643 \n",
      "     Training Step: 26 Training Loss: 2.2602274417877197 \n",
      "     Training Step: 27 Training Loss: 3.323695182800293 \n",
      "     Training Step: 28 Training Loss: 3.188131332397461 \n",
      "     Training Step: 29 Training Loss: 2.478773355484009 \n",
      "     Training Step: 30 Training Loss: 3.789468288421631 \n",
      "     Training Step: 31 Training Loss: 2.4470529556274414 \n",
      "     Training Step: 32 Training Loss: 4.3180952072143555 \n",
      "     Training Step: 33 Training Loss: 2.6905622482299805 \n",
      "     Training Step: 34 Training Loss: 2.7983803749084473 \n",
      "     Training Step: 35 Training Loss: 3.360830783843994 \n",
      "     Training Step: 36 Training Loss: 4.083382606506348 \n",
      "     Training Step: 37 Training Loss: 3.2307586669921875 \n",
      "     Training Step: 38 Training Loss: 2.388859272003174 \n",
      "     Training Step: 39 Training Loss: 2.8826651573181152 \n",
      "     Training Step: 40 Training Loss: 2.946993350982666 \n",
      "     Training Step: 41 Training Loss: 2.1932783126831055 \n",
      "     Training Step: 42 Training Loss: 3.3830959796905518 \n",
      "     Training Step: 43 Training Loss: 2.4144978523254395 \n",
      "     Training Step: 44 Training Loss: 2.3941166400909424 \n",
      "     Training Step: 45 Training Loss: 2.5171072483062744 \n",
      "     Training Step: 46 Training Loss: 3.056046485900879 \n",
      "     Training Step: 47 Training Loss: 3.1376700401306152 \n",
      "     Training Step: 48 Training Loss: 2.766237735748291 \n",
      "     Training Step: 49 Training Loss: 2.592337131500244 \n",
      "     Training Step: 50 Training Loss: 3.278775691986084 \n",
      "     Training Step: 51 Training Loss: 3.86395263671875 \n",
      "     Training Step: 52 Training Loss: 2.16896653175354 \n",
      "     Training Step: 53 Training Loss: 3.320070266723633 \n",
      "     Training Step: 54 Training Loss: 2.5186586380004883 \n",
      "     Training Step: 55 Training Loss: 2.8593835830688477 \n",
      "     Training Step: 56 Training Loss: 3.698887348175049 \n",
      "     Training Step: 57 Training Loss: 2.9580163955688477 \n",
      "     Training Step: 58 Training Loss: 3.2136096954345703 \n",
      "     Training Step: 59 Training Loss: 2.8642892837524414 \n",
      "     Training Step: 60 Training Loss: 2.330922842025757 \n",
      "     Training Step: 61 Training Loss: 3.437296152114868 \n",
      "     Training Step: 62 Training Loss: 2.1598846912384033 \n",
      "     Training Step: 63 Training Loss: 2.2632458209991455 \n",
      "     Training Step: 64 Training Loss: 2.8880136013031006 \n",
      "     Training Step: 65 Training Loss: 2.495447874069214 \n",
      "     Training Step: 66 Training Loss: 2.151644229888916 \n",
      "     Training Step: 67 Training Loss: 2.451550006866455 \n",
      "     Training Step: 68 Training Loss: 2.062305450439453 \n",
      "     Training Step: 69 Training Loss: 2.991849184036255 \n",
      "     Training Step: 70 Training Loss: 3.783809185028076 \n",
      "     Training Step: 71 Training Loss: 3.2997899055480957 \n",
      "     Training Step: 72 Training Loss: 3.0370612144470215 \n",
      "     Training Step: 73 Training Loss: 2.220616340637207 \n",
      "     Training Step: 74 Training Loss: 3.090393543243408 \n",
      "     Training Step: 75 Training Loss: 3.166991710662842 \n",
      "     Training Step: 76 Training Loss: 3.572793483734131 \n",
      "     Training Step: 77 Training Loss: 2.8608930110931396 \n",
      "     Training Step: 78 Training Loss: 3.4956488609313965 \n",
      "     Training Step: 79 Training Loss: 3.1344923973083496 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.193514823913574 \n",
      "     Validation Step: 1 Validation Loss: 3.089625835418701 \n",
      "     Validation Step: 2 Validation Loss: 4.0391621589660645 \n",
      "     Validation Step: 3 Validation Loss: 3.1712398529052734 \n",
      "     Validation Step: 4 Validation Loss: 3.6554532051086426 \n",
      "     Validation Step: 5 Validation Loss: 3.148606061935425 \n",
      "     Validation Step: 6 Validation Loss: 3.6471035480499268 \n",
      "     Validation Step: 7 Validation Loss: 3.608064889907837 \n",
      "     Validation Step: 8 Validation Loss: 2.848747491836548 \n",
      "     Validation Step: 9 Validation Loss: 3.7084648609161377 \n",
      "     Validation Step: 10 Validation Loss: 2.706368923187256 \n",
      "     Validation Step: 11 Validation Loss: 2.272247791290283 \n",
      "     Validation Step: 12 Validation Loss: 3.391892671585083 \n",
      "     Validation Step: 13 Validation Loss: 3.1126391887664795 \n",
      "     Validation Step: 14 Validation Loss: 2.73085880279541 \n",
      "Epoch: 104\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.844653844833374 \n",
      "     Training Step: 1 Training Loss: 3.009634017944336 \n",
      "     Training Step: 2 Training Loss: 2.414318561553955 \n",
      "     Training Step: 3 Training Loss: 3.1627614498138428 \n",
      "     Training Step: 4 Training Loss: 3.34763765335083 \n",
      "     Training Step: 5 Training Loss: 2.1049671173095703 \n",
      "     Training Step: 6 Training Loss: 3.845095634460449 \n",
      "     Training Step: 7 Training Loss: 2.515392303466797 \n",
      "     Training Step: 8 Training Loss: 2.5980277061462402 \n",
      "     Training Step: 9 Training Loss: 3.2495455741882324 \n",
      "     Training Step: 10 Training Loss: 3.368612289428711 \n",
      "     Training Step: 11 Training Loss: 2.777829170227051 \n",
      "     Training Step: 12 Training Loss: 2.704043388366699 \n",
      "     Training Step: 13 Training Loss: 3.4856467247009277 \n",
      "     Training Step: 14 Training Loss: 2.0925285816192627 \n",
      "     Training Step: 15 Training Loss: 2.775172710418701 \n",
      "     Training Step: 16 Training Loss: 3.0750508308410645 \n",
      "     Training Step: 17 Training Loss: 3.0514683723449707 \n",
      "     Training Step: 18 Training Loss: 2.873255729675293 \n",
      "     Training Step: 19 Training Loss: 2.8823435306549072 \n",
      "     Training Step: 20 Training Loss: 2.298971176147461 \n",
      "     Training Step: 21 Training Loss: 3.0446696281433105 \n",
      "     Training Step: 22 Training Loss: 3.2583224773406982 \n",
      "     Training Step: 23 Training Loss: 2.528074026107788 \n",
      "     Training Step: 24 Training Loss: 3.8751895427703857 \n",
      "     Training Step: 25 Training Loss: 3.1331465244293213 \n",
      "     Training Step: 26 Training Loss: 2.909620761871338 \n",
      "     Training Step: 27 Training Loss: 3.203841209411621 \n",
      "     Training Step: 28 Training Loss: 2.6844305992126465 \n",
      "     Training Step: 29 Training Loss: 2.968883514404297 \n",
      "     Training Step: 30 Training Loss: 2.4032535552978516 \n",
      "     Training Step: 31 Training Loss: 3.3355050086975098 \n",
      "     Training Step: 32 Training Loss: 4.416654109954834 \n",
      "     Training Step: 33 Training Loss: 2.5274627208709717 \n",
      "     Training Step: 34 Training Loss: 2.3615946769714355 \n",
      "     Training Step: 35 Training Loss: 2.1793270111083984 \n",
      "     Training Step: 36 Training Loss: 2.805704355239868 \n",
      "     Training Step: 37 Training Loss: 2.7237532138824463 \n",
      "     Training Step: 38 Training Loss: 2.571726083755493 \n",
      "     Training Step: 39 Training Loss: 2.8222599029541016 \n",
      "     Training Step: 40 Training Loss: 2.8553051948547363 \n",
      "     Training Step: 41 Training Loss: 2.4203662872314453 \n",
      "     Training Step: 42 Training Loss: 4.214142322540283 \n",
      "     Training Step: 43 Training Loss: 2.7900562286376953 \n",
      "     Training Step: 44 Training Loss: 3.3696999549865723 \n",
      "     Training Step: 45 Training Loss: 3.5196433067321777 \n",
      "     Training Step: 46 Training Loss: 2.031001567840576 \n",
      "     Training Step: 47 Training Loss: 3.597087860107422 \n",
      "     Training Step: 48 Training Loss: 3.6659700870513916 \n",
      "     Training Step: 49 Training Loss: 3.092038154602051 \n",
      "     Training Step: 50 Training Loss: 2.248385190963745 \n",
      "     Training Step: 51 Training Loss: 2.3184616565704346 \n",
      "     Training Step: 52 Training Loss: 2.28808331489563 \n",
      "     Training Step: 53 Training Loss: 3.879870891571045 \n",
      "     Training Step: 54 Training Loss: 2.235431432723999 \n",
      "     Training Step: 55 Training Loss: 3.321155071258545 \n",
      "     Training Step: 56 Training Loss: 3.280120611190796 \n",
      "     Training Step: 57 Training Loss: 2.6782727241516113 \n",
      "     Training Step: 58 Training Loss: 3.1032140254974365 \n",
      "     Training Step: 59 Training Loss: 3.1449475288391113 \n",
      "     Training Step: 60 Training Loss: 4.144785404205322 \n",
      "     Training Step: 61 Training Loss: 2.972330093383789 \n",
      "     Training Step: 62 Training Loss: 2.8669803142547607 \n",
      "     Training Step: 63 Training Loss: 3.3711276054382324 \n",
      "     Training Step: 64 Training Loss: 2.785025119781494 \n",
      "     Training Step: 65 Training Loss: 2.0766806602478027 \n",
      "     Training Step: 66 Training Loss: 2.748809576034546 \n",
      "     Training Step: 67 Training Loss: 4.331437110900879 \n",
      "     Training Step: 68 Training Loss: 2.708777666091919 \n",
      "     Training Step: 69 Training Loss: 3.1863598823547363 \n",
      "     Training Step: 70 Training Loss: 3.4005520343780518 \n",
      "     Training Step: 71 Training Loss: 3.8390581607818604 \n",
      "     Training Step: 72 Training Loss: 3.0232276916503906 \n",
      "     Training Step: 73 Training Loss: 2.821971893310547 \n",
      "     Training Step: 74 Training Loss: 3.3777456283569336 \n",
      "     Training Step: 75 Training Loss: 4.699954032897949 \n",
      "     Training Step: 76 Training Loss: 2.55487060546875 \n",
      "     Training Step: 77 Training Loss: 2.446608543395996 \n",
      "     Training Step: 78 Training Loss: 2.98185396194458 \n",
      "     Training Step: 79 Training Loss: 2.957113265991211 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6135497093200684 \n",
      "     Validation Step: 1 Validation Loss: 3.0272293090820312 \n",
      "     Validation Step: 2 Validation Loss: 2.298826217651367 \n",
      "     Validation Step: 3 Validation Loss: 3.5106866359710693 \n",
      "     Validation Step: 4 Validation Loss: 3.310067653656006 \n",
      "     Validation Step: 5 Validation Loss: 3.522737979888916 \n",
      "     Validation Step: 6 Validation Loss: 3.7382748126983643 \n",
      "     Validation Step: 7 Validation Loss: 3.647136926651001 \n",
      "     Validation Step: 8 Validation Loss: 2.6746137142181396 \n",
      "     Validation Step: 9 Validation Loss: 2.9829044342041016 \n",
      "     Validation Step: 10 Validation Loss: 3.007905960083008 \n",
      "     Validation Step: 11 Validation Loss: 3.840679168701172 \n",
      "     Validation Step: 12 Validation Loss: 3.131558895111084 \n",
      "     Validation Step: 13 Validation Loss: 2.845050811767578 \n",
      "     Validation Step: 14 Validation Loss: 2.5810890197753906 \n",
      "Epoch: 105\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.0312955379486084 \n",
      "     Training Step: 1 Training Loss: 2.497365951538086 \n",
      "     Training Step: 2 Training Loss: 2.679490804672241 \n",
      "     Training Step: 3 Training Loss: 3.7252962589263916 \n",
      "     Training Step: 4 Training Loss: 3.9309134483337402 \n",
      "     Training Step: 5 Training Loss: 3.2401576042175293 \n",
      "     Training Step: 6 Training Loss: 2.6963818073272705 \n",
      "     Training Step: 7 Training Loss: 2.8472487926483154 \n",
      "     Training Step: 8 Training Loss: 2.269838333129883 \n",
      "     Training Step: 9 Training Loss: 2.986513614654541 \n",
      "     Training Step: 10 Training Loss: 2.203761577606201 \n",
      "     Training Step: 11 Training Loss: 2.7110445499420166 \n",
      "     Training Step: 12 Training Loss: 2.5165436267852783 \n",
      "     Training Step: 13 Training Loss: 2.468012571334839 \n",
      "     Training Step: 14 Training Loss: 2.813955783843994 \n",
      "     Training Step: 15 Training Loss: 2.8784570693969727 \n",
      "     Training Step: 16 Training Loss: 2.2854442596435547 \n",
      "     Training Step: 17 Training Loss: 3.273512363433838 \n",
      "     Training Step: 18 Training Loss: 2.286195755004883 \n",
      "     Training Step: 19 Training Loss: 3.2067158222198486 \n",
      "     Training Step: 20 Training Loss: 3.2978005409240723 \n",
      "     Training Step: 21 Training Loss: 3.0785107612609863 \n",
      "     Training Step: 22 Training Loss: 2.6303937435150146 \n",
      "     Training Step: 23 Training Loss: 3.264955520629883 \n",
      "     Training Step: 24 Training Loss: 3.341789960861206 \n",
      "     Training Step: 25 Training Loss: 4.298022747039795 \n",
      "     Training Step: 26 Training Loss: 4.33636474609375 \n",
      "     Training Step: 27 Training Loss: 3.5382556915283203 \n",
      "     Training Step: 28 Training Loss: 3.4535224437713623 \n",
      "     Training Step: 29 Training Loss: 2.1637771129608154 \n",
      "     Training Step: 30 Training Loss: 3.3866970539093018 \n",
      "     Training Step: 31 Training Loss: 3.6486358642578125 \n",
      "     Training Step: 32 Training Loss: 3.785438060760498 \n",
      "     Training Step: 33 Training Loss: 2.5676021575927734 \n",
      "     Training Step: 34 Training Loss: 3.1130213737487793 \n",
      "     Training Step: 35 Training Loss: 3.200063705444336 \n",
      "     Training Step: 36 Training Loss: 2.9048914909362793 \n",
      "     Training Step: 37 Training Loss: 2.8692502975463867 \n",
      "     Training Step: 38 Training Loss: 2.407701015472412 \n",
      "     Training Step: 39 Training Loss: 2.237729072570801 \n",
      "     Training Step: 40 Training Loss: 3.5480313301086426 \n",
      "     Training Step: 41 Training Loss: 3.2945170402526855 \n",
      "     Training Step: 42 Training Loss: 2.0863332748413086 \n",
      "     Training Step: 43 Training Loss: 2.570420742034912 \n",
      "     Training Step: 44 Training Loss: 3.5298399925231934 \n",
      "     Training Step: 45 Training Loss: 2.929485321044922 \n",
      "     Training Step: 46 Training Loss: 3.3268165588378906 \n",
      "     Training Step: 47 Training Loss: 3.9487195014953613 \n",
      "     Training Step: 48 Training Loss: 2.5739755630493164 \n",
      "     Training Step: 49 Training Loss: 2.9825024604797363 \n",
      "     Training Step: 50 Training Loss: 2.4128270149230957 \n",
      "     Training Step: 51 Training Loss: 3.378995180130005 \n",
      "     Training Step: 52 Training Loss: 3.158094882965088 \n",
      "     Training Step: 53 Training Loss: 3.360520124435425 \n",
      "     Training Step: 54 Training Loss: 4.735495090484619 \n",
      "     Training Step: 55 Training Loss: 3.4151740074157715 \n",
      "     Training Step: 56 Training Loss: 2.693044424057007 \n",
      "     Training Step: 57 Training Loss: 2.2233047485351562 \n",
      "     Training Step: 58 Training Loss: 2.1618711948394775 \n",
      "     Training Step: 59 Training Loss: 3.3031818866729736 \n",
      "     Training Step: 60 Training Loss: 3.3919615745544434 \n",
      "     Training Step: 61 Training Loss: 2.103034734725952 \n",
      "     Training Step: 62 Training Loss: 2.553053855895996 \n",
      "     Training Step: 63 Training Loss: 3.812894821166992 \n",
      "     Training Step: 64 Training Loss: 3.188171863555908 \n",
      "     Training Step: 65 Training Loss: 4.333383560180664 \n",
      "     Training Step: 66 Training Loss: 3.260923385620117 \n",
      "     Training Step: 67 Training Loss: 2.8927149772644043 \n",
      "     Training Step: 68 Training Loss: 2.790147304534912 \n",
      "     Training Step: 69 Training Loss: 2.8329718112945557 \n",
      "     Training Step: 70 Training Loss: 3.00767183303833 \n",
      "     Training Step: 71 Training Loss: 2.379845380783081 \n",
      "     Training Step: 72 Training Loss: 2.495676279067993 \n",
      "     Training Step: 73 Training Loss: 2.992323875427246 \n",
      "     Training Step: 74 Training Loss: 2.86972975730896 \n",
      "     Training Step: 75 Training Loss: 2.3685765266418457 \n",
      "     Training Step: 76 Training Loss: 2.969572067260742 \n",
      "     Training Step: 77 Training Loss: 2.8950681686401367 \n",
      "     Training Step: 78 Training Loss: 2.401794195175171 \n",
      "     Training Step: 79 Training Loss: 2.98016095161438 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.257733106613159 \n",
      "     Validation Step: 1 Validation Loss: 3.873067855834961 \n",
      "     Validation Step: 2 Validation Loss: 3.514416456222534 \n",
      "     Validation Step: 3 Validation Loss: 2.595580816268921 \n",
      "     Validation Step: 4 Validation Loss: 2.684910297393799 \n",
      "     Validation Step: 5 Validation Loss: 2.998843193054199 \n",
      "     Validation Step: 6 Validation Loss: 3.1151976585388184 \n",
      "     Validation Step: 7 Validation Loss: 2.831960678100586 \n",
      "     Validation Step: 8 Validation Loss: 3.5890872478485107 \n",
      "     Validation Step: 9 Validation Loss: 3.4902586936950684 \n",
      "     Validation Step: 10 Validation Loss: 2.3489413261413574 \n",
      "     Validation Step: 11 Validation Loss: 3.5567867755889893 \n",
      "     Validation Step: 12 Validation Loss: 3.735220432281494 \n",
      "     Validation Step: 13 Validation Loss: 3.045626163482666 \n",
      "     Validation Step: 14 Validation Loss: 3.0297341346740723 \n",
      "Epoch: 106\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.31824254989624 \n",
      "     Training Step: 1 Training Loss: 2.409783363342285 \n",
      "     Training Step: 2 Training Loss: 2.790457010269165 \n",
      "     Training Step: 3 Training Loss: 2.3644440174102783 \n",
      "     Training Step: 4 Training Loss: 2.268383502960205 \n",
      "     Training Step: 5 Training Loss: 2.3838210105895996 \n",
      "     Training Step: 6 Training Loss: 2.6898765563964844 \n",
      "     Training Step: 7 Training Loss: 2.7594826221466064 \n",
      "     Training Step: 8 Training Loss: 2.9485418796539307 \n",
      "     Training Step: 9 Training Loss: 3.862377643585205 \n",
      "     Training Step: 10 Training Loss: 3.0865273475646973 \n",
      "     Training Step: 11 Training Loss: 2.678732395172119 \n",
      "     Training Step: 12 Training Loss: 2.820394515991211 \n",
      "     Training Step: 13 Training Loss: 4.811887741088867 \n",
      "     Training Step: 14 Training Loss: 2.6212878227233887 \n",
      "     Training Step: 15 Training Loss: 3.4266703128814697 \n",
      "     Training Step: 16 Training Loss: 3.413390636444092 \n",
      "     Training Step: 17 Training Loss: 3.191603183746338 \n",
      "     Training Step: 18 Training Loss: 2.5185773372650146 \n",
      "     Training Step: 19 Training Loss: 2.1261868476867676 \n",
      "     Training Step: 20 Training Loss: 3.009683609008789 \n",
      "     Training Step: 21 Training Loss: 3.4427762031555176 \n",
      "     Training Step: 22 Training Loss: 3.1757278442382812 \n",
      "     Training Step: 23 Training Loss: 3.4771459102630615 \n",
      "     Training Step: 24 Training Loss: 2.8645074367523193 \n",
      "     Training Step: 25 Training Loss: 2.3143441677093506 \n",
      "     Training Step: 26 Training Loss: 2.4620461463928223 \n",
      "     Training Step: 27 Training Loss: 3.0199713706970215 \n",
      "     Training Step: 28 Training Loss: 3.200773239135742 \n",
      "     Training Step: 29 Training Loss: 3.2289175987243652 \n",
      "     Training Step: 30 Training Loss: 3.3099913597106934 \n",
      "     Training Step: 31 Training Loss: 3.090909957885742 \n",
      "     Training Step: 32 Training Loss: 3.1345882415771484 \n",
      "     Training Step: 33 Training Loss: 3.240922451019287 \n",
      "     Training Step: 34 Training Loss: 3.11160945892334 \n",
      "     Training Step: 35 Training Loss: 3.5591511726379395 \n",
      "     Training Step: 36 Training Loss: 4.267612934112549 \n",
      "     Training Step: 37 Training Loss: 2.684040069580078 \n",
      "     Training Step: 38 Training Loss: 3.120546579360962 \n",
      "     Training Step: 39 Training Loss: 2.528066873550415 \n",
      "     Training Step: 40 Training Loss: 2.82912540435791 \n",
      "     Training Step: 41 Training Loss: 2.700023651123047 \n",
      "     Training Step: 42 Training Loss: 3.5006422996520996 \n",
      "     Training Step: 43 Training Loss: 3.056896924972534 \n",
      "     Training Step: 44 Training Loss: 4.298721790313721 \n",
      "     Training Step: 45 Training Loss: 2.881610870361328 \n",
      "     Training Step: 46 Training Loss: 3.0488476753234863 \n",
      "     Training Step: 47 Training Loss: 2.1143126487731934 \n",
      "     Training Step: 48 Training Loss: 3.5300188064575195 \n",
      "     Training Step: 49 Training Loss: 2.2932186126708984 \n",
      "     Training Step: 50 Training Loss: 4.368923187255859 \n",
      "     Training Step: 51 Training Loss: 2.216409206390381 \n",
      "     Training Step: 52 Training Loss: 3.005960464477539 \n",
      "     Training Step: 53 Training Loss: 2.69663405418396 \n",
      "     Training Step: 54 Training Loss: 2.479884624481201 \n",
      "     Training Step: 55 Training Loss: 2.7888529300689697 \n",
      "     Training Step: 56 Training Loss: 3.5655012130737305 \n",
      "     Training Step: 57 Training Loss: 3.754546642303467 \n",
      "     Training Step: 58 Training Loss: 3.131117820739746 \n",
      "     Training Step: 59 Training Loss: 2.1603293418884277 \n",
      "     Training Step: 60 Training Loss: 2.490668773651123 \n",
      "     Training Step: 61 Training Loss: 3.086738109588623 \n",
      "     Training Step: 62 Training Loss: 2.373063087463379 \n",
      "     Training Step: 63 Training Loss: 2.409191608428955 \n",
      "     Training Step: 64 Training Loss: 2.93424916267395 \n",
      "     Training Step: 65 Training Loss: 2.505892038345337 \n",
      "     Training Step: 66 Training Loss: 3.9475674629211426 \n",
      "     Training Step: 67 Training Loss: 2.484174966812134 \n",
      "     Training Step: 68 Training Loss: 2.8043198585510254 \n",
      "     Training Step: 69 Training Loss: 2.475515842437744 \n",
      "     Training Step: 70 Training Loss: 2.91776704788208 \n",
      "     Training Step: 71 Training Loss: 3.6150436401367188 \n",
      "     Training Step: 72 Training Loss: 2.527533769607544 \n",
      "     Training Step: 73 Training Loss: 2.130436420440674 \n",
      "     Training Step: 74 Training Loss: 2.4207816123962402 \n",
      "     Training Step: 75 Training Loss: 2.946000576019287 \n",
      "     Training Step: 76 Training Loss: 3.307222604751587 \n",
      "     Training Step: 77 Training Loss: 3.7562432289123535 \n",
      "     Training Step: 78 Training Loss: 2.8162498474121094 \n",
      "     Training Step: 79 Training Loss: 2.2845311164855957 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.3142292499542236 \n",
      "     Validation Step: 1 Validation Loss: 2.7094669342041016 \n",
      "     Validation Step: 2 Validation Loss: 2.763568878173828 \n",
      "     Validation Step: 3 Validation Loss: 3.6142690181732178 \n",
      "     Validation Step: 4 Validation Loss: 2.7732033729553223 \n",
      "     Validation Step: 5 Validation Loss: 3.169400691986084 \n",
      "     Validation Step: 6 Validation Loss: 3.5586092472076416 \n",
      "     Validation Step: 7 Validation Loss: 3.759159803390503 \n",
      "     Validation Step: 8 Validation Loss: 3.6247003078460693 \n",
      "     Validation Step: 9 Validation Loss: 3.1004533767700195 \n",
      "     Validation Step: 10 Validation Loss: 2.5867154598236084 \n",
      "     Validation Step: 11 Validation Loss: 3.4638915061950684 \n",
      "     Validation Step: 12 Validation Loss: 3.3420181274414062 \n",
      "     Validation Step: 13 Validation Loss: 2.30340576171875 \n",
      "     Validation Step: 14 Validation Loss: 2.6589322090148926 \n",
      "Epoch: 107\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.424025297164917 \n",
      "     Training Step: 1 Training Loss: 2.7562594413757324 \n",
      "     Training Step: 2 Training Loss: 3.493189811706543 \n",
      "     Training Step: 3 Training Loss: 3.412118434906006 \n",
      "     Training Step: 4 Training Loss: 3.2106804847717285 \n",
      "     Training Step: 5 Training Loss: 3.666934013366699 \n",
      "     Training Step: 6 Training Loss: 2.4150004386901855 \n",
      "     Training Step: 7 Training Loss: 3.034579277038574 \n",
      "     Training Step: 8 Training Loss: 4.137019634246826 \n",
      "     Training Step: 9 Training Loss: 2.619919776916504 \n",
      "     Training Step: 10 Training Loss: 3.086203098297119 \n",
      "     Training Step: 11 Training Loss: 2.092261552810669 \n",
      "     Training Step: 12 Training Loss: 2.4787418842315674 \n",
      "     Training Step: 13 Training Loss: 2.1239819526672363 \n",
      "     Training Step: 14 Training Loss: 2.255584239959717 \n",
      "     Training Step: 15 Training Loss: 2.5380754470825195 \n",
      "     Training Step: 16 Training Loss: 2.894542694091797 \n",
      "     Training Step: 17 Training Loss: 2.1491575241088867 \n",
      "     Training Step: 18 Training Loss: 2.290374517440796 \n",
      "     Training Step: 19 Training Loss: 2.4125819206237793 \n",
      "     Training Step: 20 Training Loss: 2.7900922298431396 \n",
      "     Training Step: 21 Training Loss: 2.896637439727783 \n",
      "     Training Step: 22 Training Loss: 2.3840999603271484 \n",
      "     Training Step: 23 Training Loss: 3.0309219360351562 \n",
      "     Training Step: 24 Training Loss: 4.0980730056762695 \n",
      "     Training Step: 25 Training Loss: 3.8061108589172363 \n",
      "     Training Step: 26 Training Loss: 2.4452321529388428 \n",
      "     Training Step: 27 Training Loss: 2.9433093070983887 \n",
      "     Training Step: 28 Training Loss: 2.2557263374328613 \n",
      "     Training Step: 29 Training Loss: 3.151327610015869 \n",
      "     Training Step: 30 Training Loss: 2.753171920776367 \n",
      "     Training Step: 31 Training Loss: 2.7458553314208984 \n",
      "     Training Step: 32 Training Loss: 2.2492928504943848 \n",
      "     Training Step: 33 Training Loss: 4.4817352294921875 \n",
      "     Training Step: 34 Training Loss: 2.808375120162964 \n",
      "     Training Step: 35 Training Loss: 2.623173236846924 \n",
      "     Training Step: 36 Training Loss: 2.474823474884033 \n",
      "     Training Step: 37 Training Loss: 3.142068862915039 \n",
      "     Training Step: 38 Training Loss: 2.5516536235809326 \n",
      "     Training Step: 39 Training Loss: 3.1639366149902344 \n",
      "     Training Step: 40 Training Loss: 3.7822532653808594 \n",
      "     Training Step: 41 Training Loss: 2.9346373081207275 \n",
      "     Training Step: 42 Training Loss: 3.410726547241211 \n",
      "     Training Step: 43 Training Loss: 3.7653019428253174 \n",
      "     Training Step: 44 Training Loss: 3.4967150688171387 \n",
      "     Training Step: 45 Training Loss: 2.320300817489624 \n",
      "     Training Step: 46 Training Loss: 2.930216073989868 \n",
      "     Training Step: 47 Training Loss: 3.0022315979003906 \n",
      "     Training Step: 48 Training Loss: 2.66713809967041 \n",
      "     Training Step: 49 Training Loss: 4.732925891876221 \n",
      "     Training Step: 50 Training Loss: 3.9920742511749268 \n",
      "     Training Step: 51 Training Loss: 3.305591344833374 \n",
      "     Training Step: 52 Training Loss: 3.427945613861084 \n",
      "     Training Step: 53 Training Loss: 2.1032299995422363 \n",
      "     Training Step: 54 Training Loss: 3.238807201385498 \n",
      "     Training Step: 55 Training Loss: 3.389700412750244 \n",
      "     Training Step: 56 Training Loss: 3.5064258575439453 \n",
      "     Training Step: 57 Training Loss: 3.4420008659362793 \n",
      "     Training Step: 58 Training Loss: 2.609666347503662 \n",
      "     Training Step: 59 Training Loss: 3.3873565196990967 \n",
      "     Training Step: 60 Training Loss: 2.7717337608337402 \n",
      "     Training Step: 61 Training Loss: 3.490050792694092 \n",
      "     Training Step: 62 Training Loss: 2.5313568115234375 \n",
      "     Training Step: 63 Training Loss: 3.517199754714966 \n",
      "     Training Step: 64 Training Loss: 2.4571945667266846 \n",
      "     Training Step: 65 Training Loss: 2.182992458343506 \n",
      "     Training Step: 66 Training Loss: 3.2815749645233154 \n",
      "     Training Step: 67 Training Loss: 3.0247600078582764 \n",
      "     Training Step: 68 Training Loss: 3.0492115020751953 \n",
      "     Training Step: 69 Training Loss: 2.960871458053589 \n",
      "     Training Step: 70 Training Loss: 3.4074394702911377 \n",
      "     Training Step: 71 Training Loss: 3.180267333984375 \n",
      "     Training Step: 72 Training Loss: 2.6749467849731445 \n",
      "     Training Step: 73 Training Loss: 2.3006656169891357 \n",
      "     Training Step: 74 Training Loss: 2.945425510406494 \n",
      "     Training Step: 75 Training Loss: 2.895139217376709 \n",
      "     Training Step: 76 Training Loss: 4.3114013671875 \n",
      "     Training Step: 77 Training Loss: 3.6234328746795654 \n",
      "     Training Step: 78 Training Loss: 3.4240994453430176 \n",
      "     Training Step: 79 Training Loss: 2.5422420501708984 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.741196632385254 \n",
      "     Validation Step: 1 Validation Loss: 3.123640298843384 \n",
      "     Validation Step: 2 Validation Loss: 3.0390892028808594 \n",
      "     Validation Step: 3 Validation Loss: 3.2222228050231934 \n",
      "     Validation Step: 4 Validation Loss: 2.7278084754943848 \n",
      "     Validation Step: 5 Validation Loss: 3.435080051422119 \n",
      "     Validation Step: 6 Validation Loss: 3.1684210300445557 \n",
      "     Validation Step: 7 Validation Loss: 3.063100576400757 \n",
      "     Validation Step: 8 Validation Loss: 3.6655781269073486 \n",
      "     Validation Step: 9 Validation Loss: 2.7345476150512695 \n",
      "     Validation Step: 10 Validation Loss: 3.622384786605835 \n",
      "     Validation Step: 11 Validation Loss: 2.851771593093872 \n",
      "     Validation Step: 12 Validation Loss: 2.343575954437256 \n",
      "     Validation Step: 13 Validation Loss: 3.8603036403656006 \n",
      "     Validation Step: 14 Validation Loss: 3.626389980316162 \n",
      "Epoch: 108\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.459083080291748 \n",
      "     Training Step: 1 Training Loss: 2.6555593013763428 \n",
      "     Training Step: 2 Training Loss: 4.31363582611084 \n",
      "     Training Step: 3 Training Loss: 2.657334566116333 \n",
      "     Training Step: 4 Training Loss: 2.5623817443847656 \n",
      "     Training Step: 5 Training Loss: 2.443913698196411 \n",
      "     Training Step: 6 Training Loss: 3.393543004989624 \n",
      "     Training Step: 7 Training Loss: 2.8438405990600586 \n",
      "     Training Step: 8 Training Loss: 2.5134503841400146 \n",
      "     Training Step: 9 Training Loss: 3.2452404499053955 \n",
      "     Training Step: 10 Training Loss: 3.235895872116089 \n",
      "     Training Step: 11 Training Loss: 2.8303067684173584 \n",
      "     Training Step: 12 Training Loss: 3.4745185375213623 \n",
      "     Training Step: 13 Training Loss: 3.045832633972168 \n",
      "     Training Step: 14 Training Loss: 2.248875141143799 \n",
      "     Training Step: 15 Training Loss: 2.9846818447113037 \n",
      "     Training Step: 16 Training Loss: 2.1920695304870605 \n",
      "     Training Step: 17 Training Loss: 2.5135934352874756 \n",
      "     Training Step: 18 Training Loss: 2.9668848514556885 \n",
      "     Training Step: 19 Training Loss: 2.1077401638031006 \n",
      "     Training Step: 20 Training Loss: 3.7303061485290527 \n",
      "     Training Step: 21 Training Loss: 3.459404230117798 \n",
      "     Training Step: 22 Training Loss: 4.28342866897583 \n",
      "     Training Step: 23 Training Loss: 2.1615958213806152 \n",
      "     Training Step: 24 Training Loss: 2.763652801513672 \n",
      "     Training Step: 25 Training Loss: 2.2779576778411865 \n",
      "     Training Step: 26 Training Loss: 3.295576333999634 \n",
      "     Training Step: 27 Training Loss: 2.6028342247009277 \n",
      "     Training Step: 28 Training Loss: 3.373624324798584 \n",
      "     Training Step: 29 Training Loss: 3.013061285018921 \n",
      "     Training Step: 30 Training Loss: 2.5748610496520996 \n",
      "     Training Step: 31 Training Loss: 3.8558707237243652 \n",
      "     Training Step: 32 Training Loss: 2.562105178833008 \n",
      "     Training Step: 33 Training Loss: 4.074610710144043 \n",
      "     Training Step: 34 Training Loss: 4.56192684173584 \n",
      "     Training Step: 35 Training Loss: 2.748823642730713 \n",
      "     Training Step: 36 Training Loss: 3.506138801574707 \n",
      "     Training Step: 37 Training Loss: 2.9686920642852783 \n",
      "     Training Step: 38 Training Loss: 2.4404327869415283 \n",
      "     Training Step: 39 Training Loss: 3.6092405319213867 \n",
      "     Training Step: 40 Training Loss: 2.209695816040039 \n",
      "     Training Step: 41 Training Loss: 2.6291098594665527 \n",
      "     Training Step: 42 Training Loss: 3.5798075199127197 \n",
      "     Training Step: 43 Training Loss: 3.003260374069214 \n",
      "     Training Step: 44 Training Loss: 3.001732349395752 \n",
      "     Training Step: 45 Training Loss: 2.7560057640075684 \n",
      "     Training Step: 46 Training Loss: 2.7118496894836426 \n",
      "     Training Step: 47 Training Loss: 2.0886893272399902 \n",
      "     Training Step: 48 Training Loss: 2.135237216949463 \n",
      "     Training Step: 49 Training Loss: 2.9269373416900635 \n",
      "     Training Step: 50 Training Loss: 2.5668702125549316 \n",
      "     Training Step: 51 Training Loss: 3.0190634727478027 \n",
      "     Training Step: 52 Training Loss: 3.465615749359131 \n",
      "     Training Step: 53 Training Loss: 3.3109121322631836 \n",
      "     Training Step: 54 Training Loss: 3.015867233276367 \n",
      "     Training Step: 55 Training Loss: 3.3831324577331543 \n",
      "     Training Step: 56 Training Loss: 3.1532490253448486 \n",
      "     Training Step: 57 Training Loss: 3.4087750911712646 \n",
      "     Training Step: 58 Training Loss: 2.9238994121551514 \n",
      "     Training Step: 59 Training Loss: 2.9334518909454346 \n",
      "     Training Step: 60 Training Loss: 3.139880418777466 \n",
      "     Training Step: 61 Training Loss: 4.31405782699585 \n",
      "     Training Step: 62 Training Loss: 3.396536350250244 \n",
      "     Training Step: 63 Training Loss: 3.3391523361206055 \n",
      "     Training Step: 64 Training Loss: 3.7819247245788574 \n",
      "     Training Step: 65 Training Loss: 3.522488594055176 \n",
      "     Training Step: 66 Training Loss: 3.300093173980713 \n",
      "     Training Step: 67 Training Loss: 2.422318458557129 \n",
      "     Training Step: 68 Training Loss: 2.9614624977111816 \n",
      "     Training Step: 69 Training Loss: 2.6072921752929688 \n",
      "     Training Step: 70 Training Loss: 2.798790216445923 \n",
      "     Training Step: 71 Training Loss: 3.0411462783813477 \n",
      "     Training Step: 72 Training Loss: 2.6312098503112793 \n",
      "     Training Step: 73 Training Loss: 2.3651645183563232 \n",
      "     Training Step: 74 Training Loss: 3.4521560668945312 \n",
      "     Training Step: 75 Training Loss: 2.2630844116210938 \n",
      "     Training Step: 76 Training Loss: 2.6578078269958496 \n",
      "     Training Step: 77 Training Loss: 2.600497007369995 \n",
      "     Training Step: 78 Training Loss: 3.365212917327881 \n",
      "     Training Step: 79 Training Loss: 2.0741825103759766 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.7399401664733887 \n",
      "     Validation Step: 1 Validation Loss: 2.8345093727111816 \n",
      "     Validation Step: 2 Validation Loss: 3.203923225402832 \n",
      "     Validation Step: 3 Validation Loss: 3.7034592628479004 \n",
      "     Validation Step: 4 Validation Loss: 3.6741764545440674 \n",
      "     Validation Step: 5 Validation Loss: 3.1805953979492188 \n",
      "     Validation Step: 6 Validation Loss: 3.0757710933685303 \n",
      "     Validation Step: 7 Validation Loss: 3.973179340362549 \n",
      "     Validation Step: 8 Validation Loss: 3.1432290077209473 \n",
      "     Validation Step: 9 Validation Loss: 3.3866546154022217 \n",
      "     Validation Step: 10 Validation Loss: 3.071683406829834 \n",
      "     Validation Step: 11 Validation Loss: 2.7176461219787598 \n",
      "     Validation Step: 12 Validation Loss: 3.6762404441833496 \n",
      "     Validation Step: 13 Validation Loss: 2.8941471576690674 \n",
      "     Validation Step: 14 Validation Loss: 2.2865121364593506 \n",
      "Epoch: 109\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.079787254333496 \n",
      "     Training Step: 1 Training Loss: 3.1119656562805176 \n",
      "     Training Step: 2 Training Loss: 4.0956268310546875 \n",
      "     Training Step: 3 Training Loss: 2.9665985107421875 \n",
      "     Training Step: 4 Training Loss: 3.3276288509368896 \n",
      "     Training Step: 5 Training Loss: 2.4051616191864014 \n",
      "     Training Step: 6 Training Loss: 3.405362606048584 \n",
      "     Training Step: 7 Training Loss: 2.834078311920166 \n",
      "     Training Step: 8 Training Loss: 3.4448001384735107 \n",
      "     Training Step: 9 Training Loss: 2.7862660884857178 \n",
      "     Training Step: 10 Training Loss: 2.686722755432129 \n",
      "     Training Step: 11 Training Loss: 3.1942567825317383 \n",
      "     Training Step: 12 Training Loss: 2.103095769882202 \n",
      "     Training Step: 13 Training Loss: 4.759999752044678 \n",
      "     Training Step: 14 Training Loss: 2.902834415435791 \n",
      "     Training Step: 15 Training Loss: 3.3925538063049316 \n",
      "     Training Step: 16 Training Loss: 3.0105278491973877 \n",
      "     Training Step: 17 Training Loss: 3.3037490844726562 \n",
      "     Training Step: 18 Training Loss: 3.920823097229004 \n",
      "     Training Step: 19 Training Loss: 2.5497050285339355 \n",
      "     Training Step: 20 Training Loss: 2.547410488128662 \n",
      "     Training Step: 21 Training Loss: 2.539055585861206 \n",
      "     Training Step: 22 Training Loss: 2.141726493835449 \n",
      "     Training Step: 23 Training Loss: 3.226670265197754 \n",
      "     Training Step: 24 Training Loss: 2.8509764671325684 \n",
      "     Training Step: 25 Training Loss: 2.385498285293579 \n",
      "     Training Step: 26 Training Loss: 3.376948118209839 \n",
      "     Training Step: 27 Training Loss: 3.7921910285949707 \n",
      "     Training Step: 28 Training Loss: 3.895643711090088 \n",
      "     Training Step: 29 Training Loss: 2.3968167304992676 \n",
      "     Training Step: 30 Training Loss: 3.047213554382324 \n",
      "     Training Step: 31 Training Loss: 2.9769704341888428 \n",
      "     Training Step: 32 Training Loss: 2.1642532348632812 \n",
      "     Training Step: 33 Training Loss: 3.0964841842651367 \n",
      "     Training Step: 34 Training Loss: 2.7595152854919434 \n",
      "     Training Step: 35 Training Loss: 3.484549045562744 \n",
      "     Training Step: 36 Training Loss: 2.2871885299682617 \n",
      "     Training Step: 37 Training Loss: 3.065572738647461 \n",
      "     Training Step: 38 Training Loss: 3.51769757270813 \n",
      "     Training Step: 39 Training Loss: 3.8672986030578613 \n",
      "     Training Step: 40 Training Loss: 4.351761817932129 \n",
      "     Training Step: 41 Training Loss: 3.3749732971191406 \n",
      "     Training Step: 42 Training Loss: 3.324178695678711 \n",
      "     Training Step: 43 Training Loss: 3.341029167175293 \n",
      "     Training Step: 44 Training Loss: 2.6652605533599854 \n",
      "     Training Step: 45 Training Loss: 3.233234167098999 \n",
      "     Training Step: 46 Training Loss: 2.787309169769287 \n",
      "     Training Step: 47 Training Loss: 2.419916868209839 \n",
      "     Training Step: 48 Training Loss: 3.1889641284942627 \n",
      "     Training Step: 49 Training Loss: 2.7618415355682373 \n",
      "     Training Step: 50 Training Loss: 2.330004930496216 \n",
      "     Training Step: 51 Training Loss: 3.241288661956787 \n",
      "     Training Step: 52 Training Loss: 2.874824047088623 \n",
      "     Training Step: 53 Training Loss: 2.0948312282562256 \n",
      "     Training Step: 54 Training Loss: 3.2085461616516113 \n",
      "     Training Step: 55 Training Loss: 2.3370118141174316 \n",
      "     Training Step: 56 Training Loss: 2.9780025482177734 \n",
      "     Training Step: 57 Training Loss: 3.6598877906799316 \n",
      "     Training Step: 58 Training Loss: 2.555966854095459 \n",
      "     Training Step: 59 Training Loss: 2.6220862865448 \n",
      "     Training Step: 60 Training Loss: 3.481340169906616 \n",
      "     Training Step: 61 Training Loss: 2.9394214153289795 \n",
      "     Training Step: 62 Training Loss: 2.9462599754333496 \n",
      "     Training Step: 63 Training Loss: 2.2635750770568848 \n",
      "     Training Step: 64 Training Loss: 3.415780782699585 \n",
      "     Training Step: 65 Training Loss: 4.424476146697998 \n",
      "     Training Step: 66 Training Loss: 2.574235200881958 \n",
      "     Training Step: 67 Training Loss: 2.3873178958892822 \n",
      "     Training Step: 68 Training Loss: 2.766618251800537 \n",
      "     Training Step: 69 Training Loss: 3.884575366973877 \n",
      "     Training Step: 70 Training Loss: 3.497945547103882 \n",
      "     Training Step: 71 Training Loss: 2.7288691997528076 \n",
      "     Training Step: 72 Training Loss: 2.229881525039673 \n",
      "     Training Step: 73 Training Loss: 2.717273712158203 \n",
      "     Training Step: 74 Training Loss: 2.4164674282073975 \n",
      "     Training Step: 75 Training Loss: 3.1237943172454834 \n",
      "     Training Step: 76 Training Loss: 2.565497875213623 \n",
      "     Training Step: 77 Training Loss: 4.470588684082031 \n",
      "     Training Step: 78 Training Loss: 2.1827125549316406 \n",
      "     Training Step: 79 Training Loss: 3.2634263038635254 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.00185489654541 \n",
      "     Validation Step: 1 Validation Loss: 3.8162174224853516 \n",
      "     Validation Step: 2 Validation Loss: 3.6503336429595947 \n",
      "     Validation Step: 3 Validation Loss: 2.201084613800049 \n",
      "     Validation Step: 4 Validation Loss: 4.129164695739746 \n",
      "     Validation Step: 5 Validation Loss: 3.7719781398773193 \n",
      "     Validation Step: 6 Validation Loss: 3.208860397338867 \n",
      "     Validation Step: 7 Validation Loss: 3.104787826538086 \n",
      "     Validation Step: 8 Validation Loss: 2.797346591949463 \n",
      "     Validation Step: 9 Validation Loss: 3.7361557483673096 \n",
      "     Validation Step: 10 Validation Loss: 2.942852020263672 \n",
      "     Validation Step: 11 Validation Loss: 3.440669059753418 \n",
      "     Validation Step: 12 Validation Loss: 2.80312180519104 \n",
      "     Validation Step: 13 Validation Loss: 3.189363479614258 \n",
      "     Validation Step: 14 Validation Loss: 3.105189800262451 \n",
      "Epoch: 110\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.1510448455810547 \n",
      "     Training Step: 1 Training Loss: 3.1445374488830566 \n",
      "     Training Step: 2 Training Loss: 3.5305018424987793 \n",
      "     Training Step: 3 Training Loss: 3.473217487335205 \n",
      "     Training Step: 4 Training Loss: 4.209092140197754 \n",
      "     Training Step: 5 Training Loss: 4.350616455078125 \n",
      "     Training Step: 6 Training Loss: 2.563981533050537 \n",
      "     Training Step: 7 Training Loss: 2.9574809074401855 \n",
      "     Training Step: 8 Training Loss: 2.142652988433838 \n",
      "     Training Step: 9 Training Loss: 3.0962724685668945 \n",
      "     Training Step: 10 Training Loss: 3.1926589012145996 \n",
      "     Training Step: 11 Training Loss: 3.407097816467285 \n",
      "     Training Step: 12 Training Loss: 2.923154830932617 \n",
      "     Training Step: 13 Training Loss: 2.8869385719299316 \n",
      "     Training Step: 14 Training Loss: 2.328702449798584 \n",
      "     Training Step: 15 Training Loss: 3.0900115966796875 \n",
      "     Training Step: 16 Training Loss: 3.375457763671875 \n",
      "     Training Step: 17 Training Loss: 3.1065354347229004 \n",
      "     Training Step: 18 Training Loss: 3.3445146083831787 \n",
      "     Training Step: 19 Training Loss: 2.4533660411834717 \n",
      "     Training Step: 20 Training Loss: 3.27490234375 \n",
      "     Training Step: 21 Training Loss: 3.848959445953369 \n",
      "     Training Step: 22 Training Loss: 2.723942279815674 \n",
      "     Training Step: 23 Training Loss: 2.5760748386383057 \n",
      "     Training Step: 24 Training Loss: 2.243556499481201 \n",
      "     Training Step: 25 Training Loss: 2.5916080474853516 \n",
      "     Training Step: 26 Training Loss: 2.772091865539551 \n",
      "     Training Step: 27 Training Loss: 2.8855624198913574 \n",
      "     Training Step: 28 Training Loss: 3.727484941482544 \n",
      "     Training Step: 29 Training Loss: 3.3332791328430176 \n",
      "     Training Step: 30 Training Loss: 3.2501258850097656 \n",
      "     Training Step: 31 Training Loss: 2.446312189102173 \n",
      "     Training Step: 32 Training Loss: 2.7836480140686035 \n",
      "     Training Step: 33 Training Loss: 2.0727453231811523 \n",
      "     Training Step: 34 Training Loss: 3.92360258102417 \n",
      "     Training Step: 35 Training Loss: 2.849064588546753 \n",
      "     Training Step: 36 Training Loss: 3.128248453140259 \n",
      "     Training Step: 37 Training Loss: 3.3969101905822754 \n",
      "     Training Step: 38 Training Loss: 2.6780261993408203 \n",
      "     Training Step: 39 Training Loss: 3.0001673698425293 \n",
      "     Training Step: 40 Training Loss: 4.076138019561768 \n",
      "     Training Step: 41 Training Loss: 2.3188915252685547 \n",
      "     Training Step: 42 Training Loss: 2.7212202548980713 \n",
      "     Training Step: 43 Training Loss: 2.88891863822937 \n",
      "     Training Step: 44 Training Loss: 3.4612135887145996 \n",
      "     Training Step: 45 Training Loss: 2.882582187652588 \n",
      "     Training Step: 46 Training Loss: 2.1472010612487793 \n",
      "     Training Step: 47 Training Loss: 2.9986207485198975 \n",
      "     Training Step: 48 Training Loss: 3.665184497833252 \n",
      "     Training Step: 49 Training Loss: 2.0457606315612793 \n",
      "     Training Step: 50 Training Loss: 3.4171085357666016 \n",
      "     Training Step: 51 Training Loss: 3.0101101398468018 \n",
      "     Training Step: 52 Training Loss: 2.4000563621520996 \n",
      "     Training Step: 53 Training Loss: 2.3877921104431152 \n",
      "     Training Step: 54 Training Loss: 2.1267359256744385 \n",
      "     Training Step: 55 Training Loss: 2.237610340118408 \n",
      "     Training Step: 56 Training Loss: 3.3491225242614746 \n",
      "     Training Step: 57 Training Loss: 3.0887439250946045 \n",
      "     Training Step: 58 Training Loss: 4.738469123840332 \n",
      "     Training Step: 59 Training Loss: 3.522228717803955 \n",
      "     Training Step: 60 Training Loss: 4.324684143066406 \n",
      "     Training Step: 61 Training Loss: 3.2718253135681152 \n",
      "     Training Step: 62 Training Loss: 2.417093276977539 \n",
      "     Training Step: 63 Training Loss: 3.2108938694000244 \n",
      "     Training Step: 64 Training Loss: 2.8897547721862793 \n",
      "     Training Step: 65 Training Loss: 2.8295822143554688 \n",
      "     Training Step: 66 Training Loss: 2.4506664276123047 \n",
      "     Training Step: 67 Training Loss: 2.6036477088928223 \n",
      "     Training Step: 68 Training Loss: 2.234166145324707 \n",
      "     Training Step: 69 Training Loss: 2.5448946952819824 \n",
      "     Training Step: 70 Training Loss: 2.1156511306762695 \n",
      "     Training Step: 71 Training Loss: 3.741257667541504 \n",
      "     Training Step: 72 Training Loss: 2.7875213623046875 \n",
      "     Training Step: 73 Training Loss: 3.0126500129699707 \n",
      "     Training Step: 74 Training Loss: 2.763843059539795 \n",
      "     Training Step: 75 Training Loss: 2.472385883331299 \n",
      "     Training Step: 76 Training Loss: 3.112238645553589 \n",
      "     Training Step: 77 Training Loss: 3.2918872833251953 \n",
      "     Training Step: 78 Training Loss: 2.7269387245178223 \n",
      "     Training Step: 79 Training Loss: 2.5746524333953857 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.648650646209717 \n",
      "     Validation Step: 1 Validation Loss: 3.6295855045318604 \n",
      "     Validation Step: 2 Validation Loss: 3.141963005065918 \n",
      "     Validation Step: 3 Validation Loss: 3.0516672134399414 \n",
      "     Validation Step: 4 Validation Loss: 2.879690408706665 \n",
      "     Validation Step: 5 Validation Loss: 3.887348175048828 \n",
      "     Validation Step: 6 Validation Loss: 3.112386703491211 \n",
      "     Validation Step: 7 Validation Loss: 2.3002097606658936 \n",
      "     Validation Step: 8 Validation Loss: 2.6918630599975586 \n",
      "     Validation Step: 9 Validation Loss: 3.668266773223877 \n",
      "     Validation Step: 10 Validation Loss: 3.649857997894287 \n",
      "     Validation Step: 11 Validation Loss: 2.7480380535125732 \n",
      "     Validation Step: 12 Validation Loss: 3.371460199356079 \n",
      "     Validation Step: 13 Validation Loss: 3.0574426651000977 \n",
      "     Validation Step: 14 Validation Loss: 3.082287311553955 \n",
      "Epoch: 111\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.1047158241271973 \n",
      "     Training Step: 1 Training Loss: 3.3015518188476562 \n",
      "     Training Step: 2 Training Loss: 3.445871591567993 \n",
      "     Training Step: 3 Training Loss: 3.2075629234313965 \n",
      "     Training Step: 4 Training Loss: 2.4035837650299072 \n",
      "     Training Step: 5 Training Loss: 2.489659547805786 \n",
      "     Training Step: 6 Training Loss: 3.3953943252563477 \n",
      "     Training Step: 7 Training Loss: 2.533334493637085 \n",
      "     Training Step: 8 Training Loss: 3.122311592102051 \n",
      "     Training Step: 9 Training Loss: 2.4695522785186768 \n",
      "     Training Step: 10 Training Loss: 3.3872780799865723 \n",
      "     Training Step: 11 Training Loss: 3.468740224838257 \n",
      "     Training Step: 12 Training Loss: 2.477377414703369 \n",
      "     Training Step: 13 Training Loss: 2.7105019092559814 \n",
      "     Training Step: 14 Training Loss: 2.1687569618225098 \n",
      "     Training Step: 15 Training Loss: 2.184558153152466 \n",
      "     Training Step: 16 Training Loss: 2.0680172443389893 \n",
      "     Training Step: 17 Training Loss: 2.734732151031494 \n",
      "     Training Step: 18 Training Loss: 2.796990394592285 \n",
      "     Training Step: 19 Training Loss: 2.5751264095306396 \n",
      "     Training Step: 20 Training Loss: 2.772801399230957 \n",
      "     Training Step: 21 Training Loss: 4.756649971008301 \n",
      "     Training Step: 22 Training Loss: 3.8758459091186523 \n",
      "     Training Step: 23 Training Loss: 3.0899910926818848 \n",
      "     Training Step: 24 Training Loss: 3.1374008655548096 \n",
      "     Training Step: 25 Training Loss: 2.0958638191223145 \n",
      "     Training Step: 26 Training Loss: 2.6290833950042725 \n",
      "     Training Step: 27 Training Loss: 2.3250632286071777 \n",
      "     Training Step: 28 Training Loss: 2.992805004119873 \n",
      "     Training Step: 29 Training Loss: 2.407620429992676 \n",
      "     Training Step: 30 Training Loss: 3.0381100177764893 \n",
      "     Training Step: 31 Training Loss: 3.3450698852539062 \n",
      "     Training Step: 32 Training Loss: 2.311412811279297 \n",
      "     Training Step: 33 Training Loss: 2.2504916191101074 \n",
      "     Training Step: 34 Training Loss: 3.4324350357055664 \n",
      "     Training Step: 35 Training Loss: 3.110595226287842 \n",
      "     Training Step: 36 Training Loss: 2.8672525882720947 \n",
      "     Training Step: 37 Training Loss: 3.117765426635742 \n",
      "     Training Step: 38 Training Loss: 2.5917603969573975 \n",
      "     Training Step: 39 Training Loss: 3.3400113582611084 \n",
      "     Training Step: 40 Training Loss: 3.948350191116333 \n",
      "     Training Step: 41 Training Loss: 2.310126304626465 \n",
      "     Training Step: 42 Training Loss: 4.275699138641357 \n",
      "     Training Step: 43 Training Loss: 3.3055179119110107 \n",
      "     Training Step: 44 Training Loss: 3.080674886703491 \n",
      "     Training Step: 45 Training Loss: 3.2850584983825684 \n",
      "     Training Step: 46 Training Loss: 2.7203242778778076 \n",
      "     Training Step: 47 Training Loss: 3.285067558288574 \n",
      "     Training Step: 48 Training Loss: 2.8358161449432373 \n",
      "     Training Step: 49 Training Loss: 3.3046154975891113 \n",
      "     Training Step: 50 Training Loss: 3.463855028152466 \n",
      "     Training Step: 51 Training Loss: 2.941788673400879 \n",
      "     Training Step: 52 Training Loss: 2.613506317138672 \n",
      "     Training Step: 53 Training Loss: 2.9018216133117676 \n",
      "     Training Step: 54 Training Loss: 2.5107169151306152 \n",
      "     Training Step: 55 Training Loss: 3.5726070404052734 \n",
      "     Training Step: 56 Training Loss: 2.7613189220428467 \n",
      "     Training Step: 57 Training Loss: 3.7889339923858643 \n",
      "     Training Step: 58 Training Loss: 3.0914883613586426 \n",
      "     Training Step: 59 Training Loss: 2.239499568939209 \n",
      "     Training Step: 60 Training Loss: 4.374825477600098 \n",
      "     Training Step: 61 Training Loss: 2.087308168411255 \n",
      "     Training Step: 62 Training Loss: 3.10452938079834 \n",
      "     Training Step: 63 Training Loss: 2.2544941902160645 \n",
      "     Training Step: 64 Training Loss: 3.103834867477417 \n",
      "     Training Step: 65 Training Loss: 3.2754039764404297 \n",
      "     Training Step: 66 Training Loss: 3.4061195850372314 \n",
      "     Training Step: 67 Training Loss: 3.5716724395751953 \n",
      "     Training Step: 68 Training Loss: 3.006561756134033 \n",
      "     Training Step: 69 Training Loss: 2.860118865966797 \n",
      "     Training Step: 70 Training Loss: 2.7697699069976807 \n",
      "     Training Step: 71 Training Loss: 4.097087860107422 \n",
      "     Training Step: 72 Training Loss: 2.550084352493286 \n",
      "     Training Step: 73 Training Loss: 3.0745046138763428 \n",
      "     Training Step: 74 Training Loss: 2.721214532852173 \n",
      "     Training Step: 75 Training Loss: 4.308312892913818 \n",
      "     Training Step: 76 Training Loss: 2.496438503265381 \n",
      "     Training Step: 77 Training Loss: 2.109386682510376 \n",
      "     Training Step: 78 Training Loss: 2.90602970123291 \n",
      "     Training Step: 79 Training Loss: 3.8058314323425293 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6163930892944336 \n",
      "     Validation Step: 1 Validation Loss: 2.2915563583374023 \n",
      "     Validation Step: 2 Validation Loss: 3.0500638484954834 \n",
      "     Validation Step: 3 Validation Loss: 3.043020248413086 \n",
      "     Validation Step: 4 Validation Loss: 3.5857324600219727 \n",
      "     Validation Step: 5 Validation Loss: 2.63749623298645 \n",
      "     Validation Step: 6 Validation Loss: 3.474949359893799 \n",
      "     Validation Step: 7 Validation Loss: 3.636955976486206 \n",
      "     Validation Step: 8 Validation Loss: 2.743943214416504 \n",
      "     Validation Step: 9 Validation Loss: 3.1562652587890625 \n",
      "     Validation Step: 10 Validation Loss: 2.868917465209961 \n",
      "     Validation Step: 11 Validation Loss: 3.0181829929351807 \n",
      "     Validation Step: 12 Validation Loss: 3.222360849380493 \n",
      "     Validation Step: 13 Validation Loss: 3.697878837585449 \n",
      "     Validation Step: 14 Validation Loss: 3.9439549446105957 \n",
      "Epoch: 112\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.3264689445495605 \n",
      "     Training Step: 1 Training Loss: 3.130963087081909 \n",
      "     Training Step: 2 Training Loss: 2.2299811840057373 \n",
      "     Training Step: 3 Training Loss: 2.9974477291107178 \n",
      "     Training Step: 4 Training Loss: 3.8461546897888184 \n",
      "     Training Step: 5 Training Loss: 2.1565423011779785 \n",
      "     Training Step: 6 Training Loss: 3.114469528198242 \n",
      "     Training Step: 7 Training Loss: 2.9367523193359375 \n",
      "     Training Step: 8 Training Loss: 4.310655117034912 \n",
      "     Training Step: 9 Training Loss: 2.4639716148376465 \n",
      "     Training Step: 10 Training Loss: 2.6031334400177 \n",
      "     Training Step: 11 Training Loss: 3.1662240028381348 \n",
      "     Training Step: 12 Training Loss: 2.84735107421875 \n",
      "     Training Step: 13 Training Loss: 4.670844078063965 \n",
      "     Training Step: 14 Training Loss: 3.437558174133301 \n",
      "     Training Step: 15 Training Loss: 3.550942897796631 \n",
      "     Training Step: 16 Training Loss: 2.1452622413635254 \n",
      "     Training Step: 17 Training Loss: 3.3977365493774414 \n",
      "     Training Step: 18 Training Loss: 3.410094738006592 \n",
      "     Training Step: 19 Training Loss: 3.1916303634643555 \n",
      "     Training Step: 20 Training Loss: 2.696341037750244 \n",
      "     Training Step: 21 Training Loss: 2.5112617015838623 \n",
      "     Training Step: 22 Training Loss: 2.654033660888672 \n",
      "     Training Step: 23 Training Loss: 2.274489402770996 \n",
      "     Training Step: 24 Training Loss: 2.672341823577881 \n",
      "     Training Step: 25 Training Loss: 2.4598989486694336 \n",
      "     Training Step: 26 Training Loss: 2.9995248317718506 \n",
      "     Training Step: 27 Training Loss: 3.2445592880249023 \n",
      "     Training Step: 28 Training Loss: 3.657407760620117 \n",
      "     Training Step: 29 Training Loss: 3.7406511306762695 \n",
      "     Training Step: 30 Training Loss: 3.345615863800049 \n",
      "     Training Step: 31 Training Loss: 2.6492886543273926 \n",
      "     Training Step: 32 Training Loss: 2.8685848712921143 \n",
      "     Training Step: 33 Training Loss: 3.354419469833374 \n",
      "     Training Step: 34 Training Loss: 2.3562653064727783 \n",
      "     Training Step: 35 Training Loss: 3.4480514526367188 \n",
      "     Training Step: 36 Training Loss: 2.375253438949585 \n",
      "     Training Step: 37 Training Loss: 3.5826826095581055 \n",
      "     Training Step: 38 Training Loss: 3.145839214324951 \n",
      "     Training Step: 39 Training Loss: 2.122319221496582 \n",
      "     Training Step: 40 Training Loss: 2.979358673095703 \n",
      "     Training Step: 41 Training Loss: 4.055714130401611 \n",
      "     Training Step: 42 Training Loss: 2.995694637298584 \n",
      "     Training Step: 43 Training Loss: 3.174454927444458 \n",
      "     Training Step: 44 Training Loss: 2.5196785926818848 \n",
      "     Training Step: 45 Training Loss: 2.9857935905456543 \n",
      "     Training Step: 46 Training Loss: 2.919612169265747 \n",
      "     Training Step: 47 Training Loss: 2.507105827331543 \n",
      "     Training Step: 48 Training Loss: 2.873063802719116 \n",
      "     Training Step: 49 Training Loss: 3.3374927043914795 \n",
      "     Training Step: 50 Training Loss: 3.720635414123535 \n",
      "     Training Step: 51 Training Loss: 2.6385176181793213 \n",
      "     Training Step: 52 Training Loss: 3.156782627105713 \n",
      "     Training Step: 53 Training Loss: 2.3394222259521484 \n",
      "     Training Step: 54 Training Loss: 4.069370269775391 \n",
      "     Training Step: 55 Training Loss: 3.03141450881958 \n",
      "     Training Step: 56 Training Loss: 2.969257354736328 \n",
      "     Training Step: 57 Training Loss: 2.42669677734375 \n",
      "     Training Step: 58 Training Loss: 2.3938770294189453 \n",
      "     Training Step: 59 Training Loss: 3.448431968688965 \n",
      "     Training Step: 60 Training Loss: 3.372572898864746 \n",
      "     Training Step: 61 Training Loss: 3.28513765335083 \n",
      "     Training Step: 62 Training Loss: 2.769017219543457 \n",
      "     Training Step: 63 Training Loss: 3.6084845066070557 \n",
      "     Training Step: 64 Training Loss: 2.829066753387451 \n",
      "     Training Step: 65 Training Loss: 2.854928970336914 \n",
      "     Training Step: 66 Training Loss: 2.265648603439331 \n",
      "     Training Step: 67 Training Loss: 2.213322162628174 \n",
      "     Training Step: 68 Training Loss: 2.8626253604888916 \n",
      "     Training Step: 69 Training Loss: 2.99483060836792 \n",
      "     Training Step: 70 Training Loss: 2.892106533050537 \n",
      "     Training Step: 71 Training Loss: 3.867335557937622 \n",
      "     Training Step: 72 Training Loss: 4.403916358947754 \n",
      "     Training Step: 73 Training Loss: 2.367412567138672 \n",
      "     Training Step: 74 Training Loss: 2.5594184398651123 \n",
      "     Training Step: 75 Training Loss: 2.0382401943206787 \n",
      "     Training Step: 76 Training Loss: 3.145310878753662 \n",
      "     Training Step: 77 Training Loss: 2.6940717697143555 \n",
      "     Training Step: 78 Training Loss: 2.801596164703369 \n",
      "     Training Step: 79 Training Loss: 2.2802882194519043 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.891550302505493 \n",
      "     Validation Step: 1 Validation Loss: 2.684490442276001 \n",
      "     Validation Step: 2 Validation Loss: 3.694857597351074 \n",
      "     Validation Step: 3 Validation Loss: 2.803952217102051 \n",
      "     Validation Step: 4 Validation Loss: 2.2443554401397705 \n",
      "     Validation Step: 5 Validation Loss: 3.1220762729644775 \n",
      "     Validation Step: 6 Validation Loss: 3.6337342262268066 \n",
      "     Validation Step: 7 Validation Loss: 3.1820998191833496 \n",
      "     Validation Step: 8 Validation Loss: 3.0666632652282715 \n",
      "     Validation Step: 9 Validation Loss: 3.677046060562134 \n",
      "     Validation Step: 10 Validation Loss: 3.171067476272583 \n",
      "     Validation Step: 11 Validation Loss: 3.6345126628875732 \n",
      "     Validation Step: 12 Validation Loss: 3.4840357303619385 \n",
      "     Validation Step: 13 Validation Loss: 3.8943402767181396 \n",
      "     Validation Step: 14 Validation Loss: 3.0364060401916504 \n",
      "Epoch: 113\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.2503738403320312 \n",
      "     Training Step: 1 Training Loss: 2.494466781616211 \n",
      "     Training Step: 2 Training Loss: 3.3912272453308105 \n",
      "     Training Step: 3 Training Loss: 3.099205493927002 \n",
      "     Training Step: 4 Training Loss: 2.3039939403533936 \n",
      "     Training Step: 5 Training Loss: 2.7962610721588135 \n",
      "     Training Step: 6 Training Loss: 2.7973132133483887 \n",
      "     Training Step: 7 Training Loss: 3.3790712356567383 \n",
      "     Training Step: 8 Training Loss: 2.5514960289001465 \n",
      "     Training Step: 9 Training Loss: 2.093538761138916 \n",
      "     Training Step: 10 Training Loss: 2.9634029865264893 \n",
      "     Training Step: 11 Training Loss: 2.804880142211914 \n",
      "     Training Step: 12 Training Loss: 3.0570926666259766 \n",
      "     Training Step: 13 Training Loss: 2.367706060409546 \n",
      "     Training Step: 14 Training Loss: 3.1950740814208984 \n",
      "     Training Step: 15 Training Loss: 3.490361213684082 \n",
      "     Training Step: 16 Training Loss: 3.0279195308685303 \n",
      "     Training Step: 17 Training Loss: 3.108956813812256 \n",
      "     Training Step: 18 Training Loss: 3.0070199966430664 \n",
      "     Training Step: 19 Training Loss: 2.165013551712036 \n",
      "     Training Step: 20 Training Loss: 3.162808418273926 \n",
      "     Training Step: 21 Training Loss: 3.250082015991211 \n",
      "     Training Step: 22 Training Loss: 2.7554426193237305 \n",
      "     Training Step: 23 Training Loss: 3.883319854736328 \n",
      "     Training Step: 24 Training Loss: 4.405563831329346 \n",
      "     Training Step: 25 Training Loss: 2.561674118041992 \n",
      "     Training Step: 26 Training Loss: 2.5849246978759766 \n",
      "     Training Step: 27 Training Loss: 2.774076223373413 \n",
      "     Training Step: 28 Training Loss: 2.6111838817596436 \n",
      "     Training Step: 29 Training Loss: 2.326347827911377 \n",
      "     Training Step: 30 Training Loss: 3.4396307468414307 \n",
      "     Training Step: 31 Training Loss: 2.769031047821045 \n",
      "     Training Step: 32 Training Loss: 3.2974305152893066 \n",
      "     Training Step: 33 Training Loss: 2.7073161602020264 \n",
      "     Training Step: 34 Training Loss: 2.8805150985717773 \n",
      "     Training Step: 35 Training Loss: 2.1181201934814453 \n",
      "     Training Step: 36 Training Loss: 3.7889504432678223 \n",
      "     Training Step: 37 Training Loss: 2.392364978790283 \n",
      "     Training Step: 38 Training Loss: 3.392045021057129 \n",
      "     Training Step: 39 Training Loss: 3.249453067779541 \n",
      "     Training Step: 40 Training Loss: 2.3532700538635254 \n",
      "     Training Step: 41 Training Loss: 2.4866251945495605 \n",
      "     Training Step: 42 Training Loss: 3.8668088912963867 \n",
      "     Training Step: 43 Training Loss: 4.255780220031738 \n",
      "     Training Step: 44 Training Loss: 2.2841579914093018 \n",
      "     Training Step: 45 Training Loss: 2.7350075244903564 \n",
      "     Training Step: 46 Training Loss: 2.309896469116211 \n",
      "     Training Step: 47 Training Loss: 3.84513521194458 \n",
      "     Training Step: 48 Training Loss: 3.161237955093384 \n",
      "     Training Step: 49 Training Loss: 3.3517773151397705 \n",
      "     Training Step: 50 Training Loss: 3.7390308380126953 \n",
      "     Training Step: 51 Training Loss: 3.4957549571990967 \n",
      "     Training Step: 52 Training Loss: 2.139380931854248 \n",
      "     Training Step: 53 Training Loss: 4.306427478790283 \n",
      "     Training Step: 54 Training Loss: 2.9732680320739746 \n",
      "     Training Step: 55 Training Loss: 2.441354274749756 \n",
      "     Training Step: 56 Training Loss: 2.4852423667907715 \n",
      "     Training Step: 57 Training Loss: 2.373833179473877 \n",
      "     Training Step: 58 Training Loss: 4.697381019592285 \n",
      "     Training Step: 59 Training Loss: 2.8831276893615723 \n",
      "     Training Step: 60 Training Loss: 2.873347759246826 \n",
      "     Training Step: 61 Training Loss: 3.112499713897705 \n",
      "     Training Step: 62 Training Loss: 3.141979694366455 \n",
      "     Training Step: 63 Training Loss: 2.4474639892578125 \n",
      "     Training Step: 64 Training Loss: 3.877056121826172 \n",
      "     Training Step: 65 Training Loss: 2.5950376987457275 \n",
      "     Training Step: 66 Training Loss: 3.28928279876709 \n",
      "     Training Step: 67 Training Loss: 2.231175422668457 \n",
      "     Training Step: 68 Training Loss: 2.7950525283813477 \n",
      "     Training Step: 69 Training Loss: 3.4534130096435547 \n",
      "     Training Step: 70 Training Loss: 2.4614267349243164 \n",
      "     Training Step: 71 Training Loss: 4.274367809295654 \n",
      "     Training Step: 72 Training Loss: 2.952341079711914 \n",
      "     Training Step: 73 Training Loss: 2.9835634231567383 \n",
      "     Training Step: 74 Training Loss: 3.301823139190674 \n",
      "     Training Step: 75 Training Loss: 2.146746873855591 \n",
      "     Training Step: 76 Training Loss: 2.932392120361328 \n",
      "     Training Step: 77 Training Loss: 2.9341273307800293 \n",
      "     Training Step: 78 Training Loss: 3.40109920501709 \n",
      "     Training Step: 79 Training Loss: 3.588216543197632 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9956798553466797 \n",
      "     Validation Step: 1 Validation Loss: 2.688666343688965 \n",
      "     Validation Step: 2 Validation Loss: 2.6941707134246826 \n",
      "     Validation Step: 3 Validation Loss: 3.0538763999938965 \n",
      "     Validation Step: 4 Validation Loss: 3.154961585998535 \n",
      "     Validation Step: 5 Validation Loss: 3.6513609886169434 \n",
      "     Validation Step: 6 Validation Loss: 3.60962176322937 \n",
      "     Validation Step: 7 Validation Loss: 3.8553333282470703 \n",
      "     Validation Step: 8 Validation Loss: 3.548154354095459 \n",
      "     Validation Step: 9 Validation Loss: 3.2554802894592285 \n",
      "     Validation Step: 10 Validation Loss: 2.305352210998535 \n",
      "     Validation Step: 11 Validation Loss: 3.0710694789886475 \n",
      "     Validation Step: 12 Validation Loss: 3.721625328063965 \n",
      "     Validation Step: 13 Validation Loss: 2.86912202835083 \n",
      "     Validation Step: 14 Validation Loss: 3.5098674297332764 \n",
      "Epoch: 114\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.3065454959869385 \n",
      "     Training Step: 1 Training Loss: 2.955394744873047 \n",
      "     Training Step: 2 Training Loss: 3.416224956512451 \n",
      "     Training Step: 3 Training Loss: 3.708047389984131 \n",
      "     Training Step: 4 Training Loss: 2.983177661895752 \n",
      "     Training Step: 5 Training Loss: 3.540666103363037 \n",
      "     Training Step: 6 Training Loss: 3.387596607208252 \n",
      "     Training Step: 7 Training Loss: 2.179960250854492 \n",
      "     Training Step: 8 Training Loss: 2.885744333267212 \n",
      "     Training Step: 9 Training Loss: 3.1978421211242676 \n",
      "     Training Step: 10 Training Loss: 2.3311703205108643 \n",
      "     Training Step: 11 Training Loss: 2.9769744873046875 \n",
      "     Training Step: 12 Training Loss: 2.450533628463745 \n",
      "     Training Step: 13 Training Loss: 3.184807300567627 \n",
      "     Training Step: 14 Training Loss: 2.2255728244781494 \n",
      "     Training Step: 15 Training Loss: 2.572993755340576 \n",
      "     Training Step: 16 Training Loss: 2.8218255043029785 \n",
      "     Training Step: 17 Training Loss: 3.4658751487731934 \n",
      "     Training Step: 18 Training Loss: 3.3447155952453613 \n",
      "     Training Step: 19 Training Loss: 3.466202735900879 \n",
      "     Training Step: 20 Training Loss: 2.5347695350646973 \n",
      "     Training Step: 21 Training Loss: 3.2982521057128906 \n",
      "     Training Step: 22 Training Loss: 2.3298864364624023 \n",
      "     Training Step: 23 Training Loss: 3.5873003005981445 \n",
      "     Training Step: 24 Training Loss: 3.2966487407684326 \n",
      "     Training Step: 25 Training Loss: 2.650439739227295 \n",
      "     Training Step: 26 Training Loss: 2.518375873565674 \n",
      "     Training Step: 27 Training Loss: 2.1170248985290527 \n",
      "     Training Step: 28 Training Loss: 3.766205310821533 \n",
      "     Training Step: 29 Training Loss: 4.005674839019775 \n",
      "     Training Step: 30 Training Loss: 2.5045459270477295 \n",
      "     Training Step: 31 Training Loss: 3.1507182121276855 \n",
      "     Training Step: 32 Training Loss: 2.8264050483703613 \n",
      "     Training Step: 33 Training Loss: 2.9128506183624268 \n",
      "     Training Step: 34 Training Loss: 2.8491177558898926 \n",
      "     Training Step: 35 Training Loss: 4.297513961791992 \n",
      "     Training Step: 36 Training Loss: 2.423643112182617 \n",
      "     Training Step: 37 Training Loss: 4.760954856872559 \n",
      "     Training Step: 38 Training Loss: 2.5731475353240967 \n",
      "     Training Step: 39 Training Loss: 2.0738978385925293 \n",
      "     Training Step: 40 Training Loss: 3.040048837661743 \n",
      "     Training Step: 41 Training Loss: 3.432401180267334 \n",
      "     Training Step: 42 Training Loss: 3.2826826572418213 \n",
      "     Training Step: 43 Training Loss: 2.7440946102142334 \n",
      "     Training Step: 44 Training Loss: 2.398923873901367 \n",
      "     Training Step: 45 Training Loss: 2.795858144760132 \n",
      "     Training Step: 46 Training Loss: 3.445783853530884 \n",
      "     Training Step: 47 Training Loss: 3.4711053371429443 \n",
      "     Training Step: 48 Training Loss: 2.919178009033203 \n",
      "     Training Step: 49 Training Loss: 3.3619470596313477 \n",
      "     Training Step: 50 Training Loss: 3.192587375640869 \n",
      "     Training Step: 51 Training Loss: 4.361391067504883 \n",
      "     Training Step: 52 Training Loss: 3.4168896675109863 \n",
      "     Training Step: 53 Training Loss: 2.2292139530181885 \n",
      "     Training Step: 54 Training Loss: 2.5138206481933594 \n",
      "     Training Step: 55 Training Loss: 2.101846694946289 \n",
      "     Training Step: 56 Training Loss: 2.9777376651763916 \n",
      "     Training Step: 57 Training Loss: 3.759885787963867 \n",
      "     Training Step: 58 Training Loss: 3.2836050987243652 \n",
      "     Training Step: 59 Training Loss: 3.2758402824401855 \n",
      "     Training Step: 60 Training Loss: 2.5729281902313232 \n",
      "     Training Step: 61 Training Loss: 2.905344009399414 \n",
      "     Training Step: 62 Training Loss: 2.786004066467285 \n",
      "     Training Step: 63 Training Loss: 2.6979727745056152 \n",
      "     Training Step: 64 Training Loss: 2.0707082748413086 \n",
      "     Training Step: 65 Training Loss: 2.7559542655944824 \n",
      "     Training Step: 66 Training Loss: 3.1014404296875 \n",
      "     Training Step: 67 Training Loss: 3.289780378341675 \n",
      "     Training Step: 68 Training Loss: 3.169903039932251 \n",
      "     Training Step: 69 Training Loss: 2.922558546066284 \n",
      "     Training Step: 70 Training Loss: 3.2024569511413574 \n",
      "     Training Step: 71 Training Loss: 2.7703986167907715 \n",
      "     Training Step: 72 Training Loss: 3.161646604537964 \n",
      "     Training Step: 73 Training Loss: 4.3853631019592285 \n",
      "     Training Step: 74 Training Loss: 2.354405403137207 \n",
      "     Training Step: 75 Training Loss: 3.411539077758789 \n",
      "     Training Step: 76 Training Loss: 3.877626895904541 \n",
      "     Training Step: 77 Training Loss: 2.2726073265075684 \n",
      "     Training Step: 78 Training Loss: 2.901883125305176 \n",
      "     Training Step: 79 Training Loss: 2.4998981952667236 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6410274505615234 \n",
      "     Validation Step: 1 Validation Loss: 3.049943685531616 \n",
      "     Validation Step: 2 Validation Loss: 3.964813709259033 \n",
      "     Validation Step: 3 Validation Loss: 3.600968837738037 \n",
      "     Validation Step: 4 Validation Loss: 3.7096595764160156 \n",
      "     Validation Step: 5 Validation Loss: 2.864617109298706 \n",
      "     Validation Step: 6 Validation Loss: 3.111988067626953 \n",
      "     Validation Step: 7 Validation Loss: 3.223184585571289 \n",
      "     Validation Step: 8 Validation Loss: 2.7409610748291016 \n",
      "     Validation Step: 9 Validation Loss: 3.0518834590911865 \n",
      "     Validation Step: 10 Validation Loss: 2.287853717803955 \n",
      "     Validation Step: 11 Validation Loss: 3.4069368839263916 \n",
      "     Validation Step: 12 Validation Loss: 3.12314510345459 \n",
      "     Validation Step: 13 Validation Loss: 2.7483692169189453 \n",
      "     Validation Step: 14 Validation Loss: 3.6691770553588867 \n",
      "Epoch: 115\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.152364492416382 \n",
      "     Training Step: 1 Training Loss: 3.837851047515869 \n",
      "     Training Step: 2 Training Loss: 2.095391273498535 \n",
      "     Training Step: 3 Training Loss: 3.264643669128418 \n",
      "     Training Step: 4 Training Loss: 3.08890962600708 \n",
      "     Training Step: 5 Training Loss: 3.081197738647461 \n",
      "     Training Step: 6 Training Loss: 2.213881492614746 \n",
      "     Training Step: 7 Training Loss: 2.431447982788086 \n",
      "     Training Step: 8 Training Loss: 2.3318679332733154 \n",
      "     Training Step: 9 Training Loss: 3.366715908050537 \n",
      "     Training Step: 10 Training Loss: 3.054427146911621 \n",
      "     Training Step: 11 Training Loss: 2.6956419944763184 \n",
      "     Training Step: 12 Training Loss: 3.2714128494262695 \n",
      "     Training Step: 13 Training Loss: 3.4183266162872314 \n",
      "     Training Step: 14 Training Loss: 2.9533331394195557 \n",
      "     Training Step: 15 Training Loss: 2.7094550132751465 \n",
      "     Training Step: 16 Training Loss: 2.261425495147705 \n",
      "     Training Step: 17 Training Loss: 2.6155033111572266 \n",
      "     Training Step: 18 Training Loss: 2.658531665802002 \n",
      "     Training Step: 19 Training Loss: 4.290889739990234 \n",
      "     Training Step: 20 Training Loss: 2.1331334114074707 \n",
      "     Training Step: 21 Training Loss: 2.4254794120788574 \n",
      "     Training Step: 22 Training Loss: 2.6902356147766113 \n",
      "     Training Step: 23 Training Loss: 2.490699052810669 \n",
      "     Training Step: 24 Training Loss: 3.3513426780700684 \n",
      "     Training Step: 25 Training Loss: 3.5978002548217773 \n",
      "     Training Step: 26 Training Loss: 2.7732667922973633 \n",
      "     Training Step: 27 Training Loss: 2.400275707244873 \n",
      "     Training Step: 28 Training Loss: 2.7518017292022705 \n",
      "     Training Step: 29 Training Loss: 4.3497490882873535 \n",
      "     Training Step: 30 Training Loss: 3.736302137374878 \n",
      "     Training Step: 31 Training Loss: 2.8404645919799805 \n",
      "     Training Step: 32 Training Loss: 3.210864305496216 \n",
      "     Training Step: 33 Training Loss: 2.8116226196289062 \n",
      "     Training Step: 34 Training Loss: 2.066890239715576 \n",
      "     Training Step: 35 Training Loss: 3.1381046772003174 \n",
      "     Training Step: 36 Training Loss: 2.60526180267334 \n",
      "     Training Step: 37 Training Loss: 2.730992078781128 \n",
      "     Training Step: 38 Training Loss: 2.9855377674102783 \n",
      "     Training Step: 39 Training Loss: 2.538573980331421 \n",
      "     Training Step: 40 Training Loss: 3.3923683166503906 \n",
      "     Training Step: 41 Training Loss: 2.930882453918457 \n",
      "     Training Step: 42 Training Loss: 3.3310909271240234 \n",
      "     Training Step: 43 Training Loss: 2.8066883087158203 \n",
      "     Training Step: 44 Training Loss: 2.5388340950012207 \n",
      "     Training Step: 45 Training Loss: 3.1249585151672363 \n",
      "     Training Step: 46 Training Loss: 2.9095585346221924 \n",
      "     Training Step: 47 Training Loss: 2.43392014503479 \n",
      "     Training Step: 48 Training Loss: 3.8864641189575195 \n",
      "     Training Step: 49 Training Loss: 3.020242691040039 \n",
      "     Training Step: 50 Training Loss: 2.849588632583618 \n",
      "     Training Step: 51 Training Loss: 2.3365321159362793 \n",
      "     Training Step: 52 Training Loss: 2.203500986099243 \n",
      "     Training Step: 53 Training Loss: 2.926567554473877 \n",
      "     Training Step: 54 Training Loss: 2.147104024887085 \n",
      "     Training Step: 55 Training Loss: 3.5156893730163574 \n",
      "     Training Step: 56 Training Loss: 3.730750560760498 \n",
      "     Training Step: 57 Training Loss: 3.565216302871704 \n",
      "     Training Step: 58 Training Loss: 3.3302111625671387 \n",
      "     Training Step: 59 Training Loss: 3.343583106994629 \n",
      "     Training Step: 60 Training Loss: 2.8026294708251953 \n",
      "     Training Step: 61 Training Loss: 2.397629737854004 \n",
      "     Training Step: 62 Training Loss: 3.376314401626587 \n",
      "     Training Step: 63 Training Loss: 3.505168914794922 \n",
      "     Training Step: 64 Training Loss: 2.813850164413452 \n",
      "     Training Step: 65 Training Loss: 2.2202515602111816 \n",
      "     Training Step: 66 Training Loss: 4.752504348754883 \n",
      "     Training Step: 67 Training Loss: 4.33018684387207 \n",
      "     Training Step: 68 Training Loss: 2.176623821258545 \n",
      "     Training Step: 69 Training Loss: 3.2915115356445312 \n",
      "     Training Step: 70 Training Loss: 3.3865296840667725 \n",
      "     Training Step: 71 Training Loss: 3.7770562171936035 \n",
      "     Training Step: 72 Training Loss: 2.708651304244995 \n",
      "     Training Step: 73 Training Loss: 3.085448741912842 \n",
      "     Training Step: 74 Training Loss: 2.8348565101623535 \n",
      "     Training Step: 75 Training Loss: 3.1176705360412598 \n",
      "     Training Step: 76 Training Loss: 2.981830358505249 \n",
      "     Training Step: 77 Training Loss: 3.1920559406280518 \n",
      "     Training Step: 78 Training Loss: 2.5637001991271973 \n",
      "     Training Step: 79 Training Loss: 2.322854518890381 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.589653491973877 \n",
      "     Validation Step: 1 Validation Loss: 2.944298267364502 \n",
      "     Validation Step: 2 Validation Loss: 3.0999133586883545 \n",
      "     Validation Step: 3 Validation Loss: 3.5940005779266357 \n",
      "     Validation Step: 4 Validation Loss: 3.225945234298706 \n",
      "     Validation Step: 5 Validation Loss: 3.519503116607666 \n",
      "     Validation Step: 6 Validation Loss: 3.009119987487793 \n",
      "     Validation Step: 7 Validation Loss: 3.680816411972046 \n",
      "     Validation Step: 8 Validation Loss: 3.8819925785064697 \n",
      "     Validation Step: 9 Validation Loss: 3.450007438659668 \n",
      "     Validation Step: 10 Validation Loss: 2.7248692512512207 \n",
      "     Validation Step: 11 Validation Loss: 3.0997138023376465 \n",
      "     Validation Step: 12 Validation Loss: 2.7253668308258057 \n",
      "     Validation Step: 13 Validation Loss: 2.2478160858154297 \n",
      "     Validation Step: 14 Validation Loss: 2.8915340900421143 \n",
      "Epoch: 116\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.5114388465881348 \n",
      "     Training Step: 1 Training Loss: 2.109649896621704 \n",
      "     Training Step: 2 Training Loss: 2.2285642623901367 \n",
      "     Training Step: 3 Training Loss: 2.894392490386963 \n",
      "     Training Step: 4 Training Loss: 2.569591999053955 \n",
      "     Training Step: 5 Training Loss: 2.9251997470855713 \n",
      "     Training Step: 6 Training Loss: 3.01206636428833 \n",
      "     Training Step: 7 Training Loss: 2.2960681915283203 \n",
      "     Training Step: 8 Training Loss: 2.799711227416992 \n",
      "     Training Step: 9 Training Loss: 3.318735122680664 \n",
      "     Training Step: 10 Training Loss: 3.0710411071777344 \n",
      "     Training Step: 11 Training Loss: 3.2369000911712646 \n",
      "     Training Step: 12 Training Loss: 2.1425929069519043 \n",
      "     Training Step: 13 Training Loss: 4.385705947875977 \n",
      "     Training Step: 14 Training Loss: 2.945329189300537 \n",
      "     Training Step: 15 Training Loss: 2.0844078063964844 \n",
      "     Training Step: 16 Training Loss: 2.8022756576538086 \n",
      "     Training Step: 17 Training Loss: 3.190603733062744 \n",
      "     Training Step: 18 Training Loss: 4.392192840576172 \n",
      "     Training Step: 19 Training Loss: 4.12007474899292 \n",
      "     Training Step: 20 Training Loss: 2.166764736175537 \n",
      "     Training Step: 21 Training Loss: 3.390718698501587 \n",
      "     Training Step: 22 Training Loss: 3.5200276374816895 \n",
      "     Training Step: 23 Training Loss: 2.7779407501220703 \n",
      "     Training Step: 24 Training Loss: 3.8673243522644043 \n",
      "     Training Step: 25 Training Loss: 2.929924964904785 \n",
      "     Training Step: 26 Training Loss: 3.3838143348693848 \n",
      "     Training Step: 27 Training Loss: 3.653691291809082 \n",
      "     Training Step: 28 Training Loss: 3.1004490852355957 \n",
      "     Training Step: 29 Training Loss: 3.8000712394714355 \n",
      "     Training Step: 30 Training Loss: 3.3374977111816406 \n",
      "     Training Step: 31 Training Loss: 2.4525814056396484 \n",
      "     Training Step: 32 Training Loss: 4.283289909362793 \n",
      "     Training Step: 33 Training Loss: 2.247608184814453 \n",
      "     Training Step: 34 Training Loss: 2.582547187805176 \n",
      "     Training Step: 35 Training Loss: 3.1423239707946777 \n",
      "     Training Step: 36 Training Loss: 3.1690618991851807 \n",
      "     Training Step: 37 Training Loss: 2.255967855453491 \n",
      "     Training Step: 38 Training Loss: 2.614205837249756 \n",
      "     Training Step: 39 Training Loss: 3.563504219055176 \n",
      "     Training Step: 40 Training Loss: 3.2040281295776367 \n",
      "     Training Step: 41 Training Loss: 3.223921775817871 \n",
      "     Training Step: 42 Training Loss: 2.396366596221924 \n",
      "     Training Step: 43 Training Loss: 2.670949935913086 \n",
      "     Training Step: 44 Training Loss: 2.725729465484619 \n",
      "     Training Step: 45 Training Loss: 2.706000804901123 \n",
      "     Training Step: 46 Training Loss: 3.4419779777526855 \n",
      "     Training Step: 47 Training Loss: 3.457613468170166 \n",
      "     Training Step: 48 Training Loss: 3.2006871700286865 \n",
      "     Training Step: 49 Training Loss: 3.34460711479187 \n",
      "     Training Step: 50 Training Loss: 3.3776373863220215 \n",
      "     Training Step: 51 Training Loss: 3.185666084289551 \n",
      "     Training Step: 52 Training Loss: 3.3949289321899414 \n",
      "     Training Step: 53 Training Loss: 3.384164571762085 \n",
      "     Training Step: 54 Training Loss: 2.516188144683838 \n",
      "     Training Step: 55 Training Loss: 2.092073917388916 \n",
      "     Training Step: 56 Training Loss: 2.309370756149292 \n",
      "     Training Step: 57 Training Loss: 4.722136497497559 \n",
      "     Training Step: 58 Training Loss: 3.783351421356201 \n",
      "     Training Step: 59 Training Loss: 2.372407913208008 \n",
      "     Training Step: 60 Training Loss: 2.2272753715515137 \n",
      "     Training Step: 61 Training Loss: 2.9551210403442383 \n",
      "     Training Step: 62 Training Loss: 3.1649792194366455 \n",
      "     Training Step: 63 Training Loss: 3.196277141571045 \n",
      "     Training Step: 64 Training Loss: 2.88639760017395 \n",
      "     Training Step: 65 Training Loss: 3.479543924331665 \n",
      "     Training Step: 66 Training Loss: 3.0024828910827637 \n",
      "     Training Step: 67 Training Loss: 2.6358532905578613 \n",
      "     Training Step: 68 Training Loss: 2.832078695297241 \n",
      "     Training Step: 69 Training Loss: 3.780742883682251 \n",
      "     Training Step: 70 Training Loss: 2.852309226989746 \n",
      "     Training Step: 71 Training Loss: 2.3453757762908936 \n",
      "     Training Step: 72 Training Loss: 2.7398672103881836 \n",
      "     Training Step: 73 Training Loss: 2.388777017593384 \n",
      "     Training Step: 74 Training Loss: 2.8629050254821777 \n",
      "     Training Step: 75 Training Loss: 2.634068489074707 \n",
      "     Training Step: 76 Training Loss: 2.4481234550476074 \n",
      "     Training Step: 77 Training Loss: 3.7633752822875977 \n",
      "     Training Step: 78 Training Loss: 2.072077751159668 \n",
      "     Training Step: 79 Training Loss: 2.3706610202789307 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.996957778930664 \n",
      "     Validation Step: 1 Validation Loss: 2.5996549129486084 \n",
      "     Validation Step: 2 Validation Loss: 3.5237832069396973 \n",
      "     Validation Step: 3 Validation Loss: 2.846036911010742 \n",
      "     Validation Step: 4 Validation Loss: 3.755807876586914 \n",
      "     Validation Step: 5 Validation Loss: 3.119060516357422 \n",
      "     Validation Step: 6 Validation Loss: 3.4947004318237305 \n",
      "     Validation Step: 7 Validation Loss: 2.8715219497680664 \n",
      "     Validation Step: 8 Validation Loss: 3.435213565826416 \n",
      "     Validation Step: 9 Validation Loss: 2.869967222213745 \n",
      "     Validation Step: 10 Validation Loss: 2.362992286682129 \n",
      "     Validation Step: 11 Validation Loss: 3.3197643756866455 \n",
      "     Validation Step: 12 Validation Loss: 3.619647979736328 \n",
      "     Validation Step: 13 Validation Loss: 3.848829746246338 \n",
      "     Validation Step: 14 Validation Loss: 2.664219617843628 \n",
      "Epoch: 117\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.114311695098877 \n",
      "     Training Step: 1 Training Loss: 2.101064920425415 \n",
      "     Training Step: 2 Training Loss: 2.799286365509033 \n",
      "     Training Step: 3 Training Loss: 3.1056199073791504 \n",
      "     Training Step: 4 Training Loss: 2.7768421173095703 \n",
      "     Training Step: 5 Training Loss: 3.4538931846618652 \n",
      "     Training Step: 6 Training Loss: 2.4818763732910156 \n",
      "     Training Step: 7 Training Loss: 3.753843069076538 \n",
      "     Training Step: 8 Training Loss: 2.5273890495300293 \n",
      "     Training Step: 9 Training Loss: 3.4054436683654785 \n",
      "     Training Step: 10 Training Loss: 2.1618032455444336 \n",
      "     Training Step: 11 Training Loss: 2.520028591156006 \n",
      "     Training Step: 12 Training Loss: 3.5299072265625 \n",
      "     Training Step: 13 Training Loss: 2.3544769287109375 \n",
      "     Training Step: 14 Training Loss: 3.007082223892212 \n",
      "     Training Step: 15 Training Loss: 2.883880138397217 \n",
      "     Training Step: 16 Training Loss: 3.178610324859619 \n",
      "     Training Step: 17 Training Loss: 3.1875855922698975 \n",
      "     Training Step: 18 Training Loss: 3.1262526512145996 \n",
      "     Training Step: 19 Training Loss: 3.404749870300293 \n",
      "     Training Step: 20 Training Loss: 2.7784814834594727 \n",
      "     Training Step: 21 Training Loss: 3.366955518722534 \n",
      "     Training Step: 22 Training Loss: 3.3685784339904785 \n",
      "     Training Step: 23 Training Loss: 2.6693150997161865 \n",
      "     Training Step: 24 Training Loss: 3.3776140213012695 \n",
      "     Training Step: 25 Training Loss: 2.3252596855163574 \n",
      "     Training Step: 26 Training Loss: 4.3069047927856445 \n",
      "     Training Step: 27 Training Loss: 2.073556661605835 \n",
      "     Training Step: 28 Training Loss: 2.930051803588867 \n",
      "     Training Step: 29 Training Loss: 4.312292575836182 \n",
      "     Training Step: 30 Training Loss: 3.6047749519348145 \n",
      "     Training Step: 31 Training Loss: 2.242342948913574 \n",
      "     Training Step: 32 Training Loss: 3.50056791305542 \n",
      "     Training Step: 33 Training Loss: 4.189430236816406 \n",
      "     Training Step: 34 Training Loss: 2.2826991081237793 \n",
      "     Training Step: 35 Training Loss: 3.1241512298583984 \n",
      "     Training Step: 36 Training Loss: 2.5000009536743164 \n",
      "     Training Step: 37 Training Loss: 3.842679023742676 \n",
      "     Training Step: 38 Training Loss: 2.9329607486724854 \n",
      "     Training Step: 39 Training Loss: 2.4260001182556152 \n",
      "     Training Step: 40 Training Loss: 2.3047237396240234 \n",
      "     Training Step: 41 Training Loss: 2.424482583999634 \n",
      "     Training Step: 42 Training Loss: 3.1439993381500244 \n",
      "     Training Step: 43 Training Loss: 3.158565044403076 \n",
      "     Training Step: 44 Training Loss: 2.7770514488220215 \n",
      "     Training Step: 45 Training Loss: 3.2306618690490723 \n",
      "     Training Step: 46 Training Loss: 2.7263107299804688 \n",
      "     Training Step: 47 Training Loss: 2.7326056957244873 \n",
      "     Training Step: 48 Training Loss: 3.163259506225586 \n",
      "     Training Step: 49 Training Loss: 3.2775418758392334 \n",
      "     Training Step: 50 Training Loss: 2.9964358806610107 \n",
      "     Training Step: 51 Training Loss: 2.86460280418396 \n",
      "     Training Step: 52 Training Loss: 2.9483864307403564 \n",
      "     Training Step: 53 Training Loss: 2.4464495182037354 \n",
      "     Training Step: 54 Training Loss: 3.9108219146728516 \n",
      "     Training Step: 55 Training Loss: 3.107656240463257 \n",
      "     Training Step: 56 Training Loss: 2.642484664916992 \n",
      "     Training Step: 57 Training Loss: 3.3371715545654297 \n",
      "     Training Step: 58 Training Loss: 2.5977046489715576 \n",
      "     Training Step: 59 Training Loss: 2.5443575382232666 \n",
      "     Training Step: 60 Training Loss: 2.902135133743286 \n",
      "     Training Step: 61 Training Loss: 2.9886515140533447 \n",
      "     Training Step: 62 Training Loss: 3.373671531677246 \n",
      "     Training Step: 63 Training Loss: 2.4114623069763184 \n",
      "     Training Step: 64 Training Loss: 4.740794658660889 \n",
      "     Training Step: 65 Training Loss: 2.8937759399414062 \n",
      "     Training Step: 66 Training Loss: 3.7418365478515625 \n",
      "     Training Step: 67 Training Loss: 2.8753745555877686 \n",
      "     Training Step: 68 Training Loss: 2.1679601669311523 \n",
      "     Training Step: 69 Training Loss: 4.171690940856934 \n",
      "     Training Step: 70 Training Loss: 2.655672073364258 \n",
      "     Training Step: 71 Training Loss: 3.5064234733581543 \n",
      "     Training Step: 72 Training Loss: 2.1632070541381836 \n",
      "     Training Step: 73 Training Loss: 3.0994136333465576 \n",
      "     Training Step: 74 Training Loss: 2.5747363567352295 \n",
      "     Training Step: 75 Training Loss: 2.599743366241455 \n",
      "     Training Step: 76 Training Loss: 3.367751121520996 \n",
      "     Training Step: 77 Training Loss: 2.470771312713623 \n",
      "     Training Step: 78 Training Loss: 3.331718921661377 \n",
      "     Training Step: 79 Training Loss: 3.3790760040283203 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.0993833541870117 \n",
      "     Validation Step: 1 Validation Loss: 3.7130582332611084 \n",
      "     Validation Step: 2 Validation Loss: 3.140885829925537 \n",
      "     Validation Step: 3 Validation Loss: 3.0873050689697266 \n",
      "     Validation Step: 4 Validation Loss: 2.91176176071167 \n",
      "     Validation Step: 5 Validation Loss: 3.0080337524414062 \n",
      "     Validation Step: 6 Validation Loss: 3.7297306060791016 \n",
      "     Validation Step: 7 Validation Loss: 2.7209811210632324 \n",
      "     Validation Step: 8 Validation Loss: 3.4268012046813965 \n",
      "     Validation Step: 9 Validation Loss: 3.695101261138916 \n",
      "     Validation Step: 10 Validation Loss: 3.1860296726226807 \n",
      "     Validation Step: 11 Validation Loss: 3.9644832611083984 \n",
      "     Validation Step: 12 Validation Loss: 2.2774436473846436 \n",
      "     Validation Step: 13 Validation Loss: 2.793339252471924 \n",
      "     Validation Step: 14 Validation Loss: 3.7026450634002686 \n",
      "Epoch: 118\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.639540672302246 \n",
      "     Training Step: 1 Training Loss: 4.283015251159668 \n",
      "     Training Step: 2 Training Loss: 2.2183964252471924 \n",
      "     Training Step: 3 Training Loss: 2.7839431762695312 \n",
      "     Training Step: 4 Training Loss: 4.365029335021973 \n",
      "     Training Step: 5 Training Loss: 2.563002109527588 \n",
      "     Training Step: 6 Training Loss: 3.4435577392578125 \n",
      "     Training Step: 7 Training Loss: 3.5329084396362305 \n",
      "     Training Step: 8 Training Loss: 2.1181254386901855 \n",
      "     Training Step: 9 Training Loss: 3.241787910461426 \n",
      "     Training Step: 10 Training Loss: 3.0641772747039795 \n",
      "     Training Step: 11 Training Loss: 2.540522813796997 \n",
      "     Training Step: 12 Training Loss: 3.2491583824157715 \n",
      "     Training Step: 13 Training Loss: 3.1862130165100098 \n",
      "     Training Step: 14 Training Loss: 3.008882522583008 \n",
      "     Training Step: 15 Training Loss: 3.4835093021392822 \n",
      "     Training Step: 16 Training Loss: 2.5733795166015625 \n",
      "     Training Step: 17 Training Loss: 3.193955898284912 \n",
      "     Training Step: 18 Training Loss: 4.447254180908203 \n",
      "     Training Step: 19 Training Loss: 2.7416272163391113 \n",
      "     Training Step: 20 Training Loss: 3.8925094604492188 \n",
      "     Training Step: 21 Training Loss: 2.457242488861084 \n",
      "     Training Step: 22 Training Loss: 2.1317851543426514 \n",
      "     Training Step: 23 Training Loss: 3.753140687942505 \n",
      "     Training Step: 24 Training Loss: 2.799438953399658 \n",
      "     Training Step: 25 Training Loss: 2.6359617710113525 \n",
      "     Training Step: 26 Training Loss: 2.6391191482543945 \n",
      "     Training Step: 27 Training Loss: 3.0226352214813232 \n",
      "     Training Step: 28 Training Loss: 3.18959379196167 \n",
      "     Training Step: 29 Training Loss: 3.199270009994507 \n",
      "     Training Step: 30 Training Loss: 2.133272409439087 \n",
      "     Training Step: 31 Training Loss: 3.0509090423583984 \n",
      "     Training Step: 32 Training Loss: 2.9579033851623535 \n",
      "     Training Step: 33 Training Loss: 3.5375802516937256 \n",
      "     Training Step: 34 Training Loss: 3.7754180431365967 \n",
      "     Training Step: 35 Training Loss: 2.5564520359039307 \n",
      "     Training Step: 36 Training Loss: 3.0461208820343018 \n",
      "     Training Step: 37 Training Loss: 3.419884443283081 \n",
      "     Training Step: 38 Training Loss: 2.4861180782318115 \n",
      "     Training Step: 39 Training Loss: 2.359532356262207 \n",
      "     Training Step: 40 Training Loss: 2.5665206909179688 \n",
      "     Training Step: 41 Training Loss: 3.410984992980957 \n",
      "     Training Step: 42 Training Loss: 2.0673422813415527 \n",
      "     Training Step: 43 Training Loss: 2.1873769760131836 \n",
      "     Training Step: 44 Training Loss: 2.9589924812316895 \n",
      "     Training Step: 45 Training Loss: 3.23801851272583 \n",
      "     Training Step: 46 Training Loss: 3.355760335922241 \n",
      "     Training Step: 47 Training Loss: 2.781277656555176 \n",
      "     Training Step: 48 Training Loss: 2.8946235179901123 \n",
      "     Training Step: 49 Training Loss: 2.969050645828247 \n",
      "     Training Step: 50 Training Loss: 2.8366923332214355 \n",
      "     Training Step: 51 Training Loss: 2.7621686458587646 \n",
      "     Training Step: 52 Training Loss: 2.744326591491699 \n",
      "     Training Step: 53 Training Loss: 3.8569955825805664 \n",
      "     Training Step: 54 Training Loss: 2.991999864578247 \n",
      "     Training Step: 55 Training Loss: 2.7444286346435547 \n",
      "     Training Step: 56 Training Loss: 2.715848207473755 \n",
      "     Training Step: 57 Training Loss: 3.2288572788238525 \n",
      "     Training Step: 58 Training Loss: 2.6201374530792236 \n",
      "     Training Step: 59 Training Loss: 2.508044719696045 \n",
      "     Training Step: 60 Training Loss: 3.256213665008545 \n",
      "     Training Step: 61 Training Loss: 2.3679676055908203 \n",
      "     Training Step: 62 Training Loss: 3.6417887210845947 \n",
      "     Training Step: 63 Training Loss: 2.913808584213257 \n",
      "     Training Step: 64 Training Loss: 2.373244285583496 \n",
      "     Training Step: 65 Training Loss: 4.174419403076172 \n",
      "     Training Step: 66 Training Loss: 3.387328863143921 \n",
      "     Training Step: 67 Training Loss: 3.3940656185150146 \n",
      "     Training Step: 68 Training Loss: 2.5877633094787598 \n",
      "     Training Step: 69 Training Loss: 4.796707630157471 \n",
      "     Training Step: 70 Training Loss: 3.4730257987976074 \n",
      "     Training Step: 71 Training Loss: 3.1623592376708984 \n",
      "     Training Step: 72 Training Loss: 2.5333096981048584 \n",
      "     Training Step: 73 Training Loss: 2.3026559352874756 \n",
      "     Training Step: 74 Training Loss: 2.305558681488037 \n",
      "     Training Step: 75 Training Loss: 3.5059878826141357 \n",
      "     Training Step: 76 Training Loss: 2.1172213554382324 \n",
      "     Training Step: 77 Training Loss: 3.0664968490600586 \n",
      "     Training Step: 78 Training Loss: 2.8103227615356445 \n",
      "     Training Step: 79 Training Loss: 2.3036015033721924 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1147255897521973 \n",
      "     Validation Step: 1 Validation Loss: 3.0191171169281006 \n",
      "     Validation Step: 2 Validation Loss: 3.7175936698913574 \n",
      "     Validation Step: 3 Validation Loss: 3.492964267730713 \n",
      "     Validation Step: 4 Validation Loss: 3.6346864700317383 \n",
      "     Validation Step: 5 Validation Loss: 3.567767381668091 \n",
      "     Validation Step: 6 Validation Loss: 3.811770439147949 \n",
      "     Validation Step: 7 Validation Loss: 2.29551362991333 \n",
      "     Validation Step: 8 Validation Loss: 3.2547178268432617 \n",
      "     Validation Step: 9 Validation Loss: 3.560353994369507 \n",
      "     Validation Step: 10 Validation Loss: 2.904757261276245 \n",
      "     Validation Step: 11 Validation Loss: 2.8481850624084473 \n",
      "     Validation Step: 12 Validation Loss: 2.587007999420166 \n",
      "     Validation Step: 13 Validation Loss: 2.59348726272583 \n",
      "     Validation Step: 14 Validation Loss: 2.984287738800049 \n",
      "Epoch: 119\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.3443241119384766 \n",
      "     Training Step: 1 Training Loss: 2.3782973289489746 \n",
      "     Training Step: 2 Training Loss: 2.693917751312256 \n",
      "     Training Step: 3 Training Loss: 4.337988376617432 \n",
      "     Training Step: 4 Training Loss: 2.915689468383789 \n",
      "     Training Step: 5 Training Loss: 2.2437186241149902 \n",
      "     Training Step: 6 Training Loss: 2.3967041969299316 \n",
      "     Training Step: 7 Training Loss: 2.4045987129211426 \n",
      "     Training Step: 8 Training Loss: 2.1279029846191406 \n",
      "     Training Step: 9 Training Loss: 2.857691764831543 \n",
      "     Training Step: 10 Training Loss: 3.0848872661590576 \n",
      "     Training Step: 11 Training Loss: 3.396183490753174 \n",
      "     Training Step: 12 Training Loss: 2.5701677799224854 \n",
      "     Training Step: 13 Training Loss: 3.94398832321167 \n",
      "     Training Step: 14 Training Loss: 2.823821783065796 \n",
      "     Training Step: 15 Training Loss: 2.5236527919769287 \n",
      "     Training Step: 16 Training Loss: 3.7320163249969482 \n",
      "     Training Step: 17 Training Loss: 2.272620677947998 \n",
      "     Training Step: 18 Training Loss: 2.682178497314453 \n",
      "     Training Step: 19 Training Loss: 3.758547782897949 \n",
      "     Training Step: 20 Training Loss: 2.5161690711975098 \n",
      "     Training Step: 21 Training Loss: 2.6650407314300537 \n",
      "     Training Step: 22 Training Loss: 3.2932426929473877 \n",
      "     Training Step: 23 Training Loss: 3.144105911254883 \n",
      "     Training Step: 24 Training Loss: 2.9515600204467773 \n",
      "     Training Step: 25 Training Loss: 2.2638449668884277 \n",
      "     Training Step: 26 Training Loss: 2.3301165103912354 \n",
      "     Training Step: 27 Training Loss: 3.0524203777313232 \n",
      "     Training Step: 28 Training Loss: 2.2057011127471924 \n",
      "     Training Step: 29 Training Loss: 2.9324142932891846 \n",
      "     Training Step: 30 Training Loss: 2.828829765319824 \n",
      "     Training Step: 31 Training Loss: 3.3273556232452393 \n",
      "     Training Step: 32 Training Loss: 3.2890725135803223 \n",
      "     Training Step: 33 Training Loss: 3.497802972793579 \n",
      "     Training Step: 34 Training Loss: 3.0948591232299805 \n",
      "     Training Step: 35 Training Loss: 3.060394048690796 \n",
      "     Training Step: 36 Training Loss: 2.5205585956573486 \n",
      "     Training Step: 37 Training Loss: 3.150892734527588 \n",
      "     Training Step: 38 Training Loss: 3.5374817848205566 \n",
      "     Training Step: 39 Training Loss: 2.467552661895752 \n",
      "     Training Step: 40 Training Loss: 3.2734904289245605 \n",
      "     Training Step: 41 Training Loss: 2.194334030151367 \n",
      "     Training Step: 42 Training Loss: 3.553924083709717 \n",
      "     Training Step: 43 Training Loss: 2.812267541885376 \n",
      "     Training Step: 44 Training Loss: 2.6738240718841553 \n",
      "     Training Step: 45 Training Loss: 3.1672093868255615 \n",
      "     Training Step: 46 Training Loss: 3.2831621170043945 \n",
      "     Training Step: 47 Training Loss: 2.544614791870117 \n",
      "     Training Step: 48 Training Loss: 2.9051804542541504 \n",
      "     Training Step: 49 Training Loss: 3.3119606971740723 \n",
      "     Training Step: 50 Training Loss: 2.998565673828125 \n",
      "     Training Step: 51 Training Loss: 4.29559850692749 \n",
      "     Training Step: 52 Training Loss: 2.979539394378662 \n",
      "     Training Step: 53 Training Loss: 2.3454740047454834 \n",
      "     Training Step: 54 Training Loss: 3.712247133255005 \n",
      "     Training Step: 55 Training Loss: 3.659151077270508 \n",
      "     Training Step: 56 Training Loss: 2.971073865890503 \n",
      "     Training Step: 57 Training Loss: 3.4549670219421387 \n",
      "     Training Step: 58 Training Loss: 2.9371132850646973 \n",
      "     Training Step: 59 Training Loss: 2.6655631065368652 \n",
      "     Training Step: 60 Training Loss: 4.024302959442139 \n",
      "     Training Step: 61 Training Loss: 2.088395118713379 \n",
      "     Training Step: 62 Training Loss: 4.070937156677246 \n",
      "     Training Step: 63 Training Loss: 2.758918285369873 \n",
      "     Training Step: 64 Training Loss: 3.451594352722168 \n",
      "     Training Step: 65 Training Loss: 2.148129463195801 \n",
      "     Training Step: 66 Training Loss: 4.707956790924072 \n",
      "     Training Step: 67 Training Loss: 2.3271262645721436 \n",
      "     Training Step: 68 Training Loss: 3.3669896125793457 \n",
      "     Training Step: 69 Training Loss: 3.221914768218994 \n",
      "     Training Step: 70 Training Loss: 2.9770069122314453 \n",
      "     Training Step: 71 Training Loss: 2.3546454906463623 \n",
      "     Training Step: 72 Training Loss: 2.9626877307891846 \n",
      "     Training Step: 73 Training Loss: 3.362628936767578 \n",
      "     Training Step: 74 Training Loss: 2.927015781402588 \n",
      "     Training Step: 75 Training Loss: 3.3729238510131836 \n",
      "     Training Step: 76 Training Loss: 2.9957199096679688 \n",
      "     Training Step: 77 Training Loss: 3.1210641860961914 \n",
      "     Training Step: 78 Training Loss: 2.5691945552825928 \n",
      "     Training Step: 79 Training Loss: 2.768237352371216 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6733455657958984 \n",
      "     Validation Step: 1 Validation Loss: 3.019639492034912 \n",
      "     Validation Step: 2 Validation Loss: 3.272742509841919 \n",
      "     Validation Step: 3 Validation Loss: 2.9232053756713867 \n",
      "     Validation Step: 4 Validation Loss: 3.38655424118042 \n",
      "     Validation Step: 5 Validation Loss: 2.2692501544952393 \n",
      "     Validation Step: 6 Validation Loss: 3.1075360774993896 \n",
      "     Validation Step: 7 Validation Loss: 3.1542763710021973 \n",
      "     Validation Step: 8 Validation Loss: 3.624091386795044 \n",
      "     Validation Step: 9 Validation Loss: 2.7081170082092285 \n",
      "     Validation Step: 10 Validation Loss: 3.0489721298217773 \n",
      "     Validation Step: 11 Validation Loss: 3.6231274604797363 \n",
      "     Validation Step: 12 Validation Loss: 3.8984642028808594 \n",
      "     Validation Step: 13 Validation Loss: 3.6296353340148926 \n",
      "     Validation Step: 14 Validation Loss: 2.6839590072631836 \n",
      "Epoch: 120\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.8184571266174316 \n",
      "     Training Step: 1 Training Loss: 2.7674596309661865 \n",
      "     Training Step: 2 Training Loss: 3.4459853172302246 \n",
      "     Training Step: 3 Training Loss: 4.399515151977539 \n",
      "     Training Step: 4 Training Loss: 2.3217082023620605 \n",
      "     Training Step: 5 Training Loss: 3.232769727706909 \n",
      "     Training Step: 6 Training Loss: 2.256201982498169 \n",
      "     Training Step: 7 Training Loss: 2.7522172927856445 \n",
      "     Training Step: 8 Training Loss: 3.6892247200012207 \n",
      "     Training Step: 9 Training Loss: 3.0006022453308105 \n",
      "     Training Step: 10 Training Loss: 3.7134017944335938 \n",
      "     Training Step: 11 Training Loss: 2.522428512573242 \n",
      "     Training Step: 12 Training Loss: 2.5861051082611084 \n",
      "     Training Step: 13 Training Loss: 2.393810510635376 \n",
      "     Training Step: 14 Training Loss: 2.4002747535705566 \n",
      "     Training Step: 15 Training Loss: 2.5161662101745605 \n",
      "     Training Step: 16 Training Loss: 4.060181617736816 \n",
      "     Training Step: 17 Training Loss: 2.6435325145721436 \n",
      "     Training Step: 18 Training Loss: 2.1327714920043945 \n",
      "     Training Step: 19 Training Loss: 3.3853721618652344 \n",
      "     Training Step: 20 Training Loss: 3.7346372604370117 \n",
      "     Training Step: 21 Training Loss: 2.454202175140381 \n",
      "     Training Step: 22 Training Loss: 2.099893093109131 \n",
      "     Training Step: 23 Training Loss: 4.218985080718994 \n",
      "     Training Step: 24 Training Loss: 2.1746931076049805 \n",
      "     Training Step: 25 Training Loss: 2.9654293060302734 \n",
      "     Training Step: 26 Training Loss: 3.4265940189361572 \n",
      "     Training Step: 27 Training Loss: 2.872528553009033 \n",
      "     Training Step: 28 Training Loss: 2.801743984222412 \n",
      "     Training Step: 29 Training Loss: 2.5411057472229004 \n",
      "     Training Step: 30 Training Loss: 3.622469902038574 \n",
      "     Training Step: 31 Training Loss: 3.35349178314209 \n",
      "     Training Step: 32 Training Loss: 2.562302589416504 \n",
      "     Training Step: 33 Training Loss: 3.2368721961975098 \n",
      "     Training Step: 34 Training Loss: 2.923961877822876 \n",
      "     Training Step: 35 Training Loss: 2.9005837440490723 \n",
      "     Training Step: 36 Training Loss: 2.950882911682129 \n",
      "     Training Step: 37 Training Loss: 3.169602394104004 \n",
      "     Training Step: 38 Training Loss: 3.487182140350342 \n",
      "     Training Step: 39 Training Loss: 3.1479525566101074 \n",
      "     Training Step: 40 Training Loss: 2.3862578868865967 \n",
      "     Training Step: 41 Training Loss: 2.6191916465759277 \n",
      "     Training Step: 42 Training Loss: 2.8667616844177246 \n",
      "     Training Step: 43 Training Loss: 2.1541309356689453 \n",
      "     Training Step: 44 Training Loss: 3.410121440887451 \n",
      "     Training Step: 45 Training Loss: 2.879927635192871 \n",
      "     Training Step: 46 Training Loss: 3.0943961143493652 \n",
      "     Training Step: 47 Training Loss: 2.939063310623169 \n",
      "     Training Step: 48 Training Loss: 2.736440420150757 \n",
      "     Training Step: 49 Training Loss: 2.1512720584869385 \n",
      "     Training Step: 50 Training Loss: 3.412177085876465 \n",
      "     Training Step: 51 Training Loss: 2.204451084136963 \n",
      "     Training Step: 52 Training Loss: 3.755263328552246 \n",
      "     Training Step: 53 Training Loss: 3.0738682746887207 \n",
      "     Training Step: 54 Training Loss: 2.936056137084961 \n",
      "     Training Step: 55 Training Loss: 3.4027888774871826 \n",
      "     Training Step: 56 Training Loss: 3.3913867473602295 \n",
      "     Training Step: 57 Training Loss: 4.736418724060059 \n",
      "     Training Step: 58 Training Loss: 3.5709056854248047 \n",
      "     Training Step: 59 Training Loss: 3.892063856124878 \n",
      "     Training Step: 60 Training Loss: 2.777259349822998 \n",
      "     Training Step: 61 Training Loss: 3.19533634185791 \n",
      "     Training Step: 62 Training Loss: 2.9460530281066895 \n",
      "     Training Step: 63 Training Loss: 2.9682533740997314 \n",
      "     Training Step: 64 Training Loss: 3.6004772186279297 \n",
      "     Training Step: 65 Training Loss: 2.760150909423828 \n",
      "     Training Step: 66 Training Loss: 4.445935249328613 \n",
      "     Training Step: 67 Training Loss: 3.3407087326049805 \n",
      "     Training Step: 68 Training Loss: 2.65269136428833 \n",
      "     Training Step: 69 Training Loss: 2.1738007068634033 \n",
      "     Training Step: 70 Training Loss: 3.171143054962158 \n",
      "     Training Step: 71 Training Loss: 2.4115469455718994 \n",
      "     Training Step: 72 Training Loss: 3.09187912940979 \n",
      "     Training Step: 73 Training Loss: 3.08156156539917 \n",
      "     Training Step: 74 Training Loss: 2.4558913707733154 \n",
      "     Training Step: 75 Training Loss: 2.122551202774048 \n",
      "     Training Step: 76 Training Loss: 2.921473741531372 \n",
      "     Training Step: 77 Training Loss: 3.0631933212280273 \n",
      "     Training Step: 78 Training Loss: 2.3566360473632812 \n",
      "     Training Step: 79 Training Loss: 2.889777660369873 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6084280014038086 \n",
      "     Validation Step: 1 Validation Loss: 3.2343337535858154 \n",
      "     Validation Step: 2 Validation Loss: 3.1725916862487793 \n",
      "     Validation Step: 3 Validation Loss: 3.5506176948547363 \n",
      "     Validation Step: 4 Validation Loss: 2.6643166542053223 \n",
      "     Validation Step: 5 Validation Loss: 3.055948257446289 \n",
      "     Validation Step: 6 Validation Loss: 3.697077751159668 \n",
      "     Validation Step: 7 Validation Loss: 2.2536826133728027 \n",
      "     Validation Step: 8 Validation Loss: 3.704617500305176 \n",
      "     Validation Step: 9 Validation Loss: 3.975788116455078 \n",
      "     Validation Step: 10 Validation Loss: 3.033381938934326 \n",
      "     Validation Step: 11 Validation Loss: 2.691626787185669 \n",
      "     Validation Step: 12 Validation Loss: 2.9022395610809326 \n",
      "     Validation Step: 13 Validation Loss: 3.058525323867798 \n",
      "     Validation Step: 14 Validation Loss: 3.484422206878662 \n"
     ]
    }
   ],
   "source": [
    "for i in range(61, 120, 20) :\n",
    "    start_epoch = i\n",
    "    num_epochs = 20 + i\n",
    "    loss_weights = (1.0, 1.0, 1.0)\n",
    "    model_name = \"base_model_unsupervised_physics_constrained.pt\"\n",
    "    train_input,train_output,val_input, val_output = train_validate_ACOPF(model, model_name, optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-593.8486800193787 -243.53982478380203\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 9.08173089e-01, -1.83501959e+00, -5.90927734e+01,\n        -4.32426147e+01],\n       [ 9.80060136e-01,  0.00000000e+00,  9.09702148e+01,\n        -3.20258598e+01],\n       [ 1.03807630e+00, -1.48351908e+00, -5.60315399e+01,\n        -4.20662727e+01],\n       [ 1.03758441e+00,  1.54328156e+00, -8.17986012e-01,\n         7.37901390e-01],\n       [ 1.02849822e+00,  1.60070038e+00,  2.79828215e+00,\n         8.60434711e-01],\n       [ 1.02379331e+00,  1.61828518e+00,  2.95484495e+00,\n         1.07658315e+00],\n       [ 1.05319075e+00,  1.20048451e+00, -6.75554800e+00,\n        -1.33368182e+00],\n       [ 1.04970037e+00,  9.24345732e-01, -1.38833284e+01,\n        -2.18041611e+00],\n       [ 1.05298337e+00,  1.13721991e+00, -8.29765701e+00,\n        -1.68316507e+00],\n       [ 1.03909489e+00,  2.39133453e+00,  2.30472603e+01,\n         6.45330811e+00],\n       [ 1.06325094e+00, -8.12291145e-01, -5.12167511e+01,\n        -1.27561131e+01],\n       [ 1.06275891e+00,  5.54805279e-01, -2.24758530e+01,\n        -7.71057844e+00],\n       [ 1.11087286e+00, -1.08034587e+00, -5.49251671e+01,\n        -2.01222477e+01],\n       [ 1.04399310e+00,  6.38606310e-01, -2.13300972e+01,\n        -3.76218772e+00],\n       [ 1.03638708e+00,  1.57836246e+00, -1.58142567e-01,\n        -2.26446450e-01],\n       [ 1.11484070e+00,  2.71129608e+00,  3.74011345e+01,\n         1.65829735e+01],\n       [ 1.04800790e+00,  1.04877901e+00, -1.08388004e+01,\n        -9.84343350e-01],\n       [ 1.06187564e+00,  4.20533657e-01, -2.57897511e+01,\n        -8.35899353e+00],\n       [ 1.04667220e+00,  8.54233503e-01, -1.57999144e+01,\n        -2.50964928e+00],\n       [ 1.06539757e+00, -6.87378645e-01, -4.90792694e+01,\n        -1.07747526e+01],\n       [ 1.02700549e+00,  1.60033560e+00,  2.77895689e+00,\n         9.69036281e-01],\n       [ 1.03543417e+00,  1.56838799e+00, -4.76727962e-01,\n         2.44593799e-01],\n       [ 1.02984647e+00,  1.59551716e+00,  2.70907640e+00,\n         8.00673664e-01],\n       [ 1.06586664e+00,  5.02006054e-01, -2.36038208e+01,\n        -8.46225548e+00],\n       [ 1.04963691e+00,  1.20602751e+00, -7.01246834e+00,\n        -4.66603100e-01],\n       [ 1.03440482e+00,  1.59806633e+00, -1.05355740e-01,\n        -1.30631268e-01],\n       [ 1.09338767e+00,  2.67419767e+00,  3.54734154e+01,\n         1.68675766e+01],\n       [ 1.05735051e+00,  4.46565151e-02, -3.45174599e+01,\n        -1.08645945e+01],\n       [ 1.04879539e+00,  1.21448874e+00, -6.53895283e+00,\n        -1.32367253e+00],\n       [ 1.04342277e+00,  1.09987926e+00, -9.51759434e+00,\n        -2.23112369e+00],\n       [ 1.10490709e+00, -3.70104313e-02, -3.47962761e+01,\n        -1.86017208e+01],\n       [ 1.04623039e+00,  1.01613450e+00, -1.17663660e+01,\n        -1.00340390e+00],\n       [ 1.02900515e+00,  1.59984922e+00,  2.79258871e+00,\n         8.08014095e-01],\n       [ 1.02530927e+00,  1.62475967e+00,  3.26003313e+00,\n         1.46038699e+00],\n       [ 1.02368844e+00,  1.60961866e+00,  3.02610922e+00,\n         1.62239432e+00],\n       [ 1.06201997e+00, -9.00850296e-02, -3.73605499e+01,\n        -1.08894148e+01],\n       [ 1.03693182e+00,  1.56488276e+00, -3.30797195e-01,\n        -6.23310208e-02],\n       [ 1.06338501e+00,  1.99042082e-01, -3.09224625e+01,\n        -9.73800850e+00],\n       [ 1.04240612e+00,  1.27646208e+00, -5.38701153e+00,\n        -8.20039093e-01],\n       [ 1.04953565e+00,  1.17001128e+00, -7.57848072e+00,\n        -1.56568027e+00],\n       [ 1.06356250e+00,  5.47959566e-01, -2.25301609e+01,\n        -8.53936100e+00],\n       [ 1.05183709e+00,  1.23778319e+00, -5.87703037e+00,\n        -1.50604749e+00],\n       [ 1.05130449e+00,  1.22794557e+00, -6.15515661e+00,\n        -1.32155252e+00],\n       [ 1.05308221e+00,  1.15811086e+00, -7.79037762e+00,\n        -1.58836484e+00],\n       [ 1.04220928e+00,  6.41660690e-01, -2.13405972e+01,\n        -3.74810171e+00],\n       [ 1.04101694e+00,  3.45329285e-01, -2.85472622e+01,\n        -3.89841413e+00],\n       [ 1.05179631e+00,  1.21071792e+00, -6.54533386e+00,\n        -1.52291942e+00],\n       [ 1.04534447e+00,  2.38566589e+00,  2.35531597e+01,\n         7.09362507e+00],\n       [ 1.05292761e+00,  1.19509006e+00, -6.91728497e+00,\n        -1.41930890e+00],\n       [ 1.03545109e+00,  2.31351423e+00,  2.11422005e+01,\n         6.24062538e+00],\n       [ 1.02475579e+00,  1.61665726e+00,  3.06926107e+00,\n         1.43117452e+00],\n       [ 1.02805141e+00,  1.60891628e+00,  2.97897768e+00,\n         1.07247901e+00],\n       [ 1.05195403e+00,  1.03628588e+00, -1.08744020e+01,\n        -1.90766454e+00],\n       [ 1.05946364e+00,  4.51036692e-01, -2.51464062e+01,\n        -8.16252136e+00],\n       [ 1.05379444e+00,  1.13885927e+00, -8.23964405e+00,\n        -1.66914129e+00],\n       [ 1.04030803e+00,  3.25567722e-01, -2.89689369e+01,\n        -5.03463173e+00],\n       [ 1.05277446e+00,  1.11298800e+00, -8.89925766e+00,\n        -1.62536550e+00],\n       [ 1.05321558e+00,  1.16000962e+00, -7.73745680e+00,\n        -1.55750155e+00],\n       [ 1.02827717e+00,  1.60002565e+00,  2.76085043e+00,\n         8.79548252e-01],\n       [ 1.05259497e+00,  1.20167589e+00, -6.75953770e+00,\n        -1.40296769e+00],\n       [ 1.05170781e+00,  9.95320559e-01, -1.19557323e+01,\n        -1.98313093e+00],\n       [ 1.05246201e+00,  1.10347557e+00, -9.11016560e+00,\n        -1.74773812e+00],\n       [ 1.04958122e+00,  8.82838488e-01, -1.49465609e+01,\n        -2.44760275e+00],\n       [ 1.05187961e+00,  1.07718349e+00, -9.81683922e+00,\n        -1.76201677e+00]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(train_output.output[:, 2]), sum(train_output.output[:, 3]))\n",
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-511.95279839634895 -215.83535793423653\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 9.07937301e-01, -1.87421203e+00, -5.86987801e+01,\n        -4.31370201e+01],\n       [ 9.78537469e-01,  0.00000000e+00,  9.15387573e+01,\n        -3.18711929e+01],\n       [ 1.03759433e+00, -1.51700187e+00, -5.56474800e+01,\n        -4.19558372e+01],\n       [ 1.03786753e+00,  1.56929421e+00, -2.91552931e-01,\n         5.73150635e-01],\n       [ 1.02684687e+00,  1.61438823e+00,  2.79138613e+00,\n         9.32665348e-01],\n       [ 1.02212753e+00,  1.63119197e+00,  2.91438818e+00,\n         1.13415098e+00],\n       [ 1.05419103e+00,  1.20319247e+00, -6.55964136e+00,\n        -1.39674544e+00],\n       [ 1.05039056e+00,  9.21392202e-01, -1.37815561e+01,\n        -2.24824166e+00],\n       [ 1.05342338e+00,  1.15047312e+00, -7.85218763e+00,\n        -1.60835695e+00],\n       [ 1.03538874e+00,  2.41840339e+00,  2.30585728e+01,\n         6.94025660e+00],\n       [ 1.06390915e+00, -8.37588310e-01, -5.08798523e+01,\n        -1.26762056e+01],\n       [ 1.06576177e+00,  5.09796381e-01, -2.31083965e+01,\n        -8.29451561e+00],\n       [ 1.10853521e+00, -1.09562016e+00, -5.43942871e+01,\n        -1.96498795e+01],\n       [ 1.04392242e+00,  6.43275738e-01, -2.09690762e+01,\n        -3.62716198e+00],\n       [ 1.03664391e+00,  1.60454869e+00,  3.56376737e-01,\n        -3.78850937e-01],\n       [ 1.11313026e+00,  2.74871421e+00,  3.75459595e+01,\n         1.64656582e+01],\n       [ 1.04838319e+00,  1.05062175e+00, -1.06897240e+01,\n        -9.46598053e-01],\n       [ 1.06335775e+00,  4.12539959e-01, -2.55640736e+01,\n        -8.69000244e+00],\n       [ 1.04673282e+00,  8.57007980e-01, -1.55750837e+01,\n        -2.40364528e+00],\n       [ 1.06625338e+00, -7.12612152e-01, -4.87778969e+01,\n        -1.07718163e+01],\n       [ 1.02520024e+00,  1.60813451e+00,  2.57109118e+00,\n         8.72414589e-01],\n       [ 1.03570543e+00,  1.59455371e+00,  4.24204767e-02,\n         8.58158916e-02],\n       [ 1.02876060e+00,  1.61393094e+00,  2.87852621e+00,\n         9.05761242e-01],\n       [ 1.06686214e+00,  4.90499735e-01, -2.35167732e+01,\n        -8.59967327e+00],\n       [ 1.05005965e+00,  1.21517229e+00, -6.70573092e+00,\n        -3.89456272e-01],\n       [ 1.03465972e+00,  1.62460160e+00,  4.10573572e-01,\n        -2.82017231e-01],\n       [ 1.10395910e+00,  2.77285409e+00,  3.75991096e+01,\n         1.66821957e+01],\n       [ 1.05832637e+00,  1.22981071e-02, -3.46597633e+01,\n        -1.07392855e+01],\n       [ 1.04958635e+00,  1.21394610e+00, -6.45953321e+00,\n        -1.32239532e+00],\n       [ 1.04360254e+00,  1.10471535e+00, -9.33998680e+00,\n        -2.11040664e+00],\n       [ 1.03382131e+00,  2.90489364e+00,  3.31208191e+01,\n         8.79098511e+00],\n       [ 1.04661796e+00,  1.01696968e+00, -1.16286840e+01,\n        -9.67914581e-01],\n       [ 1.02757374e+00,  1.61341977e+00,  2.78863668e+00,\n         8.24258327e-01],\n       [ 1.02408746e+00,  1.63723826e+00,  3.22514486e+00,\n         1.43825531e+00],\n       [ 1.02224260e+00,  1.62494397e+00,  3.07503557e+00,\n         1.71696568e+00],\n       [ 1.06244354e+00, -9.74833965e-02, -3.69519081e+01,\n        -1.09176979e+01],\n       [ 1.03718948e+00,  1.59092259e+00,  1.84925169e-01,\n        -2.17633739e-01],\n       [ 1.06213760e+00,  2.03928709e-01, -3.04188366e+01,\n        -9.28130627e+00],\n       [ 1.04291569e+00,  1.28545308e+00, -5.10510206e+00,\n        -7.30353355e-01],\n       [ 1.05034270e+00,  1.17709088e+00, -7.27609539e+00,\n        -1.56540537e+00],\n       [ 1.06302365e+00,  5.46265602e-01, -2.23000622e+01,\n        -8.24017239e+00],\n       [ 1.05251389e+00,  1.24390006e+00, -5.63780022e+00,\n        -1.48716426e+00],\n       [ 1.05183092e+00,  1.22932982e+00, -6.05599022e+00,\n        -1.26505089e+00],\n       [ 1.05366565e+00,  1.16302705e+00, -7.56571531e+00,\n        -1.54746771e+00],\n       [ 1.04278280e+00,  6.29742146e-01, -2.13358345e+01,\n        -3.73330069e+00],\n       [ 1.04147651e+00,  3.35354567e-01, -2.83615379e+01,\n        -3.88021851e+00],\n       [ 1.05260807e+00,  1.21566653e+00, -6.31445551e+00,\n        -1.53221798e+00],\n       [ 1.03565771e+00,  2.35670972e+00,  2.17177143e+01,\n         6.64623117e+00],\n       [ 1.05362105e+00,  1.19829369e+00, -6.74004698e+00,\n        -1.40678382e+00],\n       [ 1.03633076e+00,  2.34963393e+00,  2.15431976e+01,\n         6.00116968e+00],\n       [ 1.02374913e+00,  1.63063979e+00,  3.08254194e+00,\n         1.36198950e+00],\n       [ 1.02660273e+00,  1.62159944e+00,  2.94081450e+00,\n         1.06041336e+00],\n       [ 1.05243177e+00,  1.03496718e+00, -1.07991285e+01,\n        -1.89943600e+00],\n       [ 1.05596931e+00,  4.97182846e-01, -2.37191906e+01,\n        -8.76403713e+00],\n       [ 1.05413430e+00,  1.14878464e+00, -7.89678144e+00,\n        -1.56792021e+00],\n       [ 1.04069984e+00,  3.14863920e-01, -2.87940044e+01,\n        -4.97088480e+00],\n       [ 1.05342858e+00,  1.11741328e+00, -8.65817070e+00,\n        -1.60007501e+00],\n       [ 1.05359691e+00,  1.16640639e+00, -7.49538946e+00,\n        -1.46536803e+00],\n       [ 1.02733265e+00,  1.61643720e+00,  2.85800314e+00,\n         9.82812881e-01],\n       [ 1.05317327e+00,  1.20813537e+00, -6.50923920e+00,\n        -1.36070895e+00],\n       [ 1.05211938e+00,  9.96773243e-01, -1.17909842e+01,\n        -1.98481584e+00],\n       [ 1.05259060e+00,  1.11171269e+00, -8.83128929e+00,\n        -1.61827540e+00],\n       [ 1.05000777e+00,  8.75979900e-01, -1.49445992e+01,\n        -2.42398238e+00],\n       [ 1.05225712e+00,  1.08231473e+00, -9.59457111e+00,\n        -1.72301269e+00]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(val_output.output[:, 2]), sum(val_output.output[:, 3]))\n",
    "val_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_ACOPFGNN_model(model: ACOPFGNN):\n",
    "    path = r\"./Models/SelfSupervised/hetero_model_unsupervised.pt\"\n",
    "\n",
    "    torch.save(model.state_dict(), path)\n",
    "save_ACOPFGNN_model(embedder_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CHAINED ACOPF MODELS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'SB': tensor([[-0.9018,  0.6162, -0.5444,  0.6173]], grad_fn=<ViewBackward0>), 'PQ': tensor([[ 2.3276, -0.0358,  1.1119,  1.2931],\n",
      "        [ 1.4120,  0.0319,  5.0174, -1.9771],\n",
      "        [-4.0095,  4.6728,  3.7275, -0.0968],\n",
      "        [ 4.4933,  0.9351,  1.4761,  1.5370],\n",
      "        [-2.6381,  0.3272,  0.2074, -1.3705],\n",
      "        [ 4.6313,  4.3858,  4.6025,  1.8075],\n",
      "        [-0.8091,  0.2785, -1.5325, -7.5600],\n",
      "        [ 3.9265, -1.6268, -0.5522, -2.7840],\n",
      "        [ 3.3903, -3.8023,  3.9481,  3.2486],\n",
      "        [ 2.7211,  0.0281,  2.6178, -0.4197],\n",
      "        [-1.0556, -2.2014, -1.2151, -0.9475],\n",
      "        [ 1.0045, -2.1738, -0.3324,  0.2518],\n",
      "        [-6.1500, -5.2787,  4.1116, -2.1850],\n",
      "        [ 3.4343,  0.1623, -3.5839, -0.8874],\n",
      "        [-4.1475, -0.2775,  0.0306,  1.4033],\n",
      "        [ 3.8743, -0.4185, -4.1092,  1.2905]], grad_fn=<ViewBackward0>), 'PV': tensor([[ 1.3684, -0.7577, -2.1886, -0.1629],\n",
      "        [-0.4488, -0.1512, -2.2093,  1.1575],\n",
      "        [ 0.0744,  0.9206,  1.2590,  2.2426],\n",
      "        [-1.8071, -2.2218, -0.2814, -2.2066],\n",
      "        [ 1.2492,  1.9957, -0.9240, -0.8702],\n",
      "        [-1.0153,  0.8703, -1.4261, -1.8821],\n",
      "        [-0.3964, -0.3827, -0.2664,  1.2847],\n",
      "        [-1.4588,  0.2244,  1.5746, -0.0927],\n",
      "        [-0.3222,  0.1224,  1.4991,  0.4502],\n",
      "        [ 2.6522,  1.9504, -1.6547, -1.0996],\n",
      "        [-0.3359,  0.2305,  1.2821,  2.0201],\n",
      "        [-0.6111, -0.5200,  0.7534,  1.9362],\n",
      "        [-0.3854, -0.5980,  0.6830,  1.1645],\n",
      "        [-3.1753,  0.2901, -0.2495,  1.0761],\n",
      "        [-0.5234, -0.3743, -0.9159,  1.3701],\n",
      "        [-0.7126, -2.0468, -0.9429,  0.7262],\n",
      "        [ 2.4877,  1.9214, -0.0791,  2.3266],\n",
      "        [ 1.6947,  0.7835,  1.5060,  1.7149],\n",
      "        [ 0.8879,  0.7585, -0.9714, -0.2809],\n",
      "        [-1.6248, -0.9718, -2.4392, -0.0931],\n",
      "        [-1.8686,  0.7332, -0.2021, -0.4967],\n",
      "        [-0.8189,  0.9839,  0.5722,  1.5931],\n",
      "        [ 1.0404,  2.5547,  1.3051, -1.0792],\n",
      "        [ 2.8001, -1.0714, -2.8966,  1.1079],\n",
      "        [ 0.4635,  0.5342,  0.8638, -0.4073],\n",
      "        [ 0.6269, -0.4888,  3.0131,  0.6962],\n",
      "        [ 0.6537,  0.3639, -0.2504, -0.7723],\n",
      "        [ 0.4328,  2.3833,  0.2089, -2.8523],\n",
      "        [ 0.3647,  0.3392,  0.5419, -0.0736],\n",
      "        [ 0.3369,  0.2259,  0.6902, -0.3154],\n",
      "        [-0.5891, -1.4792,  1.7074, -0.3224],\n",
      "        [-0.6279,  0.8149,  1.4943,  2.8964],\n",
      "        [-2.0664,  0.8759, -1.4180,  1.8038],\n",
      "        [ 1.3756,  0.3103,  1.8730, -0.2954],\n",
      "        [ 0.9619,  1.0395,  2.2169, -1.8955],\n",
      "        [-1.4668, -0.4774,  0.9156,  0.0793],\n",
      "        [-0.4378, -0.3967,  0.9684,  0.9361],\n",
      "        [-0.6130,  0.1768, -1.0584,  0.0910],\n",
      "        [-1.9337,  1.3519, -1.1984, -0.6721],\n",
      "        [-1.6073, -0.1920, -2.6832, -1.2618],\n",
      "        [-0.2145,  0.9572, -0.7517, -0.3345],\n",
      "        [-0.0153, -0.8913, -0.4180, -1.6215]], grad_fn=<ViewBackward0>), 'NB': tensor([[ 2.6810, -1.5091, -3.2318, -2.6835],\n",
      "        [ 3.6574, -1.9658, -4.3074,  0.8760],\n",
      "        [-2.9540,  1.2744, -4.0626, -0.1938],\n",
      "        [ 5.7889, -5.3212, -2.8218,  0.0292],\n",
      "        [ 3.7331, -0.7910, -1.1800, -2.8636]], grad_fn=<ViewBackward0>)})\n"
     ]
    }
   ],
   "source": [
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "embedder_model = load_ACOPFGeneral_model(grid_name, \"hetero_model_bus.pt\",16,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'SB': tensor([[ 5.9153, -0.1487, -1.9773,  6.3468]], grad_fn=<CatBackward0>), 'PQ': tensor([[-0.2141, -0.0308,  0.2188,  0.1954],\n",
      "        [-0.0782, -0.5282,  0.2190,  0.1954],\n",
      "        [-0.4411,  0.3749,  0.4222,  0.4817],\n",
      "        [-0.0600,  1.5440,  0.8493,  0.5651],\n",
      "        [-0.2426, -1.8049,  0.2189,  0.1970],\n",
      "        [-0.0842, -0.4986,  0.2209,  0.1980],\n",
      "        [-0.3681, -0.0104,  0.8526,  0.7059],\n",
      "        [-0.4406,  0.8670,  0.9460,  0.2177],\n",
      "        [-0.1032, -1.5601,  0.2229,  0.1970],\n",
      "        [-0.2986, -1.9646,  0.2227,  0.1973],\n",
      "        [-0.2111,  1.1219,  0.2204,  0.1967],\n",
      "        [-0.4429,  1.3490,  0.3457,  0.4921],\n",
      "        [-0.3948,  0.0455,  0.7214,  0.3177],\n",
      "        [-0.2853,  0.2446,  0.2199,  0.1980],\n",
      "        [-0.4306, -1.1787,  0.2182,  0.1972],\n",
      "        [-0.3970, -0.5784,  0.2182,  0.1964]], grad_fn=<CatBackward0>), 'PV': tensor([[ 4.8654e+00,  3.5560e+00, -4.3368e+00,  1.5640e-01],\n",
      "        [ 2.4065e+00, -3.4883e+00,  1.6011e-01,  1.6452e-01],\n",
      "        [-4.5114e-01, -1.5633e+00,  1.1286e-01,  1.1765e-01],\n",
      "        [-3.2758e-01,  3.2316e+00, -1.3504e-01, -1.6638e-02],\n",
      "        [-3.3942e-01,  3.1991e+00,  6.1086e-03,  9.8932e-02],\n",
      "        [-2.4040e-01,  1.6408e+00, -2.3552e+00, -1.3539e+00],\n",
      "        [-4.9156e-02, -1.6327e-01, -3.5093e-01, -5.6167e-02],\n",
      "        [ 3.8718e-03,  6.0843e-03, -2.4479e+00, -1.4536e+00],\n",
      "        [-1.7819e-01,  1.7356e+00, -4.2791e-01, -2.0504e-01],\n",
      "        [-7.9648e-03, -2.3893e+00, -1.0693e-01, -4.8984e-04],\n",
      "        [-1.4835e-01, -1.2001e+00, -2.6463e-01, -1.2745e-01],\n",
      "        [-2.8041e-01, -1.0640e+00, -2.1312e-01, -9.1760e-02],\n",
      "        [-4.5039e-01,  1.4323e+00, -1.9446e+00, -1.0876e+00],\n",
      "        [ 3.0733e-03,  2.3455e-01, -2.4203e-01, -1.4276e-01],\n",
      "        [-4.4812e-01, -3.1176e+00,  5.6198e-02,  9.7712e-02],\n",
      "        [-2.5388e-01, -9.5503e-03, -6.3161e-01, -3.3318e-01],\n",
      "        [ 1.7017e-04,  1.4242e+00,  4.0779e-02,  8.5262e-02],\n",
      "        [ 3.9037e-03,  1.3521e+00, -1.2181e-01, -1.0019e-02],\n",
      "        [ 3.6311e-03, -2.4486e+00, -1.4213e-01, -2.1674e-02],\n",
      "        [-4.6449e-02, -2.9563e-01, -9.9275e-01, -3.4726e-01],\n",
      "        [ 3.6082e-03, -7.7180e-01, -6.7068e-01, -1.6855e-01],\n",
      "        [-2.1127e-01, -2.3343e+00,  2.8785e-02,  8.1209e-02],\n",
      "        [-4.3901e-01,  1.0570e+00, -1.8170e-03,  9.1429e-02],\n",
      "        [-4.3730e-02, -1.3730e+00, -3.5293e-01,  3.6564e-02],\n",
      "        [-4.4293e-01, -2.8197e+00,  9.3509e-02,  1.1325e-01],\n",
      "        [-7.8538e-02,  1.5139e-01,  7.1170e-02,  1.2912e-01],\n",
      "        [ 1.0048e-03,  5.1875e+00,  3.1541e-02,  1.1023e-01],\n",
      "        [-4.4984e-01, -7.3527e-01, -4.5497e-01, -2.0044e-01],\n",
      "        [-1.6811e-03, -6.0854e-01, -7.2374e-01, -3.7179e-01],\n",
      "        [-3.4245e-01, -1.3881e-01,  6.0726e-02,  1.3122e-01],\n",
      "        [-4.1571e-01,  8.1002e-01,  6.0020e-02,  1.3006e-01],\n",
      "        [-3.4035e-01,  3.1641e+00, -3.4058e-02,  3.6451e-02],\n",
      "        [-4.4681e-01,  3.2144e+00, -3.6015e-01, -2.2354e-01],\n",
      "        [-1.1625e-02, -7.3352e-01,  1.5934e-02,  7.3438e-02],\n",
      "        [-1.0627e-02, -1.6340e-01, -6.9399e-01, -3.6281e-01],\n",
      "        [-1.3169e-02, -3.4286e-02, -2.1784e-02,  8.2863e-02],\n",
      "        [-1.9120e-01, -2.8283e+00,  3.6436e-02,  1.0913e-01],\n",
      "        [-1.8831e-02, -1.1176e+00,  1.1022e-01,  1.1255e-01],\n",
      "        [-3.9035e-01, -2.5695e-01, -8.8689e-02, -7.5494e-03],\n",
      "        [-4.4725e-01, -4.7546e-01,  1.2353e-02,  7.5259e-02],\n",
      "        [-4.4932e-01,  7.5011e-01, -2.2216e-01, -4.4713e-02],\n",
      "        [ 3.4991e-03, -1.3635e+00, -6.0086e-02,  4.8128e-02]],\n",
      "       grad_fn=<CatBackward0>), 'NB': tensor([[-4.3836e-01,  3.9781e+00,  1.6721e-01,  1.6454e-01],\n",
      "        [-4.4347e-01, -9.3286e-01,  1.6721e-01,  1.6454e-01],\n",
      "        [ 3.6871e-03, -4.6346e+00,  1.6721e-01,  1.6454e-01],\n",
      "        [-4.4876e-01, -2.0222e+00,  1.6721e-01,  1.6454e-01],\n",
      "        [-2.7870e-01,  1.5185e+00,  1.6721e-01,  1.6454e-01]],\n",
      "       grad_fn=<CatBackward0>)})\n"
     ]
    }
   ],
   "source": [
    "# Create Minimizer, Enforcer and Embedder Models\n",
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "index_mappers,net,data = generate_unsupervised_input(grid_name)\n",
    "#minimizer_model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=1)\n",
    "#embedder_model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=1)\n",
    "\n",
    "#minimizer_model = create_ACOPFGNN_model(data,\n",
    "# net, index_mappers, hidden_channels=4, num_layers=4)\n",
    "\n",
    "#embedder_model = create_ACOPFEmbedder_model(data, net, index_mappers, hidden_channels=4, num_layers=1)\n",
    "\n",
    "embedder_model = create_ACOPFEmbedder_Bus_Constrained(data, net, index_mappers, hidden_channels=16, num_layers=1)#create_ACOPFGeneral_model(data, net, index_mappers, hidden_channels=16, num_layers=1)\n",
    "\n",
    "\n",
    "#enforcer_model = create_ACOPFEnforcer_model(data, net, index_mappers, hidden_channels=4, num_layers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Initialize the Optimizer\n",
    "from itertools import chain\n",
    "#optimizer = torch.optim.Adam(chain(minimizer_model.parameters(), enforcer_model.parameters(), embedder_model.parameters()))\n",
    "ACOPF_optimizer = torch.optim.Adam(embedder_model.parameters(), lr=1e-4)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "ACOPF_optimizer.param_groups[0]['lr']*=0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mcanbay96\u001B[0m (\u001B[33mcbml\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\wandb\\run-20231108_172328-ezekgbcl</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/ezekgbcl' target=\"_blank\">woven-spaceship-333</a></strong> to <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/ezekgbcl' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/ezekgbcl</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_NOTEBOOK_NAME = \"msi\"\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"ACOPF-GNN-SelfSupervised-Hetero\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            #\"learning_rate\": optimizer.,\n",
    "            \"architecture\": \"Hetero-SelfSupervised-GNN\",\n",
    "            \"grid_name\": grid_name,\n",
    "            \"Training Type\": \"Supervised\",\n",
    "            \"model\": \"hetero-bus-constrained\"\n",
    "\n",
    "        }\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load Inputs\n",
    "inputs = load_unsupervised_inputs(grid_name)\n",
    "#inputs = load_multiple_unsupervised_inputs()\n",
    "n = len(inputs)\n",
    "train_inputs = inputs[:int(n*0.8)]\n",
    "val_inputs = inputs[int(n*0.8): int(n*0.95)]\n",
    "test_inputs = inputs[int(n*0.95):]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 1.611656904220581 \n",
      "     Training Step: 1 Training Loss: 1.4702457189559937 \n",
      "     Training Step: 2 Training Loss: 1.35362708568573 \n",
      "     Training Step: 3 Training Loss: 1.2682572603225708 \n",
      "     Training Step: 4 Training Loss: 1.20698881149292 \n",
      "     Training Step: 5 Training Loss: 1.1645708084106445 \n",
      "     Training Step: 6 Training Loss: 1.1335264444351196 \n",
      "     Training Step: 7 Training Loss: 1.1162726879119873 \n",
      "     Training Step: 8 Training Loss: 1.0983985662460327 \n",
      "     Training Step: 9 Training Loss: 1.080711007118225 \n",
      "     Training Step: 10 Training Loss: 1.0645232200622559 \n",
      "     Training Step: 11 Training Loss: 1.0498509407043457 \n",
      "     Training Step: 12 Training Loss: 1.0334110260009766 \n",
      "     Training Step: 13 Training Loss: 1.01816987991333 \n",
      "     Training Step: 14 Training Loss: 1.005792260169983 \n",
      "     Training Step: 15 Training Loss: 0.9963366389274597 \n",
      "     Training Step: 16 Training Loss: 0.9799153208732605 \n",
      "     Training Step: 17 Training Loss: 0.9686982035636902 \n",
      "     Training Step: 18 Training Loss: 0.9580257534980774 \n",
      "     Training Step: 19 Training Loss: 0.9446306824684143 \n",
      "     Training Step: 20 Training Loss: 0.9352976083755493 \n",
      "     Training Step: 21 Training Loss: 0.9200664758682251 \n",
      "     Training Step: 22 Training Loss: 0.9130507111549377 \n",
      "     Training Step: 23 Training Loss: 0.9017789363861084 \n",
      "     Training Step: 24 Training Loss: 0.8903729319572449 \n",
      "     Training Step: 25 Training Loss: 0.8811655044555664 \n",
      "     Training Step: 26 Training Loss: 0.8687247037887573 \n",
      "     Training Step: 27 Training Loss: 0.8628472089767456 \n",
      "     Training Step: 28 Training Loss: 0.849265992641449 \n",
      "     Training Step: 29 Training Loss: 0.841225266456604 \n",
      "     Training Step: 30 Training Loss: 0.8309155106544495 \n",
      "     Training Step: 31 Training Loss: 0.8203558325767517 \n",
      "     Training Step: 32 Training Loss: 0.8106936812400818 \n",
      "     Training Step: 33 Training Loss: 0.8004312515258789 \n",
      "     Training Step: 34 Training Loss: 0.7926368117332458 \n",
      "     Training Step: 35 Training Loss: 0.7845208644866943 \n",
      "     Training Step: 36 Training Loss: 0.7732929587364197 \n",
      "     Training Step: 37 Training Loss: 0.7655777335166931 \n",
      "     Training Step: 38 Training Loss: 0.7610421776771545 \n",
      "     Training Step: 39 Training Loss: 0.751828134059906 \n",
      "     Training Step: 40 Training Loss: 0.7446352243423462 \n",
      "     Training Step: 41 Training Loss: 0.7372419238090515 \n",
      "     Training Step: 42 Training Loss: 0.7281908988952637 \n",
      "     Training Step: 43 Training Loss: 0.7225762605667114 \n",
      "     Training Step: 44 Training Loss: 0.7146902084350586 \n",
      "     Training Step: 45 Training Loss: 0.7089138627052307 \n",
      "     Training Step: 46 Training Loss: 0.7028703093528748 \n",
      "     Training Step: 47 Training Loss: 0.6967443823814392 \n",
      "     Training Step: 48 Training Loss: 0.6920406818389893 \n",
      "     Training Step: 49 Training Loss: 0.6865434646606445 \n",
      "     Training Step: 50 Training Loss: 0.6815356016159058 \n",
      "     Training Step: 51 Training Loss: 0.676947832107544 \n",
      "     Training Step: 52 Training Loss: 0.6733415722846985 \n",
      "     Training Step: 53 Training Loss: 0.6692673563957214 \n",
      "     Training Step: 54 Training Loss: 0.6646102070808411 \n",
      "     Training Step: 55 Training Loss: 0.6599507331848145 \n",
      "     Training Step: 56 Training Loss: 0.660019040107727 \n",
      "     Training Step: 57 Training Loss: 0.654209315776825 \n",
      "     Training Step: 58 Training Loss: 0.6530209183692932 \n",
      "     Training Step: 59 Training Loss: 0.6495062112808228 \n",
      "     Training Step: 60 Training Loss: 0.6473332643508911 \n",
      "     Training Step: 61 Training Loss: 0.6438395380973816 \n",
      "     Training Step: 62 Training Loss: 0.643486499786377 \n",
      "     Training Step: 63 Training Loss: 0.6393981575965881 \n",
      "     Training Step: 64 Training Loss: 0.6376273036003113 \n",
      "     Training Step: 65 Training Loss: 0.6392138600349426 \n",
      "     Training Step: 66 Training Loss: 0.6351470947265625 \n",
      "     Training Step: 67 Training Loss: 0.6329981684684753 \n",
      "     Training Step: 68 Training Loss: 0.632872998714447 \n",
      "     Training Step: 69 Training Loss: 0.6311906576156616 \n",
      "     Training Step: 70 Training Loss: 0.6294833421707153 \n",
      "     Training Step: 71 Training Loss: 0.6321662664413452 \n",
      "     Training Step: 72 Training Loss: 0.6300139427185059 \n",
      "     Training Step: 73 Training Loss: 0.6270940899848938 \n",
      "     Training Step: 74 Training Loss: 0.6256427764892578 \n",
      "     Training Step: 75 Training Loss: 0.6254010796546936 \n",
      "     Training Step: 76 Training Loss: 0.6253765821456909 \n",
      "     Training Step: 77 Training Loss: 0.625808835029602 \n",
      "     Training Step: 78 Training Loss: 0.6241343021392822 \n",
      "     Training Step: 79 Training Loss: 0.6201330423355103 \n",
      "     Training Step: 80 Training Loss: 0.6240637898445129 \n",
      "     Training Step: 81 Training Loss: 0.6216756105422974 \n",
      "     Training Step: 82 Training Loss: 0.6221097111701965 \n",
      "     Training Step: 83 Training Loss: 0.6194124221801758 \n",
      "     Training Step: 84 Training Loss: 0.6216392517089844 \n",
      "     Training Step: 85 Training Loss: 0.6175054907798767 \n",
      "     Training Step: 86 Training Loss: 0.6168799996376038 \n",
      "     Training Step: 87 Training Loss: 0.6200383901596069 \n",
      "     Training Step: 88 Training Loss: 0.6189149022102356 \n",
      "     Training Step: 89 Training Loss: 0.619132399559021 \n",
      "     Training Step: 90 Training Loss: 0.6179044842720032 \n",
      "     Training Step: 91 Training Loss: 0.616863489151001 \n",
      "     Training Step: 92 Training Loss: 0.6190685629844666 \n",
      "     Training Step: 93 Training Loss: 0.6201285123825073 \n",
      "     Training Step: 94 Training Loss: 0.6203335523605347 \n",
      "     Training Step: 95 Training Loss: 0.6174185872077942 \n",
      "     Training Step: 96 Training Loss: 0.6153805255889893 \n",
      "     Training Step: 97 Training Loss: 0.6176466941833496 \n",
      "     Training Step: 98 Training Loss: 0.6163100600242615 \n",
      "     Training Step: 99 Training Loss: 0.6165114641189575 \n",
      "     Training Step: 100 Training Loss: 0.6158658862113953 \n",
      "     Training Step: 101 Training Loss: 0.6174622178077698 \n",
      "     Training Step: 102 Training Loss: 0.6195258498191833 \n",
      "     Training Step: 103 Training Loss: 0.6153245568275452 \n",
      "     Training Step: 104 Training Loss: 0.6161590218544006 \n",
      "     Training Step: 105 Training Loss: 0.6151095032691956 \n",
      "     Training Step: 106 Training Loss: 0.617272675037384 \n",
      "     Training Step: 107 Training Loss: 0.6181692481040955 \n",
      "     Training Step: 108 Training Loss: 0.6149172186851501 \n",
      "     Training Step: 109 Training Loss: 0.6151677370071411 \n",
      "     Training Step: 110 Training Loss: 0.6134431958198547 \n",
      "     Training Step: 111 Training Loss: 0.6161381602287292 \n",
      "     Training Step: 112 Training Loss: 0.6174550652503967 \n",
      "     Training Step: 113 Training Loss: 0.6161532402038574 \n",
      "     Training Step: 114 Training Loss: 0.6178344488143921 \n",
      "     Training Step: 115 Training Loss: 0.6130477786064148 \n",
      "     Training Step: 116 Training Loss: 0.61654132604599 \n",
      "     Training Step: 117 Training Loss: 0.6165838837623596 \n",
      "     Training Step: 118 Training Loss: 0.6153092980384827 \n",
      "     Training Step: 119 Training Loss: 0.6131289601325989 \n",
      "     Training Step: 120 Training Loss: 0.6153981685638428 \n",
      "     Training Step: 121 Training Loss: 0.6161485314369202 \n",
      "     Training Step: 122 Training Loss: 0.6135927438735962 \n",
      "     Training Step: 123 Training Loss: 0.62044757604599 \n",
      "     Training Step: 124 Training Loss: 0.6129864454269409 \n",
      "     Training Step: 125 Training Loss: 0.6135400533676147 \n",
      "     Training Step: 126 Training Loss: 0.6181378364562988 \n",
      "     Training Step: 127 Training Loss: 0.6148095726966858 \n",
      "     Training Step: 128 Training Loss: 0.614920437335968 \n",
      "     Training Step: 129 Training Loss: 0.6141825318336487 \n",
      "     Training Step: 130 Training Loss: 0.6151581406593323 \n",
      "     Training Step: 131 Training Loss: 0.6149605512619019 \n",
      "     Training Step: 132 Training Loss: 0.6162997484207153 \n",
      "     Training Step: 133 Training Loss: 0.6196823120117188 \n",
      "     Training Step: 134 Training Loss: 0.6134527921676636 \n",
      "     Training Step: 135 Training Loss: 0.6181393265724182 \n",
      "     Training Step: 136 Training Loss: 0.6127409338951111 \n",
      "     Training Step: 137 Training Loss: 0.6120967268943787 \n",
      "     Training Step: 138 Training Loss: 0.6188110113143921 \n",
      "     Training Step: 139 Training Loss: 0.6203868389129639 \n",
      "     Training Step: 140 Training Loss: 0.612093985080719 \n",
      "     Training Step: 141 Training Loss: 0.6155186295509338 \n",
      "     Training Step: 142 Training Loss: 0.6161777377128601 \n",
      "     Training Step: 143 Training Loss: 0.6165136098861694 \n",
      "     Training Step: 144 Training Loss: 0.6131888628005981 \n",
      "     Training Step: 145 Training Loss: 0.6165454983711243 \n",
      "     Training Step: 146 Training Loss: 0.6179885864257812 \n",
      "     Training Step: 147 Training Loss: 0.617685079574585 \n",
      "     Training Step: 148 Training Loss: 0.6165266036987305 \n",
      "     Training Step: 149 Training Loss: 0.617605984210968 \n",
      "     Training Step: 150 Training Loss: 0.6163647174835205 \n",
      "     Training Step: 151 Training Loss: 0.6163071990013123 \n",
      "     Training Step: 152 Training Loss: 0.6149048209190369 \n",
      "     Training Step: 153 Training Loss: 0.6134997010231018 \n",
      "     Training Step: 154 Training Loss: 0.6137733459472656 \n",
      "     Training Step: 155 Training Loss: 0.614712655544281 \n",
      "     Training Step: 156 Training Loss: 0.6177836060523987 \n",
      "     Training Step: 157 Training Loss: 0.6157051920890808 \n",
      "     Training Step: 158 Training Loss: 0.6181477904319763 \n",
      "     Training Step: 159 Training Loss: 0.6156472563743591 \n",
      "     Training Step: 160 Training Loss: 0.6130693554878235 \n",
      "     Training Step: 161 Training Loss: 0.6121480464935303 \n",
      "     Training Step: 162 Training Loss: 0.6110448837280273 \n",
      "     Training Step: 163 Training Loss: 0.6139196753501892 \n",
      "     Training Step: 164 Training Loss: 0.6159443259239197 \n",
      "     Training Step: 165 Training Loss: 0.6151863932609558 \n",
      "     Training Step: 166 Training Loss: 0.6172217726707458 \n",
      "     Training Step: 167 Training Loss: 0.6158329248428345 \n",
      "     Training Step: 168 Training Loss: 0.6112317442893982 \n",
      "     Training Step: 169 Training Loss: 0.6176614761352539 \n",
      "     Training Step: 170 Training Loss: 0.6120601296424866 \n",
      "     Training Step: 171 Training Loss: 0.6131868362426758 \n",
      "     Training Step: 172 Training Loss: 0.6160969734191895 \n",
      "     Training Step: 173 Training Loss: 0.612062931060791 \n",
      "     Training Step: 174 Training Loss: 0.6170743107795715 \n",
      "     Training Step: 175 Training Loss: 0.6143119931221008 \n",
      "     Training Step: 176 Training Loss: 0.6156773567199707 \n",
      "     Training Step: 177 Training Loss: 0.6164067387580872 \n",
      "     Training Step: 178 Training Loss: 0.6153784990310669 \n",
      "     Training Step: 179 Training Loss: 0.6136783361434937 \n",
      "     Training Step: 180 Training Loss: 0.6164150834083557 \n",
      "     Training Step: 181 Training Loss: 0.6162028312683105 \n",
      "     Training Step: 182 Training Loss: 0.618499219417572 \n",
      "     Training Step: 183 Training Loss: 0.6153656840324402 \n",
      "     Training Step: 184 Training Loss: 0.615096390247345 \n",
      "     Training Step: 185 Training Loss: 0.6135545372962952 \n",
      "     Training Step: 186 Training Loss: 0.6154240965843201 \n",
      "     Training Step: 187 Training Loss: 0.6139299869537354 \n",
      "     Training Step: 188 Training Loss: 0.6142688393592834 \n",
      "     Training Step: 189 Training Loss: 0.6148773431777954 \n",
      "     Training Step: 190 Training Loss: 0.6159967184066772 \n",
      "     Training Step: 191 Training Loss: 0.6155393719673157 \n",
      "     Training Step: 192 Training Loss: 0.6171298623085022 \n",
      "     Training Step: 193 Training Loss: 0.6127305030822754 \n",
      "     Training Step: 194 Training Loss: 0.615197479724884 \n",
      "     Training Step: 195 Training Loss: 0.6162721514701843 \n",
      "     Training Step: 196 Training Loss: 0.6167744398117065 \n",
      "     Training Step: 197 Training Loss: 0.6184890866279602 \n",
      "     Training Step: 198 Training Loss: 0.6140933632850647 \n",
      "     Training Step: 199 Training Loss: 0.6187535524368286 \n",
      "     Training Step: 200 Training Loss: 0.6171370148658752 \n",
      "     Training Step: 201 Training Loss: 0.6153207421302795 \n",
      "     Training Step: 202 Training Loss: 0.6145296692848206 \n",
      "     Training Step: 203 Training Loss: 0.6133789420127869 \n",
      "     Training Step: 204 Training Loss: 0.6189742684364319 \n",
      "     Training Step: 205 Training Loss: 0.6132137179374695 \n",
      "     Training Step: 206 Training Loss: 0.6110855937004089 \n",
      "     Training Step: 207 Training Loss: 0.6189961433410645 \n",
      "     Training Step: 208 Training Loss: 0.6172335147857666 \n",
      "     Training Step: 209 Training Loss: 0.6168371438980103 \n",
      "     Training Step: 210 Training Loss: 0.6135831475257874 \n",
      "     Training Step: 211 Training Loss: 0.6139494776725769 \n",
      "     Training Step: 212 Training Loss: 0.6169114708900452 \n",
      "     Training Step: 213 Training Loss: 0.6121476888656616 \n",
      "     Training Step: 214 Training Loss: 0.6209130883216858 \n",
      "     Training Step: 215 Training Loss: 0.6125772595405579 \n",
      "     Training Step: 216 Training Loss: 0.6186627745628357 \n",
      "     Training Step: 217 Training Loss: 0.6162323951721191 \n",
      "     Training Step: 218 Training Loss: 0.6154908537864685 \n",
      "     Training Step: 219 Training Loss: 0.6137957572937012 \n",
      "     Training Step: 220 Training Loss: 0.6155950427055359 \n",
      "     Training Step: 221 Training Loss: 0.6175543665885925 \n",
      "     Training Step: 222 Training Loss: 0.6147184371948242 \n",
      "     Training Step: 223 Training Loss: 0.6181618571281433 \n",
      "     Training Step: 224 Training Loss: 0.6116499900817871 \n",
      "     Training Step: 225 Training Loss: 0.6131607294082642 \n",
      "     Training Step: 226 Training Loss: 0.6180285215377808 \n",
      "     Training Step: 227 Training Loss: 0.6139759421348572 \n",
      "     Training Step: 228 Training Loss: 0.6198858618736267 \n",
      "     Training Step: 229 Training Loss: 0.6138676404953003 \n",
      "     Training Step: 230 Training Loss: 0.6144599318504333 \n",
      "     Training Step: 231 Training Loss: 0.6140100359916687 \n",
      "     Training Step: 232 Training Loss: 0.6166239976882935 \n",
      "     Training Step: 233 Training Loss: 0.6190245151519775 \n",
      "     Training Step: 234 Training Loss: 0.6143900156021118 \n",
      "     Training Step: 235 Training Loss: 0.61629319190979 \n",
      "     Training Step: 236 Training Loss: 0.6130181550979614 \n",
      "     Training Step: 237 Training Loss: 0.6134918928146362 \n",
      "     Training Step: 238 Training Loss: 0.6130532622337341 \n",
      "     Training Step: 239 Training Loss: 0.6115409731864929 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6085970997810364 \n",
      "     Validation Step: 1 Validation Loss: 0.6136864423751831 \n",
      "     Validation Step: 2 Validation Loss: 0.6159427165985107 \n",
      "     Validation Step: 3 Validation Loss: 0.613507091999054 \n",
      "     Validation Step: 4 Validation Loss: 0.6146434545516968 \n",
      "     Validation Step: 5 Validation Loss: 0.6128999590873718 \n",
      "     Validation Step: 6 Validation Loss: 0.6109058260917664 \n",
      "     Validation Step: 7 Validation Loss: 0.6156668663024902 \n",
      "     Validation Step: 8 Validation Loss: 0.614818274974823 \n",
      "     Validation Step: 9 Validation Loss: 0.6113434433937073 \n",
      "     Validation Step: 10 Validation Loss: 0.615487813949585 \n",
      "     Validation Step: 11 Validation Loss: 0.6148416996002197 \n",
      "     Validation Step: 12 Validation Loss: 0.6166238784790039 \n",
      "     Validation Step: 13 Validation Loss: 0.6150442957878113 \n",
      "     Validation Step: 14 Validation Loss: 0.6187408566474915 \n",
      "     Validation Step: 15 Validation Loss: 0.6126843094825745 \n",
      "     Validation Step: 16 Validation Loss: 0.6118147373199463 \n",
      "     Validation Step: 17 Validation Loss: 0.6154727339744568 \n",
      "     Validation Step: 18 Validation Loss: 0.6124082803726196 \n",
      "     Validation Step: 19 Validation Loss: 0.6114314794540405 \n",
      "     Validation Step: 20 Validation Loss: 0.6110894083976746 \n",
      "     Validation Step: 21 Validation Loss: 0.6143664121627808 \n",
      "     Validation Step: 22 Validation Loss: 0.6174312233924866 \n",
      "     Validation Step: 23 Validation Loss: 0.6159705519676208 \n",
      "     Validation Step: 24 Validation Loss: 0.6123437881469727 \n",
      "     Validation Step: 25 Validation Loss: 0.6111292839050293 \n",
      "     Validation Step: 26 Validation Loss: 0.6144387125968933 \n",
      "     Validation Step: 27 Validation Loss: 0.6186333894729614 \n",
      "     Validation Step: 28 Validation Loss: 0.6178054809570312 \n",
      "     Validation Step: 29 Validation Loss: 0.6151720285415649 \n",
      "     Validation Step: 30 Validation Loss: 0.6168519854545593 \n",
      "     Validation Step: 31 Validation Loss: 0.616152822971344 \n",
      "     Validation Step: 32 Validation Loss: 0.6150879859924316 \n",
      "     Validation Step: 33 Validation Loss: 0.6120215654373169 \n",
      "     Validation Step: 34 Validation Loss: 0.6176567077636719 \n",
      "     Validation Step: 35 Validation Loss: 0.6147565841674805 \n",
      "     Validation Step: 36 Validation Loss: 0.6180649995803833 \n",
      "     Validation Step: 37 Validation Loss: 0.6162527799606323 \n",
      "     Validation Step: 38 Validation Loss: 0.615770161151886 \n",
      "     Validation Step: 39 Validation Loss: 0.618689239025116 \n",
      "     Validation Step: 40 Validation Loss: 0.6141507029533386 \n",
      "     Validation Step: 41 Validation Loss: 0.6187148094177246 \n",
      "     Validation Step: 42 Validation Loss: 0.6138063669204712 \n",
      "     Validation Step: 43 Validation Loss: 0.6184107661247253 \n",
      "     Validation Step: 44 Validation Loss: 0.6114606261253357 \n",
      "Epoch: 1\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6143108606338501 \n",
      "     Training Step: 1 Training Loss: 0.6115078926086426 \n",
      "     Training Step: 2 Training Loss: 0.613220751285553 \n",
      "     Training Step: 3 Training Loss: 0.6158987283706665 \n",
      "     Training Step: 4 Training Loss: 0.6190387010574341 \n",
      "     Training Step: 5 Training Loss: 0.6138492226600647 \n",
      "     Training Step: 6 Training Loss: 0.6174900531768799 \n",
      "     Training Step: 7 Training Loss: 0.6133821606636047 \n",
      "     Training Step: 8 Training Loss: 0.618056058883667 \n",
      "     Training Step: 9 Training Loss: 0.614522397518158 \n",
      "     Training Step: 10 Training Loss: 0.614720344543457 \n",
      "     Training Step: 11 Training Loss: 0.61614990234375 \n",
      "     Training Step: 12 Training Loss: 0.6155222058296204 \n",
      "     Training Step: 13 Training Loss: 0.6134857535362244 \n",
      "     Training Step: 14 Training Loss: 0.6198287010192871 \n",
      "     Training Step: 15 Training Loss: 0.6124539375305176 \n",
      "     Training Step: 16 Training Loss: 0.6135846972465515 \n",
      "     Training Step: 17 Training Loss: 0.6114837527275085 \n",
      "     Training Step: 18 Training Loss: 0.6130922436714172 \n",
      "     Training Step: 19 Training Loss: 0.6108750700950623 \n",
      "     Training Step: 20 Training Loss: 0.6124790906906128 \n",
      "     Training Step: 21 Training Loss: 0.6113826632499695 \n",
      "     Training Step: 22 Training Loss: 0.6151921153068542 \n",
      "     Training Step: 23 Training Loss: 0.6135611534118652 \n",
      "     Training Step: 24 Training Loss: 0.616376519203186 \n",
      "     Training Step: 25 Training Loss: 0.6091946363449097 \n",
      "     Training Step: 26 Training Loss: 0.616940975189209 \n",
      "     Training Step: 27 Training Loss: 0.6141963601112366 \n",
      "     Training Step: 28 Training Loss: 0.6167454123497009 \n",
      "     Training Step: 29 Training Loss: 0.6114599108695984 \n",
      "     Training Step: 30 Training Loss: 0.6208809018135071 \n",
      "     Training Step: 31 Training Loss: 0.6159419417381287 \n",
      "     Training Step: 32 Training Loss: 0.6195411086082458 \n",
      "     Training Step: 33 Training Loss: 0.6170605421066284 \n",
      "     Training Step: 34 Training Loss: 0.6150014996528625 \n",
      "     Training Step: 35 Training Loss: 0.6123711466789246 \n",
      "     Training Step: 36 Training Loss: 0.6137927770614624 \n",
      "     Training Step: 37 Training Loss: 0.6123268604278564 \n",
      "     Training Step: 38 Training Loss: 0.612065851688385 \n",
      "     Training Step: 39 Training Loss: 0.6173558235168457 \n",
      "     Training Step: 40 Training Loss: 0.6129067540168762 \n",
      "     Training Step: 41 Training Loss: 0.6171348690986633 \n",
      "     Training Step: 42 Training Loss: 0.6186501383781433 \n",
      "     Training Step: 43 Training Loss: 0.6121305227279663 \n",
      "     Training Step: 44 Training Loss: 0.6139768958091736 \n",
      "     Training Step: 45 Training Loss: 0.6137150526046753 \n",
      "     Training Step: 46 Training Loss: 0.612254798412323 \n",
      "     Training Step: 47 Training Loss: 0.6123601794242859 \n",
      "     Training Step: 48 Training Loss: 0.6128972768783569 \n",
      "     Training Step: 49 Training Loss: 0.6157309412956238 \n",
      "     Training Step: 50 Training Loss: 0.6163236498832703 \n",
      "     Training Step: 51 Training Loss: 0.6108273267745972 \n",
      "     Training Step: 52 Training Loss: 0.6158241629600525 \n",
      "     Training Step: 53 Training Loss: 0.6176267266273499 \n",
      "     Training Step: 54 Training Loss: 0.617689847946167 \n",
      "     Training Step: 55 Training Loss: 0.6156424283981323 \n",
      "     Training Step: 56 Training Loss: 0.6199502348899841 \n",
      "     Training Step: 57 Training Loss: 0.6164810657501221 \n",
      "     Training Step: 58 Training Loss: 0.6172683238983154 \n",
      "     Training Step: 59 Training Loss: 0.6144585609436035 \n",
      "     Training Step: 60 Training Loss: 0.618131160736084 \n",
      "     Training Step: 61 Training Loss: 0.6138855218887329 \n",
      "     Training Step: 62 Training Loss: 0.6182148456573486 \n",
      "     Training Step: 63 Training Loss: 0.6121135354042053 \n",
      "     Training Step: 64 Training Loss: 0.6114247441291809 \n",
      "     Training Step: 65 Training Loss: 0.6177056431770325 \n",
      "     Training Step: 66 Training Loss: 0.6150737404823303 \n",
      "     Training Step: 67 Training Loss: 0.6153706312179565 \n",
      "     Training Step: 68 Training Loss: 0.6151849627494812 \n",
      "     Training Step: 69 Training Loss: 0.6120703816413879 \n",
      "     Training Step: 70 Training Loss: 0.6131376624107361 \n",
      "     Training Step: 71 Training Loss: 0.6121439933776855 \n",
      "     Training Step: 72 Training Loss: 0.6147806644439697 \n",
      "     Training Step: 73 Training Loss: 0.6155632138252258 \n",
      "     Training Step: 74 Training Loss: 0.6112138032913208 \n",
      "     Training Step: 75 Training Loss: 0.6101984977722168 \n",
      "     Training Step: 76 Training Loss: 0.6153746247291565 \n",
      "     Training Step: 77 Training Loss: 0.6128112077713013 \n",
      "     Training Step: 78 Training Loss: 0.6145878434181213 \n",
      "     Training Step: 79 Training Loss: 0.6180175542831421 \n",
      "     Training Step: 80 Training Loss: 0.6136114001274109 \n",
      "     Training Step: 81 Training Loss: 0.6117213368415833 \n",
      "     Training Step: 82 Training Loss: 0.6135003566741943 \n",
      "     Training Step: 83 Training Loss: 0.615006685256958 \n",
      "     Training Step: 84 Training Loss: 0.6169943809509277 \n",
      "     Training Step: 85 Training Loss: 0.6151852011680603 \n",
      "     Training Step: 86 Training Loss: 0.6138513684272766 \n",
      "     Training Step: 87 Training Loss: 0.616897463798523 \n",
      "     Training Step: 88 Training Loss: 0.6151877641677856 \n",
      "     Training Step: 89 Training Loss: 0.6152918338775635 \n",
      "     Training Step: 90 Training Loss: 0.6121630072593689 \n",
      "     Training Step: 91 Training Loss: 0.610421895980835 \n",
      "     Training Step: 92 Training Loss: 0.6114956140518188 \n",
      "     Training Step: 93 Training Loss: 0.614933967590332 \n",
      "     Training Step: 94 Training Loss: 0.615120530128479 \n",
      "     Training Step: 95 Training Loss: 0.6150532960891724 \n",
      "     Training Step: 96 Training Loss: 0.6181927919387817 \n",
      "     Training Step: 97 Training Loss: 0.6170127391815186 \n",
      "     Training Step: 98 Training Loss: 0.6141448616981506 \n",
      "     Training Step: 99 Training Loss: 0.6141692996025085 \n",
      "     Training Step: 100 Training Loss: 0.6146276593208313 \n",
      "     Training Step: 101 Training Loss: 0.6149944067001343 \n",
      "     Training Step: 102 Training Loss: 0.6168861985206604 \n",
      "     Training Step: 103 Training Loss: 0.6135527491569519 \n",
      "     Training Step: 104 Training Loss: 0.6120548248291016 \n",
      "     Training Step: 105 Training Loss: 0.6137878894805908 \n",
      "     Training Step: 106 Training Loss: 0.6123046875 \n",
      "     Training Step: 107 Training Loss: 0.6124724745750427 \n",
      "     Training Step: 108 Training Loss: 0.6105052828788757 \n",
      "     Training Step: 109 Training Loss: 0.6156445741653442 \n",
      "     Training Step: 110 Training Loss: 0.6124107241630554 \n",
      "     Training Step: 111 Training Loss: 0.6157092452049255 \n",
      "     Training Step: 112 Training Loss: 0.6128358244895935 \n",
      "     Training Step: 113 Training Loss: 0.6144580245018005 \n",
      "     Training Step: 114 Training Loss: 0.6126905679702759 \n",
      "     Training Step: 115 Training Loss: 0.6158215403556824 \n",
      "     Training Step: 116 Training Loss: 0.6156243681907654 \n",
      "     Training Step: 117 Training Loss: 0.6101958751678467 \n",
      "     Training Step: 118 Training Loss: 0.613734245300293 \n",
      "     Training Step: 119 Training Loss: 0.6147710680961609 \n",
      "     Training Step: 120 Training Loss: 0.6159821152687073 \n",
      "     Training Step: 121 Training Loss: 0.6168804168701172 \n",
      "     Training Step: 122 Training Loss: 0.6160341501235962 \n",
      "     Training Step: 123 Training Loss: 0.6160190105438232 \n",
      "     Training Step: 124 Training Loss: 0.6183451414108276 \n",
      "     Training Step: 125 Training Loss: 0.6197103261947632 \n",
      "     Training Step: 126 Training Loss: 0.6157532930374146 \n",
      "     Training Step: 127 Training Loss: 0.6163907647132874 \n",
      "     Training Step: 128 Training Loss: 0.614475429058075 \n",
      "     Training Step: 129 Training Loss: 0.6148471832275391 \n",
      "     Training Step: 130 Training Loss: 0.613642156124115 \n",
      "     Training Step: 131 Training Loss: 0.6202043890953064 \n",
      "     Training Step: 132 Training Loss: 0.6187049746513367 \n",
      "     Training Step: 133 Training Loss: 0.6155504584312439 \n",
      "     Training Step: 134 Training Loss: 0.6159076690673828 \n",
      "     Training Step: 135 Training Loss: 0.6151943206787109 \n",
      "     Training Step: 136 Training Loss: 0.6169801950454712 \n",
      "     Training Step: 137 Training Loss: 0.6164200305938721 \n",
      "     Training Step: 138 Training Loss: 0.6153323650360107 \n",
      "     Training Step: 139 Training Loss: 0.6166647672653198 \n",
      "     Training Step: 140 Training Loss: 0.6142833828926086 \n",
      "     Training Step: 141 Training Loss: 0.6126957535743713 \n",
      "     Training Step: 142 Training Loss: 0.6185113191604614 \n",
      "     Training Step: 143 Training Loss: 0.6147106289863586 \n",
      "     Training Step: 144 Training Loss: 0.6146053075790405 \n",
      "     Training Step: 145 Training Loss: 0.6116708517074585 \n",
      "     Training Step: 146 Training Loss: 0.61788409948349 \n",
      "     Training Step: 147 Training Loss: 0.6143713593482971 \n",
      "     Training Step: 148 Training Loss: 0.614504873752594 \n",
      "     Training Step: 149 Training Loss: 0.6102930903434753 \n",
      "     Training Step: 150 Training Loss: 0.6157194375991821 \n",
      "     Training Step: 151 Training Loss: 0.6136457920074463 \n",
      "     Training Step: 152 Training Loss: 0.6131004095077515 \n",
      "     Training Step: 153 Training Loss: 0.6139665246009827 \n",
      "     Training Step: 154 Training Loss: 0.6161399483680725 \n",
      "     Training Step: 155 Training Loss: 0.6173609495162964 \n",
      "     Training Step: 156 Training Loss: 0.6130025386810303 \n",
      "     Training Step: 157 Training Loss: 0.6132157444953918 \n",
      "     Training Step: 158 Training Loss: 0.6107077598571777 \n",
      "     Training Step: 159 Training Loss: 0.6185457706451416 \n",
      "     Training Step: 160 Training Loss: 0.6116125583648682 \n",
      "     Training Step: 161 Training Loss: 0.6189965009689331 \n",
      "     Training Step: 162 Training Loss: 0.6154782176017761 \n",
      "     Training Step: 163 Training Loss: 0.6156750321388245 \n",
      "     Training Step: 164 Training Loss: 0.6141796112060547 \n",
      "     Training Step: 165 Training Loss: 0.6111887097358704 \n",
      "     Training Step: 166 Training Loss: 0.612666666507721 \n",
      "     Training Step: 167 Training Loss: 0.6141064763069153 \n",
      "     Training Step: 168 Training Loss: 0.6128714680671692 \n",
      "     Training Step: 169 Training Loss: 0.6137475967407227 \n",
      "     Training Step: 170 Training Loss: 0.6178306937217712 \n",
      "     Training Step: 171 Training Loss: 0.6144612431526184 \n",
      "     Training Step: 172 Training Loss: 0.6110806465148926 \n",
      "     Training Step: 173 Training Loss: 0.6149033904075623 \n",
      "     Training Step: 174 Training Loss: 0.6137982606887817 \n",
      "     Training Step: 175 Training Loss: 0.6132885813713074 \n",
      "     Training Step: 176 Training Loss: 0.6166282296180725 \n",
      "     Training Step: 177 Training Loss: 0.6181631684303284 \n",
      "     Training Step: 178 Training Loss: 0.6127614974975586 \n",
      "     Training Step: 179 Training Loss: 0.6122405529022217 \n",
      "     Training Step: 180 Training Loss: 0.6150306463241577 \n",
      "     Training Step: 181 Training Loss: 0.6112176775932312 \n",
      "     Training Step: 182 Training Loss: 0.6160402894020081 \n",
      "     Training Step: 183 Training Loss: 0.6169899106025696 \n",
      "     Training Step: 184 Training Loss: 0.6129628419876099 \n",
      "     Training Step: 185 Training Loss: 0.6126550436019897 \n",
      "     Training Step: 186 Training Loss: 0.6169638633728027 \n",
      "     Training Step: 187 Training Loss: 0.6179232597351074 \n",
      "     Training Step: 188 Training Loss: 0.6120023727416992 \n",
      "     Training Step: 189 Training Loss: 0.6109585165977478 \n",
      "     Training Step: 190 Training Loss: 0.6126426458358765 \n",
      "     Training Step: 191 Training Loss: 0.6120617389678955 \n",
      "     Training Step: 192 Training Loss: 0.6107821464538574 \n",
      "     Training Step: 193 Training Loss: 0.6125142574310303 \n",
      "     Training Step: 194 Training Loss: 0.6147449612617493 \n",
      "     Training Step: 195 Training Loss: 0.6186359524726868 \n",
      "     Training Step: 196 Training Loss: 0.6115307211875916 \n",
      "     Training Step: 197 Training Loss: 0.6136377453804016 \n",
      "     Training Step: 198 Training Loss: 0.6139101386070251 \n",
      "     Training Step: 199 Training Loss: 0.6166012287139893 \n",
      "     Training Step: 200 Training Loss: 0.6106388568878174 \n",
      "     Training Step: 201 Training Loss: 0.6112438440322876 \n",
      "     Training Step: 202 Training Loss: 0.6155568361282349 \n",
      "     Training Step: 203 Training Loss: 0.6161370277404785 \n",
      "     Training Step: 204 Training Loss: 0.6119017004966736 \n",
      "     Training Step: 205 Training Loss: 0.6102380156517029 \n",
      "     Training Step: 206 Training Loss: 0.6131522059440613 \n",
      "     Training Step: 207 Training Loss: 0.6097654104232788 \n",
      "     Training Step: 208 Training Loss: 0.6144230365753174 \n",
      "     Training Step: 209 Training Loss: 0.615686297416687 \n",
      "     Training Step: 210 Training Loss: 0.6165800094604492 \n",
      "     Training Step: 211 Training Loss: 0.6153019070625305 \n",
      "     Training Step: 212 Training Loss: 0.6172453165054321 \n",
      "     Training Step: 213 Training Loss: 0.6169722676277161 \n",
      "     Training Step: 214 Training Loss: 0.6178303360939026 \n",
      "     Training Step: 215 Training Loss: 0.615166962146759 \n",
      "     Training Step: 216 Training Loss: 0.6146901845932007 \n",
      "     Training Step: 217 Training Loss: 0.614302933216095 \n",
      "     Training Step: 218 Training Loss: 0.6121353507041931 \n",
      "     Training Step: 219 Training Loss: 0.6124419569969177 \n",
      "     Training Step: 220 Training Loss: 0.6131764054298401 \n",
      "     Training Step: 221 Training Loss: 0.6117862462997437 \n",
      "     Training Step: 222 Training Loss: 0.613477885723114 \n",
      "     Training Step: 223 Training Loss: 0.6158333420753479 \n",
      "     Training Step: 224 Training Loss: 0.6148422360420227 \n",
      "     Training Step: 225 Training Loss: 0.610514760017395 \n",
      "     Training Step: 226 Training Loss: 0.6144655346870422 \n",
      "     Training Step: 227 Training Loss: 0.6168309450149536 \n",
      "     Training Step: 228 Training Loss: 0.6146999001502991 \n",
      "     Training Step: 229 Training Loss: 0.6149276494979858 \n",
      "     Training Step: 230 Training Loss: 0.6181138753890991 \n",
      "     Training Step: 231 Training Loss: 0.6122574210166931 \n",
      "     Training Step: 232 Training Loss: 0.612095832824707 \n",
      "     Training Step: 233 Training Loss: 0.6189127564430237 \n",
      "     Training Step: 234 Training Loss: 0.6124197244644165 \n",
      "     Training Step: 235 Training Loss: 0.6173158288002014 \n",
      "     Training Step: 236 Training Loss: 0.6137681007385254 \n",
      "     Training Step: 237 Training Loss: 0.6132957339286804 \n",
      "     Training Step: 238 Training Loss: 0.614311158657074 \n",
      "     Training Step: 239 Training Loss: 0.6167870163917542 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6081034541130066 \n",
      "     Validation Step: 1 Validation Loss: 0.615314781665802 \n",
      "     Validation Step: 2 Validation Loss: 0.6139845252037048 \n",
      "     Validation Step: 3 Validation Loss: 0.6159990429878235 \n",
      "     Validation Step: 4 Validation Loss: 0.615580677986145 \n",
      "     Validation Step: 5 Validation Loss: 0.6109116673469543 \n",
      "     Validation Step: 6 Validation Loss: 0.6135114431381226 \n",
      "     Validation Step: 7 Validation Loss: 0.6147592067718506 \n",
      "     Validation Step: 8 Validation Loss: 0.6106502413749695 \n",
      "     Validation Step: 9 Validation Loss: 0.6144879460334778 \n",
      "     Validation Step: 10 Validation Loss: 0.6140459179878235 \n",
      "     Validation Step: 11 Validation Loss: 0.6125101447105408 \n",
      "     Validation Step: 12 Validation Loss: 0.6110175848007202 \n",
      "     Validation Step: 13 Validation Loss: 0.6122859716415405 \n",
      "     Validation Step: 14 Validation Loss: 0.6171454191207886 \n",
      "     Validation Step: 15 Validation Loss: 0.615853488445282 \n",
      "     Validation Step: 16 Validation Loss: 0.6114851236343384 \n",
      "     Validation Step: 17 Validation Loss: 0.6162876486778259 \n",
      "     Validation Step: 18 Validation Loss: 0.6185198426246643 \n",
      "     Validation Step: 19 Validation Loss: 0.6148611307144165 \n",
      "     Validation Step: 20 Validation Loss: 0.614334225654602 \n",
      "     Validation Step: 21 Validation Loss: 0.6157263517379761 \n",
      "     Validation Step: 22 Validation Loss: 0.611600935459137 \n",
      "     Validation Step: 23 Validation Loss: 0.6185112595558167 \n",
      "     Validation Step: 24 Validation Loss: 0.6119846105575562 \n",
      "     Validation Step: 25 Validation Loss: 0.6176072955131531 \n",
      "     Validation Step: 26 Validation Loss: 0.6131491661071777 \n",
      "     Validation Step: 27 Validation Loss: 0.6105397939682007 \n",
      "     Validation Step: 28 Validation Loss: 0.6133185625076294 \n",
      "     Validation Step: 29 Validation Loss: 0.6147665977478027 \n",
      "     Validation Step: 30 Validation Loss: 0.615143895149231 \n",
      "     Validation Step: 31 Validation Loss: 0.6138534545898438 \n",
      "     Validation Step: 32 Validation Loss: 0.6184709668159485 \n",
      "     Validation Step: 33 Validation Loss: 0.6174179315567017 \n",
      "     Validation Step: 34 Validation Loss: 0.6144518256187439 \n",
      "     Validation Step: 35 Validation Loss: 0.6110460162162781 \n",
      "     Validation Step: 36 Validation Loss: 0.6154704093933105 \n",
      "     Validation Step: 37 Validation Loss: 0.6165059804916382 \n",
      "     Validation Step: 38 Validation Loss: 0.61443692445755 \n",
      "     Validation Step: 39 Validation Loss: 0.6181622743606567 \n",
      "     Validation Step: 40 Validation Loss: 0.6151282787322998 \n",
      "     Validation Step: 41 Validation Loss: 0.6119765639305115 \n",
      "     Validation Step: 42 Validation Loss: 0.6178206205368042 \n",
      "     Validation Step: 43 Validation Loss: 0.6183711886405945 \n",
      "     Validation Step: 44 Validation Loss: 0.6106283068656921 \n",
      "Epoch: 2\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6144664883613586 \n",
      "     Training Step: 1 Training Loss: 0.6178487539291382 \n",
      "     Training Step: 2 Training Loss: 0.6167952418327332 \n",
      "     Training Step: 3 Training Loss: 0.6158243417739868 \n",
      "     Training Step: 4 Training Loss: 0.6089338660240173 \n",
      "     Training Step: 5 Training Loss: 0.611079752445221 \n",
      "     Training Step: 6 Training Loss: 0.6137679219245911 \n",
      "     Training Step: 7 Training Loss: 0.6126208901405334 \n",
      "     Training Step: 8 Training Loss: 0.6182666420936584 \n",
      "     Training Step: 9 Training Loss: 0.611995279788971 \n",
      "     Training Step: 10 Training Loss: 0.6145859956741333 \n",
      "     Training Step: 11 Training Loss: 0.6154400110244751 \n",
      "     Training Step: 12 Training Loss: 0.6130493879318237 \n",
      "     Training Step: 13 Training Loss: 0.6119499206542969 \n",
      "     Training Step: 14 Training Loss: 0.6157634854316711 \n",
      "     Training Step: 15 Training Loss: 0.6134082078933716 \n",
      "     Training Step: 16 Training Loss: 0.615271270275116 \n",
      "     Training Step: 17 Training Loss: 0.6104981899261475 \n",
      "     Training Step: 18 Training Loss: 0.6117754578590393 \n",
      "     Training Step: 19 Training Loss: 0.6110313534736633 \n",
      "     Training Step: 20 Training Loss: 0.6135380864143372 \n",
      "     Training Step: 21 Training Loss: 0.6141501665115356 \n",
      "     Training Step: 22 Training Loss: 0.614147961139679 \n",
      "     Training Step: 23 Training Loss: 0.6148768067359924 \n",
      "     Training Step: 24 Training Loss: 0.6183527112007141 \n",
      "     Training Step: 25 Training Loss: 0.6124275922775269 \n",
      "     Training Step: 26 Training Loss: 0.6127851605415344 \n",
      "     Training Step: 27 Training Loss: 0.6163142323493958 \n",
      "     Training Step: 28 Training Loss: 0.6121153235435486 \n",
      "     Training Step: 29 Training Loss: 0.6135060787200928 \n",
      "     Training Step: 30 Training Loss: 0.6111013889312744 \n",
      "     Training Step: 31 Training Loss: 0.6154263615608215 \n",
      "     Training Step: 32 Training Loss: 0.6185576915740967 \n",
      "     Training Step: 33 Training Loss: 0.6186895370483398 \n",
      "     Training Step: 34 Training Loss: 0.6168243885040283 \n",
      "     Training Step: 35 Training Loss: 0.6127833127975464 \n",
      "     Training Step: 36 Training Loss: 0.6175834536552429 \n",
      "     Training Step: 37 Training Loss: 0.611322283744812 \n",
      "     Training Step: 38 Training Loss: 0.6125829815864563 \n",
      "     Training Step: 39 Training Loss: 0.614328145980835 \n",
      "     Training Step: 40 Training Loss: 0.6184813976287842 \n",
      "     Training Step: 41 Training Loss: 0.6166208982467651 \n",
      "     Training Step: 42 Training Loss: 0.6111555695533752 \n",
      "     Training Step: 43 Training Loss: 0.6115715503692627 \n",
      "     Training Step: 44 Training Loss: 0.6118137240409851 \n",
      "     Training Step: 45 Training Loss: 0.6180770397186279 \n",
      "     Training Step: 46 Training Loss: 0.6150292754173279 \n",
      "     Training Step: 47 Training Loss: 0.6102170348167419 \n",
      "     Training Step: 48 Training Loss: 0.6143947839736938 \n",
      "     Training Step: 49 Training Loss: 0.6203200221061707 \n",
      "     Training Step: 50 Training Loss: 0.6147975325584412 \n",
      "     Training Step: 51 Training Loss: 0.6131800413131714 \n",
      "     Training Step: 52 Training Loss: 0.6116445660591125 \n",
      "     Training Step: 53 Training Loss: 0.6155524849891663 \n",
      "     Training Step: 54 Training Loss: 0.619516909122467 \n",
      "     Training Step: 55 Training Loss: 0.617872416973114 \n",
      "     Training Step: 56 Training Loss: 0.6136143207550049 \n",
      "     Training Step: 57 Training Loss: 0.6118099689483643 \n",
      "     Training Step: 58 Training Loss: 0.6155754327774048 \n",
      "     Training Step: 59 Training Loss: 0.6165584325790405 \n",
      "     Training Step: 60 Training Loss: 0.6121550798416138 \n",
      "     Training Step: 61 Training Loss: 0.6129170656204224 \n",
      "     Training Step: 62 Training Loss: 0.6157069206237793 \n",
      "     Training Step: 63 Training Loss: 0.6169573664665222 \n",
      "     Training Step: 64 Training Loss: 0.6146595478057861 \n",
      "     Training Step: 65 Training Loss: 0.61282879114151 \n",
      "     Training Step: 66 Training Loss: 0.6163501143455505 \n",
      "     Training Step: 67 Training Loss: 0.6117842793464661 \n",
      "     Training Step: 68 Training Loss: 0.6125834584236145 \n",
      "     Training Step: 69 Training Loss: 0.6134008765220642 \n",
      "     Training Step: 70 Training Loss: 0.6135557293891907 \n",
      "     Training Step: 71 Training Loss: 0.6121203899383545 \n",
      "     Training Step: 72 Training Loss: 0.614314615726471 \n",
      "     Training Step: 73 Training Loss: 0.6187968254089355 \n",
      "     Training Step: 74 Training Loss: 0.6142726540565491 \n",
      "     Training Step: 75 Training Loss: 0.6105188727378845 \n",
      "     Training Step: 76 Training Loss: 0.6130550503730774 \n",
      "     Training Step: 77 Training Loss: 0.6148802042007446 \n",
      "     Training Step: 78 Training Loss: 0.614586353302002 \n",
      "     Training Step: 79 Training Loss: 0.615464448928833 \n",
      "     Training Step: 80 Training Loss: 0.6134607791900635 \n",
      "     Training Step: 81 Training Loss: 0.617221474647522 \n",
      "     Training Step: 82 Training Loss: 0.619627058506012 \n",
      "     Training Step: 83 Training Loss: 0.6188806891441345 \n",
      "     Training Step: 84 Training Loss: 0.6180779337882996 \n",
      "     Training Step: 85 Training Loss: 0.6105746030807495 \n",
      "     Training Step: 86 Training Loss: 0.6108123660087585 \n",
      "     Training Step: 87 Training Loss: 0.6157988905906677 \n",
      "     Training Step: 88 Training Loss: 0.6108183264732361 \n",
      "     Training Step: 89 Training Loss: 0.6122241616249084 \n",
      "     Training Step: 90 Training Loss: 0.6159694790840149 \n",
      "     Training Step: 91 Training Loss: 0.6135684251785278 \n",
      "     Training Step: 92 Training Loss: 0.6125409007072449 \n",
      "     Training Step: 93 Training Loss: 0.6143438816070557 \n",
      "     Training Step: 94 Training Loss: 0.6104798913002014 \n",
      "     Training Step: 95 Training Loss: 0.6144298315048218 \n",
      "     Training Step: 96 Training Loss: 0.6119482517242432 \n",
      "     Training Step: 97 Training Loss: 0.6117960214614868 \n",
      "     Training Step: 98 Training Loss: 0.6138769388198853 \n",
      "     Training Step: 99 Training Loss: 0.6126060485839844 \n",
      "     Training Step: 100 Training Loss: 0.6139699220657349 \n",
      "     Training Step: 101 Training Loss: 0.6130558252334595 \n",
      "     Training Step: 102 Training Loss: 0.6111714839935303 \n",
      "     Training Step: 103 Training Loss: 0.6127354502677917 \n",
      "     Training Step: 104 Training Loss: 0.6110067367553711 \n",
      "     Training Step: 105 Training Loss: 0.6153990030288696 \n",
      "     Training Step: 106 Training Loss: 0.6126164793968201 \n",
      "     Training Step: 107 Training Loss: 0.616847574710846 \n",
      "     Training Step: 108 Training Loss: 0.6125817894935608 \n",
      "     Training Step: 109 Training Loss: 0.6177491545677185 \n",
      "     Training Step: 110 Training Loss: 0.6132083535194397 \n",
      "     Training Step: 111 Training Loss: 0.6119516491889954 \n",
      "     Training Step: 112 Training Loss: 0.6145293116569519 \n",
      "     Training Step: 113 Training Loss: 0.6109075546264648 \n",
      "     Training Step: 114 Training Loss: 0.6161353588104248 \n",
      "     Training Step: 115 Training Loss: 0.6172484755516052 \n",
      "     Training Step: 116 Training Loss: 0.6168168187141418 \n",
      "     Training Step: 117 Training Loss: 0.6146649718284607 \n",
      "     Training Step: 118 Training Loss: 0.6124043464660645 \n",
      "     Training Step: 119 Training Loss: 0.6121281981468201 \n",
      "     Training Step: 120 Training Loss: 0.6156114935874939 \n",
      "     Training Step: 121 Training Loss: 0.6169158220291138 \n",
      "     Training Step: 122 Training Loss: 0.6144862174987793 \n",
      "     Training Step: 123 Training Loss: 0.614662230014801 \n",
      "     Training Step: 124 Training Loss: 0.6164623498916626 \n",
      "     Training Step: 125 Training Loss: 0.6182199120521545 \n",
      "     Training Step: 126 Training Loss: 0.6110273599624634 \n",
      "     Training Step: 127 Training Loss: 0.6110941171646118 \n",
      "     Training Step: 128 Training Loss: 0.6105145812034607 \n",
      "     Training Step: 129 Training Loss: 0.6118043661117554 \n",
      "     Training Step: 130 Training Loss: 0.614974319934845 \n",
      "     Training Step: 131 Training Loss: 0.6146771311759949 \n",
      "     Training Step: 132 Training Loss: 0.6150290369987488 \n",
      "     Training Step: 133 Training Loss: 0.6139386296272278 \n",
      "     Training Step: 134 Training Loss: 0.6120722889900208 \n",
      "     Training Step: 135 Training Loss: 0.6146817803382874 \n",
      "     Training Step: 136 Training Loss: 0.6168580055236816 \n",
      "     Training Step: 137 Training Loss: 0.6148892641067505 \n",
      "     Training Step: 138 Training Loss: 0.6150127649307251 \n",
      "     Training Step: 139 Training Loss: 0.6168844699859619 \n",
      "     Training Step: 140 Training Loss: 0.6172100305557251 \n",
      "     Training Step: 141 Training Loss: 0.6169284582138062 \n",
      "     Training Step: 142 Training Loss: 0.615236222743988 \n",
      "     Training Step: 143 Training Loss: 0.6171417236328125 \n",
      "     Training Step: 144 Training Loss: 0.6165450215339661 \n",
      "     Training Step: 145 Training Loss: 0.6160858273506165 \n",
      "     Training Step: 146 Training Loss: 0.6137108206748962 \n",
      "     Training Step: 147 Training Loss: 0.613161563873291 \n",
      "     Training Step: 148 Training Loss: 0.6158166527748108 \n",
      "     Training Step: 149 Training Loss: 0.6165636777877808 \n",
      "     Training Step: 150 Training Loss: 0.6154848337173462 \n",
      "     Training Step: 151 Training Loss: 0.6153754591941833 \n",
      "     Training Step: 152 Training Loss: 0.6177528500556946 \n",
      "     Training Step: 153 Training Loss: 0.6139745712280273 \n",
      "     Training Step: 154 Training Loss: 0.6109344959259033 \n",
      "     Training Step: 155 Training Loss: 0.6137352585792542 \n",
      "     Training Step: 156 Training Loss: 0.6168957352638245 \n",
      "     Training Step: 157 Training Loss: 0.6136719584465027 \n",
      "     Training Step: 158 Training Loss: 0.6155273914337158 \n",
      "     Training Step: 159 Training Loss: 0.612363874912262 \n",
      "     Training Step: 160 Training Loss: 0.6169031858444214 \n",
      "     Training Step: 161 Training Loss: 0.6148020029067993 \n",
      "     Training Step: 162 Training Loss: 0.6188511848449707 \n",
      "     Training Step: 163 Training Loss: 0.612064003944397 \n",
      "     Training Step: 164 Training Loss: 0.6109549403190613 \n",
      "     Training Step: 165 Training Loss: 0.614595353603363 \n",
      "     Training Step: 166 Training Loss: 0.6158838868141174 \n",
      "     Training Step: 167 Training Loss: 0.6151055693626404 \n",
      "     Training Step: 168 Training Loss: 0.6167749166488647 \n",
      "     Training Step: 169 Training Loss: 0.6104291677474976 \n",
      "     Training Step: 170 Training Loss: 0.6149123907089233 \n",
      "     Training Step: 171 Training Loss: 0.6185399889945984 \n",
      "     Training Step: 172 Training Loss: 0.616862416267395 \n",
      "     Training Step: 173 Training Loss: 0.6129166483879089 \n",
      "     Training Step: 174 Training Loss: 0.6198710799217224 \n",
      "     Training Step: 175 Training Loss: 0.6144939064979553 \n",
      "     Training Step: 176 Training Loss: 0.6118676662445068 \n",
      "     Training Step: 177 Training Loss: 0.6101768016815186 \n",
      "     Training Step: 178 Training Loss: 0.6141626834869385 \n",
      "     Training Step: 179 Training Loss: 0.6148207783699036 \n",
      "     Training Step: 180 Training Loss: 0.61693274974823 \n",
      "     Training Step: 181 Training Loss: 0.6156896948814392 \n",
      "     Training Step: 182 Training Loss: 0.615980327129364 \n",
      "     Training Step: 183 Training Loss: 0.6117683053016663 \n",
      "     Training Step: 184 Training Loss: 0.6138917803764343 \n",
      "     Training Step: 185 Training Loss: 0.6115497946739197 \n",
      "     Training Step: 186 Training Loss: 0.611982524394989 \n",
      "     Training Step: 187 Training Loss: 0.6159226894378662 \n",
      "     Training Step: 188 Training Loss: 0.6148884296417236 \n",
      "     Training Step: 189 Training Loss: 0.6155152916908264 \n",
      "     Training Step: 190 Training Loss: 0.616117000579834 \n",
      "     Training Step: 191 Training Loss: 0.6160699129104614 \n",
      "     Training Step: 192 Training Loss: 0.611940324306488 \n",
      "     Training Step: 193 Training Loss: 0.6156299710273743 \n",
      "     Training Step: 194 Training Loss: 0.6148316860198975 \n",
      "     Training Step: 195 Training Loss: 0.6152646541595459 \n",
      "     Training Step: 196 Training Loss: 0.6150701642036438 \n",
      "     Training Step: 197 Training Loss: 0.6197152137756348 \n",
      "     Training Step: 198 Training Loss: 0.6099348068237305 \n",
      "     Training Step: 199 Training Loss: 0.6151334643363953 \n",
      "     Training Step: 200 Training Loss: 0.6124895215034485 \n",
      "     Training Step: 201 Training Loss: 0.6175417900085449 \n",
      "     Training Step: 202 Training Loss: 0.6133710741996765 \n",
      "     Training Step: 203 Training Loss: 0.6129048466682434 \n",
      "     Training Step: 204 Training Loss: 0.6149609684944153 \n",
      "     Training Step: 205 Training Loss: 0.6118250489234924 \n",
      "     Training Step: 206 Training Loss: 0.6139734983444214 \n",
      "     Training Step: 207 Training Loss: 0.6170895099639893 \n",
      "     Training Step: 208 Training Loss: 0.6151858568191528 \n",
      "     Training Step: 209 Training Loss: 0.618465781211853 \n",
      "     Training Step: 210 Training Loss: 0.6101769804954529 \n",
      "     Training Step: 211 Training Loss: 0.6209839582443237 \n",
      "     Training Step: 212 Training Loss: 0.6127251982688904 \n",
      "     Training Step: 213 Training Loss: 0.6136277318000793 \n",
      "     Training Step: 214 Training Loss: 0.6173033118247986 \n",
      "     Training Step: 215 Training Loss: 0.6155951619148254 \n",
      "     Training Step: 216 Training Loss: 0.6147921681404114 \n",
      "     Training Step: 217 Training Loss: 0.6162489652633667 \n",
      "     Training Step: 218 Training Loss: 0.6096962094306946 \n",
      "     Training Step: 219 Training Loss: 0.6178788542747498 \n",
      "     Training Step: 220 Training Loss: 0.6132200956344604 \n",
      "     Training Step: 221 Training Loss: 0.6156145334243774 \n",
      "     Training Step: 222 Training Loss: 0.6124207377433777 \n",
      "     Training Step: 223 Training Loss: 0.618145763874054 \n",
      "     Training Step: 224 Training Loss: 0.6133361458778381 \n",
      "     Training Step: 225 Training Loss: 0.6136985421180725 \n",
      "     Training Step: 226 Training Loss: 0.6135570406913757 \n",
      "     Training Step: 227 Training Loss: 0.6138516068458557 \n",
      "     Training Step: 228 Training Loss: 0.6120657324790955 \n",
      "     Training Step: 229 Training Loss: 0.6142257452011108 \n",
      "     Training Step: 230 Training Loss: 0.6134985089302063 \n",
      "     Training Step: 231 Training Loss: 0.6141900420188904 \n",
      "     Training Step: 232 Training Loss: 0.6181702613830566 \n",
      "     Training Step: 233 Training Loss: 0.6130792498588562 \n",
      "     Training Step: 234 Training Loss: 0.6132565140724182 \n",
      "     Training Step: 235 Training Loss: 0.614885151386261 \n",
      "     Training Step: 236 Training Loss: 0.6122890114784241 \n",
      "     Training Step: 237 Training Loss: 0.6175217032432556 \n",
      "     Training Step: 238 Training Loss: 0.6098126769065857 \n",
      "     Training Step: 239 Training Loss: 0.6130349636077881 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6131653785705566 \n",
      "     Validation Step: 1 Validation Loss: 0.6156899333000183 \n",
      "     Validation Step: 2 Validation Loss: 0.6108871698379517 \n",
      "     Validation Step: 3 Validation Loss: 0.6138375997543335 \n",
      "     Validation Step: 4 Validation Loss: 0.6185263395309448 \n",
      "     Validation Step: 5 Validation Loss: 0.6144027709960938 \n",
      "     Validation Step: 6 Validation Loss: 0.614683210849762 \n",
      "     Validation Step: 7 Validation Loss: 0.6158862113952637 \n",
      "     Validation Step: 8 Validation Loss: 0.6104527711868286 \n",
      "     Validation Step: 9 Validation Loss: 0.6113572716712952 \n",
      "     Validation Step: 10 Validation Loss: 0.6173785924911499 \n",
      "     Validation Step: 11 Validation Loss: 0.6154879927635193 \n",
      "     Validation Step: 12 Validation Loss: 0.6176424026489258 \n",
      "     Validation Step: 13 Validation Loss: 0.6107515692710876 \n",
      "     Validation Step: 14 Validation Loss: 0.6079017519950867 \n",
      "     Validation Step: 15 Validation Loss: 0.6142660975456238 \n",
      "     Validation Step: 16 Validation Loss: 0.6121028661727905 \n",
      "     Validation Step: 17 Validation Loss: 0.6181485652923584 \n",
      "     Validation Step: 18 Validation Loss: 0.6147778034210205 \n",
      "     Validation Step: 19 Validation Loss: 0.6185296177864075 \n",
      "     Validation Step: 20 Validation Loss: 0.6143253445625305 \n",
      "     Validation Step: 21 Validation Loss: 0.614371657371521 \n",
      "     Validation Step: 22 Validation Loss: 0.6177694201469421 \n",
      "     Validation Step: 23 Validation Loss: 0.6153559684753418 \n",
      "     Validation Step: 24 Validation Loss: 0.6157403588294983 \n",
      "     Validation Step: 25 Validation Loss: 0.6161561608314514 \n",
      "     Validation Step: 26 Validation Loss: 0.613876223564148 \n",
      "     Validation Step: 27 Validation Loss: 0.611853301525116 \n",
      "     Validation Step: 28 Validation Loss: 0.6146959662437439 \n",
      "     Validation Step: 29 Validation Loss: 0.6137619614601135 \n",
      "     Validation Step: 30 Validation Loss: 0.6104755401611328 \n",
      "     Validation Step: 31 Validation Loss: 0.6104076504707336 \n",
      "     Validation Step: 32 Validation Loss: 0.6130021214485168 \n",
      "     Validation Step: 33 Validation Loss: 0.6118013858795166 \n",
      "     Validation Step: 34 Validation Loss: 0.6164146661758423 \n",
      "     Validation Step: 35 Validation Loss: 0.6150512099266052 \n",
      "     Validation Step: 36 Validation Loss: 0.6134376525878906 \n",
      "     Validation Step: 37 Validation Loss: 0.6183359622955322 \n",
      "     Validation Step: 38 Validation Loss: 0.6123594045639038 \n",
      "     Validation Step: 39 Validation Loss: 0.6107815504074097 \n",
      "     Validation Step: 40 Validation Loss: 0.6184489727020264 \n",
      "     Validation Step: 41 Validation Loss: 0.6149975061416626 \n",
      "     Validation Step: 42 Validation Loss: 0.6114072799682617 \n",
      "     Validation Step: 43 Validation Loss: 0.6171292662620544 \n",
      "     Validation Step: 44 Validation Loss: 0.6152114868164062 \n",
      "Epoch: 3\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6153280138969421 \n",
      "     Training Step: 1 Training Loss: 0.6209700107574463 \n",
      "     Training Step: 2 Training Loss: 0.6096518635749817 \n",
      "     Training Step: 3 Training Loss: 0.6086626648902893 \n",
      "     Training Step: 4 Training Loss: 0.6133664846420288 \n",
      "     Training Step: 5 Training Loss: 0.6163477897644043 \n",
      "     Training Step: 6 Training Loss: 0.6169785261154175 \n",
      "     Training Step: 7 Training Loss: 0.6155376434326172 \n",
      "     Training Step: 8 Training Loss: 0.6119319796562195 \n",
      "     Training Step: 9 Training Loss: 0.6187159419059753 \n",
      "     Training Step: 10 Training Loss: 0.6115051507949829 \n",
      "     Training Step: 11 Training Loss: 0.6100957989692688 \n",
      "     Training Step: 12 Training Loss: 0.6148508787155151 \n",
      "     Training Step: 13 Training Loss: 0.61168372631073 \n",
      "     Training Step: 14 Training Loss: 0.6113917231559753 \n",
      "     Training Step: 15 Training Loss: 0.6186209321022034 \n",
      "     Training Step: 16 Training Loss: 0.6175922751426697 \n",
      "     Training Step: 17 Training Loss: 0.6110377907752991 \n",
      "     Training Step: 18 Training Loss: 0.6137024164199829 \n",
      "     Training Step: 19 Training Loss: 0.6145018339157104 \n",
      "     Training Step: 20 Training Loss: 0.6101475358009338 \n",
      "     Training Step: 21 Training Loss: 0.6110060214996338 \n",
      "     Training Step: 22 Training Loss: 0.6165887117385864 \n",
      "     Training Step: 23 Training Loss: 0.6128924489021301 \n",
      "     Training Step: 24 Training Loss: 0.6147401332855225 \n",
      "     Training Step: 25 Training Loss: 0.6149278283119202 \n",
      "     Training Step: 26 Training Loss: 0.6168755888938904 \n",
      "     Training Step: 27 Training Loss: 0.6138419508934021 \n",
      "     Training Step: 28 Training Loss: 0.6150774359703064 \n",
      "     Training Step: 29 Training Loss: 0.6145040392875671 \n",
      "     Training Step: 30 Training Loss: 0.6131302118301392 \n",
      "     Training Step: 31 Training Loss: 0.6128687262535095 \n",
      "     Training Step: 32 Training Loss: 0.6186546087265015 \n",
      "     Training Step: 33 Training Loss: 0.6136710047721863 \n",
      "     Training Step: 34 Training Loss: 0.6177536845207214 \n",
      "     Training Step: 35 Training Loss: 0.6120635867118835 \n",
      "     Training Step: 36 Training Loss: 0.6148566603660583 \n",
      "     Training Step: 37 Training Loss: 0.6155174970626831 \n",
      "     Training Step: 38 Training Loss: 0.6130107641220093 \n",
      "     Training Step: 39 Training Loss: 0.6176289319992065 \n",
      "     Training Step: 40 Training Loss: 0.6138246655464172 \n",
      "     Training Step: 41 Training Loss: 0.6123847961425781 \n",
      "     Training Step: 42 Training Loss: 0.6168118119239807 \n",
      "     Training Step: 43 Training Loss: 0.6158217191696167 \n",
      "     Training Step: 44 Training Loss: 0.6109560132026672 \n",
      "     Training Step: 45 Training Loss: 0.6103266477584839 \n",
      "     Training Step: 46 Training Loss: 0.6126999855041504 \n",
      "     Training Step: 47 Training Loss: 0.6154301166534424 \n",
      "     Training Step: 48 Training Loss: 0.6154215335845947 \n",
      "     Training Step: 49 Training Loss: 0.6163235902786255 \n",
      "     Training Step: 50 Training Loss: 0.6170157790184021 \n",
      "     Training Step: 51 Training Loss: 0.6117786765098572 \n",
      "     Training Step: 52 Training Loss: 0.6177340149879456 \n",
      "     Training Step: 53 Training Loss: 0.6148391962051392 \n",
      "     Training Step: 54 Training Loss: 0.6132853627204895 \n",
      "     Training Step: 55 Training Loss: 0.6132625937461853 \n",
      "     Training Step: 56 Training Loss: 0.6117430329322815 \n",
      "     Training Step: 57 Training Loss: 0.6181094646453857 \n",
      "     Training Step: 58 Training Loss: 0.613155722618103 \n",
      "     Training Step: 59 Training Loss: 0.6108073592185974 \n",
      "     Training Step: 60 Training Loss: 0.6142604947090149 \n",
      "     Training Step: 61 Training Loss: 0.6178733110427856 \n",
      "     Training Step: 62 Training Loss: 0.6129837036132812 \n",
      "     Training Step: 63 Training Loss: 0.612555980682373 \n",
      "     Training Step: 64 Training Loss: 0.6126115918159485 \n",
      "     Training Step: 65 Training Loss: 0.6116398572921753 \n",
      "     Training Step: 66 Training Loss: 0.6110771298408508 \n",
      "     Training Step: 67 Training Loss: 0.6158881783485413 \n",
      "     Training Step: 68 Training Loss: 0.6178131103515625 \n",
      "     Training Step: 69 Training Loss: 0.614840030670166 \n",
      "     Training Step: 70 Training Loss: 0.6175116896629333 \n",
      "     Training Step: 71 Training Loss: 0.614616334438324 \n",
      "     Training Step: 72 Training Loss: 0.6135849356651306 \n",
      "     Training Step: 73 Training Loss: 0.6196257472038269 \n",
      "     Training Step: 74 Training Loss: 0.6105629801750183 \n",
      "     Training Step: 75 Training Loss: 0.6139724254608154 \n",
      "     Training Step: 76 Training Loss: 0.6169410943984985 \n",
      "     Training Step: 77 Training Loss: 0.6143231987953186 \n",
      "     Training Step: 78 Training Loss: 0.6154136657714844 \n",
      "     Training Step: 79 Training Loss: 0.6100684404373169 \n",
      "     Training Step: 80 Training Loss: 0.6140149831771851 \n",
      "     Training Step: 81 Training Loss: 0.6134012937545776 \n",
      "     Training Step: 82 Training Loss: 0.6168100833892822 \n",
      "     Training Step: 83 Training Loss: 0.6129232048988342 \n",
      "     Training Step: 84 Training Loss: 0.618440568447113 \n",
      "     Training Step: 85 Training Loss: 0.6136682033538818 \n",
      "     Training Step: 86 Training Loss: 0.6194549202919006 \n",
      "     Training Step: 87 Training Loss: 0.6146022081375122 \n",
      "     Training Step: 88 Training Loss: 0.6159353256225586 \n",
      "     Training Step: 89 Training Loss: 0.6110599040985107 \n",
      "     Training Step: 90 Training Loss: 0.6160633563995361 \n",
      "     Training Step: 91 Training Loss: 0.6181329488754272 \n",
      "     Training Step: 92 Training Loss: 0.6171761751174927 \n",
      "     Training Step: 93 Training Loss: 0.6124415397644043 \n",
      "     Training Step: 94 Training Loss: 0.612514853477478 \n",
      "     Training Step: 95 Training Loss: 0.6183986067771912 \n",
      "     Training Step: 96 Training Loss: 0.6177732348442078 \n",
      "     Training Step: 97 Training Loss: 0.6123649477958679 \n",
      "     Training Step: 98 Training Loss: 0.6123170852661133 \n",
      "     Training Step: 99 Training Loss: 0.6167564988136292 \n",
      "     Training Step: 100 Training Loss: 0.6190162301063538 \n",
      "     Training Step: 101 Training Loss: 0.6151759624481201 \n",
      "     Training Step: 102 Training Loss: 0.6119562387466431 \n",
      "     Training Step: 103 Training Loss: 0.6130523085594177 \n",
      "     Training Step: 104 Training Loss: 0.6185025572776794 \n",
      "     Training Step: 105 Training Loss: 0.613595724105835 \n",
      "     Training Step: 106 Training Loss: 0.6122831702232361 \n",
      "     Training Step: 107 Training Loss: 0.616736650466919 \n",
      "     Training Step: 108 Training Loss: 0.6120595932006836 \n",
      "     Training Step: 109 Training Loss: 0.615994393825531 \n",
      "     Training Step: 110 Training Loss: 0.6180602312088013 \n",
      "     Training Step: 111 Training Loss: 0.6116889715194702 \n",
      "     Training Step: 112 Training Loss: 0.6102598309516907 \n",
      "     Training Step: 113 Training Loss: 0.6130720376968384 \n",
      "     Training Step: 114 Training Loss: 0.6117123961448669 \n",
      "     Training Step: 115 Training Loss: 0.6157428622245789 \n",
      "     Training Step: 116 Training Loss: 0.6105851531028748 \n",
      "     Training Step: 117 Training Loss: 0.6146426796913147 \n",
      "     Training Step: 118 Training Loss: 0.6161019802093506 \n",
      "     Training Step: 119 Training Loss: 0.6144465208053589 \n",
      "     Training Step: 120 Training Loss: 0.6149941086769104 \n",
      "     Training Step: 121 Training Loss: 0.6120306253433228 \n",
      "     Training Step: 122 Training Loss: 0.6139070391654968 \n",
      "     Training Step: 123 Training Loss: 0.6125596165657043 \n",
      "     Training Step: 124 Training Loss: 0.6157242655754089 \n",
      "     Training Step: 125 Training Loss: 0.6134068369865417 \n",
      "     Training Step: 126 Training Loss: 0.6168508529663086 \n",
      "     Training Step: 127 Training Loss: 0.6097989678382874 \n",
      "     Training Step: 128 Training Loss: 0.6147838830947876 \n",
      "     Training Step: 129 Training Loss: 0.6156330108642578 \n",
      "     Training Step: 130 Training Loss: 0.6145504713058472 \n",
      "     Training Step: 131 Training Loss: 0.6103341579437256 \n",
      "     Training Step: 132 Training Loss: 0.6126790642738342 \n",
      "     Training Step: 133 Training Loss: 0.6169056296348572 \n",
      "     Training Step: 134 Training Loss: 0.6149131655693054 \n",
      "     Training Step: 135 Training Loss: 0.6134971976280212 \n",
      "     Training Step: 136 Training Loss: 0.6119995713233948 \n",
      "     Training Step: 137 Training Loss: 0.617285966873169 \n",
      "     Training Step: 138 Training Loss: 0.6141833066940308 \n",
      "     Training Step: 139 Training Loss: 0.6180965304374695 \n",
      "     Training Step: 140 Training Loss: 0.615367591381073 \n",
      "     Training Step: 141 Training Loss: 0.6155864596366882 \n",
      "     Training Step: 142 Training Loss: 0.6130155920982361 \n",
      "     Training Step: 143 Training Loss: 0.6198961734771729 \n",
      "     Training Step: 144 Training Loss: 0.6114642024040222 \n",
      "     Training Step: 145 Training Loss: 0.612322211265564 \n",
      "     Training Step: 146 Training Loss: 0.6141353249549866 \n",
      "     Training Step: 147 Training Loss: 0.6148260831832886 \n",
      "     Training Step: 148 Training Loss: 0.6145173907279968 \n",
      "     Training Step: 149 Training Loss: 0.6148164868354797 \n",
      "     Training Step: 150 Training Loss: 0.6158027052879333 \n",
      "     Training Step: 151 Training Loss: 0.611915647983551 \n",
      "     Training Step: 152 Training Loss: 0.6108527779579163 \n",
      "     Training Step: 153 Training Loss: 0.6154168248176575 \n",
      "     Training Step: 154 Training Loss: 0.615319013595581 \n",
      "     Training Step: 155 Training Loss: 0.6119990944862366 \n",
      "     Training Step: 156 Training Loss: 0.6151605248451233 \n",
      "     Training Step: 157 Training Loss: 0.6127035021781921 \n",
      "     Training Step: 158 Training Loss: 0.6103728413581848 \n",
      "     Training Step: 159 Training Loss: 0.6152768731117249 \n",
      "     Training Step: 160 Training Loss: 0.6118301749229431 \n",
      "     Training Step: 161 Training Loss: 0.6199289560317993 \n",
      "     Training Step: 162 Training Loss: 0.615519642829895 \n",
      "     Training Step: 163 Training Loss: 0.614303708076477 \n",
      "     Training Step: 164 Training Loss: 0.6158762574195862 \n",
      "     Training Step: 165 Training Loss: 0.6163256168365479 \n",
      "     Training Step: 166 Training Loss: 0.6109270453453064 \n",
      "     Training Step: 167 Training Loss: 0.6108672618865967 \n",
      "     Training Step: 168 Training Loss: 0.6202942728996277 \n",
      "     Training Step: 169 Training Loss: 0.6148148775100708 \n",
      "     Training Step: 170 Training Loss: 0.6173208355903625 \n",
      "     Training Step: 171 Training Loss: 0.6167780160903931 \n",
      "     Training Step: 172 Training Loss: 0.6168262362480164 \n",
      "     Training Step: 173 Training Loss: 0.6167728900909424 \n",
      "     Training Step: 174 Training Loss: 0.6155523061752319 \n",
      "     Training Step: 175 Training Loss: 0.6109427809715271 \n",
      "     Training Step: 176 Training Loss: 0.6134052276611328 \n",
      "     Training Step: 177 Training Loss: 0.610694169998169 \n",
      "     Training Step: 178 Training Loss: 0.612555742263794 \n",
      "     Training Step: 179 Training Loss: 0.6133710741996765 \n",
      "     Training Step: 180 Training Loss: 0.612140417098999 \n",
      "     Training Step: 181 Training Loss: 0.6124992966651917 \n",
      "     Training Step: 182 Training Loss: 0.613953709602356 \n",
      "     Training Step: 183 Training Loss: 0.613380491733551 \n",
      "     Training Step: 184 Training Loss: 0.6164024472236633 \n",
      "     Training Step: 185 Training Loss: 0.6124865412712097 \n",
      "     Training Step: 186 Training Loss: 0.6124411225318909 \n",
      "     Training Step: 187 Training Loss: 0.6124355792999268 \n",
      "     Training Step: 188 Training Loss: 0.6157627701759338 \n",
      "     Training Step: 189 Training Loss: 0.6164619326591492 \n",
      "     Training Step: 190 Training Loss: 0.6147583723068237 \n",
      "     Training Step: 191 Training Loss: 0.6182478666305542 \n",
      "     Training Step: 192 Training Loss: 0.6188772320747375 \n",
      "     Training Step: 193 Training Loss: 0.617179274559021 \n",
      "     Training Step: 194 Training Loss: 0.6182065606117249 \n",
      "     Training Step: 195 Training Loss: 0.6147735714912415 \n",
      "     Training Step: 196 Training Loss: 0.6165338754653931 \n",
      "     Training Step: 197 Training Loss: 0.6156346797943115 \n",
      "     Training Step: 198 Training Loss: 0.6134979724884033 \n",
      "     Training Step: 199 Training Loss: 0.6104235053062439 \n",
      "     Training Step: 200 Training Loss: 0.6117435097694397 \n",
      "     Training Step: 201 Training Loss: 0.6138079166412354 \n",
      "     Training Step: 202 Training Loss: 0.6135828495025635 \n",
      "     Training Step: 203 Training Loss: 0.610801637172699 \n",
      "     Training Step: 204 Training Loss: 0.6140933632850647 \n",
      "     Training Step: 205 Training Loss: 0.6133902668952942 \n",
      "     Training Step: 206 Training Loss: 0.6116608381271362 \n",
      "     Training Step: 207 Training Loss: 0.6142861843109131 \n",
      "     Training Step: 208 Training Loss: 0.6110934019088745 \n",
      "     Training Step: 209 Training Loss: 0.609684407711029 \n",
      "     Training Step: 210 Training Loss: 0.6116728186607361 \n",
      "     Training Step: 211 Training Loss: 0.6159222722053528 \n",
      "     Training Step: 212 Training Loss: 0.614041805267334 \n",
      "     Training Step: 213 Training Loss: 0.6129942536354065 \n",
      "     Training Step: 214 Training Loss: 0.6171659231185913 \n",
      "     Training Step: 215 Training Loss: 0.6154981255531311 \n",
      "     Training Step: 216 Training Loss: 0.615564227104187 \n",
      "     Training Step: 217 Training Loss: 0.6142777800559998 \n",
      "     Training Step: 218 Training Loss: 0.6164736151695251 \n",
      "     Training Step: 219 Training Loss: 0.6168838143348694 \n",
      "     Training Step: 220 Training Loss: 0.614359974861145 \n",
      "     Training Step: 221 Training Loss: 0.616742730140686 \n",
      "     Training Step: 222 Training Loss: 0.6147335171699524 \n",
      "     Training Step: 223 Training Loss: 0.6150683760643005 \n",
      "     Training Step: 224 Training Loss: 0.6145234704017639 \n",
      "     Training Step: 225 Training Loss: 0.6135667562484741 \n",
      "     Training Step: 226 Training Loss: 0.6143596172332764 \n",
      "     Training Step: 227 Training Loss: 0.6116222739219666 \n",
      "     Training Step: 228 Training Loss: 0.6117807030677795 \n",
      "     Training Step: 229 Training Loss: 0.6134507060050964 \n",
      "     Training Step: 230 Training Loss: 0.6178631782531738 \n",
      "     Training Step: 231 Training Loss: 0.6143920421600342 \n",
      "     Training Step: 232 Training Loss: 0.6139090061187744 \n",
      "     Training Step: 233 Training Loss: 0.6142184734344482 \n",
      "     Training Step: 234 Training Loss: 0.6119890809059143 \n",
      "     Training Step: 235 Training Loss: 0.6151043772697449 \n",
      "     Training Step: 236 Training Loss: 0.6126372218132019 \n",
      "     Training Step: 237 Training Loss: 0.6149448752403259 \n",
      "     Training Step: 238 Training Loss: 0.6154853701591492 \n",
      "     Training Step: 239 Training Loss: 0.611839234828949 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6118671894073486 \n",
      "     Validation Step: 1 Validation Loss: 0.6079322695732117 \n",
      "     Validation Step: 2 Validation Loss: 0.6105085611343384 \n",
      "     Validation Step: 3 Validation Loss: 0.6143105626106262 \n",
      "     Validation Step: 4 Validation Loss: 0.6123701333999634 \n",
      "     Validation Step: 5 Validation Loss: 0.6157197952270508 \n",
      "     Validation Step: 6 Validation Loss: 0.6130251884460449 \n",
      "     Validation Step: 7 Validation Loss: 0.6138319969177246 \n",
      "     Validation Step: 8 Validation Loss: 0.6153497099876404 \n",
      "     Validation Step: 9 Validation Loss: 0.6185389757156372 \n",
      "     Validation Step: 10 Validation Loss: 0.6146873831748962 \n",
      "     Validation Step: 11 Validation Loss: 0.6183045506477356 \n",
      "     Validation Step: 12 Validation Loss: 0.616094708442688 \n",
      "     Validation Step: 13 Validation Loss: 0.6151683926582336 \n",
      "     Validation Step: 14 Validation Loss: 0.6156715154647827 \n",
      "     Validation Step: 15 Validation Loss: 0.6177396774291992 \n",
      "     Validation Step: 16 Validation Loss: 0.6181435585021973 \n",
      "     Validation Step: 17 Validation Loss: 0.6149762272834778 \n",
      "     Validation Step: 18 Validation Loss: 0.6121101975440979 \n",
      "     Validation Step: 19 Validation Loss: 0.6184311509132385 \n",
      "     Validation Step: 20 Validation Loss: 0.6104692816734314 \n",
      "     Validation Step: 21 Validation Loss: 0.6107548475265503 \n",
      "     Validation Step: 22 Validation Loss: 0.6107590794563293 \n",
      "     Validation Step: 23 Validation Loss: 0.6137772798538208 \n",
      "     Validation Step: 24 Validation Loss: 0.611398458480835 \n",
      "     Validation Step: 25 Validation Loss: 0.6137973666191101 \n",
      "     Validation Step: 26 Validation Loss: 0.6134673953056335 \n",
      "     Validation Step: 27 Validation Loss: 0.6171242594718933 \n",
      "     Validation Step: 28 Validation Loss: 0.6176789999008179 \n",
      "     Validation Step: 29 Validation Loss: 0.6104637980461121 \n",
      "     Validation Step: 30 Validation Loss: 0.6185464859008789 \n",
      "     Validation Step: 31 Validation Loss: 0.614713728427887 \n",
      "     Validation Step: 32 Validation Loss: 0.6163821816444397 \n",
      "     Validation Step: 33 Validation Loss: 0.6143710017204285 \n",
      "     Validation Step: 34 Validation Loss: 0.6144434809684753 \n",
      "     Validation Step: 35 Validation Loss: 0.6173854470252991 \n",
      "     Validation Step: 36 Validation Loss: 0.6113896369934082 \n",
      "     Validation Step: 37 Validation Loss: 0.6118083000183105 \n",
      "     Validation Step: 38 Validation Loss: 0.6150956749916077 \n",
      "     Validation Step: 39 Validation Loss: 0.6143001317977905 \n",
      "     Validation Step: 40 Validation Loss: 0.6131464242935181 \n",
      "     Validation Step: 41 Validation Loss: 0.6109203100204468 \n",
      "     Validation Step: 42 Validation Loss: 0.6158513426780701 \n",
      "     Validation Step: 43 Validation Loss: 0.6154775023460388 \n",
      "     Validation Step: 44 Validation Loss: 0.6147697567939758 \n",
      "Epoch: 4\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6148281693458557 \n",
      "     Training Step: 1 Training Loss: 0.6172707080841064 \n",
      "     Training Step: 2 Training Loss: 0.6145535707473755 \n",
      "     Training Step: 3 Training Loss: 0.6153644919395447 \n",
      "     Training Step: 4 Training Loss: 0.6181012392044067 \n",
      "     Training Step: 5 Training Loss: 0.6097956299781799 \n",
      "     Training Step: 6 Training Loss: 0.6122815608978271 \n",
      "     Training Step: 7 Training Loss: 0.6129679083824158 \n",
      "     Training Step: 8 Training Loss: 0.6133868098258972 \n",
      "     Training Step: 9 Training Loss: 0.6117182970046997 \n",
      "     Training Step: 10 Training Loss: 0.614937961101532 \n",
      "     Training Step: 11 Training Loss: 0.618316650390625 \n",
      "     Training Step: 12 Training Loss: 0.6187309622764587 \n",
      "     Training Step: 13 Training Loss: 0.6178290247917175 \n",
      "     Training Step: 14 Training Loss: 0.6124783158302307 \n",
      "     Training Step: 15 Training Loss: 0.6143879294395447 \n",
      "     Training Step: 16 Training Loss: 0.6140618920326233 \n",
      "     Training Step: 17 Training Loss: 0.6097580790519714 \n",
      "     Training Step: 18 Training Loss: 0.6198420524597168 \n",
      "     Training Step: 19 Training Loss: 0.6120670437812805 \n",
      "     Training Step: 20 Training Loss: 0.6169806718826294 \n",
      "     Training Step: 21 Training Loss: 0.6166517734527588 \n",
      "     Training Step: 22 Training Loss: 0.6184265613555908 \n",
      "     Training Step: 23 Training Loss: 0.6124886274337769 \n",
      "     Training Step: 24 Training Loss: 0.6147884130477905 \n",
      "     Training Step: 25 Training Loss: 0.6158841252326965 \n",
      "     Training Step: 26 Training Loss: 0.6118156313896179 \n",
      "     Training Step: 27 Training Loss: 0.6143763065338135 \n",
      "     Training Step: 28 Training Loss: 0.6147762537002563 \n",
      "     Training Step: 29 Training Loss: 0.6136707067489624 \n",
      "     Training Step: 30 Training Loss: 0.6099624037742615 \n",
      "     Training Step: 31 Training Loss: 0.613652229309082 \n",
      "     Training Step: 32 Training Loss: 0.6156226396560669 \n",
      "     Training Step: 33 Training Loss: 0.616816520690918 \n",
      "     Training Step: 34 Training Loss: 0.6153134703636169 \n",
      "     Training Step: 35 Training Loss: 0.6103209853172302 \n",
      "     Training Step: 36 Training Loss: 0.6142949461936951 \n",
      "     Training Step: 37 Training Loss: 0.6171264052391052 \n",
      "     Training Step: 38 Training Loss: 0.6116407513618469 \n",
      "     Training Step: 39 Training Loss: 0.612364649772644 \n",
      "     Training Step: 40 Training Loss: 0.6136184334754944 \n",
      "     Training Step: 41 Training Loss: 0.6126636862754822 \n",
      "     Training Step: 42 Training Loss: 0.614255964756012 \n",
      "     Training Step: 43 Training Loss: 0.6110260486602783 \n",
      "     Training Step: 44 Training Loss: 0.6119112372398376 \n",
      "     Training Step: 45 Training Loss: 0.616371214389801 \n",
      "     Training Step: 46 Training Loss: 0.6144401431083679 \n",
      "     Training Step: 47 Training Loss: 0.6149092316627502 \n",
      "     Training Step: 48 Training Loss: 0.6118649840354919 \n",
      "     Training Step: 49 Training Loss: 0.6122356653213501 \n",
      "     Training Step: 50 Training Loss: 0.6143503189086914 \n",
      "     Training Step: 51 Training Loss: 0.6176245212554932 \n",
      "     Training Step: 52 Training Loss: 0.6162007451057434 \n",
      "     Training Step: 53 Training Loss: 0.6169222593307495 \n",
      "     Training Step: 54 Training Loss: 0.6127263903617859 \n",
      "     Training Step: 55 Training Loss: 0.6145870685577393 \n",
      "     Training Step: 56 Training Loss: 0.6110110282897949 \n",
      "     Training Step: 57 Training Loss: 0.6128821969032288 \n",
      "     Training Step: 58 Training Loss: 0.6183648109436035 \n",
      "     Training Step: 59 Training Loss: 0.6195122599601746 \n",
      "     Training Step: 60 Training Loss: 0.6159002780914307 \n",
      "     Training Step: 61 Training Loss: 0.6109119057655334 \n",
      "     Training Step: 62 Training Loss: 0.6125987768173218 \n",
      "     Training Step: 63 Training Loss: 0.6118341088294983 \n",
      "     Training Step: 64 Training Loss: 0.6099413633346558 \n",
      "     Training Step: 65 Training Loss: 0.6094662547111511 \n",
      "     Training Step: 66 Training Loss: 0.6181067824363708 \n",
      "     Training Step: 67 Training Loss: 0.6118546724319458 \n",
      "     Training Step: 68 Training Loss: 0.6127476692199707 \n",
      "     Training Step: 69 Training Loss: 0.6154859662055969 \n",
      "     Training Step: 70 Training Loss: 0.6128026247024536 \n",
      "     Training Step: 71 Training Loss: 0.619846522808075 \n",
      "     Training Step: 72 Training Loss: 0.6124362945556641 \n",
      "     Training Step: 73 Training Loss: 0.6162707805633545 \n",
      "     Training Step: 74 Training Loss: 0.61521315574646 \n",
      "     Training Step: 75 Training Loss: 0.6120390892028809 \n",
      "     Training Step: 76 Training Loss: 0.6134850978851318 \n",
      "     Training Step: 77 Training Loss: 0.6136376261711121 \n",
      "     Training Step: 78 Training Loss: 0.6144995093345642 \n",
      "     Training Step: 79 Training Loss: 0.6133740544319153 \n",
      "     Training Step: 80 Training Loss: 0.6108637452125549 \n",
      "     Training Step: 81 Training Loss: 0.6139261722564697 \n",
      "     Training Step: 82 Training Loss: 0.6137476563453674 \n",
      "     Training Step: 83 Training Loss: 0.6122718453407288 \n",
      "     Training Step: 84 Training Loss: 0.6117686629295349 \n",
      "     Training Step: 85 Training Loss: 0.6134889125823975 \n",
      "     Training Step: 86 Training Loss: 0.6159900426864624 \n",
      "     Training Step: 87 Training Loss: 0.6185609102249146 \n",
      "     Training Step: 88 Training Loss: 0.6131166815757751 \n",
      "     Training Step: 89 Training Loss: 0.6153773069381714 \n",
      "     Training Step: 90 Training Loss: 0.6177390217781067 \n",
      "     Training Step: 91 Training Loss: 0.6139319539070129 \n",
      "     Training Step: 92 Training Loss: 0.6108630299568176 \n",
      "     Training Step: 93 Training Loss: 0.6149071455001831 \n",
      "     Training Step: 94 Training Loss: 0.6156648993492126 \n",
      "     Training Step: 95 Training Loss: 0.6173216700553894 \n",
      "     Training Step: 96 Training Loss: 0.612978994846344 \n",
      "     Training Step: 97 Training Loss: 0.6133254766464233 \n",
      "     Training Step: 98 Training Loss: 0.6131922006607056 \n",
      "     Training Step: 99 Training Loss: 0.6143308281898499 \n",
      "     Training Step: 100 Training Loss: 0.6146038174629211 \n",
      "     Training Step: 101 Training Loss: 0.6148917078971863 \n",
      "     Training Step: 102 Training Loss: 0.6120738387107849 \n",
      "     Training Step: 103 Training Loss: 0.6129468679428101 \n",
      "     Training Step: 104 Training Loss: 0.6150192618370056 \n",
      "     Training Step: 105 Training Loss: 0.6186608672142029 \n",
      "     Training Step: 106 Training Loss: 0.6160604953765869 \n",
      "     Training Step: 107 Training Loss: 0.6134810447692871 \n",
      "     Training Step: 108 Training Loss: 0.615496039390564 \n",
      "     Training Step: 109 Training Loss: 0.6124111413955688 \n",
      "     Training Step: 110 Training Loss: 0.6189104914665222 \n",
      "     Training Step: 111 Training Loss: 0.6127617359161377 \n",
      "     Training Step: 112 Training Loss: 0.6106404662132263 \n",
      "     Training Step: 113 Training Loss: 0.6152122616767883 \n",
      "     Training Step: 114 Training Loss: 0.6169208884239197 \n",
      "     Training Step: 115 Training Loss: 0.6165124773979187 \n",
      "     Training Step: 116 Training Loss: 0.6136921644210815 \n",
      "     Training Step: 117 Training Loss: 0.6167544722557068 \n",
      "     Training Step: 118 Training Loss: 0.6155487895011902 \n",
      "     Training Step: 119 Training Loss: 0.6118934750556946 \n",
      "     Training Step: 120 Training Loss: 0.6141875982284546 \n",
      "     Training Step: 121 Training Loss: 0.6131095886230469 \n",
      "     Training Step: 122 Training Loss: 0.6117263436317444 \n",
      "     Training Step: 123 Training Loss: 0.6168892979621887 \n",
      "     Training Step: 124 Training Loss: 0.613905668258667 \n",
      "     Training Step: 125 Training Loss: 0.610320508480072 \n",
      "     Training Step: 126 Training Loss: 0.6116429567337036 \n",
      "     Training Step: 127 Training Loss: 0.616899847984314 \n",
      "     Training Step: 128 Training Loss: 0.615630030632019 \n",
      "     Training Step: 129 Training Loss: 0.6125033497810364 \n",
      "     Training Step: 130 Training Loss: 0.6154312491416931 \n",
      "     Training Step: 131 Training Loss: 0.6124549508094788 \n",
      "     Training Step: 132 Training Loss: 0.6126292943954468 \n",
      "     Training Step: 133 Training Loss: 0.6128656268119812 \n",
      "     Training Step: 134 Training Loss: 0.6158325672149658 \n",
      "     Training Step: 135 Training Loss: 0.6147931218147278 \n",
      "     Training Step: 136 Training Loss: 0.6118353605270386 \n",
      "     Training Step: 137 Training Loss: 0.6172577738761902 \n",
      "     Training Step: 138 Training Loss: 0.6138004660606384 \n",
      "     Training Step: 139 Training Loss: 0.6119876503944397 \n",
      "     Training Step: 140 Training Loss: 0.6154569387435913 \n",
      "     Training Step: 141 Training Loss: 0.6150529384613037 \n",
      "     Training Step: 142 Training Loss: 0.6113580465316772 \n",
      "     Training Step: 143 Training Loss: 0.6143438220024109 \n",
      "     Training Step: 144 Training Loss: 0.6202669739723206 \n",
      "     Training Step: 145 Training Loss: 0.6174452304840088 \n",
      "     Training Step: 146 Training Loss: 0.6165263652801514 \n",
      "     Training Step: 147 Training Loss: 0.6168395280838013 \n",
      "     Training Step: 148 Training Loss: 0.6164053082466125 \n",
      "     Training Step: 149 Training Loss: 0.6155768632888794 \n",
      "     Training Step: 150 Training Loss: 0.6177603006362915 \n",
      "     Training Step: 151 Training Loss: 0.6148170828819275 \n",
      "     Training Step: 152 Training Loss: 0.6120561361312866 \n",
      "     Training Step: 153 Training Loss: 0.6140244007110596 \n",
      "     Training Step: 154 Training Loss: 0.6084669828414917 \n",
      "     Training Step: 155 Training Loss: 0.6149371862411499 \n",
      "     Training Step: 156 Training Loss: 0.613888680934906 \n",
      "     Training Step: 157 Training Loss: 0.6124951243400574 \n",
      "     Training Step: 158 Training Loss: 0.6170648336410522 \n",
      "     Training Step: 159 Training Loss: 0.6158663034439087 \n",
      "     Training Step: 160 Training Loss: 0.6134678721427917 \n",
      "     Training Step: 161 Training Loss: 0.6133838891983032 \n",
      "     Training Step: 162 Training Loss: 0.6178246140480042 \n",
      "     Training Step: 163 Training Loss: 0.6147488355636597 \n",
      "     Training Step: 164 Training Loss: 0.6153805255889893 \n",
      "     Training Step: 165 Training Loss: 0.6162168979644775 \n",
      "     Training Step: 166 Training Loss: 0.6168159246444702 \n",
      "     Training Step: 167 Training Loss: 0.6154088973999023 \n",
      "     Training Step: 168 Training Loss: 0.6132863759994507 \n",
      "     Training Step: 169 Training Loss: 0.6106449961662292 \n",
      "     Training Step: 170 Training Loss: 0.6180903911590576 \n",
      "     Training Step: 171 Training Loss: 0.6188815832138062 \n",
      "     Training Step: 172 Training Loss: 0.6136706471443176 \n",
      "     Training Step: 173 Training Loss: 0.6171740293502808 \n",
      "     Training Step: 174 Training Loss: 0.6180693507194519 \n",
      "     Training Step: 175 Training Loss: 0.6140588521957397 \n",
      "     Training Step: 176 Training Loss: 0.6110689640045166 \n",
      "     Training Step: 177 Training Loss: 0.6147056818008423 \n",
      "     Training Step: 178 Training Loss: 0.6116418242454529 \n",
      "     Training Step: 179 Training Loss: 0.6150463819503784 \n",
      "     Training Step: 180 Training Loss: 0.610723614692688 \n",
      "     Training Step: 181 Training Loss: 0.614515483379364 \n",
      "     Training Step: 182 Training Loss: 0.6151334643363953 \n",
      "     Training Step: 183 Training Loss: 0.6158487796783447 \n",
      "     Training Step: 184 Training Loss: 0.6166715621948242 \n",
      "     Training Step: 185 Training Loss: 0.6135533452033997 \n",
      "     Training Step: 186 Training Loss: 0.6104412078857422 \n",
      "     Training Step: 187 Training Loss: 0.6177451014518738 \n",
      "     Training Step: 188 Training Loss: 0.6143977046012878 \n",
      "     Training Step: 189 Training Loss: 0.6158004999160767 \n",
      "     Training Step: 190 Training Loss: 0.6108823418617249 \n",
      "     Training Step: 191 Training Loss: 0.6146442890167236 \n",
      "     Training Step: 192 Training Loss: 0.6169403791427612 \n",
      "     Training Step: 193 Training Loss: 0.6107656359672546 \n",
      "     Training Step: 194 Training Loss: 0.6177437901496887 \n",
      "     Training Step: 195 Training Loss: 0.6142638921737671 \n",
      "     Training Step: 196 Training Loss: 0.6125247478485107 \n",
      "     Training Step: 197 Training Loss: 0.6155712008476257 \n",
      "     Training Step: 198 Training Loss: 0.6120193600654602 \n",
      "     Training Step: 199 Training Loss: 0.6152103543281555 \n",
      "     Training Step: 200 Training Loss: 0.618553638458252 \n",
      "     Training Step: 201 Training Loss: 0.6102566719055176 \n",
      "     Training Step: 202 Training Loss: 0.6103590130805969 \n",
      "     Training Step: 203 Training Loss: 0.6146870255470276 \n",
      "     Training Step: 204 Training Loss: 0.6210774779319763 \n",
      "     Training Step: 205 Training Loss: 0.6163166761398315 \n",
      "     Training Step: 206 Training Loss: 0.6149343252182007 \n",
      "     Training Step: 207 Training Loss: 0.6150278449058533 \n",
      "     Training Step: 208 Training Loss: 0.6160274744033813 \n",
      "     Training Step: 209 Training Loss: 0.6116155385971069 \n",
      "     Training Step: 210 Training Loss: 0.610919713973999 \n",
      "     Training Step: 211 Training Loss: 0.6099221706390381 \n",
      "     Training Step: 212 Training Loss: 0.6116529703140259 \n",
      "     Training Step: 213 Training Loss: 0.6117126941680908 \n",
      "     Training Step: 214 Training Loss: 0.6204063296318054 \n",
      "     Training Step: 215 Training Loss: 0.6114829778671265 \n",
      "     Training Step: 216 Training Loss: 0.6148430705070496 \n",
      "     Training Step: 217 Training Loss: 0.6110005378723145 \n",
      "     Training Step: 218 Training Loss: 0.6131191849708557 \n",
      "     Training Step: 219 Training Loss: 0.6168411374092102 \n",
      "     Training Step: 220 Training Loss: 0.6130781173706055 \n",
      "     Training Step: 221 Training Loss: 0.613433301448822 \n",
      "     Training Step: 222 Training Loss: 0.610805094242096 \n",
      "     Training Step: 223 Training Loss: 0.6133096814155579 \n",
      "     Training Step: 224 Training Loss: 0.6155564785003662 \n",
      "     Training Step: 225 Training Loss: 0.6185409426689148 \n",
      "     Training Step: 226 Training Loss: 0.6181013584136963 \n",
      "     Training Step: 227 Training Loss: 0.6105404496192932 \n",
      "     Training Step: 228 Training Loss: 0.6172155737876892 \n",
      "     Training Step: 229 Training Loss: 0.6130366921424866 \n",
      "     Training Step: 230 Training Loss: 0.6156867146492004 \n",
      "     Training Step: 231 Training Loss: 0.6144730448722839 \n",
      "     Training Step: 232 Training Loss: 0.6125460863113403 \n",
      "     Training Step: 233 Training Loss: 0.6115925908088684 \n",
      "     Training Step: 234 Training Loss: 0.6146166324615479 \n",
      "     Training Step: 235 Training Loss: 0.6113402843475342 \n",
      "     Training Step: 236 Training Loss: 0.614037811756134 \n",
      "     Training Step: 237 Training Loss: 0.6155493259429932 \n",
      "     Training Step: 238 Training Loss: 0.6142550706863403 \n",
      "     Training Step: 239 Training Loss: 0.6167593598365784 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6163697242736816 \n",
      "     Validation Step: 1 Validation Loss: 0.61738520860672 \n",
      "     Validation Step: 2 Validation Loss: 0.618495523929596 \n",
      "     Validation Step: 3 Validation Loss: 0.6105625629425049 \n",
      "     Validation Step: 4 Validation Loss: 0.6105546355247498 \n",
      "     Validation Step: 5 Validation Loss: 0.6151623129844666 \n",
      "     Validation Step: 6 Validation Loss: 0.6176592111587524 \n",
      "     Validation Step: 7 Validation Loss: 0.6110208034515381 \n",
      "     Validation Step: 8 Validation Loss: 0.6171151399612427 \n",
      "     Validation Step: 9 Validation Loss: 0.6143505573272705 \n",
      "     Validation Step: 10 Validation Loss: 0.6143704652786255 \n",
      "     Validation Step: 11 Validation Loss: 0.6158459782600403 \n",
      "     Validation Step: 12 Validation Loss: 0.6114789247512817 \n",
      "     Validation Step: 13 Validation Loss: 0.6105921864509583 \n",
      "     Validation Step: 14 Validation Loss: 0.6181146502494812 \n",
      "     Validation Step: 15 Validation Loss: 0.6147724390029907 \n",
      "     Validation Step: 16 Validation Loss: 0.6154667735099792 \n",
      "     Validation Step: 17 Validation Loss: 0.6182491779327393 \n",
      "     Validation Step: 18 Validation Loss: 0.6134843230247498 \n",
      "     Validation Step: 19 Validation Loss: 0.6143082976341248 \n",
      "     Validation Step: 20 Validation Loss: 0.6114509105682373 \n",
      "     Validation Step: 21 Validation Loss: 0.6150005459785461 \n",
      "     Validation Step: 22 Validation Loss: 0.6147251725196838 \n",
      "     Validation Step: 23 Validation Loss: 0.6124231815338135 \n",
      "     Validation Step: 24 Validation Loss: 0.615643322467804 \n",
      "     Validation Step: 25 Validation Loss: 0.6080531477928162 \n",
      "     Validation Step: 26 Validation Loss: 0.6138275265693665 \n",
      "     Validation Step: 27 Validation Loss: 0.6144623160362244 \n",
      "     Validation Step: 28 Validation Loss: 0.6157311797142029 \n",
      "     Validation Step: 29 Validation Loss: 0.614742636680603 \n",
      "     Validation Step: 30 Validation Loss: 0.6160778999328613 \n",
      "     Validation Step: 31 Validation Loss: 0.6177318692207336 \n",
      "     Validation Step: 32 Validation Loss: 0.6138355135917664 \n",
      "     Validation Step: 33 Validation Loss: 0.6119469404220581 \n",
      "     Validation Step: 34 Validation Loss: 0.6185036301612854 \n",
      "     Validation Step: 35 Validation Loss: 0.6108319759368896 \n",
      "     Validation Step: 36 Validation Loss: 0.6118726134300232 \n",
      "     Validation Step: 37 Validation Loss: 0.6183754801750183 \n",
      "     Validation Step: 38 Validation Loss: 0.6153788566589355 \n",
      "     Validation Step: 39 Validation Loss: 0.6130750179290771 \n",
      "     Validation Step: 40 Validation Loss: 0.6138423085212708 \n",
      "     Validation Step: 41 Validation Loss: 0.6108310222625732 \n",
      "     Validation Step: 42 Validation Loss: 0.6121953725814819 \n",
      "     Validation Step: 43 Validation Loss: 0.6131731271743774 \n",
      "     Validation Step: 44 Validation Loss: 0.6151270866394043 \n",
      "Epoch: 5\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6146478056907654 \n",
      "     Training Step: 1 Training Loss: 0.6148399710655212 \n",
      "     Training Step: 2 Training Loss: 0.614799439907074 \n",
      "     Training Step: 3 Training Loss: 0.616108238697052 \n",
      "     Training Step: 4 Training Loss: 0.6109800338745117 \n",
      "     Training Step: 5 Training Loss: 0.6085712909698486 \n",
      "     Training Step: 6 Training Loss: 0.6134228110313416 \n",
      "     Training Step: 7 Training Loss: 0.6129017472267151 \n",
      "     Training Step: 8 Training Loss: 0.613253116607666 \n",
      "     Training Step: 9 Training Loss: 0.6145204305648804 \n",
      "     Training Step: 10 Training Loss: 0.6186982989311218 \n",
      "     Training Step: 11 Training Loss: 0.6145302057266235 \n",
      "     Training Step: 12 Training Loss: 0.6173022985458374 \n",
      "     Training Step: 13 Training Loss: 0.613713264465332 \n",
      "     Training Step: 14 Training Loss: 0.6141002178192139 \n",
      "     Training Step: 15 Training Loss: 0.6137868165969849 \n",
      "     Training Step: 16 Training Loss: 0.6134893298149109 \n",
      "     Training Step: 17 Training Loss: 0.6172940135002136 \n",
      "     Training Step: 18 Training Loss: 0.6143666505813599 \n",
      "     Training Step: 19 Training Loss: 0.6117287278175354 \n",
      "     Training Step: 20 Training Loss: 0.6150203943252563 \n",
      "     Training Step: 21 Training Loss: 0.6167849898338318 \n",
      "     Training Step: 22 Training Loss: 0.6127251982688904 \n",
      "     Training Step: 23 Training Loss: 0.6109443306922913 \n",
      "     Training Step: 24 Training Loss: 0.6119944453239441 \n",
      "     Training Step: 25 Training Loss: 0.6155492067337036 \n",
      "     Training Step: 26 Training Loss: 0.6211355924606323 \n",
      "     Training Step: 27 Training Loss: 0.6107930541038513 \n",
      "     Training Step: 28 Training Loss: 0.6157928109169006 \n",
      "     Training Step: 29 Training Loss: 0.6194834113121033 \n",
      "     Training Step: 30 Training Loss: 0.6123790144920349 \n",
      "     Training Step: 31 Training Loss: 0.6136555671691895 \n",
      "     Training Step: 32 Training Loss: 0.6159793138504028 \n",
      "     Training Step: 33 Training Loss: 0.6182225346565247 \n",
      "     Training Step: 34 Training Loss: 0.614747941493988 \n",
      "     Training Step: 35 Training Loss: 0.6118311285972595 \n",
      "     Training Step: 36 Training Loss: 0.6116058826446533 \n",
      "     Training Step: 37 Training Loss: 0.6110913753509521 \n",
      "     Training Step: 38 Training Loss: 0.61781245470047 \n",
      "     Training Step: 39 Training Loss: 0.614870548248291 \n",
      "     Training Step: 40 Training Loss: 0.6127241253852844 \n",
      "     Training Step: 41 Training Loss: 0.6125059127807617 \n",
      "     Training Step: 42 Training Loss: 0.615315318107605 \n",
      "     Training Step: 43 Training Loss: 0.6129482388496399 \n",
      "     Training Step: 44 Training Loss: 0.6118394732475281 \n",
      "     Training Step: 45 Training Loss: 0.6142376661300659 \n",
      "     Training Step: 46 Training Loss: 0.6117905974388123 \n",
      "     Training Step: 47 Training Loss: 0.6103249192237854 \n",
      "     Training Step: 48 Training Loss: 0.6145679354667664 \n",
      "     Training Step: 49 Training Loss: 0.6108534336090088 \n",
      "     Training Step: 50 Training Loss: 0.6158652901649475 \n",
      "     Training Step: 51 Training Loss: 0.6122049689292908 \n",
      "     Training Step: 52 Training Loss: 0.6130361557006836 \n",
      "     Training Step: 53 Training Loss: 0.6137480735778809 \n",
      "     Training Step: 54 Training Loss: 0.616871178150177 \n",
      "     Training Step: 55 Training Loss: 0.61716628074646 \n",
      "     Training Step: 56 Training Loss: 0.615364134311676 \n",
      "     Training Step: 57 Training Loss: 0.6141619682312012 \n",
      "     Training Step: 58 Training Loss: 0.6144309043884277 \n",
      "     Training Step: 59 Training Loss: 0.6177610754966736 \n",
      "     Training Step: 60 Training Loss: 0.6113630533218384 \n",
      "     Training Step: 61 Training Loss: 0.6133540272712708 \n",
      "     Training Step: 62 Training Loss: 0.6113319396972656 \n",
      "     Training Step: 63 Training Loss: 0.6201525926589966 \n",
      "     Training Step: 64 Training Loss: 0.615389347076416 \n",
      "     Training Step: 65 Training Loss: 0.6202664971351624 \n",
      "     Training Step: 66 Training Loss: 0.6167921423912048 \n",
      "     Training Step: 67 Training Loss: 0.6109715700149536 \n",
      "     Training Step: 68 Training Loss: 0.618985652923584 \n",
      "     Training Step: 69 Training Loss: 0.6147781610488892 \n",
      "     Training Step: 70 Training Loss: 0.6147737503051758 \n",
      "     Training Step: 71 Training Loss: 0.6149007081985474 \n",
      "     Training Step: 72 Training Loss: 0.613500714302063 \n",
      "     Training Step: 73 Training Loss: 0.6134923100471497 \n",
      "     Training Step: 74 Training Loss: 0.6198372840881348 \n",
      "     Training Step: 75 Training Loss: 0.6181364059448242 \n",
      "     Training Step: 76 Training Loss: 0.6143859028816223 \n",
      "     Training Step: 77 Training Loss: 0.6125039458274841 \n",
      "     Training Step: 78 Training Loss: 0.6116988062858582 \n",
      "     Training Step: 79 Training Loss: 0.6124552488327026 \n",
      "     Training Step: 80 Training Loss: 0.6142770648002625 \n",
      "     Training Step: 81 Training Loss: 0.6176044940948486 \n",
      "     Training Step: 82 Training Loss: 0.6148340702056885 \n",
      "     Training Step: 83 Training Loss: 0.6125180721282959 \n",
      "     Training Step: 84 Training Loss: 0.6132705807685852 \n",
      "     Training Step: 85 Training Loss: 0.6146478056907654 \n",
      "     Training Step: 86 Training Loss: 0.6154423356056213 \n",
      "     Training Step: 87 Training Loss: 0.6181574463844299 \n",
      "     Training Step: 88 Training Loss: 0.6160078048706055 \n",
      "     Training Step: 89 Training Loss: 0.611916720867157 \n",
      "     Training Step: 90 Training Loss: 0.6131883263587952 \n",
      "     Training Step: 91 Training Loss: 0.6101822853088379 \n",
      "     Training Step: 92 Training Loss: 0.6147735118865967 \n",
      "     Training Step: 93 Training Loss: 0.6154621243476868 \n",
      "     Training Step: 94 Training Loss: 0.6164026260375977 \n",
      "     Training Step: 95 Training Loss: 0.6125923991203308 \n",
      "     Training Step: 96 Training Loss: 0.6123818159103394 \n",
      "     Training Step: 97 Training Loss: 0.6133777499198914 \n",
      "     Training Step: 98 Training Loss: 0.6166490316390991 \n",
      "     Training Step: 99 Training Loss: 0.612114667892456 \n",
      "     Training Step: 100 Training Loss: 0.6107818484306335 \n",
      "     Training Step: 101 Training Loss: 0.6143763661384583 \n",
      "     Training Step: 102 Training Loss: 0.6105806231498718 \n",
      "     Training Step: 103 Training Loss: 0.6185641288757324 \n",
      "     Training Step: 104 Training Loss: 0.6156654357910156 \n",
      "     Training Step: 105 Training Loss: 0.6116583943367004 \n",
      "     Training Step: 106 Training Loss: 0.6138474345207214 \n",
      "     Training Step: 107 Training Loss: 0.6161990165710449 \n",
      "     Training Step: 108 Training Loss: 0.6135919690132141 \n",
      "     Training Step: 109 Training Loss: 0.6176483035087585 \n",
      "     Training Step: 110 Training Loss: 0.6164436936378479 \n",
      "     Training Step: 111 Training Loss: 0.6185753345489502 \n",
      "     Training Step: 112 Training Loss: 0.6163071990013123 \n",
      "     Training Step: 113 Training Loss: 0.6155093312263489 \n",
      "     Training Step: 114 Training Loss: 0.6172357797622681 \n",
      "     Training Step: 115 Training Loss: 0.6116650104522705 \n",
      "     Training Step: 116 Training Loss: 0.6143741607666016 \n",
      "     Training Step: 117 Training Loss: 0.6102522611618042 \n",
      "     Training Step: 118 Training Loss: 0.6120147705078125 \n",
      "     Training Step: 119 Training Loss: 0.615619957447052 \n",
      "     Training Step: 120 Training Loss: 0.6159037351608276 \n",
      "     Training Step: 121 Training Loss: 0.617835283279419 \n",
      "     Training Step: 122 Training Loss: 0.6140781044960022 \n",
      "     Training Step: 123 Training Loss: 0.6178056001663208 \n",
      "     Training Step: 124 Training Loss: 0.6104314923286438 \n",
      "     Training Step: 125 Training Loss: 0.6120137572288513 \n",
      "     Training Step: 126 Training Loss: 0.61366206407547 \n",
      "     Training Step: 127 Training Loss: 0.6185303330421448 \n",
      "     Training Step: 128 Training Loss: 0.6106671094894409 \n",
      "     Training Step: 129 Training Loss: 0.6165332794189453 \n",
      "     Training Step: 130 Training Loss: 0.6111173033714294 \n",
      "     Training Step: 131 Training Loss: 0.6165923476219177 \n",
      "     Training Step: 132 Training Loss: 0.6151971817016602 \n",
      "     Training Step: 133 Training Loss: 0.6170558333396912 \n",
      "     Training Step: 134 Training Loss: 0.6197559237480164 \n",
      "     Training Step: 135 Training Loss: 0.6151635050773621 \n",
      "     Training Step: 136 Training Loss: 0.6168501973152161 \n",
      "     Training Step: 137 Training Loss: 0.6167898774147034 \n",
      "     Training Step: 138 Training Loss: 0.6101134419441223 \n",
      "     Training Step: 139 Training Loss: 0.6156653761863708 \n",
      "     Training Step: 140 Training Loss: 0.6096383929252625 \n",
      "     Training Step: 141 Training Loss: 0.612557590007782 \n",
      "     Training Step: 142 Training Loss: 0.611918568611145 \n",
      "     Training Step: 143 Training Loss: 0.615601658821106 \n",
      "     Training Step: 144 Training Loss: 0.6170035600662231 \n",
      "     Training Step: 145 Training Loss: 0.6108244061470032 \n",
      "     Training Step: 146 Training Loss: 0.6155725717544556 \n",
      "     Training Step: 147 Training Loss: 0.6100283265113831 \n",
      "     Training Step: 148 Training Loss: 0.6178955435752869 \n",
      "     Training Step: 149 Training Loss: 0.6163195371627808 \n",
      "     Training Step: 150 Training Loss: 0.6102957129478455 \n",
      "     Training Step: 151 Training Loss: 0.6141710877418518 \n",
      "     Training Step: 152 Training Loss: 0.6142487525939941 \n",
      "     Training Step: 153 Training Loss: 0.6133270263671875 \n",
      "     Training Step: 154 Training Loss: 0.6148697137832642 \n",
      "     Training Step: 155 Training Loss: 0.6168270111083984 \n",
      "     Training Step: 156 Training Loss: 0.616888165473938 \n",
      "     Training Step: 157 Training Loss: 0.6154735684394836 \n",
      "     Training Step: 158 Training Loss: 0.6144394874572754 \n",
      "     Training Step: 159 Training Loss: 0.6100955009460449 \n",
      "     Training Step: 160 Training Loss: 0.6153489351272583 \n",
      "     Training Step: 161 Training Loss: 0.6127921938896179 \n",
      "     Training Step: 162 Training Loss: 0.6118255257606506 \n",
      "     Training Step: 163 Training Loss: 0.6153556108474731 \n",
      "     Training Step: 164 Training Loss: 0.6161965131759644 \n",
      "     Training Step: 165 Training Loss: 0.615876317024231 \n",
      "     Training Step: 166 Training Loss: 0.6130387783050537 \n",
      "     Training Step: 167 Training Loss: 0.6116820573806763 \n",
      "     Training Step: 168 Training Loss: 0.6158567667007446 \n",
      "     Training Step: 169 Training Loss: 0.6127405166625977 \n",
      "     Training Step: 170 Training Loss: 0.6095311641693115 \n",
      "     Training Step: 171 Training Loss: 0.6137539148330688 \n",
      "     Training Step: 172 Training Loss: 0.6122834086418152 \n",
      "     Training Step: 173 Training Loss: 0.6124259233474731 \n",
      "     Training Step: 174 Training Loss: 0.6150301098823547 \n",
      "     Training Step: 175 Training Loss: 0.6139489412307739 \n",
      "     Training Step: 176 Training Loss: 0.6138556003570557 \n",
      "     Training Step: 177 Training Loss: 0.6148484945297241 \n",
      "     Training Step: 178 Training Loss: 0.6120222210884094 \n",
      "     Training Step: 179 Training Loss: 0.6181091666221619 \n",
      "     Training Step: 180 Training Loss: 0.6169189214706421 \n",
      "     Training Step: 181 Training Loss: 0.6109156608581543 \n",
      "     Training Step: 182 Training Loss: 0.612981915473938 \n",
      "     Training Step: 183 Training Loss: 0.6116908192634583 \n",
      "     Training Step: 184 Training Loss: 0.6157088875770569 \n",
      "     Training Step: 185 Training Loss: 0.6148325800895691 \n",
      "     Training Step: 186 Training Loss: 0.6132035255432129 \n",
      "     Training Step: 187 Training Loss: 0.6124400496482849 \n",
      "     Training Step: 188 Training Loss: 0.6125650405883789 \n",
      "     Training Step: 189 Training Loss: 0.614765465259552 \n",
      "     Training Step: 190 Training Loss: 0.6136875748634338 \n",
      "     Training Step: 191 Training Loss: 0.6129245162010193 \n",
      "     Training Step: 192 Training Loss: 0.615594744682312 \n",
      "     Training Step: 193 Training Loss: 0.6129797697067261 \n",
      "     Training Step: 194 Training Loss: 0.6096449494361877 \n",
      "     Training Step: 195 Training Loss: 0.6179273724555969 \n",
      "     Training Step: 196 Training Loss: 0.6134929656982422 \n",
      "     Training Step: 197 Training Loss: 0.6168963313102722 \n",
      "     Training Step: 198 Training Loss: 0.6120726466178894 \n",
      "     Training Step: 199 Training Loss: 0.6182439923286438 \n",
      "     Training Step: 200 Training Loss: 0.6181237101554871 \n",
      "     Training Step: 201 Training Loss: 0.6150453090667725 \n",
      "     Training Step: 202 Training Loss: 0.6119074821472168 \n",
      "     Training Step: 203 Training Loss: 0.6155062913894653 \n",
      "     Training Step: 204 Training Loss: 0.6117909550666809 \n",
      "     Training Step: 205 Training Loss: 0.6133842468261719 \n",
      "     Training Step: 206 Training Loss: 0.6144701242446899 \n",
      "     Training Step: 207 Training Loss: 0.6148141622543335 \n",
      "     Training Step: 208 Training Loss: 0.61676424741745 \n",
      "     Training Step: 209 Training Loss: 0.6151183247566223 \n",
      "     Training Step: 210 Training Loss: 0.6136331558227539 \n",
      "     Training Step: 211 Training Loss: 0.6116524338722229 \n",
      "     Training Step: 212 Training Loss: 0.6167374849319458 \n",
      "     Training Step: 213 Training Loss: 0.6132791638374329 \n",
      "     Training Step: 214 Training Loss: 0.614118218421936 \n",
      "     Training Step: 215 Training Loss: 0.611282229423523 \n",
      "     Training Step: 216 Training Loss: 0.6185455322265625 \n",
      "     Training Step: 217 Training Loss: 0.6116630434989929 \n",
      "     Training Step: 218 Training Loss: 0.6138465404510498 \n",
      "     Training Step: 219 Training Loss: 0.6178543567657471 \n",
      "     Training Step: 220 Training Loss: 0.6102501153945923 \n",
      "     Training Step: 221 Training Loss: 0.6185175180435181 \n",
      "     Training Step: 222 Training Loss: 0.6121929287910461 \n",
      "     Training Step: 223 Training Loss: 0.6157448291778564 \n",
      "     Training Step: 224 Training Loss: 0.6153573989868164 \n",
      "     Training Step: 225 Training Loss: 0.6129733920097351 \n",
      "     Training Step: 226 Training Loss: 0.6108986735343933 \n",
      "     Training Step: 227 Training Loss: 0.6145206093788147 \n",
      "     Training Step: 228 Training Loss: 0.6124407649040222 \n",
      "     Training Step: 229 Training Loss: 0.6147359609603882 \n",
      "     Training Step: 230 Training Loss: 0.6189351677894592 \n",
      "     Training Step: 231 Training Loss: 0.6129475831985474 \n",
      "     Training Step: 232 Training Loss: 0.6172015070915222 \n",
      "     Training Step: 233 Training Loss: 0.6127312183380127 \n",
      "     Training Step: 234 Training Loss: 0.6139882206916809 \n",
      "     Training Step: 235 Training Loss: 0.6154209971427917 \n",
      "     Training Step: 236 Training Loss: 0.616888165473938 \n",
      "     Training Step: 237 Training Loss: 0.6167733073234558 \n",
      "     Training Step: 238 Training Loss: 0.6150684952735901 \n",
      "     Training Step: 239 Training Loss: 0.6108261942863464 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6106452941894531 \n",
      "     Validation Step: 1 Validation Loss: 0.6171193718910217 \n",
      "     Validation Step: 2 Validation Loss: 0.6107866168022156 \n",
      "     Validation Step: 3 Validation Loss: 0.6157645583152771 \n",
      "     Validation Step: 4 Validation Loss: 0.6134040355682373 \n",
      "     Validation Step: 5 Validation Loss: 0.6138747930526733 \n",
      "     Validation Step: 6 Validation Loss: 0.6143515706062317 \n",
      "     Validation Step: 7 Validation Loss: 0.616194486618042 \n",
      "     Validation Step: 8 Validation Loss: 0.6154797077178955 \n",
      "     Validation Step: 9 Validation Loss: 0.6185694932937622 \n",
      "     Validation Step: 10 Validation Loss: 0.6164138317108154 \n",
      "     Validation Step: 11 Validation Loss: 0.6184917688369751 \n",
      "     Validation Step: 12 Validation Loss: 0.6143015623092651 \n",
      "     Validation Step: 13 Validation Loss: 0.6176581978797913 \n",
      "     Validation Step: 14 Validation Loss: 0.6153574585914612 \n",
      "     Validation Step: 15 Validation Loss: 0.6159675717353821 \n",
      "     Validation Step: 16 Validation Loss: 0.6131415367126465 \n",
      "     Validation Step: 17 Validation Loss: 0.6120491623878479 \n",
      "     Validation Step: 18 Validation Loss: 0.6102889776229858 \n",
      "     Validation Step: 19 Validation Loss: 0.6173872351646423 \n",
      "     Validation Step: 20 Validation Loss: 0.6122798323631287 \n",
      "     Validation Step: 21 Validation Loss: 0.6103367805480957 \n",
      "     Validation Step: 22 Validation Loss: 0.6147900819778442 \n",
      "     Validation Step: 23 Validation Loss: 0.6113027930259705 \n",
      "     Validation Step: 24 Validation Loss: 0.611729085445404 \n",
      "     Validation Step: 25 Validation Loss: 0.614639163017273 \n",
      "     Validation Step: 26 Validation Loss: 0.610723078250885 \n",
      "     Validation Step: 27 Validation Loss: 0.6143522262573242 \n",
      "     Validation Step: 28 Validation Loss: 0.6129555702209473 \n",
      "     Validation Step: 29 Validation Loss: 0.6117655038833618 \n",
      "     Validation Step: 30 Validation Loss: 0.6181834936141968 \n",
      "     Validation Step: 31 Validation Loss: 0.6103249788284302 \n",
      "     Validation Step: 32 Validation Loss: 0.6138266324996948 \n",
      "     Validation Step: 33 Validation Loss: 0.6178250908851624 \n",
      "     Validation Step: 34 Validation Loss: 0.6137131452560425 \n",
      "     Validation Step: 35 Validation Loss: 0.6077458262443542 \n",
      "     Validation Step: 36 Validation Loss: 0.6146805286407471 \n",
      "     Validation Step: 37 Validation Loss: 0.6113420724868774 \n",
      "     Validation Step: 38 Validation Loss: 0.6142016053199768 \n",
      "     Validation Step: 39 Validation Loss: 0.6185978055000305 \n",
      "     Validation Step: 40 Validation Loss: 0.6156954765319824 \n",
      "     Validation Step: 41 Validation Loss: 0.6150123476982117 \n",
      "     Validation Step: 42 Validation Loss: 0.615214467048645 \n",
      "     Validation Step: 43 Validation Loss: 0.6149827241897583 \n",
      "     Validation Step: 44 Validation Loss: 0.6184003353118896 \n",
      "Epoch: 6\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6126310229301453 \n",
      "     Training Step: 1 Training Loss: 0.6115979552268982 \n",
      "     Training Step: 2 Training Loss: 0.6190217733383179 \n",
      "     Training Step: 3 Training Loss: 0.612234890460968 \n",
      "     Training Step: 4 Training Loss: 0.6200053095817566 \n",
      "     Training Step: 5 Training Loss: 0.6153048276901245 \n",
      "     Training Step: 6 Training Loss: 0.6148708462715149 \n",
      "     Training Step: 7 Training Loss: 0.6146314144134521 \n",
      "     Training Step: 8 Training Loss: 0.6144533753395081 \n",
      "     Training Step: 9 Training Loss: 0.6140288710594177 \n",
      "     Training Step: 10 Training Loss: 0.611792802810669 \n",
      "     Training Step: 11 Training Loss: 0.6130324006080627 \n",
      "     Training Step: 12 Training Loss: 0.6135470271110535 \n",
      "     Training Step: 13 Training Loss: 0.6138083934783936 \n",
      "     Training Step: 14 Training Loss: 0.6169919967651367 \n",
      "     Training Step: 15 Training Loss: 0.6147215962409973 \n",
      "     Training Step: 16 Training Loss: 0.6122874617576599 \n",
      "     Training Step: 17 Training Loss: 0.6163309216499329 \n",
      "     Training Step: 18 Training Loss: 0.6136974692344666 \n",
      "     Training Step: 19 Training Loss: 0.6167542338371277 \n",
      "     Training Step: 20 Training Loss: 0.613284170627594 \n",
      "     Training Step: 21 Training Loss: 0.6126642823219299 \n",
      "     Training Step: 22 Training Loss: 0.6210843324661255 \n",
      "     Training Step: 23 Training Loss: 0.6124245524406433 \n",
      "     Training Step: 24 Training Loss: 0.6129899621009827 \n",
      "     Training Step: 25 Training Loss: 0.6147664785385132 \n",
      "     Training Step: 26 Training Loss: 0.6168224811553955 \n",
      "     Training Step: 27 Training Loss: 0.612679123878479 \n",
      "     Training Step: 28 Training Loss: 0.6144770383834839 \n",
      "     Training Step: 29 Training Loss: 0.6135823130607605 \n",
      "     Training Step: 30 Training Loss: 0.6138591766357422 \n",
      "     Training Step: 31 Training Loss: 0.6198157072067261 \n",
      "     Training Step: 32 Training Loss: 0.6184536218643188 \n",
      "     Training Step: 33 Training Loss: 0.6123073101043701 \n",
      "     Training Step: 34 Training Loss: 0.6116780638694763 \n",
      "     Training Step: 35 Training Loss: 0.6182984113693237 \n",
      "     Training Step: 36 Training Loss: 0.610881507396698 \n",
      "     Training Step: 37 Training Loss: 0.6145786046981812 \n",
      "     Training Step: 38 Training Loss: 0.6108851432800293 \n",
      "     Training Step: 39 Training Loss: 0.6136792898178101 \n",
      "     Training Step: 40 Training Loss: 0.6147843599319458 \n",
      "     Training Step: 41 Training Loss: 0.613617479801178 \n",
      "     Training Step: 42 Training Loss: 0.6119153499603271 \n",
      "     Training Step: 43 Training Loss: 0.6154264807701111 \n",
      "     Training Step: 44 Training Loss: 0.6106271147727966 \n",
      "     Training Step: 45 Training Loss: 0.6138355135917664 \n",
      "     Training Step: 46 Training Loss: 0.6153804063796997 \n",
      "     Training Step: 47 Training Loss: 0.6180897355079651 \n",
      "     Training Step: 48 Training Loss: 0.6147415637969971 \n",
      "     Training Step: 49 Training Loss: 0.6134353876113892 \n",
      "     Training Step: 50 Training Loss: 0.6144753694534302 \n",
      "     Training Step: 51 Training Loss: 0.6100115180015564 \n",
      "     Training Step: 52 Training Loss: 0.6168304085731506 \n",
      "     Training Step: 53 Training Loss: 0.6186313629150391 \n",
      "     Training Step: 54 Training Loss: 0.6136599779129028 \n",
      "     Training Step: 55 Training Loss: 0.6154717206954956 \n",
      "     Training Step: 56 Training Loss: 0.6145179271697998 \n",
      "     Training Step: 57 Training Loss: 0.6144890785217285 \n",
      "     Training Step: 58 Training Loss: 0.614224910736084 \n",
      "     Training Step: 59 Training Loss: 0.6167835593223572 \n",
      "     Training Step: 60 Training Loss: 0.6125563979148865 \n",
      "     Training Step: 61 Training Loss: 0.616470456123352 \n",
      "     Training Step: 62 Training Loss: 0.6160373091697693 \n",
      "     Training Step: 63 Training Loss: 0.6184465885162354 \n",
      "     Training Step: 64 Training Loss: 0.6176968216896057 \n",
      "     Training Step: 65 Training Loss: 0.6132084727287292 \n",
      "     Training Step: 66 Training Loss: 0.6166712641716003 \n",
      "     Training Step: 67 Training Loss: 0.6150463223457336 \n",
      "     Training Step: 68 Training Loss: 0.6103029251098633 \n",
      "     Training Step: 69 Training Loss: 0.616852879524231 \n",
      "     Training Step: 70 Training Loss: 0.6115160584449768 \n",
      "     Training Step: 71 Training Loss: 0.6139854192733765 \n",
      "     Training Step: 72 Training Loss: 0.6100839972496033 \n",
      "     Training Step: 73 Training Loss: 0.6156889200210571 \n",
      "     Training Step: 74 Training Loss: 0.6179027557373047 \n",
      "     Training Step: 75 Training Loss: 0.6134316325187683 \n",
      "     Training Step: 76 Training Loss: 0.6159996390342712 \n",
      "     Training Step: 77 Training Loss: 0.6116839647293091 \n",
      "     Training Step: 78 Training Loss: 0.6155279874801636 \n",
      "     Training Step: 79 Training Loss: 0.6183997392654419 \n",
      "     Training Step: 80 Training Loss: 0.6111947298049927 \n",
      "     Training Step: 81 Training Loss: 0.6113128662109375 \n",
      "     Training Step: 82 Training Loss: 0.6131094098091125 \n",
      "     Training Step: 83 Training Loss: 0.6158843040466309 \n",
      "     Training Step: 84 Training Loss: 0.6141480207443237 \n",
      "     Training Step: 85 Training Loss: 0.6182029843330383 \n",
      "     Training Step: 86 Training Loss: 0.6154898405075073 \n",
      "     Training Step: 87 Training Loss: 0.6139400005340576 \n",
      "     Training Step: 88 Training Loss: 0.6147768497467041 \n",
      "     Training Step: 89 Training Loss: 0.6163745522499084 \n",
      "     Training Step: 90 Training Loss: 0.6145755052566528 \n",
      "     Training Step: 91 Training Loss: 0.6137931942939758 \n",
      "     Training Step: 92 Training Loss: 0.6141930818557739 \n",
      "     Training Step: 93 Training Loss: 0.6160950660705566 \n",
      "     Training Step: 94 Training Loss: 0.6099035739898682 \n",
      "     Training Step: 95 Training Loss: 0.6156193614006042 \n",
      "     Training Step: 96 Training Loss: 0.6155081391334534 \n",
      "     Training Step: 97 Training Loss: 0.6169413924217224 \n",
      "     Training Step: 98 Training Loss: 0.6144780516624451 \n",
      "     Training Step: 99 Training Loss: 0.6143088340759277 \n",
      "     Training Step: 100 Training Loss: 0.612575352191925 \n",
      "     Training Step: 101 Training Loss: 0.6149379014968872 \n",
      "     Training Step: 102 Training Loss: 0.6158656477928162 \n",
      "     Training Step: 103 Training Loss: 0.6103635430335999 \n",
      "     Training Step: 104 Training Loss: 0.6116626262664795 \n",
      "     Training Step: 105 Training Loss: 0.6118343472480774 \n",
      "     Training Step: 106 Training Loss: 0.6169450283050537 \n",
      "     Training Step: 107 Training Loss: 0.6147608757019043 \n",
      "     Training Step: 108 Training Loss: 0.6119922995567322 \n",
      "     Training Step: 109 Training Loss: 0.6144384145736694 \n",
      "     Training Step: 110 Training Loss: 0.6124206781387329 \n",
      "     Training Step: 111 Training Loss: 0.6150469779968262 \n",
      "     Training Step: 112 Training Loss: 0.6172857284545898 \n",
      "     Training Step: 113 Training Loss: 0.612982988357544 \n",
      "     Training Step: 114 Training Loss: 0.6132924556732178 \n",
      "     Training Step: 115 Training Loss: 0.6140570640563965 \n",
      "     Training Step: 116 Training Loss: 0.6158596873283386 \n",
      "     Training Step: 117 Training Loss: 0.6176416873931885 \n",
      "     Training Step: 118 Training Loss: 0.6178479790687561 \n",
      "     Training Step: 119 Training Loss: 0.6158148050308228 \n",
      "     Training Step: 120 Training Loss: 0.613961398601532 \n",
      "     Training Step: 121 Training Loss: 0.6158745884895325 \n",
      "     Training Step: 122 Training Loss: 0.6178171038627625 \n",
      "     Training Step: 123 Training Loss: 0.6123790144920349 \n",
      "     Training Step: 124 Training Loss: 0.61622154712677 \n",
      "     Training Step: 125 Training Loss: 0.6105994582176208 \n",
      "     Training Step: 126 Training Loss: 0.61729896068573 \n",
      "     Training Step: 127 Training Loss: 0.6183204054832458 \n",
      "     Training Step: 128 Training Loss: 0.6126466393470764 \n",
      "     Training Step: 129 Training Loss: 0.6117497086524963 \n",
      "     Training Step: 130 Training Loss: 0.6149638891220093 \n",
      "     Training Step: 131 Training Loss: 0.6105709075927734 \n",
      "     Training Step: 132 Training Loss: 0.6102591156959534 \n",
      "     Training Step: 133 Training Loss: 0.6179019808769226 \n",
      "     Training Step: 134 Training Loss: 0.614454448223114 \n",
      "     Training Step: 135 Training Loss: 0.6155856847763062 \n",
      "     Training Step: 136 Training Loss: 0.6135703921318054 \n",
      "     Training Step: 137 Training Loss: 0.6180865168571472 \n",
      "     Training Step: 138 Training Loss: 0.6124792098999023 \n",
      "     Training Step: 139 Training Loss: 0.6097543835639954 \n",
      "     Training Step: 140 Training Loss: 0.612676739692688 \n",
      "     Training Step: 141 Training Loss: 0.6128057241439819 \n",
      "     Training Step: 142 Training Loss: 0.6150355339050293 \n",
      "     Training Step: 143 Training Loss: 0.6107040047645569 \n",
      "     Training Step: 144 Training Loss: 0.6134049296379089 \n",
      "     Training Step: 145 Training Loss: 0.6129462122917175 \n",
      "     Training Step: 146 Training Loss: 0.6167832016944885 \n",
      "     Training Step: 147 Training Loss: 0.613262414932251 \n",
      "     Training Step: 148 Training Loss: 0.6110683083534241 \n",
      "     Training Step: 149 Training Loss: 0.6116082668304443 \n",
      "     Training Step: 150 Training Loss: 0.6181586980819702 \n",
      "     Training Step: 151 Training Loss: 0.6172099709510803 \n",
      "     Training Step: 152 Training Loss: 0.6148905158042908 \n",
      "     Training Step: 153 Training Loss: 0.6147992610931396 \n",
      "     Training Step: 154 Training Loss: 0.6121814846992493 \n",
      "     Training Step: 155 Training Loss: 0.6179192662239075 \n",
      "     Training Step: 156 Training Loss: 0.6167385578155518 \n",
      "     Training Step: 157 Training Loss: 0.6119782328605652 \n",
      "     Training Step: 158 Training Loss: 0.6116313338279724 \n",
      "     Training Step: 159 Training Loss: 0.6184918284416199 \n",
      "     Training Step: 160 Training Loss: 0.6202574968338013 \n",
      "     Training Step: 161 Training Loss: 0.6099959015846252 \n",
      "     Training Step: 162 Training Loss: 0.615301787853241 \n",
      "     Training Step: 163 Training Loss: 0.6155275702476501 \n",
      "     Training Step: 164 Training Loss: 0.6196942925453186 \n",
      "     Training Step: 165 Training Loss: 0.6134300231933594 \n",
      "     Training Step: 166 Training Loss: 0.611341118812561 \n",
      "     Training Step: 167 Training Loss: 0.6129655838012695 \n",
      "     Training Step: 168 Training Loss: 0.6127569079399109 \n",
      "     Training Step: 169 Training Loss: 0.6093759536743164 \n",
      "     Training Step: 170 Training Loss: 0.619088351726532 \n",
      "     Training Step: 171 Training Loss: 0.6147545576095581 \n",
      "     Training Step: 172 Training Loss: 0.6165933012962341 \n",
      "     Training Step: 173 Training Loss: 0.6085799932479858 \n",
      "     Training Step: 174 Training Loss: 0.6156692504882812 \n",
      "     Training Step: 175 Training Loss: 0.6119894981384277 \n",
      "     Training Step: 176 Training Loss: 0.611332356929779 \n",
      "     Training Step: 177 Training Loss: 0.6151652932167053 \n",
      "     Training Step: 178 Training Loss: 0.6147933602333069 \n",
      "     Training Step: 179 Training Loss: 0.6172019839286804 \n",
      "     Training Step: 180 Training Loss: 0.6176450252532959 \n",
      "     Training Step: 181 Training Loss: 0.6141931414604187 \n",
      "     Training Step: 182 Training Loss: 0.6104617118835449 \n",
      "     Training Step: 183 Training Loss: 0.6120138764381409 \n",
      "     Training Step: 184 Training Loss: 0.613396167755127 \n",
      "     Training Step: 185 Training Loss: 0.6159814596176147 \n",
      "     Training Step: 186 Training Loss: 0.6170610189437866 \n",
      "     Training Step: 187 Training Loss: 0.6126166582107544 \n",
      "     Training Step: 188 Training Loss: 0.6167958378791809 \n",
      "     Training Step: 189 Training Loss: 0.6148967146873474 \n",
      "     Training Step: 190 Training Loss: 0.6121140718460083 \n",
      "     Training Step: 191 Training Loss: 0.6118349432945251 \n",
      "     Training Step: 192 Training Loss: 0.6096466183662415 \n",
      "     Training Step: 193 Training Loss: 0.6119710803031921 \n",
      "     Training Step: 194 Training Loss: 0.6156595945358276 \n",
      "     Training Step: 195 Training Loss: 0.6147013902664185 \n",
      "     Training Step: 196 Training Loss: 0.612440288066864 \n",
      "     Training Step: 197 Training Loss: 0.6156579256057739 \n",
      "     Training Step: 198 Training Loss: 0.6169006824493408 \n",
      "     Training Step: 199 Training Loss: 0.6141806244850159 \n",
      "     Training Step: 200 Training Loss: 0.6194915175437927 \n",
      "     Training Step: 201 Training Loss: 0.6130269765853882 \n",
      "     Training Step: 202 Training Loss: 0.6134688258171082 \n",
      "     Training Step: 203 Training Loss: 0.6169330477714539 \n",
      "     Training Step: 204 Training Loss: 0.6102853417396545 \n",
      "     Training Step: 205 Training Loss: 0.6156827211380005 \n",
      "     Training Step: 206 Training Loss: 0.6117010712623596 \n",
      "     Training Step: 207 Training Loss: 0.6152686476707458 \n",
      "     Training Step: 208 Training Loss: 0.6107073426246643 \n",
      "     Training Step: 209 Training Loss: 0.616005003452301 \n",
      "     Training Step: 210 Training Loss: 0.6152717471122742 \n",
      "     Training Step: 211 Training Loss: 0.6164385080337524 \n",
      "     Training Step: 212 Training Loss: 0.6162664294242859 \n",
      "     Training Step: 213 Training Loss: 0.6118972897529602 \n",
      "     Training Step: 214 Training Loss: 0.6133060455322266 \n",
      "     Training Step: 215 Training Loss: 0.6125243902206421 \n",
      "     Training Step: 216 Training Loss: 0.6107273101806641 \n",
      "     Training Step: 217 Training Loss: 0.6108051538467407 \n",
      "     Training Step: 218 Training Loss: 0.6143428683280945 \n",
      "     Training Step: 219 Training Loss: 0.6175935864448547 \n",
      "     Training Step: 220 Training Loss: 0.612002968788147 \n",
      "     Training Step: 221 Training Loss: 0.615475058555603 \n",
      "     Training Step: 222 Training Loss: 0.6118975877761841 \n",
      "     Training Step: 223 Training Loss: 0.6132322549819946 \n",
      "     Training Step: 224 Training Loss: 0.612371027469635 \n",
      "     Training Step: 225 Training Loss: 0.6143345236778259 \n",
      "     Training Step: 226 Training Loss: 0.6147586703300476 \n",
      "     Training Step: 227 Training Loss: 0.6172112822532654 \n",
      "     Training Step: 228 Training Loss: 0.6107653379440308 \n",
      "     Training Step: 229 Training Loss: 0.6107236742973328 \n",
      "     Training Step: 230 Training Loss: 0.6190954446792603 \n",
      "     Training Step: 231 Training Loss: 0.6115409731864929 \n",
      "     Training Step: 232 Training Loss: 0.6151267886161804 \n",
      "     Training Step: 233 Training Loss: 0.6153711676597595 \n",
      "     Training Step: 234 Training Loss: 0.6130483150482178 \n",
      "     Training Step: 235 Training Loss: 0.6137350797653198 \n",
      "     Training Step: 236 Training Loss: 0.6130266785621643 \n",
      "     Training Step: 237 Training Loss: 0.6148015856742859 \n",
      "     Training Step: 238 Training Loss: 0.616576075553894 \n",
      "     Training Step: 239 Training Loss: 0.6108974814414978 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6154162883758545 \n",
      "     Validation Step: 1 Validation Loss: 0.6173709630966187 \n",
      "     Validation Step: 2 Validation Loss: 0.6163222789764404 \n",
      "     Validation Step: 3 Validation Loss: 0.6177614331245422 \n",
      "     Validation Step: 4 Validation Loss: 0.6113191246986389 \n",
      "     Validation Step: 5 Validation Loss: 0.6183045506477356 \n",
      "     Validation Step: 6 Validation Loss: 0.6133811473846436 \n",
      "     Validation Step: 7 Validation Loss: 0.6158878803253174 \n",
      "     Validation Step: 8 Validation Loss: 0.6150039434432983 \n",
      "     Validation Step: 9 Validation Loss: 0.6184095144271851 \n",
      "     Validation Step: 10 Validation Loss: 0.6156920790672302 \n",
      "     Validation Step: 11 Validation Loss: 0.6130882501602173 \n",
      "     Validation Step: 12 Validation Loss: 0.6185386180877686 \n",
      "     Validation Step: 13 Validation Loss: 0.6077754497528076 \n",
      "     Validation Step: 14 Validation Loss: 0.6142380237579346 \n",
      "     Validation Step: 15 Validation Loss: 0.6146253943443298 \n",
      "     Validation Step: 16 Validation Loss: 0.6143031120300293 \n",
      "     Validation Step: 17 Validation Loss: 0.6129425764083862 \n",
      "     Validation Step: 18 Validation Loss: 0.6146478056907654 \n",
      "     Validation Step: 19 Validation Loss: 0.6137717366218567 \n",
      "     Validation Step: 20 Validation Loss: 0.6116911768913269 \n",
      "     Validation Step: 21 Validation Loss: 0.6122922301292419 \n",
      "     Validation Step: 22 Validation Loss: 0.6153040528297424 \n",
      "     Validation Step: 23 Validation Loss: 0.6108083724975586 \n",
      "     Validation Step: 24 Validation Loss: 0.6137381196022034 \n",
      "     Validation Step: 25 Validation Loss: 0.6103224158287048 \n",
      "     Validation Step: 26 Validation Loss: 0.6106178164482117 \n",
      "     Validation Step: 27 Validation Loss: 0.6103101968765259 \n",
      "     Validation Step: 28 Validation Loss: 0.6156194806098938 \n",
      "     Validation Step: 29 Validation Loss: 0.6147199273109436 \n",
      "     Validation Step: 30 Validation Loss: 0.6160745024681091 \n",
      "     Validation Step: 31 Validation Loss: 0.6142383813858032 \n",
      "     Validation Step: 32 Validation Loss: 0.6117684245109558 \n",
      "     Validation Step: 33 Validation Loss: 0.6176508069038391 \n",
      "     Validation Step: 34 Validation Loss: 0.6181293725967407 \n",
      "     Validation Step: 35 Validation Loss: 0.6120367050170898 \n",
      "     Validation Step: 36 Validation Loss: 0.6185415983200073 \n",
      "     Validation Step: 37 Validation Loss: 0.6137322783470154 \n",
      "     Validation Step: 38 Validation Loss: 0.6170820593833923 \n",
      "     Validation Step: 39 Validation Loss: 0.6103315353393555 \n",
      "     Validation Step: 40 Validation Loss: 0.611301064491272 \n",
      "     Validation Step: 41 Validation Loss: 0.6149352192878723 \n",
      "     Validation Step: 42 Validation Loss: 0.6106615662574768 \n",
      "     Validation Step: 43 Validation Loss: 0.6143518090248108 \n",
      "     Validation Step: 44 Validation Loss: 0.6151256561279297 \n",
      "Epoch: 7\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6167418360710144 \n",
      "     Training Step: 1 Training Loss: 0.6134268641471863 \n",
      "     Training Step: 2 Training Loss: 0.6126110553741455 \n",
      "     Training Step: 3 Training Loss: 0.6120373010635376 \n",
      "     Training Step: 4 Training Loss: 0.616094172000885 \n",
      "     Training Step: 5 Training Loss: 0.6119202375411987 \n",
      "     Training Step: 6 Training Loss: 0.6126096248626709 \n",
      "     Training Step: 7 Training Loss: 0.6125578880310059 \n",
      "     Training Step: 8 Training Loss: 0.6172879338264465 \n",
      "     Training Step: 9 Training Loss: 0.6133500337600708 \n",
      "     Training Step: 10 Training Loss: 0.6112778186798096 \n",
      "     Training Step: 11 Training Loss: 0.6153033971786499 \n",
      "     Training Step: 12 Training Loss: 0.6129438281059265 \n",
      "     Training Step: 13 Training Loss: 0.6134483814239502 \n",
      "     Training Step: 14 Training Loss: 0.6129168272018433 \n",
      "     Training Step: 15 Training Loss: 0.6128390431404114 \n",
      "     Training Step: 16 Training Loss: 0.61454838514328 \n",
      "     Training Step: 17 Training Loss: 0.6147837042808533 \n",
      "     Training Step: 18 Training Loss: 0.6155099272727966 \n",
      "     Training Step: 19 Training Loss: 0.6099482774734497 \n",
      "     Training Step: 20 Training Loss: 0.6115739941596985 \n",
      "     Training Step: 21 Training Loss: 0.6145866513252258 \n",
      "     Training Step: 22 Training Loss: 0.6136728525161743 \n",
      "     Training Step: 23 Training Loss: 0.6122158765792847 \n",
      "     Training Step: 24 Training Loss: 0.6129299402236938 \n",
      "     Training Step: 25 Training Loss: 0.616305947303772 \n",
      "     Training Step: 26 Training Loss: 0.6167826056480408 \n",
      "     Training Step: 27 Training Loss: 0.6117895245552063 \n",
      "     Training Step: 28 Training Loss: 0.6195072531700134 \n",
      "     Training Step: 29 Training Loss: 0.6154178380966187 \n",
      "     Training Step: 30 Training Loss: 0.6165295243263245 \n",
      "     Training Step: 31 Training Loss: 0.6163673996925354 \n",
      "     Training Step: 32 Training Loss: 0.6123529076576233 \n",
      "     Training Step: 33 Training Loss: 0.6109564900398254 \n",
      "     Training Step: 34 Training Loss: 0.6173171401023865 \n",
      "     Training Step: 35 Training Loss: 0.6177546381950378 \n",
      "     Training Step: 36 Training Loss: 0.6151440739631653 \n",
      "     Training Step: 37 Training Loss: 0.6183915138244629 \n",
      "     Training Step: 38 Training Loss: 0.6147698760032654 \n",
      "     Training Step: 39 Training Loss: 0.613703727722168 \n",
      "     Training Step: 40 Training Loss: 0.6182665824890137 \n",
      "     Training Step: 41 Training Loss: 0.6103441715240479 \n",
      "     Training Step: 42 Training Loss: 0.6154325604438782 \n",
      "     Training Step: 43 Training Loss: 0.6102558970451355 \n",
      "     Training Step: 44 Training Loss: 0.6116563081741333 \n",
      "     Training Step: 45 Training Loss: 0.6161314845085144 \n",
      "     Training Step: 46 Training Loss: 0.617594838142395 \n",
      "     Training Step: 47 Training Loss: 0.6165049076080322 \n",
      "     Training Step: 48 Training Loss: 0.6178753972053528 \n",
      "     Training Step: 49 Training Loss: 0.6117123961448669 \n",
      "     Training Step: 50 Training Loss: 0.6107498407363892 \n",
      "     Training Step: 51 Training Loss: 0.6201597452163696 \n",
      "     Training Step: 52 Training Loss: 0.6100186109542847 \n",
      "     Training Step: 53 Training Loss: 0.6156051158905029 \n",
      "     Training Step: 54 Training Loss: 0.6118295788764954 \n",
      "     Training Step: 55 Training Loss: 0.6163016557693481 \n",
      "     Training Step: 56 Training Loss: 0.6132097840309143 \n",
      "     Training Step: 57 Training Loss: 0.6144252419471741 \n",
      "     Training Step: 58 Training Loss: 0.6123009920120239 \n",
      "     Training Step: 59 Training Loss: 0.6134107708930969 \n",
      "     Training Step: 60 Training Loss: 0.6149045825004578 \n",
      "     Training Step: 61 Training Loss: 0.6159480214118958 \n",
      "     Training Step: 62 Training Loss: 0.6147815585136414 \n",
      "     Training Step: 63 Training Loss: 0.6108022332191467 \n",
      "     Training Step: 64 Training Loss: 0.6116123199462891 \n",
      "     Training Step: 65 Training Loss: 0.615304708480835 \n",
      "     Training Step: 66 Training Loss: 0.6163621544837952 \n",
      "     Training Step: 67 Training Loss: 0.6096246838569641 \n",
      "     Training Step: 68 Training Loss: 0.6157543063163757 \n",
      "     Training Step: 69 Training Loss: 0.6147289872169495 \n",
      "     Training Step: 70 Training Loss: 0.6141396164894104 \n",
      "     Training Step: 71 Training Loss: 0.614541232585907 \n",
      "     Training Step: 72 Training Loss: 0.6155831217765808 \n",
      "     Training Step: 73 Training Loss: 0.6147943735122681 \n",
      "     Training Step: 74 Training Loss: 0.618121325969696 \n",
      "     Training Step: 75 Training Loss: 0.6110588312149048 \n",
      "     Training Step: 76 Training Loss: 0.6127986907958984 \n",
      "     Training Step: 77 Training Loss: 0.6134467720985413 \n",
      "     Training Step: 78 Training Loss: 0.6129478216171265 \n",
      "     Training Step: 79 Training Loss: 0.6189676523208618 \n",
      "     Training Step: 80 Training Loss: 0.6139057278633118 \n",
      "     Training Step: 81 Training Loss: 0.614231526851654 \n",
      "     Training Step: 82 Training Loss: 0.6178255081176758 \n",
      "     Training Step: 83 Training Loss: 0.6167073845863342 \n",
      "     Training Step: 84 Training Loss: 0.6167855262756348 \n",
      "     Training Step: 85 Training Loss: 0.6133540868759155 \n",
      "     Training Step: 86 Training Loss: 0.6143707036972046 \n",
      "     Training Step: 87 Training Loss: 0.6184664964675903 \n",
      "     Training Step: 88 Training Loss: 0.6123562455177307 \n",
      "     Training Step: 89 Training Loss: 0.6132580637931824 \n",
      "     Training Step: 90 Training Loss: 0.6197853088378906 \n",
      "     Training Step: 91 Training Loss: 0.6117665767669678 \n",
      "     Training Step: 92 Training Loss: 0.61158287525177 \n",
      "     Training Step: 93 Training Loss: 0.6124534606933594 \n",
      "     Training Step: 94 Training Loss: 0.6155462265014648 \n",
      "     Training Step: 95 Training Loss: 0.6187012791633606 \n",
      "     Training Step: 96 Training Loss: 0.614168107509613 \n",
      "     Training Step: 97 Training Loss: 0.6099478602409363 \n",
      "     Training Step: 98 Training Loss: 0.6107353568077087 \n",
      "     Training Step: 99 Training Loss: 0.6148138046264648 \n",
      "     Training Step: 100 Training Loss: 0.6127456426620483 \n",
      "     Training Step: 101 Training Loss: 0.6106609106063843 \n",
      "     Training Step: 102 Training Loss: 0.6117678880691528 \n",
      "     Training Step: 103 Training Loss: 0.6186308860778809 \n",
      "     Training Step: 104 Training Loss: 0.6093834638595581 \n",
      "     Training Step: 105 Training Loss: 0.6138432621955872 \n",
      "     Training Step: 106 Training Loss: 0.6168539524078369 \n",
      "     Training Step: 107 Training Loss: 0.6151312589645386 \n",
      "     Training Step: 108 Training Loss: 0.6130900382995605 \n",
      "     Training Step: 109 Training Loss: 0.6155110597610474 \n",
      "     Training Step: 110 Training Loss: 0.613775372505188 \n",
      "     Training Step: 111 Training Loss: 0.6132214665412903 \n",
      "     Training Step: 112 Training Loss: 0.6143820881843567 \n",
      "     Training Step: 113 Training Loss: 0.6167925000190735 \n",
      "     Training Step: 114 Training Loss: 0.6103395223617554 \n",
      "     Training Step: 115 Training Loss: 0.6119710206985474 \n",
      "     Training Step: 116 Training Loss: 0.6147708296775818 \n",
      "     Training Step: 117 Training Loss: 0.6125614047050476 \n",
      "     Training Step: 118 Training Loss: 0.6116844415664673 \n",
      "     Training Step: 119 Training Loss: 0.6132110953330994 \n",
      "     Training Step: 120 Training Loss: 0.6169376969337463 \n",
      "     Training Step: 121 Training Loss: 0.6143872141838074 \n",
      "     Training Step: 122 Training Loss: 0.6116661429405212 \n",
      "     Training Step: 123 Training Loss: 0.6131120920181274 \n",
      "     Training Step: 124 Training Loss: 0.6158705353736877 \n",
      "     Training Step: 125 Training Loss: 0.6148374676704407 \n",
      "     Training Step: 126 Training Loss: 0.6138420104980469 \n",
      "     Training Step: 127 Training Loss: 0.6155743598937988 \n",
      "     Training Step: 128 Training Loss: 0.613684892654419 \n",
      "     Training Step: 129 Training Loss: 0.6167389750480652 \n",
      "     Training Step: 130 Training Loss: 0.6167681217193604 \n",
      "     Training Step: 131 Training Loss: 0.6202609539031982 \n",
      "     Training Step: 132 Training Loss: 0.6119967103004456 \n",
      "     Training Step: 133 Training Loss: 0.6146734952926636 \n",
      "     Training Step: 134 Training Loss: 0.6197906732559204 \n",
      "     Training Step: 135 Training Loss: 0.610902726650238 \n",
      "     Training Step: 136 Training Loss: 0.6141359210014343 \n",
      "     Training Step: 137 Training Loss: 0.610194206237793 \n",
      "     Training Step: 138 Training Loss: 0.6212174296379089 \n",
      "     Training Step: 139 Training Loss: 0.6141894459724426 \n",
      "     Training Step: 140 Training Loss: 0.61326664686203 \n",
      "     Training Step: 141 Training Loss: 0.6096378564834595 \n",
      "     Training Step: 142 Training Loss: 0.6156125068664551 \n",
      "     Training Step: 143 Training Loss: 0.6154811978340149 \n",
      "     Training Step: 144 Training Loss: 0.6155029535293579 \n",
      "     Training Step: 145 Training Loss: 0.6124494671821594 \n",
      "     Training Step: 146 Training Loss: 0.6153389811515808 \n",
      "     Training Step: 147 Training Loss: 0.6116015315055847 \n",
      "     Training Step: 148 Training Loss: 0.613993227481842 \n",
      "     Training Step: 149 Training Loss: 0.6105028390884399 \n",
      "     Training Step: 150 Training Loss: 0.6148397326469421 \n",
      "     Training Step: 151 Training Loss: 0.6158403754234314 \n",
      "     Training Step: 152 Training Loss: 0.6161067485809326 \n",
      "     Training Step: 153 Training Loss: 0.6177680492401123 \n",
      "     Training Step: 154 Training Loss: 0.6120194792747498 \n",
      "     Training Step: 155 Training Loss: 0.6149686574935913 \n",
      "     Training Step: 156 Training Loss: 0.6145563721656799 \n",
      "     Training Step: 157 Training Loss: 0.6130517721176147 \n",
      "     Training Step: 158 Training Loss: 0.6139665842056274 \n",
      "     Training Step: 159 Training Loss: 0.6115614175796509 \n",
      "     Training Step: 160 Training Loss: 0.610717236995697 \n",
      "     Training Step: 161 Training Loss: 0.6175047755241394 \n",
      "     Training Step: 162 Training Loss: 0.6133735775947571 \n",
      "     Training Step: 163 Training Loss: 0.6187422275543213 \n",
      "     Training Step: 164 Training Loss: 0.610435426235199 \n",
      "     Training Step: 165 Training Loss: 0.6150419116020203 \n",
      "     Training Step: 166 Training Loss: 0.6142402291297913 \n",
      "     Training Step: 167 Training Loss: 0.6148557066917419 \n",
      "     Training Step: 168 Training Loss: 0.6163837313652039 \n",
      "     Training Step: 169 Training Loss: 0.612678050994873 \n",
      "     Training Step: 170 Training Loss: 0.6141635179519653 \n",
      "     Training Step: 171 Training Loss: 0.6153479218482971 \n",
      "     Training Step: 172 Training Loss: 0.6108136177062988 \n",
      "     Training Step: 173 Training Loss: 0.6169928312301636 \n",
      "     Training Step: 174 Training Loss: 0.614446759223938 \n",
      "     Training Step: 175 Training Loss: 0.6102960705757141 \n",
      "     Training Step: 176 Training Loss: 0.6170130372047424 \n",
      "     Training Step: 177 Training Loss: 0.6135709881782532 \n",
      "     Training Step: 178 Training Loss: 0.612314760684967 \n",
      "     Training Step: 179 Training Loss: 0.6139777898788452 \n",
      "     Training Step: 180 Training Loss: 0.6137036681175232 \n",
      "     Training Step: 181 Training Loss: 0.6144251823425293 \n",
      "     Training Step: 182 Training Loss: 0.6117264032363892 \n",
      "     Training Step: 183 Training Loss: 0.6153301000595093 \n",
      "     Training Step: 184 Training Loss: 0.6115179061889648 \n",
      "     Training Step: 185 Training Loss: 0.6181075572967529 \n",
      "     Training Step: 186 Training Loss: 0.6152487993240356 \n",
      "     Training Step: 187 Training Loss: 0.6168341040611267 \n",
      "     Training Step: 188 Training Loss: 0.6158059239387512 \n",
      "     Training Step: 189 Training Loss: 0.6148478984832764 \n",
      "     Training Step: 190 Training Loss: 0.6135473847389221 \n",
      "     Training Step: 191 Training Loss: 0.6145043969154358 \n",
      "     Training Step: 192 Training Loss: 0.6167012453079224 \n",
      "     Training Step: 193 Training Loss: 0.6123368144035339 \n",
      "     Training Step: 194 Training Loss: 0.6137226223945618 \n",
      "     Training Step: 195 Training Loss: 0.6119431257247925 \n",
      "     Training Step: 196 Training Loss: 0.6109681725502014 \n",
      "     Training Step: 197 Training Loss: 0.613970935344696 \n",
      "     Training Step: 198 Training Loss: 0.6183061003684998 \n",
      "     Training Step: 199 Training Loss: 0.6183596253395081 \n",
      "     Training Step: 200 Training Loss: 0.6137199997901917 \n",
      "     Training Step: 201 Training Loss: 0.6124312281608582 \n",
      "     Training Step: 202 Training Loss: 0.6120078563690186 \n",
      "     Training Step: 203 Training Loss: 0.6185460686683655 \n",
      "     Training Step: 204 Training Loss: 0.6118133664131165 \n",
      "     Training Step: 205 Training Loss: 0.6181555390357971 \n",
      "     Training Step: 206 Training Loss: 0.6150643825531006 \n",
      "     Training Step: 207 Training Loss: 0.6176919937133789 \n",
      "     Training Step: 208 Training Loss: 0.6154435276985168 \n",
      "     Training Step: 209 Training Loss: 0.616479218006134 \n",
      "     Training Step: 210 Training Loss: 0.6175079941749573 \n",
      "     Training Step: 211 Training Loss: 0.6123319268226624 \n",
      "     Training Step: 212 Training Loss: 0.6177308559417725 \n",
      "     Training Step: 213 Training Loss: 0.6124475598335266 \n",
      "     Training Step: 214 Training Loss: 0.6171696782112122 \n",
      "     Training Step: 215 Training Loss: 0.6157417893409729 \n",
      "     Training Step: 216 Training Loss: 0.6152059435844421 \n",
      "     Training Step: 217 Training Loss: 0.6121315360069275 \n",
      "     Training Step: 218 Training Loss: 0.6169023513793945 \n",
      "     Training Step: 219 Training Loss: 0.6147212386131287 \n",
      "     Training Step: 220 Training Loss: 0.6156376004219055 \n",
      "     Training Step: 221 Training Loss: 0.6133058667182922 \n",
      "     Training Step: 222 Training Loss: 0.6113440990447998 \n",
      "     Training Step: 223 Training Loss: 0.61578768491745 \n",
      "     Training Step: 224 Training Loss: 0.6168957948684692 \n",
      "     Training Step: 225 Training Loss: 0.6143107414245605 \n",
      "     Training Step: 226 Training Loss: 0.6086298823356628 \n",
      "     Training Step: 227 Training Loss: 0.6112602353096008 \n",
      "     Training Step: 228 Training Loss: 0.6172898411750793 \n",
      "     Training Step: 229 Training Loss: 0.6105937957763672 \n",
      "     Training Step: 230 Training Loss: 0.6148520112037659 \n",
      "     Training Step: 231 Training Loss: 0.6179073452949524 \n",
      "     Training Step: 232 Training Loss: 0.6107834577560425 \n",
      "     Training Step: 233 Training Loss: 0.6107887029647827 \n",
      "     Training Step: 234 Training Loss: 0.6128881573677063 \n",
      "     Training Step: 235 Training Loss: 0.6127189993858337 \n",
      "     Training Step: 236 Training Loss: 0.6151394248008728 \n",
      "     Training Step: 237 Training Loss: 0.6189829707145691 \n",
      "     Training Step: 238 Training Loss: 0.6126312613487244 \n",
      "     Training Step: 239 Training Loss: 0.6145999431610107 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6147188544273376 \n",
      "     Validation Step: 1 Validation Loss: 0.6183872818946838 \n",
      "     Validation Step: 2 Validation Loss: 0.6133952140808105 \n",
      "     Validation Step: 3 Validation Loss: 0.6151250004768372 \n",
      "     Validation Step: 4 Validation Loss: 0.617624044418335 \n",
      "     Validation Step: 5 Validation Loss: 0.6182783842086792 \n",
      "     Validation Step: 6 Validation Loss: 0.6143112778663635 \n",
      "     Validation Step: 7 Validation Loss: 0.6160802245140076 \n",
      "     Validation Step: 8 Validation Loss: 0.6163333058357239 \n",
      "     Validation Step: 9 Validation Loss: 0.6123034358024597 \n",
      "     Validation Step: 10 Validation Loss: 0.6103793978691101 \n",
      "     Validation Step: 11 Validation Loss: 0.618508517742157 \n",
      "     Validation Step: 12 Validation Loss: 0.6113188862800598 \n",
      "     Validation Step: 13 Validation Loss: 0.6143618226051331 \n",
      "     Validation Step: 14 Validation Loss: 0.6103641986846924 \n",
      "     Validation Step: 15 Validation Loss: 0.6146356463432312 \n",
      "     Validation Step: 16 Validation Loss: 0.6150179505348206 \n",
      "     Validation Step: 17 Validation Loss: 0.6154139637947083 \n",
      "     Validation Step: 18 Validation Loss: 0.6130956411361694 \n",
      "     Validation Step: 19 Validation Loss: 0.6106571555137634 \n",
      "     Validation Step: 20 Validation Loss: 0.6170747876167297 \n",
      "     Validation Step: 21 Validation Loss: 0.6177346110343933 \n",
      "     Validation Step: 22 Validation Loss: 0.614940881729126 \n",
      "     Validation Step: 23 Validation Loss: 0.6173583269119263 \n",
      "     Validation Step: 24 Validation Loss: 0.618516743183136 \n",
      "     Validation Step: 25 Validation Loss: 0.6103560924530029 \n",
      "     Validation Step: 26 Validation Loss: 0.6117308139801025 \n",
      "     Validation Step: 27 Validation Loss: 0.6153193116188049 \n",
      "     Validation Step: 28 Validation Loss: 0.6156939268112183 \n",
      "     Validation Step: 29 Validation Loss: 0.6129618883132935 \n",
      "     Validation Step: 30 Validation Loss: 0.6156259775161743 \n",
      "     Validation Step: 31 Validation Loss: 0.6108464002609253 \n",
      "     Validation Step: 32 Validation Loss: 0.613787829875946 \n",
      "     Validation Step: 33 Validation Loss: 0.6142330169677734 \n",
      "     Validation Step: 34 Validation Loss: 0.6181026101112366 \n",
      "     Validation Step: 35 Validation Loss: 0.6137563586235046 \n",
      "     Validation Step: 36 Validation Loss: 0.6120547652244568 \n",
      "     Validation Step: 37 Validation Loss: 0.6117924451828003 \n",
      "     Validation Step: 38 Validation Loss: 0.6107065677642822 \n",
      "     Validation Step: 39 Validation Loss: 0.6137307286262512 \n",
      "     Validation Step: 40 Validation Loss: 0.6158590912818909 \n",
      "     Validation Step: 41 Validation Loss: 0.6113373637199402 \n",
      "     Validation Step: 42 Validation Loss: 0.6078104972839355 \n",
      "     Validation Step: 43 Validation Loss: 0.6146575212478638 \n",
      "     Validation Step: 44 Validation Loss: 0.6142405271530151 \n",
      "Epoch: 8\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6150224804878235 \n",
      "     Training Step: 1 Training Loss: 0.6124593615531921 \n",
      "     Training Step: 2 Training Loss: 0.6169646978378296 \n",
      "     Training Step: 3 Training Loss: 0.6102925539016724 \n",
      "     Training Step: 4 Training Loss: 0.6105373501777649 \n",
      "     Training Step: 5 Training Loss: 0.615221381187439 \n",
      "     Training Step: 6 Training Loss: 0.6098403930664062 \n",
      "     Training Step: 7 Training Loss: 0.613621711730957 \n",
      "     Training Step: 8 Training Loss: 0.6148291826248169 \n",
      "     Training Step: 9 Training Loss: 0.6145943999290466 \n",
      "     Training Step: 10 Training Loss: 0.6156011819839478 \n",
      "     Training Step: 11 Training Loss: 0.6160812377929688 \n",
      "     Training Step: 12 Training Loss: 0.6139005422592163 \n",
      "     Training Step: 13 Training Loss: 0.6148083806037903 \n",
      "     Training Step: 14 Training Loss: 0.6152111291885376 \n",
      "     Training Step: 15 Training Loss: 0.6129792332649231 \n",
      "     Training Step: 16 Training Loss: 0.6123377084732056 \n",
      "     Training Step: 17 Training Loss: 0.6158635020256042 \n",
      "     Training Step: 18 Training Loss: 0.619777262210846 \n",
      "     Training Step: 19 Training Loss: 0.6165929436683655 \n",
      "     Training Step: 20 Training Loss: 0.6124693751335144 \n",
      "     Training Step: 21 Training Loss: 0.6142242550849915 \n",
      "     Training Step: 22 Training Loss: 0.6120395064353943 \n",
      "     Training Step: 23 Training Loss: 0.6177312135696411 \n",
      "     Training Step: 24 Training Loss: 0.6132818460464478 \n",
      "     Training Step: 25 Training Loss: 0.6096826791763306 \n",
      "     Training Step: 26 Training Loss: 0.6148374676704407 \n",
      "     Training Step: 27 Training Loss: 0.6147083640098572 \n",
      "     Training Step: 28 Training Loss: 0.6119173765182495 \n",
      "     Training Step: 29 Training Loss: 0.6115082502365112 \n",
      "     Training Step: 30 Training Loss: 0.6163947582244873 \n",
      "     Training Step: 31 Training Loss: 0.6139687299728394 \n",
      "     Training Step: 32 Training Loss: 0.6100277304649353 \n",
      "     Training Step: 33 Training Loss: 0.615344762802124 \n",
      "     Training Step: 34 Training Loss: 0.6155318021774292 \n",
      "     Training Step: 35 Training Loss: 0.610633134841919 \n",
      "     Training Step: 36 Training Loss: 0.6149986386299133 \n",
      "     Training Step: 37 Training Loss: 0.6145154237747192 \n",
      "     Training Step: 38 Training Loss: 0.6129119396209717 \n",
      "     Training Step: 39 Training Loss: 0.6095504760742188 \n",
      "     Training Step: 40 Training Loss: 0.6114885210990906 \n",
      "     Training Step: 41 Training Loss: 0.6186484694480896 \n",
      "     Training Step: 42 Training Loss: 0.6117724180221558 \n",
      "     Training Step: 43 Training Loss: 0.6169949173927307 \n",
      "     Training Step: 44 Training Loss: 0.6166984438896179 \n",
      "     Training Step: 45 Training Loss: 0.6139397025108337 \n",
      "     Training Step: 46 Training Loss: 0.6171584725379944 \n",
      "     Training Step: 47 Training Loss: 0.6127170324325562 \n",
      "     Training Step: 48 Training Loss: 0.6117098927497864 \n",
      "     Training Step: 49 Training Loss: 0.6143770813941956 \n",
      "     Training Step: 50 Training Loss: 0.6119083166122437 \n",
      "     Training Step: 51 Training Loss: 0.6134312152862549 \n",
      "     Training Step: 52 Training Loss: 0.6138871312141418 \n",
      "     Training Step: 53 Training Loss: 0.6118803024291992 \n",
      "     Training Step: 54 Training Loss: 0.6179656982421875 \n",
      "     Training Step: 55 Training Loss: 0.6117641925811768 \n",
      "     Training Step: 56 Training Loss: 0.6133942008018494 \n",
      "     Training Step: 57 Training Loss: 0.6142747402191162 \n",
      "     Training Step: 58 Training Loss: 0.6184676885604858 \n",
      "     Training Step: 59 Training Loss: 0.6158124804496765 \n",
      "     Training Step: 60 Training Loss: 0.612530529499054 \n",
      "     Training Step: 61 Training Loss: 0.6153808832168579 \n",
      "     Training Step: 62 Training Loss: 0.6178101897239685 \n",
      "     Training Step: 63 Training Loss: 0.6173974275588989 \n",
      "     Training Step: 64 Training Loss: 0.6196888089179993 \n",
      "     Training Step: 65 Training Loss: 0.6131827235221863 \n",
      "     Training Step: 66 Training Loss: 0.6201878786087036 \n",
      "     Training Step: 67 Training Loss: 0.6108071804046631 \n",
      "     Training Step: 68 Training Loss: 0.6108599901199341 \n",
      "     Training Step: 69 Training Loss: 0.6189062595367432 \n",
      "     Training Step: 70 Training Loss: 0.6116955876350403 \n",
      "     Training Step: 71 Training Loss: 0.6148070693016052 \n",
      "     Training Step: 72 Training Loss: 0.6155163049697876 \n",
      "     Training Step: 73 Training Loss: 0.6156607270240784 \n",
      "     Training Step: 74 Training Loss: 0.6142155528068542 \n",
      "     Training Step: 75 Training Loss: 0.6162876486778259 \n",
      "     Training Step: 76 Training Loss: 0.6188929080963135 \n",
      "     Training Step: 77 Training Loss: 0.6114840507507324 \n",
      "     Training Step: 78 Training Loss: 0.6209054589271545 \n",
      "     Training Step: 79 Training Loss: 0.616484522819519 \n",
      "     Training Step: 80 Training Loss: 0.6148344874382019 \n",
      "     Training Step: 81 Training Loss: 0.6159078478813171 \n",
      "     Training Step: 82 Training Loss: 0.616771399974823 \n",
      "     Training Step: 83 Training Loss: 0.61161869764328 \n",
      "     Training Step: 84 Training Loss: 0.6102132201194763 \n",
      "     Training Step: 85 Training Loss: 0.6147950887680054 \n",
      "     Training Step: 86 Training Loss: 0.6101255416870117 \n",
      "     Training Step: 87 Training Loss: 0.6105905175209045 \n",
      "     Training Step: 88 Training Loss: 0.6169294118881226 \n",
      "     Training Step: 89 Training Loss: 0.6154038310050964 \n",
      "     Training Step: 90 Training Loss: 0.6158372163772583 \n",
      "     Training Step: 91 Training Loss: 0.6129541993141174 \n",
      "     Training Step: 92 Training Loss: 0.6161901354789734 \n",
      "     Training Step: 93 Training Loss: 0.615368127822876 \n",
      "     Training Step: 94 Training Loss: 0.6108537316322327 \n",
      "     Training Step: 95 Training Loss: 0.6101508736610413 \n",
      "     Training Step: 96 Training Loss: 0.6169895529747009 \n",
      "     Training Step: 97 Training Loss: 0.610755980014801 \n",
      "     Training Step: 98 Training Loss: 0.6137877702713013 \n",
      "     Training Step: 99 Training Loss: 0.6126649975776672 \n",
      "     Training Step: 100 Training Loss: 0.6125462651252747 \n",
      "     Training Step: 101 Training Loss: 0.6157869100570679 \n",
      "     Training Step: 102 Training Loss: 0.6137162446975708 \n",
      "     Training Step: 103 Training Loss: 0.613521158695221 \n",
      "     Training Step: 104 Training Loss: 0.6122766733169556 \n",
      "     Training Step: 105 Training Loss: 0.6129058599472046 \n",
      "     Training Step: 106 Training Loss: 0.6117474436759949 \n",
      "     Training Step: 107 Training Loss: 0.6145740747451782 \n",
      "     Training Step: 108 Training Loss: 0.6169026494026184 \n",
      "     Training Step: 109 Training Loss: 0.611745297908783 \n",
      "     Training Step: 110 Training Loss: 0.6111024022102356 \n",
      "     Training Step: 111 Training Loss: 0.6150371432304382 \n",
      "     Training Step: 112 Training Loss: 0.6127369999885559 \n",
      "     Training Step: 113 Training Loss: 0.6122380495071411 \n",
      "     Training Step: 114 Training Loss: 0.61232590675354 \n",
      "     Training Step: 115 Training Loss: 0.6142817735671997 \n",
      "     Training Step: 116 Training Loss: 0.6138715147972107 \n",
      "     Training Step: 117 Training Loss: 0.614837110042572 \n",
      "     Training Step: 118 Training Loss: 0.6122440695762634 \n",
      "     Training Step: 119 Training Loss: 0.6137188076972961 \n",
      "     Training Step: 120 Training Loss: 0.6133698225021362 \n",
      "     Training Step: 121 Training Loss: 0.6165491938591003 \n",
      "     Training Step: 122 Training Loss: 0.6167574524879456 \n",
      "     Training Step: 123 Training Loss: 0.6155839562416077 \n",
      "     Training Step: 124 Training Loss: 0.6114873886108398 \n",
      "     Training Step: 125 Training Loss: 0.6115821003913879 \n",
      "     Training Step: 126 Training Loss: 0.6174971461296082 \n",
      "     Training Step: 127 Training Loss: 0.6129813194274902 \n",
      "     Training Step: 128 Training Loss: 0.6123305559158325 \n",
      "     Training Step: 129 Training Loss: 0.6144489645957947 \n",
      "     Training Step: 130 Training Loss: 0.6132006645202637 \n",
      "     Training Step: 131 Training Loss: 0.6172068119049072 \n",
      "     Training Step: 132 Training Loss: 0.6168487071990967 \n",
      "     Training Step: 133 Training Loss: 0.6154379844665527 \n",
      "     Training Step: 134 Training Loss: 0.6169352531433105 \n",
      "     Training Step: 135 Training Loss: 0.6134620904922485 \n",
      "     Training Step: 136 Training Loss: 0.6125246286392212 \n",
      "     Training Step: 137 Training Loss: 0.6126171350479126 \n",
      "     Training Step: 138 Training Loss: 0.6147182583808899 \n",
      "     Training Step: 139 Training Loss: 0.613192617893219 \n",
      "     Training Step: 140 Training Loss: 0.6134931445121765 \n",
      "     Training Step: 141 Training Loss: 0.6119388341903687 \n",
      "     Training Step: 142 Training Loss: 0.6164669990539551 \n",
      "     Training Step: 143 Training Loss: 0.6178315877914429 \n",
      "     Training Step: 144 Training Loss: 0.6108399033546448 \n",
      "     Training Step: 145 Training Loss: 0.6186227798461914 \n",
      "     Training Step: 146 Training Loss: 0.6118019223213196 \n",
      "     Training Step: 147 Training Loss: 0.6116843223571777 \n",
      "     Training Step: 148 Training Loss: 0.6116764545440674 \n",
      "     Training Step: 149 Training Loss: 0.618631899356842 \n",
      "     Training Step: 150 Training Loss: 0.6146851778030396 \n",
      "     Training Step: 151 Training Loss: 0.6106675267219543 \n",
      "     Training Step: 152 Training Loss: 0.6200675964355469 \n",
      "     Training Step: 153 Training Loss: 0.6151909232139587 \n",
      "     Training Step: 154 Training Loss: 0.6116787195205688 \n",
      "     Training Step: 155 Training Loss: 0.6161505579948425 \n",
      "     Training Step: 156 Training Loss: 0.6183211207389832 \n",
      "     Training Step: 157 Training Loss: 0.6155359745025635 \n",
      "     Training Step: 158 Training Loss: 0.6119829416275024 \n",
      "     Training Step: 159 Training Loss: 0.6160331964492798 \n",
      "     Training Step: 160 Training Loss: 0.6140305399894714 \n",
      "     Training Step: 161 Training Loss: 0.6155503392219543 \n",
      "     Training Step: 162 Training Loss: 0.6135087013244629 \n",
      "     Training Step: 163 Training Loss: 0.6153815984725952 \n",
      "     Training Step: 164 Training Loss: 0.6147940754890442 \n",
      "     Training Step: 165 Training Loss: 0.6130649447441101 \n",
      "     Training Step: 166 Training Loss: 0.6162542700767517 \n",
      "     Training Step: 167 Training Loss: 0.6167146563529968 \n",
      "     Training Step: 168 Training Loss: 0.6122148036956787 \n",
      "     Training Step: 169 Training Loss: 0.6132458448410034 \n",
      "     Training Step: 170 Training Loss: 0.6093852519989014 \n",
      "     Training Step: 171 Training Loss: 0.6116370558738708 \n",
      "     Training Step: 172 Training Loss: 0.6146451234817505 \n",
      "     Training Step: 173 Training Loss: 0.6121509671211243 \n",
      "     Training Step: 174 Training Loss: 0.6149319410324097 \n",
      "     Training Step: 175 Training Loss: 0.6136704683303833 \n",
      "     Training Step: 176 Training Loss: 0.6108357906341553 \n",
      "     Training Step: 177 Training Loss: 0.614192545413971 \n",
      "     Training Step: 178 Training Loss: 0.6177272200584412 \n",
      "     Training Step: 179 Training Loss: 0.6085607409477234 \n",
      "     Training Step: 180 Training Loss: 0.612472414970398 \n",
      "     Training Step: 181 Training Loss: 0.6173795461654663 \n",
      "     Training Step: 182 Training Loss: 0.6175978183746338 \n",
      "     Training Step: 183 Training Loss: 0.6103538274765015 \n",
      "     Training Step: 184 Training Loss: 0.6110100150108337 \n",
      "     Training Step: 185 Training Loss: 0.6196378469467163 \n",
      "     Training Step: 186 Training Loss: 0.6149599552154541 \n",
      "     Training Step: 187 Training Loss: 0.6143914461135864 \n",
      "     Training Step: 188 Training Loss: 0.6188343167304993 \n",
      "     Training Step: 189 Training Loss: 0.6137542128562927 \n",
      "     Training Step: 190 Training Loss: 0.6152693033218384 \n",
      "     Training Step: 191 Training Loss: 0.6158506870269775 \n",
      "     Training Step: 192 Training Loss: 0.6143997311592102 \n",
      "     Training Step: 193 Training Loss: 0.615777313709259 \n",
      "     Training Step: 194 Training Loss: 0.6140844821929932 \n",
      "     Training Step: 195 Training Loss: 0.6142007112503052 \n",
      "     Training Step: 196 Training Loss: 0.6154928207397461 \n",
      "     Training Step: 197 Training Loss: 0.6156561970710754 \n",
      "     Training Step: 198 Training Loss: 0.6182200312614441 \n",
      "     Training Step: 199 Training Loss: 0.6168507933616638 \n",
      "     Training Step: 200 Training Loss: 0.6131239533424377 \n",
      "     Training Step: 201 Training Loss: 0.6144198179244995 \n",
      "     Training Step: 202 Training Loss: 0.6167927384376526 \n",
      "     Training Step: 203 Training Loss: 0.6119908690452576 \n",
      "     Training Step: 204 Training Loss: 0.6171748042106628 \n",
      "     Training Step: 205 Training Loss: 0.6141466498374939 \n",
      "     Training Step: 206 Training Loss: 0.6133562922477722 \n",
      "     Training Step: 207 Training Loss: 0.6147048473358154 \n",
      "     Training Step: 208 Training Loss: 0.6147476434707642 \n",
      "     Training Step: 209 Training Loss: 0.617812991142273 \n",
      "     Training Step: 210 Training Loss: 0.6136159300804138 \n",
      "     Training Step: 211 Training Loss: 0.6144692897796631 \n",
      "     Training Step: 212 Training Loss: 0.6106583476066589 \n",
      "     Training Step: 213 Training Loss: 0.6155907511711121 \n",
      "     Training Step: 214 Training Loss: 0.6181598901748657 \n",
      "     Training Step: 215 Training Loss: 0.6098598837852478 \n",
      "     Training Step: 216 Training Loss: 0.6128700971603394 \n",
      "     Training Step: 217 Training Loss: 0.6185676455497742 \n",
      "     Training Step: 218 Training Loss: 0.6167646646499634 \n",
      "     Training Step: 219 Training Loss: 0.6128007769584656 \n",
      "     Training Step: 220 Training Loss: 0.6116383671760559 \n",
      "     Training Step: 221 Training Loss: 0.6182463765144348 \n",
      "     Training Step: 222 Training Loss: 0.6103388071060181 \n",
      "     Training Step: 223 Training Loss: 0.6181081533432007 \n",
      "     Training Step: 224 Training Loss: 0.6145420074462891 \n",
      "     Training Step: 225 Training Loss: 0.615547776222229 \n",
      "     Training Step: 226 Training Loss: 0.6147060394287109 \n",
      "     Training Step: 227 Training Loss: 0.6108997464179993 \n",
      "     Training Step: 228 Training Loss: 0.6112440228462219 \n",
      "     Training Step: 229 Training Loss: 0.6182000041007996 \n",
      "     Training Step: 230 Training Loss: 0.6178321242332458 \n",
      "     Training Step: 231 Training Loss: 0.616681694984436 \n",
      "     Training Step: 232 Training Loss: 0.6133871674537659 \n",
      "     Training Step: 233 Training Loss: 0.6153088212013245 \n",
      "     Training Step: 234 Training Loss: 0.6132798194885254 \n",
      "     Training Step: 235 Training Loss: 0.6133707761764526 \n",
      "     Training Step: 236 Training Loss: 0.612695574760437 \n",
      "     Training Step: 237 Training Loss: 0.6141090393066406 \n",
      "     Training Step: 238 Training Loss: 0.6122732162475586 \n",
      "     Training Step: 239 Training Loss: 0.6145579814910889 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6149511933326721 \n",
      "     Validation Step: 1 Validation Loss: 0.6146893501281738 \n",
      "     Validation Step: 2 Validation Loss: 0.6076760292053223 \n",
      "     Validation Step: 3 Validation Loss: 0.6116762757301331 \n",
      "     Validation Step: 4 Validation Loss: 0.6171460747718811 \n",
      "     Validation Step: 5 Validation Loss: 0.6102538108825684 \n",
      "     Validation Step: 6 Validation Loss: 0.6174390316009521 \n",
      "     Validation Step: 7 Validation Loss: 0.615730881690979 \n",
      "     Validation Step: 8 Validation Loss: 0.610283613204956 \n",
      "     Validation Step: 9 Validation Loss: 0.6112816333770752 \n",
      "     Validation Step: 10 Validation Loss: 0.6183647513389587 \n",
      "     Validation Step: 11 Validation Loss: 0.6143127679824829 \n",
      "     Validation Step: 12 Validation Loss: 0.6103053092956543 \n",
      "     Validation Step: 13 Validation Loss: 0.6151387095451355 \n",
      "     Validation Step: 14 Validation Loss: 0.6154255270957947 \n",
      "     Validation Step: 15 Validation Loss: 0.611757755279541 \n",
      "     Validation Step: 16 Validation Loss: 0.6181942224502563 \n",
      "     Validation Step: 17 Validation Loss: 0.6137708425521851 \n",
      "     Validation Step: 18 Validation Loss: 0.614371657371521 \n",
      "     Validation Step: 19 Validation Loss: 0.6137400269508362 \n",
      "     Validation Step: 20 Validation Loss: 0.6177152991294861 \n",
      "     Validation Step: 21 Validation Loss: 0.6161206364631653 \n",
      "     Validation Step: 22 Validation Loss: 0.6156736016273499 \n",
      "     Validation Step: 23 Validation Loss: 0.6120128631591797 \n",
      "     Validation Step: 24 Validation Loss: 0.6106282472610474 \n",
      "     Validation Step: 25 Validation Loss: 0.6163700222969055 \n",
      "     Validation Step: 26 Validation Loss: 0.6184622049331665 \n",
      "     Validation Step: 27 Validation Loss: 0.6153427362442017 \n",
      "     Validation Step: 28 Validation Loss: 0.614736020565033 \n",
      "     Validation Step: 29 Validation Loss: 0.6150311827659607 \n",
      "     Validation Step: 30 Validation Loss: 0.6122549772262573 \n",
      "     Validation Step: 31 Validation Loss: 0.6142212748527527 \n",
      "     Validation Step: 32 Validation Loss: 0.6112838387489319 \n",
      "     Validation Step: 33 Validation Loss: 0.6178131103515625 \n",
      "     Validation Step: 34 Validation Loss: 0.6186262369155884 \n",
      "     Validation Step: 35 Validation Loss: 0.6137536764144897 \n",
      "     Validation Step: 36 Validation Loss: 0.6146437525749207 \n",
      "     Validation Step: 37 Validation Loss: 0.6107879877090454 \n",
      "     Validation Step: 38 Validation Loss: 0.6159147620201111 \n",
      "     Validation Step: 39 Validation Loss: 0.6105918884277344 \n",
      "     Validation Step: 40 Validation Loss: 0.6129326224327087 \n",
      "     Validation Step: 41 Validation Loss: 0.6186145544052124 \n",
      "     Validation Step: 42 Validation Loss: 0.6133927702903748 \n",
      "     Validation Step: 43 Validation Loss: 0.6142433285713196 \n",
      "     Validation Step: 44 Validation Loss: 0.6130785346031189 \n",
      "Epoch: 9\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6141537427902222 \n",
      "     Training Step: 1 Training Loss: 0.6156113147735596 \n",
      "     Training Step: 2 Training Loss: 0.6142321228981018 \n",
      "     Training Step: 3 Training Loss: 0.6146963834762573 \n",
      "     Training Step: 4 Training Loss: 0.6144004464149475 \n",
      "     Training Step: 5 Training Loss: 0.6158050298690796 \n",
      "     Training Step: 6 Training Loss: 0.6129725575447083 \n",
      "     Training Step: 7 Training Loss: 0.6130024790763855 \n",
      "     Training Step: 8 Training Loss: 0.6102015376091003 \n",
      "     Training Step: 9 Training Loss: 0.6141239404678345 \n",
      "     Training Step: 10 Training Loss: 0.6159440875053406 \n",
      "     Training Step: 11 Training Loss: 0.6097608804702759 \n",
      "     Training Step: 12 Training Loss: 0.6161036491394043 \n",
      "     Training Step: 13 Training Loss: 0.6148520112037659 \n",
      "     Training Step: 14 Training Loss: 0.6113594174385071 \n",
      "     Training Step: 15 Training Loss: 0.6171971559524536 \n",
      "     Training Step: 16 Training Loss: 0.6153420805931091 \n",
      "     Training Step: 17 Training Loss: 0.6122846007347107 \n",
      "     Training Step: 18 Training Loss: 0.6184474229812622 \n",
      "     Training Step: 19 Training Loss: 0.6108596920967102 \n",
      "     Training Step: 20 Training Loss: 0.6145152449607849 \n",
      "     Training Step: 21 Training Loss: 0.6142155528068542 \n",
      "     Training Step: 22 Training Loss: 0.6201533675193787 \n",
      "     Training Step: 23 Training Loss: 0.6132097840309143 \n",
      "     Training Step: 24 Training Loss: 0.610632598400116 \n",
      "     Training Step: 25 Training Loss: 0.6124643683433533 \n",
      "     Training Step: 26 Training Loss: 0.6146016716957092 \n",
      "     Training Step: 27 Training Loss: 0.615394651889801 \n",
      "     Training Step: 28 Training Loss: 0.6181217432022095 \n",
      "     Training Step: 29 Training Loss: 0.6128808856010437 \n",
      "     Training Step: 30 Training Loss: 0.6118202805519104 \n",
      "     Training Step: 31 Training Loss: 0.6137444376945496 \n",
      "     Training Step: 32 Training Loss: 0.6142812967300415 \n",
      "     Training Step: 33 Training Loss: 0.6127840876579285 \n",
      "     Training Step: 34 Training Loss: 0.6116114258766174 \n",
      "     Training Step: 35 Training Loss: 0.6139673590660095 \n",
      "     Training Step: 36 Training Loss: 0.6172161102294922 \n",
      "     Training Step: 37 Training Loss: 0.6195278167724609 \n",
      "     Training Step: 38 Training Loss: 0.6108036637306213 \n",
      "     Training Step: 39 Training Loss: 0.6122062802314758 \n",
      "     Training Step: 40 Training Loss: 0.6135979294776917 \n",
      "     Training Step: 41 Training Loss: 0.6119048595428467 \n",
      "     Training Step: 42 Training Loss: 0.6165419220924377 \n",
      "     Training Step: 43 Training Loss: 0.6169869303703308 \n",
      "     Training Step: 44 Training Loss: 0.6156131625175476 \n",
      "     Training Step: 45 Training Loss: 0.6147853136062622 \n",
      "     Training Step: 46 Training Loss: 0.6164703965187073 \n",
      "     Training Step: 47 Training Loss: 0.6141311526298523 \n",
      "     Training Step: 48 Training Loss: 0.6156670451164246 \n",
      "     Training Step: 49 Training Loss: 0.6141539812088013 \n",
      "     Training Step: 50 Training Loss: 0.6101846098899841 \n",
      "     Training Step: 51 Training Loss: 0.6107646822929382 \n",
      "     Training Step: 52 Training Loss: 0.6106429696083069 \n",
      "     Training Step: 53 Training Loss: 0.6155539155006409 \n",
      "     Training Step: 54 Training Loss: 0.613800585269928 \n",
      "     Training Step: 55 Training Loss: 0.6118898987770081 \n",
      "     Training Step: 56 Training Loss: 0.6119148135185242 \n",
      "     Training Step: 57 Training Loss: 0.6145049929618835 \n",
      "     Training Step: 58 Training Loss: 0.61069655418396 \n",
      "     Training Step: 59 Training Loss: 0.6152029037475586 \n",
      "     Training Step: 60 Training Loss: 0.6142838597297668 \n",
      "     Training Step: 61 Training Loss: 0.6133142709732056 \n",
      "     Training Step: 62 Training Loss: 0.6172154545783997 \n",
      "     Training Step: 63 Training Loss: 0.6147890686988831 \n",
      "     Training Step: 64 Training Loss: 0.6151195168495178 \n",
      "     Training Step: 65 Training Loss: 0.6165224313735962 \n",
      "     Training Step: 66 Training Loss: 0.6105082035064697 \n",
      "     Training Step: 67 Training Loss: 0.6134593486785889 \n",
      "     Training Step: 68 Training Loss: 0.6178867816925049 \n",
      "     Training Step: 69 Training Loss: 0.611679196357727 \n",
      "     Training Step: 70 Training Loss: 0.6101552844047546 \n",
      "     Training Step: 71 Training Loss: 0.6161192655563354 \n",
      "     Training Step: 72 Training Loss: 0.613788902759552 \n",
      "     Training Step: 73 Training Loss: 0.6118623614311218 \n",
      "     Training Step: 74 Training Loss: 0.6158832311630249 \n",
      "     Training Step: 75 Training Loss: 0.6103652119636536 \n",
      "     Training Step: 76 Training Loss: 0.6166857481002808 \n",
      "     Training Step: 77 Training Loss: 0.6150971055030823 \n",
      "     Training Step: 78 Training Loss: 0.61285799741745 \n",
      "     Training Step: 79 Training Loss: 0.6125370860099792 \n",
      "     Training Step: 80 Training Loss: 0.6133280992507935 \n",
      "     Training Step: 81 Training Loss: 0.6126939654350281 \n",
      "     Training Step: 82 Training Loss: 0.6138566732406616 \n",
      "     Training Step: 83 Training Loss: 0.6147143840789795 \n",
      "     Training Step: 84 Training Loss: 0.6149780750274658 \n",
      "     Training Step: 85 Training Loss: 0.612292468547821 \n",
      "     Training Step: 86 Training Loss: 0.6112489700317383 \n",
      "     Training Step: 87 Training Loss: 0.6132341623306274 \n",
      "     Training Step: 88 Training Loss: 0.6083864569664001 \n",
      "     Training Step: 89 Training Loss: 0.620003342628479 \n",
      "     Training Step: 90 Training Loss: 0.6117212176322937 \n",
      "     Training Step: 91 Training Loss: 0.6178284883499146 \n",
      "     Training Step: 92 Training Loss: 0.614755392074585 \n",
      "     Training Step: 93 Training Loss: 0.6132636070251465 \n",
      "     Training Step: 94 Training Loss: 0.6124230027198792 \n",
      "     Training Step: 95 Training Loss: 0.6124922037124634 \n",
      "     Training Step: 96 Training Loss: 0.6179001331329346 \n",
      "     Training Step: 97 Training Loss: 0.6183176636695862 \n",
      "     Training Step: 98 Training Loss: 0.6097223162651062 \n",
      "     Training Step: 99 Training Loss: 0.6168556213378906 \n",
      "     Training Step: 100 Training Loss: 0.6177404522895813 \n",
      "     Training Step: 101 Training Loss: 0.6143520474433899 \n",
      "     Training Step: 102 Training Loss: 0.6146918535232544 \n",
      "     Training Step: 103 Training Loss: 0.6176534295082092 \n",
      "     Training Step: 104 Training Loss: 0.6155042052268982 \n",
      "     Training Step: 105 Training Loss: 0.6143990755081177 \n",
      "     Training Step: 106 Training Loss: 0.6153672933578491 \n",
      "     Training Step: 107 Training Loss: 0.6107360124588013 \n",
      "     Training Step: 108 Training Loss: 0.6187168955802917 \n",
      "     Training Step: 109 Training Loss: 0.6169288158416748 \n",
      "     Training Step: 110 Training Loss: 0.6116129159927368 \n",
      "     Training Step: 111 Training Loss: 0.6123800873756409 \n",
      "     Training Step: 112 Training Loss: 0.6141813397407532 \n",
      "     Training Step: 113 Training Loss: 0.6154882907867432 \n",
      "     Training Step: 114 Training Loss: 0.6148229837417603 \n",
      "     Training Step: 115 Training Loss: 0.6133763790130615 \n",
      "     Training Step: 116 Training Loss: 0.6147351861000061 \n",
      "     Training Step: 117 Training Loss: 0.615594744682312 \n",
      "     Training Step: 118 Training Loss: 0.6139976382255554 \n",
      "     Training Step: 119 Training Loss: 0.6122881174087524 \n",
      "     Training Step: 120 Training Loss: 0.6153264045715332 \n",
      "     Training Step: 121 Training Loss: 0.6187492609024048 \n",
      "     Training Step: 122 Training Loss: 0.6131448149681091 \n",
      "     Training Step: 123 Training Loss: 0.6117198467254639 \n",
      "     Training Step: 124 Training Loss: 0.6112776398658752 \n",
      "     Training Step: 125 Training Loss: 0.6149569153785706 \n",
      "     Training Step: 126 Training Loss: 0.6168994307518005 \n",
      "     Training Step: 127 Training Loss: 0.6177839636802673 \n",
      "     Training Step: 128 Training Loss: 0.6117445230484009 \n",
      "     Training Step: 129 Training Loss: 0.6157205104827881 \n",
      "     Training Step: 130 Training Loss: 0.6180806756019592 \n",
      "     Training Step: 131 Training Loss: 0.6139190793037415 \n",
      "     Training Step: 132 Training Loss: 0.6180700659751892 \n",
      "     Training Step: 133 Training Loss: 0.613004744052887 \n",
      "     Training Step: 134 Training Loss: 0.6120728850364685 \n",
      "     Training Step: 135 Training Loss: 0.613547146320343 \n",
      "     Training Step: 136 Training Loss: 0.6119081974029541 \n",
      "     Training Step: 137 Training Loss: 0.6145492196083069 \n",
      "     Training Step: 138 Training Loss: 0.6168826818466187 \n",
      "     Training Step: 139 Training Loss: 0.6133894324302673 \n",
      "     Training Step: 140 Training Loss: 0.6136077642440796 \n",
      "     Training Step: 141 Training Loss: 0.6115038394927979 \n",
      "     Training Step: 142 Training Loss: 0.6094197630882263 \n",
      "     Training Step: 143 Training Loss: 0.6148218512535095 \n",
      "     Training Step: 144 Training Loss: 0.6144407987594604 \n",
      "     Training Step: 145 Training Loss: 0.6163379549980164 \n",
      "     Training Step: 146 Training Loss: 0.6172788739204407 \n",
      "     Training Step: 147 Training Loss: 0.6178678274154663 \n",
      "     Training Step: 148 Training Loss: 0.6118694543838501 \n",
      "     Training Step: 149 Training Loss: 0.6127229332923889 \n",
      "     Training Step: 150 Training Loss: 0.6168816089630127 \n",
      "     Training Step: 151 Training Loss: 0.6148369312286377 \n",
      "     Training Step: 152 Training Loss: 0.6110576391220093 \n",
      "     Training Step: 153 Training Loss: 0.6156895160675049 \n",
      "     Training Step: 154 Training Loss: 0.6099288463592529 \n",
      "     Training Step: 155 Training Loss: 0.615254819393158 \n",
      "     Training Step: 156 Training Loss: 0.6110036373138428 \n",
      "     Training Step: 157 Training Loss: 0.6134132146835327 \n",
      "     Training Step: 158 Training Loss: 0.6169493794441223 \n",
      "     Training Step: 159 Training Loss: 0.6150042414665222 \n",
      "     Training Step: 160 Training Loss: 0.6161898374557495 \n",
      "     Training Step: 161 Training Loss: 0.6172301769256592 \n",
      "     Training Step: 162 Training Loss: 0.612705409526825 \n",
      "     Training Step: 163 Training Loss: 0.6131520867347717 \n",
      "     Training Step: 164 Training Loss: 0.6109206080436707 \n",
      "     Training Step: 165 Training Loss: 0.6169532537460327 \n",
      "     Training Step: 166 Training Loss: 0.6190295815467834 \n",
      "     Training Step: 167 Training Loss: 0.6187337636947632 \n",
      "     Training Step: 168 Training Loss: 0.616136908531189 \n",
      "     Training Step: 169 Training Loss: 0.6153756380081177 \n",
      "     Training Step: 170 Training Loss: 0.6184977293014526 \n",
      "     Training Step: 171 Training Loss: 0.6111212968826294 \n",
      "     Training Step: 172 Training Loss: 0.6167692542076111 \n",
      "     Training Step: 173 Training Loss: 0.6132219433784485 \n",
      "     Training Step: 174 Training Loss: 0.6148203611373901 \n",
      "     Training Step: 175 Training Loss: 0.6183916330337524 \n",
      "     Training Step: 176 Training Loss: 0.6129478216171265 \n",
      "     Training Step: 177 Training Loss: 0.6163588166236877 \n",
      "     Training Step: 178 Training Loss: 0.6130291819572449 \n",
      "     Training Step: 179 Training Loss: 0.6148704290390015 \n",
      "     Training Step: 180 Training Loss: 0.6145161986351013 \n",
      "     Training Step: 181 Training Loss: 0.6196601390838623 \n",
      "     Training Step: 182 Training Loss: 0.6148767471313477 \n",
      "     Training Step: 183 Training Loss: 0.615501880645752 \n",
      "     Training Step: 184 Training Loss: 0.6138486862182617 \n",
      "     Training Step: 185 Training Loss: 0.6158142685890198 \n",
      "     Training Step: 186 Training Loss: 0.6115153431892395 \n",
      "     Training Step: 187 Training Loss: 0.6182922720909119 \n",
      "     Training Step: 188 Training Loss: 0.6168955564498901 \n",
      "     Training Step: 189 Training Loss: 0.6116912364959717 \n",
      "     Training Step: 190 Training Loss: 0.6189669370651245 \n",
      "     Training Step: 191 Training Loss: 0.6126775741577148 \n",
      "     Training Step: 192 Training Loss: 0.6147139668464661 \n",
      "     Training Step: 193 Training Loss: 0.6133362054824829 \n",
      "     Training Step: 194 Training Loss: 0.6163347363471985 \n",
      "     Training Step: 195 Training Loss: 0.6123450994491577 \n",
      "     Training Step: 196 Training Loss: 0.6134437322616577 \n",
      "     Training Step: 197 Training Loss: 0.6102274060249329 \n",
      "     Training Step: 198 Training Loss: 0.6139290928840637 \n",
      "     Training Step: 199 Training Loss: 0.6115562319755554 \n",
      "     Training Step: 200 Training Loss: 0.6170056462287903 \n",
      "     Training Step: 201 Training Loss: 0.616790235042572 \n",
      "     Training Step: 202 Training Loss: 0.6130877137184143 \n",
      "     Training Step: 203 Training Loss: 0.6120705604553223 \n",
      "     Training Step: 204 Training Loss: 0.6100150942802429 \n",
      "     Training Step: 205 Training Loss: 0.6137236952781677 \n",
      "     Training Step: 206 Training Loss: 0.6187549829483032 \n",
      "     Training Step: 207 Training Loss: 0.6103532910346985 \n",
      "     Training Step: 208 Training Loss: 0.6096131205558777 \n",
      "     Training Step: 209 Training Loss: 0.6118155121803284 \n",
      "     Training Step: 210 Training Loss: 0.6123046875 \n",
      "     Training Step: 211 Training Loss: 0.6126857995986938 \n",
      "     Training Step: 212 Training Loss: 0.6156128644943237 \n",
      "     Training Step: 213 Training Loss: 0.6175211071968079 \n",
      "     Training Step: 214 Training Loss: 0.6144163012504578 \n",
      "     Training Step: 215 Training Loss: 0.620235800743103 \n",
      "     Training Step: 216 Training Loss: 0.6170321106910706 \n",
      "     Training Step: 217 Training Loss: 0.6125454902648926 \n",
      "     Training Step: 218 Training Loss: 0.6118683218955994 \n",
      "     Training Step: 219 Training Loss: 0.610779345035553 \n",
      "     Training Step: 220 Training Loss: 0.6115211248397827 \n",
      "     Training Step: 221 Training Loss: 0.6149157285690308 \n",
      "     Training Step: 222 Training Loss: 0.6166303753852844 \n",
      "     Training Step: 223 Training Loss: 0.6214213371276855 \n",
      "     Training Step: 224 Training Loss: 0.6156898736953735 \n",
      "     Training Step: 225 Training Loss: 0.6142598390579224 \n",
      "     Training Step: 226 Training Loss: 0.6111574769020081 \n",
      "     Training Step: 227 Training Loss: 0.6139299273490906 \n",
      "     Training Step: 228 Training Loss: 0.6126458644866943 \n",
      "     Training Step: 229 Training Loss: 0.6138181686401367 \n",
      "     Training Step: 230 Training Loss: 0.6125975847244263 \n",
      "     Training Step: 231 Training Loss: 0.6153218150138855 \n",
      "     Training Step: 232 Training Loss: 0.6167892217636108 \n",
      "     Training Step: 233 Training Loss: 0.6156068444252014 \n",
      "     Training Step: 234 Training Loss: 0.6117914319038391 \n",
      "     Training Step: 235 Training Loss: 0.6176202297210693 \n",
      "     Training Step: 236 Training Loss: 0.6123936176300049 \n",
      "     Training Step: 237 Training Loss: 0.6159676909446716 \n",
      "     Training Step: 238 Training Loss: 0.6169039607048035 \n",
      "     Training Step: 239 Training Loss: 0.6153203248977661 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6130334734916687 \n",
      "     Validation Step: 1 Validation Loss: 0.6178053617477417 \n",
      "     Validation Step: 2 Validation Loss: 0.6156933307647705 \n",
      "     Validation Step: 3 Validation Loss: 0.6138257384300232 \n",
      "     Validation Step: 4 Validation Loss: 0.6176943778991699 \n",
      "     Validation Step: 5 Validation Loss: 0.6174219250679016 \n",
      "     Validation Step: 6 Validation Loss: 0.6123801469802856 \n",
      "     Validation Step: 7 Validation Loss: 0.6118841767311096 \n",
      "     Validation Step: 8 Validation Loss: 0.6144137978553772 \n",
      "     Validation Step: 9 Validation Loss: 0.6114168763160706 \n",
      "     Validation Step: 10 Validation Loss: 0.6131877303123474 \n",
      "     Validation Step: 11 Validation Loss: 0.6154632568359375 \n",
      "     Validation Step: 12 Validation Loss: 0.6150761246681213 \n",
      "     Validation Step: 13 Validation Loss: 0.6121513247489929 \n",
      "     Validation Step: 14 Validation Loss: 0.613847017288208 \n",
      "     Validation Step: 15 Validation Loss: 0.6153833270072937 \n",
      "     Validation Step: 16 Validation Loss: 0.6147213578224182 \n",
      "     Validation Step: 17 Validation Loss: 0.6147812604904175 \n",
      "     Validation Step: 18 Validation Loss: 0.6183212399482727 \n",
      "     Validation Step: 19 Validation Loss: 0.6114313006401062 \n",
      "     Validation Step: 20 Validation Loss: 0.6104686856269836 \n",
      "     Validation Step: 21 Validation Loss: 0.6181631088256836 \n",
      "     Validation Step: 22 Validation Loss: 0.6104488372802734 \n",
      "     Validation Step: 23 Validation Loss: 0.616392195224762 \n",
      "     Validation Step: 24 Validation Loss: 0.6184331178665161 \n",
      "     Validation Step: 25 Validation Loss: 0.6185706853866577 \n",
      "     Validation Step: 26 Validation Loss: 0.6142957806587219 \n",
      "     Validation Step: 27 Validation Loss: 0.6118075251579285 \n",
      "     Validation Step: 28 Validation Loss: 0.6147060990333557 \n",
      "     Validation Step: 29 Validation Loss: 0.615756094455719 \n",
      "     Validation Step: 30 Validation Loss: 0.6185556650161743 \n",
      "     Validation Step: 31 Validation Loss: 0.6171232461929321 \n",
      "     Validation Step: 32 Validation Loss: 0.6143319606781006 \n",
      "     Validation Step: 33 Validation Loss: 0.6109440326690674 \n",
      "     Validation Step: 34 Validation Loss: 0.6151862740516663 \n",
      "     Validation Step: 35 Validation Loss: 0.6138178110122681 \n",
      "     Validation Step: 36 Validation Loss: 0.6079193949699402 \n",
      "     Validation Step: 37 Validation Loss: 0.6104587316513062 \n",
      "     Validation Step: 38 Validation Loss: 0.6150044798851013 \n",
      "     Validation Step: 39 Validation Loss: 0.6159347295761108 \n",
      "     Validation Step: 40 Validation Loss: 0.6143646836280823 \n",
      "     Validation Step: 41 Validation Loss: 0.6107738018035889 \n",
      "     Validation Step: 42 Validation Loss: 0.6107571125030518 \n",
      "     Validation Step: 43 Validation Loss: 0.61346036195755 \n",
      "     Validation Step: 44 Validation Loss: 0.616138219833374 \n",
      "Epoch: 10\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6139473915100098 \n",
      "     Training Step: 1 Training Loss: 0.6147778630256653 \n",
      "     Training Step: 2 Training Loss: 0.616879403591156 \n",
      "     Training Step: 3 Training Loss: 0.6117724776268005 \n",
      "     Training Step: 4 Training Loss: 0.6146020889282227 \n",
      "     Training Step: 5 Training Loss: 0.6184812188148499 \n",
      "     Training Step: 6 Training Loss: 0.6134522557258606 \n",
      "     Training Step: 7 Training Loss: 0.6145213842391968 \n",
      "     Training Step: 8 Training Loss: 0.6144853830337524 \n",
      "     Training Step: 9 Training Loss: 0.6120186448097229 \n",
      "     Training Step: 10 Training Loss: 0.6116369366645813 \n",
      "     Training Step: 11 Training Loss: 0.6139568090438843 \n",
      "     Training Step: 12 Training Loss: 0.6148407459259033 \n",
      "     Training Step: 13 Training Loss: 0.6125766038894653 \n",
      "     Training Step: 14 Training Loss: 0.6156603097915649 \n",
      "     Training Step: 15 Training Loss: 0.617039144039154 \n",
      "     Training Step: 16 Training Loss: 0.61646968126297 \n",
      "     Training Step: 17 Training Loss: 0.6114978194236755 \n",
      "     Training Step: 18 Training Loss: 0.6178412437438965 \n",
      "     Training Step: 19 Training Loss: 0.6143017411231995 \n",
      "     Training Step: 20 Training Loss: 0.6119836568832397 \n",
      "     Training Step: 21 Training Loss: 0.6130195260047913 \n",
      "     Training Step: 22 Training Loss: 0.6129125952720642 \n",
      "     Training Step: 23 Training Loss: 0.6154518723487854 \n",
      "     Training Step: 24 Training Loss: 0.6128849387168884 \n",
      "     Training Step: 25 Training Loss: 0.6097817420959473 \n",
      "     Training Step: 26 Training Loss: 0.6183292865753174 \n",
      "     Training Step: 27 Training Loss: 0.6122410893440247 \n",
      "     Training Step: 28 Training Loss: 0.6108511090278625 \n",
      "     Training Step: 29 Training Loss: 0.6127503514289856 \n",
      "     Training Step: 30 Training Loss: 0.6171086430549622 \n",
      "     Training Step: 31 Training Loss: 0.6129626631736755 \n",
      "     Training Step: 32 Training Loss: 0.6196645498275757 \n",
      "     Training Step: 33 Training Loss: 0.614138662815094 \n",
      "     Training Step: 34 Training Loss: 0.6119208931922913 \n",
      "     Training Step: 35 Training Loss: 0.6107218861579895 \n",
      "     Training Step: 36 Training Loss: 0.6178699731826782 \n",
      "     Training Step: 37 Training Loss: 0.6102690100669861 \n",
      "     Training Step: 38 Training Loss: 0.6179239749908447 \n",
      "     Training Step: 39 Training Loss: 0.6102938055992126 \n",
      "     Training Step: 40 Training Loss: 0.6158743500709534 \n",
      "     Training Step: 41 Training Loss: 0.6106511354446411 \n",
      "     Training Step: 42 Training Loss: 0.6144759058952332 \n",
      "     Training Step: 43 Training Loss: 0.6157420873641968 \n",
      "     Training Step: 44 Training Loss: 0.6148151755332947 \n",
      "     Training Step: 45 Training Loss: 0.6153727173805237 \n",
      "     Training Step: 46 Training Loss: 0.6209414005279541 \n",
      "     Training Step: 47 Training Loss: 0.6160747408866882 \n",
      "     Training Step: 48 Training Loss: 0.6128230094909668 \n",
      "     Training Step: 49 Training Loss: 0.616773247718811 \n",
      "     Training Step: 50 Training Loss: 0.6124528646469116 \n",
      "     Training Step: 51 Training Loss: 0.6161567568778992 \n",
      "     Training Step: 52 Training Loss: 0.6134041547775269 \n",
      "     Training Step: 53 Training Loss: 0.6122792363166809 \n",
      "     Training Step: 54 Training Loss: 0.6119322180747986 \n",
      "     Training Step: 55 Training Loss: 0.615599513053894 \n",
      "     Training Step: 56 Training Loss: 0.613829493522644 \n",
      "     Training Step: 57 Training Loss: 0.6185758709907532 \n",
      "     Training Step: 58 Training Loss: 0.6134796738624573 \n",
      "     Training Step: 59 Training Loss: 0.6158185601234436 \n",
      "     Training Step: 60 Training Loss: 0.6157292723655701 \n",
      "     Training Step: 61 Training Loss: 0.6171598434448242 \n",
      "     Training Step: 62 Training Loss: 0.6142185926437378 \n",
      "     Training Step: 63 Training Loss: 0.6178431510925293 \n",
      "     Training Step: 64 Training Loss: 0.6172425150871277 \n",
      "     Training Step: 65 Training Loss: 0.6164781451225281 \n",
      "     Training Step: 66 Training Loss: 0.6143625378608704 \n",
      "     Training Step: 67 Training Loss: 0.6133114099502563 \n",
      "     Training Step: 68 Training Loss: 0.6147940754890442 \n",
      "     Training Step: 69 Training Loss: 0.616439938545227 \n",
      "     Training Step: 70 Training Loss: 0.6164193153381348 \n",
      "     Training Step: 71 Training Loss: 0.6103527545928955 \n",
      "     Training Step: 72 Training Loss: 0.6154186129570007 \n",
      "     Training Step: 73 Training Loss: 0.6119228005409241 \n",
      "     Training Step: 74 Training Loss: 0.6118944883346558 \n",
      "     Training Step: 75 Training Loss: 0.618535041809082 \n",
      "     Training Step: 76 Training Loss: 0.6148493885993958 \n",
      "     Training Step: 77 Training Loss: 0.6141874194145203 \n",
      "     Training Step: 78 Training Loss: 0.6136170029640198 \n",
      "     Training Step: 79 Training Loss: 0.6130882501602173 \n",
      "     Training Step: 80 Training Loss: 0.6121222376823425 \n",
      "     Training Step: 81 Training Loss: 0.6124919056892395 \n",
      "     Training Step: 82 Training Loss: 0.6169313192367554 \n",
      "     Training Step: 83 Training Loss: 0.6198645830154419 \n",
      "     Training Step: 84 Training Loss: 0.6129860877990723 \n",
      "     Training Step: 85 Training Loss: 0.6109482645988464 \n",
      "     Training Step: 86 Training Loss: 0.6150493025779724 \n",
      "     Training Step: 87 Training Loss: 0.6126824617385864 \n",
      "     Training Step: 88 Training Loss: 0.6143257021903992 \n",
      "     Training Step: 89 Training Loss: 0.6155452728271484 \n",
      "     Training Step: 90 Training Loss: 0.6133230328559875 \n",
      "     Training Step: 91 Training Loss: 0.6107912659645081 \n",
      "     Training Step: 92 Training Loss: 0.6116018891334534 \n",
      "     Training Step: 93 Training Loss: 0.6201596856117249 \n",
      "     Training Step: 94 Training Loss: 0.6167370080947876 \n",
      "     Training Step: 95 Training Loss: 0.6136090755462646 \n",
      "     Training Step: 96 Training Loss: 0.6184802651405334 \n",
      "     Training Step: 97 Training Loss: 0.6135083436965942 \n",
      "     Training Step: 98 Training Loss: 0.617457389831543 \n",
      "     Training Step: 99 Training Loss: 0.6151344776153564 \n",
      "     Training Step: 100 Training Loss: 0.6111240386962891 \n",
      "     Training Step: 101 Training Loss: 0.611257791519165 \n",
      "     Training Step: 102 Training Loss: 0.6124919652938843 \n",
      "     Training Step: 103 Training Loss: 0.613821268081665 \n",
      "     Training Step: 104 Training Loss: 0.6121798157691956 \n",
      "     Training Step: 105 Training Loss: 0.6107110977172852 \n",
      "     Training Step: 106 Training Loss: 0.6148284673690796 \n",
      "     Training Step: 107 Training Loss: 0.6132819056510925 \n",
      "     Training Step: 108 Training Loss: 0.6125773191452026 \n",
      "     Training Step: 109 Training Loss: 0.6168782711029053 \n",
      "     Training Step: 110 Training Loss: 0.6145607829093933 \n",
      "     Training Step: 111 Training Loss: 0.6155740022659302 \n",
      "     Training Step: 112 Training Loss: 0.6116440892219543 \n",
      "     Training Step: 113 Training Loss: 0.6138179302215576 \n",
      "     Training Step: 114 Training Loss: 0.6117168664932251 \n",
      "     Training Step: 115 Training Loss: 0.6140979528427124 \n",
      "     Training Step: 116 Training Loss: 0.6153347492218018 \n",
      "     Training Step: 117 Training Loss: 0.6104437708854675 \n",
      "     Training Step: 118 Training Loss: 0.6137518286705017 \n",
      "     Training Step: 119 Training Loss: 0.610130250453949 \n",
      "     Training Step: 120 Training Loss: 0.6115942001342773 \n",
      "     Training Step: 121 Training Loss: 0.6145136952400208 \n",
      "     Training Step: 122 Training Loss: 0.6139688491821289 \n",
      "     Training Step: 123 Training Loss: 0.6122466921806335 \n",
      "     Training Step: 124 Training Loss: 0.6157404184341431 \n",
      "     Training Step: 125 Training Loss: 0.6154165863990784 \n",
      "     Training Step: 126 Training Loss: 0.612339973449707 \n",
      "     Training Step: 127 Training Loss: 0.6185948252677917 \n",
      "     Training Step: 128 Training Loss: 0.6110349297523499 \n",
      "     Training Step: 129 Training Loss: 0.609400749206543 \n",
      "     Training Step: 130 Training Loss: 0.6155975461006165 \n",
      "     Training Step: 131 Training Loss: 0.6135919690132141 \n",
      "     Training Step: 132 Training Loss: 0.6173644065856934 \n",
      "     Training Step: 133 Training Loss: 0.61782306432724 \n",
      "     Training Step: 134 Training Loss: 0.6108988523483276 \n",
      "     Training Step: 135 Training Loss: 0.6132380962371826 \n",
      "     Training Step: 136 Training Loss: 0.614764928817749 \n",
      "     Training Step: 137 Training Loss: 0.6123058795928955 \n",
      "     Training Step: 138 Training Loss: 0.616355299949646 \n",
      "     Training Step: 139 Training Loss: 0.610754132270813 \n",
      "     Training Step: 140 Training Loss: 0.6149501800537109 \n",
      "     Training Step: 141 Training Loss: 0.6116671562194824 \n",
      "     Training Step: 142 Training Loss: 0.6161260008811951 \n",
      "     Training Step: 143 Training Loss: 0.6182776689529419 \n",
      "     Training Step: 144 Training Loss: 0.6132367849349976 \n",
      "     Training Step: 145 Training Loss: 0.616521418094635 \n",
      "     Training Step: 146 Training Loss: 0.615622878074646 \n",
      "     Training Step: 147 Training Loss: 0.615850567817688 \n",
      "     Training Step: 148 Training Loss: 0.6153709292411804 \n",
      "     Training Step: 149 Training Loss: 0.6129926443099976 \n",
      "     Training Step: 150 Training Loss: 0.6147446036338806 \n",
      "     Training Step: 151 Training Loss: 0.614471435546875 \n",
      "     Training Step: 152 Training Loss: 0.6183435916900635 \n",
      "     Training Step: 153 Training Loss: 0.6178733706474304 \n",
      "     Training Step: 154 Training Loss: 0.6147921681404114 \n",
      "     Training Step: 155 Training Loss: 0.6168583035469055 \n",
      "     Training Step: 156 Training Loss: 0.6169196367263794 \n",
      "     Training Step: 157 Training Loss: 0.6180764436721802 \n",
      "     Training Step: 158 Training Loss: 0.6153838038444519 \n",
      "     Training Step: 159 Training Loss: 0.616701602935791 \n",
      "     Training Step: 160 Training Loss: 0.6144454479217529 \n",
      "     Training Step: 161 Training Loss: 0.6172242760658264 \n",
      "     Training Step: 162 Training Loss: 0.6188896298408508 \n",
      "     Training Step: 163 Training Loss: 0.6115232706069946 \n",
      "     Training Step: 164 Training Loss: 0.6149989366531372 \n",
      "     Training Step: 165 Training Loss: 0.6151747703552246 \n",
      "     Training Step: 166 Training Loss: 0.6133284568786621 \n",
      "     Training Step: 167 Training Loss: 0.6116644740104675 \n",
      "     Training Step: 168 Training Loss: 0.6154264211654663 \n",
      "     Training Step: 169 Training Loss: 0.6182550191879272 \n",
      "     Training Step: 170 Training Loss: 0.6123197078704834 \n",
      "     Training Step: 171 Training Loss: 0.6131526231765747 \n",
      "     Training Step: 172 Training Loss: 0.6099189519882202 \n",
      "     Training Step: 173 Training Loss: 0.6147009134292603 \n",
      "     Training Step: 174 Training Loss: 0.6115210652351379 \n",
      "     Training Step: 175 Training Loss: 0.613169252872467 \n",
      "     Training Step: 176 Training Loss: 0.6158875226974487 \n",
      "     Training Step: 177 Training Loss: 0.6162176132202148 \n",
      "     Training Step: 178 Training Loss: 0.612461268901825 \n",
      "     Training Step: 179 Training Loss: 0.6168360710144043 \n",
      "     Training Step: 180 Training Loss: 0.6147065758705139 \n",
      "     Training Step: 181 Training Loss: 0.6159124970436096 \n",
      "     Training Step: 182 Training Loss: 0.6152569651603699 \n",
      "     Training Step: 183 Training Loss: 0.6118683815002441 \n",
      "     Training Step: 184 Training Loss: 0.611240804195404 \n",
      "     Training Step: 185 Training Loss: 0.609462320804596 \n",
      "     Training Step: 186 Training Loss: 0.6116612553596497 \n",
      "     Training Step: 187 Training Loss: 0.6208307147026062 \n",
      "     Training Step: 188 Training Loss: 0.6183217167854309 \n",
      "     Training Step: 189 Training Loss: 0.6120917797088623 \n",
      "     Training Step: 190 Training Loss: 0.6168156266212463 \n",
      "     Training Step: 191 Training Loss: 0.6142266392707825 \n",
      "     Training Step: 192 Training Loss: 0.6104143261909485 \n",
      "     Training Step: 193 Training Loss: 0.6125656962394714 \n",
      "     Training Step: 194 Training Loss: 0.6149061322212219 \n",
      "     Training Step: 195 Training Loss: 0.6095159649848938 \n",
      "     Training Step: 196 Training Loss: 0.612696647644043 \n",
      "     Training Step: 197 Training Loss: 0.617041289806366 \n",
      "     Training Step: 198 Training Loss: 0.6119594573974609 \n",
      "     Training Step: 199 Training Loss: 0.6140148639678955 \n",
      "     Training Step: 200 Training Loss: 0.6128186583518982 \n",
      "     Training Step: 201 Training Loss: 0.6163881421089172 \n",
      "     Training Step: 202 Training Loss: 0.6110513806343079 \n",
      "     Training Step: 203 Training Loss: 0.6137433648109436 \n",
      "     Training Step: 204 Training Loss: 0.6177209615707397 \n",
      "     Training Step: 205 Training Loss: 0.6146876215934753 \n",
      "     Training Step: 206 Training Loss: 0.6144171953201294 \n",
      "     Training Step: 207 Training Loss: 0.6147118806838989 \n",
      "     Training Step: 208 Training Loss: 0.6169411540031433 \n",
      "     Training Step: 209 Training Loss: 0.618705689907074 \n",
      "     Training Step: 210 Training Loss: 0.6134224534034729 \n",
      "     Training Step: 211 Training Loss: 0.6167284846305847 \n",
      "     Training Step: 212 Training Loss: 0.6144305467605591 \n",
      "     Training Step: 213 Training Loss: 0.6103602051734924 \n",
      "     Training Step: 214 Training Loss: 0.613616406917572 \n",
      "     Training Step: 215 Training Loss: 0.6146230697631836 \n",
      "     Training Step: 216 Training Loss: 0.6136928796768188 \n",
      "     Training Step: 217 Training Loss: 0.6153796315193176 \n",
      "     Training Step: 218 Training Loss: 0.612350344657898 \n",
      "     Training Step: 219 Training Loss: 0.6105371713638306 \n",
      "     Training Step: 220 Training Loss: 0.6115108728408813 \n",
      "     Training Step: 221 Training Loss: 0.6155840754508972 \n",
      "     Training Step: 222 Training Loss: 0.6154466867446899 \n",
      "     Training Step: 223 Training Loss: 0.6137341856956482 \n",
      "     Training Step: 224 Training Loss: 0.6174966096878052 \n",
      "     Training Step: 225 Training Loss: 0.6150009632110596 \n",
      "     Training Step: 226 Training Loss: 0.6141112446784973 \n",
      "     Training Step: 227 Training Loss: 0.6109102964401245 \n",
      "     Training Step: 228 Training Loss: 0.6168491840362549 \n",
      "     Training Step: 229 Training Loss: 0.6083501577377319 \n",
      "     Training Step: 230 Training Loss: 0.6191359758377075 \n",
      "     Training Step: 231 Training Loss: 0.6152358055114746 \n",
      "     Training Step: 232 Training Loss: 0.6129193305969238 \n",
      "     Training Step: 233 Training Loss: 0.6117633581161499 \n",
      "     Training Step: 234 Training Loss: 0.6098790168762207 \n",
      "     Training Step: 235 Training Loss: 0.611510694026947 \n",
      "     Training Step: 236 Training Loss: 0.6197141408920288 \n",
      "     Training Step: 237 Training Loss: 0.6130094528198242 \n",
      "     Training Step: 238 Training Loss: 0.6148293614387512 \n",
      "     Training Step: 239 Training Loss: 0.6142747402191162 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.618435263633728 \n",
      "     Validation Step: 1 Validation Loss: 0.61515873670578 \n",
      "     Validation Step: 2 Validation Loss: 0.6124075651168823 \n",
      "     Validation Step: 3 Validation Loss: 0.6160733699798584 \n",
      "     Validation Step: 4 Validation Loss: 0.6130544543266296 \n",
      "     Validation Step: 5 Validation Loss: 0.6131449937820435 \n",
      "     Validation Step: 6 Validation Loss: 0.610485315322876 \n",
      "     Validation Step: 7 Validation Loss: 0.6143003702163696 \n",
      "     Validation Step: 8 Validation Loss: 0.615635097026825 \n",
      "     Validation Step: 9 Validation Loss: 0.6147391200065613 \n",
      "     Validation Step: 10 Validation Loss: 0.6138005256652832 \n",
      "     Validation Step: 11 Validation Loss: 0.6147246360778809 \n",
      "     Validation Step: 12 Validation Loss: 0.6177203059196472 \n",
      "     Validation Step: 13 Validation Loss: 0.6149818897247314 \n",
      "     Validation Step: 14 Validation Loss: 0.6158930063247681 \n",
      "     Validation Step: 15 Validation Loss: 0.615140974521637 \n",
      "     Validation Step: 16 Validation Loss: 0.6153789162635803 \n",
      "     Validation Step: 17 Validation Loss: 0.6163415312767029 \n",
      "     Validation Step: 18 Validation Loss: 0.6154928207397461 \n",
      "     Validation Step: 19 Validation Loss: 0.6181638836860657 \n",
      "     Validation Step: 20 Validation Loss: 0.6107574105262756 \n",
      "     Validation Step: 21 Validation Loss: 0.617770254611969 \n",
      "     Validation Step: 22 Validation Loss: 0.6143615245819092 \n",
      "     Validation Step: 23 Validation Loss: 0.6118009686470032 \n",
      "     Validation Step: 24 Validation Loss: 0.6138296723365784 \n",
      "     Validation Step: 25 Validation Loss: 0.6171453595161438 \n",
      "     Validation Step: 26 Validation Loss: 0.6118895411491394 \n",
      "     Validation Step: 27 Validation Loss: 0.6185758113861084 \n",
      "     Validation Step: 28 Validation Loss: 0.6137853860855103 \n",
      "     Validation Step: 29 Validation Loss: 0.6121605038642883 \n",
      "     Validation Step: 30 Validation Loss: 0.6185826063156128 \n",
      "     Validation Step: 31 Validation Loss: 0.61073237657547 \n",
      "     Validation Step: 32 Validation Loss: 0.6109622120857239 \n",
      "     Validation Step: 33 Validation Loss: 0.6114333868026733 \n",
      "     Validation Step: 34 Validation Loss: 0.6134874224662781 \n",
      "     Validation Step: 35 Validation Loss: 0.6144695281982422 \n",
      "     Validation Step: 36 Validation Loss: 0.6183107495307922 \n",
      "     Validation Step: 37 Validation Loss: 0.6104881167411804 \n",
      "     Validation Step: 38 Validation Loss: 0.6079576015472412 \n",
      "     Validation Step: 39 Validation Loss: 0.6104639172554016 \n",
      "     Validation Step: 40 Validation Loss: 0.6147767901420593 \n",
      "     Validation Step: 41 Validation Loss: 0.617427408695221 \n",
      "     Validation Step: 42 Validation Loss: 0.611423909664154 \n",
      "     Validation Step: 43 Validation Loss: 0.6143674254417419 \n",
      "     Validation Step: 44 Validation Loss: 0.6157354712486267 \n",
      "Epoch: 11\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6119130253791809 \n",
      "     Training Step: 1 Training Loss: 0.6167247891426086 \n",
      "     Training Step: 2 Training Loss: 0.6132832765579224 \n",
      "     Training Step: 3 Training Loss: 0.6112545132637024 \n",
      "     Training Step: 4 Training Loss: 0.6119164824485779 \n",
      "     Training Step: 5 Training Loss: 0.6150833964347839 \n",
      "     Training Step: 6 Training Loss: 0.6178825497627258 \n",
      "     Training Step: 7 Training Loss: 0.6139839887619019 \n",
      "     Training Step: 8 Training Loss: 0.6142939329147339 \n",
      "     Training Step: 9 Training Loss: 0.6112999320030212 \n",
      "     Training Step: 10 Training Loss: 0.6116218566894531 \n",
      "     Training Step: 11 Training Loss: 0.6124090552330017 \n",
      "     Training Step: 12 Training Loss: 0.6145259141921997 \n",
      "     Training Step: 13 Training Loss: 0.6165875196456909 \n",
      "     Training Step: 14 Training Loss: 0.612274706363678 \n",
      "     Training Step: 15 Training Loss: 0.6164228320121765 \n",
      "     Training Step: 16 Training Loss: 0.6146692037582397 \n",
      "     Training Step: 17 Training Loss: 0.6115651726722717 \n",
      "     Training Step: 18 Training Loss: 0.6170254945755005 \n",
      "     Training Step: 19 Training Loss: 0.6098880767822266 \n",
      "     Training Step: 20 Training Loss: 0.6132811307907104 \n",
      "     Training Step: 21 Training Loss: 0.6147270202636719 \n",
      "     Training Step: 22 Training Loss: 0.6149826645851135 \n",
      "     Training Step: 23 Training Loss: 0.6126251816749573 \n",
      "     Training Step: 24 Training Loss: 0.6178265810012817 \n",
      "     Training Step: 25 Training Loss: 0.6146455407142639 \n",
      "     Training Step: 26 Training Loss: 0.6116523146629333 \n",
      "     Training Step: 27 Training Loss: 0.6143699288368225 \n",
      "     Training Step: 28 Training Loss: 0.6116172671318054 \n",
      "     Training Step: 29 Training Loss: 0.6135642528533936 \n",
      "     Training Step: 30 Training Loss: 0.6135401725769043 \n",
      "     Training Step: 31 Training Loss: 0.6144164204597473 \n",
      "     Training Step: 32 Training Loss: 0.6098451614379883 \n",
      "     Training Step: 33 Training Loss: 0.6199206113815308 \n",
      "     Training Step: 34 Training Loss: 0.6145662665367126 \n",
      "     Training Step: 35 Training Loss: 0.6158096790313721 \n",
      "     Training Step: 36 Training Loss: 0.6209174394607544 \n",
      "     Training Step: 37 Training Loss: 0.6139722466468811 \n",
      "     Training Step: 38 Training Loss: 0.6128225326538086 \n",
      "     Training Step: 39 Training Loss: 0.6168563961982727 \n",
      "     Training Step: 40 Training Loss: 0.6194667816162109 \n",
      "     Training Step: 41 Training Loss: 0.6186133027076721 \n",
      "     Training Step: 42 Training Loss: 0.6111012697219849 \n",
      "     Training Step: 43 Training Loss: 0.6141961216926575 \n",
      "     Training Step: 44 Training Loss: 0.6155247688293457 \n",
      "     Training Step: 45 Training Loss: 0.6184950470924377 \n",
      "     Training Step: 46 Training Loss: 0.6178754568099976 \n",
      "     Training Step: 47 Training Loss: 0.6134791374206543 \n",
      "     Training Step: 48 Training Loss: 0.6148678064346313 \n",
      "     Training Step: 49 Training Loss: 0.6184540390968323 \n",
      "     Training Step: 50 Training Loss: 0.6119679808616638 \n",
      "     Training Step: 51 Training Loss: 0.6177023649215698 \n",
      "     Training Step: 52 Training Loss: 0.6163154244422913 \n",
      "     Training Step: 53 Training Loss: 0.6108188033103943 \n",
      "     Training Step: 54 Training Loss: 0.61244797706604 \n",
      "     Training Step: 55 Training Loss: 0.6158481240272522 \n",
      "     Training Step: 56 Training Loss: 0.6173271536827087 \n",
      "     Training Step: 57 Training Loss: 0.6169577836990356 \n",
      "     Training Step: 58 Training Loss: 0.6160397529602051 \n",
      "     Training Step: 59 Training Loss: 0.6105107665061951 \n",
      "     Training Step: 60 Training Loss: 0.614133358001709 \n",
      "     Training Step: 61 Training Loss: 0.6142009496688843 \n",
      "     Training Step: 62 Training Loss: 0.615485668182373 \n",
      "     Training Step: 63 Training Loss: 0.6116341948509216 \n",
      "     Training Step: 64 Training Loss: 0.6154062747955322 \n",
      "     Training Step: 65 Training Loss: 0.6116859912872314 \n",
      "     Training Step: 66 Training Loss: 0.6141437292098999 \n",
      "     Training Step: 67 Training Loss: 0.6153837442398071 \n",
      "     Training Step: 68 Training Loss: 0.6143761873245239 \n",
      "     Training Step: 69 Training Loss: 0.6131356954574585 \n",
      "     Training Step: 70 Training Loss: 0.6129820346832275 \n",
      "     Training Step: 71 Training Loss: 0.6122344732284546 \n",
      "     Training Step: 72 Training Loss: 0.61786949634552 \n",
      "     Training Step: 73 Training Loss: 0.6124438047409058 \n",
      "     Training Step: 74 Training Loss: 0.6136761903762817 \n",
      "     Training Step: 75 Training Loss: 0.61336350440979 \n",
      "     Training Step: 76 Training Loss: 0.6142167448997498 \n",
      "     Training Step: 77 Training Loss: 0.6093024015426636 \n",
      "     Training Step: 78 Training Loss: 0.616055428981781 \n",
      "     Training Step: 79 Training Loss: 0.6168535351753235 \n",
      "     Training Step: 80 Training Loss: 0.6119958162307739 \n",
      "     Training Step: 81 Training Loss: 0.6154303550720215 \n",
      "     Training Step: 82 Training Loss: 0.6101740002632141 \n",
      "     Training Step: 83 Training Loss: 0.614115834236145 \n",
      "     Training Step: 84 Training Loss: 0.6166448593139648 \n",
      "     Training Step: 85 Training Loss: 0.6118735671043396 \n",
      "     Training Step: 86 Training Loss: 0.6122908592224121 \n",
      "     Training Step: 87 Training Loss: 0.6115508675575256 \n",
      "     Training Step: 88 Training Loss: 0.6149845719337463 \n",
      "     Training Step: 89 Training Loss: 0.6139915585517883 \n",
      "     Training Step: 90 Training Loss: 0.6168991327285767 \n",
      "     Training Step: 91 Training Loss: 0.610255777835846 \n",
      "     Training Step: 92 Training Loss: 0.6122474670410156 \n",
      "     Training Step: 93 Training Loss: 0.6162164211273193 \n",
      "     Training Step: 94 Training Loss: 0.6107490062713623 \n",
      "     Training Step: 95 Training Loss: 0.6114987730979919 \n",
      "     Training Step: 96 Training Loss: 0.61269611120224 \n",
      "     Training Step: 97 Training Loss: 0.6120904684066772 \n",
      "     Training Step: 98 Training Loss: 0.6163538694381714 \n",
      "     Training Step: 99 Training Loss: 0.6147298812866211 \n",
      "     Training Step: 100 Training Loss: 0.6145113706588745 \n",
      "     Training Step: 101 Training Loss: 0.6085110902786255 \n",
      "     Training Step: 102 Training Loss: 0.6095806360244751 \n",
      "     Training Step: 103 Training Loss: 0.610493540763855 \n",
      "     Training Step: 104 Training Loss: 0.6153846383094788 \n",
      "     Training Step: 105 Training Loss: 0.6139252781867981 \n",
      "     Training Step: 106 Training Loss: 0.6107561588287354 \n",
      "     Training Step: 107 Training Loss: 0.6095582246780396 \n",
      "     Training Step: 108 Training Loss: 0.6185922622680664 \n",
      "     Training Step: 109 Training Loss: 0.6151522994041443 \n",
      "     Training Step: 110 Training Loss: 0.6133454442024231 \n",
      "     Training Step: 111 Training Loss: 0.6180613040924072 \n",
      "     Training Step: 112 Training Loss: 0.6145768761634827 \n",
      "     Training Step: 113 Training Loss: 0.6126696467399597 \n",
      "     Training Step: 114 Training Loss: 0.6156333684921265 \n",
      "     Training Step: 115 Training Loss: 0.6102316379547119 \n",
      "     Training Step: 116 Training Loss: 0.6138392686843872 \n",
      "     Training Step: 117 Training Loss: 0.6179381608963013 \n",
      "     Training Step: 118 Training Loss: 0.6116403937339783 \n",
      "     Training Step: 119 Training Loss: 0.6114658713340759 \n",
      "     Training Step: 120 Training Loss: 0.6118490099906921 \n",
      "     Training Step: 121 Training Loss: 0.6183199882507324 \n",
      "     Training Step: 122 Training Loss: 0.6148431897163391 \n",
      "     Training Step: 123 Training Loss: 0.6147460341453552 \n",
      "     Training Step: 124 Training Loss: 0.6174889802932739 \n",
      "     Training Step: 125 Training Loss: 0.6182050108909607 \n",
      "     Training Step: 126 Training Loss: 0.6167599558830261 \n",
      "     Training Step: 127 Training Loss: 0.613396167755127 \n",
      "     Training Step: 128 Training Loss: 0.6147453188896179 \n",
      "     Training Step: 129 Training Loss: 0.6131729483604431 \n",
      "     Training Step: 130 Training Loss: 0.6131851077079773 \n",
      "     Training Step: 131 Training Loss: 0.61066073179245 \n",
      "     Training Step: 132 Training Loss: 0.6159068942070007 \n",
      "     Training Step: 133 Training Loss: 0.6161811947822571 \n",
      "     Training Step: 134 Training Loss: 0.6153478026390076 \n",
      "     Training Step: 135 Training Loss: 0.6123647093772888 \n",
      "     Training Step: 136 Training Loss: 0.6166747212409973 \n",
      "     Training Step: 137 Training Loss: 0.6166635751724243 \n",
      "     Training Step: 138 Training Loss: 0.614769697189331 \n",
      "     Training Step: 139 Training Loss: 0.615418553352356 \n",
      "     Training Step: 140 Training Loss: 0.6151965856552124 \n",
      "     Training Step: 141 Training Loss: 0.6197084784507751 \n",
      "     Training Step: 142 Training Loss: 0.6126053333282471 \n",
      "     Training Step: 143 Training Loss: 0.6128540635108948 \n",
      "     Training Step: 144 Training Loss: 0.6158091425895691 \n",
      "     Training Step: 145 Training Loss: 0.613429069519043 \n",
      "     Training Step: 146 Training Loss: 0.6172215938568115 \n",
      "     Training Step: 147 Training Loss: 0.6180869340896606 \n",
      "     Training Step: 148 Training Loss: 0.6154796481132507 \n",
      "     Training Step: 149 Training Loss: 0.6167951226234436 \n",
      "     Training Step: 150 Training Loss: 0.612321674823761 \n",
      "     Training Step: 151 Training Loss: 0.6153379678726196 \n",
      "     Training Step: 152 Training Loss: 0.6105130910873413 \n",
      "     Training Step: 153 Training Loss: 0.6182557940483093 \n",
      "     Training Step: 154 Training Loss: 0.6156504154205322 \n",
      "     Training Step: 155 Training Loss: 0.6173224449157715 \n",
      "     Training Step: 156 Training Loss: 0.6150603890419006 \n",
      "     Training Step: 157 Training Loss: 0.6109214425086975 \n",
      "     Training Step: 158 Training Loss: 0.6174418330192566 \n",
      "     Training Step: 159 Training Loss: 0.6202295422554016 \n",
      "     Training Step: 160 Training Loss: 0.6108604669570923 \n",
      "     Training Step: 161 Training Loss: 0.6167565584182739 \n",
      "     Training Step: 162 Training Loss: 0.6099334955215454 \n",
      "     Training Step: 163 Training Loss: 0.6187069416046143 \n",
      "     Training Step: 164 Training Loss: 0.61258465051651 \n",
      "     Training Step: 165 Training Loss: 0.6190206408500671 \n",
      "     Training Step: 166 Training Loss: 0.6158503890037537 \n",
      "     Training Step: 167 Training Loss: 0.6167021989822388 \n",
      "     Training Step: 168 Training Loss: 0.6154330968856812 \n",
      "     Training Step: 169 Training Loss: 0.6180394291877747 \n",
      "     Training Step: 170 Training Loss: 0.613171398639679 \n",
      "     Training Step: 171 Training Loss: 0.6108157634735107 \n",
      "     Training Step: 172 Training Loss: 0.6124299764633179 \n",
      "     Training Step: 173 Training Loss: 0.6133881211280823 \n",
      "     Training Step: 174 Training Loss: 0.6109642386436462 \n",
      "     Training Step: 175 Training Loss: 0.6131154298782349 \n",
      "     Training Step: 176 Training Loss: 0.614090085029602 \n",
      "     Training Step: 177 Training Loss: 0.6156759858131409 \n",
      "     Training Step: 178 Training Loss: 0.6117492318153381 \n",
      "     Training Step: 179 Training Loss: 0.6151699423789978 \n",
      "     Training Step: 180 Training Loss: 0.6171245574951172 \n",
      "     Training Step: 181 Training Loss: 0.6137531399726868 \n",
      "     Training Step: 182 Training Loss: 0.6177178621292114 \n",
      "     Training Step: 183 Training Loss: 0.6129187345504761 \n",
      "     Training Step: 184 Training Loss: 0.6147527098655701 \n",
      "     Training Step: 185 Training Loss: 0.6190789341926575 \n",
      "     Training Step: 186 Training Loss: 0.6144548654556274 \n",
      "     Training Step: 187 Training Loss: 0.6115024089813232 \n",
      "     Training Step: 188 Training Loss: 0.6124088168144226 \n",
      "     Training Step: 189 Training Loss: 0.6147719621658325 \n",
      "     Training Step: 190 Training Loss: 0.6116162538528442 \n",
      "     Training Step: 191 Training Loss: 0.616909921169281 \n",
      "     Training Step: 192 Training Loss: 0.6129082441329956 \n",
      "     Training Step: 193 Training Loss: 0.615129828453064 \n",
      "     Training Step: 194 Training Loss: 0.6143214106559753 \n",
      "     Training Step: 195 Training Loss: 0.6168251037597656 \n",
      "     Training Step: 196 Training Loss: 0.6119431853294373 \n",
      "     Training Step: 197 Training Loss: 0.6148307919502258 \n",
      "     Training Step: 198 Training Loss: 0.6167115569114685 \n",
      "     Training Step: 199 Training Loss: 0.613217830657959 \n",
      "     Training Step: 200 Training Loss: 0.6171625852584839 \n",
      "     Training Step: 201 Training Loss: 0.6135396957397461 \n",
      "     Training Step: 202 Training Loss: 0.6108415126800537 \n",
      "     Training Step: 203 Training Loss: 0.6200136542320251 \n",
      "     Training Step: 204 Training Loss: 0.6137253046035767 \n",
      "     Training Step: 205 Training Loss: 0.6135559678077698 \n",
      "     Training Step: 206 Training Loss: 0.6140794157981873 \n",
      "     Training Step: 207 Training Loss: 0.6119317412376404 \n",
      "     Training Step: 208 Training Loss: 0.6147527694702148 \n",
      "     Training Step: 209 Training Loss: 0.6148459315299988 \n",
      "     Training Step: 210 Training Loss: 0.6102038025856018 \n",
      "     Training Step: 211 Training Loss: 0.615938663482666 \n",
      "     Training Step: 212 Training Loss: 0.6153204441070557 \n",
      "     Training Step: 213 Training Loss: 0.6106246113777161 \n",
      "     Training Step: 214 Training Loss: 0.6147215366363525 \n",
      "     Training Step: 215 Training Loss: 0.6122965812683105 \n",
      "     Training Step: 216 Training Loss: 0.6184943318367004 \n",
      "     Training Step: 217 Training Loss: 0.6117072701454163 \n",
      "     Training Step: 218 Training Loss: 0.6153772473335266 \n",
      "     Training Step: 219 Training Loss: 0.6157317757606506 \n",
      "     Training Step: 220 Training Loss: 0.6155216097831726 \n",
      "     Training Step: 221 Training Loss: 0.6129053235054016 \n",
      "     Training Step: 222 Training Loss: 0.611293613910675 \n",
      "     Training Step: 223 Training Loss: 0.6143829822540283 \n",
      "     Training Step: 224 Training Loss: 0.6163778305053711 \n",
      "     Training Step: 225 Training Loss: 0.61286860704422 \n",
      "     Training Step: 226 Training Loss: 0.6155670285224915 \n",
      "     Training Step: 227 Training Loss: 0.6130437254905701 \n",
      "     Training Step: 228 Training Loss: 0.6164383292198181 \n",
      "     Training Step: 229 Training Loss: 0.6137007474899292 \n",
      "     Training Step: 230 Training Loss: 0.6129328012466431 \n",
      "     Training Step: 231 Training Loss: 0.6106315851211548 \n",
      "     Training Step: 232 Training Loss: 0.6145409345626831 \n",
      "     Training Step: 233 Training Loss: 0.6138666272163391 \n",
      "     Training Step: 234 Training Loss: 0.613369345664978 \n",
      "     Training Step: 235 Training Loss: 0.6135888695716858 \n",
      "     Training Step: 236 Training Loss: 0.6101303100585938 \n",
      "     Training Step: 237 Training Loss: 0.6122793555259705 \n",
      "     Training Step: 238 Training Loss: 0.6143457889556885 \n",
      "     Training Step: 239 Training Loss: 0.6117062568664551 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6173300743103027 \n",
      "     Validation Step: 1 Validation Loss: 0.6105889678001404 \n",
      "     Validation Step: 2 Validation Loss: 0.6076558828353882 \n",
      "     Validation Step: 3 Validation Loss: 0.6142465472221375 \n",
      "     Validation Step: 4 Validation Loss: 0.615092933177948 \n",
      "     Validation Step: 5 Validation Loss: 0.611956000328064 \n",
      "     Validation Step: 6 Validation Loss: 0.6107192635536194 \n",
      "     Validation Step: 7 Validation Loss: 0.6177459955215454 \n",
      "     Validation Step: 8 Validation Loss: 0.6102384924888611 \n",
      "     Validation Step: 9 Validation Loss: 0.6128711700439453 \n",
      "     Validation Step: 10 Validation Loss: 0.6176137328147888 \n",
      "     Validation Step: 11 Validation Loss: 0.6142801642417908 \n",
      "     Validation Step: 12 Validation Loss: 0.6130366325378418 \n",
      "     Validation Step: 13 Validation Loss: 0.6149241924285889 \n",
      "     Validation Step: 14 Validation Loss: 0.6112511157989502 \n",
      "     Validation Step: 15 Validation Loss: 0.6141858696937561 \n",
      "     Validation Step: 16 Validation Loss: 0.6182835698127747 \n",
      "     Validation Step: 17 Validation Loss: 0.6112323999404907 \n",
      "     Validation Step: 18 Validation Loss: 0.6102139949798584 \n",
      "     Validation Step: 19 Validation Loss: 0.6185246706008911 \n",
      "     Validation Step: 20 Validation Loss: 0.6170431971549988 \n",
      "     Validation Step: 21 Validation Loss: 0.615264356136322 \n",
      "     Validation Step: 22 Validation Loss: 0.6145961880683899 \n",
      "     Validation Step: 23 Validation Loss: 0.6105554699897766 \n",
      "     Validation Step: 24 Validation Loss: 0.6158738136291504 \n",
      "     Validation Step: 25 Validation Loss: 0.611702561378479 \n",
      "     Validation Step: 26 Validation Loss: 0.6145690083503723 \n",
      "     Validation Step: 27 Validation Loss: 0.6137402057647705 \n",
      "     Validation Step: 28 Validation Loss: 0.6153532862663269 \n",
      "     Validation Step: 29 Validation Loss: 0.6156584620475769 \n",
      "     Validation Step: 30 Validation Loss: 0.6156204342842102 \n",
      "     Validation Step: 31 Validation Loss: 0.6116229295730591 \n",
      "     Validation Step: 32 Validation Loss: 0.618098795413971 \n",
      "     Validation Step: 33 Validation Loss: 0.6160649061203003 \n",
      "     Validation Step: 34 Validation Loss: 0.6141598224639893 \n",
      "     Validation Step: 35 Validation Loss: 0.6163015365600586 \n",
      "     Validation Step: 36 Validation Loss: 0.6185035705566406 \n",
      "     Validation Step: 37 Validation Loss: 0.6137076020240784 \n",
      "     Validation Step: 38 Validation Loss: 0.6133221387863159 \n",
      "     Validation Step: 39 Validation Loss: 0.6148922443389893 \n",
      "     Validation Step: 40 Validation Loss: 0.6183813810348511 \n",
      "     Validation Step: 41 Validation Loss: 0.6121917963027954 \n",
      "     Validation Step: 42 Validation Loss: 0.6146844625473022 \n",
      "     Validation Step: 43 Validation Loss: 0.6102429628372192 \n",
      "     Validation Step: 44 Validation Loss: 0.6136710047721863 \n",
      "Epoch: 12\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6106888651847839 \n",
      "     Training Step: 1 Training Loss: 0.6128795742988586 \n",
      "     Training Step: 2 Training Loss: 0.6116468906402588 \n",
      "     Training Step: 3 Training Loss: 0.6158294081687927 \n",
      "     Training Step: 4 Training Loss: 0.6153574585914612 \n",
      "     Training Step: 5 Training Loss: 0.6124217510223389 \n",
      "     Training Step: 6 Training Loss: 0.6116310358047485 \n",
      "     Training Step: 7 Training Loss: 0.6146560311317444 \n",
      "     Training Step: 8 Training Loss: 0.6134374141693115 \n",
      "     Training Step: 9 Training Loss: 0.6158236861228943 \n",
      "     Training Step: 10 Training Loss: 0.6194933652877808 \n",
      "     Training Step: 11 Training Loss: 0.6126500964164734 \n",
      "     Training Step: 12 Training Loss: 0.6162251830101013 \n",
      "     Training Step: 13 Training Loss: 0.614133894443512 \n",
      "     Training Step: 14 Training Loss: 0.6164641976356506 \n",
      "     Training Step: 15 Training Loss: 0.6167579889297485 \n",
      "     Training Step: 16 Training Loss: 0.6117216348648071 \n",
      "     Training Step: 17 Training Loss: 0.6098816990852356 \n",
      "     Training Step: 18 Training Loss: 0.6135755777359009 \n",
      "     Training Step: 19 Training Loss: 0.613502025604248 \n",
      "     Training Step: 20 Training Loss: 0.6116483211517334 \n",
      "     Training Step: 21 Training Loss: 0.617264449596405 \n",
      "     Training Step: 22 Training Loss: 0.6115909218788147 \n",
      "     Training Step: 23 Training Loss: 0.6154130697250366 \n",
      "     Training Step: 24 Training Loss: 0.6102815866470337 \n",
      "     Training Step: 25 Training Loss: 0.6108320951461792 \n",
      "     Training Step: 26 Training Loss: 0.610825777053833 \n",
      "     Training Step: 27 Training Loss: 0.6115938425064087 \n",
      "     Training Step: 28 Training Loss: 0.6100311279296875 \n",
      "     Training Step: 29 Training Loss: 0.6131265163421631 \n",
      "     Training Step: 30 Training Loss: 0.6120405197143555 \n",
      "     Training Step: 31 Training Loss: 0.6146688461303711 \n",
      "     Training Step: 32 Training Loss: 0.6154850125312805 \n",
      "     Training Step: 33 Training Loss: 0.6102595925331116 \n",
      "     Training Step: 34 Training Loss: 0.6135872602462769 \n",
      "     Training Step: 35 Training Loss: 0.6152003407478333 \n",
      "     Training Step: 36 Training Loss: 0.6164196133613586 \n",
      "     Training Step: 37 Training Loss: 0.6199000477790833 \n",
      "     Training Step: 38 Training Loss: 0.617737889289856 \n",
      "     Training Step: 39 Training Loss: 0.6141817569732666 \n",
      "     Training Step: 40 Training Loss: 0.6141840815544128 \n",
      "     Training Step: 41 Training Loss: 0.609904944896698 \n",
      "     Training Step: 42 Training Loss: 0.6119544506072998 \n",
      "     Training Step: 43 Training Loss: 0.6115318536758423 \n",
      "     Training Step: 44 Training Loss: 0.6160311698913574 \n",
      "     Training Step: 45 Training Loss: 0.6174802184104919 \n",
      "     Training Step: 46 Training Loss: 0.6130374073982239 \n",
      "     Training Step: 47 Training Loss: 0.6172916889190674 \n",
      "     Training Step: 48 Training Loss: 0.610777735710144 \n",
      "     Training Step: 49 Training Loss: 0.6167163848876953 \n",
      "     Training Step: 50 Training Loss: 0.6148474216461182 \n",
      "     Training Step: 51 Training Loss: 0.6085697412490845 \n",
      "     Training Step: 52 Training Loss: 0.6142119765281677 \n",
      "     Training Step: 53 Training Loss: 0.614433765411377 \n",
      "     Training Step: 54 Training Loss: 0.6093180179595947 \n",
      "     Training Step: 55 Training Loss: 0.6153340935707092 \n",
      "     Training Step: 56 Training Loss: 0.6102023720741272 \n",
      "     Training Step: 57 Training Loss: 0.6169840097427368 \n",
      "     Training Step: 58 Training Loss: 0.6106542944908142 \n",
      "     Training Step: 59 Training Loss: 0.6153818368911743 \n",
      "     Training Step: 60 Training Loss: 0.6133843064308167 \n",
      "     Training Step: 61 Training Loss: 0.6158186793327332 \n",
      "     Training Step: 62 Training Loss: 0.6123977899551392 \n",
      "     Training Step: 63 Training Loss: 0.6181026697158813 \n",
      "     Training Step: 64 Training Loss: 0.614195704460144 \n",
      "     Training Step: 65 Training Loss: 0.615321159362793 \n",
      "     Training Step: 66 Training Loss: 0.6122292876243591 \n",
      "     Training Step: 67 Training Loss: 0.6119993329048157 \n",
      "     Training Step: 68 Training Loss: 0.6156776547431946 \n",
      "     Training Step: 69 Training Loss: 0.6199426651000977 \n",
      "     Training Step: 70 Training Loss: 0.6126120090484619 \n",
      "     Training Step: 71 Training Loss: 0.6176162958145142 \n",
      "     Training Step: 72 Training Loss: 0.6114102005958557 \n",
      "     Training Step: 73 Training Loss: 0.6127106547355652 \n",
      "     Training Step: 74 Training Loss: 0.6119502782821655 \n",
      "     Training Step: 75 Training Loss: 0.6125121712684631 \n",
      "     Training Step: 76 Training Loss: 0.6148245334625244 \n",
      "     Training Step: 77 Training Loss: 0.6125926971435547 \n",
      "     Training Step: 78 Training Loss: 0.614482581615448 \n",
      "     Training Step: 79 Training Loss: 0.6148366928100586 \n",
      "     Training Step: 80 Training Loss: 0.6175606846809387 \n",
      "     Training Step: 81 Training Loss: 0.6168570518493652 \n",
      "     Training Step: 82 Training Loss: 0.6139683723449707 \n",
      "     Training Step: 83 Training Loss: 0.6209278702735901 \n",
      "     Training Step: 84 Training Loss: 0.6168264746665955 \n",
      "     Training Step: 85 Training Loss: 0.6144934892654419 \n",
      "     Training Step: 86 Training Loss: 0.6128784418106079 \n",
      "     Training Step: 87 Training Loss: 0.6178212761878967 \n",
      "     Training Step: 88 Training Loss: 0.6162241101264954 \n",
      "     Training Step: 89 Training Loss: 0.6105740070343018 \n",
      "     Training Step: 90 Training Loss: 0.6109404563903809 \n",
      "     Training Step: 91 Training Loss: 0.6132106781005859 \n",
      "     Training Step: 92 Training Loss: 0.6149857044219971 \n",
      "     Training Step: 93 Training Loss: 0.6123492121696472 \n",
      "     Training Step: 94 Training Loss: 0.6136426329612732 \n",
      "     Training Step: 95 Training Loss: 0.6116109490394592 \n",
      "     Training Step: 96 Training Loss: 0.6123082637786865 \n",
      "     Training Step: 97 Training Loss: 0.6184471249580383 \n",
      "     Training Step: 98 Training Loss: 0.6135634779930115 \n",
      "     Training Step: 99 Training Loss: 0.618658721446991 \n",
      "     Training Step: 100 Training Loss: 0.6152136921882629 \n",
      "     Training Step: 101 Training Loss: 0.6125239729881287 \n",
      "     Training Step: 102 Training Loss: 0.6146716475486755 \n",
      "     Training Step: 103 Training Loss: 0.6131587624549866 \n",
      "     Training Step: 104 Training Loss: 0.6116841435432434 \n",
      "     Training Step: 105 Training Loss: 0.6199347376823425 \n",
      "     Training Step: 106 Training Loss: 0.6095293760299683 \n",
      "     Training Step: 107 Training Loss: 0.6155009269714355 \n",
      "     Training Step: 108 Training Loss: 0.6117534041404724 \n",
      "     Training Step: 109 Training Loss: 0.6112647652626038 \n",
      "     Training Step: 110 Training Loss: 0.6140820980072021 \n",
      "     Training Step: 111 Training Loss: 0.6147884726524353 \n",
      "     Training Step: 112 Training Loss: 0.6125921010971069 \n",
      "     Training Step: 113 Training Loss: 0.6163557767868042 \n",
      "     Training Step: 114 Training Loss: 0.6123123168945312 \n",
      "     Training Step: 115 Training Loss: 0.6144993901252747 \n",
      "     Training Step: 116 Training Loss: 0.610552966594696 \n",
      "     Training Step: 117 Training Loss: 0.614843487739563 \n",
      "     Training Step: 118 Training Loss: 0.6107615828514099 \n",
      "     Training Step: 119 Training Loss: 0.6148399710655212 \n",
      "     Training Step: 120 Training Loss: 0.6147764921188354 \n",
      "     Training Step: 121 Training Loss: 0.6162784695625305 \n",
      "     Training Step: 122 Training Loss: 0.611638069152832 \n",
      "     Training Step: 123 Training Loss: 0.6116195321083069 \n",
      "     Training Step: 124 Training Loss: 0.613014280796051 \n",
      "     Training Step: 125 Training Loss: 0.6169446110725403 \n",
      "     Training Step: 126 Training Loss: 0.6175088286399841 \n",
      "     Training Step: 127 Training Loss: 0.6133390069007874 \n",
      "     Training Step: 128 Training Loss: 0.610007643699646 \n",
      "     Training Step: 129 Training Loss: 0.6178786158561707 \n",
      "     Training Step: 130 Training Loss: 0.616044819355011 \n",
      "     Training Step: 131 Training Loss: 0.6106888055801392 \n",
      "     Training Step: 132 Training Loss: 0.6157605051994324 \n",
      "     Training Step: 133 Training Loss: 0.6139944791793823 \n",
      "     Training Step: 134 Training Loss: 0.6186495423316956 \n",
      "     Training Step: 135 Training Loss: 0.6107804775238037 \n",
      "     Training Step: 136 Training Loss: 0.6129048466682434 \n",
      "     Training Step: 137 Training Loss: 0.6153423190116882 \n",
      "     Training Step: 138 Training Loss: 0.6156149506568909 \n",
      "     Training Step: 139 Training Loss: 0.6131513714790344 \n",
      "     Training Step: 140 Training Loss: 0.614709734916687 \n",
      "     Training Step: 141 Training Loss: 0.6147347092628479 \n",
      "     Training Step: 142 Training Loss: 0.6095654964447021 \n",
      "     Training Step: 143 Training Loss: 0.618344247341156 \n",
      "     Training Step: 144 Training Loss: 0.6167182326316833 \n",
      "     Training Step: 145 Training Loss: 0.6184918284416199 \n",
      "     Training Step: 146 Training Loss: 0.6165223121643066 \n",
      "     Training Step: 147 Training Loss: 0.6148117780685425 \n",
      "     Training Step: 148 Training Loss: 0.6129375100135803 \n",
      "     Training Step: 149 Training Loss: 0.6160610914230347 \n",
      "     Training Step: 150 Training Loss: 0.6138640642166138 \n",
      "     Training Step: 151 Training Loss: 0.6108202934265137 \n",
      "     Training Step: 152 Training Loss: 0.610933780670166 \n",
      "     Training Step: 153 Training Loss: 0.6168811321258545 \n",
      "     Training Step: 154 Training Loss: 0.613598108291626 \n",
      "     Training Step: 155 Training Loss: 0.6130200624465942 \n",
      "     Training Step: 156 Training Loss: 0.6182911992073059 \n",
      "     Training Step: 157 Training Loss: 0.6140071153640747 \n",
      "     Training Step: 158 Training Loss: 0.6122884750366211 \n",
      "     Training Step: 159 Training Loss: 0.6169912219047546 \n",
      "     Training Step: 160 Training Loss: 0.617209792137146 \n",
      "     Training Step: 161 Training Loss: 0.6177818775177002 \n",
      "     Training Step: 162 Training Loss: 0.6178075075149536 \n",
      "     Training Step: 163 Training Loss: 0.6137658357620239 \n",
      "     Training Step: 164 Training Loss: 0.6121139526367188 \n",
      "     Training Step: 165 Training Loss: 0.613318145275116 \n",
      "     Training Step: 166 Training Loss: 0.6155735850334167 \n",
      "     Training Step: 167 Training Loss: 0.6148617267608643 \n",
      "     Training Step: 168 Training Loss: 0.6161297559738159 \n",
      "     Training Step: 169 Training Loss: 0.614555299282074 \n",
      "     Training Step: 170 Training Loss: 0.6183453798294067 \n",
      "     Training Step: 171 Training Loss: 0.6137909293174744 \n",
      "     Training Step: 172 Training Loss: 0.6138079166412354 \n",
      "     Training Step: 173 Training Loss: 0.6156372427940369 \n",
      "     Training Step: 174 Training Loss: 0.6127468347549438 \n",
      "     Training Step: 175 Training Loss: 0.6169009804725647 \n",
      "     Training Step: 176 Training Loss: 0.6115073561668396 \n",
      "     Training Step: 177 Training Loss: 0.6138585805892944 \n",
      "     Training Step: 178 Training Loss: 0.615381121635437 \n",
      "     Training Step: 179 Training Loss: 0.6129142642021179 \n",
      "     Training Step: 180 Training Loss: 0.6122817397117615 \n",
      "     Training Step: 181 Training Loss: 0.61594557762146 \n",
      "     Training Step: 182 Training Loss: 0.6142669916152954 \n",
      "     Training Step: 183 Training Loss: 0.6122725009918213 \n",
      "     Training Step: 184 Training Loss: 0.6143817901611328 \n",
      "     Training Step: 185 Training Loss: 0.6168583631515503 \n",
      "     Training Step: 186 Training Loss: 0.6150365471839905 \n",
      "     Training Step: 187 Training Loss: 0.6148208379745483 \n",
      "     Training Step: 188 Training Loss: 0.6202547550201416 \n",
      "     Training Step: 189 Training Loss: 0.6145105957984924 \n",
      "     Training Step: 190 Training Loss: 0.6151499152183533 \n",
      "     Training Step: 191 Training Loss: 0.6132797002792358 \n",
      "     Training Step: 192 Training Loss: 0.6146871447563171 \n",
      "     Training Step: 193 Training Loss: 0.6106457114219666 \n",
      "     Training Step: 194 Training Loss: 0.613416314125061 \n",
      "     Training Step: 195 Training Loss: 0.6146230697631836 \n",
      "     Training Step: 196 Training Loss: 0.6190910339355469 \n",
      "     Training Step: 197 Training Loss: 0.6181120276451111 \n",
      "     Training Step: 198 Training Loss: 0.615867018699646 \n",
      "     Training Step: 199 Training Loss: 0.6144609451293945 \n",
      "     Training Step: 200 Training Loss: 0.6115714311599731 \n",
      "     Training Step: 201 Training Loss: 0.6153930425643921 \n",
      "     Training Step: 202 Training Loss: 0.6185876131057739 \n",
      "     Training Step: 203 Training Loss: 0.6101407408714294 \n",
      "     Training Step: 204 Training Loss: 0.616946280002594 \n",
      "     Training Step: 205 Training Loss: 0.6170666217803955 \n",
      "     Training Step: 206 Training Loss: 0.6134498715400696 \n",
      "     Training Step: 207 Training Loss: 0.6143476963043213 \n",
      "     Training Step: 208 Training Loss: 0.6189316511154175 \n",
      "     Training Step: 209 Training Loss: 0.6152790784835815 \n",
      "     Training Step: 210 Training Loss: 0.6120062470436096 \n",
      "     Training Step: 211 Training Loss: 0.6104076504707336 \n",
      "     Training Step: 212 Training Loss: 0.6149839758872986 \n",
      "     Training Step: 213 Training Loss: 0.6178101301193237 \n",
      "     Training Step: 214 Training Loss: 0.6168211698532104 \n",
      "     Training Step: 215 Training Loss: 0.6150028109550476 \n",
      "     Training Step: 216 Training Loss: 0.6156101822853088 \n",
      "     Training Step: 217 Training Loss: 0.6171679496765137 \n",
      "     Training Step: 218 Training Loss: 0.6181060671806335 \n",
      "     Training Step: 219 Training Loss: 0.6167646050453186 \n",
      "     Training Step: 220 Training Loss: 0.6142800450325012 \n",
      "     Training Step: 221 Training Loss: 0.6128840446472168 \n",
      "     Training Step: 222 Training Loss: 0.6155057549476624 \n",
      "     Training Step: 223 Training Loss: 0.6133611798286438 \n",
      "     Training Step: 224 Training Loss: 0.6118872761726379 \n",
      "     Training Step: 225 Training Loss: 0.6134142875671387 \n",
      "     Training Step: 226 Training Loss: 0.6165883541107178 \n",
      "     Training Step: 227 Training Loss: 0.612120509147644 \n",
      "     Training Step: 228 Training Loss: 0.6154223680496216 \n",
      "     Training Step: 229 Training Loss: 0.6142085194587708 \n",
      "     Training Step: 230 Training Loss: 0.613254725933075 \n",
      "     Training Step: 231 Training Loss: 0.6117714643478394 \n",
      "     Training Step: 232 Training Loss: 0.6138509511947632 \n",
      "     Training Step: 233 Training Loss: 0.6155338883399963 \n",
      "     Training Step: 234 Training Loss: 0.6118956208229065 \n",
      "     Training Step: 235 Training Loss: 0.6114846467971802 \n",
      "     Training Step: 236 Training Loss: 0.6183176636695862 \n",
      "     Training Step: 237 Training Loss: 0.6121882796287537 \n",
      "     Training Step: 238 Training Loss: 0.6143990755081177 \n",
      "     Training Step: 239 Training Loss: 0.611813485622406 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6107902526855469 \n",
      "     Validation Step: 1 Validation Loss: 0.6137555837631226 \n",
      "     Validation Step: 2 Validation Loss: 0.6137219071388245 \n",
      "     Validation Step: 3 Validation Loss: 0.6113003492355347 \n",
      "     Validation Step: 4 Validation Loss: 0.6148974895477295 \n",
      "     Validation Step: 5 Validation Loss: 0.6149598956108093 \n",
      "     Validation Step: 6 Validation Loss: 0.6170307993888855 \n",
      "     Validation Step: 7 Validation Loss: 0.616042971611023 \n",
      "     Validation Step: 8 Validation Loss: 0.6120158433914185 \n",
      "     Validation Step: 9 Validation Loss: 0.6153753995895386 \n",
      "     Validation Step: 10 Validation Loss: 0.6173052191734314 \n",
      "     Validation Step: 11 Validation Loss: 0.610321044921875 \n",
      "     Validation Step: 12 Validation Loss: 0.6184730529785156 \n",
      "     Validation Step: 13 Validation Loss: 0.6106517910957336 \n",
      "     Validation Step: 14 Validation Loss: 0.6183595657348633 \n",
      "     Validation Step: 15 Validation Loss: 0.6176037192344666 \n",
      "     Validation Step: 16 Validation Loss: 0.610319197177887 \n",
      "     Validation Step: 17 Validation Loss: 0.6133697628974915 \n",
      "     Validation Step: 18 Validation Loss: 0.6152710914611816 \n",
      "     Validation Step: 19 Validation Loss: 0.6145991086959839 \n",
      "     Validation Step: 20 Validation Loss: 0.6136945486068726 \n",
      "     Validation Step: 21 Validation Loss: 0.6142029166221619 \n",
      "     Validation Step: 22 Validation Loss: 0.614214301109314 \n",
      "     Validation Step: 23 Validation Loss: 0.6177085041999817 \n",
      "     Validation Step: 24 Validation Loss: 0.6150968074798584 \n",
      "     Validation Step: 25 Validation Loss: 0.6142635941505432 \n",
      "     Validation Step: 26 Validation Loss: 0.611758828163147 \n",
      "     Validation Step: 27 Validation Loss: 0.607776939868927 \n",
      "     Validation Step: 28 Validation Loss: 0.6143173575401306 \n",
      "     Validation Step: 29 Validation Loss: 0.618487536907196 \n",
      "     Validation Step: 30 Validation Loss: 0.613074004650116 \n",
      "     Validation Step: 31 Validation Loss: 0.6122525334358215 \n",
      "     Validation Step: 32 Validation Loss: 0.6112953424453735 \n",
      "     Validation Step: 33 Validation Loss: 0.6103372573852539 \n",
      "     Validation Step: 34 Validation Loss: 0.6147026419639587 \n",
      "     Validation Step: 35 Validation Loss: 0.6182524561882019 \n",
      "     Validation Step: 36 Validation Loss: 0.615606427192688 \n",
      "     Validation Step: 37 Validation Loss: 0.6162963509559631 \n",
      "     Validation Step: 38 Validation Loss: 0.6146105527877808 \n",
      "     Validation Step: 39 Validation Loss: 0.615846574306488 \n",
      "     Validation Step: 40 Validation Loss: 0.6129181981086731 \n",
      "     Validation Step: 41 Validation Loss: 0.6156582236289978 \n",
      "     Validation Step: 42 Validation Loss: 0.6180738210678101 \n",
      "     Validation Step: 43 Validation Loss: 0.6116800904273987 \n",
      "     Validation Step: 44 Validation Loss: 0.6106265187263489 \n",
      "Epoch: 13\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6162992715835571 \n",
      "     Training Step: 1 Training Loss: 0.6142350435256958 \n",
      "     Training Step: 2 Training Loss: 0.617838442325592 \n",
      "     Training Step: 3 Training Loss: 0.6166955232620239 \n",
      "     Training Step: 4 Training Loss: 0.6132996678352356 \n",
      "     Training Step: 5 Training Loss: 0.6136460900306702 \n",
      "     Training Step: 6 Training Loss: 0.6147121787071228 \n",
      "     Training Step: 7 Training Loss: 0.6105547547340393 \n",
      "     Training Step: 8 Training Loss: 0.618898868560791 \n",
      "     Training Step: 9 Training Loss: 0.6140846014022827 \n",
      "     Training Step: 10 Training Loss: 0.6147528290748596 \n",
      "     Training Step: 11 Training Loss: 0.6138123869895935 \n",
      "     Training Step: 12 Training Loss: 0.6161487698554993 \n",
      "     Training Step: 13 Training Loss: 0.6168212294578552 \n",
      "     Training Step: 14 Training Loss: 0.610722541809082 \n",
      "     Training Step: 15 Training Loss: 0.6184661388397217 \n",
      "     Training Step: 16 Training Loss: 0.616795539855957 \n",
      "     Training Step: 17 Training Loss: 0.6128861308097839 \n",
      "     Training Step: 18 Training Loss: 0.6119283437728882 \n",
      "     Training Step: 19 Training Loss: 0.6123173832893372 \n",
      "     Training Step: 20 Training Loss: 0.6127572059631348 \n",
      "     Training Step: 21 Training Loss: 0.6116824150085449 \n",
      "     Training Step: 22 Training Loss: 0.6125864386558533 \n",
      "     Training Step: 23 Training Loss: 0.6123265624046326 \n",
      "     Training Step: 24 Training Loss: 0.6155053973197937 \n",
      "     Training Step: 25 Training Loss: 0.6129617691040039 \n",
      "     Training Step: 26 Training Loss: 0.6177073121070862 \n",
      "     Training Step: 27 Training Loss: 0.6152036190032959 \n",
      "     Training Step: 28 Training Loss: 0.615584135055542 \n",
      "     Training Step: 29 Training Loss: 0.6117626428604126 \n",
      "     Training Step: 30 Training Loss: 0.6107238531112671 \n",
      "     Training Step: 31 Training Loss: 0.6122609972953796 \n",
      "     Training Step: 32 Training Loss: 0.6168783903121948 \n",
      "     Training Step: 33 Training Loss: 0.6148892045021057 \n",
      "     Training Step: 34 Training Loss: 0.6115296483039856 \n",
      "     Training Step: 35 Training Loss: 0.6170784831047058 \n",
      "     Training Step: 36 Training Loss: 0.6148256659507751 \n",
      "     Training Step: 37 Training Loss: 0.6124873161315918 \n",
      "     Training Step: 38 Training Loss: 0.6154295802116394 \n",
      "     Training Step: 39 Training Loss: 0.6185061931610107 \n",
      "     Training Step: 40 Training Loss: 0.6119529008865356 \n",
      "     Training Step: 41 Training Loss: 0.6101171970367432 \n",
      "     Training Step: 42 Training Loss: 0.6118959784507751 \n",
      "     Training Step: 43 Training Loss: 0.6136130690574646 \n",
      "     Training Step: 44 Training Loss: 0.615925669670105 \n",
      "     Training Step: 45 Training Loss: 0.6107551455497742 \n",
      "     Training Step: 46 Training Loss: 0.6141090989112854 \n",
      "     Training Step: 47 Training Loss: 0.6159741282463074 \n",
      "     Training Step: 48 Training Loss: 0.6160703301429749 \n",
      "     Training Step: 49 Training Loss: 0.6145383715629578 \n",
      "     Training Step: 50 Training Loss: 0.612342119216919 \n",
      "     Training Step: 51 Training Loss: 0.6157485246658325 \n",
      "     Training Step: 52 Training Loss: 0.6137114763259888 \n",
      "     Training Step: 53 Training Loss: 0.6135983467102051 \n",
      "     Training Step: 54 Training Loss: 0.6155154705047607 \n",
      "     Training Step: 55 Training Loss: 0.6167669296264648 \n",
      "     Training Step: 56 Training Loss: 0.6178886294364929 \n",
      "     Training Step: 57 Training Loss: 0.612689733505249 \n",
      "     Training Step: 58 Training Loss: 0.614203155040741 \n",
      "     Training Step: 59 Training Loss: 0.6162564754486084 \n",
      "     Training Step: 60 Training Loss: 0.6155735850334167 \n",
      "     Training Step: 61 Training Loss: 0.615651547908783 \n",
      "     Training Step: 62 Training Loss: 0.6196924448013306 \n",
      "     Training Step: 63 Training Loss: 0.6106488108634949 \n",
      "     Training Step: 64 Training Loss: 0.6202375888824463 \n",
      "     Training Step: 65 Training Loss: 0.6102417707443237 \n",
      "     Training Step: 66 Training Loss: 0.6102424263954163 \n",
      "     Training Step: 67 Training Loss: 0.6130978465080261 \n",
      "     Training Step: 68 Training Loss: 0.6183263659477234 \n",
      "     Training Step: 69 Training Loss: 0.615028977394104 \n",
      "     Training Step: 70 Training Loss: 0.6148027777671814 \n",
      "     Training Step: 71 Training Loss: 0.6132434606552124 \n",
      "     Training Step: 72 Training Loss: 0.6194396018981934 \n",
      "     Training Step: 73 Training Loss: 0.6182429194450378 \n",
      "     Training Step: 74 Training Loss: 0.6133562922477722 \n",
      "     Training Step: 75 Training Loss: 0.6132680177688599 \n",
      "     Training Step: 76 Training Loss: 0.6155034303665161 \n",
      "     Training Step: 77 Training Loss: 0.6167604327201843 \n",
      "     Training Step: 78 Training Loss: 0.6185067892074585 \n",
      "     Training Step: 79 Training Loss: 0.6167330145835876 \n",
      "     Training Step: 80 Training Loss: 0.6188920736312866 \n",
      "     Training Step: 81 Training Loss: 0.6147860288619995 \n",
      "     Training Step: 82 Training Loss: 0.6128442883491516 \n",
      "     Training Step: 83 Training Loss: 0.6152074933052063 \n",
      "     Training Step: 84 Training Loss: 0.6145352125167847 \n",
      "     Training Step: 85 Training Loss: 0.6168495416641235 \n",
      "     Training Step: 86 Training Loss: 0.6162871718406677 \n",
      "     Training Step: 87 Training Loss: 0.6145398616790771 \n",
      "     Training Step: 88 Training Loss: 0.6181724667549133 \n",
      "     Training Step: 89 Training Loss: 0.6123230457305908 \n",
      "     Training Step: 90 Training Loss: 0.6152758002281189 \n",
      "     Training Step: 91 Training Loss: 0.6172441840171814 \n",
      "     Training Step: 92 Training Loss: 0.6165575385093689 \n",
      "     Training Step: 93 Training Loss: 0.6125002503395081 \n",
      "     Training Step: 94 Training Loss: 0.6117558479309082 \n",
      "     Training Step: 95 Training Loss: 0.6147210001945496 \n",
      "     Training Step: 96 Training Loss: 0.6136863231658936 \n",
      "     Training Step: 97 Training Loss: 0.6161893010139465 \n",
      "     Training Step: 98 Training Loss: 0.6141185164451599 \n",
      "     Training Step: 99 Training Loss: 0.609833300113678 \n",
      "     Training Step: 100 Training Loss: 0.6188952922821045 \n",
      "     Training Step: 101 Training Loss: 0.6119598150253296 \n",
      "     Training Step: 102 Training Loss: 0.6144310235977173 \n",
      "     Training Step: 103 Training Loss: 0.6155449151992798 \n",
      "     Training Step: 104 Training Loss: 0.6112491488456726 \n",
      "     Training Step: 105 Training Loss: 0.6169503927230835 \n",
      "     Training Step: 106 Training Loss: 0.6129851937294006 \n",
      "     Training Step: 107 Training Loss: 0.6115979552268982 \n",
      "     Training Step: 108 Training Loss: 0.6178422570228577 \n",
      "     Training Step: 109 Training Loss: 0.6150071024894714 \n",
      "     Training Step: 110 Training Loss: 0.6119104623794556 \n",
      "     Training Step: 111 Training Loss: 0.6153209209442139 \n",
      "     Training Step: 112 Training Loss: 0.6102212071418762 \n",
      "     Training Step: 113 Training Loss: 0.61341392993927 \n",
      "     Training Step: 114 Training Loss: 0.6143548488616943 \n",
      "     Training Step: 115 Training Loss: 0.6151452660560608 \n",
      "     Training Step: 116 Training Loss: 0.6101958751678467 \n",
      "     Training Step: 117 Training Loss: 0.6172225475311279 \n",
      "     Training Step: 118 Training Loss: 0.6147658228874207 \n",
      "     Training Step: 119 Training Loss: 0.6119146943092346 \n",
      "     Training Step: 120 Training Loss: 0.6114734411239624 \n",
      "     Training Step: 121 Training Loss: 0.6153616905212402 \n",
      "     Training Step: 122 Training Loss: 0.6129599213600159 \n",
      "     Training Step: 123 Training Loss: 0.6124081611633301 \n",
      "     Training Step: 124 Training Loss: 0.6112185716629028 \n",
      "     Training Step: 125 Training Loss: 0.618672788143158 \n",
      "     Training Step: 126 Training Loss: 0.6129058599472046 \n",
      "     Training Step: 127 Training Loss: 0.6119805574417114 \n",
      "     Training Step: 128 Training Loss: 0.615726113319397 \n",
      "     Training Step: 129 Training Loss: 0.6145283579826355 \n",
      "     Training Step: 130 Training Loss: 0.6139679551124573 \n",
      "     Training Step: 131 Training Loss: 0.6125831007957458 \n",
      "     Training Step: 132 Training Loss: 0.6117392182350159 \n",
      "     Training Step: 133 Training Loss: 0.6106146574020386 \n",
      "     Training Step: 134 Training Loss: 0.6153370141983032 \n",
      "     Training Step: 135 Training Loss: 0.6154857277870178 \n",
      "     Training Step: 136 Training Loss: 0.6135469675064087 \n",
      "     Training Step: 137 Training Loss: 0.6115396022796631 \n",
      "     Training Step: 138 Training Loss: 0.6107112765312195 \n",
      "     Training Step: 139 Training Loss: 0.6197327375411987 \n",
      "     Training Step: 140 Training Loss: 0.6176257133483887 \n",
      "     Training Step: 141 Training Loss: 0.6116758584976196 \n",
      "     Training Step: 142 Training Loss: 0.6164445877075195 \n",
      "     Training Step: 143 Training Loss: 0.6096452474594116 \n",
      "     Training Step: 144 Training Loss: 0.6139297485351562 \n",
      "     Training Step: 145 Training Loss: 0.6092681288719177 \n",
      "     Training Step: 146 Training Loss: 0.612501323223114 \n",
      "     Training Step: 147 Training Loss: 0.6116510629653931 \n",
      "     Training Step: 148 Training Loss: 0.6142995357513428 \n",
      "     Training Step: 149 Training Loss: 0.6155626177787781 \n",
      "     Training Step: 150 Training Loss: 0.6135717630386353 \n",
      "     Training Step: 151 Training Loss: 0.6183279156684875 \n",
      "     Training Step: 152 Training Loss: 0.6137685179710388 \n",
      "     Training Step: 153 Training Loss: 0.6168997883796692 \n",
      "     Training Step: 154 Training Loss: 0.614970862865448 \n",
      "     Training Step: 155 Training Loss: 0.6143459677696228 \n",
      "     Training Step: 156 Training Loss: 0.6144872903823853 \n",
      "     Training Step: 157 Training Loss: 0.6153836846351624 \n",
      "     Training Step: 158 Training Loss: 0.616564154624939 \n",
      "     Training Step: 159 Training Loss: 0.6129168272018433 \n",
      "     Training Step: 160 Training Loss: 0.6133865714073181 \n",
      "     Training Step: 161 Training Loss: 0.6167788505554199 \n",
      "     Training Step: 162 Training Loss: 0.6132677793502808 \n",
      "     Training Step: 163 Training Loss: 0.6146841645240784 \n",
      "     Training Step: 164 Training Loss: 0.6085347533226013 \n",
      "     Training Step: 165 Training Loss: 0.6172350645065308 \n",
      "     Training Step: 166 Training Loss: 0.613353431224823 \n",
      "     Training Step: 167 Training Loss: 0.617600679397583 \n",
      "     Training Step: 168 Training Loss: 0.6106785535812378 \n",
      "     Training Step: 169 Training Loss: 0.6160038709640503 \n",
      "     Training Step: 170 Training Loss: 0.6112713813781738 \n",
      "     Training Step: 171 Training Loss: 0.6157715320587158 \n",
      "     Training Step: 172 Training Loss: 0.6155135035514832 \n",
      "     Training Step: 173 Training Loss: 0.6155794858932495 \n",
      "     Training Step: 174 Training Loss: 0.6116064786911011 \n",
      "     Training Step: 175 Training Loss: 0.6163700819015503 \n",
      "     Training Step: 176 Training Loss: 0.6133425235748291 \n",
      "     Training Step: 177 Training Loss: 0.6174548268318176 \n",
      "     Training Step: 178 Training Loss: 0.6168234348297119 \n",
      "     Training Step: 179 Training Loss: 0.6158057451248169 \n",
      "     Training Step: 180 Training Loss: 0.6143110394477844 \n",
      "     Training Step: 181 Training Loss: 0.6149136424064636 \n",
      "     Training Step: 182 Training Loss: 0.6099256277084351 \n",
      "     Training Step: 183 Training Loss: 0.61818528175354 \n",
      "     Training Step: 184 Training Loss: 0.6125902533531189 \n",
      "     Training Step: 185 Training Loss: 0.6133797764778137 \n",
      "     Training Step: 186 Training Loss: 0.6095044612884521 \n",
      "     Training Step: 187 Training Loss: 0.6172357201576233 \n",
      "     Training Step: 188 Training Loss: 0.6138690114021301 \n",
      "     Training Step: 189 Training Loss: 0.6143831610679626 \n",
      "     Training Step: 190 Training Loss: 0.6141263246536255 \n",
      "     Training Step: 191 Training Loss: 0.6177855134010315 \n",
      "     Training Step: 192 Training Loss: 0.6117026209831238 \n",
      "     Training Step: 193 Training Loss: 0.6138490438461304 \n",
      "     Training Step: 194 Training Loss: 0.6110123991966248 \n",
      "     Training Step: 195 Training Loss: 0.6147915124893188 \n",
      "     Training Step: 196 Training Loss: 0.6202576160430908 \n",
      "     Training Step: 197 Training Loss: 0.6108381152153015 \n",
      "     Training Step: 198 Training Loss: 0.6099276542663574 \n",
      "     Training Step: 199 Training Loss: 0.6148292422294617 \n",
      "     Training Step: 200 Training Loss: 0.6211195588111877 \n",
      "     Training Step: 201 Training Loss: 0.6130563616752625 \n",
      "     Training Step: 202 Training Loss: 0.614021360874176 \n",
      "     Training Step: 203 Training Loss: 0.6118310689926147 \n",
      "     Training Step: 204 Training Loss: 0.61826092004776 \n",
      "     Training Step: 205 Training Loss: 0.6146999597549438 \n",
      "     Training Step: 206 Training Loss: 0.6123654246330261 \n",
      "     Training Step: 207 Training Loss: 0.6104444265365601 \n",
      "     Training Step: 208 Training Loss: 0.6101254820823669 \n",
      "     Training Step: 209 Training Loss: 0.6169975399971008 \n",
      "     Training Step: 210 Training Loss: 0.6123040914535522 \n",
      "     Training Step: 211 Training Loss: 0.6145668029785156 \n",
      "     Training Step: 212 Training Loss: 0.6122145652770996 \n",
      "     Training Step: 213 Training Loss: 0.6108551621437073 \n",
      "     Training Step: 214 Training Loss: 0.6131347417831421 \n",
      "     Training Step: 215 Training Loss: 0.6132363080978394 \n",
      "     Training Step: 216 Training Loss: 0.6115095019340515 \n",
      "     Training Step: 217 Training Loss: 0.612835168838501 \n",
      "     Training Step: 218 Training Loss: 0.6121184229850769 \n",
      "     Training Step: 219 Training Loss: 0.6120961904525757 \n",
      "     Training Step: 220 Training Loss: 0.6150332093238831 \n",
      "     Training Step: 221 Training Loss: 0.611696720123291 \n",
      "     Training Step: 222 Training Loss: 0.6133127808570862 \n",
      "     Training Step: 223 Training Loss: 0.6172422170639038 \n",
      "     Training Step: 224 Training Loss: 0.6108238101005554 \n",
      "     Training Step: 225 Training Loss: 0.6141905784606934 \n",
      "     Training Step: 226 Training Loss: 0.6167191863059998 \n",
      "     Training Step: 227 Training Loss: 0.6143960356712341 \n",
      "     Training Step: 228 Training Loss: 0.6158061623573303 \n",
      "     Training Step: 229 Training Loss: 0.611696183681488 \n",
      "     Training Step: 230 Training Loss: 0.613791823387146 \n",
      "     Training Step: 231 Training Loss: 0.6153865456581116 \n",
      "     Training Step: 232 Training Loss: 0.6151386499404907 \n",
      "     Training Step: 233 Training Loss: 0.6177828311920166 \n",
      "     Training Step: 234 Training Loss: 0.6147193312644958 \n",
      "     Training Step: 235 Training Loss: 0.6111125349998474 \n",
      "     Training Step: 236 Training Loss: 0.6147517561912537 \n",
      "     Training Step: 237 Training Loss: 0.6137346029281616 \n",
      "     Training Step: 238 Training Loss: 0.6125601530075073 \n",
      "     Training Step: 239 Training Loss: 0.6182170510292053 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6153632998466492 \n",
      "     Validation Step: 1 Validation Loss: 0.6130391955375671 \n",
      "     Validation Step: 2 Validation Loss: 0.6076249480247498 \n",
      "     Validation Step: 3 Validation Loss: 0.6102262735366821 \n",
      "     Validation Step: 4 Validation Loss: 0.6177572011947632 \n",
      "     Validation Step: 5 Validation Loss: 0.6119408011436462 \n",
      "     Validation Step: 6 Validation Loss: 0.6150926947593689 \n",
      "     Validation Step: 7 Validation Loss: 0.6181213855743408 \n",
      "     Validation Step: 8 Validation Loss: 0.6146804094314575 \n",
      "     Validation Step: 9 Validation Loss: 0.6137300729751587 \n",
      "     Validation Step: 10 Validation Loss: 0.6152797341346741 \n",
      "     Validation Step: 11 Validation Loss: 0.6146224141120911 \n",
      "     Validation Step: 12 Validation Loss: 0.6141715049743652 \n",
      "     Validation Step: 13 Validation Loss: 0.610550582408905 \n",
      "     Validation Step: 14 Validation Loss: 0.6158633828163147 \n",
      "     Validation Step: 15 Validation Loss: 0.6176487803459167 \n",
      "     Validation Step: 16 Validation Loss: 0.6170644164085388 \n",
      "     Validation Step: 17 Validation Loss: 0.6156691908836365 \n",
      "     Validation Step: 18 Validation Loss: 0.6121947169303894 \n",
      "     Validation Step: 19 Validation Loss: 0.6136844754219055 \n",
      "     Validation Step: 20 Validation Loss: 0.610249936580658 \n",
      "     Validation Step: 21 Validation Loss: 0.6148892045021057 \n",
      "     Validation Step: 22 Validation Loss: 0.6105799674987793 \n",
      "     Validation Step: 23 Validation Loss: 0.6101952195167542 \n",
      "     Validation Step: 24 Validation Loss: 0.6185283660888672 \n",
      "     Validation Step: 25 Validation Loss: 0.6142985224723816 \n",
      "     Validation Step: 26 Validation Loss: 0.6117022633552551 \n",
      "     Validation Step: 27 Validation Loss: 0.613341748714447 \n",
      "     Validation Step: 28 Validation Loss: 0.6145819425582886 \n",
      "     Validation Step: 29 Validation Loss: 0.6183010935783386 \n",
      "     Validation Step: 30 Validation Loss: 0.6142486929893494 \n",
      "     Validation Step: 31 Validation Loss: 0.6160653233528137 \n",
      "     Validation Step: 32 Validation Loss: 0.6112229824066162 \n",
      "     Validation Step: 33 Validation Loss: 0.6156413555145264 \n",
      "     Validation Step: 34 Validation Loss: 0.612873375415802 \n",
      "     Validation Step: 35 Validation Loss: 0.618396520614624 \n",
      "     Validation Step: 36 Validation Loss: 0.6149367094039917 \n",
      "     Validation Step: 37 Validation Loss: 0.6116101741790771 \n",
      "     Validation Step: 38 Validation Loss: 0.614173412322998 \n",
      "     Validation Step: 39 Validation Loss: 0.6173637509346008 \n",
      "     Validation Step: 40 Validation Loss: 0.618563711643219 \n",
      "     Validation Step: 41 Validation Loss: 0.6163116097450256 \n",
      "     Validation Step: 42 Validation Loss: 0.6112363934516907 \n",
      "     Validation Step: 43 Validation Loss: 0.6107152700424194 \n",
      "     Validation Step: 44 Validation Loss: 0.6136979460716248 \n",
      "Epoch: 14\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6168694496154785 \n",
      "     Training Step: 1 Training Loss: 0.6108919978141785 \n",
      "     Training Step: 2 Training Loss: 0.6145843267440796 \n",
      "     Training Step: 3 Training Loss: 0.6106780171394348 \n",
      "     Training Step: 4 Training Loss: 0.6147605776786804 \n",
      "     Training Step: 5 Training Loss: 0.6157359480857849 \n",
      "     Training Step: 6 Training Loss: 0.6182262897491455 \n",
      "     Training Step: 7 Training Loss: 0.6176629066467285 \n",
      "     Training Step: 8 Training Loss: 0.6127055883407593 \n",
      "     Training Step: 9 Training Loss: 0.6141979694366455 \n",
      "     Training Step: 10 Training Loss: 0.617137610912323 \n",
      "     Training Step: 11 Training Loss: 0.6159070730209351 \n",
      "     Training Step: 12 Training Loss: 0.6184800863265991 \n",
      "     Training Step: 13 Training Loss: 0.6174218058586121 \n",
      "     Training Step: 14 Training Loss: 0.6115910410881042 \n",
      "     Training Step: 15 Training Loss: 0.6202322244644165 \n",
      "     Training Step: 16 Training Loss: 0.6095966100692749 \n",
      "     Training Step: 17 Training Loss: 0.6123026609420776 \n",
      "     Training Step: 18 Training Loss: 0.6141033172607422 \n",
      "     Training Step: 19 Training Loss: 0.6125755310058594 \n",
      "     Training Step: 20 Training Loss: 0.6154353022575378 \n",
      "     Training Step: 21 Training Loss: 0.611565351486206 \n",
      "     Training Step: 22 Training Loss: 0.6167190074920654 \n",
      "     Training Step: 23 Training Loss: 0.6162263751029968 \n",
      "     Training Step: 24 Training Loss: 0.6138013601303101 \n",
      "     Training Step: 25 Training Loss: 0.6171513795852661 \n",
      "     Training Step: 26 Training Loss: 0.6110903024673462 \n",
      "     Training Step: 27 Training Loss: 0.6153829097747803 \n",
      "     Training Step: 28 Training Loss: 0.6093640923500061 \n",
      "     Training Step: 29 Training Loss: 0.6136919856071472 \n",
      "     Training Step: 30 Training Loss: 0.6154552102088928 \n",
      "     Training Step: 31 Training Loss: 0.6201878190040588 \n",
      "     Training Step: 32 Training Loss: 0.614799439907074 \n",
      "     Training Step: 33 Training Loss: 0.6136817336082458 \n",
      "     Training Step: 34 Training Loss: 0.616554319858551 \n",
      "     Training Step: 35 Training Loss: 0.6132487654685974 \n",
      "     Training Step: 36 Training Loss: 0.616760790348053 \n",
      "     Training Step: 37 Training Loss: 0.6107545495033264 \n",
      "     Training Step: 38 Training Loss: 0.6154797673225403 \n",
      "     Training Step: 39 Training Loss: 0.6107907295227051 \n",
      "     Training Step: 40 Training Loss: 0.6169832348823547 \n",
      "     Training Step: 41 Training Loss: 0.6133822202682495 \n",
      "     Training Step: 42 Training Loss: 0.6116183996200562 \n",
      "     Training Step: 43 Training Loss: 0.6142356991767883 \n",
      "     Training Step: 44 Training Loss: 0.6149500608444214 \n",
      "     Training Step: 45 Training Loss: 0.6182489991188049 \n",
      "     Training Step: 46 Training Loss: 0.6100230813026428 \n",
      "     Training Step: 47 Training Loss: 0.6184365749359131 \n",
      "     Training Step: 48 Training Loss: 0.6146767139434814 \n",
      "     Training Step: 49 Training Loss: 0.6153225302696228 \n",
      "     Training Step: 50 Training Loss: 0.61300128698349 \n",
      "     Training Step: 51 Training Loss: 0.6151129603385925 \n",
      "     Training Step: 52 Training Loss: 0.6157301068305969 \n",
      "     Training Step: 53 Training Loss: 0.6180658340454102 \n",
      "     Training Step: 54 Training Loss: 0.6171958446502686 \n",
      "     Training Step: 55 Training Loss: 0.6109737157821655 \n",
      "     Training Step: 56 Training Loss: 0.6139459609985352 \n",
      "     Training Step: 57 Training Loss: 0.6186565160751343 \n",
      "     Training Step: 58 Training Loss: 0.6147127747535706 \n",
      "     Training Step: 59 Training Loss: 0.6144367456436157 \n",
      "     Training Step: 60 Training Loss: 0.6184409260749817 \n",
      "     Training Step: 61 Training Loss: 0.6125103235244751 \n",
      "     Training Step: 62 Training Loss: 0.6129305958747864 \n",
      "     Training Step: 63 Training Loss: 0.6118922233581543 \n",
      "     Training Step: 64 Training Loss: 0.6134783029556274 \n",
      "     Training Step: 65 Training Loss: 0.6176959276199341 \n",
      "     Training Step: 66 Training Loss: 0.6156549453735352 \n",
      "     Training Step: 67 Training Loss: 0.6147708892822266 \n",
      "     Training Step: 68 Training Loss: 0.6166584491729736 \n",
      "     Training Step: 69 Training Loss: 0.6144983172416687 \n",
      "     Training Step: 70 Training Loss: 0.6162487864494324 \n",
      "     Training Step: 71 Training Loss: 0.61098313331604 \n",
      "     Training Step: 72 Training Loss: 0.6150105595588684 \n",
      "     Training Step: 73 Training Loss: 0.6155324578285217 \n",
      "     Training Step: 74 Training Loss: 0.6101113557815552 \n",
      "     Training Step: 75 Training Loss: 0.6139429211616516 \n",
      "     Training Step: 76 Training Loss: 0.6199718117713928 \n",
      "     Training Step: 77 Training Loss: 0.6148321032524109 \n",
      "     Training Step: 78 Training Loss: 0.611770749092102 \n",
      "     Training Step: 79 Training Loss: 0.6125043034553528 \n",
      "     Training Step: 80 Training Loss: 0.6145218014717102 \n",
      "     Training Step: 81 Training Loss: 0.6158851981163025 \n",
      "     Training Step: 82 Training Loss: 0.6143972873687744 \n",
      "     Training Step: 83 Training Loss: 0.6115114092826843 \n",
      "     Training Step: 84 Training Loss: 0.614894688129425 \n",
      "     Training Step: 85 Training Loss: 0.618196964263916 \n",
      "     Training Step: 86 Training Loss: 0.6135745644569397 \n",
      "     Training Step: 87 Training Loss: 0.6163332462310791 \n",
      "     Training Step: 88 Training Loss: 0.6161177754402161 \n",
      "     Training Step: 89 Training Loss: 0.6119291186332703 \n",
      "     Training Step: 90 Training Loss: 0.6129378080368042 \n",
      "     Training Step: 91 Training Loss: 0.6158571839332581 \n",
      "     Training Step: 92 Training Loss: 0.6182965636253357 \n",
      "     Training Step: 93 Training Loss: 0.6123085618019104 \n",
      "     Training Step: 94 Training Loss: 0.6153389811515808 \n",
      "     Training Step: 95 Training Loss: 0.6102162599563599 \n",
      "     Training Step: 96 Training Loss: 0.6147844195365906 \n",
      "     Training Step: 97 Training Loss: 0.6178889870643616 \n",
      "     Training Step: 98 Training Loss: 0.6131204962730408 \n",
      "     Training Step: 99 Training Loss: 0.6136085391044617 \n",
      "     Training Step: 100 Training Loss: 0.614509642124176 \n",
      "     Training Step: 101 Training Loss: 0.615435004234314 \n",
      "     Training Step: 102 Training Loss: 0.6112666726112366 \n",
      "     Training Step: 103 Training Loss: 0.6104533672332764 \n",
      "     Training Step: 104 Training Loss: 0.6171456575393677 \n",
      "     Training Step: 105 Training Loss: 0.6155084371566772 \n",
      "     Training Step: 106 Training Loss: 0.6101500391960144 \n",
      "     Training Step: 107 Training Loss: 0.6186656355857849 \n",
      "     Training Step: 108 Training Loss: 0.6096398234367371 \n",
      "     Training Step: 109 Training Loss: 0.612575352191925 \n",
      "     Training Step: 110 Training Loss: 0.6123275756835938 \n",
      "     Training Step: 111 Training Loss: 0.6148551106452942 \n",
      "     Training Step: 112 Training Loss: 0.6149020791053772 \n",
      "     Training Step: 113 Training Loss: 0.6139954924583435 \n",
      "     Training Step: 114 Training Loss: 0.615606427192688 \n",
      "     Training Step: 115 Training Loss: 0.6154817342758179 \n",
      "     Training Step: 116 Training Loss: 0.6128366589546204 \n",
      "     Training Step: 117 Training Loss: 0.6142710447311401 \n",
      "     Training Step: 118 Training Loss: 0.6169912219047546 \n",
      "     Training Step: 119 Training Loss: 0.6105995178222656 \n",
      "     Training Step: 120 Training Loss: 0.6097504496574402 \n",
      "     Training Step: 121 Training Loss: 0.6138421297073364 \n",
      "     Training Step: 122 Training Loss: 0.6121479868888855 \n",
      "     Training Step: 123 Training Loss: 0.6102421283721924 \n",
      "     Training Step: 124 Training Loss: 0.6135355830192566 \n",
      "     Training Step: 125 Training Loss: 0.6142202615737915 \n",
      "     Training Step: 126 Training Loss: 0.6133148074150085 \n",
      "     Training Step: 127 Training Loss: 0.6140890121459961 \n",
      "     Training Step: 128 Training Loss: 0.6102732419967651 \n",
      "     Training Step: 129 Training Loss: 0.617781937122345 \n",
      "     Training Step: 130 Training Loss: 0.613369345664978 \n",
      "     Training Step: 131 Training Loss: 0.6101917624473572 \n",
      "     Training Step: 132 Training Loss: 0.6165871024131775 \n",
      "     Training Step: 133 Training Loss: 0.6152576804161072 \n",
      "     Training Step: 134 Training Loss: 0.6146844625473022 \n",
      "     Training Step: 135 Training Loss: 0.6147178411483765 \n",
      "     Training Step: 136 Training Loss: 0.6130700707435608 \n",
      "     Training Step: 137 Training Loss: 0.6132738590240479 \n",
      "     Training Step: 138 Training Loss: 0.6150010824203491 \n",
      "     Training Step: 139 Training Loss: 0.6178181767463684 \n",
      "     Training Step: 140 Training Loss: 0.611756443977356 \n",
      "     Training Step: 141 Training Loss: 0.6146800518035889 \n",
      "     Training Step: 142 Training Loss: 0.6168411374092102 \n",
      "     Training Step: 143 Training Loss: 0.6167492270469666 \n",
      "     Training Step: 144 Training Loss: 0.6120843291282654 \n",
      "     Training Step: 145 Training Loss: 0.61182701587677 \n",
      "     Training Step: 146 Training Loss: 0.6115086078643799 \n",
      "     Training Step: 147 Training Loss: 0.6167806386947632 \n",
      "     Training Step: 148 Training Loss: 0.6106389760971069 \n",
      "     Training Step: 149 Training Loss: 0.6165761351585388 \n",
      "     Training Step: 150 Training Loss: 0.6116781830787659 \n",
      "     Training Step: 151 Training Loss: 0.611703097820282 \n",
      "     Training Step: 152 Training Loss: 0.612365186214447 \n",
      "     Training Step: 153 Training Loss: 0.6198080778121948 \n",
      "     Training Step: 154 Training Loss: 0.6180546879768372 \n",
      "     Training Step: 155 Training Loss: 0.6139088869094849 \n",
      "     Training Step: 156 Training Loss: 0.6134566068649292 \n",
      "     Training Step: 157 Training Loss: 0.6122154593467712 \n",
      "     Training Step: 158 Training Loss: 0.6127288937568665 \n",
      "     Training Step: 159 Training Loss: 0.6169834733009338 \n",
      "     Training Step: 160 Training Loss: 0.6186248064041138 \n",
      "     Training Step: 161 Training Loss: 0.6138611435890198 \n",
      "     Training Step: 162 Training Loss: 0.6108555793762207 \n",
      "     Training Step: 163 Training Loss: 0.611279308795929 \n",
      "     Training Step: 164 Training Loss: 0.6123453974723816 \n",
      "     Training Step: 165 Training Loss: 0.61285799741745 \n",
      "     Training Step: 166 Training Loss: 0.6132743954658508 \n",
      "     Training Step: 167 Training Loss: 0.6124609112739563 \n",
      "     Training Step: 168 Training Loss: 0.6122050881385803 \n",
      "     Training Step: 169 Training Loss: 0.6168187856674194 \n",
      "     Training Step: 170 Training Loss: 0.6165218949317932 \n",
      "     Training Step: 171 Training Loss: 0.6152923703193665 \n",
      "     Training Step: 172 Training Loss: 0.6159713268280029 \n",
      "     Training Step: 173 Training Loss: 0.6130049228668213 \n",
      "     Training Step: 174 Training Loss: 0.6178625822067261 \n",
      "     Training Step: 175 Training Loss: 0.614413857460022 \n",
      "     Training Step: 176 Training Loss: 0.6135684251785278 \n",
      "     Training Step: 177 Training Loss: 0.6098973155021667 \n",
      "     Training Step: 178 Training Loss: 0.6133279800415039 \n",
      "     Training Step: 179 Training Loss: 0.6156007647514343 \n",
      "     Training Step: 180 Training Loss: 0.6130441427230835 \n",
      "     Training Step: 181 Training Loss: 0.6137217283248901 \n",
      "     Training Step: 182 Training Loss: 0.6161743402481079 \n",
      "     Training Step: 183 Training Loss: 0.6177259087562561 \n",
      "     Training Step: 184 Training Loss: 0.6117216348648071 \n",
      "     Training Step: 185 Training Loss: 0.6155674457550049 \n",
      "     Training Step: 186 Training Loss: 0.6122591495513916 \n",
      "     Training Step: 187 Training Loss: 0.618899941444397 \n",
      "     Training Step: 188 Training Loss: 0.6137426495552063 \n",
      "     Training Step: 189 Training Loss: 0.6158435344696045 \n",
      "     Training Step: 190 Training Loss: 0.6142017841339111 \n",
      "     Training Step: 191 Training Loss: 0.6083672642707825 \n",
      "     Training Step: 192 Training Loss: 0.614459216594696 \n",
      "     Training Step: 193 Training Loss: 0.61091148853302 \n",
      "     Training Step: 194 Training Loss: 0.6170069575309753 \n",
      "     Training Step: 195 Training Loss: 0.6143242120742798 \n",
      "     Training Step: 196 Training Loss: 0.612270176410675 \n",
      "     Training Step: 197 Training Loss: 0.6189057230949402 \n",
      "     Training Step: 198 Training Loss: 0.6172108054161072 \n",
      "     Training Step: 199 Training Loss: 0.6117185950279236 \n",
      "     Training Step: 200 Training Loss: 0.6160616874694824 \n",
      "     Training Step: 201 Training Loss: 0.611919641494751 \n",
      "     Training Step: 202 Training Loss: 0.6132410764694214 \n",
      "     Training Step: 203 Training Loss: 0.617241621017456 \n",
      "     Training Step: 204 Training Loss: 0.6111891269683838 \n",
      "     Training Step: 205 Training Loss: 0.6145831942558289 \n",
      "     Training Step: 206 Training Loss: 0.6116607785224915 \n",
      "     Training Step: 207 Training Loss: 0.6106506586074829 \n",
      "     Training Step: 208 Training Loss: 0.6125369071960449 \n",
      "     Training Step: 209 Training Loss: 0.611890435218811 \n",
      "     Training Step: 210 Training Loss: 0.6104339361190796 \n",
      "     Training Step: 211 Training Loss: 0.6128172278404236 \n",
      "     Training Step: 212 Training Loss: 0.6157557368278503 \n",
      "     Training Step: 213 Training Loss: 0.6115589141845703 \n",
      "     Training Step: 214 Training Loss: 0.6151241064071655 \n",
      "     Training Step: 215 Training Loss: 0.6148225665092468 \n",
      "     Training Step: 216 Training Loss: 0.6157684922218323 \n",
      "     Training Step: 217 Training Loss: 0.6126009821891785 \n",
      "     Training Step: 218 Training Loss: 0.6149477362632751 \n",
      "     Training Step: 219 Training Loss: 0.61778324842453 \n",
      "     Training Step: 220 Training Loss: 0.6118687987327576 \n",
      "     Training Step: 221 Training Loss: 0.6151271462440491 \n",
      "     Training Step: 222 Training Loss: 0.6131801605224609 \n",
      "     Training Step: 223 Training Loss: 0.6121934652328491 \n",
      "     Training Step: 224 Training Loss: 0.6146783828735352 \n",
      "     Training Step: 225 Training Loss: 0.6116834878921509 \n",
      "     Training Step: 226 Training Loss: 0.6154431104660034 \n",
      "     Training Step: 227 Training Loss: 0.614151656627655 \n",
      "     Training Step: 228 Training Loss: 0.6144559979438782 \n",
      "     Training Step: 229 Training Loss: 0.6194483041763306 \n",
      "     Training Step: 230 Training Loss: 0.6120521426200867 \n",
      "     Training Step: 231 Training Loss: 0.6167262196540833 \n",
      "     Training Step: 232 Training Loss: 0.6139810681343079 \n",
      "     Training Step: 233 Training Loss: 0.6167495846748352 \n",
      "     Training Step: 234 Training Loss: 0.6153690218925476 \n",
      "     Training Step: 235 Training Loss: 0.6132512092590332 \n",
      "     Training Step: 236 Training Loss: 0.6142870187759399 \n",
      "     Training Step: 237 Training Loss: 0.6129211783409119 \n",
      "     Training Step: 238 Training Loss: 0.6114817261695862 \n",
      "     Training Step: 239 Training Loss: 0.6211317777633667 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6116852760314941 \n",
      "     Validation Step: 1 Validation Loss: 0.6185461282730103 \n",
      "     Validation Step: 2 Validation Loss: 0.6170586347579956 \n",
      "     Validation Step: 3 Validation Loss: 0.6119470596313477 \n",
      "     Validation Step: 4 Validation Loss: 0.6112281680107117 \n",
      "     Validation Step: 5 Validation Loss: 0.6133294105529785 \n",
      "     Validation Step: 6 Validation Loss: 0.614163339138031 \n",
      "     Validation Step: 7 Validation Loss: 0.6128708124160767 \n",
      "     Validation Step: 8 Validation Loss: 0.6160641312599182 \n",
      "     Validation Step: 9 Validation Loss: 0.6105712056159973 \n",
      "     Validation Step: 10 Validation Loss: 0.6102336049079895 \n",
      "     Validation Step: 11 Validation Loss: 0.6102119088172913 \n",
      "     Validation Step: 12 Validation Loss: 0.6107097864151001 \n",
      "     Validation Step: 13 Validation Loss: 0.6142966747283936 \n",
      "     Validation Step: 14 Validation Loss: 0.6176314353942871 \n",
      "     Validation Step: 15 Validation Loss: 0.6148797273635864 \n",
      "     Validation Step: 16 Validation Loss: 0.6149510741233826 \n",
      "     Validation Step: 17 Validation Loss: 0.6142598390579224 \n",
      "     Validation Step: 18 Validation Loss: 0.6183047294616699 \n",
      "     Validation Step: 19 Validation Loss: 0.6173379421234131 \n",
      "     Validation Step: 20 Validation Loss: 0.6181166768074036 \n",
      "     Validation Step: 21 Validation Loss: 0.6150963306427002 \n",
      "     Validation Step: 22 Validation Loss: 0.6121900677680969 \n",
      "     Validation Step: 23 Validation Loss: 0.6141918897628784 \n",
      "     Validation Step: 24 Validation Loss: 0.6137043833732605 \n",
      "     Validation Step: 25 Validation Loss: 0.6105471253395081 \n",
      "     Validation Step: 26 Validation Loss: 0.6156147122383118 \n",
      "     Validation Step: 27 Validation Loss: 0.6184027791023254 \n",
      "     Validation Step: 28 Validation Loss: 0.6158619523048401 \n",
      "     Validation Step: 29 Validation Loss: 0.6153764128684998 \n",
      "     Validation Step: 30 Validation Loss: 0.611235499382019 \n",
      "     Validation Step: 31 Validation Loss: 0.6130295991897583 \n",
      "     Validation Step: 32 Validation Loss: 0.6102322340011597 \n",
      "     Validation Step: 33 Validation Loss: 0.6146867275238037 \n",
      "     Validation Step: 34 Validation Loss: 0.613736629486084 \n",
      "     Validation Step: 35 Validation Loss: 0.6145913004875183 \n",
      "     Validation Step: 36 Validation Loss: 0.6136572360992432 \n",
      "     Validation Step: 37 Validation Loss: 0.6076448559761047 \n",
      "     Validation Step: 38 Validation Loss: 0.6156502962112427 \n",
      "     Validation Step: 39 Validation Loss: 0.6116195917129517 \n",
      "     Validation Step: 40 Validation Loss: 0.6152580380439758 \n",
      "     Validation Step: 41 Validation Loss: 0.6145771741867065 \n",
      "     Validation Step: 42 Validation Loss: 0.6185259222984314 \n",
      "     Validation Step: 43 Validation Loss: 0.6177374124526978 \n",
      "     Validation Step: 44 Validation Loss: 0.6163093447685242 \n",
      "Epoch: 15\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6124125123023987 \n",
      "     Training Step: 1 Training Loss: 0.6188810467720032 \n",
      "     Training Step: 2 Training Loss: 0.6145166754722595 \n",
      "     Training Step: 3 Training Loss: 0.6149817705154419 \n",
      "     Training Step: 4 Training Loss: 0.6171377897262573 \n",
      "     Training Step: 5 Training Loss: 0.6145313382148743 \n",
      "     Training Step: 6 Training Loss: 0.6115331053733826 \n",
      "     Training Step: 7 Training Loss: 0.6153478622436523 \n",
      "     Training Step: 8 Training Loss: 0.619888424873352 \n",
      "     Training Step: 9 Training Loss: 0.6164171695709229 \n",
      "     Training Step: 10 Training Loss: 0.6157317757606506 \n",
      "     Training Step: 11 Training Loss: 0.6129639148712158 \n",
      "     Training Step: 12 Training Loss: 0.6178269982337952 \n",
      "     Training Step: 13 Training Loss: 0.6117889285087585 \n",
      "     Training Step: 14 Training Loss: 0.6126178503036499 \n",
      "     Training Step: 15 Training Loss: 0.6168476939201355 \n",
      "     Training Step: 16 Training Loss: 0.6151729822158813 \n",
      "     Training Step: 17 Training Loss: 0.6105283498764038 \n",
      "     Training Step: 18 Training Loss: 0.6105976104736328 \n",
      "     Training Step: 19 Training Loss: 0.6107832193374634 \n",
      "     Training Step: 20 Training Loss: 0.6122393608093262 \n",
      "     Training Step: 21 Training Loss: 0.6155266165733337 \n",
      "     Training Step: 22 Training Loss: 0.6116344332695007 \n",
      "     Training Step: 23 Training Loss: 0.6110026836395264 \n",
      "     Training Step: 24 Training Loss: 0.613543689250946 \n",
      "     Training Step: 25 Training Loss: 0.6132728457450867 \n",
      "     Training Step: 26 Training Loss: 0.6116107106208801 \n",
      "     Training Step: 27 Training Loss: 0.6173117756843567 \n",
      "     Training Step: 28 Training Loss: 0.6147854328155518 \n",
      "     Training Step: 29 Training Loss: 0.6116675734519958 \n",
      "     Training Step: 30 Training Loss: 0.6153191328048706 \n",
      "     Training Step: 31 Training Loss: 0.617179274559021 \n",
      "     Training Step: 32 Training Loss: 0.6147676110267639 \n",
      "     Training Step: 33 Training Loss: 0.6107299327850342 \n",
      "     Training Step: 34 Training Loss: 0.6154640316963196 \n",
      "     Training Step: 35 Training Loss: 0.615111231803894 \n",
      "     Training Step: 36 Training Loss: 0.6171733736991882 \n",
      "     Training Step: 37 Training Loss: 0.6177452802658081 \n",
      "     Training Step: 38 Training Loss: 0.6143746972084045 \n",
      "     Training Step: 39 Training Loss: 0.6126409769058228 \n",
      "     Training Step: 40 Training Loss: 0.6158100366592407 \n",
      "     Training Step: 41 Training Loss: 0.6175469160079956 \n",
      "     Training Step: 42 Training Loss: 0.6158018112182617 \n",
      "     Training Step: 43 Training Loss: 0.6118806600570679 \n",
      "     Training Step: 44 Training Loss: 0.616710364818573 \n",
      "     Training Step: 45 Training Loss: 0.6144013404846191 \n",
      "     Training Step: 46 Training Loss: 0.6119492053985596 \n",
      "     Training Step: 47 Training Loss: 0.6167382597923279 \n",
      "     Training Step: 48 Training Loss: 0.6125795245170593 \n",
      "     Training Step: 49 Training Loss: 0.6121869087219238 \n",
      "     Training Step: 50 Training Loss: 0.6114662885665894 \n",
      "     Training Step: 51 Training Loss: 0.6129183769226074 \n",
      "     Training Step: 52 Training Loss: 0.6129122376441956 \n",
      "     Training Step: 53 Training Loss: 0.612738311290741 \n",
      "     Training Step: 54 Training Loss: 0.612563967704773 \n",
      "     Training Step: 55 Training Loss: 0.6097626090049744 \n",
      "     Training Step: 56 Training Loss: 0.6155677437782288 \n",
      "     Training Step: 57 Training Loss: 0.6168144345283508 \n",
      "     Training Step: 58 Training Loss: 0.6093893051147461 \n",
      "     Training Step: 59 Training Loss: 0.612294614315033 \n",
      "     Training Step: 60 Training Loss: 0.6133601665496826 \n",
      "     Training Step: 61 Training Loss: 0.6106281876564026 \n",
      "     Training Step: 62 Training Loss: 0.610628068447113 \n",
      "     Training Step: 63 Training Loss: 0.6104561686515808 \n",
      "     Training Step: 64 Training Loss: 0.6138293743133545 \n",
      "     Training Step: 65 Training Loss: 0.6094977855682373 \n",
      "     Training Step: 66 Training Loss: 0.6155079007148743 \n",
      "     Training Step: 67 Training Loss: 0.6147769689559937 \n",
      "     Training Step: 68 Training Loss: 0.6155489683151245 \n",
      "     Training Step: 69 Training Loss: 0.6118043661117554 \n",
      "     Training Step: 70 Training Loss: 0.6142936944961548 \n",
      "     Training Step: 71 Training Loss: 0.6129413843154907 \n",
      "     Training Step: 72 Training Loss: 0.6143185496330261 \n",
      "     Training Step: 73 Training Loss: 0.612847626209259 \n",
      "     Training Step: 74 Training Loss: 0.6102561354637146 \n",
      "     Training Step: 75 Training Loss: 0.613432765007019 \n",
      "     Training Step: 76 Training Loss: 0.6168495416641235 \n",
      "     Training Step: 77 Training Loss: 0.6122721433639526 \n",
      "     Training Step: 78 Training Loss: 0.6132460832595825 \n",
      "     Training Step: 79 Training Loss: 0.6116137504577637 \n",
      "     Training Step: 80 Training Loss: 0.6186363101005554 \n",
      "     Training Step: 81 Training Loss: 0.612453281879425 \n",
      "     Training Step: 82 Training Loss: 0.6133309602737427 \n",
      "     Training Step: 83 Training Loss: 0.6142892837524414 \n",
      "     Training Step: 84 Training Loss: 0.6138079166412354 \n",
      "     Training Step: 85 Training Loss: 0.615199089050293 \n",
      "     Training Step: 86 Training Loss: 0.6155697107315063 \n",
      "     Training Step: 87 Training Loss: 0.6141016483306885 \n",
      "     Training Step: 88 Training Loss: 0.6122534871101379 \n",
      "     Training Step: 89 Training Loss: 0.6127007007598877 \n",
      "     Training Step: 90 Training Loss: 0.6178080439567566 \n",
      "     Training Step: 91 Training Loss: 0.6184976696968079 \n",
      "     Training Step: 92 Training Loss: 0.61823970079422 \n",
      "     Training Step: 93 Training Loss: 0.6102951765060425 \n",
      "     Training Step: 94 Training Loss: 0.61561119556427 \n",
      "     Training Step: 95 Training Loss: 0.6131288409233093 \n",
      "     Training Step: 96 Training Loss: 0.6148020625114441 \n",
      "     Training Step: 97 Training Loss: 0.6134021282196045 \n",
      "     Training Step: 98 Training Loss: 0.6098086833953857 \n",
      "     Training Step: 99 Training Loss: 0.6201187968254089 \n",
      "     Training Step: 100 Training Loss: 0.6136962175369263 \n",
      "     Training Step: 101 Training Loss: 0.6162381172180176 \n",
      "     Training Step: 102 Training Loss: 0.6145246028900146 \n",
      "     Training Step: 103 Training Loss: 0.6177063584327698 \n",
      "     Training Step: 104 Training Loss: 0.6184377074241638 \n",
      "     Training Step: 105 Training Loss: 0.6113333106040955 \n",
      "     Training Step: 106 Training Loss: 0.6138603687286377 \n",
      "     Training Step: 107 Training Loss: 0.615904688835144 \n",
      "     Training Step: 108 Training Loss: 0.6158221364021301 \n",
      "     Training Step: 109 Training Loss: 0.6189242601394653 \n",
      "     Training Step: 110 Training Loss: 0.6180664896965027 \n",
      "     Training Step: 111 Training Loss: 0.6109127998352051 \n",
      "     Training Step: 112 Training Loss: 0.6103489995002747 \n",
      "     Training Step: 113 Training Loss: 0.6134675145149231 \n",
      "     Training Step: 114 Training Loss: 0.6144124865531921 \n",
      "     Training Step: 115 Training Loss: 0.6126049757003784 \n",
      "     Training Step: 116 Training Loss: 0.6132492423057556 \n",
      "     Training Step: 117 Training Loss: 0.6149306893348694 \n",
      "     Training Step: 118 Training Loss: 0.6148380637168884 \n",
      "     Training Step: 119 Training Loss: 0.6119464635848999 \n",
      "     Training Step: 120 Training Loss: 0.6182612776756287 \n",
      "     Training Step: 121 Training Loss: 0.6111018657684326 \n",
      "     Training Step: 122 Training Loss: 0.6197791695594788 \n",
      "     Training Step: 123 Training Loss: 0.6154578328132629 \n",
      "     Training Step: 124 Training Loss: 0.6150493621826172 \n",
      "     Training Step: 125 Training Loss: 0.6143968105316162 \n",
      "     Training Step: 126 Training Loss: 0.6182093024253845 \n",
      "     Training Step: 127 Training Loss: 0.6153191924095154 \n",
      "     Training Step: 128 Training Loss: 0.61128169298172 \n",
      "     Training Step: 129 Training Loss: 0.6179202198982239 \n",
      "     Training Step: 130 Training Loss: 0.6155233979225159 \n",
      "     Training Step: 131 Training Loss: 0.614791214466095 \n",
      "     Training Step: 132 Training Loss: 0.6117972135543823 \n",
      "     Training Step: 133 Training Loss: 0.6181047558784485 \n",
      "     Training Step: 134 Training Loss: 0.6115351319313049 \n",
      "     Training Step: 135 Training Loss: 0.618682861328125 \n",
      "     Training Step: 136 Training Loss: 0.6165292859077454 \n",
      "     Training Step: 137 Training Loss: 0.6131205558776855 \n",
      "     Training Step: 138 Training Loss: 0.6149948239326477 \n",
      "     Training Step: 139 Training Loss: 0.6143900156021118 \n",
      "     Training Step: 140 Training Loss: 0.6147119998931885 \n",
      "     Training Step: 141 Training Loss: 0.6160286664962769 \n",
      "     Training Step: 142 Training Loss: 0.6118357181549072 \n",
      "     Training Step: 143 Training Loss: 0.6184667944908142 \n",
      "     Training Step: 144 Training Loss: 0.6097955107688904 \n",
      "     Training Step: 145 Training Loss: 0.6176934838294983 \n",
      "     Training Step: 146 Training Loss: 0.6100354194641113 \n",
      "     Training Step: 147 Training Loss: 0.6094731688499451 \n",
      "     Training Step: 148 Training Loss: 0.6107625365257263 \n",
      "     Training Step: 149 Training Loss: 0.6169921159744263 \n",
      "     Training Step: 150 Training Loss: 0.6137141585350037 \n",
      "     Training Step: 151 Training Loss: 0.6146712303161621 \n",
      "     Training Step: 152 Training Loss: 0.6151332259178162 \n",
      "     Training Step: 153 Training Loss: 0.614539384841919 \n",
      "     Training Step: 154 Training Loss: 0.6159762144088745 \n",
      "     Training Step: 155 Training Loss: 0.6124348044395447 \n",
      "     Training Step: 156 Training Loss: 0.6135936379432678 \n",
      "     Training Step: 157 Training Loss: 0.6162762641906738 \n",
      "     Training Step: 158 Training Loss: 0.6083354353904724 \n",
      "     Training Step: 159 Training Loss: 0.6169384121894836 \n",
      "     Training Step: 160 Training Loss: 0.6170384287834167 \n",
      "     Training Step: 161 Training Loss: 0.6132511496543884 \n",
      "     Training Step: 162 Training Loss: 0.6164634227752686 \n",
      "     Training Step: 163 Training Loss: 0.6137412190437317 \n",
      "     Training Step: 164 Training Loss: 0.6130356788635254 \n",
      "     Training Step: 165 Training Loss: 0.6140861511230469 \n",
      "     Training Step: 166 Training Loss: 0.6130768656730652 \n",
      "     Training Step: 167 Training Loss: 0.6116645336151123 \n",
      "     Training Step: 168 Training Loss: 0.6124588251113892 \n",
      "     Training Step: 169 Training Loss: 0.6112267374992371 \n",
      "     Training Step: 170 Training Loss: 0.6157961487770081 \n",
      "     Training Step: 171 Training Loss: 0.6211038827896118 \n",
      "     Training Step: 172 Training Loss: 0.6109060049057007 \n",
      "     Training Step: 173 Training Loss: 0.6134042739868164 \n",
      "     Training Step: 174 Training Loss: 0.6122257709503174 \n",
      "     Training Step: 175 Training Loss: 0.6142275929450989 \n",
      "     Training Step: 176 Training Loss: 0.6139745712280273 \n",
      "     Training Step: 177 Training Loss: 0.6161680817604065 \n",
      "     Training Step: 178 Training Loss: 0.6147661805152893 \n",
      "     Training Step: 179 Training Loss: 0.6177434921264648 \n",
      "     Training Step: 180 Training Loss: 0.6148700714111328 \n",
      "     Training Step: 181 Training Loss: 0.6138717532157898 \n",
      "     Training Step: 182 Training Loss: 0.6123635172843933 \n",
      "     Training Step: 183 Training Loss: 0.6102929711341858 \n",
      "     Training Step: 184 Training Loss: 0.6164275407791138 \n",
      "     Training Step: 185 Training Loss: 0.610679030418396 \n",
      "     Training Step: 186 Training Loss: 0.6128939390182495 \n",
      "     Training Step: 187 Training Loss: 0.6114920973777771 \n",
      "     Training Step: 188 Training Loss: 0.612350344657898 \n",
      "     Training Step: 189 Training Loss: 0.6173537373542786 \n",
      "     Training Step: 190 Training Loss: 0.6147614121437073 \n",
      "     Training Step: 191 Training Loss: 0.6147426962852478 \n",
      "     Training Step: 192 Training Loss: 0.6180188059806824 \n",
      "     Training Step: 193 Training Loss: 0.6120770573616028 \n",
      "     Training Step: 194 Training Loss: 0.6145036220550537 \n",
      "     Training Step: 195 Training Loss: 0.6119378805160522 \n",
      "     Training Step: 196 Training Loss: 0.6117306351661682 \n",
      "     Training Step: 197 Training Loss: 0.6116145849227905 \n",
      "     Training Step: 198 Training Loss: 0.6198022961616516 \n",
      "     Training Step: 199 Training Loss: 0.6169306635856628 \n",
      "     Training Step: 200 Training Loss: 0.6122835278511047 \n",
      "     Training Step: 201 Training Loss: 0.6155160069465637 \n",
      "     Training Step: 202 Training Loss: 0.6108282804489136 \n",
      "     Training Step: 203 Training Loss: 0.6184085011482239 \n",
      "     Training Step: 204 Training Loss: 0.6141131520271301 \n",
      "     Training Step: 205 Training Loss: 0.6147085428237915 \n",
      "     Training Step: 206 Training Loss: 0.6170111894607544 \n",
      "     Training Step: 207 Training Loss: 0.6116340756416321 \n",
      "     Training Step: 208 Training Loss: 0.6152566075325012 \n",
      "     Training Step: 209 Training Loss: 0.6153329014778137 \n",
      "     Training Step: 210 Training Loss: 0.615860641002655 \n",
      "     Training Step: 211 Training Loss: 0.6167564392089844 \n",
      "     Training Step: 212 Training Loss: 0.6133829951286316 \n",
      "     Training Step: 213 Training Loss: 0.6101652979850769 \n",
      "     Training Step: 214 Training Loss: 0.6139671206474304 \n",
      "     Training Step: 215 Training Loss: 0.6169320940971375 \n",
      "     Training Step: 216 Training Loss: 0.6114973425865173 \n",
      "     Training Step: 217 Training Loss: 0.6137892007827759 \n",
      "     Training Step: 218 Training Loss: 0.6141241788864136 \n",
      "     Training Step: 219 Training Loss: 0.6168447732925415 \n",
      "     Training Step: 220 Training Loss: 0.6155380010604858 \n",
      "     Training Step: 221 Training Loss: 0.616183876991272 \n",
      "     Training Step: 222 Training Loss: 0.6147240996360779 \n",
      "     Training Step: 223 Training Loss: 0.6120339035987854 \n",
      "     Training Step: 224 Training Loss: 0.6202890872955322 \n",
      "     Training Step: 225 Training Loss: 0.6144497394561768 \n",
      "     Training Step: 226 Training Loss: 0.6139159202575684 \n",
      "     Training Step: 227 Training Loss: 0.6122235655784607 \n",
      "     Training Step: 228 Training Loss: 0.6168451309204102 \n",
      "     Training Step: 229 Training Loss: 0.6153851747512817 \n",
      "     Training Step: 230 Training Loss: 0.6129733324050903 \n",
      "     Training Step: 231 Training Loss: 0.6132020950317383 \n",
      "     Training Step: 232 Training Loss: 0.6167200207710266 \n",
      "     Training Step: 233 Training Loss: 0.6141515970230103 \n",
      "     Training Step: 234 Training Loss: 0.615563690662384 \n",
      "     Training Step: 235 Training Loss: 0.6148964762687683 \n",
      "     Training Step: 236 Training Loss: 0.6135475039482117 \n",
      "     Training Step: 237 Training Loss: 0.616436243057251 \n",
      "     Training Step: 238 Training Loss: 0.6135267615318298 \n",
      "     Training Step: 239 Training Loss: 0.6173937916755676 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6112569570541382 \n",
      "     Validation Step: 1 Validation Loss: 0.6102952361106873 \n",
      "     Validation Step: 2 Validation Loss: 0.6148691773414612 \n",
      "     Validation Step: 3 Validation Loss: 0.6153407692909241 \n",
      "     Validation Step: 4 Validation Loss: 0.6122140288352966 \n",
      "     Validation Step: 5 Validation Loss: 0.6183333396911621 \n",
      "     Validation Step: 6 Validation Loss: 0.6107606291770935 \n",
      "     Validation Step: 7 Validation Loss: 0.6162651777267456 \n",
      "     Validation Step: 8 Validation Loss: 0.6102657914161682 \n",
      "     Validation Step: 9 Validation Loss: 0.6128916144371033 \n",
      "     Validation Step: 10 Validation Loss: 0.611645519733429 \n",
      "     Validation Step: 11 Validation Loss: 0.6145902872085571 \n",
      "     Validation Step: 12 Validation Loss: 0.6172940135002136 \n",
      "     Validation Step: 13 Validation Loss: 0.6117264628410339 \n",
      "     Validation Step: 14 Validation Loss: 0.6133405566215515 \n",
      "     Validation Step: 15 Validation Loss: 0.614234983921051 \n",
      "     Validation Step: 16 Validation Loss: 0.6175951957702637 \n",
      "     Validation Step: 17 Validation Loss: 0.6112633943557739 \n",
      "     Validation Step: 18 Validation Loss: 0.6142957806587219 \n",
      "     Validation Step: 19 Validation Loss: 0.6150525212287903 \n",
      "     Validation Step: 20 Validation Loss: 0.6156324148178101 \n",
      "     Validation Step: 21 Validation Loss: 0.6119804382324219 \n",
      "     Validation Step: 22 Validation Loss: 0.6077248454093933 \n",
      "     Validation Step: 23 Validation Loss: 0.6155766248703003 \n",
      "     Validation Step: 24 Validation Loss: 0.6182236075401306 \n",
      "     Validation Step: 25 Validation Loss: 0.6149442195892334 \n",
      "     Validation Step: 26 Validation Loss: 0.6102928519248962 \n",
      "     Validation Step: 27 Validation Loss: 0.6106076836585999 \n",
      "     Validation Step: 28 Validation Loss: 0.6141772270202637 \n",
      "     Validation Step: 29 Validation Loss: 0.6158188581466675 \n",
      "     Validation Step: 30 Validation Loss: 0.6184622049331665 \n",
      "     Validation Step: 31 Validation Loss: 0.6145694255828857 \n",
      "     Validation Step: 32 Validation Loss: 0.6184782981872559 \n",
      "     Validation Step: 33 Validation Loss: 0.6176876425743103 \n",
      "     Validation Step: 34 Validation Loss: 0.6170095205307007 \n",
      "     Validation Step: 35 Validation Loss: 0.6152446269989014 \n",
      "     Validation Step: 36 Validation Loss: 0.6136757731437683 \n",
      "     Validation Step: 37 Validation Loss: 0.6136808395385742 \n",
      "     Validation Step: 38 Validation Loss: 0.6180515289306641 \n",
      "     Validation Step: 39 Validation Loss: 0.6160047054290771 \n",
      "     Validation Step: 40 Validation Loss: 0.6130301356315613 \n",
      "     Validation Step: 41 Validation Loss: 0.6137146949768066 \n",
      "     Validation Step: 42 Validation Loss: 0.6105783581733704 \n",
      "     Validation Step: 43 Validation Loss: 0.614669919013977 \n",
      "     Validation Step: 44 Validation Loss: 0.6141700744628906 \n",
      "Epoch: 16\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6107929944992065 \n",
      "     Training Step: 1 Training Loss: 0.6139195561408997 \n",
      "     Training Step: 2 Training Loss: 0.6167271137237549 \n",
      "     Training Step: 3 Training Loss: 0.6142407655715942 \n",
      "     Training Step: 4 Training Loss: 0.6147921681404114 \n",
      "     Training Step: 5 Training Loss: 0.6145539879798889 \n",
      "     Training Step: 6 Training Loss: 0.6157064437866211 \n",
      "     Training Step: 7 Training Loss: 0.6117459535598755 \n",
      "     Training Step: 8 Training Loss: 0.6147245168685913 \n",
      "     Training Step: 9 Training Loss: 0.617802083492279 \n",
      "     Training Step: 10 Training Loss: 0.6168333292007446 \n",
      "     Training Step: 11 Training Loss: 0.6166699528694153 \n",
      "     Training Step: 12 Training Loss: 0.6148057579994202 \n",
      "     Training Step: 13 Training Loss: 0.6085609197616577 \n",
      "     Training Step: 14 Training Loss: 0.6122295260429382 \n",
      "     Training Step: 15 Training Loss: 0.61594158411026 \n",
      "     Training Step: 16 Training Loss: 0.616218626499176 \n",
      "     Training Step: 17 Training Loss: 0.6135675311088562 \n",
      "     Training Step: 18 Training Loss: 0.6114705204963684 \n",
      "     Training Step: 19 Training Loss: 0.6119011044502258 \n",
      "     Training Step: 20 Training Loss: 0.6119593977928162 \n",
      "     Training Step: 21 Training Loss: 0.6105786561965942 \n",
      "     Training Step: 22 Training Loss: 0.6169693470001221 \n",
      "     Training Step: 23 Training Loss: 0.6136766672134399 \n",
      "     Training Step: 24 Training Loss: 0.6130287647247314 \n",
      "     Training Step: 25 Training Loss: 0.6158003211021423 \n",
      "     Training Step: 26 Training Loss: 0.6098834872245789 \n",
      "     Training Step: 27 Training Loss: 0.6095358729362488 \n",
      "     Training Step: 28 Training Loss: 0.6169539093971252 \n",
      "     Training Step: 29 Training Loss: 0.6168531179428101 \n",
      "     Training Step: 30 Training Loss: 0.6137661337852478 \n",
      "     Training Step: 31 Training Loss: 0.6171718835830688 \n",
      "     Training Step: 32 Training Loss: 0.6147910952568054 \n",
      "     Training Step: 33 Training Loss: 0.6117674112319946 \n",
      "     Training Step: 34 Training Loss: 0.6116313934326172 \n",
      "     Training Step: 35 Training Loss: 0.616071343421936 \n",
      "     Training Step: 36 Training Loss: 0.6141232252120972 \n",
      "     Training Step: 37 Training Loss: 0.6101332306861877 \n",
      "     Training Step: 38 Training Loss: 0.618270218372345 \n",
      "     Training Step: 39 Training Loss: 0.6182073354721069 \n",
      "     Training Step: 40 Training Loss: 0.6115670800209045 \n",
      "     Training Step: 41 Training Loss: 0.6099740862846375 \n",
      "     Training Step: 42 Training Loss: 0.6135240197181702 \n",
      "     Training Step: 43 Training Loss: 0.6106393337249756 \n",
      "     Training Step: 44 Training Loss: 0.6200971007347107 \n",
      "     Training Step: 45 Training Loss: 0.6168330907821655 \n",
      "     Training Step: 46 Training Loss: 0.61312335729599 \n",
      "     Training Step: 47 Training Loss: 0.6145064830780029 \n",
      "     Training Step: 48 Training Loss: 0.6167572140693665 \n",
      "     Training Step: 49 Training Loss: 0.6147881746292114 \n",
      "     Training Step: 50 Training Loss: 0.6140051484107971 \n",
      "     Training Step: 51 Training Loss: 0.6115955710411072 \n",
      "     Training Step: 52 Training Loss: 0.6094473600387573 \n",
      "     Training Step: 53 Training Loss: 0.6139458417892456 \n",
      "     Training Step: 54 Training Loss: 0.6186422109603882 \n",
      "     Training Step: 55 Training Loss: 0.6151984333992004 \n",
      "     Training Step: 56 Training Loss: 0.6107327938079834 \n",
      "     Training Step: 57 Training Loss: 0.6116639971733093 \n",
      "     Training Step: 58 Training Loss: 0.6127557754516602 \n",
      "     Training Step: 59 Training Loss: 0.6125836968421936 \n",
      "     Training Step: 60 Training Loss: 0.6140400171279907 \n",
      "     Training Step: 61 Training Loss: 0.6117169260978699 \n",
      "     Training Step: 62 Training Loss: 0.6139215230941772 \n",
      "     Training Step: 63 Training Loss: 0.6155269742012024 \n",
      "     Training Step: 64 Training Loss: 0.6170146465301514 \n",
      "     Training Step: 65 Training Loss: 0.6153920292854309 \n",
      "     Training Step: 66 Training Loss: 0.6156120300292969 \n",
      "     Training Step: 67 Training Loss: 0.6172448396682739 \n",
      "     Training Step: 68 Training Loss: 0.6165160536766052 \n",
      "     Training Step: 69 Training Loss: 0.6145598292350769 \n",
      "     Training Step: 70 Training Loss: 0.6102651953697205 \n",
      "     Training Step: 71 Training Loss: 0.6163259148597717 \n",
      "     Training Step: 72 Training Loss: 0.6101272106170654 \n",
      "     Training Step: 73 Training Loss: 0.6153112649917603 \n",
      "     Training Step: 74 Training Loss: 0.6132085919380188 \n",
      "     Training Step: 75 Training Loss: 0.6152544021606445 \n",
      "     Training Step: 76 Training Loss: 0.6153799891471863 \n",
      "     Training Step: 77 Training Loss: 0.6122763752937317 \n",
      "     Training Step: 78 Training Loss: 0.6145384311676025 \n",
      "     Training Step: 79 Training Loss: 0.6107569932937622 \n",
      "     Training Step: 80 Training Loss: 0.6128777265548706 \n",
      "     Training Step: 81 Training Loss: 0.6168826818466187 \n",
      "     Training Step: 82 Training Loss: 0.6113137602806091 \n",
      "     Training Step: 83 Training Loss: 0.6144126653671265 \n",
      "     Training Step: 84 Training Loss: 0.6146976351737976 \n",
      "     Training Step: 85 Training Loss: 0.6108238697052002 \n",
      "     Training Step: 86 Training Loss: 0.6169688701629639 \n",
      "     Training Step: 87 Training Loss: 0.6186397671699524 \n",
      "     Training Step: 88 Training Loss: 0.6117038726806641 \n",
      "     Training Step: 89 Training Loss: 0.6100935339927673 \n",
      "     Training Step: 90 Training Loss: 0.6097782254219055 \n",
      "     Training Step: 91 Training Loss: 0.6137871742248535 \n",
      "     Training Step: 92 Training Loss: 0.6180480122566223 \n",
      "     Training Step: 93 Training Loss: 0.6133615970611572 \n",
      "     Training Step: 94 Training Loss: 0.6101548075675964 \n",
      "     Training Step: 95 Training Loss: 0.6160272359848022 \n",
      "     Training Step: 96 Training Loss: 0.6132941842079163 \n",
      "     Training Step: 97 Training Loss: 0.6115403175354004 \n",
      "     Training Step: 98 Training Loss: 0.6106277704238892 \n",
      "     Training Step: 99 Training Loss: 0.6147704720497131 \n",
      "     Training Step: 100 Training Loss: 0.6144770383834839 \n",
      "     Training Step: 101 Training Loss: 0.6204599738121033 \n",
      "     Training Step: 102 Training Loss: 0.6154655814170837 \n",
      "     Training Step: 103 Training Loss: 0.6113613247871399 \n",
      "     Training Step: 104 Training Loss: 0.6149215698242188 \n",
      "     Training Step: 105 Training Loss: 0.6178197264671326 \n",
      "     Training Step: 106 Training Loss: 0.6119012832641602 \n",
      "     Training Step: 107 Training Loss: 0.6173425316810608 \n",
      "     Training Step: 108 Training Loss: 0.6107466220855713 \n",
      "     Training Step: 109 Training Loss: 0.6122890710830688 \n",
      "     Training Step: 110 Training Loss: 0.6149967312812805 \n",
      "     Training Step: 111 Training Loss: 0.6200991272926331 \n",
      "     Training Step: 112 Training Loss: 0.6124407052993774 \n",
      "     Training Step: 113 Training Loss: 0.6120377779006958 \n",
      "     Training Step: 114 Training Loss: 0.6129707098007202 \n",
      "     Training Step: 115 Training Loss: 0.6150096654891968 \n",
      "     Training Step: 116 Training Loss: 0.6178635358810425 \n",
      "     Training Step: 117 Training Loss: 0.610459566116333 \n",
      "     Training Step: 118 Training Loss: 0.6134324669837952 \n",
      "     Training Step: 119 Training Loss: 0.6126223206520081 \n",
      "     Training Step: 120 Training Loss: 0.6132065653800964 \n",
      "     Training Step: 121 Training Loss: 0.6133536696434021 \n",
      "     Training Step: 122 Training Loss: 0.6144580841064453 \n",
      "     Training Step: 123 Training Loss: 0.614680290222168 \n",
      "     Training Step: 124 Training Loss: 0.6128295660018921 \n",
      "     Training Step: 125 Training Loss: 0.6129797697067261 \n",
      "     Training Step: 126 Training Loss: 0.6128624081611633 \n",
      "     Training Step: 127 Training Loss: 0.6109353303909302 \n",
      "     Training Step: 128 Training Loss: 0.6122998595237732 \n",
      "     Training Step: 129 Training Loss: 0.6165763735771179 \n",
      "     Training Step: 130 Training Loss: 0.6178663372993469 \n",
      "     Training Step: 131 Training Loss: 0.6153064966201782 \n",
      "     Training Step: 132 Training Loss: 0.6139041781425476 \n",
      "     Training Step: 133 Training Loss: 0.6109617948532104 \n",
      "     Training Step: 134 Training Loss: 0.6108208894729614 \n",
      "     Training Step: 135 Training Loss: 0.6154136061668396 \n",
      "     Training Step: 136 Training Loss: 0.6153465509414673 \n",
      "     Training Step: 137 Training Loss: 0.6115278005599976 \n",
      "     Training Step: 138 Training Loss: 0.6163972020149231 \n",
      "     Training Step: 139 Training Loss: 0.6182458400726318 \n",
      "     Training Step: 140 Training Loss: 0.6180952191352844 \n",
      "     Training Step: 141 Training Loss: 0.612552285194397 \n",
      "     Training Step: 142 Training Loss: 0.6133248805999756 \n",
      "     Training Step: 143 Training Loss: 0.6165715456008911 \n",
      "     Training Step: 144 Training Loss: 0.6183440089225769 \n",
      "     Training Step: 145 Training Loss: 0.615473747253418 \n",
      "     Training Step: 146 Training Loss: 0.6104835867881775 \n",
      "     Training Step: 147 Training Loss: 0.614511251449585 \n",
      "     Training Step: 148 Training Loss: 0.6155433654785156 \n",
      "     Training Step: 149 Training Loss: 0.6189891695976257 \n",
      "     Training Step: 150 Training Loss: 0.6128522753715515 \n",
      "     Training Step: 151 Training Loss: 0.6124192476272583 \n",
      "     Training Step: 152 Training Loss: 0.6188888549804688 \n",
      "     Training Step: 153 Training Loss: 0.6174600720405579 \n",
      "     Training Step: 154 Training Loss: 0.6126137375831604 \n",
      "     Training Step: 155 Training Loss: 0.6122220754623413 \n",
      "     Training Step: 156 Training Loss: 0.6135368943214417 \n",
      "     Training Step: 157 Training Loss: 0.6144222617149353 \n",
      "     Training Step: 158 Training Loss: 0.6117562651634216 \n",
      "     Training Step: 159 Training Loss: 0.6119030714035034 \n",
      "     Training Step: 160 Training Loss: 0.6142098307609558 \n",
      "     Training Step: 161 Training Loss: 0.6140660047531128 \n",
      "     Training Step: 162 Training Loss: 0.6129465103149414 \n",
      "     Training Step: 163 Training Loss: 0.6147313714027405 \n",
      "     Training Step: 164 Training Loss: 0.6120925545692444 \n",
      "     Training Step: 165 Training Loss: 0.6133242249488831 \n",
      "     Training Step: 166 Training Loss: 0.617572009563446 \n",
      "     Training Step: 167 Training Loss: 0.6149582266807556 \n",
      "     Training Step: 168 Training Loss: 0.6119121313095093 \n",
      "     Training Step: 169 Training Loss: 0.6133156418800354 \n",
      "     Training Step: 170 Training Loss: 0.6142644286155701 \n",
      "     Training Step: 171 Training Loss: 0.6140767931938171 \n",
      "     Training Step: 172 Training Loss: 0.6148003339767456 \n",
      "     Training Step: 173 Training Loss: 0.6157075762748718 \n",
      "     Training Step: 174 Training Loss: 0.6153978109359741 \n",
      "     Training Step: 175 Training Loss: 0.6124869585037231 \n",
      "     Training Step: 176 Training Loss: 0.6184026598930359 \n",
      "     Training Step: 177 Training Loss: 0.612162709236145 \n",
      "     Training Step: 178 Training Loss: 0.6102695465087891 \n",
      "     Training Step: 179 Training Loss: 0.6132656931877136 \n",
      "     Training Step: 180 Training Loss: 0.6167840361595154 \n",
      "     Training Step: 181 Training Loss: 0.6177819967269897 \n",
      "     Training Step: 182 Training Loss: 0.6164757609367371 \n",
      "     Training Step: 183 Training Loss: 0.6153436899185181 \n",
      "     Training Step: 184 Training Loss: 0.6124559640884399 \n",
      "     Training Step: 185 Training Loss: 0.609478771686554 \n",
      "     Training Step: 186 Training Loss: 0.6124293208122253 \n",
      "     Training Step: 187 Training Loss: 0.6174048781394958 \n",
      "     Training Step: 188 Training Loss: 0.618067741394043 \n",
      "     Training Step: 189 Training Loss: 0.6156594753265381 \n",
      "     Training Step: 190 Training Loss: 0.6113202571868896 \n",
      "     Training Step: 191 Training Loss: 0.6168630719184875 \n",
      "     Training Step: 192 Training Loss: 0.6143760681152344 \n",
      "     Training Step: 193 Training Loss: 0.6155913472175598 \n",
      "     Training Step: 194 Training Loss: 0.618720531463623 \n",
      "     Training Step: 195 Training Loss: 0.6117220520973206 \n",
      "     Training Step: 196 Training Loss: 0.6167917847633362 \n",
      "     Training Step: 197 Training Loss: 0.6147114038467407 \n",
      "     Training Step: 198 Training Loss: 0.615513026714325 \n",
      "     Training Step: 199 Training Loss: 0.6158247590065002 \n",
      "     Training Step: 200 Training Loss: 0.6110506057739258 \n",
      "     Training Step: 201 Training Loss: 0.6115272045135498 \n",
      "     Training Step: 202 Training Loss: 0.6122375726699829 \n",
      "     Training Step: 203 Training Loss: 0.6137874722480774 \n",
      "     Training Step: 204 Training Loss: 0.6116482019424438 \n",
      "     Training Step: 205 Training Loss: 0.6152215600013733 \n",
      "     Training Step: 206 Training Loss: 0.6117241382598877 \n",
      "     Training Step: 207 Training Loss: 0.6172012090682983 \n",
      "     Training Step: 208 Training Loss: 0.6147222518920898 \n",
      "     Training Step: 209 Training Loss: 0.6133053302764893 \n",
      "     Training Step: 210 Training Loss: 0.6136398911476135 \n",
      "     Training Step: 211 Training Loss: 0.6156293749809265 \n",
      "     Training Step: 212 Training Loss: 0.6184117794036865 \n",
      "     Training Step: 213 Training Loss: 0.6130043864250183 \n",
      "     Training Step: 214 Training Loss: 0.614714503288269 \n",
      "     Training Step: 215 Training Loss: 0.6210243701934814 \n",
      "     Training Step: 216 Training Loss: 0.6161898374557495 \n",
      "     Training Step: 217 Training Loss: 0.6135661602020264 \n",
      "     Training Step: 218 Training Loss: 0.6139196157455444 \n",
      "     Training Step: 219 Training Loss: 0.6142594814300537 \n",
      "     Training Step: 220 Training Loss: 0.6155132055282593 \n",
      "     Training Step: 221 Training Loss: 0.6133790016174316 \n",
      "     Training Step: 222 Training Loss: 0.6163318753242493 \n",
      "     Training Step: 223 Training Loss: 0.6145018935203552 \n",
      "     Training Step: 224 Training Loss: 0.6125928163528442 \n",
      "     Training Step: 225 Training Loss: 0.6148061752319336 \n",
      "     Training Step: 226 Training Loss: 0.6167091727256775 \n",
      "     Training Step: 227 Training Loss: 0.6159563660621643 \n",
      "     Training Step: 228 Training Loss: 0.6126857399940491 \n",
      "     Training Step: 229 Training Loss: 0.6153339743614197 \n",
      "     Training Step: 230 Training Loss: 0.6130090355873108 \n",
      "     Training Step: 231 Training Loss: 0.614170253276825 \n",
      "     Training Step: 232 Training Loss: 0.6195539236068726 \n",
      "     Training Step: 233 Training Loss: 0.6185328364372253 \n",
      "     Training Step: 234 Training Loss: 0.6196610331535339 \n",
      "     Training Step: 235 Training Loss: 0.6120061874389648 \n",
      "     Training Step: 236 Training Loss: 0.6158377528190613 \n",
      "     Training Step: 237 Training Loss: 0.6142364740371704 \n",
      "     Training Step: 238 Training Loss: 0.618466854095459 \n",
      "     Training Step: 239 Training Loss: 0.6150312423706055 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6143112182617188 \n",
      "     Validation Step: 1 Validation Loss: 0.6156586408615112 \n",
      "     Validation Step: 2 Validation Loss: 0.615278422832489 \n",
      "     Validation Step: 3 Validation Loss: 0.6185246706008911 \n",
      "     Validation Step: 4 Validation Loss: 0.6146755218505859 \n",
      "     Validation Step: 5 Validation Loss: 0.6112384796142578 \n",
      "     Validation Step: 6 Validation Loss: 0.6156308650970459 \n",
      "     Validation Step: 7 Validation Loss: 0.613718569278717 \n",
      "     Validation Step: 8 Validation Loss: 0.6107374429702759 \n",
      "     Validation Step: 9 Validation Loss: 0.6105591654777527 \n",
      "     Validation Step: 10 Validation Loss: 0.6076344847679138 \n",
      "     Validation Step: 11 Validation Loss: 0.6153597235679626 \n",
      "     Validation Step: 12 Validation Loss: 0.6119469404220581 \n",
      "     Validation Step: 13 Validation Loss: 0.6121903657913208 \n",
      "     Validation Step: 14 Validation Loss: 0.6150822639465332 \n",
      "     Validation Step: 15 Validation Loss: 0.6128729581832886 \n",
      "     Validation Step: 16 Validation Loss: 0.6170699596405029 \n",
      "     Validation Step: 17 Validation Loss: 0.611701250076294 \n",
      "     Validation Step: 18 Validation Loss: 0.6173521280288696 \n",
      "     Validation Step: 19 Validation Loss: 0.6163107752799988 \n",
      "     Validation Step: 20 Validation Loss: 0.6136648058891296 \n",
      "     Validation Step: 21 Validation Loss: 0.6142550706863403 \n",
      "     Validation Step: 22 Validation Loss: 0.614960253238678 \n",
      "     Validation Step: 23 Validation Loss: 0.6146147847175598 \n",
      "     Validation Step: 24 Validation Loss: 0.616060733795166 \n",
      "     Validation Step: 25 Validation Loss: 0.614586353302002 \n",
      "     Validation Step: 26 Validation Loss: 0.6181093454360962 \n",
      "     Validation Step: 27 Validation Loss: 0.6102665662765503 \n",
      "     Validation Step: 28 Validation Loss: 0.6102473139762878 \n",
      "     Validation Step: 29 Validation Loss: 0.6182873249053955 \n",
      "     Validation Step: 30 Validation Loss: 0.6177310347557068 \n",
      "     Validation Step: 31 Validation Loss: 0.6148808002471924 \n",
      "     Validation Step: 32 Validation Loss: 0.6158368587493896 \n",
      "     Validation Step: 33 Validation Loss: 0.6133395433425903 \n",
      "     Validation Step: 34 Validation Loss: 0.6176353096961975 \n",
      "     Validation Step: 35 Validation Loss: 0.6112322211265564 \n",
      "     Validation Step: 36 Validation Loss: 0.6141699552536011 \n",
      "     Validation Step: 37 Validation Loss: 0.6116301417350769 \n",
      "     Validation Step: 38 Validation Loss: 0.6185469031333923 \n",
      "     Validation Step: 39 Validation Loss: 0.6102132201194763 \n",
      "     Validation Step: 40 Validation Loss: 0.6137033104896545 \n",
      "     Validation Step: 41 Validation Loss: 0.6130239963531494 \n",
      "     Validation Step: 42 Validation Loss: 0.6141713261604309 \n",
      "     Validation Step: 43 Validation Loss: 0.6105803847312927 \n",
      "     Validation Step: 44 Validation Loss: 0.6183872818946838 \n",
      "Epoch: 17\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6118712425231934 \n",
      "     Training Step: 1 Training Loss: 0.6178776621818542 \n",
      "     Training Step: 2 Training Loss: 0.6147325038909912 \n",
      "     Training Step: 3 Training Loss: 0.6127991080284119 \n",
      "     Training Step: 4 Training Loss: 0.6168532967567444 \n",
      "     Training Step: 5 Training Loss: 0.6116467118263245 \n",
      "     Training Step: 6 Training Loss: 0.6142853498458862 \n",
      "     Training Step: 7 Training Loss: 0.6162588596343994 \n",
      "     Training Step: 8 Training Loss: 0.6112198233604431 \n",
      "     Training Step: 9 Training Loss: 0.6149905920028687 \n",
      "     Training Step: 10 Training Loss: 0.6138631701469421 \n",
      "     Training Step: 11 Training Loss: 0.615715742111206 \n",
      "     Training Step: 12 Training Loss: 0.6139491200447083 \n",
      "     Training Step: 13 Training Loss: 0.6123477220535278 \n",
      "     Training Step: 14 Training Loss: 0.6177893280982971 \n",
      "     Training Step: 15 Training Loss: 0.612578809261322 \n",
      "     Training Step: 16 Training Loss: 0.6114723682403564 \n",
      "     Training Step: 17 Training Loss: 0.6153609156608582 \n",
      "     Training Step: 18 Training Loss: 0.6138260364532471 \n",
      "     Training Step: 19 Training Loss: 0.6181435585021973 \n",
      "     Training Step: 20 Training Loss: 0.6102057695388794 \n",
      "     Training Step: 21 Training Loss: 0.6117620468139648 \n",
      "     Training Step: 22 Training Loss: 0.6181237697601318 \n",
      "     Training Step: 23 Training Loss: 0.6159157752990723 \n",
      "     Training Step: 24 Training Loss: 0.6098558902740479 \n",
      "     Training Step: 25 Training Loss: 0.6124264001846313 \n",
      "     Training Step: 26 Training Loss: 0.6183209419250488 \n",
      "     Training Step: 27 Training Loss: 0.6124195456504822 \n",
      "     Training Step: 28 Training Loss: 0.6121929287910461 \n",
      "     Training Step: 29 Training Loss: 0.6118402481079102 \n",
      "     Training Step: 30 Training Loss: 0.6105355620384216 \n",
      "     Training Step: 31 Training Loss: 0.6182891726493835 \n",
      "     Training Step: 32 Training Loss: 0.6094433665275574 \n",
      "     Training Step: 33 Training Loss: 0.6179206967353821 \n",
      "     Training Step: 34 Training Loss: 0.6084182262420654 \n",
      "     Training Step: 35 Training Loss: 0.6104694604873657 \n",
      "     Training Step: 36 Training Loss: 0.6148467063903809 \n",
      "     Training Step: 37 Training Loss: 0.6147140264511108 \n",
      "     Training Step: 38 Training Loss: 0.6102217435836792 \n",
      "     Training Step: 39 Training Loss: 0.6137847304344177 \n",
      "     Training Step: 40 Training Loss: 0.6133266091346741 \n",
      "     Training Step: 41 Training Loss: 0.6199883222579956 \n",
      "     Training Step: 42 Training Loss: 0.6099802851676941 \n",
      "     Training Step: 43 Training Loss: 0.61322021484375 \n",
      "     Training Step: 44 Training Loss: 0.6194800734519958 \n",
      "     Training Step: 45 Training Loss: 0.6137950420379639 \n",
      "     Training Step: 46 Training Loss: 0.6144164204597473 \n",
      "     Training Step: 47 Training Loss: 0.6129473447799683 \n",
      "     Training Step: 48 Training Loss: 0.611270010471344 \n",
      "     Training Step: 49 Training Loss: 0.6157338619232178 \n",
      "     Training Step: 50 Training Loss: 0.6135392785072327 \n",
      "     Training Step: 51 Training Loss: 0.6116936802864075 \n",
      "     Training Step: 52 Training Loss: 0.6146285533905029 \n",
      "     Training Step: 53 Training Loss: 0.612503170967102 \n",
      "     Training Step: 54 Training Loss: 0.6122725009918213 \n",
      "     Training Step: 55 Training Loss: 0.6144293546676636 \n",
      "     Training Step: 56 Training Loss: 0.61483234167099 \n",
      "     Training Step: 57 Training Loss: 0.6169582605361938 \n",
      "     Training Step: 58 Training Loss: 0.6107469201087952 \n",
      "     Training Step: 59 Training Loss: 0.6133705973625183 \n",
      "     Training Step: 60 Training Loss: 0.6141868233680725 \n",
      "     Training Step: 61 Training Loss: 0.6147334575653076 \n",
      "     Training Step: 62 Training Loss: 0.6184617877006531 \n",
      "     Training Step: 63 Training Loss: 0.6158043146133423 \n",
      "     Training Step: 64 Training Loss: 0.6146236062049866 \n",
      "     Training Step: 65 Training Loss: 0.6168478727340698 \n",
      "     Training Step: 66 Training Loss: 0.6168496608734131 \n",
      "     Training Step: 67 Training Loss: 0.6155771017074585 \n",
      "     Training Step: 68 Training Loss: 0.6140011548995972 \n",
      "     Training Step: 69 Training Loss: 0.6129390597343445 \n",
      "     Training Step: 70 Training Loss: 0.6135259866714478 \n",
      "     Training Step: 71 Training Loss: 0.6104106307029724 \n",
      "     Training Step: 72 Training Loss: 0.6132804751396179 \n",
      "     Training Step: 73 Training Loss: 0.6176658868789673 \n",
      "     Training Step: 74 Training Loss: 0.6144027709960938 \n",
      "     Training Step: 75 Training Loss: 0.6153773069381714 \n",
      "     Training Step: 76 Training Loss: 0.6107063293457031 \n",
      "     Training Step: 77 Training Loss: 0.6096351742744446 \n",
      "     Training Step: 78 Training Loss: 0.6162163019180298 \n",
      "     Training Step: 79 Training Loss: 0.6137248873710632 \n",
      "     Training Step: 80 Training Loss: 0.6141313314437866 \n",
      "     Training Step: 81 Training Loss: 0.6122972369194031 \n",
      "     Training Step: 82 Training Loss: 0.6131532192230225 \n",
      "     Training Step: 83 Training Loss: 0.6167619824409485 \n",
      "     Training Step: 84 Training Loss: 0.6172098517417908 \n",
      "     Training Step: 85 Training Loss: 0.6142883896827698 \n",
      "     Training Step: 86 Training Loss: 0.6164525151252747 \n",
      "     Training Step: 87 Training Loss: 0.6116530895233154 \n",
      "     Training Step: 88 Training Loss: 0.6177665591239929 \n",
      "     Training Step: 89 Training Loss: 0.6121202111244202 \n",
      "     Training Step: 90 Training Loss: 0.6177753806114197 \n",
      "     Training Step: 91 Training Loss: 0.6100969910621643 \n",
      "     Training Step: 92 Training Loss: 0.6198013424873352 \n",
      "     Training Step: 93 Training Loss: 0.6154634952545166 \n",
      "     Training Step: 94 Training Loss: 0.6168208122253418 \n",
      "     Training Step: 95 Training Loss: 0.6110462546348572 \n",
      "     Training Step: 96 Training Loss: 0.6154055595397949 \n",
      "     Training Step: 97 Training Loss: 0.6128039956092834 \n",
      "     Training Step: 98 Training Loss: 0.6170536279678345 \n",
      "     Training Step: 99 Training Loss: 0.6154013872146606 \n",
      "     Training Step: 100 Training Loss: 0.6210806965827942 \n",
      "     Training Step: 101 Training Loss: 0.6129157543182373 \n",
      "     Training Step: 102 Training Loss: 0.6167290806770325 \n",
      "     Training Step: 103 Training Loss: 0.6154718995094299 \n",
      "     Training Step: 104 Training Loss: 0.6116617321968079 \n",
      "     Training Step: 105 Training Loss: 0.6189004778862 \n",
      "     Training Step: 106 Training Loss: 0.6136857867240906 \n",
      "     Training Step: 107 Training Loss: 0.6155966520309448 \n",
      "     Training Step: 108 Training Loss: 0.616101086139679 \n",
      "     Training Step: 109 Training Loss: 0.6116194128990173 \n",
      "     Training Step: 110 Training Loss: 0.6154622435569763 \n",
      "     Training Step: 111 Training Loss: 0.6107443571090698 \n",
      "     Training Step: 112 Training Loss: 0.6174951195716858 \n",
      "     Training Step: 113 Training Loss: 0.6187156438827515 \n",
      "     Training Step: 114 Training Loss: 0.6153439879417419 \n",
      "     Training Step: 115 Training Loss: 0.6126616597175598 \n",
      "     Training Step: 116 Training Loss: 0.6157947182655334 \n",
      "     Training Step: 117 Training Loss: 0.6116700768470764 \n",
      "     Training Step: 118 Training Loss: 0.6160803437232971 \n",
      "     Training Step: 119 Training Loss: 0.6173673868179321 \n",
      "     Training Step: 120 Training Loss: 0.6125941276550293 \n",
      "     Training Step: 121 Training Loss: 0.6102409362792969 \n",
      "     Training Step: 122 Training Loss: 0.6150136590003967 \n",
      "     Training Step: 123 Training Loss: 0.6187055110931396 \n",
      "     Training Step: 124 Training Loss: 0.614241898059845 \n",
      "     Training Step: 125 Training Loss: 0.6147239804267883 \n",
      "     Training Step: 126 Training Loss: 0.6167519688606262 \n",
      "     Training Step: 127 Training Loss: 0.6133949160575867 \n",
      "     Training Step: 128 Training Loss: 0.611780047416687 \n",
      "     Training Step: 129 Training Loss: 0.6146841049194336 \n",
      "     Training Step: 130 Training Loss: 0.6119403839111328 \n",
      "     Training Step: 131 Training Loss: 0.6164158582687378 \n",
      "     Training Step: 132 Training Loss: 0.6156801581382751 \n",
      "     Training Step: 133 Training Loss: 0.6140887141227722 \n",
      "     Training Step: 134 Training Loss: 0.6155819296836853 \n",
      "     Training Step: 135 Training Loss: 0.6108894348144531 \n",
      "     Training Step: 136 Training Loss: 0.6182540059089661 \n",
      "     Training Step: 137 Training Loss: 0.609836220741272 \n",
      "     Training Step: 138 Training Loss: 0.6140760779380798 \n",
      "     Training Step: 139 Training Loss: 0.6107261180877686 \n",
      "     Training Step: 140 Training Loss: 0.6158572435379028 \n",
      "     Training Step: 141 Training Loss: 0.6155304312705994 \n",
      "     Training Step: 142 Training Loss: 0.6142513751983643 \n",
      "     Training Step: 143 Training Loss: 0.6110730171203613 \n",
      "     Training Step: 144 Training Loss: 0.6129997968673706 \n",
      "     Training Step: 145 Training Loss: 0.6157686114311218 \n",
      "     Training Step: 146 Training Loss: 0.6093174815177917 \n",
      "     Training Step: 147 Training Loss: 0.6147075891494751 \n",
      "     Training Step: 148 Training Loss: 0.6186515688896179 \n",
      "     Training Step: 149 Training Loss: 0.6160374283790588 \n",
      "     Training Step: 150 Training Loss: 0.6151972413063049 \n",
      "     Training Step: 151 Training Loss: 0.618391752243042 \n",
      "     Training Step: 152 Training Loss: 0.6131614446640015 \n",
      "     Training Step: 153 Training Loss: 0.6146077513694763 \n",
      "     Training Step: 154 Training Loss: 0.6149321794509888 \n",
      "     Training Step: 155 Training Loss: 0.6101869344711304 \n",
      "     Training Step: 156 Training Loss: 0.6152437925338745 \n",
      "     Training Step: 157 Training Loss: 0.6145839691162109 \n",
      "     Training Step: 158 Training Loss: 0.617854118347168 \n",
      "     Training Step: 159 Training Loss: 0.6101258993148804 \n",
      "     Training Step: 160 Training Loss: 0.6147447228431702 \n",
      "     Training Step: 161 Training Loss: 0.6148026585578918 \n",
      "     Training Step: 162 Training Loss: 0.6165601015090942 \n",
      "     Training Step: 163 Training Loss: 0.6135204434394836 \n",
      "     Training Step: 164 Training Loss: 0.6118784546852112 \n",
      "     Training Step: 165 Training Loss: 0.6109489798545837 \n",
      "     Training Step: 166 Training Loss: 0.6182017922401428 \n",
      "     Training Step: 167 Training Loss: 0.6112828254699707 \n",
      "     Training Step: 168 Training Loss: 0.6172038912773132 \n",
      "     Training Step: 169 Training Loss: 0.6144582629203796 \n",
      "     Training Step: 170 Training Loss: 0.6171693801879883 \n",
      "     Training Step: 171 Training Loss: 0.6156085729598999 \n",
      "     Training Step: 172 Training Loss: 0.6140315532684326 \n",
      "     Training Step: 173 Training Loss: 0.6166784763336182 \n",
      "     Training Step: 174 Training Loss: 0.6152034997940063 \n",
      "     Training Step: 175 Training Loss: 0.6149653792381287 \n",
      "     Training Step: 176 Training Loss: 0.6132398247718811 \n",
      "     Training Step: 177 Training Loss: 0.6178280711174011 \n",
      "     Training Step: 178 Training Loss: 0.6130834817886353 \n",
      "     Training Step: 179 Training Loss: 0.6166380643844604 \n",
      "     Training Step: 180 Training Loss: 0.6134041547775269 \n",
      "     Training Step: 181 Training Loss: 0.6143966317176819 \n",
      "     Training Step: 182 Training Loss: 0.6107628345489502 \n",
      "     Training Step: 183 Training Loss: 0.614680290222168 \n",
      "     Training Step: 184 Training Loss: 0.6168078780174255 \n",
      "     Training Step: 185 Training Loss: 0.6135134100914001 \n",
      "     Training Step: 186 Training Loss: 0.6121485233306885 \n",
      "     Training Step: 187 Training Loss: 0.6198139190673828 \n",
      "     Training Step: 188 Training Loss: 0.6128743886947632 \n",
      "     Training Step: 189 Training Loss: 0.6117700338363647 \n",
      "     Training Step: 190 Training Loss: 0.6148244738578796 \n",
      "     Training Step: 191 Training Loss: 0.6121262311935425 \n",
      "     Training Step: 192 Training Loss: 0.6141471266746521 \n",
      "     Training Step: 193 Training Loss: 0.6143855452537537 \n",
      "     Training Step: 194 Training Loss: 0.6116949915885925 \n",
      "     Training Step: 195 Training Loss: 0.6115519404411316 \n",
      "     Training Step: 196 Training Loss: 0.6123412847518921 \n",
      "     Training Step: 197 Training Loss: 0.6164464354515076 \n",
      "     Training Step: 198 Training Loss: 0.6136407256126404 \n",
      "     Training Step: 199 Training Loss: 0.6135982871055603 \n",
      "     Training Step: 200 Training Loss: 0.6115472316741943 \n",
      "     Training Step: 201 Training Loss: 0.6118788719177246 \n",
      "     Training Step: 202 Training Loss: 0.6163160800933838 \n",
      "     Training Step: 203 Training Loss: 0.6107053756713867 \n",
      "     Training Step: 204 Training Loss: 0.6168867945671082 \n",
      "     Training Step: 205 Training Loss: 0.6127171516418457 \n",
      "     Training Step: 206 Training Loss: 0.6152785420417786 \n",
      "     Training Step: 207 Training Loss: 0.6122167706489563 \n",
      "     Training Step: 208 Training Loss: 0.6122474074363708 \n",
      "     Training Step: 209 Training Loss: 0.618950366973877 \n",
      "     Training Step: 210 Training Loss: 0.6126097440719604 \n",
      "     Training Step: 211 Training Loss: 0.6133400201797485 \n",
      "     Training Step: 212 Training Loss: 0.6141195893287659 \n",
      "     Training Step: 213 Training Loss: 0.6136797666549683 \n",
      "     Training Step: 214 Training Loss: 0.6167820692062378 \n",
      "     Training Step: 215 Training Loss: 0.613547146320343 \n",
      "     Training Step: 216 Training Loss: 0.6128917932510376 \n",
      "     Training Step: 217 Training Loss: 0.611655592918396 \n",
      "     Training Step: 218 Training Loss: 0.6132452487945557 \n",
      "     Training Step: 219 Training Loss: 0.6172614693641663 \n",
      "     Training Step: 220 Training Loss: 0.6131277680397034 \n",
      "     Training Step: 221 Training Loss: 0.613000214099884 \n",
      "     Training Step: 222 Training Loss: 0.6118429899215698 \n",
      "     Training Step: 223 Training Loss: 0.61531662940979 \n",
      "     Training Step: 224 Training Loss: 0.6155170798301697 \n",
      "     Training Step: 225 Training Loss: 0.6147357225418091 \n",
      "     Training Step: 226 Training Loss: 0.6119316220283508 \n",
      "     Training Step: 227 Training Loss: 0.6144736409187317 \n",
      "     Training Step: 228 Training Loss: 0.6114470958709717 \n",
      "     Training Step: 229 Training Loss: 0.6146953701972961 \n",
      "     Training Step: 230 Training Loss: 0.620376706123352 \n",
      "     Training Step: 231 Training Loss: 0.6124218106269836 \n",
      "     Training Step: 232 Training Loss: 0.6184585690498352 \n",
      "     Training Step: 233 Training Loss: 0.6107609272003174 \n",
      "     Training Step: 234 Training Loss: 0.6123420596122742 \n",
      "     Training Step: 235 Training Loss: 0.6164572834968567 \n",
      "     Training Step: 236 Training Loss: 0.611500084400177 \n",
      "     Training Step: 237 Training Loss: 0.6151622533798218 \n",
      "     Training Step: 238 Training Loss: 0.6167622804641724 \n",
      "     Training Step: 239 Training Loss: 0.6151291728019714 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6137351989746094 \n",
      "     Validation Step: 1 Validation Loss: 0.6145700216293335 \n",
      "     Validation Step: 2 Validation Loss: 0.6119740605354309 \n",
      "     Validation Step: 3 Validation Loss: 0.6156463027000427 \n",
      "     Validation Step: 4 Validation Loss: 0.6150780916213989 \n",
      "     Validation Step: 5 Validation Loss: 0.6112548112869263 \n",
      "     Validation Step: 6 Validation Loss: 0.6102634072303772 \n",
      "     Validation Step: 7 Validation Loss: 0.618077278137207 \n",
      "     Validation Step: 8 Validation Loss: 0.6149527430534363 \n",
      "     Validation Step: 9 Validation Loss: 0.6153573989868164 \n",
      "     Validation Step: 10 Validation Loss: 0.6137059330940247 \n",
      "     Validation Step: 11 Validation Loss: 0.610580325126648 \n",
      "     Validation Step: 12 Validation Loss: 0.6158452033996582 \n",
      "     Validation Step: 13 Validation Loss: 0.6112497448921204 \n",
      "     Validation Step: 14 Validation Loss: 0.6102406978607178 \n",
      "     Validation Step: 15 Validation Loss: 0.6184850931167603 \n",
      "     Validation Step: 16 Validation Loss: 0.611648678779602 \n",
      "     Validation Step: 17 Validation Loss: 0.6162964105606079 \n",
      "     Validation Step: 18 Validation Loss: 0.6170353293418884 \n",
      "     Validation Step: 19 Validation Loss: 0.6148850917816162 \n",
      "     Validation Step: 20 Validation Loss: 0.6185041666030884 \n",
      "     Validation Step: 21 Validation Loss: 0.6141803860664368 \n",
      "     Validation Step: 22 Validation Loss: 0.6173204779624939 \n",
      "     Validation Step: 23 Validation Loss: 0.6076876521110535 \n",
      "     Validation Step: 24 Validation Loss: 0.6122123003005981 \n",
      "     Validation Step: 25 Validation Loss: 0.6145968437194824 \n",
      "     Validation Step: 26 Validation Loss: 0.6146721839904785 \n",
      "     Validation Step: 27 Validation Loss: 0.6141714453697205 \n",
      "     Validation Step: 28 Validation Loss: 0.6142571568489075 \n",
      "     Validation Step: 29 Validation Loss: 0.617605447769165 \n",
      "     Validation Step: 30 Validation Loss: 0.6133337020874023 \n",
      "     Validation Step: 31 Validation Loss: 0.6106137633323669 \n",
      "     Validation Step: 32 Validation Loss: 0.610271155834198 \n",
      "     Validation Step: 33 Validation Loss: 0.6117123365402222 \n",
      "     Validation Step: 34 Validation Loss: 0.6130496263504028 \n",
      "     Validation Step: 35 Validation Loss: 0.6160544157028198 \n",
      "     Validation Step: 36 Validation Loss: 0.612892210483551 \n",
      "     Validation Step: 37 Validation Loss: 0.6142924427986145 \n",
      "     Validation Step: 38 Validation Loss: 0.6177111268043518 \n",
      "     Validation Step: 39 Validation Loss: 0.6152567267417908 \n",
      "     Validation Step: 40 Validation Loss: 0.6183605790138245 \n",
      "     Validation Step: 41 Validation Loss: 0.6182577013969421 \n",
      "     Validation Step: 42 Validation Loss: 0.6136749386787415 \n",
      "     Validation Step: 43 Validation Loss: 0.6156028509140015 \n",
      "     Validation Step: 44 Validation Loss: 0.6107450723648071 \n",
      "Epoch: 18\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6098430156707764 \n",
      "     Training Step: 1 Training Loss: 0.6172531247138977 \n",
      "     Training Step: 2 Training Loss: 0.6141281723976135 \n",
      "     Training Step: 3 Training Loss: 0.6157804727554321 \n",
      "     Training Step: 4 Training Loss: 0.6153855323791504 \n",
      "     Training Step: 5 Training Loss: 0.6154102087020874 \n",
      "     Training Step: 6 Training Loss: 0.6106777787208557 \n",
      "     Training Step: 7 Training Loss: 0.6131816506385803 \n",
      "     Training Step: 8 Training Loss: 0.611668586730957 \n",
      "     Training Step: 9 Training Loss: 0.6158397793769836 \n",
      "     Training Step: 10 Training Loss: 0.6161032915115356 \n",
      "     Training Step: 11 Training Loss: 0.615788459777832 \n",
      "     Training Step: 12 Training Loss: 0.6139855980873108 \n",
      "     Training Step: 13 Training Loss: 0.6126716136932373 \n",
      "     Training Step: 14 Training Loss: 0.6168127655982971 \n",
      "     Training Step: 15 Training Loss: 0.6148019433021545 \n",
      "     Training Step: 16 Training Loss: 0.613681972026825 \n",
      "     Training Step: 17 Training Loss: 0.6107261776924133 \n",
      "     Training Step: 18 Training Loss: 0.6101089119911194 \n",
      "     Training Step: 19 Training Loss: 0.6094696521759033 \n",
      "     Training Step: 20 Training Loss: 0.6180499792098999 \n",
      "     Training Step: 21 Training Loss: 0.6183873414993286 \n",
      "     Training Step: 22 Training Loss: 0.6120099425315857 \n",
      "     Training Step: 23 Training Loss: 0.6108191609382629 \n",
      "     Training Step: 24 Training Loss: 0.6150132417678833 \n",
      "     Training Step: 25 Training Loss: 0.6164812445640564 \n",
      "     Training Step: 26 Training Loss: 0.6143796443939209 \n",
      "     Training Step: 27 Training Loss: 0.6106191277503967 \n",
      "     Training Step: 28 Training Loss: 0.6117398142814636 \n",
      "     Training Step: 29 Training Loss: 0.6140850186347961 \n",
      "     Training Step: 30 Training Loss: 0.6146613955497742 \n",
      "     Training Step: 31 Training Loss: 0.614489734172821 \n",
      "     Training Step: 32 Training Loss: 0.6119148135185242 \n",
      "     Training Step: 33 Training Loss: 0.6186520457267761 \n",
      "     Training Step: 34 Training Loss: 0.6161748170852661 \n",
      "     Training Step: 35 Training Loss: 0.6166694164276123 \n",
      "     Training Step: 36 Training Loss: 0.6147509813308716 \n",
      "     Training Step: 37 Training Loss: 0.6176963448524475 \n",
      "     Training Step: 38 Training Loss: 0.6146940588951111 \n",
      "     Training Step: 39 Training Loss: 0.6123222708702087 \n",
      "     Training Step: 40 Training Loss: 0.6084843873977661 \n",
      "     Training Step: 41 Training Loss: 0.6153037548065186 \n",
      "     Training Step: 42 Training Loss: 0.6115790009498596 \n",
      "     Training Step: 43 Training Loss: 0.611234188079834 \n",
      "     Training Step: 44 Training Loss: 0.61078280210495 \n",
      "     Training Step: 45 Training Loss: 0.6156101226806641 \n",
      "     Training Step: 46 Training Loss: 0.6203948855400085 \n",
      "     Training Step: 47 Training Loss: 0.6178078055381775 \n",
      "     Training Step: 48 Training Loss: 0.6143057346343994 \n",
      "     Training Step: 49 Training Loss: 0.6154383420944214 \n",
      "     Training Step: 50 Training Loss: 0.6124449372291565 \n",
      "     Training Step: 51 Training Loss: 0.6155528426170349 \n",
      "     Training Step: 52 Training Loss: 0.6113232970237732 \n",
      "     Training Step: 53 Training Loss: 0.6133388876914978 \n",
      "     Training Step: 54 Training Loss: 0.6117168068885803 \n",
      "     Training Step: 55 Training Loss: 0.6152578592300415 \n",
      "     Training Step: 56 Training Loss: 0.6141811013221741 \n",
      "     Training Step: 57 Training Loss: 0.6163437962532043 \n",
      "     Training Step: 58 Training Loss: 0.6164520382881165 \n",
      "     Training Step: 59 Training Loss: 0.6169641613960266 \n",
      "     Training Step: 60 Training Loss: 0.6138044595718384 \n",
      "     Training Step: 61 Training Loss: 0.615852415561676 \n",
      "     Training Step: 62 Training Loss: 0.6141918301582336 \n",
      "     Training Step: 63 Training Loss: 0.6168110370635986 \n",
      "     Training Step: 64 Training Loss: 0.6129577159881592 \n",
      "     Training Step: 65 Training Loss: 0.6142599582672119 \n",
      "     Training Step: 66 Training Loss: 0.6125273108482361 \n",
      "     Training Step: 67 Training Loss: 0.6108190417289734 \n",
      "     Training Step: 68 Training Loss: 0.6179187893867493 \n",
      "     Training Step: 69 Training Loss: 0.6159796714782715 \n",
      "     Training Step: 70 Training Loss: 0.6121902465820312 \n",
      "     Training Step: 71 Training Loss: 0.6107237339019775 \n",
      "     Training Step: 72 Training Loss: 0.611854612827301 \n",
      "     Training Step: 73 Training Loss: 0.6131484508514404 \n",
      "     Training Step: 74 Training Loss: 0.6134882569313049 \n",
      "     Training Step: 75 Training Loss: 0.6152545213699341 \n",
      "     Training Step: 76 Training Loss: 0.6114716529846191 \n",
      "     Training Step: 77 Training Loss: 0.6116459965705872 \n",
      "     Training Step: 78 Training Loss: 0.6149271130561829 \n",
      "     Training Step: 79 Training Loss: 0.6130139231681824 \n",
      "     Training Step: 80 Training Loss: 0.6153255701065063 \n",
      "     Training Step: 81 Training Loss: 0.6164757013320923 \n",
      "     Training Step: 82 Training Loss: 0.6143598556518555 \n",
      "     Training Step: 83 Training Loss: 0.6134161949157715 \n",
      "     Training Step: 84 Training Loss: 0.6145527362823486 \n",
      "     Training Step: 85 Training Loss: 0.6141918301582336 \n",
      "     Training Step: 86 Training Loss: 0.6146739721298218 \n",
      "     Training Step: 87 Training Loss: 0.6123080849647522 \n",
      "     Training Step: 88 Training Loss: 0.61327064037323 \n",
      "     Training Step: 89 Training Loss: 0.6146642565727234 \n",
      "     Training Step: 90 Training Loss: 0.6135114431381226 \n",
      "     Training Step: 91 Training Loss: 0.6182027459144592 \n",
      "     Training Step: 92 Training Loss: 0.6144660711288452 \n",
      "     Training Step: 93 Training Loss: 0.611659586429596 \n",
      "     Training Step: 94 Training Loss: 0.6124331951141357 \n",
      "     Training Step: 95 Training Loss: 0.6126998066902161 \n",
      "     Training Step: 96 Training Loss: 0.6118859052658081 \n",
      "     Training Step: 97 Training Loss: 0.6111876368522644 \n",
      "     Training Step: 98 Training Loss: 0.6157283186912537 \n",
      "     Training Step: 99 Training Loss: 0.6114766597747803 \n",
      "     Training Step: 100 Training Loss: 0.6144458651542664 \n",
      "     Training Step: 101 Training Loss: 0.6125913262367249 \n",
      "     Training Step: 102 Training Loss: 0.6102087497711182 \n",
      "     Training Step: 103 Training Loss: 0.6114444732666016 \n",
      "     Training Step: 104 Training Loss: 0.6154378652572632 \n",
      "     Training Step: 105 Training Loss: 0.6155909895896912 \n",
      "     Training Step: 106 Training Loss: 0.6135526299476624 \n",
      "     Training Step: 107 Training Loss: 0.6182541847229004 \n",
      "     Training Step: 108 Training Loss: 0.6133378744125366 \n",
      "     Training Step: 109 Training Loss: 0.612883448600769 \n",
      "     Training Step: 110 Training Loss: 0.6171783804893494 \n",
      "     Training Step: 111 Training Loss: 0.6167709231376648 \n",
      "     Training Step: 112 Training Loss: 0.6107998490333557 \n",
      "     Training Step: 113 Training Loss: 0.6183640360832214 \n",
      "     Training Step: 114 Training Loss: 0.6137855648994446 \n",
      "     Training Step: 115 Training Loss: 0.6100999712944031 \n",
      "     Training Step: 116 Training Loss: 0.612520694732666 \n",
      "     Training Step: 117 Training Loss: 0.6123663783073425 \n",
      "     Training Step: 118 Training Loss: 0.6152387261390686 \n",
      "     Training Step: 119 Training Loss: 0.6153833866119385 \n",
      "     Training Step: 120 Training Loss: 0.6197035908699036 \n",
      "     Training Step: 121 Training Loss: 0.6118701100349426 \n",
      "     Training Step: 122 Training Loss: 0.6139020323753357 \n",
      "     Training Step: 123 Training Loss: 0.6131458878517151 \n",
      "     Training Step: 124 Training Loss: 0.6147383451461792 \n",
      "     Training Step: 125 Training Loss: 0.6106806397438049 \n",
      "     Training Step: 126 Training Loss: 0.6168655157089233 \n",
      "     Training Step: 127 Training Loss: 0.6133471131324768 \n",
      "     Training Step: 128 Training Loss: 0.6147409677505493 \n",
      "     Training Step: 129 Training Loss: 0.6135672926902771 \n",
      "     Training Step: 130 Training Loss: 0.6134605407714844 \n",
      "     Training Step: 131 Training Loss: 0.6121407151222229 \n",
      "     Training Step: 132 Training Loss: 0.6160508394241333 \n",
      "     Training Step: 133 Training Loss: 0.6167232990264893 \n",
      "     Training Step: 134 Training Loss: 0.613184928894043 \n",
      "     Training Step: 135 Training Loss: 0.6144282817840576 \n",
      "     Training Step: 136 Training Loss: 0.6167489290237427 \n",
      "     Training Step: 137 Training Loss: 0.6177650094032288 \n",
      "     Training Step: 138 Training Loss: 0.6172100305557251 \n",
      "     Training Step: 139 Training Loss: 0.6104173064231873 \n",
      "     Training Step: 140 Training Loss: 0.6123399138450623 \n",
      "     Training Step: 141 Training Loss: 0.612460732460022 \n",
      "     Training Step: 142 Training Loss: 0.6121930480003357 \n",
      "     Training Step: 143 Training Loss: 0.6119266748428345 \n",
      "     Training Step: 144 Training Loss: 0.6155514717102051 \n",
      "     Training Step: 145 Training Loss: 0.6147691011428833 \n",
      "     Training Step: 146 Training Loss: 0.6129684448242188 \n",
      "     Training Step: 147 Training Loss: 0.6163004636764526 \n",
      "     Training Step: 148 Training Loss: 0.6175558567047119 \n",
      "     Training Step: 149 Training Loss: 0.6130900979042053 \n",
      "     Training Step: 150 Training Loss: 0.6171497702598572 \n",
      "     Training Step: 151 Training Loss: 0.6210277080535889 \n",
      "     Training Step: 152 Training Loss: 0.6154820322990417 \n",
      "     Training Step: 153 Training Loss: 0.6184563040733337 \n",
      "     Training Step: 154 Training Loss: 0.6137905716896057 \n",
      "     Training Step: 155 Training Loss: 0.6130151152610779 \n",
      "     Training Step: 156 Training Loss: 0.6137932538986206 \n",
      "     Training Step: 157 Training Loss: 0.617266833782196 \n",
      "     Training Step: 158 Training Loss: 0.6143888235092163 \n",
      "     Training Step: 159 Training Loss: 0.6117309927940369 \n",
      "     Training Step: 160 Training Loss: 0.6157506108283997 \n",
      "     Training Step: 161 Training Loss: 0.6102001070976257 \n",
      "     Training Step: 162 Training Loss: 0.6104094386100769 \n",
      "     Training Step: 163 Training Loss: 0.6127339005470276 \n",
      "     Training Step: 164 Training Loss: 0.610501766204834 \n",
      "     Training Step: 165 Training Loss: 0.6140093803405762 \n",
      "     Training Step: 166 Training Loss: 0.6142802834510803 \n",
      "     Training Step: 167 Training Loss: 0.6147887706756592 \n",
      "     Training Step: 168 Training Loss: 0.6136717200279236 \n",
      "     Training Step: 169 Training Loss: 0.6115990877151489 \n",
      "     Training Step: 170 Training Loss: 0.6165180802345276 \n",
      "     Training Step: 171 Training Loss: 0.6184687614440918 \n",
      "     Training Step: 172 Training Loss: 0.6199447512626648 \n",
      "     Training Step: 173 Training Loss: 0.6096577048301697 \n",
      "     Training Step: 174 Training Loss: 0.6145389676094055 \n",
      "     Training Step: 175 Training Loss: 0.6133289933204651 \n",
      "     Training Step: 176 Training Loss: 0.6097956895828247 \n",
      "     Training Step: 177 Training Loss: 0.6129254102706909 \n",
      "     Training Step: 178 Training Loss: 0.6136072874069214 \n",
      "     Training Step: 179 Training Loss: 0.6175997257232666 \n",
      "     Training Step: 180 Training Loss: 0.6123933792114258 \n",
      "     Training Step: 181 Training Loss: 0.6189106702804565 \n",
      "     Training Step: 182 Training Loss: 0.6180984973907471 \n",
      "     Training Step: 183 Training Loss: 0.6117744445800781 \n",
      "     Training Step: 184 Training Loss: 0.6166984438896179 \n",
      "     Training Step: 185 Training Loss: 0.6150389313697815 \n",
      "     Training Step: 186 Training Loss: 0.6181032061576843 \n",
      "     Training Step: 187 Training Loss: 0.613286554813385 \n",
      "     Training Step: 188 Training Loss: 0.6148055195808411 \n",
      "     Training Step: 189 Training Loss: 0.616685152053833 \n",
      "     Training Step: 190 Training Loss: 0.6167543530464172 \n",
      "     Training Step: 191 Training Loss: 0.6145530939102173 \n",
      "     Training Step: 192 Training Loss: 0.6147432327270508 \n",
      "     Training Step: 193 Training Loss: 0.6122497320175171 \n",
      "     Training Step: 194 Training Loss: 0.610950767993927 \n",
      "     Training Step: 195 Training Loss: 0.61695796251297 \n",
      "     Training Step: 196 Training Loss: 0.6151628494262695 \n",
      "     Training Step: 197 Training Loss: 0.61558997631073 \n",
      "     Training Step: 198 Training Loss: 0.6154107451438904 \n",
      "     Training Step: 199 Training Loss: 0.6130309700965881 \n",
      "     Training Step: 200 Training Loss: 0.615982174873352 \n",
      "     Training Step: 201 Training Loss: 0.6163089275360107 \n",
      "     Training Step: 202 Training Loss: 0.6149527430534363 \n",
      "     Training Step: 203 Training Loss: 0.6118724346160889 \n",
      "     Training Step: 204 Training Loss: 0.6186928153038025 \n",
      "     Training Step: 205 Training Loss: 0.614302933216095 \n",
      "     Training Step: 206 Training Loss: 0.617651104927063 \n",
      "     Training Step: 207 Training Loss: 0.609435498714447 \n",
      "     Training Step: 208 Training Loss: 0.6124305129051208 \n",
      "     Training Step: 209 Training Loss: 0.6155945658683777 \n",
      "     Training Step: 210 Training Loss: 0.6115480065345764 \n",
      "     Training Step: 211 Training Loss: 0.610409677028656 \n",
      "     Training Step: 212 Training Loss: 0.6108981370925903 \n",
      "     Training Step: 213 Training Loss: 0.6097335815429688 \n",
      "     Training Step: 214 Training Loss: 0.6122551560401917 \n",
      "     Training Step: 215 Training Loss: 0.6183023452758789 \n",
      "     Training Step: 216 Training Loss: 0.6168687343597412 \n",
      "     Training Step: 217 Training Loss: 0.6188792586326599 \n",
      "     Training Step: 218 Training Loss: 0.6140995621681213 \n",
      "     Training Step: 219 Training Loss: 0.6196789145469666 \n",
      "     Training Step: 220 Training Loss: 0.6118884682655334 \n",
      "     Training Step: 221 Training Loss: 0.6104008555412292 \n",
      "     Training Step: 222 Training Loss: 0.6115859150886536 \n",
      "     Training Step: 223 Training Loss: 0.6199923753738403 \n",
      "     Training Step: 224 Training Loss: 0.6148638129234314 \n",
      "     Training Step: 225 Training Loss: 0.6170694828033447 \n",
      "     Training Step: 226 Training Loss: 0.6148118376731873 \n",
      "     Training Step: 227 Training Loss: 0.6136005520820618 \n",
      "     Training Step: 228 Training Loss: 0.6156557202339172 \n",
      "     Training Step: 229 Training Loss: 0.612643837928772 \n",
      "     Training Step: 230 Training Loss: 0.6185393929481506 \n",
      "     Training Step: 231 Training Loss: 0.6154267191886902 \n",
      "     Training Step: 232 Training Loss: 0.6129769682884216 \n",
      "     Training Step: 233 Training Loss: 0.6177835464477539 \n",
      "     Training Step: 234 Training Loss: 0.614882230758667 \n",
      "     Training Step: 235 Training Loss: 0.614231526851654 \n",
      "     Training Step: 236 Training Loss: 0.6158440113067627 \n",
      "     Training Step: 237 Training Loss: 0.6168205738067627 \n",
      "     Training Step: 238 Training Loss: 0.6139289140701294 \n",
      "     Training Step: 239 Training Loss: 0.6118813753128052 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6112145781517029 \n",
      "     Validation Step: 1 Validation Loss: 0.6116836667060852 \n",
      "     Validation Step: 2 Validation Loss: 0.6151201128959656 \n",
      "     Validation Step: 3 Validation Loss: 0.6105389595031738 \n",
      "     Validation Step: 4 Validation Loss: 0.6101624369621277 \n",
      "     Validation Step: 5 Validation Loss: 0.6111888885498047 \n",
      "     Validation Step: 6 Validation Loss: 0.6075364947319031 \n",
      "     Validation Step: 7 Validation Loss: 0.61540287733078 \n",
      "     Validation Step: 8 Validation Loss: 0.6105142831802368 \n",
      "     Validation Step: 9 Validation Loss: 0.6128715872764587 \n",
      "     Validation Step: 10 Validation Loss: 0.618195652961731 \n",
      "     Validation Step: 11 Validation Loss: 0.6106650829315186 \n",
      "     Validation Step: 12 Validation Loss: 0.616365373134613 \n",
      "     Validation Step: 13 Validation Loss: 0.6137469410896301 \n",
      "     Validation Step: 14 Validation Loss: 0.6147142648696899 \n",
      "     Validation Step: 15 Validation Loss: 0.6171355247497559 \n",
      "     Validation Step: 16 Validation Loss: 0.6186168193817139 \n",
      "     Validation Step: 17 Validation Loss: 0.6102075576782227 \n",
      "     Validation Step: 18 Validation Loss: 0.6119195222854614 \n",
      "     Validation Step: 19 Validation Loss: 0.6141794919967651 \n",
      "     Validation Step: 20 Validation Loss: 0.6184707283973694 \n",
      "     Validation Step: 21 Validation Loss: 0.6115862131118774 \n",
      "     Validation Step: 22 Validation Loss: 0.6159122586250305 \n",
      "     Validation Step: 23 Validation Loss: 0.6177247762680054 \n",
      "     Validation Step: 24 Validation Loss: 0.6146486401557922 \n",
      "     Validation Step: 25 Validation Loss: 0.6156978011131287 \n",
      "     Validation Step: 26 Validation Loss: 0.6143157482147217 \n",
      "     Validation Step: 27 Validation Loss: 0.6152942180633545 \n",
      "     Validation Step: 28 Validation Loss: 0.6186444759368896 \n",
      "     Validation Step: 29 Validation Loss: 0.6141846179962158 \n",
      "     Validation Step: 30 Validation Loss: 0.6142663955688477 \n",
      "     Validation Step: 31 Validation Loss: 0.6130343675613403 \n",
      "     Validation Step: 32 Validation Loss: 0.6137157678604126 \n",
      "     Validation Step: 33 Validation Loss: 0.6174226999282837 \n",
      "     Validation Step: 34 Validation Loss: 0.617817759513855 \n",
      "     Validation Step: 35 Validation Loss: 0.6183825731277466 \n",
      "     Validation Step: 36 Validation Loss: 0.6149699091911316 \n",
      "     Validation Step: 37 Validation Loss: 0.615683913230896 \n",
      "     Validation Step: 38 Validation Loss: 0.612168550491333 \n",
      "     Validation Step: 39 Validation Loss: 0.6101276874542236 \n",
      "     Validation Step: 40 Validation Loss: 0.6149059534072876 \n",
      "     Validation Step: 41 Validation Loss: 0.614600658416748 \n",
      "     Validation Step: 42 Validation Loss: 0.6161147952079773 \n",
      "     Validation Step: 43 Validation Loss: 0.6133567094802856 \n",
      "     Validation Step: 44 Validation Loss: 0.6136971712112427 \n",
      "Epoch: 19\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6173012256622314 \n",
      "     Training Step: 1 Training Loss: 0.6124257445335388 \n",
      "     Training Step: 2 Training Loss: 0.6106271147727966 \n",
      "     Training Step: 3 Training Loss: 0.6101700663566589 \n",
      "     Training Step: 4 Training Loss: 0.6174786686897278 \n",
      "     Training Step: 5 Training Loss: 0.6109582185745239 \n",
      "     Training Step: 6 Training Loss: 0.615304172039032 \n",
      "     Training Step: 7 Training Loss: 0.6149278879165649 \n",
      "     Training Step: 8 Training Loss: 0.6114763021469116 \n",
      "     Training Step: 9 Training Loss: 0.6136243343353271 \n",
      "     Training Step: 10 Training Loss: 0.614089846611023 \n",
      "     Training Step: 11 Training Loss: 0.6118075847625732 \n",
      "     Training Step: 12 Training Loss: 0.6125009655952454 \n",
      "     Training Step: 13 Training Loss: 0.6136656999588013 \n",
      "     Training Step: 14 Training Loss: 0.6142820715904236 \n",
      "     Training Step: 15 Training Loss: 0.6133389472961426 \n",
      "     Training Step: 16 Training Loss: 0.6153129935264587 \n",
      "     Training Step: 17 Training Loss: 0.6121671795845032 \n",
      "     Training Step: 18 Training Loss: 0.6135104894638062 \n",
      "     Training Step: 19 Training Loss: 0.6154037117958069 \n",
      "     Training Step: 20 Training Loss: 0.6135402917861938 \n",
      "     Training Step: 21 Training Loss: 0.6167348027229309 \n",
      "     Training Step: 22 Training Loss: 0.6181051731109619 \n",
      "     Training Step: 23 Training Loss: 0.6101222634315491 \n",
      "     Training Step: 24 Training Loss: 0.6115550994873047 \n",
      "     Training Step: 25 Training Loss: 0.6122162342071533 \n",
      "     Training Step: 26 Training Loss: 0.6187268495559692 \n",
      "     Training Step: 27 Training Loss: 0.6156189441680908 \n",
      "     Training Step: 28 Training Loss: 0.6116762161254883 \n",
      "     Training Step: 29 Training Loss: 0.6137043833732605 \n",
      "     Training Step: 30 Training Loss: 0.6155785918235779 \n",
      "     Training Step: 31 Training Loss: 0.6181150674819946 \n",
      "     Training Step: 32 Training Loss: 0.6099185347557068 \n",
      "     Training Step: 33 Training Loss: 0.6184196472167969 \n",
      "     Training Step: 34 Training Loss: 0.615310549736023 \n",
      "     Training Step: 35 Training Loss: 0.616114616394043 \n",
      "     Training Step: 36 Training Loss: 0.6133373975753784 \n",
      "     Training Step: 37 Training Loss: 0.6168062090873718 \n",
      "     Training Step: 38 Training Loss: 0.6127581000328064 \n",
      "     Training Step: 39 Training Loss: 0.6146403551101685 \n",
      "     Training Step: 40 Training Loss: 0.6180599331855774 \n",
      "     Training Step: 41 Training Loss: 0.6158069372177124 \n",
      "     Training Step: 42 Training Loss: 0.6129785776138306 \n",
      "     Training Step: 43 Training Loss: 0.6183199286460876 \n",
      "     Training Step: 44 Training Loss: 0.6160370707511902 \n",
      "     Training Step: 45 Training Loss: 0.6141129732131958 \n",
      "     Training Step: 46 Training Loss: 0.6120184063911438 \n",
      "     Training Step: 47 Training Loss: 0.6175214052200317 \n",
      "     Training Step: 48 Training Loss: 0.6111984848976135 \n",
      "     Training Step: 49 Training Loss: 0.6125805974006653 \n",
      "     Training Step: 50 Training Loss: 0.6182116270065308 \n",
      "     Training Step: 51 Training Loss: 0.6129008531570435 \n",
      "     Training Step: 52 Training Loss: 0.6108142733573914 \n",
      "     Training Step: 53 Training Loss: 0.6139640212059021 \n",
      "     Training Step: 54 Training Loss: 0.617671549320221 \n",
      "     Training Step: 55 Training Loss: 0.6101338863372803 \n",
      "     Training Step: 56 Training Loss: 0.6162918210029602 \n",
      "     Training Step: 57 Training Loss: 0.6121835708618164 \n",
      "     Training Step: 58 Training Loss: 0.6123183965682983 \n",
      "     Training Step: 59 Training Loss: 0.6116878390312195 \n",
      "     Training Step: 60 Training Loss: 0.6114639639854431 \n",
      "     Training Step: 61 Training Loss: 0.6185700297355652 \n",
      "     Training Step: 62 Training Loss: 0.6157109141349792 \n",
      "     Training Step: 63 Training Loss: 0.6182370781898499 \n",
      "     Training Step: 64 Training Loss: 0.6155940890312195 \n",
      "     Training Step: 65 Training Loss: 0.6168505549430847 \n",
      "     Training Step: 66 Training Loss: 0.6154686212539673 \n",
      "     Training Step: 67 Training Loss: 0.6155272126197815 \n",
      "     Training Step: 68 Training Loss: 0.616849422454834 \n",
      "     Training Step: 69 Training Loss: 0.6142216324806213 \n",
      "     Training Step: 70 Training Loss: 0.6146780252456665 \n",
      "     Training Step: 71 Training Loss: 0.6116787791252136 \n",
      "     Training Step: 72 Training Loss: 0.6122726798057556 \n",
      "     Training Step: 73 Training Loss: 0.6127989888191223 \n",
      "     Training Step: 74 Training Loss: 0.616536021232605 \n",
      "     Training Step: 75 Training Loss: 0.6203258037567139 \n",
      "     Training Step: 76 Training Loss: 0.6165083050727844 \n",
      "     Training Step: 77 Training Loss: 0.614843487739563 \n",
      "     Training Step: 78 Training Loss: 0.6188504695892334 \n",
      "     Training Step: 79 Training Loss: 0.6125906705856323 \n",
      "     Training Step: 80 Training Loss: 0.6171731352806091 \n",
      "     Training Step: 81 Training Loss: 0.6197502613067627 \n",
      "     Training Step: 82 Training Loss: 0.6147306561470032 \n",
      "     Training Step: 83 Training Loss: 0.6144922375679016 \n",
      "     Training Step: 84 Training Loss: 0.6154597997665405 \n",
      "     Training Step: 85 Training Loss: 0.6149461269378662 \n",
      "     Training Step: 86 Training Loss: 0.6166940331459045 \n",
      "     Training Step: 87 Training Loss: 0.6117883920669556 \n",
      "     Training Step: 88 Training Loss: 0.6152065396308899 \n",
      "     Training Step: 89 Training Loss: 0.6139334440231323 \n",
      "     Training Step: 90 Training Loss: 0.6167470812797546 \n",
      "     Training Step: 91 Training Loss: 0.6123465895652771 \n",
      "     Training Step: 92 Training Loss: 0.6167364120483398 \n",
      "     Training Step: 93 Training Loss: 0.6135228276252747 \n",
      "     Training Step: 94 Training Loss: 0.6113137602806091 \n",
      "     Training Step: 95 Training Loss: 0.6161398887634277 \n",
      "     Training Step: 96 Training Loss: 0.6132174134254456 \n",
      "     Training Step: 97 Training Loss: 0.6159279942512512 \n",
      "     Training Step: 98 Training Loss: 0.6143801808357239 \n",
      "     Training Step: 99 Training Loss: 0.6158141493797302 \n",
      "     Training Step: 100 Training Loss: 0.6131070852279663 \n",
      "     Training Step: 101 Training Loss: 0.6169072389602661 \n",
      "     Training Step: 102 Training Loss: 0.6177319884300232 \n",
      "     Training Step: 103 Training Loss: 0.6127532124519348 \n",
      "     Training Step: 104 Training Loss: 0.616426944732666 \n",
      "     Training Step: 105 Training Loss: 0.6133821606636047 \n",
      "     Training Step: 106 Training Loss: 0.6140570044517517 \n",
      "     Training Step: 107 Training Loss: 0.6178099513053894 \n",
      "     Training Step: 108 Training Loss: 0.615119218826294 \n",
      "     Training Step: 109 Training Loss: 0.6156204342842102 \n",
      "     Training Step: 110 Training Loss: 0.6143110394477844 \n",
      "     Training Step: 111 Training Loss: 0.6128369569778442 \n",
      "     Training Step: 112 Training Loss: 0.6138261556625366 \n",
      "     Training Step: 113 Training Loss: 0.6121687889099121 \n",
      "     Training Step: 114 Training Loss: 0.6116655468940735 \n",
      "     Training Step: 115 Training Loss: 0.6104525327682495 \n",
      "     Training Step: 116 Training Loss: 0.6107555031776428 \n",
      "     Training Step: 117 Training Loss: 0.614827573299408 \n",
      "     Training Step: 118 Training Loss: 0.6109115481376648 \n",
      "     Training Step: 119 Training Loss: 0.610618531703949 \n",
      "     Training Step: 120 Training Loss: 0.6168663501739502 \n",
      "     Training Step: 121 Training Loss: 0.615427553653717 \n",
      "     Training Step: 122 Training Loss: 0.6099323630332947 \n",
      "     Training Step: 123 Training Loss: 0.6115826964378357 \n",
      "     Training Step: 124 Training Loss: 0.611894428730011 \n",
      "     Training Step: 125 Training Loss: 0.6133083701133728 \n",
      "     Training Step: 126 Training Loss: 0.6144730448722839 \n",
      "     Training Step: 127 Training Loss: 0.6169171333312988 \n",
      "     Training Step: 128 Training Loss: 0.6147510409355164 \n",
      "     Training Step: 129 Training Loss: 0.6177971363067627 \n",
      "     Training Step: 130 Training Loss: 0.6148281693458557 \n",
      "     Training Step: 131 Training Loss: 0.6133395433425903 \n",
      "     Training Step: 132 Training Loss: 0.6149641871452332 \n",
      "     Training Step: 133 Training Loss: 0.6153880953788757 \n",
      "     Training Step: 134 Training Loss: 0.6155748963356018 \n",
      "     Training Step: 135 Training Loss: 0.6094368696212769 \n",
      "     Training Step: 136 Training Loss: 0.6144886016845703 \n",
      "     Training Step: 137 Training Loss: 0.6116757392883301 \n",
      "     Training Step: 138 Training Loss: 0.6145130395889282 \n",
      "     Training Step: 139 Training Loss: 0.6172105073928833 \n",
      "     Training Step: 140 Training Loss: 0.6177241206169128 \n",
      "     Training Step: 141 Training Loss: 0.6146557927131653 \n",
      "     Training Step: 142 Training Loss: 0.610725462436676 \n",
      "     Training Step: 143 Training Loss: 0.6125840544700623 \n",
      "     Training Step: 144 Training Loss: 0.6136698126792908 \n",
      "     Training Step: 145 Training Loss: 0.6116538643836975 \n",
      "     Training Step: 146 Training Loss: 0.6171784996986389 \n",
      "     Training Step: 147 Training Loss: 0.6148483753204346 \n",
      "     Training Step: 148 Training Loss: 0.6151471138000488 \n",
      "     Training Step: 149 Training Loss: 0.6127395033836365 \n",
      "     Training Step: 150 Training Loss: 0.6147584319114685 \n",
      "     Training Step: 151 Training Loss: 0.6150068044662476 \n",
      "     Training Step: 152 Training Loss: 0.6083475351333618 \n",
      "     Training Step: 153 Training Loss: 0.6114838123321533 \n",
      "     Training Step: 154 Training Loss: 0.6105547547340393 \n",
      "     Training Step: 155 Training Loss: 0.6119116544723511 \n",
      "     Training Step: 156 Training Loss: 0.6133936643600464 \n",
      "     Training Step: 157 Training Loss: 0.6142188906669617 \n",
      "     Training Step: 158 Training Loss: 0.6103224158287048 \n",
      "     Training Step: 159 Training Loss: 0.6129234433174133 \n",
      "     Training Step: 160 Training Loss: 0.6138079762458801 \n",
      "     Training Step: 161 Training Loss: 0.6104820370674133 \n",
      "     Training Step: 162 Training Loss: 0.6135110855102539 \n",
      "     Training Step: 163 Training Loss: 0.6106804609298706 \n",
      "     Training Step: 164 Training Loss: 0.6107979416847229 \n",
      "     Training Step: 165 Training Loss: 0.6164442300796509 \n",
      "     Training Step: 166 Training Loss: 0.6147739291191101 \n",
      "     Training Step: 167 Training Loss: 0.6152710318565369 \n",
      "     Training Step: 168 Training Loss: 0.6153905391693115 \n",
      "     Training Step: 169 Training Loss: 0.6182801723480225 \n",
      "     Training Step: 170 Training Loss: 0.6154516339302063 \n",
      "     Training Step: 171 Training Loss: 0.6172332763671875 \n",
      "     Training Step: 172 Training Loss: 0.6143196225166321 \n",
      "     Training Step: 173 Training Loss: 0.6102809906005859 \n",
      "     Training Step: 174 Training Loss: 0.619627058506012 \n",
      "     Training Step: 175 Training Loss: 0.6138394474983215 \n",
      "     Training Step: 176 Training Loss: 0.6189144849777222 \n",
      "     Training Step: 177 Training Loss: 0.6134716868400574 \n",
      "     Training Step: 178 Training Loss: 0.6130703091621399 \n",
      "     Training Step: 179 Training Loss: 0.6124641299247742 \n",
      "     Training Step: 180 Training Loss: 0.6122786998748779 \n",
      "     Training Step: 181 Training Loss: 0.6118537783622742 \n",
      "     Training Step: 182 Training Loss: 0.6136122345924377 \n",
      "     Training Step: 183 Training Loss: 0.611228883266449 \n",
      "     Training Step: 184 Training Loss: 0.6129086017608643 \n",
      "     Training Step: 185 Training Loss: 0.6147223711013794 \n",
      "     Training Step: 186 Training Loss: 0.6199302673339844 \n",
      "     Training Step: 187 Training Loss: 0.6133381128311157 \n",
      "     Training Step: 188 Training Loss: 0.6119323968887329 \n",
      "     Training Step: 189 Training Loss: 0.6185809969902039 \n",
      "     Training Step: 190 Training Loss: 0.6167716979980469 \n",
      "     Training Step: 191 Training Loss: 0.6131284236907959 \n",
      "     Training Step: 192 Training Loss: 0.6178911328315735 \n",
      "     Training Step: 193 Training Loss: 0.615456223487854 \n",
      "     Training Step: 194 Training Loss: 0.6171103119850159 \n",
      "     Training Step: 195 Training Loss: 0.6120043992996216 \n",
      "     Training Step: 196 Training Loss: 0.6094417572021484 \n",
      "     Training Step: 197 Training Loss: 0.6164605617523193 \n",
      "     Training Step: 198 Training Loss: 0.6123062372207642 \n",
      "     Training Step: 199 Training Loss: 0.6158799529075623 \n",
      "     Training Step: 200 Training Loss: 0.6097909212112427 \n",
      "     Training Step: 201 Training Loss: 0.6148166060447693 \n",
      "     Training Step: 202 Training Loss: 0.611532986164093 \n",
      "     Training Step: 203 Training Loss: 0.6119265556335449 \n",
      "     Training Step: 204 Training Loss: 0.6178469061851501 \n",
      "     Training Step: 205 Training Loss: 0.6184993386268616 \n",
      "     Training Step: 206 Training Loss: 0.6163375973701477 \n",
      "     Training Step: 207 Training Loss: 0.614403247833252 \n",
      "     Training Step: 208 Training Loss: 0.6143057942390442 \n",
      "     Training Step: 209 Training Loss: 0.6134812831878662 \n",
      "     Training Step: 210 Training Loss: 0.6148144602775574 \n",
      "     Training Step: 211 Training Loss: 0.6143414974212646 \n",
      "     Training Step: 212 Training Loss: 0.6158280968666077 \n",
      "     Training Step: 213 Training Loss: 0.6151533722877502 \n",
      "     Training Step: 214 Training Loss: 0.6115304827690125 \n",
      "     Training Step: 215 Training Loss: 0.6210291385650635 \n",
      "     Training Step: 216 Training Loss: 0.612636387348175 \n",
      "     Training Step: 217 Training Loss: 0.6102241277694702 \n",
      "     Training Step: 218 Training Loss: 0.619768500328064 \n",
      "     Training Step: 219 Training Loss: 0.6133488416671753 \n",
      "     Training Step: 220 Training Loss: 0.6158031225204468 \n",
      "     Training Step: 221 Training Loss: 0.6167036294937134 \n",
      "     Training Step: 222 Training Loss: 0.609603226184845 \n",
      "     Training Step: 223 Training Loss: 0.6138532161712646 \n",
      "     Training Step: 224 Training Loss: 0.6139661073684692 \n",
      "     Training Step: 225 Training Loss: 0.6145063638687134 \n",
      "     Training Step: 226 Training Loss: 0.6140681505203247 \n",
      "     Training Step: 227 Training Loss: 0.6129204630851746 \n",
      "     Training Step: 228 Training Loss: 0.6144751906394958 \n",
      "     Training Step: 229 Training Loss: 0.6130122542381287 \n",
      "     Training Step: 230 Training Loss: 0.6105667948722839 \n",
      "     Training Step: 231 Training Loss: 0.6167911291122437 \n",
      "     Training Step: 232 Training Loss: 0.6120729446411133 \n",
      "     Training Step: 233 Training Loss: 0.6147401928901672 \n",
      "     Training Step: 234 Training Loss: 0.6107118129730225 \n",
      "     Training Step: 235 Training Loss: 0.6145462989807129 \n",
      "     Training Step: 236 Training Loss: 0.6115629076957703 \n",
      "     Training Step: 237 Training Loss: 0.6121944785118103 \n",
      "     Training Step: 238 Training Loss: 0.6160203218460083 \n",
      "     Training Step: 239 Training Loss: 0.6168258786201477 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6156410574913025 \n",
      "     Validation Step: 1 Validation Loss: 0.6148813962936401 \n",
      "     Validation Step: 2 Validation Loss: 0.6153441071510315 \n",
      "     Validation Step: 3 Validation Loss: 0.6176312565803528 \n",
      "     Validation Step: 4 Validation Loss: 0.6105892658233643 \n",
      "     Validation Step: 5 Validation Loss: 0.61418217420578 \n",
      "     Validation Step: 6 Validation Loss: 0.6130335330963135 \n",
      "     Validation Step: 7 Validation Loss: 0.6185141205787659 \n",
      "     Validation Step: 8 Validation Loss: 0.6141948699951172 \n",
      "     Validation Step: 9 Validation Loss: 0.6183565258979797 \n",
      "     Validation Step: 10 Validation Loss: 0.6162785887718201 \n",
      "     Validation Step: 11 Validation Loss: 0.6146731972694397 \n",
      "     Validation Step: 12 Validation Loss: 0.6105947494506836 \n",
      "     Validation Step: 13 Validation Loss: 0.6136860847473145 \n",
      "     Validation Step: 14 Validation Loss: 0.6158156991004944 \n",
      "     Validation Step: 15 Validation Loss: 0.6103108525276184 \n",
      "     Validation Step: 16 Validation Loss: 0.6150699257850647 \n",
      "     Validation Step: 17 Validation Loss: 0.6149601340293884 \n",
      "     Validation Step: 18 Validation Loss: 0.6133561134338379 \n",
      "     Validation Step: 19 Validation Loss: 0.610764741897583 \n",
      "     Validation Step: 20 Validation Loss: 0.6102540493011475 \n",
      "     Validation Step: 21 Validation Loss: 0.6152650117874146 \n",
      "     Validation Step: 22 Validation Loss: 0.6182488203048706 \n",
      "     Validation Step: 23 Validation Loss: 0.6112580895423889 \n",
      "     Validation Step: 24 Validation Loss: 0.6173290610313416 \n",
      "     Validation Step: 25 Validation Loss: 0.6177076697349548 \n",
      "     Validation Step: 26 Validation Loss: 0.6137077808380127 \n",
      "     Validation Step: 27 Validation Loss: 0.611728847026825 \n",
      "     Validation Step: 28 Validation Loss: 0.6146054863929749 \n",
      "     Validation Step: 29 Validation Loss: 0.6122212409973145 \n",
      "     Validation Step: 30 Validation Loss: 0.6116452217102051 \n",
      "     Validation Step: 31 Validation Loss: 0.6119674444198608 \n",
      "     Validation Step: 32 Validation Loss: 0.6112654805183411 \n",
      "     Validation Step: 33 Validation Loss: 0.6077070236206055 \n",
      "     Validation Step: 34 Validation Loss: 0.6170443296432495 \n",
      "     Validation Step: 35 Validation Loss: 0.618499755859375 \n",
      "     Validation Step: 36 Validation Loss: 0.6156091094017029 \n",
      "     Validation Step: 37 Validation Loss: 0.6102951169013977 \n",
      "     Validation Step: 38 Validation Loss: 0.6160160303115845 \n",
      "     Validation Step: 39 Validation Loss: 0.6128975749015808 \n",
      "     Validation Step: 40 Validation Loss: 0.6142479181289673 \n",
      "     Validation Step: 41 Validation Loss: 0.6136855483055115 \n",
      "     Validation Step: 42 Validation Loss: 0.6143301725387573 \n",
      "     Validation Step: 43 Validation Loss: 0.618079423904419 \n",
      "     Validation Step: 44 Validation Loss: 0.6145991086959839 \n",
      "Epoch: 20\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.612595796585083 \n",
      "     Training Step: 1 Training Loss: 0.6119977831840515 \n",
      "     Training Step: 2 Training Loss: 0.6186317801475525 \n",
      "     Training Step: 3 Training Loss: 0.6104812026023865 \n",
      "     Training Step: 4 Training Loss: 0.6117008328437805 \n",
      "     Training Step: 5 Training Loss: 0.6128838062286377 \n",
      "     Training Step: 6 Training Loss: 0.6106888651847839 \n",
      "     Training Step: 7 Training Loss: 0.6135513186454773 \n",
      "     Training Step: 8 Training Loss: 0.6147105097770691 \n",
      "     Training Step: 9 Training Loss: 0.6178352236747742 \n",
      "     Training Step: 10 Training Loss: 0.6119386553764343 \n",
      "     Training Step: 11 Training Loss: 0.6154425144195557 \n",
      "     Training Step: 12 Training Loss: 0.6099416017532349 \n",
      "     Training Step: 13 Training Loss: 0.6142518520355225 \n",
      "     Training Step: 14 Training Loss: 0.6148785352706909 \n",
      "     Training Step: 15 Training Loss: 0.6116414070129395 \n",
      "     Training Step: 16 Training Loss: 0.6107475161552429 \n",
      "     Training Step: 17 Training Loss: 0.6175716519355774 \n",
      "     Training Step: 18 Training Loss: 0.6121980547904968 \n",
      "     Training Step: 19 Training Loss: 0.6178597211837769 \n",
      "     Training Step: 20 Training Loss: 0.6136058568954468 \n",
      "     Training Step: 21 Training Loss: 0.616855800151825 \n",
      "     Training Step: 22 Training Loss: 0.6177188158035278 \n",
      "     Training Step: 23 Training Loss: 0.6164248585700989 \n",
      "     Training Step: 24 Training Loss: 0.6180291771888733 \n",
      "     Training Step: 25 Training Loss: 0.6158608794212341 \n",
      "     Training Step: 26 Training Loss: 0.6133252382278442 \n",
      "     Training Step: 27 Training Loss: 0.6117430329322815 \n",
      "     Training Step: 28 Training Loss: 0.611214280128479 \n",
      "     Training Step: 29 Training Loss: 0.6173964738845825 \n",
      "     Training Step: 30 Training Loss: 0.6155436038970947 \n",
      "     Training Step: 31 Training Loss: 0.614668607711792 \n",
      "     Training Step: 32 Training Loss: 0.614140510559082 \n",
      "     Training Step: 33 Training Loss: 0.6119152903556824 \n",
      "     Training Step: 34 Training Loss: 0.613247811794281 \n",
      "     Training Step: 35 Training Loss: 0.6129565238952637 \n",
      "     Training Step: 36 Training Loss: 0.6153752207756042 \n",
      "     Training Step: 37 Training Loss: 0.6101288199424744 \n",
      "     Training Step: 38 Training Loss: 0.6101558804512024 \n",
      "     Training Step: 39 Training Loss: 0.6133212447166443 \n",
      "     Training Step: 40 Training Loss: 0.6107662916183472 \n",
      "     Training Step: 41 Training Loss: 0.6144213676452637 \n",
      "     Training Step: 42 Training Loss: 0.6203526258468628 \n",
      "     Training Step: 43 Training Loss: 0.6107580661773682 \n",
      "     Training Step: 44 Training Loss: 0.6123277544975281 \n",
      "     Training Step: 45 Training Loss: 0.6125132441520691 \n",
      "     Training Step: 46 Training Loss: 0.6183051466941833 \n",
      "     Training Step: 47 Training Loss: 0.6167100667953491 \n",
      "     Training Step: 48 Training Loss: 0.6129662990570068 \n",
      "     Training Step: 49 Training Loss: 0.6096029877662659 \n",
      "     Training Step: 50 Training Loss: 0.6162386536598206 \n",
      "     Training Step: 51 Training Loss: 0.615764319896698 \n",
      "     Training Step: 52 Training Loss: 0.6168271899223328 \n",
      "     Training Step: 53 Training Loss: 0.6122810244560242 \n",
      "     Training Step: 54 Training Loss: 0.6150975227355957 \n",
      "     Training Step: 55 Training Loss: 0.6124306321144104 \n",
      "     Training Step: 56 Training Loss: 0.6121050715446472 \n",
      "     Training Step: 57 Training Loss: 0.6147311329841614 \n",
      "     Training Step: 58 Training Loss: 0.6170306205749512 \n",
      "     Training Step: 59 Training Loss: 0.6115232706069946 \n",
      "     Training Step: 60 Training Loss: 0.6146463751792908 \n",
      "     Training Step: 61 Training Loss: 0.6115724444389343 \n",
      "     Training Step: 62 Training Loss: 0.6154696345329285 \n",
      "     Training Step: 63 Training Loss: 0.6114826202392578 \n",
      "     Training Step: 64 Training Loss: 0.6147188544273376 \n",
      "     Training Step: 65 Training Loss: 0.6157307624816895 \n",
      "     Training Step: 66 Training Loss: 0.6184660792350769 \n",
      "     Training Step: 67 Training Loss: 0.6129047870635986 \n",
      "     Training Step: 68 Training Loss: 0.6167393326759338 \n",
      "     Training Step: 69 Training Loss: 0.6185629963874817 \n",
      "     Training Step: 70 Training Loss: 0.6129223704338074 \n",
      "     Training Step: 71 Training Loss: 0.6159627437591553 \n",
      "     Training Step: 72 Training Loss: 0.6113158464431763 \n",
      "     Training Step: 73 Training Loss: 0.6167430877685547 \n",
      "     Training Step: 74 Training Loss: 0.6167595386505127 \n",
      "     Training Step: 75 Training Loss: 0.6143123507499695 \n",
      "     Training Step: 76 Training Loss: 0.6125852465629578 \n",
      "     Training Step: 77 Training Loss: 0.6123979687690735 \n",
      "     Training Step: 78 Training Loss: 0.6161665916442871 \n",
      "     Training Step: 79 Training Loss: 0.6106253862380981 \n",
      "     Training Step: 80 Training Loss: 0.6200805902481079 \n",
      "     Training Step: 81 Training Loss: 0.6165604591369629 \n",
      "     Training Step: 82 Training Loss: 0.6158118844032288 \n",
      "     Training Step: 83 Training Loss: 0.6160916686058044 \n",
      "     Training Step: 84 Training Loss: 0.6126793622970581 \n",
      "     Training Step: 85 Training Loss: 0.6125462651252747 \n",
      "     Training Step: 86 Training Loss: 0.6160908937454224 \n",
      "     Training Step: 87 Training Loss: 0.6149390935897827 \n",
      "     Training Step: 88 Training Loss: 0.6163700819015503 \n",
      "     Training Step: 89 Training Loss: 0.6168941259384155 \n",
      "     Training Step: 90 Training Loss: 0.6210036277770996 \n",
      "     Training Step: 91 Training Loss: 0.6153790950775146 \n",
      "     Training Step: 92 Training Loss: 0.6137509346008301 \n",
      "     Training Step: 93 Training Loss: 0.6121271848678589 \n",
      "     Training Step: 94 Training Loss: 0.6115326881408691 \n",
      "     Training Step: 95 Training Loss: 0.6157363057136536 \n",
      "     Training Step: 96 Training Loss: 0.6121638417243958 \n",
      "     Training Step: 97 Training Loss: 0.6184017658233643 \n",
      "     Training Step: 98 Training Loss: 0.614529550075531 \n",
      "     Training Step: 99 Training Loss: 0.6124987602233887 \n",
      "     Training Step: 100 Training Loss: 0.6108203530311584 \n",
      "     Training Step: 101 Training Loss: 0.6099127531051636 \n",
      "     Training Step: 102 Training Loss: 0.6112349033355713 \n",
      "     Training Step: 103 Training Loss: 0.6094723343849182 \n",
      "     Training Step: 104 Training Loss: 0.6145384907722473 \n",
      "     Training Step: 105 Training Loss: 0.6157026886940002 \n",
      "     Training Step: 106 Training Loss: 0.61356121301651 \n",
      "     Training Step: 107 Training Loss: 0.6189090609550476 \n",
      "     Training Step: 108 Training Loss: 0.6170251965522766 \n",
      "     Training Step: 109 Training Loss: 0.6156194806098938 \n",
      "     Training Step: 110 Training Loss: 0.6106055974960327 \n",
      "     Training Step: 111 Training Loss: 0.6137520670890808 \n",
      "     Training Step: 112 Training Loss: 0.6167322993278503 \n",
      "     Training Step: 113 Training Loss: 0.6158157587051392 \n",
      "     Training Step: 114 Training Loss: 0.6179885864257812 \n",
      "     Training Step: 115 Training Loss: 0.6135465502738953 \n",
      "     Training Step: 116 Training Loss: 0.613370418548584 \n",
      "     Training Step: 117 Training Loss: 0.6150941848754883 \n",
      "     Training Step: 118 Training Loss: 0.617229163646698 \n",
      "     Training Step: 119 Training Loss: 0.6134434342384338 \n",
      "     Training Step: 120 Training Loss: 0.6171293258666992 \n",
      "     Training Step: 121 Training Loss: 0.6140720844268799 \n",
      "     Training Step: 122 Training Loss: 0.6148442625999451 \n",
      "     Training Step: 123 Training Loss: 0.6134023666381836 \n",
      "     Training Step: 124 Training Loss: 0.6154067516326904 \n",
      "     Training Step: 125 Training Loss: 0.616766631603241 \n",
      "     Training Step: 126 Training Loss: 0.6151976585388184 \n",
      "     Training Step: 127 Training Loss: 0.6117175221443176 \n",
      "     Training Step: 128 Training Loss: 0.6085006594657898 \n",
      "     Training Step: 129 Training Loss: 0.6138730645179749 \n",
      "     Training Step: 130 Training Loss: 0.6148622035980225 \n",
      "     Training Step: 131 Training Loss: 0.6143430471420288 \n",
      "     Training Step: 132 Training Loss: 0.6172933578491211 \n",
      "     Training Step: 133 Training Loss: 0.6131944060325623 \n",
      "     Training Step: 134 Training Loss: 0.6131798028945923 \n",
      "     Training Step: 135 Training Loss: 0.6127135157585144 \n",
      "     Training Step: 136 Training Loss: 0.6133765578269958 \n",
      "     Training Step: 137 Training Loss: 0.6102665066719055 \n",
      "     Training Step: 138 Training Loss: 0.618657648563385 \n",
      "     Training Step: 139 Training Loss: 0.6145119667053223 \n",
      "     Training Step: 140 Training Loss: 0.6145547032356262 \n",
      "     Training Step: 141 Training Loss: 0.6147210001945496 \n",
      "     Training Step: 142 Training Loss: 0.616777777671814 \n",
      "     Training Step: 143 Training Loss: 0.6107847094535828 \n",
      "     Training Step: 144 Training Loss: 0.6106441020965576 \n",
      "     Training Step: 145 Training Loss: 0.6152592897415161 \n",
      "     Training Step: 146 Training Loss: 0.6180780529975891 \n",
      "     Training Step: 147 Training Loss: 0.6176906228065491 \n",
      "     Training Step: 148 Training Loss: 0.6167478561401367 \n",
      "     Training Step: 149 Training Loss: 0.6129474639892578 \n",
      "     Training Step: 150 Training Loss: 0.6123900413513184 \n",
      "     Training Step: 151 Training Loss: 0.6120128035545349 \n",
      "     Training Step: 152 Training Loss: 0.6139369606971741 \n",
      "     Training Step: 153 Training Loss: 0.614284098148346 \n",
      "     Training Step: 154 Training Loss: 0.6157857775688171 \n",
      "     Training Step: 155 Training Loss: 0.6165051460266113 \n",
      "     Training Step: 156 Training Loss: 0.6158071756362915 \n",
      "     Training Step: 157 Training Loss: 0.614212691783905 \n",
      "     Training Step: 158 Training Loss: 0.6141040921211243 \n",
      "     Training Step: 159 Training Loss: 0.6119478344917297 \n",
      "     Training Step: 160 Training Loss: 0.6158267855644226 \n",
      "     Training Step: 161 Training Loss: 0.6183158159255981 \n",
      "     Training Step: 162 Training Loss: 0.6163110733032227 \n",
      "     Training Step: 163 Training Loss: 0.6157140135765076 \n",
      "     Training Step: 164 Training Loss: 0.61472487449646 \n",
      "     Training Step: 165 Training Loss: 0.6116464734077454 \n",
      "     Training Step: 166 Training Loss: 0.6115492582321167 \n",
      "     Training Step: 167 Training Loss: 0.6149752736091614 \n",
      "     Training Step: 168 Training Loss: 0.6153342127799988 \n",
      "     Training Step: 169 Training Loss: 0.6106032729148865 \n",
      "     Training Step: 170 Training Loss: 0.6116373538970947 \n",
      "     Training Step: 171 Training Loss: 0.6127116084098816 \n",
      "     Training Step: 172 Training Loss: 0.6133237481117249 \n",
      "     Training Step: 173 Training Loss: 0.6168863773345947 \n",
      "     Training Step: 174 Training Loss: 0.6093845367431641 \n",
      "     Training Step: 175 Training Loss: 0.618878185749054 \n",
      "     Training Step: 176 Training Loss: 0.615131676197052 \n",
      "     Training Step: 177 Training Loss: 0.6153931617736816 \n",
      "     Training Step: 178 Training Loss: 0.6108981966972351 \n",
      "     Training Step: 179 Training Loss: 0.6177473664283752 \n",
      "     Training Step: 180 Training Loss: 0.6102285981178284 \n",
      "     Training Step: 181 Training Loss: 0.6117369532585144 \n",
      "     Training Step: 182 Training Loss: 0.6156781911849976 \n",
      "     Training Step: 183 Training Loss: 0.6136781573295593 \n",
      "     Training Step: 184 Training Loss: 0.6139487028121948 \n",
      "     Training Step: 185 Training Loss: 0.6195398569107056 \n",
      "     Training Step: 186 Training Loss: 0.6147736310958862 \n",
      "     Training Step: 187 Training Loss: 0.6123284101486206 \n",
      "     Training Step: 188 Training Loss: 0.6119177937507629 \n",
      "     Training Step: 189 Training Loss: 0.6128122806549072 \n",
      "     Training Step: 190 Training Loss: 0.6145488023757935 \n",
      "     Training Step: 191 Training Loss: 0.6144703030586243 \n",
      "     Training Step: 192 Training Loss: 0.610451340675354 \n",
      "     Training Step: 193 Training Loss: 0.6146879196166992 \n",
      "     Training Step: 194 Training Loss: 0.6116989254951477 \n",
      "     Training Step: 195 Training Loss: 0.6124142408370972 \n",
      "     Training Step: 196 Training Loss: 0.6137673854827881 \n",
      "     Training Step: 197 Training Loss: 0.6147624850273132 \n",
      "     Training Step: 198 Training Loss: 0.6142775416374207 \n",
      "     Training Step: 199 Training Loss: 0.6184476017951965 \n",
      "     Training Step: 200 Training Loss: 0.6144997477531433 \n",
      "     Training Step: 201 Training Loss: 0.6138184666633606 \n",
      "     Training Step: 202 Training Loss: 0.6166428923606873 \n",
      "     Training Step: 203 Training Loss: 0.6145529747009277 \n",
      "     Training Step: 204 Training Loss: 0.6176230907440186 \n",
      "     Training Step: 205 Training Loss: 0.6180519461631775 \n",
      "     Training Step: 206 Training Loss: 0.6122519373893738 \n",
      "     Training Step: 207 Training Loss: 0.6148048639297485 \n",
      "     Training Step: 208 Training Loss: 0.6140544414520264 \n",
      "     Training Step: 209 Training Loss: 0.6171980500221252 \n",
      "     Training Step: 210 Training Loss: 0.6196874380111694 \n",
      "     Training Step: 211 Training Loss: 0.6128602623939514 \n",
      "     Training Step: 212 Training Loss: 0.6101363897323608 \n",
      "     Training Step: 213 Training Loss: 0.6136246919631958 \n",
      "     Training Step: 214 Training Loss: 0.6149678826332092 \n",
      "     Training Step: 215 Training Loss: 0.6185621619224548 \n",
      "     Training Step: 216 Training Loss: 0.6155548095703125 \n",
      "     Training Step: 217 Training Loss: 0.6130291223526001 \n",
      "     Training Step: 218 Training Loss: 0.6180647015571594 \n",
      "     Training Step: 219 Training Loss: 0.6131681203842163 \n",
      "     Training Step: 220 Training Loss: 0.609898567199707 \n",
      "     Training Step: 221 Training Loss: 0.6129593253135681 \n",
      "     Training Step: 222 Training Loss: 0.6136695742607117 \n",
      "     Training Step: 223 Training Loss: 0.6116056442260742 \n",
      "     Training Step: 224 Training Loss: 0.6134946346282959 \n",
      "     Training Step: 225 Training Loss: 0.6132641434669495 \n",
      "     Training Step: 226 Training Loss: 0.6109362840652466 \n",
      "     Training Step: 227 Training Loss: 0.6152424812316895 \n",
      "     Training Step: 228 Training Loss: 0.6144551038742065 \n",
      "     Training Step: 229 Training Loss: 0.6116988658905029 \n",
      "     Training Step: 230 Training Loss: 0.6122215390205383 \n",
      "     Training Step: 231 Training Loss: 0.6115313768386841 \n",
      "     Training Step: 232 Training Loss: 0.6109461784362793 \n",
      "     Training Step: 233 Training Loss: 0.6139112710952759 \n",
      "     Training Step: 234 Training Loss: 0.6199619174003601 \n",
      "     Training Step: 235 Training Loss: 0.6167150735855103 \n",
      "     Training Step: 236 Training Loss: 0.614225447177887 \n",
      "     Training Step: 237 Training Loss: 0.6157121658325195 \n",
      "     Training Step: 238 Training Loss: 0.6148972511291504 \n",
      "     Training Step: 239 Training Loss: 0.6153406500816345 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6176732778549194 \n",
      "     Validation Step: 1 Validation Loss: 0.6076155304908752 \n",
      "     Validation Step: 2 Validation Loss: 0.6171101331710815 \n",
      "     Validation Step: 3 Validation Loss: 0.6157185435295105 \n",
      "     Validation Step: 4 Validation Loss: 0.6142943501472473 \n",
      "     Validation Step: 5 Validation Loss: 0.6149352788925171 \n",
      "     Validation Step: 6 Validation Loss: 0.6129010915756226 \n",
      "     Validation Step: 7 Validation Loss: 0.613078773021698 \n",
      "     Validation Step: 8 Validation Loss: 0.6106230020523071 \n",
      "     Validation Step: 9 Validation Loss: 0.6161397695541382 \n",
      "     Validation Step: 10 Validation Loss: 0.6119629144668579 \n",
      "     Validation Step: 11 Validation Loss: 0.6137552261352539 \n",
      "     Validation Step: 12 Validation Loss: 0.6151458024978638 \n",
      "     Validation Step: 13 Validation Loss: 0.6116383075714111 \n",
      "     Validation Step: 14 Validation Loss: 0.6137944459915161 \n",
      "     Validation Step: 15 Validation Loss: 0.610239565372467 \n",
      "     Validation Step: 16 Validation Loss: 0.6159128546714783 \n",
      "     Validation Step: 17 Validation Loss: 0.6185692548751831 \n",
      "     Validation Step: 18 Validation Loss: 0.6177965402603149 \n",
      "     Validation Step: 19 Validation Loss: 0.6107134819030762 \n",
      "     Validation Step: 20 Validation Loss: 0.6133722066879272 \n",
      "     Validation Step: 21 Validation Loss: 0.6102087497711182 \n",
      "     Validation Step: 22 Validation Loss: 0.6143243908882141 \n",
      "     Validation Step: 23 Validation Loss: 0.6105706095695496 \n",
      "     Validation Step: 24 Validation Loss: 0.6117157340049744 \n",
      "     Validation Step: 25 Validation Loss: 0.6183686256408691 \n",
      "     Validation Step: 26 Validation Loss: 0.6153138875961304 \n",
      "     Validation Step: 27 Validation Loss: 0.6147286295890808 \n",
      "     Validation Step: 28 Validation Loss: 0.6141800880432129 \n",
      "     Validation Step: 29 Validation Loss: 0.6186000108718872 \n",
      "     Validation Step: 30 Validation Loss: 0.612217903137207 \n",
      "     Validation Step: 31 Validation Loss: 0.6181629300117493 \n",
      "     Validation Step: 32 Validation Loss: 0.6163724660873413 \n",
      "     Validation Step: 33 Validation Loss: 0.6112411022186279 \n",
      "     Validation Step: 34 Validation Loss: 0.6112469434738159 \n",
      "     Validation Step: 35 Validation Loss: 0.6136974692344666 \n",
      "     Validation Step: 36 Validation Loss: 0.6173936128616333 \n",
      "     Validation Step: 37 Validation Loss: 0.6142178177833557 \n",
      "     Validation Step: 38 Validation Loss: 0.6149644255638123 \n",
      "     Validation Step: 39 Validation Loss: 0.6102275252342224 \n",
      "     Validation Step: 40 Validation Loss: 0.6146517395973206 \n",
      "     Validation Step: 41 Validation Loss: 0.6156872510910034 \n",
      "     Validation Step: 42 Validation Loss: 0.6184571385383606 \n",
      "     Validation Step: 43 Validation Loss: 0.6154133081436157 \n",
      "     Validation Step: 44 Validation Loss: 0.6146074533462524 \n",
      "Epoch: 21\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6138821244239807 \n",
      "     Training Step: 1 Training Loss: 0.6212717890739441 \n",
      "     Training Step: 2 Training Loss: 0.6177573800086975 \n",
      "     Training Step: 3 Training Loss: 0.6171595454216003 \n",
      "     Training Step: 4 Training Loss: 0.6158093214035034 \n",
      "     Training Step: 5 Training Loss: 0.6117954850196838 \n",
      "     Training Step: 6 Training Loss: 0.6086564064025879 \n",
      "     Training Step: 7 Training Loss: 0.614714503288269 \n",
      "     Training Step: 8 Training Loss: 0.6175724267959595 \n",
      "     Training Step: 9 Training Loss: 0.6133624911308289 \n",
      "     Training Step: 10 Training Loss: 0.6117006540298462 \n",
      "     Training Step: 11 Training Loss: 0.6176066398620605 \n",
      "     Training Step: 12 Training Loss: 0.6106828451156616 \n",
      "     Training Step: 13 Training Loss: 0.6178457140922546 \n",
      "     Training Step: 14 Training Loss: 0.6127060055732727 \n",
      "     Training Step: 15 Training Loss: 0.6145459413528442 \n",
      "     Training Step: 16 Training Loss: 0.6107755899429321 \n",
      "     Training Step: 17 Training Loss: 0.6125343441963196 \n",
      "     Training Step: 18 Training Loss: 0.6140387058258057 \n",
      "     Training Step: 19 Training Loss: 0.6129739284515381 \n",
      "     Training Step: 20 Training Loss: 0.6178517937660217 \n",
      "     Training Step: 21 Training Loss: 0.6189175248146057 \n",
      "     Training Step: 22 Training Loss: 0.6159335374832153 \n",
      "     Training Step: 23 Training Loss: 0.6131709218025208 \n",
      "     Training Step: 24 Training Loss: 0.6155929565429688 \n",
      "     Training Step: 25 Training Loss: 0.618050754070282 \n",
      "     Training Step: 26 Training Loss: 0.6195574402809143 \n",
      "     Training Step: 27 Training Loss: 0.618323802947998 \n",
      "     Training Step: 28 Training Loss: 0.6166558861732483 \n",
      "     Training Step: 29 Training Loss: 0.6148452758789062 \n",
      "     Training Step: 30 Training Loss: 0.6107054352760315 \n",
      "     Training Step: 31 Training Loss: 0.6138156652450562 \n",
      "     Training Step: 32 Training Loss: 0.615277111530304 \n",
      "     Training Step: 33 Training Loss: 0.6147061586380005 \n",
      "     Training Step: 34 Training Loss: 0.6122012734413147 \n",
      "     Training Step: 35 Training Loss: 0.6169446110725403 \n",
      "     Training Step: 36 Training Loss: 0.6120452880859375 \n",
      "     Training Step: 37 Training Loss: 0.6144535541534424 \n",
      "     Training Step: 38 Training Loss: 0.6151592135429382 \n",
      "     Training Step: 39 Training Loss: 0.6171706914901733 \n",
      "     Training Step: 40 Training Loss: 0.6107425093650818 \n",
      "     Training Step: 41 Training Loss: 0.6166740655899048 \n",
      "     Training Step: 42 Training Loss: 0.6158130168914795 \n",
      "     Training Step: 43 Training Loss: 0.6116969585418701 \n",
      "     Training Step: 44 Training Loss: 0.6141939759254456 \n",
      "     Training Step: 45 Training Loss: 0.6141519546508789 \n",
      "     Training Step: 46 Training Loss: 0.615329921245575 \n",
      "     Training Step: 47 Training Loss: 0.6144987344741821 \n",
      "     Training Step: 48 Training Loss: 0.614509642124176 \n",
      "     Training Step: 49 Training Loss: 0.6102004051208496 \n",
      "     Training Step: 50 Training Loss: 0.6147037148475647 \n",
      "     Training Step: 51 Training Loss: 0.6114418506622314 \n",
      "     Training Step: 52 Training Loss: 0.6119330525398254 \n",
      "     Training Step: 53 Training Loss: 0.6135966181755066 \n",
      "     Training Step: 54 Training Loss: 0.611830472946167 \n",
      "     Training Step: 55 Training Loss: 0.6137962341308594 \n",
      "     Training Step: 56 Training Loss: 0.6102832555770874 \n",
      "     Training Step: 57 Training Loss: 0.6142871379852295 \n",
      "     Training Step: 58 Training Loss: 0.6154755353927612 \n",
      "     Training Step: 59 Training Loss: 0.6109429597854614 \n",
      "     Training Step: 60 Training Loss: 0.6141839027404785 \n",
      "     Training Step: 61 Training Loss: 0.6200665235519409 \n",
      "     Training Step: 62 Training Loss: 0.6123447418212891 \n",
      "     Training Step: 63 Training Loss: 0.6143866181373596 \n",
      "     Training Step: 64 Training Loss: 0.6131365299224854 \n",
      "     Training Step: 65 Training Loss: 0.6168279647827148 \n",
      "     Training Step: 66 Training Loss: 0.6106847524642944 \n",
      "     Training Step: 67 Training Loss: 0.6142730116844177 \n",
      "     Training Step: 68 Training Loss: 0.611456573009491 \n",
      "     Training Step: 69 Training Loss: 0.615034818649292 \n",
      "     Training Step: 70 Training Loss: 0.61607825756073 \n",
      "     Training Step: 71 Training Loss: 0.6133385896682739 \n",
      "     Training Step: 72 Training Loss: 0.6122913956642151 \n",
      "     Training Step: 73 Training Loss: 0.6156362295150757 \n",
      "     Training Step: 74 Training Loss: 0.6161433458328247 \n",
      "     Training Step: 75 Training Loss: 0.612754225730896 \n",
      "     Training Step: 76 Training Loss: 0.6185890436172485 \n",
      "     Training Step: 77 Training Loss: 0.6177763938903809 \n",
      "     Training Step: 78 Training Loss: 0.614384651184082 \n",
      "     Training Step: 79 Training Loss: 0.6154612302780151 \n",
      "     Training Step: 80 Training Loss: 0.6108656525611877 \n",
      "     Training Step: 81 Training Loss: 0.615696370601654 \n",
      "     Training Step: 82 Training Loss: 0.61393803358078 \n",
      "     Training Step: 83 Training Loss: 0.6164849996566772 \n",
      "     Training Step: 84 Training Loss: 0.6143998503684998 \n",
      "     Training Step: 85 Training Loss: 0.6116517186164856 \n",
      "     Training Step: 86 Training Loss: 0.6184842586517334 \n",
      "     Training Step: 87 Training Loss: 0.6167358756065369 \n",
      "     Training Step: 88 Training Loss: 0.6116566061973572 \n",
      "     Training Step: 89 Training Loss: 0.6153943538665771 \n",
      "     Training Step: 90 Training Loss: 0.6127937436103821 \n",
      "     Training Step: 91 Training Loss: 0.6171221733093262 \n",
      "     Training Step: 92 Training Loss: 0.6098041534423828 \n",
      "     Training Step: 93 Training Loss: 0.6183252334594727 \n",
      "     Training Step: 94 Training Loss: 0.6165285706520081 \n",
      "     Training Step: 95 Training Loss: 0.6101697087287903 \n",
      "     Training Step: 96 Training Loss: 0.6140773296356201 \n",
      "     Training Step: 97 Training Loss: 0.6153742074966431 \n",
      "     Training Step: 98 Training Loss: 0.6169781684875488 \n",
      "     Training Step: 99 Training Loss: 0.6111912727355957 \n",
      "     Training Step: 100 Training Loss: 0.615909218788147 \n",
      "     Training Step: 101 Training Loss: 0.61717289686203 \n",
      "     Training Step: 102 Training Loss: 0.614307701587677 \n",
      "     Training Step: 103 Training Loss: 0.6128984689712524 \n",
      "     Training Step: 104 Training Loss: 0.6126353144645691 \n",
      "     Training Step: 105 Training Loss: 0.6116507649421692 \n",
      "     Training Step: 106 Training Loss: 0.6121405959129333 \n",
      "     Training Step: 107 Training Loss: 0.6147148013114929 \n",
      "     Training Step: 108 Training Loss: 0.6117265820503235 \n",
      "     Training Step: 109 Training Loss: 0.6178416013717651 \n",
      "     Training Step: 110 Training Loss: 0.614753782749176 \n",
      "     Training Step: 111 Training Loss: 0.6122891902923584 \n",
      "     Training Step: 112 Training Loss: 0.6142374873161316 \n",
      "     Training Step: 113 Training Loss: 0.6095905303955078 \n",
      "     Training Step: 114 Training Loss: 0.6184430718421936 \n",
      "     Training Step: 115 Training Loss: 0.615405797958374 \n",
      "     Training Step: 116 Training Loss: 0.6130639314651489 \n",
      "     Training Step: 117 Training Loss: 0.6161174774169922 \n",
      "     Training Step: 118 Training Loss: 0.6158441305160522 \n",
      "     Training Step: 119 Training Loss: 0.6156182289123535 \n",
      "     Training Step: 120 Training Loss: 0.6197241544723511 \n",
      "     Training Step: 121 Training Loss: 0.6152418851852417 \n",
      "     Training Step: 122 Training Loss: 0.6117626428604126 \n",
      "     Training Step: 123 Training Loss: 0.6137790083885193 \n",
      "     Training Step: 124 Training Loss: 0.6115735769271851 \n",
      "     Training Step: 125 Training Loss: 0.6115443706512451 \n",
      "     Training Step: 126 Training Loss: 0.6192730665206909 \n",
      "     Training Step: 127 Training Loss: 0.6163533926010132 \n",
      "     Training Step: 128 Training Loss: 0.6184387803077698 \n",
      "     Training Step: 129 Training Loss: 0.6110258102416992 \n",
      "     Training Step: 130 Training Loss: 0.6146288514137268 \n",
      "     Training Step: 131 Training Loss: 0.6119588017463684 \n",
      "     Training Step: 132 Training Loss: 0.615328848361969 \n",
      "     Training Step: 133 Training Loss: 0.6149389743804932 \n",
      "     Training Step: 134 Training Loss: 0.6092897057533264 \n",
      "     Training Step: 135 Training Loss: 0.6172209978103638 \n",
      "     Training Step: 136 Training Loss: 0.6187704205513 \n",
      "     Training Step: 137 Training Loss: 0.61226886510849 \n",
      "     Training Step: 138 Training Loss: 0.615584671497345 \n",
      "     Training Step: 139 Training Loss: 0.6120899319648743 \n",
      "     Training Step: 140 Training Loss: 0.6178447008132935 \n",
      "     Training Step: 141 Training Loss: 0.61069655418396 \n",
      "     Training Step: 142 Training Loss: 0.6186767816543579 \n",
      "     Training Step: 143 Training Loss: 0.6136257648468018 \n",
      "     Training Step: 144 Training Loss: 0.6147581934928894 \n",
      "     Training Step: 145 Training Loss: 0.613463282585144 \n",
      "     Training Step: 146 Training Loss: 0.6135175824165344 \n",
      "     Training Step: 147 Training Loss: 0.6129918694496155 \n",
      "     Training Step: 148 Training Loss: 0.609580397605896 \n",
      "     Training Step: 149 Training Loss: 0.6112083792686462 \n",
      "     Training Step: 150 Training Loss: 0.6114581227302551 \n",
      "     Training Step: 151 Training Loss: 0.6101009249687195 \n",
      "     Training Step: 152 Training Loss: 0.6146952509880066 \n",
      "     Training Step: 153 Training Loss: 0.6132223010063171 \n",
      "     Training Step: 154 Training Loss: 0.6134193539619446 \n",
      "     Training Step: 155 Training Loss: 0.6140859127044678 \n",
      "     Training Step: 156 Training Loss: 0.6119526624679565 \n",
      "     Training Step: 157 Training Loss: 0.6137120723724365 \n",
      "     Training Step: 158 Training Loss: 0.6125959157943726 \n",
      "     Training Step: 159 Training Loss: 0.6112567186355591 \n",
      "     Training Step: 160 Training Loss: 0.6141136288642883 \n",
      "     Training Step: 161 Training Loss: 0.6168889403343201 \n",
      "     Training Step: 162 Training Loss: 0.6107544302940369 \n",
      "     Training Step: 163 Training Loss: 0.6128638982772827 \n",
      "     Training Step: 164 Training Loss: 0.6133513450622559 \n",
      "     Training Step: 165 Training Loss: 0.6181780695915222 \n",
      "     Training Step: 166 Training Loss: 0.6149229407310486 \n",
      "     Training Step: 167 Training Loss: 0.6149929165840149 \n",
      "     Training Step: 168 Training Loss: 0.6147283315658569 \n",
      "     Training Step: 169 Training Loss: 0.6149569153785706 \n",
      "     Training Step: 170 Training Loss: 0.6106544733047485 \n",
      "     Training Step: 171 Training Loss: 0.61174076795578 \n",
      "     Training Step: 172 Training Loss: 0.6164970397949219 \n",
      "     Training Step: 173 Training Loss: 0.6136801838874817 \n",
      "     Training Step: 174 Training Loss: 0.6136844158172607 \n",
      "     Training Step: 175 Training Loss: 0.6154587268829346 \n",
      "     Training Step: 176 Training Loss: 0.6132657527923584 \n",
      "     Training Step: 177 Training Loss: 0.6152174472808838 \n",
      "     Training Step: 178 Training Loss: 0.6100944876670837 \n",
      "     Training Step: 179 Training Loss: 0.6167700290679932 \n",
      "     Training Step: 180 Training Loss: 0.6146571636199951 \n",
      "     Training Step: 181 Training Loss: 0.6155261993408203 \n",
      "     Training Step: 182 Training Loss: 0.6107193231582642 \n",
      "     Training Step: 183 Training Loss: 0.6180954575538635 \n",
      "     Training Step: 184 Training Loss: 0.6102160215377808 \n",
      "     Training Step: 185 Training Loss: 0.6146931648254395 \n",
      "     Training Step: 186 Training Loss: 0.6139557957649231 \n",
      "     Training Step: 187 Training Loss: 0.6122835278511047 \n",
      "     Training Step: 188 Training Loss: 0.6168558597564697 \n",
      "     Training Step: 189 Training Loss: 0.6151438355445862 \n",
      "     Training Step: 190 Training Loss: 0.6128504872322083 \n",
      "     Training Step: 191 Training Loss: 0.6162588596343994 \n",
      "     Training Step: 192 Training Loss: 0.6133825182914734 \n",
      "     Training Step: 193 Training Loss: 0.6174086332321167 \n",
      "     Training Step: 194 Training Loss: 0.6110780835151672 \n",
      "     Training Step: 195 Training Loss: 0.6148477792739868 \n",
      "     Training Step: 196 Training Loss: 0.6132603883743286 \n",
      "     Training Step: 197 Training Loss: 0.612339198589325 \n",
      "     Training Step: 198 Training Loss: 0.6182123422622681 \n",
      "     Training Step: 199 Training Loss: 0.611912727355957 \n",
      "     Training Step: 200 Training Loss: 0.6202845573425293 \n",
      "     Training Step: 201 Training Loss: 0.616814136505127 \n",
      "     Training Step: 202 Training Loss: 0.6177501082420349 \n",
      "     Training Step: 203 Training Loss: 0.6137468814849854 \n",
      "     Training Step: 204 Training Loss: 0.6151672601699829 \n",
      "     Training Step: 205 Training Loss: 0.6122366189956665 \n",
      "     Training Step: 206 Training Loss: 0.6167280077934265 \n",
      "     Training Step: 207 Training Loss: 0.6136104464530945 \n",
      "     Training Step: 208 Training Loss: 0.6162978410720825 \n",
      "     Training Step: 209 Training Loss: 0.611497700214386 \n",
      "     Training Step: 210 Training Loss: 0.6168341040611267 \n",
      "     Training Step: 211 Training Loss: 0.6130114197731018 \n",
      "     Training Step: 212 Training Loss: 0.6124517917633057 \n",
      "     Training Step: 213 Training Loss: 0.6129162907600403 \n",
      "     Training Step: 214 Training Loss: 0.6124083995819092 \n",
      "     Training Step: 215 Training Loss: 0.6147741079330444 \n",
      "     Training Step: 216 Training Loss: 0.6097113490104675 \n",
      "     Training Step: 217 Training Loss: 0.6097512245178223 \n",
      "     Training Step: 218 Training Loss: 0.6124098896980286 \n",
      "     Training Step: 219 Training Loss: 0.6158491969108582 \n",
      "     Training Step: 220 Training Loss: 0.6104037165641785 \n",
      "     Training Step: 221 Training Loss: 0.6168531775474548 \n",
      "     Training Step: 222 Training Loss: 0.6133309006690979 \n",
      "     Training Step: 223 Training Loss: 0.6131266951560974 \n",
      "     Training Step: 224 Training Loss: 0.6152904033660889 \n",
      "     Training Step: 225 Training Loss: 0.6153876781463623 \n",
      "     Training Step: 226 Training Loss: 0.616757869720459 \n",
      "     Training Step: 227 Training Loss: 0.6123970150947571 \n",
      "     Training Step: 228 Training Loss: 0.6143296360969543 \n",
      "     Training Step: 229 Training Loss: 0.6133441925048828 \n",
      "     Training Step: 230 Training Loss: 0.619737982749939 \n",
      "     Training Step: 231 Training Loss: 0.6160334944725037 \n",
      "     Training Step: 232 Training Loss: 0.6132252216339111 \n",
      "     Training Step: 233 Training Loss: 0.6119584441184998 \n",
      "     Training Step: 234 Training Loss: 0.6147563457489014 \n",
      "     Training Step: 235 Training Loss: 0.6115817427635193 \n",
      "     Training Step: 236 Training Loss: 0.6156675219535828 \n",
      "     Training Step: 237 Training Loss: 0.6148117780685425 \n",
      "     Training Step: 238 Training Loss: 0.6165381669998169 \n",
      "     Training Step: 239 Training Loss: 0.6126125454902649 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6160683631896973 \n",
      "     Validation Step: 1 Validation Loss: 0.6153841614723206 \n",
      "     Validation Step: 2 Validation Loss: 0.6112927794456482 \n",
      "     Validation Step: 3 Validation Loss: 0.6143164038658142 \n",
      "     Validation Step: 4 Validation Loss: 0.6141981482505798 \n",
      "     Validation Step: 5 Validation Loss: 0.6180903911590576 \n",
      "     Validation Step: 6 Validation Loss: 0.6176139712333679 \n",
      "     Validation Step: 7 Validation Loss: 0.6107860803604126 \n",
      "     Validation Step: 8 Validation Loss: 0.6147087812423706 \n",
      "     Validation Step: 9 Validation Loss: 0.6158795356750488 \n",
      "     Validation Step: 10 Validation Loss: 0.617323100566864 \n",
      "     Validation Step: 11 Validation Loss: 0.6077595949172974 \n",
      "     Validation Step: 12 Validation Loss: 0.6120123863220215 \n",
      "     Validation Step: 13 Validation Loss: 0.6146175265312195 \n",
      "     Validation Step: 14 Validation Loss: 0.6103053092956543 \n",
      "     Validation Step: 15 Validation Loss: 0.6142869591712952 \n",
      "     Validation Step: 16 Validation Loss: 0.6137019991874695 \n",
      "     Validation Step: 17 Validation Loss: 0.6113035678863525 \n",
      "     Validation Step: 18 Validation Loss: 0.612259566783905 \n",
      "     Validation Step: 19 Validation Loss: 0.6133691668510437 \n",
      "     Validation Step: 20 Validation Loss: 0.6185117363929749 \n",
      "     Validation Step: 21 Validation Loss: 0.6129222512245178 \n",
      "     Validation Step: 22 Validation Loss: 0.6163065433502197 \n",
      "     Validation Step: 23 Validation Loss: 0.6149104833602905 \n",
      "     Validation Step: 24 Validation Loss: 0.6183760166168213 \n",
      "     Validation Step: 25 Validation Loss: 0.6142303943634033 \n",
      "     Validation Step: 26 Validation Loss: 0.6117517948150635 \n",
      "     Validation Step: 27 Validation Loss: 0.6106191277503967 \n",
      "     Validation Step: 28 Validation Loss: 0.6152843832969666 \n",
      "     Validation Step: 29 Validation Loss: 0.6184914112091064 \n",
      "     Validation Step: 30 Validation Loss: 0.6130915284156799 \n",
      "     Validation Step: 31 Validation Loss: 0.6156285405158997 \n",
      "     Validation Step: 32 Validation Loss: 0.6145984530448914 \n",
      "     Validation Step: 33 Validation Loss: 0.6116811037063599 \n",
      "     Validation Step: 34 Validation Loss: 0.610315203666687 \n",
      "     Validation Step: 35 Validation Loss: 0.6137679815292358 \n",
      "     Validation Step: 36 Validation Loss: 0.617034912109375 \n",
      "     Validation Step: 37 Validation Loss: 0.6102969646453857 \n",
      "     Validation Step: 38 Validation Loss: 0.6137286424636841 \n",
      "     Validation Step: 39 Validation Loss: 0.6151138544082642 \n",
      "     Validation Step: 40 Validation Loss: 0.6156647801399231 \n",
      "     Validation Step: 41 Validation Loss: 0.6182712316513062 \n",
      "     Validation Step: 42 Validation Loss: 0.6149572730064392 \n",
      "     Validation Step: 43 Validation Loss: 0.6106554269790649 \n",
      "     Validation Step: 44 Validation Loss: 0.6177366375923157 \n",
      "Epoch: 22\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.618254542350769 \n",
      "     Training Step: 1 Training Loss: 0.6167265176773071 \n",
      "     Training Step: 2 Training Loss: 0.6108584403991699 \n",
      "     Training Step: 3 Training Loss: 0.6188237071037292 \n",
      "     Training Step: 4 Training Loss: 0.6103180050849915 \n",
      "     Training Step: 5 Training Loss: 0.6179331541061401 \n",
      "     Training Step: 6 Training Loss: 0.6114981174468994 \n",
      "     Training Step: 7 Training Loss: 0.6185322403907776 \n",
      "     Training Step: 8 Training Loss: 0.6124216914176941 \n",
      "     Training Step: 9 Training Loss: 0.6116800308227539 \n",
      "     Training Step: 10 Training Loss: 0.6138047575950623 \n",
      "     Training Step: 11 Training Loss: 0.6155750155448914 \n",
      "     Training Step: 12 Training Loss: 0.612562894821167 \n",
      "     Training Step: 13 Training Loss: 0.610016942024231 \n",
      "     Training Step: 14 Training Loss: 0.6179410815238953 \n",
      "     Training Step: 15 Training Loss: 0.6135414242744446 \n",
      "     Training Step: 16 Training Loss: 0.6184490323066711 \n",
      "     Training Step: 17 Training Loss: 0.6145214438438416 \n",
      "     Training Step: 18 Training Loss: 0.6124517321586609 \n",
      "     Training Step: 19 Training Loss: 0.6167728304862976 \n",
      "     Training Step: 20 Training Loss: 0.614783525466919 \n",
      "     Training Step: 21 Training Loss: 0.6106438040733337 \n",
      "     Training Step: 22 Training Loss: 0.6155865788459778 \n",
      "     Training Step: 23 Training Loss: 0.6115841865539551 \n",
      "     Training Step: 24 Training Loss: 0.6158477067947388 \n",
      "     Training Step: 25 Training Loss: 0.6133062243461609 \n",
      "     Training Step: 26 Training Loss: 0.6134239435195923 \n",
      "     Training Step: 27 Training Loss: 0.6117837429046631 \n",
      "     Training Step: 28 Training Loss: 0.6133826375007629 \n",
      "     Training Step: 29 Training Loss: 0.6140681505203247 \n",
      "     Training Step: 30 Training Loss: 0.6116443276405334 \n",
      "     Training Step: 31 Training Loss: 0.6108116507530212 \n",
      "     Training Step: 32 Training Loss: 0.6152753829956055 \n",
      "     Training Step: 33 Training Loss: 0.6142177581787109 \n",
      "     Training Step: 34 Training Loss: 0.6125629544258118 \n",
      "     Training Step: 35 Training Loss: 0.6116161942481995 \n",
      "     Training Step: 36 Training Loss: 0.6145172119140625 \n",
      "     Training Step: 37 Training Loss: 0.6145120859146118 \n",
      "     Training Step: 38 Training Loss: 0.6144264936447144 \n",
      "     Training Step: 39 Training Loss: 0.6132609844207764 \n",
      "     Training Step: 40 Training Loss: 0.6134056448936462 \n",
      "     Training Step: 41 Training Loss: 0.6169998049736023 \n",
      "     Training Step: 42 Training Loss: 0.614494800567627 \n",
      "     Training Step: 43 Training Loss: 0.6146507263183594 \n",
      "     Training Step: 44 Training Loss: 0.6112877130508423 \n",
      "     Training Step: 45 Training Loss: 0.6142553687095642 \n",
      "     Training Step: 46 Training Loss: 0.6185185313224792 \n",
      "     Training Step: 47 Training Loss: 0.6202591061592102 \n",
      "     Training Step: 48 Training Loss: 0.6153332591056824 \n",
      "     Training Step: 49 Training Loss: 0.6160498857498169 \n",
      "     Training Step: 50 Training Loss: 0.613474428653717 \n",
      "     Training Step: 51 Training Loss: 0.6171690821647644 \n",
      "     Training Step: 52 Training Loss: 0.6164310574531555 \n",
      "     Training Step: 53 Training Loss: 0.6153997778892517 \n",
      "     Training Step: 54 Training Loss: 0.6143774390220642 \n",
      "     Training Step: 55 Training Loss: 0.6141796708106995 \n",
      "     Training Step: 56 Training Loss: 0.6147175431251526 \n",
      "     Training Step: 57 Training Loss: 0.6107972264289856 \n",
      "     Training Step: 58 Training Loss: 0.6136593222618103 \n",
      "     Training Step: 59 Training Loss: 0.6114358901977539 \n",
      "     Training Step: 60 Training Loss: 0.6163784265518188 \n",
      "     Training Step: 61 Training Loss: 0.6127214431762695 \n",
      "     Training Step: 62 Training Loss: 0.6181899309158325 \n",
      "     Training Step: 63 Training Loss: 0.6180844306945801 \n",
      "     Training Step: 64 Training Loss: 0.6120077967643738 \n",
      "     Training Step: 65 Training Loss: 0.612254798412323 \n",
      "     Training Step: 66 Training Loss: 0.6185668110847473 \n",
      "     Training Step: 67 Training Loss: 0.6147221326828003 \n",
      "     Training Step: 68 Training Loss: 0.6122975945472717 \n",
      "     Training Step: 69 Training Loss: 0.6129319071769714 \n",
      "     Training Step: 70 Training Loss: 0.6109073162078857 \n",
      "     Training Step: 71 Training Loss: 0.6134935021400452 \n",
      "     Training Step: 72 Training Loss: 0.6178577542304993 \n",
      "     Training Step: 73 Training Loss: 0.6126976013183594 \n",
      "     Training Step: 74 Training Loss: 0.6195209622383118 \n",
      "     Training Step: 75 Training Loss: 0.6186672449111938 \n",
      "     Training Step: 76 Training Loss: 0.6154981255531311 \n",
      "     Training Step: 77 Training Loss: 0.6129471659660339 \n",
      "     Training Step: 78 Training Loss: 0.6141729354858398 \n",
      "     Training Step: 79 Training Loss: 0.6184470057487488 \n",
      "     Training Step: 80 Training Loss: 0.6172508001327515 \n",
      "     Training Step: 81 Training Loss: 0.6162410378456116 \n",
      "     Training Step: 82 Training Loss: 0.6147916913032532 \n",
      "     Training Step: 83 Training Loss: 0.6153549551963806 \n",
      "     Training Step: 84 Training Loss: 0.6141582727432251 \n",
      "     Training Step: 85 Training Loss: 0.6116349101066589 \n",
      "     Training Step: 86 Training Loss: 0.6150001883506775 \n",
      "     Training Step: 87 Training Loss: 0.6140443682670593 \n",
      "     Training Step: 88 Training Loss: 0.6144135594367981 \n",
      "     Training Step: 89 Training Loss: 0.6168291568756104 \n",
      "     Training Step: 90 Training Loss: 0.6104310154914856 \n",
      "     Training Step: 91 Training Loss: 0.6199010014533997 \n",
      "     Training Step: 92 Training Loss: 0.6123124957084656 \n",
      "     Training Step: 93 Training Loss: 0.615371584892273 \n",
      "     Training Step: 94 Training Loss: 0.6151504516601562 \n",
      "     Training Step: 95 Training Loss: 0.6151253581047058 \n",
      "     Training Step: 96 Training Loss: 0.6168785095214844 \n",
      "     Training Step: 97 Training Loss: 0.6165505647659302 \n",
      "     Training Step: 98 Training Loss: 0.6115290522575378 \n",
      "     Training Step: 99 Training Loss: 0.610144317150116 \n",
      "     Training Step: 100 Training Loss: 0.6168144941329956 \n",
      "     Training Step: 101 Training Loss: 0.6189770102500916 \n",
      "     Training Step: 102 Training Loss: 0.6137030720710754 \n",
      "     Training Step: 103 Training Loss: 0.613113284111023 \n",
      "     Training Step: 104 Training Loss: 0.6138090491294861 \n",
      "     Training Step: 105 Training Loss: 0.615494966506958 \n",
      "     Training Step: 106 Training Loss: 0.6106137037277222 \n",
      "     Training Step: 107 Training Loss: 0.6149101257324219 \n",
      "     Training Step: 108 Training Loss: 0.6122096180915833 \n",
      "     Training Step: 109 Training Loss: 0.6116830706596375 \n",
      "     Training Step: 110 Training Loss: 0.6146928668022156 \n",
      "     Training Step: 111 Training Loss: 0.6092875003814697 \n",
      "     Training Step: 112 Training Loss: 0.6082990765571594 \n",
      "     Training Step: 113 Training Loss: 0.6121921539306641 \n",
      "     Training Step: 114 Training Loss: 0.6201489567756653 \n",
      "     Training Step: 115 Training Loss: 0.6158116459846497 \n",
      "     Training Step: 116 Training Loss: 0.6158370971679688 \n",
      "     Training Step: 117 Training Loss: 0.6120067834854126 \n",
      "     Training Step: 118 Training Loss: 0.6101592183113098 \n",
      "     Training Step: 119 Training Loss: 0.6130691170692444 \n",
      "     Training Step: 120 Training Loss: 0.6147820353507996 \n",
      "     Training Step: 121 Training Loss: 0.6124992966651917 \n",
      "     Training Step: 122 Training Loss: 0.616242527961731 \n",
      "     Training Step: 123 Training Loss: 0.6132181882858276 \n",
      "     Training Step: 124 Training Loss: 0.6159857511520386 \n",
      "     Training Step: 125 Training Loss: 0.6148973107337952 \n",
      "     Training Step: 126 Training Loss: 0.6137292385101318 \n",
      "     Training Step: 127 Training Loss: 0.6141784191131592 \n",
      "     Training Step: 128 Training Loss: 0.6126453280448914 \n",
      "     Training Step: 129 Training Loss: 0.6115145087242126 \n",
      "     Training Step: 130 Training Loss: 0.6145418882369995 \n",
      "     Training Step: 131 Training Loss: 0.6129993200302124 \n",
      "     Training Step: 132 Training Loss: 0.615449070930481 \n",
      "     Training Step: 133 Training Loss: 0.6097875833511353 \n",
      "     Training Step: 134 Training Loss: 0.6143134236335754 \n",
      "     Training Step: 135 Training Loss: 0.6128660440444946 \n",
      "     Training Step: 136 Training Loss: 0.6158266067504883 \n",
      "     Training Step: 137 Training Loss: 0.6137211322784424 \n",
      "     Training Step: 138 Training Loss: 0.6140918135643005 \n",
      "     Training Step: 139 Training Loss: 0.6114450097084045 \n",
      "     Training Step: 140 Training Loss: 0.6126302480697632 \n",
      "     Training Step: 141 Training Loss: 0.6094270348548889 \n",
      "     Training Step: 142 Training Loss: 0.6138088703155518 \n",
      "     Training Step: 143 Training Loss: 0.6097582578659058 \n",
      "     Training Step: 144 Training Loss: 0.6147856116294861 \n",
      "     Training Step: 145 Training Loss: 0.6125251054763794 \n",
      "     Training Step: 146 Training Loss: 0.6149986982345581 \n",
      "     Training Step: 147 Training Loss: 0.6196778416633606 \n",
      "     Training Step: 148 Training Loss: 0.6154581308364868 \n",
      "     Training Step: 149 Training Loss: 0.6108954548835754 \n",
      "     Training Step: 150 Training Loss: 0.6154519319534302 \n",
      "     Training Step: 151 Training Loss: 0.6110249757766724 \n",
      "     Training Step: 152 Training Loss: 0.610658586025238 \n",
      "     Training Step: 153 Training Loss: 0.6129715442657471 \n",
      "     Training Step: 154 Training Loss: 0.611594021320343 \n",
      "     Training Step: 155 Training Loss: 0.6178325414657593 \n",
      "     Training Step: 156 Training Loss: 0.6163734793663025 \n",
      "     Training Step: 157 Training Loss: 0.6173003315925598 \n",
      "     Training Step: 158 Training Loss: 0.6111246943473816 \n",
      "     Training Step: 159 Training Loss: 0.6117302179336548 \n",
      "     Training Step: 160 Training Loss: 0.6107886433601379 \n",
      "     Training Step: 161 Training Loss: 0.614852786064148 \n",
      "     Training Step: 162 Training Loss: 0.6167839765548706 \n",
      "     Training Step: 163 Training Loss: 0.6184045076370239 \n",
      "     Training Step: 164 Training Loss: 0.6112316250801086 \n",
      "     Training Step: 165 Training Loss: 0.6132764220237732 \n",
      "     Training Step: 166 Training Loss: 0.6120819449424744 \n",
      "     Training Step: 167 Training Loss: 0.6158198714256287 \n",
      "     Training Step: 168 Training Loss: 0.6178041696548462 \n",
      "     Training Step: 169 Training Loss: 0.6119228601455688 \n",
      "     Training Step: 170 Training Loss: 0.6111900806427002 \n",
      "     Training Step: 171 Training Loss: 0.6130144596099854 \n",
      "     Training Step: 172 Training Loss: 0.6158066391944885 \n",
      "     Training Step: 173 Training Loss: 0.6147756576538086 \n",
      "     Training Step: 174 Training Loss: 0.6121803522109985 \n",
      "     Training Step: 175 Training Loss: 0.6136763691902161 \n",
      "     Training Step: 176 Training Loss: 0.6144682168960571 \n",
      "     Training Step: 177 Training Loss: 0.6167229413986206 \n",
      "     Training Step: 178 Training Loss: 0.6102523803710938 \n",
      "     Training Step: 179 Training Loss: 0.6168628931045532 \n",
      "     Training Step: 180 Training Loss: 0.6132362484931946 \n",
      "     Training Step: 181 Training Loss: 0.6151579022407532 \n",
      "     Training Step: 182 Training Loss: 0.6163854598999023 \n",
      "     Training Step: 183 Training Loss: 0.6156046390533447 \n",
      "     Training Step: 184 Training Loss: 0.6155437231063843 \n",
      "     Training Step: 185 Training Loss: 0.61711585521698 \n",
      "     Training Step: 186 Training Loss: 0.6148878931999207 \n",
      "     Training Step: 187 Training Loss: 0.613958477973938 \n",
      "     Training Step: 188 Training Loss: 0.6153229475021362 \n",
      "     Training Step: 189 Training Loss: 0.6133143305778503 \n",
      "     Training Step: 190 Training Loss: 0.6162129044532776 \n",
      "     Training Step: 191 Training Loss: 0.6183074712753296 \n",
      "     Training Step: 192 Training Loss: 0.6151901483535767 \n",
      "     Training Step: 193 Training Loss: 0.6144170165061951 \n",
      "     Training Step: 194 Training Loss: 0.6209086775779724 \n",
      "     Training Step: 195 Training Loss: 0.6168507933616638 \n",
      "     Training Step: 196 Training Loss: 0.6103564500808716 \n",
      "     Training Step: 197 Training Loss: 0.6144954562187195 \n",
      "     Training Step: 198 Training Loss: 0.61294025182724 \n",
      "     Training Step: 199 Training Loss: 0.6182252168655396 \n",
      "     Training Step: 200 Training Loss: 0.616836428642273 \n",
      "     Training Step: 201 Training Loss: 0.6132588982582092 \n",
      "     Training Step: 202 Training Loss: 0.6132374405860901 \n",
      "     Training Step: 203 Training Loss: 0.6118504405021667 \n",
      "     Training Step: 204 Training Loss: 0.6154302358627319 \n",
      "     Training Step: 205 Training Loss: 0.6133878827095032 \n",
      "     Training Step: 206 Training Loss: 0.6173187494277954 \n",
      "     Training Step: 207 Training Loss: 0.6094847321510315 \n",
      "     Training Step: 208 Training Loss: 0.6122065782546997 \n",
      "     Training Step: 209 Training Loss: 0.6177548766136169 \n",
      "     Training Step: 210 Training Loss: 0.6135034561157227 \n",
      "     Training Step: 211 Training Loss: 0.6166781187057495 \n",
      "     Training Step: 212 Training Loss: 0.6102702617645264 \n",
      "     Training Step: 213 Training Loss: 0.6118592023849487 \n",
      "     Training Step: 214 Training Loss: 0.612943172454834 \n",
      "     Training Step: 215 Training Loss: 0.6176448464393616 \n",
      "     Training Step: 216 Training Loss: 0.6149884462356567 \n",
      "     Training Step: 217 Training Loss: 0.6167734861373901 \n",
      "     Training Step: 218 Training Loss: 0.6118974089622498 \n",
      "     Training Step: 219 Training Loss: 0.6117357015609741 \n",
      "     Training Step: 220 Training Loss: 0.6159810423851013 \n",
      "     Training Step: 221 Training Loss: 0.6174285411834717 \n",
      "     Training Step: 222 Training Loss: 0.6146263480186462 \n",
      "     Training Step: 223 Training Loss: 0.6104772686958313 \n",
      "     Training Step: 224 Training Loss: 0.6167476177215576 \n",
      "     Training Step: 225 Training Loss: 0.614717960357666 \n",
      "     Training Step: 226 Training Loss: 0.6177873015403748 \n",
      "     Training Step: 227 Training Loss: 0.6139253377914429 \n",
      "     Training Step: 228 Training Loss: 0.6124637722969055 \n",
      "     Training Step: 229 Training Loss: 0.6123605370521545 \n",
      "     Training Step: 230 Training Loss: 0.6154969930648804 \n",
      "     Training Step: 231 Training Loss: 0.6146835088729858 \n",
      "     Training Step: 232 Training Loss: 0.6138387322425842 \n",
      "     Training Step: 233 Training Loss: 0.6167477965354919 \n",
      "     Training Step: 234 Training Loss: 0.612265944480896 \n",
      "     Training Step: 235 Training Loss: 0.6169015765190125 \n",
      "     Training Step: 236 Training Loss: 0.6152684688568115 \n",
      "     Training Step: 237 Training Loss: 0.6107144951820374 \n",
      "     Training Step: 238 Training Loss: 0.6122614145278931 \n",
      "     Training Step: 239 Training Loss: 0.6118502616882324 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6136946082115173 \n",
      "     Validation Step: 1 Validation Loss: 0.6154165863990784 \n",
      "     Validation Step: 2 Validation Loss: 0.6184272170066833 \n",
      "     Validation Step: 3 Validation Loss: 0.6121542453765869 \n",
      "     Validation Step: 4 Validation Loss: 0.6143139004707336 \n",
      "     Validation Step: 5 Validation Loss: 0.6106439828872681 \n",
      "     Validation Step: 6 Validation Loss: 0.6128580570220947 \n",
      "     Validation Step: 7 Validation Loss: 0.617161214351654 \n",
      "     Validation Step: 8 Validation Loss: 0.6141981482505798 \n",
      "     Validation Step: 9 Validation Loss: 0.6101760268211365 \n",
      "     Validation Step: 10 Validation Loss: 0.6157259345054626 \n",
      "     Validation Step: 11 Validation Loss: 0.6146674752235413 \n",
      "     Validation Step: 12 Validation Loss: 0.6137706637382507 \n",
      "     Validation Step: 13 Validation Loss: 0.6185100674629211 \n",
      "     Validation Step: 14 Validation Loss: 0.6111828684806824 \n",
      "     Validation Step: 15 Validation Loss: 0.6177378296852112 \n",
      "     Validation Step: 16 Validation Loss: 0.6115771532058716 \n",
      "     Validation Step: 17 Validation Loss: 0.6149224638938904 \n",
      "     Validation Step: 18 Validation Loss: 0.6112023591995239 \n",
      "     Validation Step: 19 Validation Loss: 0.6178640723228455 \n",
      "     Validation Step: 20 Validation Loss: 0.6186869740486145 \n",
      "     Validation Step: 21 Validation Loss: 0.6151541471481323 \n",
      "     Validation Step: 22 Validation Loss: 0.6119019389152527 \n",
      "     Validation Step: 23 Validation Loss: 0.614956796169281 \n",
      "     Validation Step: 24 Validation Loss: 0.6163909435272217 \n",
      "     Validation Step: 25 Validation Loss: 0.6105359792709351 \n",
      "     Validation Step: 26 Validation Loss: 0.6161585450172424 \n",
      "     Validation Step: 27 Validation Loss: 0.6104990839958191 \n",
      "     Validation Step: 28 Validation Loss: 0.6157316565513611 \n",
      "     Validation Step: 29 Validation Loss: 0.6116735935211182 \n",
      "     Validation Step: 30 Validation Loss: 0.6182308793067932 \n",
      "     Validation Step: 31 Validation Loss: 0.617449164390564 \n",
      "     Validation Step: 32 Validation Loss: 0.6141648888587952 \n",
      "     Validation Step: 33 Validation Loss: 0.61532062292099 \n",
      "     Validation Step: 34 Validation Loss: 0.610101044178009 \n",
      "     Validation Step: 35 Validation Loss: 0.6074891686439514 \n",
      "     Validation Step: 36 Validation Loss: 0.6142771244049072 \n",
      "     Validation Step: 37 Validation Loss: 0.613738477230072 \n",
      "     Validation Step: 38 Validation Loss: 0.6186456084251404 \n",
      "     Validation Step: 39 Validation Loss: 0.6130387783050537 \n",
      "     Validation Step: 40 Validation Loss: 0.6159528493881226 \n",
      "     Validation Step: 41 Validation Loss: 0.6133546829223633 \n",
      "     Validation Step: 42 Validation Loss: 0.6147341132164001 \n",
      "     Validation Step: 43 Validation Loss: 0.6145997047424316 \n",
      "     Validation Step: 44 Validation Loss: 0.610135018825531 \n",
      "Epoch: 23\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.614094614982605 \n",
      "     Training Step: 1 Training Loss: 0.6148005723953247 \n",
      "     Training Step: 2 Training Loss: 0.6159660816192627 \n",
      "     Training Step: 3 Training Loss: 0.6178390383720398 \n",
      "     Training Step: 4 Training Loss: 0.6166866421699524 \n",
      "     Training Step: 5 Training Loss: 0.6125915050506592 \n",
      "     Training Step: 6 Training Loss: 0.6118128299713135 \n",
      "     Training Step: 7 Training Loss: 0.6107359528541565 \n",
      "     Training Step: 8 Training Loss: 0.6173990964889526 \n",
      "     Training Step: 9 Training Loss: 0.616923987865448 \n",
      "     Training Step: 10 Training Loss: 0.6119754910469055 \n",
      "     Training Step: 11 Training Loss: 0.6139385104179382 \n",
      "     Training Step: 12 Training Loss: 0.6125916838645935 \n",
      "     Training Step: 13 Training Loss: 0.6133574843406677 \n",
      "     Training Step: 14 Training Loss: 0.6109970211982727 \n",
      "     Training Step: 15 Training Loss: 0.6188085675239563 \n",
      "     Training Step: 16 Training Loss: 0.6125383973121643 \n",
      "     Training Step: 17 Training Loss: 0.6129088401794434 \n",
      "     Training Step: 18 Training Loss: 0.6118228435516357 \n",
      "     Training Step: 19 Training Loss: 0.6097827553749084 \n",
      "     Training Step: 20 Training Loss: 0.6163293719291687 \n",
      "     Training Step: 21 Training Loss: 0.6146436333656311 \n",
      "     Training Step: 22 Training Loss: 0.6147715449333191 \n",
      "     Training Step: 23 Training Loss: 0.6180179715156555 \n",
      "     Training Step: 24 Training Loss: 0.6138147115707397 \n",
      "     Training Step: 25 Training Loss: 0.6166784167289734 \n",
      "     Training Step: 26 Training Loss: 0.6156182885169983 \n",
      "     Training Step: 27 Training Loss: 0.6164637804031372 \n",
      "     Training Step: 28 Training Loss: 0.6146339774131775 \n",
      "     Training Step: 29 Training Loss: 0.612562894821167 \n",
      "     Training Step: 30 Training Loss: 0.6129028797149658 \n",
      "     Training Step: 31 Training Loss: 0.6102225184440613 \n",
      "     Training Step: 32 Training Loss: 0.6097689270973206 \n",
      "     Training Step: 33 Training Loss: 0.6097139716148376 \n",
      "     Training Step: 34 Training Loss: 0.6119080185890198 \n",
      "     Training Step: 35 Training Loss: 0.6149767637252808 \n",
      "     Training Step: 36 Training Loss: 0.6122033596038818 \n",
      "     Training Step: 37 Training Loss: 0.6177927255630493 \n",
      "     Training Step: 38 Training Loss: 0.6117204427719116 \n",
      "     Training Step: 39 Training Loss: 0.6209436058998108 \n",
      "     Training Step: 40 Training Loss: 0.6180487275123596 \n",
      "     Training Step: 41 Training Loss: 0.6121661067008972 \n",
      "     Training Step: 42 Training Loss: 0.6135051250457764 \n",
      "     Training Step: 43 Training Loss: 0.6133846044540405 \n",
      "     Training Step: 44 Training Loss: 0.6144931316375732 \n",
      "     Training Step: 45 Training Loss: 0.6123385429382324 \n",
      "     Training Step: 46 Training Loss: 0.6156372427940369 \n",
      "     Training Step: 47 Training Loss: 0.611727774143219 \n",
      "     Training Step: 48 Training Loss: 0.6202889084815979 \n",
      "     Training Step: 49 Training Loss: 0.6149352788925171 \n",
      "     Training Step: 50 Training Loss: 0.6158688068389893 \n",
      "     Training Step: 51 Training Loss: 0.6153706908226013 \n",
      "     Training Step: 52 Training Loss: 0.6084383726119995 \n",
      "     Training Step: 53 Training Loss: 0.6092626452445984 \n",
      "     Training Step: 54 Training Loss: 0.6154717803001404 \n",
      "     Training Step: 55 Training Loss: 0.6149539947509766 \n",
      "     Training Step: 56 Training Loss: 0.6186323165893555 \n",
      "     Training Step: 57 Training Loss: 0.6130034923553467 \n",
      "     Training Step: 58 Training Loss: 0.618527889251709 \n",
      "     Training Step: 59 Training Loss: 0.6170546412467957 \n",
      "     Training Step: 60 Training Loss: 0.6157475113868713 \n",
      "     Training Step: 61 Training Loss: 0.6144617795944214 \n",
      "     Training Step: 62 Training Loss: 0.6133106350898743 \n",
      "     Training Step: 63 Training Loss: 0.6177430152893066 \n",
      "     Training Step: 64 Training Loss: 0.6167141199111938 \n",
      "     Training Step: 65 Training Loss: 0.614812970161438 \n",
      "     Training Step: 66 Training Loss: 0.6122892498970032 \n",
      "     Training Step: 67 Training Loss: 0.6177613139152527 \n",
      "     Training Step: 68 Training Loss: 0.6116431355476379 \n",
      "     Training Step: 69 Training Loss: 0.6120024919509888 \n",
      "     Training Step: 70 Training Loss: 0.6136927604675293 \n",
      "     Training Step: 71 Training Loss: 0.6154446601867676 \n",
      "     Training Step: 72 Training Loss: 0.614653468132019 \n",
      "     Training Step: 73 Training Loss: 0.6122156977653503 \n",
      "     Training Step: 74 Training Loss: 0.6139891743659973 \n",
      "     Training Step: 75 Training Loss: 0.6184878349304199 \n",
      "     Training Step: 76 Training Loss: 0.6166938543319702 \n",
      "     Training Step: 77 Training Loss: 0.6143126487731934 \n",
      "     Training Step: 78 Training Loss: 0.6123135685920715 \n",
      "     Training Step: 79 Training Loss: 0.6154327392578125 \n",
      "     Training Step: 80 Training Loss: 0.6178538203239441 \n",
      "     Training Step: 81 Training Loss: 0.6105777621269226 \n",
      "     Training Step: 82 Training Loss: 0.6137336492538452 \n",
      "     Training Step: 83 Training Loss: 0.6183759570121765 \n",
      "     Training Step: 84 Training Loss: 0.6157906651496887 \n",
      "     Training Step: 85 Training Loss: 0.6107093095779419 \n",
      "     Training Step: 86 Training Loss: 0.6117225885391235 \n",
      "     Training Step: 87 Training Loss: 0.6133586764335632 \n",
      "     Training Step: 88 Training Loss: 0.6151731014251709 \n",
      "     Training Step: 89 Training Loss: 0.6151753067970276 \n",
      "     Training Step: 90 Training Loss: 0.6139920353889465 \n",
      "     Training Step: 91 Training Loss: 0.6147065162658691 \n",
      "     Training Step: 92 Training Loss: 0.6116396188735962 \n",
      "     Training Step: 93 Training Loss: 0.6146848797798157 \n",
      "     Training Step: 94 Training Loss: 0.6153018474578857 \n",
      "     Training Step: 95 Training Loss: 0.6168304085731506 \n",
      "     Training Step: 96 Training Loss: 0.6113075613975525 \n",
      "     Training Step: 97 Training Loss: 0.6147547364234924 \n",
      "     Training Step: 98 Training Loss: 0.6124163269996643 \n",
      "     Training Step: 99 Training Loss: 0.6122099757194519 \n",
      "     Training Step: 100 Training Loss: 0.6133352518081665 \n",
      "     Training Step: 101 Training Loss: 0.611572802066803 \n",
      "     Training Step: 102 Training Loss: 0.618704617023468 \n",
      "     Training Step: 103 Training Loss: 0.6114727258682251 \n",
      "     Training Step: 104 Training Loss: 0.6104766130447388 \n",
      "     Training Step: 105 Training Loss: 0.6147474050521851 \n",
      "     Training Step: 106 Training Loss: 0.6181520223617554 \n",
      "     Training Step: 107 Training Loss: 0.6154156923294067 \n",
      "     Training Step: 108 Training Loss: 0.6120858192443848 \n",
      "     Training Step: 109 Training Loss: 0.6101574301719666 \n",
      "     Training Step: 110 Training Loss: 0.6100354194641113 \n",
      "     Training Step: 111 Training Loss: 0.6095081567764282 \n",
      "     Training Step: 112 Training Loss: 0.6136192083358765 \n",
      "     Training Step: 113 Training Loss: 0.6112478375434875 \n",
      "     Training Step: 114 Training Loss: 0.6150646805763245 \n",
      "     Training Step: 115 Training Loss: 0.6153868436813354 \n",
      "     Training Step: 116 Training Loss: 0.6142841577529907 \n",
      "     Training Step: 117 Training Loss: 0.6150550246238708 \n",
      "     Training Step: 118 Training Loss: 0.6128200888633728 \n",
      "     Training Step: 119 Training Loss: 0.6167386770248413 \n",
      "     Training Step: 120 Training Loss: 0.6141052842140198 \n",
      "     Training Step: 121 Training Loss: 0.6138848662376404 \n",
      "     Training Step: 122 Training Loss: 0.6115382313728333 \n",
      "     Training Step: 123 Training Loss: 0.6147501468658447 \n",
      "     Training Step: 124 Training Loss: 0.6170241832733154 \n",
      "     Training Step: 125 Training Loss: 0.6106575131416321 \n",
      "     Training Step: 126 Training Loss: 0.6153109073638916 \n",
      "     Training Step: 127 Training Loss: 0.6163697242736816 \n",
      "     Training Step: 128 Training Loss: 0.612474262714386 \n",
      "     Training Step: 129 Training Loss: 0.6168423891067505 \n",
      "     Training Step: 130 Training Loss: 0.6155714392662048 \n",
      "     Training Step: 131 Training Loss: 0.6121675372123718 \n",
      "     Training Step: 132 Training Loss: 0.6115289330482483 \n",
      "     Training Step: 133 Training Loss: 0.6115750670433044 \n",
      "     Training Step: 134 Training Loss: 0.6127341389656067 \n",
      "     Training Step: 135 Training Loss: 0.6100718975067139 \n",
      "     Training Step: 136 Training Loss: 0.61725914478302 \n",
      "     Training Step: 137 Training Loss: 0.6118990182876587 \n",
      "     Training Step: 138 Training Loss: 0.6122952699661255 \n",
      "     Training Step: 139 Training Loss: 0.6105931997299194 \n",
      "     Training Step: 140 Training Loss: 0.6101727485656738 \n",
      "     Training Step: 141 Training Loss: 0.6148375868797302 \n",
      "     Training Step: 142 Training Loss: 0.616051971912384 \n",
      "     Training Step: 143 Training Loss: 0.6167860627174377 \n",
      "     Training Step: 144 Training Loss: 0.6128082871437073 \n",
      "     Training Step: 145 Training Loss: 0.6143162846565247 \n",
      "     Training Step: 146 Training Loss: 0.614408016204834 \n",
      "     Training Step: 147 Training Loss: 0.6146811842918396 \n",
      "     Training Step: 148 Training Loss: 0.6173763275146484 \n",
      "     Training Step: 149 Training Loss: 0.6184226274490356 \n",
      "     Training Step: 150 Training Loss: 0.6115378737449646 \n",
      "     Training Step: 151 Training Loss: 0.6117132306098938 \n",
      "     Training Step: 152 Training Loss: 0.613100528717041 \n",
      "     Training Step: 153 Training Loss: 0.6100782155990601 \n",
      "     Training Step: 154 Training Loss: 0.6103993058204651 \n",
      "     Training Step: 155 Training Loss: 0.617064893245697 \n",
      "     Training Step: 156 Training Loss: 0.6132622361183167 \n",
      "     Training Step: 157 Training Loss: 0.6200694441795349 \n",
      "     Training Step: 158 Training Loss: 0.6169711351394653 \n",
      "     Training Step: 159 Training Loss: 0.6135776042938232 \n",
      "     Training Step: 160 Training Loss: 0.6153567433357239 \n",
      "     Training Step: 161 Training Loss: 0.616758406162262 \n",
      "     Training Step: 162 Training Loss: 0.6145464181900024 \n",
      "     Training Step: 163 Training Loss: 0.6154968738555908 \n",
      "     Training Step: 164 Training Loss: 0.618201494216919 \n",
      "     Training Step: 165 Training Loss: 0.6135198473930359 \n",
      "     Training Step: 166 Training Loss: 0.6137951016426086 \n",
      "     Training Step: 167 Training Loss: 0.6171611547470093 \n",
      "     Training Step: 168 Training Loss: 0.6138487458229065 \n",
      "     Training Step: 169 Training Loss: 0.6119335293769836 \n",
      "     Training Step: 170 Training Loss: 0.6116254925727844 \n",
      "     Training Step: 171 Training Loss: 0.6169703602790833 \n",
      "     Training Step: 172 Training Loss: 0.6128848791122437 \n",
      "     Training Step: 173 Training Loss: 0.6116931438446045 \n",
      "     Training Step: 174 Training Loss: 0.6154953837394714 \n",
      "     Training Step: 175 Training Loss: 0.6157212257385254 \n",
      "     Training Step: 176 Training Loss: 0.6145303845405579 \n",
      "     Training Step: 177 Training Loss: 0.6108694076538086 \n",
      "     Training Step: 178 Training Loss: 0.6188638806343079 \n",
      "     Training Step: 179 Training Loss: 0.6135255098342896 \n",
      "     Training Step: 180 Training Loss: 0.6178351044654846 \n",
      "     Training Step: 181 Training Loss: 0.6125098466873169 \n",
      "     Training Step: 182 Training Loss: 0.6152124404907227 \n",
      "     Training Step: 183 Training Loss: 0.6130098700523376 \n",
      "     Training Step: 184 Training Loss: 0.6107000112533569 \n",
      "     Training Step: 185 Training Loss: 0.6195197701454163 \n",
      "     Training Step: 186 Training Loss: 0.6109246015548706 \n",
      "     Training Step: 187 Training Loss: 0.6125596165657043 \n",
      "     Training Step: 188 Training Loss: 0.6135286688804626 \n",
      "     Training Step: 189 Training Loss: 0.6168295741081238 \n",
      "     Training Step: 190 Training Loss: 0.6158374547958374 \n",
      "     Training Step: 191 Training Loss: 0.6149016618728638 \n",
      "     Training Step: 192 Training Loss: 0.6171479225158691 \n",
      "     Training Step: 193 Training Loss: 0.6141788363456726 \n",
      "     Training Step: 194 Training Loss: 0.6160188913345337 \n",
      "     Training Step: 195 Training Loss: 0.6164373159408569 \n",
      "     Training Step: 196 Training Loss: 0.6155145168304443 \n",
      "     Training Step: 197 Training Loss: 0.6144664883613586 \n",
      "     Training Step: 198 Training Loss: 0.6196489930152893 \n",
      "     Training Step: 199 Training Loss: 0.6095400452613831 \n",
      "     Training Step: 200 Training Loss: 0.6130871176719666 \n",
      "     Training Step: 201 Training Loss: 0.6131253838539124 \n",
      "     Training Step: 202 Training Loss: 0.6106327772140503 \n",
      "     Training Step: 203 Training Loss: 0.6140801310539246 \n",
      "     Training Step: 204 Training Loss: 0.6158530116081238 \n",
      "     Training Step: 205 Training Loss: 0.6150137782096863 \n",
      "     Training Step: 206 Training Loss: 0.6157631874084473 \n",
      "     Training Step: 207 Training Loss: 0.6151658296585083 \n",
      "     Training Step: 208 Training Loss: 0.6129180192947388 \n",
      "     Training Step: 209 Training Loss: 0.6136332154273987 \n",
      "     Training Step: 210 Training Loss: 0.6176480054855347 \n",
      "     Training Step: 211 Training Loss: 0.6172459125518799 \n",
      "     Training Step: 212 Training Loss: 0.6163934469223022 \n",
      "     Training Step: 213 Training Loss: 0.616136908531189 \n",
      "     Training Step: 214 Training Loss: 0.6154796481132507 \n",
      "     Training Step: 215 Training Loss: 0.6154292225837708 \n",
      "     Training Step: 216 Training Loss: 0.6142162680625916 \n",
      "     Training Step: 217 Training Loss: 0.6144186854362488 \n",
      "     Training Step: 218 Training Loss: 0.6137633919715881 \n",
      "     Training Step: 219 Training Loss: 0.6107805371284485 \n",
      "     Training Step: 220 Training Loss: 0.6111803650856018 \n",
      "     Training Step: 221 Training Loss: 0.6115193963050842 \n",
      "     Training Step: 222 Training Loss: 0.6191112995147705 \n",
      "     Training Step: 223 Training Loss: 0.6144744753837585 \n",
      "     Training Step: 224 Training Loss: 0.6162682175636292 \n",
      "     Training Step: 225 Training Loss: 0.6143683791160583 \n",
      "     Training Step: 226 Training Loss: 0.6161755919456482 \n",
      "     Training Step: 227 Training Loss: 0.6129663586616516 \n",
      "     Training Step: 228 Training Loss: 0.6124078631401062 \n",
      "     Training Step: 229 Training Loss: 0.6107985973358154 \n",
      "     Training Step: 230 Training Loss: 0.6142016649246216 \n",
      "     Training Step: 231 Training Loss: 0.6181052923202515 \n",
      "     Training Step: 232 Training Loss: 0.6134873032569885 \n",
      "     Training Step: 233 Training Loss: 0.6131875514984131 \n",
      "     Training Step: 234 Training Loss: 0.6124970316886902 \n",
      "     Training Step: 235 Training Loss: 0.6130316853523254 \n",
      "     Training Step: 236 Training Loss: 0.6142991185188293 \n",
      "     Training Step: 237 Training Loss: 0.6198325753211975 \n",
      "     Training Step: 238 Training Loss: 0.6132190227508545 \n",
      "     Training Step: 239 Training Loss: 0.6182807683944702 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6141932606697083 \n",
      "     Validation Step: 1 Validation Loss: 0.6076963543891907 \n",
      "     Validation Step: 2 Validation Loss: 0.611246645450592 \n",
      "     Validation Step: 3 Validation Loss: 0.6155847907066345 \n",
      "     Validation Step: 4 Validation Loss: 0.615638792514801 \n",
      "     Validation Step: 5 Validation Loss: 0.6180616021156311 \n",
      "     Validation Step: 6 Validation Loss: 0.6162632703781128 \n",
      "     Validation Step: 7 Validation Loss: 0.6146562695503235 \n",
      "     Validation Step: 8 Validation Loss: 0.6130275130271912 \n",
      "     Validation Step: 9 Validation Loss: 0.6184806227684021 \n",
      "     Validation Step: 10 Validation Loss: 0.6150495409965515 \n",
      "     Validation Step: 11 Validation Loss: 0.6116292476654053 \n",
      "     Validation Step: 12 Validation Loss: 0.6102718114852905 \n",
      "     Validation Step: 13 Validation Loss: 0.6137035489082336 \n",
      "     Validation Step: 14 Validation Loss: 0.6176999807357788 \n",
      "     Validation Step: 15 Validation Loss: 0.6146082282066345 \n",
      "     Validation Step: 16 Validation Loss: 0.6143061518669128 \n",
      "     Validation Step: 17 Validation Loss: 0.6122162938117981 \n",
      "     Validation Step: 18 Validation Loss: 0.6142234802246094 \n",
      "     Validation Step: 19 Validation Loss: 0.6149487495422363 \n",
      "     Validation Step: 20 Validation Loss: 0.6128934621810913 \n",
      "     Validation Step: 21 Validation Loss: 0.6182293891906738 \n",
      "     Validation Step: 22 Validation Loss: 0.6117313504219055 \n",
      "     Validation Step: 23 Validation Loss: 0.6141524910926819 \n",
      "     Validation Step: 24 Validation Loss: 0.6105927228927612 \n",
      "     Validation Step: 25 Validation Loss: 0.6112638711929321 \n",
      "     Validation Step: 26 Validation Loss: 0.6158161759376526 \n",
      "     Validation Step: 27 Validation Loss: 0.6183288097381592 \n",
      "     Validation Step: 28 Validation Loss: 0.6148709058761597 \n",
      "     Validation Step: 29 Validation Loss: 0.6184965372085571 \n",
      "     Validation Step: 30 Validation Loss: 0.6160047054290771 \n",
      "     Validation Step: 31 Validation Loss: 0.6145780682563782 \n",
      "     Validation Step: 32 Validation Loss: 0.6102373600006104 \n",
      "     Validation Step: 33 Validation Loss: 0.6170312762260437 \n",
      "     Validation Step: 34 Validation Loss: 0.6133462190628052 \n",
      "     Validation Step: 35 Validation Loss: 0.6107574701309204 \n",
      "     Validation Step: 36 Validation Loss: 0.6136757731437683 \n",
      "     Validation Step: 37 Validation Loss: 0.6105756759643555 \n",
      "     Validation Step: 38 Validation Loss: 0.6136961579322815 \n",
      "     Validation Step: 39 Validation Loss: 0.617614209651947 \n",
      "     Validation Step: 40 Validation Loss: 0.6152572631835938 \n",
      "     Validation Step: 41 Validation Loss: 0.6153331995010376 \n",
      "     Validation Step: 42 Validation Loss: 0.6119710803031921 \n",
      "     Validation Step: 43 Validation Loss: 0.6103031635284424 \n",
      "     Validation Step: 44 Validation Loss: 0.6173238158226013 \n",
      "Epoch: 24\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6105107665061951 \n",
      "     Training Step: 1 Training Loss: 0.6118927597999573 \n",
      "     Training Step: 2 Training Loss: 0.6150948405265808 \n",
      "     Training Step: 3 Training Loss: 0.6155411601066589 \n",
      "     Training Step: 4 Training Loss: 0.6184645295143127 \n",
      "     Training Step: 5 Training Loss: 0.608497142791748 \n",
      "     Training Step: 6 Training Loss: 0.6186633110046387 \n",
      "     Training Step: 7 Training Loss: 0.6148024201393127 \n",
      "     Training Step: 8 Training Loss: 0.6167511940002441 \n",
      "     Training Step: 9 Training Loss: 0.6125447154045105 \n",
      "     Training Step: 10 Training Loss: 0.6139121651649475 \n",
      "     Training Step: 11 Training Loss: 0.6100953221321106 \n",
      "     Training Step: 12 Training Loss: 0.6092492938041687 \n",
      "     Training Step: 13 Training Loss: 0.6139117479324341 \n",
      "     Training Step: 14 Training Loss: 0.6158873438835144 \n",
      "     Training Step: 15 Training Loss: 0.6115556955337524 \n",
      "     Training Step: 16 Training Loss: 0.6189451217651367 \n",
      "     Training Step: 17 Training Loss: 0.6102325320243835 \n",
      "     Training Step: 18 Training Loss: 0.6116942763328552 \n",
      "     Training Step: 19 Training Loss: 0.6146251559257507 \n",
      "     Training Step: 20 Training Loss: 0.6120989322662354 \n",
      "     Training Step: 21 Training Loss: 0.6157815456390381 \n",
      "     Training Step: 22 Training Loss: 0.6116867065429688 \n",
      "     Training Step: 23 Training Loss: 0.6122143268585205 \n",
      "     Training Step: 24 Training Loss: 0.6147527098655701 \n",
      "     Training Step: 25 Training Loss: 0.6199740767478943 \n",
      "     Training Step: 26 Training Loss: 0.613560140132904 \n",
      "     Training Step: 27 Training Loss: 0.6169544458389282 \n",
      "     Training Step: 28 Training Loss: 0.6148696541786194 \n",
      "     Training Step: 29 Training Loss: 0.6153498888015747 \n",
      "     Training Step: 30 Training Loss: 0.6169290542602539 \n",
      "     Training Step: 31 Training Loss: 0.6149801015853882 \n",
      "     Training Step: 32 Training Loss: 0.6158121824264526 \n",
      "     Training Step: 33 Training Loss: 0.6132480502128601 \n",
      "     Training Step: 34 Training Loss: 0.6141855716705322 \n",
      "     Training Step: 35 Training Loss: 0.6143954396247864 \n",
      "     Training Step: 36 Training Loss: 0.6121007204055786 \n",
      "     Training Step: 37 Training Loss: 0.6176630854606628 \n",
      "     Training Step: 38 Training Loss: 0.6134108901023865 \n",
      "     Training Step: 39 Training Loss: 0.6175307035446167 \n",
      "     Training Step: 40 Training Loss: 0.6126847863197327 \n",
      "     Training Step: 41 Training Loss: 0.6148971915245056 \n",
      "     Training Step: 42 Training Loss: 0.612555742263794 \n",
      "     Training Step: 43 Training Loss: 0.6146478652954102 \n",
      "     Training Step: 44 Training Loss: 0.6151473522186279 \n",
      "     Training Step: 45 Training Loss: 0.6184411644935608 \n",
      "     Training Step: 46 Training Loss: 0.6176950335502625 \n",
      "     Training Step: 47 Training Loss: 0.6154217720031738 \n",
      "     Training Step: 48 Training Loss: 0.6117941737174988 \n",
      "     Training Step: 49 Training Loss: 0.6162037253379822 \n",
      "     Training Step: 50 Training Loss: 0.6112267374992371 \n",
      "     Training Step: 51 Training Loss: 0.614082932472229 \n",
      "     Training Step: 52 Training Loss: 0.6142060160636902 \n",
      "     Training Step: 53 Training Loss: 0.6134768128395081 \n",
      "     Training Step: 54 Training Loss: 0.6163175106048584 \n",
      "     Training Step: 55 Training Loss: 0.6123373508453369 \n",
      "     Training Step: 56 Training Loss: 0.6133585572242737 \n",
      "     Training Step: 57 Training Loss: 0.6143920421600342 \n",
      "     Training Step: 58 Training Loss: 0.6188612580299377 \n",
      "     Training Step: 59 Training Loss: 0.6121861338615417 \n",
      "     Training Step: 60 Training Loss: 0.6153361797332764 \n",
      "     Training Step: 61 Training Loss: 0.6133754253387451 \n",
      "     Training Step: 62 Training Loss: 0.611835777759552 \n",
      "     Training Step: 63 Training Loss: 0.6138516068458557 \n",
      "     Training Step: 64 Training Loss: 0.6128476858139038 \n",
      "     Training Step: 65 Training Loss: 0.6131056547164917 \n",
      "     Training Step: 66 Training Loss: 0.6133092641830444 \n",
      "     Training Step: 67 Training Loss: 0.6156348586082458 \n",
      "     Training Step: 68 Training Loss: 0.6128260493278503 \n",
      "     Training Step: 69 Training Loss: 0.6137889623641968 \n",
      "     Training Step: 70 Training Loss: 0.6116241216659546 \n",
      "     Training Step: 71 Training Loss: 0.6177778244018555 \n",
      "     Training Step: 72 Training Loss: 0.6165417432785034 \n",
      "     Training Step: 73 Training Loss: 0.6145240068435669 \n",
      "     Training Step: 74 Training Loss: 0.6151089072227478 \n",
      "     Training Step: 75 Training Loss: 0.6167006492614746 \n",
      "     Training Step: 76 Training Loss: 0.610866367816925 \n",
      "     Training Step: 77 Training Loss: 0.6159681081771851 \n",
      "     Training Step: 78 Training Loss: 0.6117686033248901 \n",
      "     Training Step: 79 Training Loss: 0.6147540211677551 \n",
      "     Training Step: 80 Training Loss: 0.6167296767234802 \n",
      "     Training Step: 81 Training Loss: 0.6152235269546509 \n",
      "     Training Step: 82 Training Loss: 0.6153805255889893 \n",
      "     Training Step: 83 Training Loss: 0.6196444630622864 \n",
      "     Training Step: 84 Training Loss: 0.6117876768112183 \n",
      "     Training Step: 85 Training Loss: 0.6155844926834106 \n",
      "     Training Step: 86 Training Loss: 0.6117023229598999 \n",
      "     Training Step: 87 Training Loss: 0.6152097582817078 \n",
      "     Training Step: 88 Training Loss: 0.6203949451446533 \n",
      "     Training Step: 89 Training Loss: 0.6149551272392273 \n",
      "     Training Step: 90 Training Loss: 0.6132380366325378 \n",
      "     Training Step: 91 Training Loss: 0.6133337616920471 \n",
      "     Training Step: 92 Training Loss: 0.6135501265525818 \n",
      "     Training Step: 93 Training Loss: 0.6118564605712891 \n",
      "     Training Step: 94 Training Loss: 0.6168681979179382 \n",
      "     Training Step: 95 Training Loss: 0.6114558577537537 \n",
      "     Training Step: 96 Training Loss: 0.6114366054534912 \n",
      "     Training Step: 97 Training Loss: 0.6146882772445679 \n",
      "     Training Step: 98 Training Loss: 0.6127811670303345 \n",
      "     Training Step: 99 Training Loss: 0.6131279468536377 \n",
      "     Training Step: 100 Training Loss: 0.6154143214225769 \n",
      "     Training Step: 101 Training Loss: 0.616500735282898 \n",
      "     Training Step: 102 Training Loss: 0.6153191328048706 \n",
      "     Training Step: 103 Training Loss: 0.6142916679382324 \n",
      "     Training Step: 104 Training Loss: 0.6123194098472595 \n",
      "     Training Step: 105 Training Loss: 0.6154459714889526 \n",
      "     Training Step: 106 Training Loss: 0.6168621778488159 \n",
      "     Training Step: 107 Training Loss: 0.6122496128082275 \n",
      "     Training Step: 108 Training Loss: 0.6118614077568054 \n",
      "     Training Step: 109 Training Loss: 0.615912139415741 \n",
      "     Training Step: 110 Training Loss: 0.6145623326301575 \n",
      "     Training Step: 111 Training Loss: 0.6097863912582397 \n",
      "     Training Step: 112 Training Loss: 0.617838442325592 \n",
      "     Training Step: 113 Training Loss: 0.6160658001899719 \n",
      "     Training Step: 114 Training Loss: 0.6129630208015442 \n",
      "     Training Step: 115 Training Loss: 0.6139776706695557 \n",
      "     Training Step: 116 Training Loss: 0.6156964898109436 \n",
      "     Training Step: 117 Training Loss: 0.6142644286155701 \n",
      "     Training Step: 118 Training Loss: 0.6163038611412048 \n",
      "     Training Step: 119 Training Loss: 0.6106641292572021 \n",
      "     Training Step: 120 Training Loss: 0.6167601346969604 \n",
      "     Training Step: 121 Training Loss: 0.6108130216598511 \n",
      "     Training Step: 122 Training Loss: 0.6119042038917542 \n",
      "     Training Step: 123 Training Loss: 0.6186560392379761 \n",
      "     Training Step: 124 Training Loss: 0.6142658591270447 \n",
      "     Training Step: 125 Training Loss: 0.6098551750183105 \n",
      "     Training Step: 126 Training Loss: 0.6105724573135376 \n",
      "     Training Step: 127 Training Loss: 0.6169052720069885 \n",
      "     Training Step: 128 Training Loss: 0.6114108562469482 \n",
      "     Training Step: 129 Training Loss: 0.6173436045646667 \n",
      "     Training Step: 130 Training Loss: 0.6124821305274963 \n",
      "     Training Step: 131 Training Loss: 0.61159348487854 \n",
      "     Training Step: 132 Training Loss: 0.6166910529136658 \n",
      "     Training Step: 133 Training Loss: 0.6136864423751831 \n",
      "     Training Step: 134 Training Loss: 0.6144917607307434 \n",
      "     Training Step: 135 Training Loss: 0.6124285459518433 \n",
      "     Training Step: 136 Training Loss: 0.6147294044494629 \n",
      "     Training Step: 137 Training Loss: 0.6122556328773499 \n",
      "     Training Step: 138 Training Loss: 0.6106274724006653 \n",
      "     Training Step: 139 Training Loss: 0.6133439540863037 \n",
      "     Training Step: 140 Training Loss: 0.6101331114768982 \n",
      "     Training Step: 141 Training Loss: 0.6105837225914001 \n",
      "     Training Step: 142 Training Loss: 0.6101828813552856 \n",
      "     Training Step: 143 Training Loss: 0.6145034432411194 \n",
      "     Training Step: 144 Training Loss: 0.6137050986289978 \n",
      "     Training Step: 145 Training Loss: 0.6154372096061707 \n",
      "     Training Step: 146 Training Loss: 0.6149933338165283 \n",
      "     Training Step: 147 Training Loss: 0.618039608001709 \n",
      "     Training Step: 148 Training Loss: 0.6147235631942749 \n",
      "     Training Step: 149 Training Loss: 0.6145011782646179 \n",
      "     Training Step: 150 Training Loss: 0.6148101091384888 \n",
      "     Training Step: 151 Training Loss: 0.6106981039047241 \n",
      "     Training Step: 152 Training Loss: 0.6119627356529236 \n",
      "     Training Step: 153 Training Loss: 0.6131957173347473 \n",
      "     Training Step: 154 Training Loss: 0.610426127910614 \n",
      "     Training Step: 155 Training Loss: 0.6147847771644592 \n",
      "     Training Step: 156 Training Loss: 0.6123180985450745 \n",
      "     Training Step: 157 Training Loss: 0.6172494292259216 \n",
      "     Training Step: 158 Training Loss: 0.6181184649467468 \n",
      "     Training Step: 159 Training Loss: 0.614255964756012 \n",
      "     Training Step: 160 Training Loss: 0.6166861057281494 \n",
      "     Training Step: 161 Training Loss: 0.6153377890586853 \n",
      "     Training Step: 162 Training Loss: 0.6130558252334595 \n",
      "     Training Step: 163 Training Loss: 0.6148781776428223 \n",
      "     Training Step: 164 Training Loss: 0.6183432340621948 \n",
      "     Training Step: 165 Training Loss: 0.616517961025238 \n",
      "     Training Step: 166 Training Loss: 0.61125248670578 \n",
      "     Training Step: 167 Training Loss: 0.6123267412185669 \n",
      "     Training Step: 168 Training Loss: 0.6141257882118225 \n",
      "     Training Step: 169 Training Loss: 0.6124196648597717 \n",
      "     Training Step: 170 Training Loss: 0.6114963293075562 \n",
      "     Training Step: 171 Training Loss: 0.6168295741081238 \n",
      "     Training Step: 172 Training Loss: 0.6161625981330872 \n",
      "     Training Step: 173 Training Loss: 0.6125636696815491 \n",
      "     Training Step: 174 Training Loss: 0.610165536403656 \n",
      "     Training Step: 175 Training Loss: 0.610790491104126 \n",
      "     Training Step: 176 Training Loss: 0.6136873960494995 \n",
      "     Training Step: 177 Training Loss: 0.6112205386161804 \n",
      "     Training Step: 178 Training Loss: 0.6115971803665161 \n",
      "     Training Step: 179 Training Loss: 0.6118584871292114 \n",
      "     Training Step: 180 Training Loss: 0.6107431650161743 \n",
      "     Training Step: 181 Training Loss: 0.6105241775512695 \n",
      "     Training Step: 182 Training Loss: 0.6172972917556763 \n",
      "     Training Step: 183 Training Loss: 0.6155214905738831 \n",
      "     Training Step: 184 Training Loss: 0.6132632493972778 \n",
      "     Training Step: 185 Training Loss: 0.61481112241745 \n",
      "     Training Step: 186 Training Loss: 0.6171476244926453 \n",
      "     Training Step: 187 Training Loss: 0.6154431104660034 \n",
      "     Training Step: 188 Training Loss: 0.6140905022621155 \n",
      "     Training Step: 189 Training Loss: 0.6095073223114014 \n",
      "     Training Step: 190 Training Loss: 0.6144403219223022 \n",
      "     Training Step: 191 Training Loss: 0.6175528168678284 \n",
      "     Training Step: 192 Training Loss: 0.6128915548324585 \n",
      "     Training Step: 193 Training Loss: 0.6131832599639893 \n",
      "     Training Step: 194 Training Loss: 0.6147540807723999 \n",
      "     Training Step: 195 Training Loss: 0.6127804517745972 \n",
      "     Training Step: 196 Training Loss: 0.6143153309822083 \n",
      "     Training Step: 197 Training Loss: 0.6195398569107056 \n",
      "     Training Step: 198 Training Loss: 0.6198221445083618 \n",
      "     Training Step: 199 Training Loss: 0.6122012138366699 \n",
      "     Training Step: 200 Training Loss: 0.615816056728363 \n",
      "     Training Step: 201 Training Loss: 0.6135331988334656 \n",
      "     Training Step: 202 Training Loss: 0.6125855445861816 \n",
      "     Training Step: 203 Training Loss: 0.6161791086196899 \n",
      "     Training Step: 204 Training Loss: 0.6116686463356018 \n",
      "     Training Step: 205 Training Loss: 0.6155962944030762 \n",
      "     Training Step: 206 Training Loss: 0.6152646541595459 \n",
      "     Training Step: 207 Training Loss: 0.6100630760192871 \n",
      "     Training Step: 208 Training Loss: 0.6181278228759766 \n",
      "     Training Step: 209 Training Loss: 0.6178290247917175 \n",
      "     Training Step: 210 Training Loss: 0.6182434558868408 \n",
      "     Training Step: 211 Training Loss: 0.613670825958252 \n",
      "     Training Step: 212 Training Loss: 0.6167241334915161 \n",
      "     Training Step: 213 Training Loss: 0.61110520362854 \n",
      "     Training Step: 214 Training Loss: 0.6154914498329163 \n",
      "     Training Step: 215 Training Loss: 0.6210765242576599 \n",
      "     Training Step: 216 Training Loss: 0.6147128939628601 \n",
      "     Training Step: 217 Training Loss: 0.6164079308509827 \n",
      "     Training Step: 218 Training Loss: 0.6125096082687378 \n",
      "     Training Step: 219 Training Loss: 0.6109858751296997 \n",
      "     Training Step: 220 Training Loss: 0.6137701869010925 \n",
      "     Training Step: 221 Training Loss: 0.60971599817276 \n",
      "     Training Step: 222 Training Loss: 0.6140300631523132 \n",
      "     Training Step: 223 Training Loss: 0.6161172986030579 \n",
      "     Training Step: 224 Training Loss: 0.6128786206245422 \n",
      "     Training Step: 225 Training Loss: 0.6114689707756042 \n",
      "     Training Step: 226 Training Loss: 0.6144197583198547 \n",
      "     Training Step: 227 Training Loss: 0.6181215047836304 \n",
      "     Training Step: 228 Training Loss: 0.6178004741668701 \n",
      "     Training Step: 229 Training Loss: 0.6171061992645264 \n",
      "     Training Step: 230 Training Loss: 0.6130317449569702 \n",
      "     Training Step: 231 Training Loss: 0.6184501647949219 \n",
      "     Training Step: 232 Training Loss: 0.6142086386680603 \n",
      "     Training Step: 233 Training Loss: 0.612932026386261 \n",
      "     Training Step: 234 Training Loss: 0.6135546565055847 \n",
      "     Training Step: 235 Training Loss: 0.6178165078163147 \n",
      "     Training Step: 236 Training Loss: 0.6095063090324402 \n",
      "     Training Step: 237 Training Loss: 0.6168251633644104 \n",
      "     Training Step: 238 Training Loss: 0.6183991432189941 \n",
      "     Training Step: 239 Training Loss: 0.6168711185455322 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6130028367042542 \n",
      "     Validation Step: 1 Validation Loss: 0.6113879084587097 \n",
      "     Validation Step: 2 Validation Loss: 0.6170743107795715 \n",
      "     Validation Step: 3 Validation Loss: 0.6183578372001648 \n",
      "     Validation Step: 4 Validation Loss: 0.6106939315795898 \n",
      "     Validation Step: 5 Validation Loss: 0.6143059134483337 \n",
      "     Validation Step: 6 Validation Loss: 0.6162946224212646 \n",
      "     Validation Step: 7 Validation Loss: 0.6150960922241211 \n",
      "     Validation Step: 8 Validation Loss: 0.6123442649841309 \n",
      "     Validation Step: 9 Validation Loss: 0.6106970906257629 \n",
      "     Validation Step: 10 Validation Loss: 0.614314079284668 \n",
      "     Validation Step: 11 Validation Loss: 0.6185226440429688 \n",
      "     Validation Step: 12 Validation Loss: 0.6131215691566467 \n",
      "     Validation Step: 13 Validation Loss: 0.61091148853302 \n",
      "     Validation Step: 14 Validation Loss: 0.6146736741065979 \n",
      "     Validation Step: 15 Validation Loss: 0.6118494868278503 \n",
      "     Validation Step: 16 Validation Loss: 0.614720344543457 \n",
      "     Validation Step: 17 Validation Loss: 0.6104550957679749 \n",
      "     Validation Step: 18 Validation Loss: 0.6160246133804321 \n",
      "     Validation Step: 19 Validation Loss: 0.6104363799095154 \n",
      "     Validation Step: 20 Validation Loss: 0.6113638281822205 \n",
      "     Validation Step: 21 Validation Loss: 0.6144121289253235 \n",
      "     Validation Step: 22 Validation Loss: 0.6158550977706909 \n",
      "     Validation Step: 23 Validation Loss: 0.6117481589317322 \n",
      "     Validation Step: 24 Validation Loss: 0.6180964112281799 \n",
      "     Validation Step: 25 Validation Loss: 0.6156275272369385 \n",
      "     Validation Step: 26 Validation Loss: 0.6137908101081848 \n",
      "     Validation Step: 27 Validation Loss: 0.6078858375549316 \n",
      "     Validation Step: 28 Validation Loss: 0.618527352809906 \n",
      "     Validation Step: 29 Validation Loss: 0.6142388582229614 \n",
      "     Validation Step: 30 Validation Loss: 0.6173709630966187 \n",
      "     Validation Step: 31 Validation Loss: 0.6120915412902832 \n",
      "     Validation Step: 32 Validation Loss: 0.6146708130836487 \n",
      "     Validation Step: 33 Validation Loss: 0.6156837344169617 \n",
      "     Validation Step: 34 Validation Loss: 0.6103942394256592 \n",
      "     Validation Step: 35 Validation Loss: 0.6137279868125916 \n",
      "     Validation Step: 36 Validation Loss: 0.6149446964263916 \n",
      "     Validation Step: 37 Validation Loss: 0.6182419061660767 \n",
      "     Validation Step: 38 Validation Loss: 0.6134416460990906 \n",
      "     Validation Step: 39 Validation Loss: 0.6150437593460083 \n",
      "     Validation Step: 40 Validation Loss: 0.6177331209182739 \n",
      "     Validation Step: 41 Validation Loss: 0.6153896450996399 \n",
      "     Validation Step: 42 Validation Loss: 0.6137504577636719 \n",
      "     Validation Step: 43 Validation Loss: 0.6153256893157959 \n",
      "     Validation Step: 44 Validation Loss: 0.6176754236221313 \n",
      "Epoch: 25\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6184705495834351 \n",
      "     Training Step: 1 Training Loss: 0.616779625415802 \n",
      "     Training Step: 2 Training Loss: 0.6198537945747375 \n",
      "     Training Step: 3 Training Loss: 0.6117179989814758 \n",
      "     Training Step: 4 Training Loss: 0.6116600632667542 \n",
      "     Training Step: 5 Training Loss: 0.6130050420761108 \n",
      "     Training Step: 6 Training Loss: 0.6148034930229187 \n",
      "     Training Step: 7 Training Loss: 0.6174014806747437 \n",
      "     Training Step: 8 Training Loss: 0.6175475716590881 \n",
      "     Training Step: 9 Training Loss: 0.610752284526825 \n",
      "     Training Step: 10 Training Loss: 0.6189498901367188 \n",
      "     Training Step: 11 Training Loss: 0.6141836643218994 \n",
      "     Training Step: 12 Training Loss: 0.6157582998275757 \n",
      "     Training Step: 13 Training Loss: 0.6180655360221863 \n",
      "     Training Step: 14 Training Loss: 0.6203327178955078 \n",
      "     Training Step: 15 Training Loss: 0.6151361465454102 \n",
      "     Training Step: 16 Training Loss: 0.6128683686256409 \n",
      "     Training Step: 17 Training Loss: 0.6167084574699402 \n",
      "     Training Step: 18 Training Loss: 0.6107129454612732 \n",
      "     Training Step: 19 Training Loss: 0.611507773399353 \n",
      "     Training Step: 20 Training Loss: 0.6127460598945618 \n",
      "     Training Step: 21 Training Loss: 0.6147719025611877 \n",
      "     Training Step: 22 Training Loss: 0.613449215888977 \n",
      "     Training Step: 23 Training Loss: 0.6123751401901245 \n",
      "     Training Step: 24 Training Loss: 0.6135718822479248 \n",
      "     Training Step: 25 Training Loss: 0.6135146021842957 \n",
      "     Training Step: 26 Training Loss: 0.61407870054245 \n",
      "     Training Step: 27 Training Loss: 0.6155648231506348 \n",
      "     Training Step: 28 Training Loss: 0.6177822351455688 \n",
      "     Training Step: 29 Training Loss: 0.6126025915145874 \n",
      "     Training Step: 30 Training Loss: 0.6131191253662109 \n",
      "     Training Step: 31 Training Loss: 0.6118830442428589 \n",
      "     Training Step: 32 Training Loss: 0.6132638454437256 \n",
      "     Training Step: 33 Training Loss: 0.6128503680229187 \n",
      "     Training Step: 34 Training Loss: 0.6130108833312988 \n",
      "     Training Step: 35 Training Loss: 0.6142134666442871 \n",
      "     Training Step: 36 Training Loss: 0.6180998682975769 \n",
      "     Training Step: 37 Training Loss: 0.6102506518363953 \n",
      "     Training Step: 38 Training Loss: 0.6177664995193481 \n",
      "     Training Step: 39 Training Loss: 0.6119004487991333 \n",
      "     Training Step: 40 Training Loss: 0.6184972524642944 \n",
      "     Training Step: 41 Training Loss: 0.6128664612770081 \n",
      "     Training Step: 42 Training Loss: 0.6132950186729431 \n",
      "     Training Step: 43 Training Loss: 0.612520158290863 \n",
      "     Training Step: 44 Training Loss: 0.6146504878997803 \n",
      "     Training Step: 45 Training Loss: 0.6168457269668579 \n",
      "     Training Step: 46 Training Loss: 0.6109357476234436 \n",
      "     Training Step: 47 Training Loss: 0.6171491742134094 \n",
      "     Training Step: 48 Training Loss: 0.6119541525840759 \n",
      "     Training Step: 49 Training Loss: 0.61211758852005 \n",
      "     Training Step: 50 Training Loss: 0.6153526306152344 \n",
      "     Training Step: 51 Training Loss: 0.6139246821403503 \n",
      "     Training Step: 52 Training Loss: 0.6131435036659241 \n",
      "     Training Step: 53 Training Loss: 0.6148403882980347 \n",
      "     Training Step: 54 Training Loss: 0.6146621108055115 \n",
      "     Training Step: 55 Training Loss: 0.6115882396697998 \n",
      "     Training Step: 56 Training Loss: 0.6178125739097595 \n",
      "     Training Step: 57 Training Loss: 0.6154199242591858 \n",
      "     Training Step: 58 Training Loss: 0.6158044934272766 \n",
      "     Training Step: 59 Training Loss: 0.6175915598869324 \n",
      "     Training Step: 60 Training Loss: 0.6177011132240295 \n",
      "     Training Step: 61 Training Loss: 0.6158679723739624 \n",
      "     Training Step: 62 Training Loss: 0.6148077845573425 \n",
      "     Training Step: 63 Training Loss: 0.6169048547744751 \n",
      "     Training Step: 64 Training Loss: 0.6142206192016602 \n",
      "     Training Step: 65 Training Loss: 0.6144894361495972 \n",
      "     Training Step: 66 Training Loss: 0.6106231212615967 \n",
      "     Training Step: 67 Training Loss: 0.6135293841362 \n",
      "     Training Step: 68 Training Loss: 0.61676025390625 \n",
      "     Training Step: 69 Training Loss: 0.6158132553100586 \n",
      "     Training Step: 70 Training Loss: 0.6154565215110779 \n",
      "     Training Step: 71 Training Loss: 0.6172651648521423 \n",
      "     Training Step: 72 Training Loss: 0.6153334379196167 \n",
      "     Training Step: 73 Training Loss: 0.614799439907074 \n",
      "     Training Step: 74 Training Loss: 0.6151551604270935 \n",
      "     Training Step: 75 Training Loss: 0.6122473478317261 \n",
      "     Training Step: 76 Training Loss: 0.6105443239212036 \n",
      "     Training Step: 77 Training Loss: 0.6134498715400696 \n",
      "     Training Step: 78 Training Loss: 0.6118634343147278 \n",
      "     Training Step: 79 Training Loss: 0.6143504977226257 \n",
      "     Training Step: 80 Training Loss: 0.6105980277061462 \n",
      "     Training Step: 81 Training Loss: 0.6115397810935974 \n",
      "     Training Step: 82 Training Loss: 0.612960159778595 \n",
      "     Training Step: 83 Training Loss: 0.6149430871009827 \n",
      "     Training Step: 84 Training Loss: 0.615522563457489 \n",
      "     Training Step: 85 Training Loss: 0.6092924475669861 \n",
      "     Training Step: 86 Training Loss: 0.6186676025390625 \n",
      "     Training Step: 87 Training Loss: 0.6166796684265137 \n",
      "     Training Step: 88 Training Loss: 0.6143627166748047 \n",
      "     Training Step: 89 Training Loss: 0.6123406291007996 \n",
      "     Training Step: 90 Training Loss: 0.6153555512428284 \n",
      "     Training Step: 91 Training Loss: 0.6095953583717346 \n",
      "     Training Step: 92 Training Loss: 0.6189252734184265 \n",
      "     Training Step: 93 Training Loss: 0.6144063472747803 \n",
      "     Training Step: 94 Training Loss: 0.6138067245483398 \n",
      "     Training Step: 95 Training Loss: 0.614565372467041 \n",
      "     Training Step: 96 Training Loss: 0.6128261089324951 \n",
      "     Training Step: 97 Training Loss: 0.616072416305542 \n",
      "     Training Step: 98 Training Loss: 0.6131833791732788 \n",
      "     Training Step: 99 Training Loss: 0.6153479814529419 \n",
      "     Training Step: 100 Training Loss: 0.6154937744140625 \n",
      "     Training Step: 101 Training Loss: 0.6116442084312439 \n",
      "     Training Step: 102 Training Loss: 0.6121748685836792 \n",
      "     Training Step: 103 Training Loss: 0.6083680391311646 \n",
      "     Training Step: 104 Training Loss: 0.6123544573783875 \n",
      "     Training Step: 105 Training Loss: 0.6134039163589478 \n",
      "     Training Step: 106 Training Loss: 0.6100172400474548 \n",
      "     Training Step: 107 Training Loss: 0.6144576668739319 \n",
      "     Training Step: 108 Training Loss: 0.6129148602485657 \n",
      "     Training Step: 109 Training Loss: 0.6098107695579529 \n",
      "     Training Step: 110 Training Loss: 0.6121928095817566 \n",
      "     Training Step: 111 Training Loss: 0.6151492595672607 \n",
      "     Training Step: 112 Training Loss: 0.6168133020401001 \n",
      "     Training Step: 113 Training Loss: 0.6150153875350952 \n",
      "     Training Step: 114 Training Loss: 0.6163649559020996 \n",
      "     Training Step: 115 Training Loss: 0.6182475090026855 \n",
      "     Training Step: 116 Training Loss: 0.6138115525245667 \n",
      "     Training Step: 117 Training Loss: 0.6186115145683289 \n",
      "     Training Step: 118 Training Loss: 0.6167604923248291 \n",
      "     Training Step: 119 Training Loss: 0.6143518686294556 \n",
      "     Training Step: 120 Training Loss: 0.6102175116539001 \n",
      "     Training Step: 121 Training Loss: 0.6153202056884766 \n",
      "     Training Step: 122 Training Loss: 0.6147748827934265 \n",
      "     Training Step: 123 Training Loss: 0.6145107746124268 \n",
      "     Training Step: 124 Training Loss: 0.6139529943466187 \n",
      "     Training Step: 125 Training Loss: 0.6095865964889526 \n",
      "     Training Step: 126 Training Loss: 0.6162114143371582 \n",
      "     Training Step: 127 Training Loss: 0.6114469170570374 \n",
      "     Training Step: 128 Training Loss: 0.6156091690063477 \n",
      "     Training Step: 129 Training Loss: 0.6133375763893127 \n",
      "     Training Step: 130 Training Loss: 0.6184937357902527 \n",
      "     Training Step: 131 Training Loss: 0.6124104857444763 \n",
      "     Training Step: 132 Training Loss: 0.6138666272163391 \n",
      "     Training Step: 133 Training Loss: 0.6127574443817139 \n",
      "     Training Step: 134 Training Loss: 0.6197062730789185 \n",
      "     Training Step: 135 Training Loss: 0.6144044399261475 \n",
      "     Training Step: 136 Training Loss: 0.6107684373855591 \n",
      "     Training Step: 137 Training Loss: 0.6151826977729797 \n",
      "     Training Step: 138 Training Loss: 0.6181346774101257 \n",
      "     Training Step: 139 Training Loss: 0.6181167960166931 \n",
      "     Training Step: 140 Training Loss: 0.6119291186332703 \n",
      "     Training Step: 141 Training Loss: 0.6156201362609863 \n",
      "     Training Step: 142 Training Loss: 0.6120868921279907 \n",
      "     Training Step: 143 Training Loss: 0.6125541925430298 \n",
      "     Training Step: 144 Training Loss: 0.6169446110725403 \n",
      "     Training Step: 145 Training Loss: 0.6107240319252014 \n",
      "     Training Step: 146 Training Loss: 0.6124029755592346 \n",
      "     Training Step: 147 Training Loss: 0.6109309792518616 \n",
      "     Training Step: 148 Training Loss: 0.6155527830123901 \n",
      "     Training Step: 149 Training Loss: 0.6139177680015564 \n",
      "     Training Step: 150 Training Loss: 0.6155663728713989 \n",
      "     Training Step: 151 Training Loss: 0.6102316379547119 \n",
      "     Training Step: 152 Training Loss: 0.6147163510322571 \n",
      "     Training Step: 153 Training Loss: 0.6134928464889526 \n",
      "     Training Step: 154 Training Loss: 0.6170401573181152 \n",
      "     Training Step: 155 Training Loss: 0.6164589524269104 \n",
      "     Training Step: 156 Training Loss: 0.613673210144043 \n",
      "     Training Step: 157 Training Loss: 0.6209295392036438 \n",
      "     Training Step: 158 Training Loss: 0.6106758117675781 \n",
      "     Training Step: 159 Training Loss: 0.6134554147720337 \n",
      "     Training Step: 160 Training Loss: 0.6165062785148621 \n",
      "     Training Step: 161 Training Loss: 0.6116794943809509 \n",
      "     Training Step: 162 Training Loss: 0.6161370277404785 \n",
      "     Training Step: 163 Training Loss: 0.616796612739563 \n",
      "     Training Step: 164 Training Loss: 0.6106544137001038 \n",
      "     Training Step: 165 Training Loss: 0.6117035150527954 \n",
      "     Training Step: 166 Training Loss: 0.6196994185447693 \n",
      "     Training Step: 167 Training Loss: 0.6125967502593994 \n",
      "     Training Step: 168 Training Loss: 0.6098718047142029 \n",
      "     Training Step: 169 Training Loss: 0.616686224937439 \n",
      "     Training Step: 170 Training Loss: 0.613314688205719 \n",
      "     Training Step: 171 Training Loss: 0.6146448850631714 \n",
      "     Training Step: 172 Training Loss: 0.6149531006813049 \n",
      "     Training Step: 173 Training Loss: 0.6122666001319885 \n",
      "     Training Step: 174 Training Loss: 0.6101945042610168 \n",
      "     Training Step: 175 Training Loss: 0.6131606101989746 \n",
      "     Training Step: 176 Training Loss: 0.6122493147850037 \n",
      "     Training Step: 177 Training Loss: 0.6157764792442322 \n",
      "     Training Step: 178 Training Loss: 0.6178783178329468 \n",
      "     Training Step: 179 Training Loss: 0.610795259475708 \n",
      "     Training Step: 180 Training Loss: 0.6194400787353516 \n",
      "     Training Step: 181 Training Loss: 0.6177075505256653 \n",
      "     Training Step: 182 Training Loss: 0.616462230682373 \n",
      "     Training Step: 183 Training Loss: 0.6099919080734253 \n",
      "     Training Step: 184 Training Loss: 0.6128974556922913 \n",
      "     Training Step: 185 Training Loss: 0.6142858266830444 \n",
      "     Training Step: 186 Training Loss: 0.6116209626197815 \n",
      "     Training Step: 187 Training Loss: 0.6186805963516235 \n",
      "     Training Step: 188 Training Loss: 0.6171461939811707 \n",
      "     Training Step: 189 Training Loss: 0.6113406419754028 \n",
      "     Training Step: 190 Training Loss: 0.6154060959815979 \n",
      "     Training Step: 191 Training Loss: 0.6154904961585999 \n",
      "     Training Step: 192 Training Loss: 0.6107886433601379 \n",
      "     Training Step: 193 Training Loss: 0.6124293208122253 \n",
      "     Training Step: 194 Training Loss: 0.6164262294769287 \n",
      "     Training Step: 195 Training Loss: 0.618432343006134 \n",
      "     Training Step: 196 Training Loss: 0.6137545108795166 \n",
      "     Training Step: 197 Training Loss: 0.6116366982460022 \n",
      "     Training Step: 198 Training Loss: 0.6123151779174805 \n",
      "     Training Step: 199 Training Loss: 0.6159746050834656 \n",
      "     Training Step: 200 Training Loss: 0.6136773228645325 \n",
      "     Training Step: 201 Training Loss: 0.6111804246902466 \n",
      "     Training Step: 202 Training Loss: 0.6147345900535583 \n",
      "     Training Step: 203 Training Loss: 0.6164108514785767 \n",
      "     Training Step: 204 Training Loss: 0.6147242784500122 \n",
      "     Training Step: 205 Training Loss: 0.6112001538276672 \n",
      "     Training Step: 206 Training Loss: 0.611634373664856 \n",
      "     Training Step: 207 Training Loss: 0.614709198474884 \n",
      "     Training Step: 208 Training Loss: 0.6105648279190063 \n",
      "     Training Step: 209 Training Loss: 0.6169503927230835 \n",
      "     Training Step: 210 Training Loss: 0.6176338791847229 \n",
      "     Training Step: 211 Training Loss: 0.6157822012901306 \n",
      "     Training Step: 212 Training Loss: 0.613254725933075 \n",
      "     Training Step: 213 Training Loss: 0.6141774654388428 \n",
      "     Training Step: 214 Training Loss: 0.6136343479156494 \n",
      "     Training Step: 215 Training Loss: 0.6132462620735168 \n",
      "     Training Step: 216 Training Loss: 0.6152130365371704 \n",
      "     Training Step: 217 Training Loss: 0.6141270399093628 \n",
      "     Training Step: 218 Training Loss: 0.614092230796814 \n",
      "     Training Step: 219 Training Loss: 0.6162626147270203 \n",
      "     Training Step: 220 Training Loss: 0.6144501566886902 \n",
      "     Training Step: 221 Training Loss: 0.6118665337562561 \n",
      "     Training Step: 222 Training Loss: 0.6141882538795471 \n",
      "     Training Step: 223 Training Loss: 0.611683189868927 \n",
      "     Training Step: 224 Training Loss: 0.6137790679931641 \n",
      "     Training Step: 225 Training Loss: 0.6117247939109802 \n",
      "     Training Step: 226 Training Loss: 0.6150301098823547 \n",
      "     Training Step: 227 Training Loss: 0.6147990226745605 \n",
      "     Training Step: 228 Training Loss: 0.6102347373962402 \n",
      "     Training Step: 229 Training Loss: 0.6158005595207214 \n",
      "     Training Step: 230 Training Loss: 0.6172074675559998 \n",
      "     Training Step: 231 Training Loss: 0.6167119741439819 \n",
      "     Training Step: 232 Training Loss: 0.6167805790901184 \n",
      "     Training Step: 233 Training Loss: 0.6148263216018677 \n",
      "     Training Step: 234 Training Loss: 0.611544668674469 \n",
      "     Training Step: 235 Training Loss: 0.614500105381012 \n",
      "     Training Step: 236 Training Loss: 0.6154614090919495 \n",
      "     Training Step: 237 Training Loss: 0.61163729429245 \n",
      "     Training Step: 238 Training Loss: 0.6118335127830505 \n",
      "     Training Step: 239 Training Loss: 0.6124992370605469 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6105458736419678 \n",
      "     Validation Step: 1 Validation Loss: 0.6141486763954163 \n",
      "     Validation Step: 2 Validation Loss: 0.6159180998802185 \n",
      "     Validation Step: 3 Validation Loss: 0.607542097568512 \n",
      "     Validation Step: 4 Validation Loss: 0.6170993447303772 \n",
      "     Validation Step: 5 Validation Loss: 0.6133435368537903 \n",
      "     Validation Step: 6 Validation Loss: 0.6147244572639465 \n",
      "     Validation Step: 7 Validation Loss: 0.6161213517189026 \n",
      "     Validation Step: 8 Validation Loss: 0.615282416343689 \n",
      "     Validation Step: 9 Validation Loss: 0.6105072498321533 \n",
      "     Validation Step: 10 Validation Loss: 0.6181799173355103 \n",
      "     Validation Step: 11 Validation Loss: 0.6116587519645691 \n",
      "     Validation Step: 12 Validation Loss: 0.6156831979751587 \n",
      "     Validation Step: 13 Validation Loss: 0.6185821890830994 \n",
      "     Validation Step: 14 Validation Loss: 0.6186208724975586 \n",
      "     Validation Step: 15 Validation Loss: 0.6178063750267029 \n",
      "     Validation Step: 16 Validation Loss: 0.618476152420044 \n",
      "     Validation Step: 17 Validation Loss: 0.6106471419334412 \n",
      "     Validation Step: 18 Validation Loss: 0.6173765659332275 \n",
      "     Validation Step: 19 Validation Loss: 0.6121594309806824 \n",
      "     Validation Step: 20 Validation Loss: 0.6101739406585693 \n",
      "     Validation Step: 21 Validation Loss: 0.6163554787635803 \n",
      "     Validation Step: 22 Validation Loss: 0.6130411624908447 \n",
      "     Validation Step: 23 Validation Loss: 0.6115777492523193 \n",
      "     Validation Step: 24 Validation Loss: 0.6148967146873474 \n",
      "     Validation Step: 25 Validation Loss: 0.6149278283119202 \n",
      "     Validation Step: 26 Validation Loss: 0.6145795583724976 \n",
      "     Validation Step: 27 Validation Loss: 0.6142039895057678 \n",
      "     Validation Step: 28 Validation Loss: 0.6128448247909546 \n",
      "     Validation Step: 29 Validation Loss: 0.6119065284729004 \n",
      "     Validation Step: 30 Validation Loss: 0.6154045462608337 \n",
      "     Validation Step: 31 Validation Loss: 0.6142948269844055 \n",
      "     Validation Step: 32 Validation Loss: 0.6137624382972717 \n",
      "     Validation Step: 33 Validation Loss: 0.6112101674079895 \n",
      "     Validation Step: 34 Validation Loss: 0.6146172881126404 \n",
      "     Validation Step: 35 Validation Loss: 0.6142703294754028 \n",
      "     Validation Step: 36 Validation Loss: 0.613651692867279 \n",
      "     Validation Step: 37 Validation Loss: 0.6101638078689575 \n",
      "     Validation Step: 38 Validation Loss: 0.6111962199211121 \n",
      "     Validation Step: 39 Validation Loss: 0.6157001256942749 \n",
      "     Validation Step: 40 Validation Loss: 0.6183891296386719 \n",
      "     Validation Step: 41 Validation Loss: 0.6176824569702148 \n",
      "     Validation Step: 42 Validation Loss: 0.6137262582778931 \n",
      "     Validation Step: 43 Validation Loss: 0.615140438079834 \n",
      "     Validation Step: 44 Validation Loss: 0.6101469993591309 \n",
      "Epoch: 26\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6118398308753967 \n",
      "     Training Step: 1 Training Loss: 0.6151374578475952 \n",
      "     Training Step: 2 Training Loss: 0.6177150011062622 \n",
      "     Training Step: 3 Training Loss: 0.6110454201698303 \n",
      "     Training Step: 4 Training Loss: 0.6142678260803223 \n",
      "     Training Step: 5 Training Loss: 0.613361120223999 \n",
      "     Training Step: 6 Training Loss: 0.6121718883514404 \n",
      "     Training Step: 7 Training Loss: 0.615648090839386 \n",
      "     Training Step: 8 Training Loss: 0.6102340817451477 \n",
      "     Training Step: 9 Training Loss: 0.6157952547073364 \n",
      "     Training Step: 10 Training Loss: 0.6169852614402771 \n",
      "     Training Step: 11 Training Loss: 0.6183464527130127 \n",
      "     Training Step: 12 Training Loss: 0.6142482757568359 \n",
      "     Training Step: 13 Training Loss: 0.6126959919929504 \n",
      "     Training Step: 14 Training Loss: 0.6171607971191406 \n",
      "     Training Step: 15 Training Loss: 0.611903190612793 \n",
      "     Training Step: 16 Training Loss: 0.6172372698783875 \n",
      "     Training Step: 17 Training Loss: 0.6128291487693787 \n",
      "     Training Step: 18 Training Loss: 0.6163172125816345 \n",
      "     Training Step: 19 Training Loss: 0.6148234605789185 \n",
      "     Training Step: 20 Training Loss: 0.6084304451942444 \n",
      "     Training Step: 21 Training Loss: 0.6136616468429565 \n",
      "     Training Step: 22 Training Loss: 0.6168468594551086 \n",
      "     Training Step: 23 Training Loss: 0.6155403256416321 \n",
      "     Training Step: 24 Training Loss: 0.6157381534576416 \n",
      "     Training Step: 25 Training Loss: 0.617535412311554 \n",
      "     Training Step: 26 Training Loss: 0.6168152093887329 \n",
      "     Training Step: 27 Training Loss: 0.6139075756072998 \n",
      "     Training Step: 28 Training Loss: 0.611811101436615 \n",
      "     Training Step: 29 Training Loss: 0.6162688732147217 \n",
      "     Training Step: 30 Training Loss: 0.6181628108024597 \n",
      "     Training Step: 31 Training Loss: 0.6115041375160217 \n",
      "     Training Step: 32 Training Loss: 0.6121110916137695 \n",
      "     Training Step: 33 Training Loss: 0.6195353269577026 \n",
      "     Training Step: 34 Training Loss: 0.6101101040840149 \n",
      "     Training Step: 35 Training Loss: 0.6152676939964294 \n",
      "     Training Step: 36 Training Loss: 0.6145079731941223 \n",
      "     Training Step: 37 Training Loss: 0.6145343780517578 \n",
      "     Training Step: 38 Training Loss: 0.6114745736122131 \n",
      "     Training Step: 39 Training Loss: 0.6146847009658813 \n",
      "     Training Step: 40 Training Loss: 0.6185309886932373 \n",
      "     Training Step: 41 Training Loss: 0.6174688935279846 \n",
      "     Training Step: 42 Training Loss: 0.6112902164459229 \n",
      "     Training Step: 43 Training Loss: 0.6185505986213684 \n",
      "     Training Step: 44 Training Loss: 0.6177181601524353 \n",
      "     Training Step: 45 Training Loss: 0.6167157888412476 \n",
      "     Training Step: 46 Training Loss: 0.6134193539619446 \n",
      "     Training Step: 47 Training Loss: 0.6202100515365601 \n",
      "     Training Step: 48 Training Loss: 0.6154031753540039 \n",
      "     Training Step: 49 Training Loss: 0.6182436347007751 \n",
      "     Training Step: 50 Training Loss: 0.6133719086647034 \n",
      "     Training Step: 51 Training Loss: 0.615768551826477 \n",
      "     Training Step: 52 Training Loss: 0.6129505038261414 \n",
      "     Training Step: 53 Training Loss: 0.6125568151473999 \n",
      "     Training Step: 54 Training Loss: 0.6177999973297119 \n",
      "     Training Step: 55 Training Loss: 0.6144090890884399 \n",
      "     Training Step: 56 Training Loss: 0.6169516444206238 \n",
      "     Training Step: 57 Training Loss: 0.611515462398529 \n",
      "     Training Step: 58 Training Loss: 0.6151235699653625 \n",
      "     Training Step: 59 Training Loss: 0.6133262515068054 \n",
      "     Training Step: 60 Training Loss: 0.612284243106842 \n",
      "     Training Step: 61 Training Loss: 0.6149609684944153 \n",
      "     Training Step: 62 Training Loss: 0.6160767674446106 \n",
      "     Training Step: 63 Training Loss: 0.611852765083313 \n",
      "     Training Step: 64 Training Loss: 0.6116049885749817 \n",
      "     Training Step: 65 Training Loss: 0.6142721772193909 \n",
      "     Training Step: 66 Training Loss: 0.6164718270301819 \n",
      "     Training Step: 67 Training Loss: 0.6172335743904114 \n",
      "     Training Step: 68 Training Loss: 0.617814838886261 \n",
      "     Training Step: 69 Training Loss: 0.6144234538078308 \n",
      "     Training Step: 70 Training Loss: 0.6124135851860046 \n",
      "     Training Step: 71 Training Loss: 0.616133451461792 \n",
      "     Training Step: 72 Training Loss: 0.6117101311683655 \n",
      "     Training Step: 73 Training Loss: 0.6145545244216919 \n",
      "     Training Step: 74 Training Loss: 0.6169586181640625 \n",
      "     Training Step: 75 Training Loss: 0.6143971681594849 \n",
      "     Training Step: 76 Training Loss: 0.6137863397598267 \n",
      "     Training Step: 77 Training Loss: 0.6153382658958435 \n",
      "     Training Step: 78 Training Loss: 0.6153274178504944 \n",
      "     Training Step: 79 Training Loss: 0.6135535836219788 \n",
      "     Training Step: 80 Training Loss: 0.6127488613128662 \n",
      "     Training Step: 81 Training Loss: 0.6115077137947083 \n",
      "     Training Step: 82 Training Loss: 0.6162872910499573 \n",
      "     Training Step: 83 Training Loss: 0.6160945296287537 \n",
      "     Training Step: 84 Training Loss: 0.6144585609436035 \n",
      "     Training Step: 85 Training Loss: 0.6098732948303223 \n",
      "     Training Step: 86 Training Loss: 0.610712468624115 \n",
      "     Training Step: 87 Training Loss: 0.6123297214508057 \n",
      "     Training Step: 88 Training Loss: 0.6121975183486938 \n",
      "     Training Step: 89 Training Loss: 0.6105008125305176 \n",
      "     Training Step: 90 Training Loss: 0.6133329272270203 \n",
      "     Training Step: 91 Training Loss: 0.6153020858764648 \n",
      "     Training Step: 92 Training Loss: 0.6106115579605103 \n",
      "     Training Step: 93 Training Loss: 0.6152228713035583 \n",
      "     Training Step: 94 Training Loss: 0.6135223507881165 \n",
      "     Training Step: 95 Training Loss: 0.6116648316383362 \n",
      "     Training Step: 96 Training Loss: 0.6107679009437561 \n",
      "     Training Step: 97 Training Loss: 0.6103945970535278 \n",
      "     Training Step: 98 Training Loss: 0.6123212575912476 \n",
      "     Training Step: 99 Training Loss: 0.6130662560462952 \n",
      "     Training Step: 100 Training Loss: 0.616603672504425 \n",
      "     Training Step: 101 Training Loss: 0.6135391592979431 \n",
      "     Training Step: 102 Training Loss: 0.6129569411277771 \n",
      "     Training Step: 103 Training Loss: 0.619953989982605 \n",
      "     Training Step: 104 Training Loss: 0.614248514175415 \n",
      "     Training Step: 105 Training Loss: 0.6103047132492065 \n",
      "     Training Step: 106 Training Loss: 0.6153154373168945 \n",
      "     Training Step: 107 Training Loss: 0.6150861978530884 \n",
      "     Training Step: 108 Training Loss: 0.6169622540473938 \n",
      "     Training Step: 109 Training Loss: 0.6133014559745789 \n",
      "     Training Step: 110 Training Loss: 0.6154027581214905 \n",
      "     Training Step: 111 Training Loss: 0.6167241930961609 \n",
      "     Training Step: 112 Training Loss: 0.6106178164482117 \n",
      "     Training Step: 113 Training Loss: 0.6153942346572876 \n",
      "     Training Step: 114 Training Loss: 0.618473470211029 \n",
      "     Training Step: 115 Training Loss: 0.6136981844902039 \n",
      "     Training Step: 116 Training Loss: 0.6106147170066833 \n",
      "     Training Step: 117 Training Loss: 0.6123965382575989 \n",
      "     Training Step: 118 Training Loss: 0.6111496090888977 \n",
      "     Training Step: 119 Training Loss: 0.6149751543998718 \n",
      "     Training Step: 120 Training Loss: 0.6181446313858032 \n",
      "     Training Step: 121 Training Loss: 0.6101371049880981 \n",
      "     Training Step: 122 Training Loss: 0.6117051839828491 \n",
      "     Training Step: 123 Training Loss: 0.6125593185424805 \n",
      "     Training Step: 124 Training Loss: 0.6151117086410522 \n",
      "     Training Step: 125 Training Loss: 0.6164876818656921 \n",
      "     Training Step: 126 Training Loss: 0.6129210591316223 \n",
      "     Training Step: 127 Training Loss: 0.611577033996582 \n",
      "     Training Step: 128 Training Loss: 0.6125324368476868 \n",
      "     Training Step: 129 Training Loss: 0.6121935248374939 \n",
      "     Training Step: 130 Training Loss: 0.6107386350631714 \n",
      "     Training Step: 131 Training Loss: 0.6132386326789856 \n",
      "     Training Step: 132 Training Loss: 0.6147780418395996 \n",
      "     Training Step: 133 Training Loss: 0.6154868006706238 \n",
      "     Training Step: 134 Training Loss: 0.6154993772506714 \n",
      "     Training Step: 135 Training Loss: 0.6138191223144531 \n",
      "     Training Step: 136 Training Loss: 0.6132610440254211 \n",
      "     Training Step: 137 Training Loss: 0.6096154451370239 \n",
      "     Training Step: 138 Training Loss: 0.618077278137207 \n",
      "     Training Step: 139 Training Loss: 0.6185534596443176 \n",
      "     Training Step: 140 Training Loss: 0.6181342601776123 \n",
      "     Training Step: 141 Training Loss: 0.6142551898956299 \n",
      "     Training Step: 142 Training Loss: 0.6120607852935791 \n",
      "     Training Step: 143 Training Loss: 0.6147661209106445 \n",
      "     Training Step: 144 Training Loss: 0.6182327270507812 \n",
      "     Training Step: 145 Training Loss: 0.6147571206092834 \n",
      "     Training Step: 146 Training Loss: 0.6146465539932251 \n",
      "     Training Step: 147 Training Loss: 0.6139253377914429 \n",
      "     Training Step: 148 Training Loss: 0.617802083492279 \n",
      "     Training Step: 149 Training Loss: 0.6144688725471497 \n",
      "     Training Step: 150 Training Loss: 0.6148948073387146 \n",
      "     Training Step: 151 Training Loss: 0.613412618637085 \n",
      "     Training Step: 152 Training Loss: 0.6127973198890686 \n",
      "     Training Step: 153 Training Loss: 0.6146833300590515 \n",
      "     Training Step: 154 Training Loss: 0.6126616597175598 \n",
      "     Training Step: 155 Training Loss: 0.6118443608283997 \n",
      "     Training Step: 156 Training Loss: 0.6135756373405457 \n",
      "     Training Step: 157 Training Loss: 0.6132572889328003 \n",
      "     Training Step: 158 Training Loss: 0.6115447282791138 \n",
      "     Training Step: 159 Training Loss: 0.6097776889801025 \n",
      "     Training Step: 160 Training Loss: 0.6143875122070312 \n",
      "     Training Step: 161 Training Loss: 0.6167164444923401 \n",
      "     Training Step: 162 Training Loss: 0.6112160086631775 \n",
      "     Training Step: 163 Training Loss: 0.6157927513122559 \n",
      "     Training Step: 164 Training Loss: 0.6188713312149048 \n",
      "     Training Step: 165 Training Loss: 0.6153054237365723 \n",
      "     Training Step: 166 Training Loss: 0.6094394326210022 \n",
      "     Training Step: 167 Training Loss: 0.6146590709686279 \n",
      "     Training Step: 168 Training Loss: 0.6124235391616821 \n",
      "     Training Step: 169 Training Loss: 0.6135089993476868 \n",
      "     Training Step: 170 Training Loss: 0.6101764440536499 \n",
      "     Training Step: 171 Training Loss: 0.619020402431488 \n",
      "     Training Step: 172 Training Loss: 0.6178107857704163 \n",
      "     Training Step: 173 Training Loss: 0.6164153814315796 \n",
      "     Training Step: 174 Training Loss: 0.6127185225486755 \n",
      "     Training Step: 175 Training Loss: 0.6107891201972961 \n",
      "     Training Step: 176 Training Loss: 0.6154738068580627 \n",
      "     Training Step: 177 Training Loss: 0.6137107610702515 \n",
      "     Training Step: 178 Training Loss: 0.6123291254043579 \n",
      "     Training Step: 179 Training Loss: 0.612923800945282 \n",
      "     Training Step: 180 Training Loss: 0.6100183129310608 \n",
      "     Training Step: 181 Training Loss: 0.6106817722320557 \n",
      "     Training Step: 182 Training Loss: 0.6157577633857727 \n",
      "     Training Step: 183 Training Loss: 0.6139548420906067 \n",
      "     Training Step: 184 Training Loss: 0.6147386431694031 \n",
      "     Training Step: 185 Training Loss: 0.6186857223510742 \n",
      "     Training Step: 186 Training Loss: 0.6147914528846741 \n",
      "     Training Step: 187 Training Loss: 0.6157979369163513 \n",
      "     Training Step: 188 Training Loss: 0.6162079572677612 \n",
      "     Training Step: 189 Training Loss: 0.6167272925376892 \n",
      "     Training Step: 190 Training Loss: 0.6154194474220276 \n",
      "     Training Step: 191 Training Loss: 0.6129022240638733 \n",
      "     Training Step: 192 Training Loss: 0.6106345057487488 \n",
      "     Training Step: 193 Training Loss: 0.6129525899887085 \n",
      "     Training Step: 194 Training Loss: 0.610900342464447 \n",
      "     Training Step: 195 Training Loss: 0.6170252561569214 \n",
      "     Training Step: 196 Training Loss: 0.6114225387573242 \n",
      "     Training Step: 197 Training Loss: 0.6171829104423523 \n",
      "     Training Step: 198 Training Loss: 0.6117730736732483 \n",
      "     Training Step: 199 Training Loss: 0.6124779582023621 \n",
      "     Training Step: 200 Training Loss: 0.611516535282135 \n",
      "     Training Step: 201 Training Loss: 0.6167894005775452 \n",
      "     Training Step: 202 Training Loss: 0.6147412061691284 \n",
      "     Training Step: 203 Training Loss: 0.6136261224746704 \n",
      "     Training Step: 204 Training Loss: 0.6147276163101196 \n",
      "     Training Step: 205 Training Loss: 0.6141141653060913 \n",
      "     Training Step: 206 Training Loss: 0.6119324564933777 \n",
      "     Training Step: 207 Training Loss: 0.6095588207244873 \n",
      "     Training Step: 208 Training Loss: 0.6121801733970642 \n",
      "     Training Step: 209 Training Loss: 0.6140837073326111 \n",
      "     Training Step: 210 Training Loss: 0.6132067441940308 \n",
      "     Training Step: 211 Training Loss: 0.6096811890602112 \n",
      "     Training Step: 212 Training Loss: 0.613184928894043 \n",
      "     Training Step: 213 Training Loss: 0.6146748661994934 \n",
      "     Training Step: 214 Training Loss: 0.6155966520309448 \n",
      "     Training Step: 215 Training Loss: 0.6196935176849365 \n",
      "     Training Step: 216 Training Loss: 0.6148472428321838 \n",
      "     Training Step: 217 Training Loss: 0.6166791915893555 \n",
      "     Training Step: 218 Training Loss: 0.6109842658042908 \n",
      "     Training Step: 219 Training Loss: 0.615882396697998 \n",
      "     Training Step: 220 Training Loss: 0.6143032312393188 \n",
      "     Training Step: 221 Training Loss: 0.6168785691261292 \n",
      "     Training Step: 222 Training Loss: 0.6140828728675842 \n",
      "     Training Step: 223 Training Loss: 0.6143261194229126 \n",
      "     Training Step: 224 Training Loss: 0.613860011100769 \n",
      "     Training Step: 225 Training Loss: 0.6155810356140137 \n",
      "     Training Step: 226 Training Loss: 0.6209970712661743 \n",
      "     Training Step: 227 Training Loss: 0.6141046285629272 \n",
      "     Training Step: 228 Training Loss: 0.6197459101676941 \n",
      "     Training Step: 229 Training Loss: 0.6178267002105713 \n",
      "     Training Step: 230 Training Loss: 0.612159252166748 \n",
      "     Training Step: 231 Training Loss: 0.6116899251937866 \n",
      "     Training Step: 232 Training Loss: 0.6115906238555908 \n",
      "     Training Step: 233 Training Loss: 0.6156986355781555 \n",
      "     Training Step: 234 Training Loss: 0.6168540120124817 \n",
      "     Training Step: 235 Training Loss: 0.6146902441978455 \n",
      "     Training Step: 236 Training Loss: 0.6171565651893616 \n",
      "     Training Step: 237 Training Loss: 0.6140472292900085 \n",
      "     Training Step: 238 Training Loss: 0.6132037043571472 \n",
      "     Training Step: 239 Training Loss: 0.6119415760040283 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6153601408004761 \n",
      "     Validation Step: 1 Validation Loss: 0.614613950252533 \n",
      "     Validation Step: 2 Validation Loss: 0.6185518503189087 \n",
      "     Validation Step: 3 Validation Loss: 0.610676646232605 \n",
      "     Validation Step: 4 Validation Loss: 0.6184152960777283 \n",
      "     Validation Step: 5 Validation Loss: 0.6137205362319946 \n",
      "     Validation Step: 6 Validation Loss: 0.6121565103530884 \n",
      "     Validation Step: 7 Validation Loss: 0.6130172610282898 \n",
      "     Validation Step: 8 Validation Loss: 0.6170875430107117 \n",
      "     Validation Step: 9 Validation Loss: 0.6152664422988892 \n",
      "     Validation Step: 10 Validation Loss: 0.6149413585662842 \n",
      "     Validation Step: 11 Validation Loss: 0.6185704469680786 \n",
      "     Validation Step: 12 Validation Loss: 0.6133272647857666 \n",
      "     Validation Step: 13 Validation Loss: 0.6173648834228516 \n",
      "     Validation Step: 14 Validation Loss: 0.6112048029899597 \n",
      "     Validation Step: 15 Validation Loss: 0.6137024164199829 \n",
      "     Validation Step: 16 Validation Loss: 0.6156419515609741 \n",
      "     Validation Step: 17 Validation Loss: 0.614162802696228 \n",
      "     Validation Step: 18 Validation Loss: 0.6128523945808411 \n",
      "     Validation Step: 19 Validation Loss: 0.6142931580543518 \n",
      "     Validation Step: 20 Validation Loss: 0.6136611104011536 \n",
      "     Validation Step: 21 Validation Loss: 0.6075601577758789 \n",
      "     Validation Step: 22 Validation Loss: 0.6116743087768555 \n",
      "     Validation Step: 23 Validation Loss: 0.6176623702049255 \n",
      "     Validation Step: 24 Validation Loss: 0.611591637134552 \n",
      "     Validation Step: 25 Validation Loss: 0.6142419576644897 \n",
      "     Validation Step: 26 Validation Loss: 0.6111953854560852 \n",
      "     Validation Step: 27 Validation Loss: 0.6183212995529175 \n",
      "     Validation Step: 28 Validation Loss: 0.6105449795722961 \n",
      "     Validation Step: 29 Validation Loss: 0.6141552925109863 \n",
      "     Validation Step: 30 Validation Loss: 0.6101487278938293 \n",
      "     Validation Step: 31 Validation Loss: 0.6146854162216187 \n",
      "     Validation Step: 32 Validation Loss: 0.6119180917739868 \n",
      "     Validation Step: 33 Validation Loss: 0.610181450843811 \n",
      "     Validation Step: 34 Validation Loss: 0.6163266897201538 \n",
      "     Validation Step: 35 Validation Loss: 0.6102088689804077 \n",
      "     Validation Step: 36 Validation Loss: 0.6160790324211121 \n",
      "     Validation Step: 37 Validation Loss: 0.6145732402801514 \n",
      "     Validation Step: 38 Validation Loss: 0.6148814558982849 \n",
      "     Validation Step: 39 Validation Loss: 0.6150906085968018 \n",
      "     Validation Step: 40 Validation Loss: 0.6158648133277893 \n",
      "     Validation Step: 41 Validation Loss: 0.6177625060081482 \n",
      "     Validation Step: 42 Validation Loss: 0.615668535232544 \n",
      "     Validation Step: 43 Validation Loss: 0.6105218529701233 \n",
      "     Validation Step: 44 Validation Loss: 0.6181369423866272 \n",
      "Epoch: 27\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6122604608535767 \n",
      "     Training Step: 1 Training Loss: 0.6186339259147644 \n",
      "     Training Step: 2 Training Loss: 0.6109635829925537 \n",
      "     Training Step: 3 Training Loss: 0.6136907339096069 \n",
      "     Training Step: 4 Training Loss: 0.6122801303863525 \n",
      "     Training Step: 5 Training Loss: 0.6128129363059998 \n",
      "     Training Step: 6 Training Loss: 0.6169725060462952 \n",
      "     Training Step: 7 Training Loss: 0.6155739426612854 \n",
      "     Training Step: 8 Training Loss: 0.6145033240318298 \n",
      "     Training Step: 9 Training Loss: 0.6154434084892273 \n",
      "     Training Step: 10 Training Loss: 0.6135441064834595 \n",
      "     Training Step: 11 Training Loss: 0.6118313074111938 \n",
      "     Training Step: 12 Training Loss: 0.6157966256141663 \n",
      "     Training Step: 13 Training Loss: 0.6152089834213257 \n",
      "     Training Step: 14 Training Loss: 0.613498330116272 \n",
      "     Training Step: 15 Training Loss: 0.6148937940597534 \n",
      "     Training Step: 16 Training Loss: 0.6171578764915466 \n",
      "     Training Step: 17 Training Loss: 0.6178131103515625 \n",
      "     Training Step: 18 Training Loss: 0.6152056455612183 \n",
      "     Training Step: 19 Training Loss: 0.6163830757141113 \n",
      "     Training Step: 20 Training Loss: 0.6152153611183167 \n",
      "     Training Step: 21 Training Loss: 0.6180456876754761 \n",
      "     Training Step: 22 Training Loss: 0.6198976039886475 \n",
      "     Training Step: 23 Training Loss: 0.6102729439735413 \n",
      "     Training Step: 24 Training Loss: 0.6116227507591248 \n",
      "     Training Step: 25 Training Loss: 0.6178337335586548 \n",
      "     Training Step: 26 Training Loss: 0.6164647936820984 \n",
      "     Training Step: 27 Training Loss: 0.6124051809310913 \n",
      "     Training Step: 28 Training Loss: 0.6133832931518555 \n",
      "     Training Step: 29 Training Loss: 0.6181598901748657 \n",
      "     Training Step: 30 Training Loss: 0.6149464845657349 \n",
      "     Training Step: 31 Training Loss: 0.6158905029296875 \n",
      "     Training Step: 32 Training Loss: 0.6127669811248779 \n",
      "     Training Step: 33 Training Loss: 0.6133313179016113 \n",
      "     Training Step: 34 Training Loss: 0.6104135513305664 \n",
      "     Training Step: 35 Training Loss: 0.6169067621231079 \n",
      "     Training Step: 36 Training Loss: 0.6197845935821533 \n",
      "     Training Step: 37 Training Loss: 0.6104723811149597 \n",
      "     Training Step: 38 Training Loss: 0.6146305799484253 \n",
      "     Training Step: 39 Training Loss: 0.6136780381202698 \n",
      "     Training Step: 40 Training Loss: 0.6180784702301025 \n",
      "     Training Step: 41 Training Loss: 0.612795352935791 \n",
      "     Training Step: 42 Training Loss: 0.6167786717414856 \n",
      "     Training Step: 43 Training Loss: 0.6197688579559326 \n",
      "     Training Step: 44 Training Loss: 0.61844402551651 \n",
      "     Training Step: 45 Training Loss: 0.610872209072113 \n",
      "     Training Step: 46 Training Loss: 0.6117388010025024 \n",
      "     Training Step: 47 Training Loss: 0.6161423921585083 \n",
      "     Training Step: 48 Training Loss: 0.61473149061203 \n",
      "     Training Step: 49 Training Loss: 0.6167832016944885 \n",
      "     Training Step: 50 Training Loss: 0.6148256063461304 \n",
      "     Training Step: 51 Training Loss: 0.6142705082893372 \n",
      "     Training Step: 52 Training Loss: 0.6133456230163574 \n",
      "     Training Step: 53 Training Loss: 0.6112789511680603 \n",
      "     Training Step: 54 Training Loss: 0.6167131662368774 \n",
      "     Training Step: 55 Training Loss: 0.61599200963974 \n",
      "     Training Step: 56 Training Loss: 0.6136594414710999 \n",
      "     Training Step: 57 Training Loss: 0.6154534816741943 \n",
      "     Training Step: 58 Training Loss: 0.6124047636985779 \n",
      "     Training Step: 59 Training Loss: 0.6152874827384949 \n",
      "     Training Step: 60 Training Loss: 0.6112334728240967 \n",
      "     Training Step: 61 Training Loss: 0.6136059761047363 \n",
      "     Training Step: 62 Training Loss: 0.6105467677116394 \n",
      "     Training Step: 63 Training Loss: 0.6129535436630249 \n",
      "     Training Step: 64 Training Loss: 0.6114663481712341 \n",
      "     Training Step: 65 Training Loss: 0.611461341381073 \n",
      "     Training Step: 66 Training Loss: 0.6116833090782166 \n",
      "     Training Step: 67 Training Loss: 0.6114950776100159 \n",
      "     Training Step: 68 Training Loss: 0.6144142746925354 \n",
      "     Training Step: 69 Training Loss: 0.6125544905662537 \n",
      "     Training Step: 70 Training Loss: 0.6141014099121094 \n",
      "     Training Step: 71 Training Loss: 0.6156963109970093 \n",
      "     Training Step: 72 Training Loss: 0.6153753399848938 \n",
      "     Training Step: 73 Training Loss: 0.6140817999839783 \n",
      "     Training Step: 74 Training Loss: 0.6176167130470276 \n",
      "     Training Step: 75 Training Loss: 0.6118943095207214 \n",
      "     Training Step: 76 Training Loss: 0.6209338307380676 \n",
      "     Training Step: 77 Training Loss: 0.612478494644165 \n",
      "     Training Step: 78 Training Loss: 0.6094061732292175 \n",
      "     Training Step: 79 Training Loss: 0.6153779625892639 \n",
      "     Training Step: 80 Training Loss: 0.6156200766563416 \n",
      "     Training Step: 81 Training Loss: 0.6169092655181885 \n",
      "     Training Step: 82 Training Loss: 0.6125142574310303 \n",
      "     Training Step: 83 Training Loss: 0.6135622262954712 \n",
      "     Training Step: 84 Training Loss: 0.6129586100578308 \n",
      "     Training Step: 85 Training Loss: 0.6131557822227478 \n",
      "     Training Step: 86 Training Loss: 0.6115455627441406 \n",
      "     Training Step: 87 Training Loss: 0.6173046827316284 \n",
      "     Training Step: 88 Training Loss: 0.6153587698936462 \n",
      "     Training Step: 89 Training Loss: 0.6188819408416748 \n",
      "     Training Step: 90 Training Loss: 0.6185556650161743 \n",
      "     Training Step: 91 Training Loss: 0.6141207814216614 \n",
      "     Training Step: 92 Training Loss: 0.6132588982582092 \n",
      "     Training Step: 93 Training Loss: 0.6168038845062256 \n",
      "     Training Step: 94 Training Loss: 0.6182951927185059 \n",
      "     Training Step: 95 Training Loss: 0.6098633408546448 \n",
      "     Training Step: 96 Training Loss: 0.6106361150741577 \n",
      "     Training Step: 97 Training Loss: 0.610798180103302 \n",
      "     Training Step: 98 Training Loss: 0.6155909895896912 \n",
      "     Training Step: 99 Training Loss: 0.6147197484970093 \n",
      "     Training Step: 100 Training Loss: 0.6167570352554321 \n",
      "     Training Step: 101 Training Loss: 0.6110681295394897 \n",
      "     Training Step: 102 Training Loss: 0.6168363094329834 \n",
      "     Training Step: 103 Training Loss: 0.6162746548652649 \n",
      "     Training Step: 104 Training Loss: 0.6148337721824646 \n",
      "     Training Step: 105 Training Loss: 0.6101506352424622 \n",
      "     Training Step: 106 Training Loss: 0.6130549311637878 \n",
      "     Training Step: 107 Training Loss: 0.6140130162239075 \n",
      "     Training Step: 108 Training Loss: 0.6147433519363403 \n",
      "     Training Step: 109 Training Loss: 0.6185561418533325 \n",
      "     Training Step: 110 Training Loss: 0.609564483165741 \n",
      "     Training Step: 111 Training Loss: 0.6180696487426758 \n",
      "     Training Step: 112 Training Loss: 0.6133514046669006 \n",
      "     Training Step: 113 Training Loss: 0.6138781905174255 \n",
      "     Training Step: 114 Training Loss: 0.6116269826889038 \n",
      "     Training Step: 115 Training Loss: 0.6139547228813171 \n",
      "     Training Step: 116 Training Loss: 0.616847574710846 \n",
      "     Training Step: 117 Training Loss: 0.6123211979866028 \n",
      "     Training Step: 118 Training Loss: 0.6102442145347595 \n",
      "     Training Step: 119 Training Loss: 0.6116874814033508 \n",
      "     Training Step: 120 Training Loss: 0.6131771802902222 \n",
      "     Training Step: 121 Training Loss: 0.6132919192314148 \n",
      "     Training Step: 122 Training Loss: 0.6140640377998352 \n",
      "     Training Step: 123 Training Loss: 0.6178390383720398 \n",
      "     Training Step: 124 Training Loss: 0.6144561767578125 \n",
      "     Training Step: 125 Training Loss: 0.6138153672218323 \n",
      "     Training Step: 126 Training Loss: 0.6147536039352417 \n",
      "     Training Step: 127 Training Loss: 0.6166372895240784 \n",
      "     Training Step: 128 Training Loss: 0.6182388067245483 \n",
      "     Training Step: 129 Training Loss: 0.6107438206672668 \n",
      "     Training Step: 130 Training Loss: 0.612260103225708 \n",
      "     Training Step: 131 Training Loss: 0.616945207118988 \n",
      "     Training Step: 132 Training Loss: 0.6140671968460083 \n",
      "     Training Step: 133 Training Loss: 0.612097978591919 \n",
      "     Training Step: 134 Training Loss: 0.6147411465644836 \n",
      "     Training Step: 135 Training Loss: 0.6145116686820984 \n",
      "     Training Step: 136 Training Loss: 0.6116131544113159 \n",
      "     Training Step: 137 Training Loss: 0.6146869659423828 \n",
      "     Training Step: 138 Training Loss: 0.6119415163993835 \n",
      "     Training Step: 139 Training Loss: 0.6101698875427246 \n",
      "     Training Step: 140 Training Loss: 0.6155439019203186 \n",
      "     Training Step: 141 Training Loss: 0.6178297996520996 \n",
      "     Training Step: 142 Training Loss: 0.6132360100746155 \n",
      "     Training Step: 143 Training Loss: 0.6107931137084961 \n",
      "     Training Step: 144 Training Loss: 0.6166508793830872 \n",
      "     Training Step: 145 Training Loss: 0.6121399402618408 \n",
      "     Training Step: 146 Training Loss: 0.611732542514801 \n",
      "     Training Step: 147 Training Loss: 0.6185041666030884 \n",
      "     Training Step: 148 Training Loss: 0.6138072609901428 \n",
      "     Training Step: 149 Training Loss: 0.6153104305267334 \n",
      "     Training Step: 150 Training Loss: 0.6167431473731995 \n",
      "     Training Step: 151 Training Loss: 0.6112651228904724 \n",
      "     Training Step: 152 Training Loss: 0.6122827529907227 \n",
      "     Training Step: 153 Training Loss: 0.612557053565979 \n",
      "     Training Step: 154 Training Loss: 0.6203947067260742 \n",
      "     Training Step: 155 Training Loss: 0.6128637194633484 \n",
      "     Training Step: 156 Training Loss: 0.6083160638809204 \n",
      "     Training Step: 157 Training Loss: 0.6125265955924988 \n",
      "     Training Step: 158 Training Loss: 0.6128991842269897 \n",
      "     Training Step: 159 Training Loss: 0.6114347577095032 \n",
      "     Training Step: 160 Training Loss: 0.6114168763160706 \n",
      "     Training Step: 161 Training Loss: 0.6147598624229431 \n",
      "     Training Step: 162 Training Loss: 0.6150112748146057 \n",
      "     Training Step: 163 Training Loss: 0.6142439842224121 \n",
      "     Training Step: 164 Training Loss: 0.6148309707641602 \n",
      "     Training Step: 165 Training Loss: 0.6123276352882385 \n",
      "     Training Step: 166 Training Loss: 0.6135926842689514 \n",
      "     Training Step: 167 Training Loss: 0.6135070323944092 \n",
      "     Training Step: 168 Training Loss: 0.610645055770874 \n",
      "     Training Step: 169 Training Loss: 0.6122309565544128 \n",
      "     Training Step: 170 Training Loss: 0.6154241561889648 \n",
      "     Training Step: 171 Training Loss: 0.6163733005523682 \n",
      "     Training Step: 172 Training Loss: 0.615568995475769 \n",
      "     Training Step: 173 Training Loss: 0.615483820438385 \n",
      "     Training Step: 174 Training Loss: 0.6155233383178711 \n",
      "     Training Step: 175 Training Loss: 0.6144265532493591 \n",
      "     Training Step: 176 Training Loss: 0.6103282570838928 \n",
      "     Training Step: 177 Training Loss: 0.6129392981529236 \n",
      "     Training Step: 178 Training Loss: 0.6173598170280457 \n",
      "     Training Step: 179 Training Loss: 0.6146312355995178 \n",
      "     Training Step: 180 Training Loss: 0.6117581725120544 \n",
      "     Training Step: 181 Training Loss: 0.6118924617767334 \n",
      "     Training Step: 182 Training Loss: 0.6142939329147339 \n",
      "     Training Step: 183 Training Loss: 0.6145275831222534 \n",
      "     Training Step: 184 Training Loss: 0.6098620891571045 \n",
      "     Training Step: 185 Training Loss: 0.614697277545929 \n",
      "     Training Step: 186 Training Loss: 0.6128627061843872 \n",
      "     Training Step: 187 Training Loss: 0.6107513308525085 \n",
      "     Training Step: 188 Training Loss: 0.6118666529655457 \n",
      "     Training Step: 189 Training Loss: 0.613243043422699 \n",
      "     Training Step: 190 Training Loss: 0.6160582304000854 \n",
      "     Training Step: 191 Training Loss: 0.6160627603530884 \n",
      "     Training Step: 192 Training Loss: 0.6099493503570557 \n",
      "     Training Step: 193 Training Loss: 0.6133161783218384 \n",
      "     Training Step: 194 Training Loss: 0.6143773198127747 \n",
      "     Training Step: 195 Training Loss: 0.6133657693862915 \n",
      "     Training Step: 196 Training Loss: 0.6122041940689087 \n",
      "     Training Step: 197 Training Loss: 0.6163445115089417 \n",
      "     Training Step: 198 Training Loss: 0.617426335811615 \n",
      "     Training Step: 199 Training Loss: 0.6144821643829346 \n",
      "     Training Step: 200 Training Loss: 0.6186263561248779 \n",
      "     Training Step: 201 Training Loss: 0.6158523559570312 \n",
      "     Training Step: 202 Training Loss: 0.6153410077095032 \n",
      "     Training Step: 203 Training Loss: 0.6177022457122803 \n",
      "     Training Step: 204 Training Loss: 0.6116983294487 \n",
      "     Training Step: 205 Training Loss: 0.6196523904800415 \n",
      "     Training Step: 206 Training Loss: 0.6158205270767212 \n",
      "     Training Step: 207 Training Loss: 0.611876368522644 \n",
      "     Training Step: 208 Training Loss: 0.6106102466583252 \n",
      "     Training Step: 209 Training Loss: 0.6147332787513733 \n",
      "     Training Step: 210 Training Loss: 0.6157970428466797 \n",
      "     Training Step: 211 Training Loss: 0.6169012784957886 \n",
      "     Training Step: 212 Training Loss: 0.6145468354225159 \n",
      "     Training Step: 213 Training Loss: 0.6149716973304749 \n",
      "     Training Step: 214 Training Loss: 0.6126164793968201 \n",
      "     Training Step: 215 Training Loss: 0.6165305376052856 \n",
      "     Training Step: 216 Training Loss: 0.6175075173377991 \n",
      "     Training Step: 217 Training Loss: 0.6141765713691711 \n",
      "     Training Step: 218 Training Loss: 0.6124058365821838 \n",
      "     Training Step: 219 Training Loss: 0.611906111240387 \n",
      "     Training Step: 220 Training Loss: 0.6126893758773804 \n",
      "     Training Step: 221 Training Loss: 0.6094866394996643 \n",
      "     Training Step: 222 Training Loss: 0.6157795786857605 \n",
      "     Training Step: 223 Training Loss: 0.614257276058197 \n",
      "     Training Step: 224 Training Loss: 0.617274820804596 \n",
      "     Training Step: 225 Training Loss: 0.6138502955436707 \n",
      "     Training Step: 226 Training Loss: 0.6152448654174805 \n",
      "     Training Step: 227 Training Loss: 0.616479754447937 \n",
      "     Training Step: 228 Training Loss: 0.6171653270721436 \n",
      "     Training Step: 229 Training Loss: 0.611523449420929 \n",
      "     Training Step: 230 Training Loss: 0.6142312288284302 \n",
      "     Training Step: 231 Training Loss: 0.6101350784301758 \n",
      "     Training Step: 232 Training Loss: 0.6148902773857117 \n",
      "     Training Step: 233 Training Loss: 0.6179071068763733 \n",
      "     Training Step: 234 Training Loss: 0.6189611554145813 \n",
      "     Training Step: 235 Training Loss: 0.6108444929122925 \n",
      "     Training Step: 236 Training Loss: 0.6158666014671326 \n",
      "     Training Step: 237 Training Loss: 0.6148325204849243 \n",
      "     Training Step: 238 Training Loss: 0.6131198406219482 \n",
      "     Training Step: 239 Training Loss: 0.6151435375213623 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6186456680297852 \n",
      "     Validation Step: 1 Validation Loss: 0.6075598001480103 \n",
      "     Validation Step: 2 Validation Loss: 0.6107137203216553 \n",
      "     Validation Step: 3 Validation Loss: 0.6178243160247803 \n",
      "     Validation Step: 4 Validation Loss: 0.6149185299873352 \n",
      "     Validation Step: 5 Validation Loss: 0.6137175559997559 \n",
      "     Validation Step: 6 Validation Loss: 0.6147288084030151 \n",
      "     Validation Step: 7 Validation Loss: 0.6161131262779236 \n",
      "     Validation Step: 8 Validation Loss: 0.6157205104827881 \n",
      "     Validation Step: 9 Validation Loss: 0.6141937375068665 \n",
      "     Validation Step: 10 Validation Loss: 0.6119338870048523 \n",
      "     Validation Step: 11 Validation Loss: 0.615327000617981 \n",
      "     Validation Step: 12 Validation Loss: 0.6151338219642639 \n",
      "     Validation Step: 13 Validation Loss: 0.6137287616729736 \n",
      "     Validation Step: 14 Validation Loss: 0.6143711805343628 \n",
      "     Validation Step: 15 Validation Loss: 0.617759108543396 \n",
      "     Validation Step: 16 Validation Loss: 0.6112059354782104 \n",
      "     Validation Step: 17 Validation Loss: 0.6128890514373779 \n",
      "     Validation Step: 18 Validation Loss: 0.6133923530578613 \n",
      "     Validation Step: 19 Validation Loss: 0.6154116988182068 \n",
      "     Validation Step: 20 Validation Loss: 0.6174547076225281 \n",
      "     Validation Step: 21 Validation Loss: 0.6102584600448608 \n",
      "     Validation Step: 22 Validation Loss: 0.6105469465255737 \n",
      "     Validation Step: 23 Validation Loss: 0.6116111278533936 \n",
      "     Validation Step: 24 Validation Loss: 0.6142188906669617 \n",
      "     Validation Step: 25 Validation Loss: 0.6186685562133789 \n",
      "     Validation Step: 26 Validation Loss: 0.6150041222572327 \n",
      "     Validation Step: 27 Validation Loss: 0.6121914982795715 \n",
      "     Validation Step: 28 Validation Loss: 0.6102114915847778 \n",
      "     Validation Step: 29 Validation Loss: 0.6158912181854248 \n",
      "     Validation Step: 30 Validation Loss: 0.6184877753257751 \n",
      "     Validation Step: 31 Validation Loss: 0.6112460494041443 \n",
      "     Validation Step: 32 Validation Loss: 0.6171747446060181 \n",
      "     Validation Step: 33 Validation Loss: 0.6146416664123535 \n",
      "     Validation Step: 34 Validation Loss: 0.6182169318199158 \n",
      "     Validation Step: 35 Validation Loss: 0.6130374670028687 \n",
      "     Validation Step: 36 Validation Loss: 0.6101614832878113 \n",
      "     Validation Step: 37 Validation Loss: 0.6163772344589233 \n",
      "     Validation Step: 38 Validation Loss: 0.6137396693229675 \n",
      "     Validation Step: 39 Validation Loss: 0.6105490922927856 \n",
      "     Validation Step: 40 Validation Loss: 0.618393063545227 \n",
      "     Validation Step: 41 Validation Loss: 0.6142817139625549 \n",
      "     Validation Step: 42 Validation Loss: 0.6146845817565918 \n",
      "     Validation Step: 43 Validation Loss: 0.6117162108421326 \n",
      "     Validation Step: 44 Validation Loss: 0.6157132983207703 \n",
      "Epoch: 28\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6163379549980164 \n",
      "     Training Step: 1 Training Loss: 0.6152183413505554 \n",
      "     Training Step: 2 Training Loss: 0.6159607768058777 \n",
      "     Training Step: 3 Training Loss: 0.6172438263893127 \n",
      "     Training Step: 4 Training Loss: 0.6148101687431335 \n",
      "     Training Step: 5 Training Loss: 0.6124086976051331 \n",
      "     Training Step: 6 Training Loss: 0.6116947531700134 \n",
      "     Training Step: 7 Training Loss: 0.6155124306678772 \n",
      "     Training Step: 8 Training Loss: 0.6133055686950684 \n",
      "     Training Step: 9 Training Loss: 0.6147217154502869 \n",
      "     Training Step: 10 Training Loss: 0.6168035268783569 \n",
      "     Training Step: 11 Training Loss: 0.6128158569335938 \n",
      "     Training Step: 12 Training Loss: 0.6126183867454529 \n",
      "     Training Step: 13 Training Loss: 0.6134097576141357 \n",
      "     Training Step: 14 Training Loss: 0.6171206831932068 \n",
      "     Training Step: 15 Training Loss: 0.6148093938827515 \n",
      "     Training Step: 16 Training Loss: 0.6177868247032166 \n",
      "     Training Step: 17 Training Loss: 0.6168493628501892 \n",
      "     Training Step: 18 Training Loss: 0.6175023913383484 \n",
      "     Training Step: 19 Training Loss: 0.6150121092796326 \n",
      "     Training Step: 20 Training Loss: 0.6196640133857727 \n",
      "     Training Step: 21 Training Loss: 0.6147509217262268 \n",
      "     Training Step: 22 Training Loss: 0.6116465926170349 \n",
      "     Training Step: 23 Training Loss: 0.6166622638702393 \n",
      "     Training Step: 24 Training Loss: 0.6161524653434753 \n",
      "     Training Step: 25 Training Loss: 0.614854633808136 \n",
      "     Training Step: 26 Training Loss: 0.6168675422668457 \n",
      "     Training Step: 27 Training Loss: 0.6140457987785339 \n",
      "     Training Step: 28 Training Loss: 0.6102653741836548 \n",
      "     Training Step: 29 Training Loss: 0.6136003136634827 \n",
      "     Training Step: 30 Training Loss: 0.6165162324905396 \n",
      "     Training Step: 31 Training Loss: 0.6143313646316528 \n",
      "     Training Step: 32 Training Loss: 0.6177524328231812 \n",
      "     Training Step: 33 Training Loss: 0.616472601890564 \n",
      "     Training Step: 34 Training Loss: 0.6154600381851196 \n",
      "     Training Step: 35 Training Loss: 0.6166257858276367 \n",
      "     Training Step: 36 Training Loss: 0.6180251240730286 \n",
      "     Training Step: 37 Training Loss: 0.6121302247047424 \n",
      "     Training Step: 38 Training Loss: 0.6146953701972961 \n",
      "     Training Step: 39 Training Loss: 0.619647741317749 \n",
      "     Training Step: 40 Training Loss: 0.6188716292381287 \n",
      "     Training Step: 41 Training Loss: 0.6142371892929077 \n",
      "     Training Step: 42 Training Loss: 0.618571937084198 \n",
      "     Training Step: 43 Training Loss: 0.6134403347969055 \n",
      "     Training Step: 44 Training Loss: 0.6151747703552246 \n",
      "     Training Step: 45 Training Loss: 0.6125815510749817 \n",
      "     Training Step: 46 Training Loss: 0.6115490198135376 \n",
      "     Training Step: 47 Training Loss: 0.6158714890480042 \n",
      "     Training Step: 48 Training Loss: 0.6145356297492981 \n",
      "     Training Step: 49 Training Loss: 0.6172921657562256 \n",
      "     Training Step: 50 Training Loss: 0.620290994644165 \n",
      "     Training Step: 51 Training Loss: 0.6154963374137878 \n",
      "     Training Step: 52 Training Loss: 0.6144455075263977 \n",
      "     Training Step: 53 Training Loss: 0.6120707988739014 \n",
      "     Training Step: 54 Training Loss: 0.6109999418258667 \n",
      "     Training Step: 55 Training Loss: 0.615891695022583 \n",
      "     Training Step: 56 Training Loss: 0.6101567149162292 \n",
      "     Training Step: 57 Training Loss: 0.610116720199585 \n",
      "     Training Step: 58 Training Loss: 0.6168212294578552 \n",
      "     Training Step: 59 Training Loss: 0.6139012575149536 \n",
      "     Training Step: 60 Training Loss: 0.6134147047996521 \n",
      "     Training Step: 61 Training Loss: 0.6158468723297119 \n",
      "     Training Step: 62 Training Loss: 0.6185801029205322 \n",
      "     Training Step: 63 Training Loss: 0.6129305958747864 \n",
      "     Training Step: 64 Training Loss: 0.6106812953948975 \n",
      "     Training Step: 65 Training Loss: 0.6123347282409668 \n",
      "     Training Step: 66 Training Loss: 0.611763596534729 \n",
      "     Training Step: 67 Training Loss: 0.6171846985816956 \n",
      "     Training Step: 68 Training Loss: 0.6106391549110413 \n",
      "     Training Step: 69 Training Loss: 0.618506908416748 \n",
      "     Training Step: 70 Training Loss: 0.6123074889183044 \n",
      "     Training Step: 71 Training Loss: 0.6142662167549133 \n",
      "     Training Step: 72 Training Loss: 0.6137669086456299 \n",
      "     Training Step: 73 Training Loss: 0.6133630275726318 \n",
      "     Training Step: 74 Training Loss: 0.6185815930366516 \n",
      "     Training Step: 75 Training Loss: 0.6128885746002197 \n",
      "     Training Step: 76 Training Loss: 0.61080402135849 \n",
      "     Training Step: 77 Training Loss: 0.612240195274353 \n",
      "     Training Step: 78 Training Loss: 0.6101107001304626 \n",
      "     Training Step: 79 Training Loss: 0.6161680221557617 \n",
      "     Training Step: 80 Training Loss: 0.6144272685050964 \n",
      "     Training Step: 81 Training Loss: 0.6093317866325378 \n",
      "     Training Step: 82 Training Loss: 0.6153382658958435 \n",
      "     Training Step: 83 Training Loss: 0.6112127900123596 \n",
      "     Training Step: 84 Training Loss: 0.612308144569397 \n",
      "     Training Step: 85 Training Loss: 0.6129449009895325 \n",
      "     Training Step: 86 Training Loss: 0.6168004274368286 \n",
      "     Training Step: 87 Training Loss: 0.6157624125480652 \n",
      "     Training Step: 88 Training Loss: 0.6167177557945251 \n",
      "     Training Step: 89 Training Loss: 0.6142086982727051 \n",
      "     Training Step: 90 Training Loss: 0.614323079586029 \n",
      "     Training Step: 91 Training Loss: 0.6121869683265686 \n",
      "     Training Step: 92 Training Loss: 0.6129027009010315 \n",
      "     Training Step: 93 Training Loss: 0.6145328879356384 \n",
      "     Training Step: 94 Training Loss: 0.6171167492866516 \n",
      "     Training Step: 95 Training Loss: 0.6097292900085449 \n",
      "     Training Step: 96 Training Loss: 0.6129037737846375 \n",
      "     Training Step: 97 Training Loss: 0.6152896881103516 \n",
      "     Training Step: 98 Training Loss: 0.6182908415794373 \n",
      "     Training Step: 99 Training Loss: 0.6121399998664856 \n",
      "     Training Step: 100 Training Loss: 0.616389811038971 \n",
      "     Training Step: 101 Training Loss: 0.6155629754066467 \n",
      "     Training Step: 102 Training Loss: 0.6162267327308655 \n",
      "     Training Step: 103 Training Loss: 0.6146306395530701 \n",
      "     Training Step: 104 Training Loss: 0.6181173920631409 \n",
      "     Training Step: 105 Training Loss: 0.6118839979171753 \n",
      "     Training Step: 106 Training Loss: 0.6139768362045288 \n",
      "     Training Step: 107 Training Loss: 0.6134021878242493 \n",
      "     Training Step: 108 Training Loss: 0.6137731075286865 \n",
      "     Training Step: 109 Training Loss: 0.61150062084198 \n",
      "     Training Step: 110 Training Loss: 0.6106188893318176 \n",
      "     Training Step: 111 Training Loss: 0.6142241358757019 \n",
      "     Training Step: 112 Training Loss: 0.6131272912025452 \n",
      "     Training Step: 113 Training Loss: 0.6107063889503479 \n",
      "     Training Step: 114 Training Loss: 0.6178775429725647 \n",
      "     Training Step: 115 Training Loss: 0.6130167841911316 \n",
      "     Training Step: 116 Training Loss: 0.6124268174171448 \n",
      "     Training Step: 117 Training Loss: 0.6130963563919067 \n",
      "     Training Step: 118 Training Loss: 0.6114999651908875 \n",
      "     Training Step: 119 Training Loss: 0.6151679158210754 \n",
      "     Training Step: 120 Training Loss: 0.6107335090637207 \n",
      "     Training Step: 121 Training Loss: 0.6156914234161377 \n",
      "     Training Step: 122 Training Loss: 0.6155046820640564 \n",
      "     Training Step: 123 Training Loss: 0.6157989501953125 \n",
      "     Training Step: 124 Training Loss: 0.613294243812561 \n",
      "     Training Step: 125 Training Loss: 0.6135636568069458 \n",
      "     Training Step: 126 Training Loss: 0.6153033971786499 \n",
      "     Training Step: 127 Training Loss: 0.6146863698959351 \n",
      "     Training Step: 128 Training Loss: 0.6094579100608826 \n",
      "     Training Step: 129 Training Loss: 0.6097321510314941 \n",
      "     Training Step: 130 Training Loss: 0.613706648349762 \n",
      "     Training Step: 131 Training Loss: 0.6156376004219055 \n",
      "     Training Step: 132 Training Loss: 0.6115517616271973 \n",
      "     Training Step: 133 Training Loss: 0.610621988773346 \n",
      "     Training Step: 134 Training Loss: 0.6117541790008545 \n",
      "     Training Step: 135 Training Loss: 0.6174464821815491 \n",
      "     Training Step: 136 Training Loss: 0.6104661226272583 \n",
      "     Training Step: 137 Training Loss: 0.611238956451416 \n",
      "     Training Step: 138 Training Loss: 0.6168155670166016 \n",
      "     Training Step: 139 Training Loss: 0.6149578094482422 \n",
      "     Training Step: 140 Training Loss: 0.6139636039733887 \n",
      "     Training Step: 141 Training Loss: 0.6177114844322205 \n",
      "     Training Step: 142 Training Loss: 0.6148144006729126 \n",
      "     Training Step: 143 Training Loss: 0.6147760152816772 \n",
      "     Training Step: 144 Training Loss: 0.6117433309555054 \n",
      "     Training Step: 145 Training Loss: 0.6118577122688293 \n",
      "     Training Step: 146 Training Loss: 0.6119109988212585 \n",
      "     Training Step: 147 Training Loss: 0.6156247854232788 \n",
      "     Training Step: 148 Training Loss: 0.6129863858222961 \n",
      "     Training Step: 149 Training Loss: 0.6167314648628235 \n",
      "     Training Step: 150 Training Loss: 0.6116946339607239 \n",
      "     Training Step: 151 Training Loss: 0.612497866153717 \n",
      "     Training Step: 152 Training Loss: 0.6200306415557861 \n",
      "     Training Step: 153 Training Loss: 0.6194609999656677 \n",
      "     Training Step: 154 Training Loss: 0.6158500909805298 \n",
      "     Training Step: 155 Training Loss: 0.6135345697402954 \n",
      "     Training Step: 156 Training Loss: 0.6158506274223328 \n",
      "     Training Step: 157 Training Loss: 0.6099419593811035 \n",
      "     Training Step: 158 Training Loss: 0.6172534823417664 \n",
      "     Training Step: 159 Training Loss: 0.6112246513366699 \n",
      "     Training Step: 160 Training Loss: 0.6138271689414978 \n",
      "     Training Step: 161 Training Loss: 0.6116270422935486 \n",
      "     Training Step: 162 Training Loss: 0.6131563186645508 \n",
      "     Training Step: 163 Training Loss: 0.6147645711898804 \n",
      "     Training Step: 164 Training Loss: 0.6131633520126343 \n",
      "     Training Step: 165 Training Loss: 0.6136927604675293 \n",
      "     Training Step: 166 Training Loss: 0.6144389510154724 \n",
      "     Training Step: 167 Training Loss: 0.6163351535797119 \n",
      "     Training Step: 168 Training Loss: 0.6133280992507935 \n",
      "     Training Step: 169 Training Loss: 0.618028461933136 \n",
      "     Training Step: 170 Training Loss: 0.6155542135238647 \n",
      "     Training Step: 171 Training Loss: 0.6153128147125244 \n",
      "     Training Step: 172 Training Loss: 0.6150916218757629 \n",
      "     Training Step: 173 Training Loss: 0.6155349612236023 \n",
      "     Training Step: 174 Training Loss: 0.6122372150421143 \n",
      "     Training Step: 175 Training Loss: 0.6144725680351257 \n",
      "     Training Step: 176 Training Loss: 0.6109299063682556 \n",
      "     Training Step: 177 Training Loss: 0.613225519657135 \n",
      "     Training Step: 178 Training Loss: 0.6115456223487854 \n",
      "     Training Step: 179 Training Loss: 0.6147745847702026 \n",
      "     Training Step: 180 Training Loss: 0.618752121925354 \n",
      "     Training Step: 181 Training Loss: 0.6146848201751709 \n",
      "     Training Step: 182 Training Loss: 0.6141571998596191 \n",
      "     Training Step: 183 Training Loss: 0.6152615547180176 \n",
      "     Training Step: 184 Training Loss: 0.6114741563796997 \n",
      "     Training Step: 185 Training Loss: 0.6104002594947815 \n",
      "     Training Step: 186 Training Loss: 0.6138054132461548 \n",
      "     Training Step: 187 Training Loss: 0.6125549674034119 \n",
      "     Training Step: 188 Training Loss: 0.6114773154258728 \n",
      "     Training Step: 189 Training Loss: 0.6122763156890869 \n",
      "     Training Step: 190 Training Loss: 0.6189104914665222 \n",
      "     Training Step: 191 Training Loss: 0.6177737712860107 \n",
      "     Training Step: 192 Training Loss: 0.6104964017868042 \n",
      "     Training Step: 193 Training Loss: 0.6107606887817383 \n",
      "     Training Step: 194 Training Loss: 0.6127228736877441 \n",
      "     Training Step: 195 Training Loss: 0.6141250729560852 \n",
      "     Training Step: 196 Training Loss: 0.6124945878982544 \n",
      "     Training Step: 197 Training Loss: 0.6156420111656189 \n",
      "     Training Step: 198 Training Loss: 0.6105014681816101 \n",
      "     Training Step: 199 Training Loss: 0.6127759218215942 \n",
      "     Training Step: 200 Training Loss: 0.6117377281188965 \n",
      "     Training Step: 201 Training Loss: 0.6118720173835754 \n",
      "     Training Step: 202 Training Loss: 0.6143872737884521 \n",
      "     Training Step: 203 Training Loss: 0.6183183193206787 \n",
      "     Training Step: 204 Training Loss: 0.6181552410125732 \n",
      "     Training Step: 205 Training Loss: 0.6178092956542969 \n",
      "     Training Step: 206 Training Loss: 0.6126545071601868 \n",
      "     Training Step: 207 Training Loss: 0.6127867698669434 \n",
      "     Training Step: 208 Training Loss: 0.61191725730896 \n",
      "     Training Step: 209 Training Loss: 0.6123529672622681 \n",
      "     Training Step: 210 Training Loss: 0.6122084856033325 \n",
      "     Training Step: 211 Training Loss: 0.6101106405258179 \n",
      "     Training Step: 212 Training Loss: 0.615481436252594 \n",
      "     Training Step: 213 Training Loss: 0.6106754541397095 \n",
      "     Training Step: 214 Training Loss: 0.6172504425048828 \n",
      "     Training Step: 215 Training Loss: 0.6126677989959717 \n",
      "     Training Step: 216 Training Loss: 0.6184449791908264 \n",
      "     Training Step: 217 Training Loss: 0.6116182208061218 \n",
      "     Training Step: 218 Training Loss: 0.6147847175598145 \n",
      "     Training Step: 219 Training Loss: 0.6147280931472778 \n",
      "     Training Step: 220 Training Loss: 0.6145871877670288 \n",
      "     Training Step: 221 Training Loss: 0.6177446842193604 \n",
      "     Training Step: 222 Training Loss: 0.6084147691726685 \n",
      "     Training Step: 223 Training Loss: 0.6160552501678467 \n",
      "     Training Step: 224 Training Loss: 0.6209918260574341 \n",
      "     Training Step: 225 Training Loss: 0.6167198419570923 \n",
      "     Training Step: 226 Training Loss: 0.6150480508804321 \n",
      "     Training Step: 227 Training Loss: 0.6153558492660522 \n",
      "     Training Step: 228 Training Loss: 0.6139319539070129 \n",
      "     Training Step: 229 Training Loss: 0.6136130094528198 \n",
      "     Training Step: 230 Training Loss: 0.6168094277381897 \n",
      "     Training Step: 231 Training Loss: 0.6169634461402893 \n",
      "     Training Step: 232 Training Loss: 0.6143596172332764 \n",
      "     Training Step: 233 Training Loss: 0.6135540008544922 \n",
      "     Training Step: 234 Training Loss: 0.6141359806060791 \n",
      "     Training Step: 235 Training Loss: 0.6135662794113159 \n",
      "     Training Step: 236 Training Loss: 0.6144780516624451 \n",
      "     Training Step: 237 Training Loss: 0.6094250679016113 \n",
      "     Training Step: 238 Training Loss: 0.6149768829345703 \n",
      "     Training Step: 239 Training Loss: 0.617013692855835 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6142927408218384 \n",
      "     Validation Step: 1 Validation Loss: 0.6177789568901062 \n",
      "     Validation Step: 2 Validation Loss: 0.6101804375648499 \n",
      "     Validation Step: 3 Validation Loss: 0.6146830916404724 \n",
      "     Validation Step: 4 Validation Loss: 0.6104867458343506 \n",
      "     Validation Step: 5 Validation Loss: 0.615374743938446 \n",
      "     Validation Step: 6 Validation Loss: 0.6149325370788574 \n",
      "     Validation Step: 7 Validation Loss: 0.6173830628395081 \n",
      "     Validation Step: 8 Validation Loss: 0.6101455092430115 \n",
      "     Validation Step: 9 Validation Loss: 0.6156748533248901 \n",
      "     Validation Step: 10 Validation Loss: 0.6142417192459106 \n",
      "     Validation Step: 11 Validation Loss: 0.6176756620407104 \n",
      "     Validation Step: 12 Validation Loss: 0.613714873790741 \n",
      "     Validation Step: 13 Validation Loss: 0.6121454834938049 \n",
      "     Validation Step: 14 Validation Loss: 0.618571937084198 \n",
      "     Validation Step: 15 Validation Loss: 0.6163198351860046 \n",
      "     Validation Step: 16 Validation Loss: 0.6156455278396606 \n",
      "     Validation Step: 17 Validation Loss: 0.6145710349082947 \n",
      "     Validation Step: 18 Validation Loss: 0.6118922829627991 \n",
      "     Validation Step: 19 Validation Loss: 0.6184397339820862 \n",
      "     Validation Step: 20 Validation Loss: 0.6111934781074524 \n",
      "     Validation Step: 21 Validation Loss: 0.6141491532325745 \n",
      "     Validation Step: 22 Validation Loss: 0.60752272605896 \n",
      "     Validation Step: 23 Validation Loss: 0.6141576170921326 \n",
      "     Validation Step: 24 Validation Loss: 0.6152727603912354 \n",
      "     Validation Step: 25 Validation Loss: 0.617094874382019 \n",
      "     Validation Step: 26 Validation Loss: 0.6150997877120972 \n",
      "     Validation Step: 27 Validation Loss: 0.6160789132118225 \n",
      "     Validation Step: 28 Validation Loss: 0.6181609630584717 \n",
      "     Validation Step: 29 Validation Loss: 0.6146177053451538 \n",
      "     Validation Step: 30 Validation Loss: 0.6130021810531616 \n",
      "     Validation Step: 31 Validation Loss: 0.6136924028396606 \n",
      "     Validation Step: 32 Validation Loss: 0.6128315329551697 \n",
      "     Validation Step: 33 Validation Loss: 0.6111726760864258 \n",
      "     Validation Step: 34 Validation Loss: 0.6105172634124756 \n",
      "     Validation Step: 35 Validation Loss: 0.6106507778167725 \n",
      "     Validation Step: 36 Validation Loss: 0.6158726215362549 \n",
      "     Validation Step: 37 Validation Loss: 0.618605375289917 \n",
      "     Validation Step: 38 Validation Loss: 0.6148713231086731 \n",
      "     Validation Step: 39 Validation Loss: 0.6101189255714417 \n",
      "     Validation Step: 40 Validation Loss: 0.6115574836730957 \n",
      "     Validation Step: 41 Validation Loss: 0.6116511821746826 \n",
      "     Validation Step: 42 Validation Loss: 0.6136543154716492 \n",
      "     Validation Step: 43 Validation Loss: 0.6133227348327637 \n",
      "     Validation Step: 44 Validation Loss: 0.6183506846427917 \n",
      "Epoch: 29\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.614682674407959 \n",
      "     Training Step: 1 Training Loss: 0.6153171062469482 \n",
      "     Training Step: 2 Training Loss: 0.6145001649856567 \n",
      "     Training Step: 3 Training Loss: 0.6131994724273682 \n",
      "     Training Step: 4 Training Loss: 0.6135324835777283 \n",
      "     Training Step: 5 Training Loss: 0.6132411360740662 \n",
      "     Training Step: 6 Training Loss: 0.6167210340499878 \n",
      "     Training Step: 7 Training Loss: 0.609451949596405 \n",
      "     Training Step: 8 Training Loss: 0.6155281066894531 \n",
      "     Training Step: 9 Training Loss: 0.6097897887229919 \n",
      "     Training Step: 10 Training Loss: 0.6083012223243713 \n",
      "     Training Step: 11 Training Loss: 0.6146935224533081 \n",
      "     Training Step: 12 Training Loss: 0.6152433156967163 \n",
      "     Training Step: 13 Training Loss: 0.6139681339263916 \n",
      "     Training Step: 14 Training Loss: 0.6120515465736389 \n",
      "     Training Step: 15 Training Loss: 0.6128829121589661 \n",
      "     Training Step: 16 Training Loss: 0.6116882562637329 \n",
      "     Training Step: 17 Training Loss: 0.6168167591094971 \n",
      "     Training Step: 18 Training Loss: 0.6129416227340698 \n",
      "     Training Step: 19 Training Loss: 0.6181202530860901 \n",
      "     Training Step: 20 Training Loss: 0.6116950511932373 \n",
      "     Training Step: 21 Training Loss: 0.6172491908073425 \n",
      "     Training Step: 22 Training Loss: 0.6174249053001404 \n",
      "     Training Step: 23 Training Loss: 0.6167300939559937 \n",
      "     Training Step: 24 Training Loss: 0.6103512644767761 \n",
      "     Training Step: 25 Training Loss: 0.6106570959091187 \n",
      "     Training Step: 26 Training Loss: 0.6095002293586731 \n",
      "     Training Step: 27 Training Loss: 0.6097212433815002 \n",
      "     Training Step: 28 Training Loss: 0.6181144118309021 \n",
      "     Training Step: 29 Training Loss: 0.6116588711738586 \n",
      "     Training Step: 30 Training Loss: 0.6158403158187866 \n",
      "     Training Step: 31 Training Loss: 0.6155473589897156 \n",
      "     Training Step: 32 Training Loss: 0.6131066679954529 \n",
      "     Training Step: 33 Training Loss: 0.6168083548545837 \n",
      "     Training Step: 34 Training Loss: 0.6147854328155518 \n",
      "     Training Step: 35 Training Loss: 0.6128548383712769 \n",
      "     Training Step: 36 Training Loss: 0.6158844232559204 \n",
      "     Training Step: 37 Training Loss: 0.6179519295692444 \n",
      "     Training Step: 38 Training Loss: 0.6117689609527588 \n",
      "     Training Step: 39 Training Loss: 0.6156381368637085 \n",
      "     Training Step: 40 Training Loss: 0.6160755753517151 \n",
      "     Training Step: 41 Training Loss: 0.6134161949157715 \n",
      "     Training Step: 42 Training Loss: 0.6147871613502502 \n",
      "     Training Step: 43 Training Loss: 0.6106260418891907 \n",
      "     Training Step: 44 Training Loss: 0.6182535886764526 \n",
      "     Training Step: 45 Training Loss: 0.614738404750824 \n",
      "     Training Step: 46 Training Loss: 0.6132604479789734 \n",
      "     Training Step: 47 Training Loss: 0.6177546977996826 \n",
      "     Training Step: 48 Training Loss: 0.6142835021018982 \n",
      "     Training Step: 49 Training Loss: 0.6126781702041626 \n",
      "     Training Step: 50 Training Loss: 0.6144137978553772 \n",
      "     Training Step: 51 Training Loss: 0.6123460531234741 \n",
      "     Training Step: 52 Training Loss: 0.6134756207466125 \n",
      "     Training Step: 53 Training Loss: 0.6167973875999451 \n",
      "     Training Step: 54 Training Loss: 0.6155174970626831 \n",
      "     Training Step: 55 Training Loss: 0.6121228933334351 \n",
      "     Training Step: 56 Training Loss: 0.6133426427841187 \n",
      "     Training Step: 57 Training Loss: 0.6110027432441711 \n",
      "     Training Step: 58 Training Loss: 0.610055148601532 \n",
      "     Training Step: 59 Training Loss: 0.6172477602958679 \n",
      "     Training Step: 60 Training Loss: 0.614702582359314 \n",
      "     Training Step: 61 Training Loss: 0.6133402585983276 \n",
      "     Training Step: 62 Training Loss: 0.6144096851348877 \n",
      "     Training Step: 63 Training Loss: 0.6147950291633606 \n",
      "     Training Step: 64 Training Loss: 0.6122478246688843 \n",
      "     Training Step: 65 Training Loss: 0.6168264746665955 \n",
      "     Training Step: 66 Training Loss: 0.6151219606399536 \n",
      "     Training Step: 67 Training Loss: 0.611219048500061 \n",
      "     Training Step: 68 Training Loss: 0.6169223189353943 \n",
      "     Training Step: 69 Training Loss: 0.6188844442367554 \n",
      "     Training Step: 70 Training Loss: 0.6153728365898132 \n",
      "     Training Step: 71 Training Loss: 0.6123155355453491 \n",
      "     Training Step: 72 Training Loss: 0.6153865456581116 \n",
      "     Training Step: 73 Training Loss: 0.6163564324378967 \n",
      "     Training Step: 74 Training Loss: 0.6180399060249329 \n",
      "     Training Step: 75 Training Loss: 0.6185889840126038 \n",
      "     Training Step: 76 Training Loss: 0.6118878722190857 \n",
      "     Training Step: 77 Training Loss: 0.6162456274032593 \n",
      "     Training Step: 78 Training Loss: 0.6154375672340393 \n",
      "     Training Step: 79 Training Loss: 0.6139653325080872 \n",
      "     Training Step: 80 Training Loss: 0.6158782243728638 \n",
      "     Training Step: 81 Training Loss: 0.6182501912117004 \n",
      "     Training Step: 82 Training Loss: 0.6136658787727356 \n",
      "     Training Step: 83 Training Loss: 0.6151623725891113 \n",
      "     Training Step: 84 Training Loss: 0.6134878993034363 \n",
      "     Training Step: 85 Training Loss: 0.6144912838935852 \n",
      "     Training Step: 86 Training Loss: 0.6107370257377625 \n",
      "     Training Step: 87 Training Loss: 0.6129887104034424 \n",
      "     Training Step: 88 Training Loss: 0.6161158680915833 \n",
      "     Training Step: 89 Training Loss: 0.6211021542549133 \n",
      "     Training Step: 90 Training Loss: 0.6122828722000122 \n",
      "     Training Step: 91 Training Loss: 0.6117728352546692 \n",
      "     Training Step: 92 Training Loss: 0.6141172647476196 \n",
      "     Training Step: 93 Training Loss: 0.6171583533287048 \n",
      "     Training Step: 94 Training Loss: 0.6101832985877991 \n",
      "     Training Step: 95 Training Loss: 0.6154870390892029 \n",
      "     Training Step: 96 Training Loss: 0.6184601187705994 \n",
      "     Training Step: 97 Training Loss: 0.6116268038749695 \n",
      "     Training Step: 98 Training Loss: 0.6137662529945374 \n",
      "     Training Step: 99 Training Loss: 0.6161190271377563 \n",
      "     Training Step: 100 Training Loss: 0.6145209670066833 \n",
      "     Training Step: 101 Training Loss: 0.6127338409423828 \n",
      "     Training Step: 102 Training Loss: 0.6104273200035095 \n",
      "     Training Step: 103 Training Loss: 0.6117820739746094 \n",
      "     Training Step: 104 Training Loss: 0.6114324331283569 \n",
      "     Training Step: 105 Training Loss: 0.6158470511436462 \n",
      "     Training Step: 106 Training Loss: 0.6172264814376831 \n",
      "     Training Step: 107 Training Loss: 0.6147034764289856 \n",
      "     Training Step: 108 Training Loss: 0.618492603302002 \n",
      "     Training Step: 109 Training Loss: 0.6135455369949341 \n",
      "     Training Step: 110 Training Loss: 0.6196709275245667 \n",
      "     Training Step: 111 Training Loss: 0.6147860288619995 \n",
      "     Training Step: 112 Training Loss: 0.611906111240387 \n",
      "     Training Step: 113 Training Loss: 0.6156443357467651 \n",
      "     Training Step: 114 Training Loss: 0.6172487139701843 \n",
      "     Training Step: 115 Training Loss: 0.6157824993133545 \n",
      "     Training Step: 116 Training Loss: 0.616729736328125 \n",
      "     Training Step: 117 Training Loss: 0.61848384141922 \n",
      "     Training Step: 118 Training Loss: 0.6129106879234314 \n",
      "     Training Step: 119 Training Loss: 0.6169812083244324 \n",
      "     Training Step: 120 Training Loss: 0.6101805567741394 \n",
      "     Training Step: 121 Training Loss: 0.6155892014503479 \n",
      "     Training Step: 122 Training Loss: 0.6152090430259705 \n",
      "     Training Step: 123 Training Loss: 0.619910717010498 \n",
      "     Training Step: 124 Training Loss: 0.6119135022163391 \n",
      "     Training Step: 125 Training Loss: 0.613379180431366 \n",
      "     Training Step: 126 Training Loss: 0.6168439388275146 \n",
      "     Training Step: 127 Training Loss: 0.6158415675163269 \n",
      "     Training Step: 128 Training Loss: 0.6194170117378235 \n",
      "     Training Step: 129 Training Loss: 0.6148496866226196 \n",
      "     Training Step: 130 Training Loss: 0.6126210689544678 \n",
      "     Training Step: 131 Training Loss: 0.6162634491920471 \n",
      "     Training Step: 132 Training Loss: 0.6105867624282837 \n",
      "     Training Step: 133 Training Loss: 0.6203526854515076 \n",
      "     Training Step: 134 Training Loss: 0.6155155897140503 \n",
      "     Training Step: 135 Training Loss: 0.6162413358688354 \n",
      "     Training Step: 136 Training Loss: 0.6107614636421204 \n",
      "     Training Step: 137 Training Loss: 0.6122297048568726 \n",
      "     Training Step: 138 Training Loss: 0.6138322353363037 \n",
      "     Training Step: 139 Training Loss: 0.6128963828086853 \n",
      "     Training Step: 140 Training Loss: 0.6114872694015503 \n",
      "     Training Step: 141 Training Loss: 0.6112366914749146 \n",
      "     Training Step: 142 Training Loss: 0.6101279854774475 \n",
      "     Training Step: 143 Training Loss: 0.6115372776985168 \n",
      "     Training Step: 144 Training Loss: 0.614467203617096 \n",
      "     Training Step: 145 Training Loss: 0.6168285012245178 \n",
      "     Training Step: 146 Training Loss: 0.614670991897583 \n",
      "     Training Step: 147 Training Loss: 0.6108261942863464 \n",
      "     Training Step: 148 Training Loss: 0.6115292906761169 \n",
      "     Training Step: 149 Training Loss: 0.6165116429328918 \n",
      "     Training Step: 150 Training Loss: 0.6143051385879517 \n",
      "     Training Step: 151 Training Loss: 0.6128249764442444 \n",
      "     Training Step: 152 Training Loss: 0.6175836324691772 \n",
      "     Training Step: 153 Training Loss: 0.6093571782112122 \n",
      "     Training Step: 154 Training Loss: 0.612557053565979 \n",
      "     Training Step: 155 Training Loss: 0.6133013963699341 \n",
      "     Training Step: 156 Training Loss: 0.6147286891937256 \n",
      "     Training Step: 157 Training Loss: 0.6131680607795715 \n",
      "     Training Step: 158 Training Loss: 0.617933988571167 \n",
      "     Training Step: 159 Training Loss: 0.6165290474891663 \n",
      "     Training Step: 160 Training Loss: 0.6121913194656372 \n",
      "     Training Step: 161 Training Loss: 0.611788272857666 \n",
      "     Training Step: 162 Training Loss: 0.6116252541542053 \n",
      "     Training Step: 163 Training Loss: 0.6168087124824524 \n",
      "     Training Step: 164 Training Loss: 0.6156166195869446 \n",
      "     Training Step: 165 Training Loss: 0.6111613512039185 \n",
      "     Training Step: 166 Training Loss: 0.6166948676109314 \n",
      "     Training Step: 167 Training Loss: 0.6149834394454956 \n",
      "     Training Step: 168 Training Loss: 0.6131250858306885 \n",
      "     Training Step: 169 Training Loss: 0.6143764853477478 \n",
      "     Training Step: 170 Training Loss: 0.6144912242889404 \n",
      "     Training Step: 171 Training Loss: 0.6146365404129028 \n",
      "     Training Step: 172 Training Loss: 0.6125509738922119 \n",
      "     Training Step: 173 Training Loss: 0.6186963319778442 \n",
      "     Training Step: 174 Training Loss: 0.6137784719467163 \n",
      "     Training Step: 175 Training Loss: 0.6157222390174866 \n",
      "     Training Step: 176 Training Loss: 0.6124200820922852 \n",
      "     Training Step: 177 Training Loss: 0.6122899055480957 \n",
      "     Training Step: 178 Training Loss: 0.6176377534866333 \n",
      "     Training Step: 179 Training Loss: 0.6118872761726379 \n",
      "     Training Step: 180 Training Loss: 0.6108009815216064 \n",
      "     Training Step: 181 Training Loss: 0.6136857867240906 \n",
      "     Training Step: 182 Training Loss: 0.6141438484191895 \n",
      "     Training Step: 183 Training Loss: 0.6101852059364319 \n",
      "     Training Step: 184 Training Loss: 0.6122019290924072 \n",
      "     Training Step: 185 Training Loss: 0.6126682758331299 \n",
      "     Training Step: 186 Training Loss: 0.6142790913581848 \n",
      "     Training Step: 187 Training Loss: 0.6139572858810425 \n",
      "     Training Step: 188 Training Loss: 0.6182563304901123 \n",
      "     Training Step: 189 Training Loss: 0.6147501468658447 \n",
      "     Training Step: 190 Training Loss: 0.6183987855911255 \n",
      "     Training Step: 191 Training Loss: 0.6143431663513184 \n",
      "     Training Step: 192 Training Loss: 0.6119534373283386 \n",
      "     Training Step: 193 Training Loss: 0.6140791773796082 \n",
      "     Training Step: 194 Training Loss: 0.6145027875900269 \n",
      "     Training Step: 195 Training Loss: 0.6138807535171509 \n",
      "     Training Step: 196 Training Loss: 0.6179389953613281 \n",
      "     Training Step: 197 Training Loss: 0.6135311722755432 \n",
      "     Training Step: 198 Training Loss: 0.6119166016578674 \n",
      "     Training Step: 199 Training Loss: 0.6141988039016724 \n",
      "     Training Step: 200 Training Loss: 0.6109758615493774 \n",
      "     Training Step: 201 Training Loss: 0.6107295155525208 \n",
      "     Training Step: 202 Training Loss: 0.6125048398971558 \n",
      "     Training Step: 203 Training Loss: 0.6202859878540039 \n",
      "     Training Step: 204 Training Loss: 0.6135810017585754 \n",
      "     Training Step: 205 Training Loss: 0.6140539050102234 \n",
      "     Training Step: 206 Training Loss: 0.6166645884513855 \n",
      "     Training Step: 207 Training Loss: 0.6108697056770325 \n",
      "     Training Step: 208 Training Loss: 0.6125137209892273 \n",
      "     Training Step: 209 Training Loss: 0.6136118173599243 \n",
      "     Training Step: 210 Training Loss: 0.613674521446228 \n",
      "     Training Step: 211 Training Loss: 0.6165784001350403 \n",
      "     Training Step: 212 Training Loss: 0.6178948283195496 \n",
      "     Training Step: 213 Training Loss: 0.6188929677009583 \n",
      "     Training Step: 214 Training Loss: 0.6133487224578857 \n",
      "     Training Step: 215 Training Loss: 0.6143398880958557 \n",
      "     Training Step: 216 Training Loss: 0.616860568523407 \n",
      "     Training Step: 217 Training Loss: 0.6129775047302246 \n",
      "     Training Step: 218 Training Loss: 0.6114755868911743 \n",
      "     Training Step: 219 Training Loss: 0.6161170601844788 \n",
      "     Training Step: 220 Training Loss: 0.6132110953330994 \n",
      "     Training Step: 221 Training Loss: 0.6140835881233215 \n",
      "     Training Step: 222 Training Loss: 0.6153232455253601 \n",
      "     Training Step: 223 Training Loss: 0.6124431490898132 \n",
      "     Training Step: 224 Training Loss: 0.6153246760368347 \n",
      "     Training Step: 225 Training Loss: 0.6148378849029541 \n",
      "     Training Step: 226 Training Loss: 0.6106057167053223 \n",
      "     Training Step: 227 Training Loss: 0.6149622201919556 \n",
      "     Training Step: 228 Training Loss: 0.6114709973335266 \n",
      "     Training Step: 229 Training Loss: 0.6154540777206421 \n",
      "     Training Step: 230 Training Loss: 0.6181935667991638 \n",
      "     Training Step: 231 Training Loss: 0.609785258769989 \n",
      "     Training Step: 232 Training Loss: 0.6152272820472717 \n",
      "     Training Step: 233 Training Loss: 0.6153450012207031 \n",
      "     Training Step: 234 Training Loss: 0.6149243712425232 \n",
      "     Training Step: 235 Training Loss: 0.6115816831588745 \n",
      "     Training Step: 236 Training Loss: 0.6123174428939819 \n",
      "     Training Step: 237 Training Loss: 0.6150820255279541 \n",
      "     Training Step: 238 Training Loss: 0.6115669012069702 \n",
      "     Training Step: 239 Training Loss: 0.6103929877281189 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6128440499305725 \n",
      "     Validation Step: 1 Validation Loss: 0.6149038076400757 \n",
      "     Validation Step: 2 Validation Loss: 0.6182005405426025 \n",
      "     Validation Step: 3 Validation Loss: 0.615692138671875 \n",
      "     Validation Step: 4 Validation Loss: 0.6151494979858398 \n",
      "     Validation Step: 5 Validation Loss: 0.6154160499572754 \n",
      "     Validation Step: 6 Validation Loss: 0.6159448027610779 \n",
      "     Validation Step: 7 Validation Loss: 0.6106258630752563 \n",
      "     Validation Step: 8 Validation Loss: 0.6105389595031738 \n",
      "     Validation Step: 9 Validation Loss: 0.61615389585495 \n",
      "     Validation Step: 10 Validation Loss: 0.6111925840377808 \n",
      "     Validation Step: 11 Validation Loss: 0.6184985637664795 \n",
      "     Validation Step: 12 Validation Loss: 0.614727795124054 \n",
      "     Validation Step: 13 Validation Loss: 0.6142838597297668 \n",
      "     Validation Step: 14 Validation Loss: 0.6121426820755005 \n",
      "     Validation Step: 15 Validation Loss: 0.6101433634757996 \n",
      "     Validation Step: 16 Validation Loss: 0.6119020581245422 \n",
      "     Validation Step: 17 Validation Loss: 0.6141367554664612 \n",
      "     Validation Step: 18 Validation Loss: 0.6074942350387573 \n",
      "     Validation Step: 19 Validation Loss: 0.614270031452179 \n",
      "     Validation Step: 20 Validation Loss: 0.618415892124176 \n",
      "     Validation Step: 21 Validation Loss: 0.6136538982391357 \n",
      "     Validation Step: 22 Validation Loss: 0.6149391531944275 \n",
      "     Validation Step: 23 Validation Loss: 0.615294337272644 \n",
      "     Validation Step: 24 Validation Loss: 0.6115720868110657 \n",
      "     Validation Step: 25 Validation Loss: 0.6163771748542786 \n",
      "     Validation Step: 26 Validation Loss: 0.6157119274139404 \n",
      "     Validation Step: 27 Validation Loss: 0.6116457581520081 \n",
      "     Validation Step: 28 Validation Loss: 0.6186074614524841 \n",
      "     Validation Step: 29 Validation Loss: 0.617698073387146 \n",
      "     Validation Step: 30 Validation Loss: 0.6141999363899231 \n",
      "     Validation Step: 31 Validation Loss: 0.6130392551422119 \n",
      "     Validation Step: 32 Validation Loss: 0.6145727634429932 \n",
      "     Validation Step: 33 Validation Loss: 0.6137389540672302 \n",
      "     Validation Step: 34 Validation Loss: 0.611181914806366 \n",
      "     Validation Step: 35 Validation Loss: 0.6101214289665222 \n",
      "     Validation Step: 36 Validation Loss: 0.6104897260665894 \n",
      "     Validation Step: 37 Validation Loss: 0.6101114749908447 \n",
      "     Validation Step: 38 Validation Loss: 0.6137726902961731 \n",
      "     Validation Step: 39 Validation Loss: 0.6146340370178223 \n",
      "     Validation Step: 40 Validation Loss: 0.6171240210533142 \n",
      "     Validation Step: 41 Validation Loss: 0.6174005270004272 \n",
      "     Validation Step: 42 Validation Loss: 0.6186478734016418 \n",
      "     Validation Step: 43 Validation Loss: 0.6178349256515503 \n",
      "     Validation Step: 44 Validation Loss: 0.6133350133895874 \n",
      "Epoch: 30\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.616783082485199 \n",
      "     Training Step: 1 Training Loss: 0.6106072068214417 \n",
      "     Training Step: 2 Training Loss: 0.6107591390609741 \n",
      "     Training Step: 3 Training Loss: 0.617561936378479 \n",
      "     Training Step: 4 Training Loss: 0.6104182004928589 \n",
      "     Training Step: 5 Training Loss: 0.6151994466781616 \n",
      "     Training Step: 6 Training Loss: 0.6094666719436646 \n",
      "     Training Step: 7 Training Loss: 0.6149191856384277 \n",
      "     Training Step: 8 Training Loss: 0.6128950119018555 \n",
      "     Training Step: 9 Training Loss: 0.614736020565033 \n",
      "     Training Step: 10 Training Loss: 0.6202634572982788 \n",
      "     Training Step: 11 Training Loss: 0.6117039918899536 \n",
      "     Training Step: 12 Training Loss: 0.6141372919082642 \n",
      "     Training Step: 13 Training Loss: 0.6149690747261047 \n",
      "     Training Step: 14 Training Loss: 0.6121664643287659 \n",
      "     Training Step: 15 Training Loss: 0.6144806146621704 \n",
      "     Training Step: 16 Training Loss: 0.6137796640396118 \n",
      "     Training Step: 17 Training Loss: 0.6135044097900391 \n",
      "     Training Step: 18 Training Loss: 0.6178355813026428 \n",
      "     Training Step: 19 Training Loss: 0.6209823489189148 \n",
      "     Training Step: 20 Training Loss: 0.6137129068374634 \n",
      "     Training Step: 21 Training Loss: 0.6134078502655029 \n",
      "     Training Step: 22 Training Loss: 0.6131937503814697 \n",
      "     Training Step: 23 Training Loss: 0.6114537715911865 \n",
      "     Training Step: 24 Training Loss: 0.6101369857788086 \n",
      "     Training Step: 25 Training Loss: 0.6160382628440857 \n",
      "     Training Step: 26 Training Loss: 0.6154301762580872 \n",
      "     Training Step: 27 Training Loss: 0.614740252494812 \n",
      "     Training Step: 28 Training Loss: 0.6147568225860596 \n",
      "     Training Step: 29 Training Loss: 0.6186947226524353 \n",
      "     Training Step: 30 Training Loss: 0.6154343485832214 \n",
      "     Training Step: 31 Training Loss: 0.6130650043487549 \n",
      "     Training Step: 32 Training Loss: 0.6148518919944763 \n",
      "     Training Step: 33 Training Loss: 0.6124250888824463 \n",
      "     Training Step: 34 Training Loss: 0.6165744662284851 \n",
      "     Training Step: 35 Training Loss: 0.6094548106193542 \n",
      "     Training Step: 36 Training Loss: 0.616961658000946 \n",
      "     Training Step: 37 Training Loss: 0.6155235767364502 \n",
      "     Training Step: 38 Training Loss: 0.6117836236953735 \n",
      "     Training Step: 39 Training Loss: 0.6140190362930298 \n",
      "     Training Step: 40 Training Loss: 0.6123500466346741 \n",
      "     Training Step: 41 Training Loss: 0.6140997409820557 \n",
      "     Training Step: 42 Training Loss: 0.6139821410179138 \n",
      "     Training Step: 43 Training Loss: 0.6146172881126404 \n",
      "     Training Step: 44 Training Loss: 0.6196897625923157 \n",
      "     Training Step: 45 Training Loss: 0.6157762408256531 \n",
      "     Training Step: 46 Training Loss: 0.6102964282035828 \n",
      "     Training Step: 47 Training Loss: 0.6168574690818787 \n",
      "     Training Step: 48 Training Loss: 0.6128113865852356 \n",
      "     Training Step: 49 Training Loss: 0.6141252517700195 \n",
      "     Training Step: 50 Training Loss: 0.6138455867767334 \n",
      "     Training Step: 51 Training Loss: 0.6116346120834351 \n",
      "     Training Step: 52 Training Loss: 0.6129993200302124 \n",
      "     Training Step: 53 Training Loss: 0.6133962869644165 \n",
      "     Training Step: 54 Training Loss: 0.6112038493156433 \n",
      "     Training Step: 55 Training Loss: 0.6131545901298523 \n",
      "     Training Step: 56 Training Loss: 0.6109085083007812 \n",
      "     Training Step: 57 Training Loss: 0.618537187576294 \n",
      "     Training Step: 58 Training Loss: 0.6136340498924255 \n",
      "     Training Step: 59 Training Loss: 0.6158286333084106 \n",
      "     Training Step: 60 Training Loss: 0.614747941493988 \n",
      "     Training Step: 61 Training Loss: 0.6147142052650452 \n",
      "     Training Step: 62 Training Loss: 0.6140656471252441 \n",
      "     Training Step: 63 Training Loss: 0.6129994988441467 \n",
      "     Training Step: 64 Training Loss: 0.6125518083572388 \n",
      "     Training Step: 65 Training Loss: 0.6168622374534607 \n",
      "     Training Step: 66 Training Loss: 0.6164098381996155 \n",
      "     Training Step: 67 Training Loss: 0.6121978163719177 \n",
      "     Training Step: 68 Training Loss: 0.6126999258995056 \n",
      "     Training Step: 69 Training Loss: 0.6107149720191956 \n",
      "     Training Step: 70 Training Loss: 0.6125617027282715 \n",
      "     Training Step: 71 Training Loss: 0.6154399514198303 \n",
      "     Training Step: 72 Training Loss: 0.6133022904396057 \n",
      "     Training Step: 73 Training Loss: 0.6201177835464478 \n",
      "     Training Step: 74 Training Loss: 0.614738404750824 \n",
      "     Training Step: 75 Training Loss: 0.6153402924537659 \n",
      "     Training Step: 76 Training Loss: 0.6125485301017761 \n",
      "     Training Step: 77 Training Loss: 0.6144925355911255 \n",
      "     Training Step: 78 Training Loss: 0.6124477982521057 \n",
      "     Training Step: 79 Training Loss: 0.6167713403701782 \n",
      "     Training Step: 80 Training Loss: 0.6155171990394592 \n",
      "     Training Step: 81 Training Loss: 0.6116788387298584 \n",
      "     Training Step: 82 Training Loss: 0.6133341193199158 \n",
      "     Training Step: 83 Training Loss: 0.6145126819610596 \n",
      "     Training Step: 84 Training Loss: 0.6118417382240295 \n",
      "     Training Step: 85 Training Loss: 0.6133303642272949 \n",
      "     Training Step: 86 Training Loss: 0.6138213872909546 \n",
      "     Training Step: 87 Training Loss: 0.6134472489356995 \n",
      "     Training Step: 88 Training Loss: 0.6142320036888123 \n",
      "     Training Step: 89 Training Loss: 0.6121706962585449 \n",
      "     Training Step: 90 Training Loss: 0.6151077151298523 \n",
      "     Training Step: 91 Training Loss: 0.6180477142333984 \n",
      "     Training Step: 92 Training Loss: 0.6146427989006042 \n",
      "     Training Step: 93 Training Loss: 0.6155809164047241 \n",
      "     Training Step: 94 Training Loss: 0.6123654246330261 \n",
      "     Training Step: 95 Training Loss: 0.6143598556518555 \n",
      "     Training Step: 96 Training Loss: 0.6122561693191528 \n",
      "     Training Step: 97 Training Loss: 0.6104666590690613 \n",
      "     Training Step: 98 Training Loss: 0.6161860823631287 \n",
      "     Training Step: 99 Training Loss: 0.611589252948761 \n",
      "     Training Step: 100 Training Loss: 0.6105021238327026 \n",
      "     Training Step: 101 Training Loss: 0.6099918484687805 \n",
      "     Training Step: 102 Training Loss: 0.6123735308647156 \n",
      "     Training Step: 103 Training Loss: 0.6150211095809937 \n",
      "     Training Step: 104 Training Loss: 0.6114761829376221 \n",
      "     Training Step: 105 Training Loss: 0.6190289258956909 \n",
      "     Training Step: 106 Training Loss: 0.6157282590866089 \n",
      "     Training Step: 107 Training Loss: 0.6146777868270874 \n",
      "     Training Step: 108 Training Loss: 0.6181129813194275 \n",
      "     Training Step: 109 Training Loss: 0.6107730865478516 \n",
      "     Training Step: 110 Training Loss: 0.6107487678527832 \n",
      "     Training Step: 111 Training Loss: 0.6177647709846497 \n",
      "     Training Step: 112 Training Loss: 0.6148955225944519 \n",
      "     Training Step: 113 Training Loss: 0.6132169961929321 \n",
      "     Training Step: 114 Training Loss: 0.6121118664741516 \n",
      "     Training Step: 115 Training Loss: 0.6151225566864014 \n",
      "     Training Step: 116 Training Loss: 0.6115977764129639 \n",
      "     Training Step: 117 Training Loss: 0.6125857830047607 \n",
      "     Training Step: 118 Training Loss: 0.613543689250946 \n",
      "     Training Step: 119 Training Loss: 0.613314688205719 \n",
      "     Training Step: 120 Training Loss: 0.6161144971847534 \n",
      "     Training Step: 121 Training Loss: 0.6142380833625793 \n",
      "     Training Step: 122 Training Loss: 0.6111994981765747 \n",
      "     Training Step: 123 Training Loss: 0.6185148358345032 \n",
      "     Training Step: 124 Training Loss: 0.6139227151870728 \n",
      "     Training Step: 125 Training Loss: 0.6171814203262329 \n",
      "     Training Step: 126 Training Loss: 0.6117091774940491 \n",
      "     Training Step: 127 Training Loss: 0.6158109903335571 \n",
      "     Training Step: 128 Training Loss: 0.6153693795204163 \n",
      "     Training Step: 129 Training Loss: 0.6136850714683533 \n",
      "     Training Step: 130 Training Loss: 0.6122856140136719 \n",
      "     Training Step: 131 Training Loss: 0.6144177913665771 \n",
      "     Training Step: 132 Training Loss: 0.6155217289924622 \n",
      "     Training Step: 133 Training Loss: 0.6109573841094971 \n",
      "     Training Step: 134 Training Loss: 0.6181201934814453 \n",
      "     Training Step: 135 Training Loss: 0.6114681959152222 \n",
      "     Training Step: 136 Training Loss: 0.6142693758010864 \n",
      "     Training Step: 137 Training Loss: 0.6118491291999817 \n",
      "     Training Step: 138 Training Loss: 0.6172093749046326 \n",
      "     Training Step: 139 Training Loss: 0.6118046641349792 \n",
      "     Training Step: 140 Training Loss: 0.6116270422935486 \n",
      "     Training Step: 141 Training Loss: 0.6097493767738342 \n",
      "     Training Step: 142 Training Loss: 0.6106607913970947 \n",
      "     Training Step: 143 Training Loss: 0.615030825138092 \n",
      "     Training Step: 144 Training Loss: 0.6169105768203735 \n",
      "     Training Step: 145 Training Loss: 0.6166957020759583 \n",
      "     Training Step: 146 Training Loss: 0.611911416053772 \n",
      "     Training Step: 147 Training Loss: 0.6183972954750061 \n",
      "     Training Step: 148 Training Loss: 0.614433765411377 \n",
      "     Training Step: 149 Training Loss: 0.6159616708755493 \n",
      "     Training Step: 150 Training Loss: 0.6120040416717529 \n",
      "     Training Step: 151 Training Loss: 0.6197839975357056 \n",
      "     Training Step: 152 Training Loss: 0.6157207489013672 \n",
      "     Training Step: 153 Training Loss: 0.6171631217002869 \n",
      "     Training Step: 154 Training Loss: 0.6151363253593445 \n",
      "     Training Step: 155 Training Loss: 0.6154249906539917 \n",
      "     Training Step: 156 Training Loss: 0.6155859231948853 \n",
      "     Training Step: 157 Training Loss: 0.612549364566803 \n",
      "     Training Step: 158 Training Loss: 0.6146936416625977 \n",
      "     Training Step: 159 Training Loss: 0.6181347370147705 \n",
      "     Training Step: 160 Training Loss: 0.614050030708313 \n",
      "     Training Step: 161 Training Loss: 0.6118587851524353 \n",
      "     Training Step: 162 Training Loss: 0.6172515749931335 \n",
      "     Training Step: 163 Training Loss: 0.6128057241439819 \n",
      "     Training Step: 164 Training Loss: 0.6136937737464905 \n",
      "     Training Step: 165 Training Loss: 0.608397901058197 \n",
      "     Training Step: 166 Training Loss: 0.6143897771835327 \n",
      "     Training Step: 167 Training Loss: 0.6144427061080933 \n",
      "     Training Step: 168 Training Loss: 0.6198574900627136 \n",
      "     Training Step: 169 Training Loss: 0.6101164221763611 \n",
      "     Training Step: 170 Training Loss: 0.6178164482116699 \n",
      "     Training Step: 171 Training Loss: 0.6169074177742004 \n",
      "     Training Step: 172 Training Loss: 0.6164526343345642 \n",
      "     Training Step: 173 Training Loss: 0.6188189387321472 \n",
      "     Training Step: 174 Training Loss: 0.613036572933197 \n",
      "     Training Step: 175 Training Loss: 0.6122291088104248 \n",
      "     Training Step: 176 Training Loss: 0.6174406409263611 \n",
      "     Training Step: 177 Training Loss: 0.6117281913757324 \n",
      "     Training Step: 178 Training Loss: 0.6177493333816528 \n",
      "     Training Step: 179 Training Loss: 0.6125184893608093 \n",
      "     Training Step: 180 Training Loss: 0.6184702515602112 \n",
      "     Training Step: 181 Training Loss: 0.6167736649513245 \n",
      "     Training Step: 182 Training Loss: 0.6146930456161499 \n",
      "     Training Step: 183 Training Loss: 0.6116172671318054 \n",
      "     Training Step: 184 Training Loss: 0.616241991519928 \n",
      "     Training Step: 185 Training Loss: 0.6124263405799866 \n",
      "     Training Step: 186 Training Loss: 0.6092855930328369 \n",
      "     Training Step: 187 Training Loss: 0.6168695688247681 \n",
      "     Training Step: 188 Training Loss: 0.6179829239845276 \n",
      "     Training Step: 189 Training Loss: 0.6170479655265808 \n",
      "     Training Step: 190 Training Loss: 0.6153915524482727 \n",
      "     Training Step: 191 Training Loss: 0.6155563592910767 \n",
      "     Training Step: 192 Training Loss: 0.6171971559524536 \n",
      "     Training Step: 193 Training Loss: 0.6167008280754089 \n",
      "     Training Step: 194 Training Loss: 0.6185993552207947 \n",
      "     Training Step: 195 Training Loss: 0.6155668497085571 \n",
      "     Training Step: 196 Training Loss: 0.6119696497917175 \n",
      "     Training Step: 197 Training Loss: 0.6153844594955444 \n",
      "     Training Step: 198 Training Loss: 0.6147544384002686 \n",
      "     Training Step: 199 Training Loss: 0.6162424683570862 \n",
      "     Training Step: 200 Training Loss: 0.6131991147994995 \n",
      "     Training Step: 201 Training Loss: 0.6115157008171082 \n",
      "     Training Step: 202 Training Loss: 0.6162673830986023 \n",
      "     Training Step: 203 Training Loss: 0.6115458607673645 \n",
      "     Training Step: 204 Training Loss: 0.6149125099182129 \n",
      "     Training Step: 205 Training Loss: 0.6116049885749817 \n",
      "     Training Step: 206 Training Loss: 0.6183367371559143 \n",
      "     Training Step: 207 Training Loss: 0.6156370043754578 \n",
      "     Training Step: 208 Training Loss: 0.6145100593566895 \n",
      "     Training Step: 209 Training Loss: 0.6142658591270447 \n",
      "     Training Step: 210 Training Loss: 0.6165125370025635 \n",
      "     Training Step: 211 Training Loss: 0.6102346181869507 \n",
      "     Training Step: 212 Training Loss: 0.6158891320228577 \n",
      "     Training Step: 213 Training Loss: 0.6138318777084351 \n",
      "     Training Step: 214 Training Loss: 0.6183914542198181 \n",
      "     Training Step: 215 Training Loss: 0.6132323145866394 \n",
      "     Training Step: 216 Training Loss: 0.6101817488670349 \n",
      "     Training Step: 217 Training Loss: 0.6098390817642212 \n",
      "     Training Step: 218 Training Loss: 0.6178603768348694 \n",
      "     Training Step: 219 Training Loss: 0.6169025897979736 \n",
      "     Training Step: 220 Training Loss: 0.6107507944107056 \n",
      "     Training Step: 221 Training Loss: 0.6168092489242554 \n",
      "     Training Step: 222 Training Loss: 0.6097575426101685 \n",
      "     Training Step: 223 Training Loss: 0.6153737306594849 \n",
      "     Training Step: 224 Training Loss: 0.6178142428398132 \n",
      "     Training Step: 225 Training Loss: 0.6142455339431763 \n",
      "     Training Step: 226 Training Loss: 0.6128925085067749 \n",
      "     Training Step: 227 Training Loss: 0.6108428239822388 \n",
      "     Training Step: 228 Training Loss: 0.6112018823623657 \n",
      "     Training Step: 229 Training Loss: 0.6106253266334534 \n",
      "     Training Step: 230 Training Loss: 0.6133830547332764 \n",
      "     Training Step: 231 Training Loss: 0.6137065887451172 \n",
      "     Training Step: 232 Training Loss: 0.6164027452468872 \n",
      "     Training Step: 233 Training Loss: 0.6134993433952332 \n",
      "     Training Step: 234 Training Loss: 0.6154428720474243 \n",
      "     Training Step: 235 Training Loss: 0.6133288145065308 \n",
      "     Training Step: 236 Training Loss: 0.6168105602264404 \n",
      "     Training Step: 237 Training Loss: 0.6128868460655212 \n",
      "     Training Step: 238 Training Loss: 0.6128117442131042 \n",
      "     Training Step: 239 Training Loss: 0.6146618723869324 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6137450933456421 \n",
      "     Validation Step: 1 Validation Loss: 0.6117114424705505 \n",
      "     Validation Step: 2 Validation Loss: 0.6178321838378906 \n",
      "     Validation Step: 3 Validation Loss: 0.6151400804519653 \n",
      "     Validation Step: 4 Validation Loss: 0.6105493307113647 \n",
      "     Validation Step: 5 Validation Loss: 0.6116027235984802 \n",
      "     Validation Step: 6 Validation Loss: 0.6163748502731323 \n",
      "     Validation Step: 7 Validation Loss: 0.6147293448448181 \n",
      "     Validation Step: 8 Validation Loss: 0.6142008304595947 \n",
      "     Validation Step: 9 Validation Loss: 0.6184884309768677 \n",
      "     Validation Step: 10 Validation Loss: 0.6146360039710999 \n",
      "     Validation Step: 11 Validation Loss: 0.6128745675086975 \n",
      "     Validation Step: 12 Validation Loss: 0.6102492213249207 \n",
      "     Validation Step: 13 Validation Loss: 0.6119126677513123 \n",
      "     Validation Step: 14 Validation Loss: 0.6157249808311462 \n",
      "     Validation Step: 15 Validation Loss: 0.6105393171310425 \n",
      "     Validation Step: 16 Validation Loss: 0.6146829724311829 \n",
      "     Validation Step: 17 Validation Loss: 0.6137257814407349 \n",
      "     Validation Step: 18 Validation Loss: 0.615330159664154 \n",
      "     Validation Step: 19 Validation Loss: 0.6186406016349792 \n",
      "     Validation Step: 20 Validation Loss: 0.6101519465446472 \n",
      "     Validation Step: 21 Validation Loss: 0.6177487969398499 \n",
      "     Validation Step: 22 Validation Loss: 0.6149800419807434 \n",
      "     Validation Step: 23 Validation Loss: 0.6141918897628784 \n",
      "     Validation Step: 24 Validation Loss: 0.6111955046653748 \n",
      "     Validation Step: 25 Validation Loss: 0.6186742782592773 \n",
      "     Validation Step: 26 Validation Loss: 0.6161178946495056 \n",
      "     Validation Step: 27 Validation Loss: 0.6075457334518433 \n",
      "     Validation Step: 28 Validation Loss: 0.6133894920349121 \n",
      "     Validation Step: 29 Validation Loss: 0.6154083609580994 \n",
      "     Validation Step: 30 Validation Loss: 0.6142851114273071 \n",
      "     Validation Step: 31 Validation Loss: 0.6137124300003052 \n",
      "     Validation Step: 32 Validation Loss: 0.6106995940208435 \n",
      "     Validation Step: 33 Validation Loss: 0.6158961057662964 \n",
      "     Validation Step: 34 Validation Loss: 0.6157281398773193 \n",
      "     Validation Step: 35 Validation Loss: 0.6182201504707336 \n",
      "     Validation Step: 36 Validation Loss: 0.614918053150177 \n",
      "     Validation Step: 37 Validation Loss: 0.613040030002594 \n",
      "     Validation Step: 38 Validation Loss: 0.6171683669090271 \n",
      "     Validation Step: 39 Validation Loss: 0.6183950901031494 \n",
      "     Validation Step: 40 Validation Loss: 0.612185537815094 \n",
      "     Validation Step: 41 Validation Loss: 0.6102049946784973 \n",
      "     Validation Step: 42 Validation Loss: 0.6112366914749146 \n",
      "     Validation Step: 43 Validation Loss: 0.6174495220184326 \n",
      "     Validation Step: 44 Validation Loss: 0.6143640279769897 \n",
      "Epoch: 31\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6170801520347595 \n",
      "     Training Step: 1 Training Loss: 0.6119252443313599 \n",
      "     Training Step: 2 Training Loss: 0.6095216870307922 \n",
      "     Training Step: 3 Training Loss: 0.6157610416412354 \n",
      "     Training Step: 4 Training Loss: 0.6155537366867065 \n",
      "     Training Step: 5 Training Loss: 0.612273633480072 \n",
      "     Training Step: 6 Training Loss: 0.6125196218490601 \n",
      "     Training Step: 7 Training Loss: 0.6149907112121582 \n",
      "     Training Step: 8 Training Loss: 0.6132081151008606 \n",
      "     Training Step: 9 Training Loss: 0.614723265171051 \n",
      "     Training Step: 10 Training Loss: 0.6147279143333435 \n",
      "     Training Step: 11 Training Loss: 0.6133814454078674 \n",
      "     Training Step: 12 Training Loss: 0.6128668189048767 \n",
      "     Training Step: 13 Training Loss: 0.6145064234733582 \n",
      "     Training Step: 14 Training Loss: 0.6166608333587646 \n",
      "     Training Step: 15 Training Loss: 0.6106432676315308 \n",
      "     Training Step: 16 Training Loss: 0.6139230728149414 \n",
      "     Training Step: 17 Training Loss: 0.6121737360954285 \n",
      "     Training Step: 18 Training Loss: 0.614406406879425 \n",
      "     Training Step: 19 Training Loss: 0.6187011003494263 \n",
      "     Training Step: 20 Training Loss: 0.6147720217704773 \n",
      "     Training Step: 21 Training Loss: 0.6131224632263184 \n",
      "     Training Step: 22 Training Loss: 0.6153990030288696 \n",
      "     Training Step: 23 Training Loss: 0.6151214241981506 \n",
      "     Training Step: 24 Training Loss: 0.6176876425743103 \n",
      "     Training Step: 25 Training Loss: 0.61149662733078 \n",
      "     Training Step: 26 Training Loss: 0.6145143508911133 \n",
      "     Training Step: 27 Training Loss: 0.6184984445571899 \n",
      "     Training Step: 28 Training Loss: 0.6123007535934448 \n",
      "     Training Step: 29 Training Loss: 0.6118040680885315 \n",
      "     Training Step: 30 Training Loss: 0.6125115156173706 \n",
      "     Training Step: 31 Training Loss: 0.6133358478546143 \n",
      "     Training Step: 32 Training Loss: 0.6142870783805847 \n",
      "     Training Step: 33 Training Loss: 0.6101213693618774 \n",
      "     Training Step: 34 Training Loss: 0.6179001331329346 \n",
      "     Training Step: 35 Training Loss: 0.6105882525444031 \n",
      "     Training Step: 36 Training Loss: 0.6122502088546753 \n",
      "     Training Step: 37 Training Loss: 0.6140316724777222 \n",
      "     Training Step: 38 Training Loss: 0.6128756403923035 \n",
      "     Training Step: 39 Training Loss: 0.6156143546104431 \n",
      "     Training Step: 40 Training Loss: 0.6121454834938049 \n",
      "     Training Step: 41 Training Loss: 0.620227575302124 \n",
      "     Training Step: 42 Training Loss: 0.6168147325515747 \n",
      "     Training Step: 43 Training Loss: 0.6115685701370239 \n",
      "     Training Step: 44 Training Loss: 0.6147280931472778 \n",
      "     Training Step: 45 Training Loss: 0.6150965690612793 \n",
      "     Training Step: 46 Training Loss: 0.6118791699409485 \n",
      "     Training Step: 47 Training Loss: 0.6177309155464172 \n",
      "     Training Step: 48 Training Loss: 0.6135306358337402 \n",
      "     Training Step: 49 Training Loss: 0.6141656637191772 \n",
      "     Training Step: 50 Training Loss: 0.6146929264068604 \n",
      "     Training Step: 51 Training Loss: 0.6116794347763062 \n",
      "     Training Step: 52 Training Loss: 0.6141676902770996 \n",
      "     Training Step: 53 Training Loss: 0.616038978099823 \n",
      "     Training Step: 54 Training Loss: 0.6114546656608582 \n",
      "     Training Step: 55 Training Loss: 0.6137576103210449 \n",
      "     Training Step: 56 Training Loss: 0.6168498992919922 \n",
      "     Training Step: 57 Training Loss: 0.6186343431472778 \n",
      "     Training Step: 58 Training Loss: 0.6174849271774292 \n",
      "     Training Step: 59 Training Loss: 0.6132489442825317 \n",
      "     Training Step: 60 Training Loss: 0.6172012090682983 \n",
      "     Training Step: 61 Training Loss: 0.615329384803772 \n",
      "     Training Step: 62 Training Loss: 0.6110532283782959 \n",
      "     Training Step: 63 Training Loss: 0.6122791767120361 \n",
      "     Training Step: 64 Training Loss: 0.611500084400177 \n",
      "     Training Step: 65 Training Loss: 0.61147141456604 \n",
      "     Training Step: 66 Training Loss: 0.6135956645011902 \n",
      "     Training Step: 67 Training Loss: 0.6148176789283752 \n",
      "     Training Step: 68 Training Loss: 0.612775981426239 \n",
      "     Training Step: 69 Training Loss: 0.6118865013122559 \n",
      "     Training Step: 70 Training Loss: 0.6153071522712708 \n",
      "     Training Step: 71 Training Loss: 0.6106861233711243 \n",
      "     Training Step: 72 Training Loss: 0.6105508804321289 \n",
      "     Training Step: 73 Training Loss: 0.616791307926178 \n",
      "     Training Step: 74 Training Loss: 0.613923192024231 \n",
      "     Training Step: 75 Training Loss: 0.6118786334991455 \n",
      "     Training Step: 76 Training Loss: 0.6129054427146912 \n",
      "     Training Step: 77 Training Loss: 0.6133222579956055 \n",
      "     Training Step: 78 Training Loss: 0.6184414029121399 \n",
      "     Training Step: 79 Training Loss: 0.6144721508026123 \n",
      "     Training Step: 80 Training Loss: 0.6126526594161987 \n",
      "     Training Step: 81 Training Loss: 0.6153830885887146 \n",
      "     Training Step: 82 Training Loss: 0.613141655921936 \n",
      "     Training Step: 83 Training Loss: 0.6101027727127075 \n",
      "     Training Step: 84 Training Loss: 0.6107186675071716 \n",
      "     Training Step: 85 Training Loss: 0.6180605888366699 \n",
      "     Training Step: 86 Training Loss: 0.6136406660079956 \n",
      "     Training Step: 87 Training Loss: 0.6138233542442322 \n",
      "     Training Step: 88 Training Loss: 0.6181073784828186 \n",
      "     Training Step: 89 Training Loss: 0.6129699945449829 \n",
      "     Training Step: 90 Training Loss: 0.6143950819969177 \n",
      "     Training Step: 91 Training Loss: 0.6132683157920837 \n",
      "     Training Step: 92 Training Loss: 0.6119760274887085 \n",
      "     Training Step: 93 Training Loss: 0.6179272532463074 \n",
      "     Training Step: 94 Training Loss: 0.6116518974304199 \n",
      "     Training Step: 95 Training Loss: 0.6094523668289185 \n",
      "     Training Step: 96 Training Loss: 0.6164944171905518 \n",
      "     Training Step: 97 Training Loss: 0.6155372262001038 \n",
      "     Training Step: 98 Training Loss: 0.6133852601051331 \n",
      "     Training Step: 99 Training Loss: 0.6167201399803162 \n",
      "     Training Step: 100 Training Loss: 0.6194972991943359 \n",
      "     Training Step: 101 Training Loss: 0.6181081533432007 \n",
      "     Training Step: 102 Training Loss: 0.610488772392273 \n",
      "     Training Step: 103 Training Loss: 0.6129897832870483 \n",
      "     Training Step: 104 Training Loss: 0.6133254170417786 \n",
      "     Training Step: 105 Training Loss: 0.6112699508666992 \n",
      "     Training Step: 106 Training Loss: 0.6098389625549316 \n",
      "     Training Step: 107 Training Loss: 0.6149722337722778 \n",
      "     Training Step: 108 Training Loss: 0.6142827272415161 \n",
      "     Training Step: 109 Training Loss: 0.6116767525672913 \n",
      "     Training Step: 110 Training Loss: 0.6168848276138306 \n",
      "     Training Step: 111 Training Loss: 0.6178008317947388 \n",
      "     Training Step: 112 Training Loss: 0.6156091690063477 \n",
      "     Training Step: 113 Training Loss: 0.6113407611846924 \n",
      "     Training Step: 114 Training Loss: 0.6147817969322205 \n",
      "     Training Step: 115 Training Loss: 0.6166950464248657 \n",
      "     Training Step: 116 Training Loss: 0.6136470437049866 \n",
      "     Training Step: 117 Training Loss: 0.6135169863700867 \n",
      "     Training Step: 118 Training Loss: 0.6146516799926758 \n",
      "     Training Step: 119 Training Loss: 0.6146904230117798 \n",
      "     Training Step: 120 Training Loss: 0.6150945425033569 \n",
      "     Training Step: 121 Training Loss: 0.6122323870658875 \n",
      "     Training Step: 122 Training Loss: 0.6167908906936646 \n",
      "     Training Step: 123 Training Loss: 0.6141052842140198 \n",
      "     Training Step: 124 Training Loss: 0.6101672053337097 \n",
      "     Training Step: 125 Training Loss: 0.6160021424293518 \n",
      "     Training Step: 126 Training Loss: 0.6159499287605286 \n",
      "     Training Step: 127 Training Loss: 0.615319013595581 \n",
      "     Training Step: 128 Training Loss: 0.615809977054596 \n",
      "     Training Step: 129 Training Loss: 0.6112576723098755 \n",
      "     Training Step: 130 Training Loss: 0.6155392527580261 \n",
      "     Training Step: 131 Training Loss: 0.6146734356880188 \n",
      "     Training Step: 132 Training Loss: 0.6121079921722412 \n",
      "     Training Step: 133 Training Loss: 0.6154699325561523 \n",
      "     Training Step: 134 Training Loss: 0.6189509630203247 \n",
      "     Training Step: 135 Training Loss: 0.6151959896087646 \n",
      "     Training Step: 136 Training Loss: 0.6140532493591309 \n",
      "     Training Step: 137 Training Loss: 0.6126031279563904 \n",
      "     Training Step: 138 Training Loss: 0.6174052357673645 \n",
      "     Training Step: 139 Training Loss: 0.6162583231925964 \n",
      "     Training Step: 140 Training Loss: 0.6123133897781372 \n",
      "     Training Step: 141 Training Loss: 0.615532636642456 \n",
      "     Training Step: 142 Training Loss: 0.6153045892715454 \n",
      "     Training Step: 143 Training Loss: 0.6160886883735657 \n",
      "     Training Step: 144 Training Loss: 0.613307774066925 \n",
      "     Training Step: 145 Training Loss: 0.6115933656692505 \n",
      "     Training Step: 146 Training Loss: 0.6171639561653137 \n",
      "     Training Step: 147 Training Loss: 0.6148473024368286 \n",
      "     Training Step: 148 Training Loss: 0.6197471022605896 \n",
      "     Training Step: 149 Training Loss: 0.6143984198570251 \n",
      "     Training Step: 150 Training Loss: 0.612488865852356 \n",
      "     Training Step: 151 Training Loss: 0.6162534356117249 \n",
      "     Training Step: 152 Training Loss: 0.6145437955856323 \n",
      "     Training Step: 153 Training Loss: 0.6209542751312256 \n",
      "     Training Step: 154 Training Loss: 0.6167330741882324 \n",
      "     Training Step: 155 Training Loss: 0.6157798171043396 \n",
      "     Training Step: 156 Training Loss: 0.6124692559242249 \n",
      "     Training Step: 157 Training Loss: 0.6177315711975098 \n",
      "     Training Step: 158 Training Loss: 0.6116505265235901 \n",
      "     Training Step: 159 Training Loss: 0.6125701665878296 \n",
      "     Training Step: 160 Training Loss: 0.6157296895980835 \n",
      "     Training Step: 161 Training Loss: 0.6114410758018494 \n",
      "     Training Step: 162 Training Loss: 0.6144046187400818 \n",
      "     Training Step: 163 Training Loss: 0.6161710619926453 \n",
      "     Training Step: 164 Training Loss: 0.6143125295639038 \n",
      "     Training Step: 165 Training Loss: 0.6141772270202637 \n",
      "     Training Step: 166 Training Loss: 0.614984393119812 \n",
      "     Training Step: 167 Training Loss: 0.6188874244689941 \n",
      "     Training Step: 168 Training Loss: 0.6107997894287109 \n",
      "     Training Step: 169 Training Loss: 0.616923451423645 \n",
      "     Training Step: 170 Training Loss: 0.6136985421180725 \n",
      "     Training Step: 171 Training Loss: 0.6118210554122925 \n",
      "     Training Step: 172 Training Loss: 0.6126766800880432 \n",
      "     Training Step: 173 Training Loss: 0.6123225092887878 \n",
      "     Training Step: 174 Training Loss: 0.6157982349395752 \n",
      "     Training Step: 175 Training Loss: 0.6129354238510132 \n",
      "     Training Step: 176 Training Loss: 0.6105390191078186 \n",
      "     Training Step: 177 Training Loss: 0.6126955151557922 \n",
      "     Training Step: 178 Training Loss: 0.6134241223335266 \n",
      "     Training Step: 179 Training Loss: 0.6124134659767151 \n",
      "     Training Step: 180 Training Loss: 0.6117289066314697 \n",
      "     Training Step: 181 Training Loss: 0.6165648102760315 \n",
      "     Training Step: 182 Training Loss: 0.616806149482727 \n",
      "     Training Step: 183 Training Loss: 0.6084319949150085 \n",
      "     Training Step: 184 Training Loss: 0.6146194934844971 \n",
      "     Training Step: 185 Training Loss: 0.6148626804351807 \n",
      "     Training Step: 186 Training Loss: 0.6130865216255188 \n",
      "     Training Step: 187 Training Loss: 0.617211639881134 \n",
      "     Training Step: 188 Training Loss: 0.6182702779769897 \n",
      "     Training Step: 189 Training Loss: 0.614945113658905 \n",
      "     Training Step: 190 Training Loss: 0.6117754578590393 \n",
      "     Training Step: 191 Training Loss: 0.613257110118866 \n",
      "     Training Step: 192 Training Loss: 0.6104802489280701 \n",
      "     Training Step: 193 Training Loss: 0.615442156791687 \n",
      "     Training Step: 194 Training Loss: 0.6103750467300415 \n",
      "     Training Step: 195 Training Loss: 0.6092146039009094 \n",
      "     Training Step: 196 Training Loss: 0.6096999645233154 \n",
      "     Training Step: 197 Training Loss: 0.618334174156189 \n",
      "     Training Step: 198 Training Loss: 0.613025963306427 \n",
      "     Training Step: 199 Training Loss: 0.6168031692504883 \n",
      "     Training Step: 200 Training Loss: 0.614551305770874 \n",
      "     Training Step: 201 Training Loss: 0.6116774082183838 \n",
      "     Training Step: 202 Training Loss: 0.6137872338294983 \n",
      "     Training Step: 203 Training Loss: 0.6199795603752136 \n",
      "     Training Step: 204 Training Loss: 0.6100960969924927 \n",
      "     Training Step: 205 Training Loss: 0.6147380471229553 \n",
      "     Training Step: 206 Training Loss: 0.6162630319595337 \n",
      "     Training Step: 207 Training Loss: 0.611689567565918 \n",
      "     Training Step: 208 Training Loss: 0.6165048480033875 \n",
      "     Training Step: 209 Training Loss: 0.6109858155250549 \n",
      "     Training Step: 210 Training Loss: 0.6182454824447632 \n",
      "     Training Step: 211 Training Loss: 0.609756588935852 \n",
      "     Training Step: 212 Training Loss: 0.6142842769622803 \n",
      "     Training Step: 213 Training Loss: 0.6185935735702515 \n",
      "     Training Step: 214 Training Loss: 0.6164814233779907 \n",
      "     Training Step: 215 Training Loss: 0.6135343909263611 \n",
      "     Training Step: 216 Training Loss: 0.6121706366539001 \n",
      "     Training Step: 217 Training Loss: 0.6167340874671936 \n",
      "     Training Step: 218 Training Loss: 0.6154584288597107 \n",
      "     Training Step: 219 Training Loss: 0.6148872375488281 \n",
      "     Training Step: 220 Training Loss: 0.6127952337265015 \n",
      "     Training Step: 221 Training Loss: 0.6106935739517212 \n",
      "     Training Step: 222 Training Loss: 0.6172360181808472 \n",
      "     Training Step: 223 Training Loss: 0.6107339859008789 \n",
      "     Training Step: 224 Training Loss: 0.6198055148124695 \n",
      "     Training Step: 225 Training Loss: 0.6139097809791565 \n",
      "     Training Step: 226 Training Loss: 0.6181082725524902 \n",
      "     Training Step: 227 Training Loss: 0.6102962493896484 \n",
      "     Training Step: 228 Training Loss: 0.6116265058517456 \n",
      "     Training Step: 229 Training Loss: 0.6167252063751221 \n",
      "     Training Step: 230 Training Loss: 0.6154186129570007 \n",
      "     Training Step: 231 Training Loss: 0.6154031753540039 \n",
      "     Training Step: 232 Training Loss: 0.6157564520835876 \n",
      "     Training Step: 233 Training Loss: 0.6171774864196777 \n",
      "     Training Step: 234 Training Loss: 0.6152776479721069 \n",
      "     Training Step: 235 Training Loss: 0.6137858033180237 \n",
      "     Training Step: 236 Training Loss: 0.6183000206947327 \n",
      "     Training Step: 237 Training Loss: 0.6140400171279907 \n",
      "     Training Step: 238 Training Loss: 0.6141120195388794 \n",
      "     Training Step: 239 Training Loss: 0.6106944680213928 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6146965026855469 \n",
      "     Validation Step: 1 Validation Loss: 0.6142606735229492 \n",
      "     Validation Step: 2 Validation Loss: 0.6183497309684753 \n",
      "     Validation Step: 3 Validation Loss: 0.6141688227653503 \n",
      "     Validation Step: 4 Validation Loss: 0.6177017092704773 \n",
      "     Validation Step: 5 Validation Loss: 0.615868091583252 \n",
      "     Validation Step: 6 Validation Loss: 0.6105469465255737 \n",
      "     Validation Step: 7 Validation Loss: 0.617128849029541 \n",
      "     Validation Step: 8 Validation Loss: 0.6121718287467957 \n",
      "     Validation Step: 9 Validation Loss: 0.6184407472610474 \n",
      "     Validation Step: 10 Validation Loss: 0.6111966371536255 \n",
      "     Validation Step: 11 Validation Loss: 0.6137109398841858 \n",
      "     Validation Step: 12 Validation Loss: 0.616346538066864 \n",
      "     Validation Step: 13 Validation Loss: 0.6130191087722778 \n",
      "     Validation Step: 14 Validation Loss: 0.6156883835792542 \n",
      "     Validation Step: 15 Validation Loss: 0.61861652135849 \n",
      "     Validation Step: 16 Validation Loss: 0.617785632610321 \n",
      "     Validation Step: 17 Validation Loss: 0.6174123287200928 \n",
      "     Validation Step: 18 Validation Loss: 0.6152991652488708 \n",
      "     Validation Step: 19 Validation Loss: 0.613726019859314 \n",
      "     Validation Step: 20 Validation Loss: 0.6119199395179749 \n",
      "     Validation Step: 21 Validation Loss: 0.6156712770462036 \n",
      "     Validation Step: 22 Validation Loss: 0.6143308281898499 \n",
      "     Validation Step: 23 Validation Loss: 0.6151072978973389 \n",
      "     Validation Step: 24 Validation Loss: 0.6146067976951599 \n",
      "     Validation Step: 25 Validation Loss: 0.6106937527656555 \n",
      "     Validation Step: 26 Validation Loss: 0.6153806447982788 \n",
      "     Validation Step: 27 Validation Loss: 0.614895761013031 \n",
      "     Validation Step: 28 Validation Loss: 0.614972710609436 \n",
      "     Validation Step: 29 Validation Loss: 0.6185948252677917 \n",
      "     Validation Step: 30 Validation Loss: 0.6128689646720886 \n",
      "     Validation Step: 31 Validation Loss: 0.6112183332443237 \n",
      "     Validation Step: 32 Validation Loss: 0.6146506667137146 \n",
      "     Validation Step: 33 Validation Loss: 0.6101462841033936 \n",
      "     Validation Step: 34 Validation Loss: 0.6136906147003174 \n",
      "     Validation Step: 35 Validation Loss: 0.6105309128761292 \n",
      "     Validation Step: 36 Validation Loss: 0.6116920113563538 \n",
      "     Validation Step: 37 Validation Loss: 0.6102339625358582 \n",
      "     Validation Step: 38 Validation Loss: 0.6160908937454224 \n",
      "     Validation Step: 39 Validation Loss: 0.611598789691925 \n",
      "     Validation Step: 40 Validation Loss: 0.6181714534759521 \n",
      "     Validation Step: 41 Validation Loss: 0.6101887226104736 \n",
      "     Validation Step: 42 Validation Loss: 0.6075534820556641 \n",
      "     Validation Step: 43 Validation Loss: 0.6133559346199036 \n",
      "     Validation Step: 44 Validation Loss: 0.6141794323921204 \n",
      "Epoch: 32\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6109098792076111 \n",
      "     Training Step: 1 Training Loss: 0.6115186810493469 \n",
      "     Training Step: 2 Training Loss: 0.6139275431632996 \n",
      "     Training Step: 3 Training Loss: 0.6168176531791687 \n",
      "     Training Step: 4 Training Loss: 0.6129918098449707 \n",
      "     Training Step: 5 Training Loss: 0.6123160719871521 \n",
      "     Training Step: 6 Training Loss: 0.6145015954971313 \n",
      "     Training Step: 7 Training Loss: 0.6197644472122192 \n",
      "     Training Step: 8 Training Loss: 0.6167340278625488 \n",
      "     Training Step: 9 Training Loss: 0.6144793033599854 \n",
      "     Training Step: 10 Training Loss: 0.6121940016746521 \n",
      "     Training Step: 11 Training Loss: 0.6209518909454346 \n",
      "     Training Step: 12 Training Loss: 0.6122556328773499 \n",
      "     Training Step: 13 Training Loss: 0.6146641969680786 \n",
      "     Training Step: 14 Training Loss: 0.614814817905426 \n",
      "     Training Step: 15 Training Loss: 0.6140806078910828 \n",
      "     Training Step: 16 Training Loss: 0.6157106161117554 \n",
      "     Training Step: 17 Training Loss: 0.6134751439094543 \n",
      "     Training Step: 18 Training Loss: 0.6184999346733093 \n",
      "     Training Step: 19 Training Loss: 0.6174065470695496 \n",
      "     Training Step: 20 Training Loss: 0.6130513548851013 \n",
      "     Training Step: 21 Training Loss: 0.6177098751068115 \n",
      "     Training Step: 22 Training Loss: 0.6137366890907288 \n",
      "     Training Step: 23 Training Loss: 0.6117345690727234 \n",
      "     Training Step: 24 Training Loss: 0.6122573018074036 \n",
      "     Training Step: 25 Training Loss: 0.6178498268127441 \n",
      "     Training Step: 26 Training Loss: 0.6179684996604919 \n",
      "     Training Step: 27 Training Loss: 0.6146945357322693 \n",
      "     Training Step: 28 Training Loss: 0.6116406917572021 \n",
      "     Training Step: 29 Training Loss: 0.6147339940071106 \n",
      "     Training Step: 30 Training Loss: 0.6178507208824158 \n",
      "     Training Step: 31 Training Loss: 0.6141884326934814 \n",
      "     Training Step: 32 Training Loss: 0.6142764687538147 \n",
      "     Training Step: 33 Training Loss: 0.6124956607818604 \n",
      "     Training Step: 34 Training Loss: 0.6120924353599548 \n",
      "     Training Step: 35 Training Loss: 0.6143690347671509 \n",
      "     Training Step: 36 Training Loss: 0.6133553385734558 \n",
      "     Training Step: 37 Training Loss: 0.6136246919631958 \n",
      "     Training Step: 38 Training Loss: 0.6126757264137268 \n",
      "     Training Step: 39 Training Loss: 0.6105673909187317 \n",
      "     Training Step: 40 Training Loss: 0.6143933534622192 \n",
      "     Training Step: 41 Training Loss: 0.6147322654724121 \n",
      "     Training Step: 42 Training Loss: 0.6153253316879272 \n",
      "     Training Step: 43 Training Loss: 0.6168399453163147 \n",
      "     Training Step: 44 Training Loss: 0.6125931143760681 \n",
      "     Training Step: 45 Training Loss: 0.6133363246917725 \n",
      "     Training Step: 46 Training Loss: 0.6115232110023499 \n",
      "     Training Step: 47 Training Loss: 0.6114769577980042 \n",
      "     Training Step: 48 Training Loss: 0.613935112953186 \n",
      "     Training Step: 49 Training Loss: 0.6163963675498962 \n",
      "     Training Step: 50 Training Loss: 0.6133123636245728 \n",
      "     Training Step: 51 Training Loss: 0.6111652255058289 \n",
      "     Training Step: 52 Training Loss: 0.6124272346496582 \n",
      "     Training Step: 53 Training Loss: 0.618057906627655 \n",
      "     Training Step: 54 Training Loss: 0.609818696975708 \n",
      "     Training Step: 55 Training Loss: 0.6129587888717651 \n",
      "     Training Step: 56 Training Loss: 0.6165471076965332 \n",
      "     Training Step: 57 Training Loss: 0.6146645545959473 \n",
      "     Training Step: 58 Training Loss: 0.6153539419174194 \n",
      "     Training Step: 59 Training Loss: 0.6171880960464478 \n",
      "     Training Step: 60 Training Loss: 0.6155814528465271 \n",
      "     Training Step: 61 Training Loss: 0.6123637557029724 \n",
      "     Training Step: 62 Training Loss: 0.6182219386100769 \n",
      "     Training Step: 63 Training Loss: 0.6136793494224548 \n",
      "     Training Step: 64 Training Loss: 0.6144022941589355 \n",
      "     Training Step: 65 Training Loss: 0.610253632068634 \n",
      "     Training Step: 66 Training Loss: 0.6162770390510559 \n",
      "     Training Step: 67 Training Loss: 0.6106314659118652 \n",
      "     Training Step: 68 Training Loss: 0.6158319115638733 \n",
      "     Training Step: 69 Training Loss: 0.6181418895721436 \n",
      "     Training Step: 70 Training Loss: 0.614814281463623 \n",
      "     Training Step: 71 Training Loss: 0.615979015827179 \n",
      "     Training Step: 72 Training Loss: 0.6141865253448486 \n",
      "     Training Step: 73 Training Loss: 0.6135237812995911 \n",
      "     Training Step: 74 Training Loss: 0.6116622090339661 \n",
      "     Training Step: 75 Training Loss: 0.6083135008811951 \n",
      "     Training Step: 76 Training Loss: 0.6154417395591736 \n",
      "     Training Step: 77 Training Loss: 0.6158900260925293 \n",
      "     Training Step: 78 Training Loss: 0.6114707589149475 \n",
      "     Training Step: 79 Training Loss: 0.6203542947769165 \n",
      "     Training Step: 80 Training Loss: 0.6136212944984436 \n",
      "     Training Step: 81 Training Loss: 0.61432284116745 \n",
      "     Training Step: 82 Training Loss: 0.6145815849304199 \n",
      "     Training Step: 83 Training Loss: 0.6102920174598694 \n",
      "     Training Step: 84 Training Loss: 0.6129547953605652 \n",
      "     Training Step: 85 Training Loss: 0.6104656457901001 \n",
      "     Training Step: 86 Training Loss: 0.6143419742584229 \n",
      "     Training Step: 87 Training Loss: 0.6118488907814026 \n",
      "     Training Step: 88 Training Loss: 0.6151854395866394 \n",
      "     Training Step: 89 Training Loss: 0.6195464134216309 \n",
      "     Training Step: 90 Training Loss: 0.6165549159049988 \n",
      "     Training Step: 91 Training Loss: 0.6164388656616211 \n",
      "     Training Step: 92 Training Loss: 0.6161776185035706 \n",
      "     Training Step: 93 Training Loss: 0.6182405352592468 \n",
      "     Training Step: 94 Training Loss: 0.6166438460350037 \n",
      "     Training Step: 95 Training Loss: 0.6144126653671265 \n",
      "     Training Step: 96 Training Loss: 0.613806426525116 \n",
      "     Training Step: 97 Training Loss: 0.6101141571998596 \n",
      "     Training Step: 98 Training Loss: 0.6112560629844666 \n",
      "     Training Step: 99 Training Loss: 0.6133338809013367 \n",
      "     Training Step: 100 Training Loss: 0.6122286319732666 \n",
      "     Training Step: 101 Training Loss: 0.6129229068756104 \n",
      "     Training Step: 102 Training Loss: 0.6170483231544495 \n",
      "     Training Step: 103 Training Loss: 0.612885057926178 \n",
      "     Training Step: 104 Training Loss: 0.6141541600227356 \n",
      "     Training Step: 105 Training Loss: 0.6095986366271973 \n",
      "     Training Step: 106 Training Loss: 0.6098650693893433 \n",
      "     Training Step: 107 Training Loss: 0.6106330752372742 \n",
      "     Training Step: 108 Training Loss: 0.6157000660896301 \n",
      "     Training Step: 109 Training Loss: 0.6158311367034912 \n",
      "     Training Step: 110 Training Loss: 0.6132484674453735 \n",
      "     Training Step: 111 Training Loss: 0.6184149384498596 \n",
      "     Training Step: 112 Training Loss: 0.6148440837860107 \n",
      "     Training Step: 113 Training Loss: 0.6158568263053894 \n",
      "     Training Step: 114 Training Loss: 0.6195762753486633 \n",
      "     Training Step: 115 Training Loss: 0.6134078502655029 \n",
      "     Training Step: 116 Training Loss: 0.6147091388702393 \n",
      "     Training Step: 117 Training Loss: 0.6153197288513184 \n",
      "     Training Step: 118 Training Loss: 0.613735020160675 \n",
      "     Training Step: 119 Training Loss: 0.6147089004516602 \n",
      "     Training Step: 120 Training Loss: 0.6161590218544006 \n",
      "     Training Step: 121 Training Loss: 0.6167361736297607 \n",
      "     Training Step: 122 Training Loss: 0.6150152087211609 \n",
      "     Training Step: 123 Training Loss: 0.6141687035560608 \n",
      "     Training Step: 124 Training Loss: 0.6117505431175232 \n",
      "     Training Step: 125 Training Loss: 0.6118699908256531 \n",
      "     Training Step: 126 Training Loss: 0.6105486750602722 \n",
      "     Training Step: 127 Training Loss: 0.6153310537338257 \n",
      "     Training Step: 128 Training Loss: 0.6114267706871033 \n",
      "     Training Step: 129 Training Loss: 0.6132333278656006 \n",
      "     Training Step: 130 Training Loss: 0.6173277497291565 \n",
      "     Training Step: 131 Training Loss: 0.6115317344665527 \n",
      "     Training Step: 132 Training Loss: 0.6164869070053101 \n",
      "     Training Step: 133 Training Loss: 0.6139627695083618 \n",
      "     Training Step: 134 Training Loss: 0.610752284526825 \n",
      "     Training Step: 135 Training Loss: 0.6189429759979248 \n",
      "     Training Step: 136 Training Loss: 0.6154037714004517 \n",
      "     Training Step: 137 Training Loss: 0.6167235970497131 \n",
      "     Training Step: 138 Training Loss: 0.6185494661331177 \n",
      "     Training Step: 139 Training Loss: 0.6167088150978088 \n",
      "     Training Step: 140 Training Loss: 0.6177391409873962 \n",
      "     Training Step: 141 Training Loss: 0.6150466203689575 \n",
      "     Training Step: 142 Training Loss: 0.6111657023429871 \n",
      "     Training Step: 143 Training Loss: 0.6154722571372986 \n",
      "     Training Step: 144 Training Loss: 0.6190440058708191 \n",
      "     Training Step: 145 Training Loss: 0.6125722527503967 \n",
      "     Training Step: 146 Training Loss: 0.612395703792572 \n",
      "     Training Step: 147 Training Loss: 0.6154747605323792 \n",
      "     Training Step: 148 Training Loss: 0.6119413375854492 \n",
      "     Training Step: 149 Training Loss: 0.6168386936187744 \n",
      "     Training Step: 150 Training Loss: 0.6144043207168579 \n",
      "     Training Step: 151 Training Loss: 0.6118923425674438 \n",
      "     Training Step: 152 Training Loss: 0.6168784499168396 \n",
      "     Training Step: 153 Training Loss: 0.6118623614311218 \n",
      "     Training Step: 154 Training Loss: 0.6116600036621094 \n",
      "     Training Step: 155 Training Loss: 0.6140590310096741 \n",
      "     Training Step: 156 Training Loss: 0.6133835911750793 \n",
      "     Training Step: 157 Training Loss: 0.6101059913635254 \n",
      "     Training Step: 158 Training Loss: 0.6092667579650879 \n",
      "     Training Step: 159 Training Loss: 0.6152185201644897 \n",
      "     Training Step: 160 Training Loss: 0.6147454977035522 \n",
      "     Training Step: 161 Training Loss: 0.6131002902984619 \n",
      "     Training Step: 162 Training Loss: 0.6167667508125305 \n",
      "     Training Step: 163 Training Loss: 0.6117261648178101 \n",
      "     Training Step: 164 Training Loss: 0.6147430539131165 \n",
      "     Training Step: 165 Training Loss: 0.6128280162811279 \n",
      "     Training Step: 166 Training Loss: 0.6147799491882324 \n",
      "     Training Step: 167 Training Loss: 0.6111981272697449 \n",
      "     Training Step: 168 Training Loss: 0.6172598004341125 \n",
      "     Training Step: 169 Training Loss: 0.6117036938667297 \n",
      "     Training Step: 170 Training Loss: 0.6148293018341064 \n",
      "     Training Step: 171 Training Loss: 0.6123639345169067 \n",
      "     Training Step: 172 Training Loss: 0.6138154864311218 \n",
      "     Training Step: 173 Training Loss: 0.6132457852363586 \n",
      "     Training Step: 174 Training Loss: 0.6154234409332275 \n",
      "     Training Step: 175 Training Loss: 0.611591100692749 \n",
      "     Training Step: 176 Training Loss: 0.6181041598320007 \n",
      "     Training Step: 177 Training Loss: 0.6123589873313904 \n",
      "     Training Step: 178 Training Loss: 0.6199843883514404 \n",
      "     Training Step: 179 Training Loss: 0.6149687170982361 \n",
      "     Training Step: 180 Training Loss: 0.6116760969161987 \n",
      "     Training Step: 181 Training Loss: 0.6150816082954407 \n",
      "     Training Step: 182 Training Loss: 0.6107845306396484 \n",
      "     Training Step: 183 Training Loss: 0.6101447939872742 \n",
      "     Training Step: 184 Training Loss: 0.6146084666252136 \n",
      "     Training Step: 185 Training Loss: 0.615864098072052 \n",
      "     Training Step: 186 Training Loss: 0.6154337525367737 \n",
      "     Training Step: 187 Training Loss: 0.6129088997840881 \n",
      "     Training Step: 188 Training Loss: 0.6171419620513916 \n",
      "     Training Step: 189 Training Loss: 0.6177842020988464 \n",
      "     Training Step: 190 Training Loss: 0.6155897974967957 \n",
      "     Training Step: 191 Training Loss: 0.6095377802848816 \n",
      "     Training Step: 192 Training Loss: 0.6158872246742249 \n",
      "     Training Step: 193 Training Loss: 0.6128847599029541 \n",
      "     Training Step: 194 Training Loss: 0.6153408885002136 \n",
      "     Training Step: 195 Training Loss: 0.616895318031311 \n",
      "     Training Step: 196 Training Loss: 0.6139098405838013 \n",
      "     Training Step: 197 Training Loss: 0.6124883890151978 \n",
      "     Training Step: 198 Training Loss: 0.6154600381851196 \n",
      "     Training Step: 199 Training Loss: 0.6107015609741211 \n",
      "     Training Step: 200 Training Loss: 0.6115835905075073 \n",
      "     Training Step: 201 Training Loss: 0.6132169961929321 \n",
      "     Training Step: 202 Training Loss: 0.6100834608078003 \n",
      "     Training Step: 203 Training Loss: 0.6118518710136414 \n",
      "     Training Step: 204 Training Loss: 0.6135995388031006 \n",
      "     Training Step: 205 Training Loss: 0.6164510846138 \n",
      "     Training Step: 206 Training Loss: 0.6105133295059204 \n",
      "     Training Step: 207 Training Loss: 0.6099194288253784 \n",
      "     Training Step: 208 Training Loss: 0.6120803952217102 \n",
      "     Training Step: 209 Training Loss: 0.6107412576675415 \n",
      "     Training Step: 210 Training Loss: 0.6189122796058655 \n",
      "     Training Step: 211 Training Loss: 0.6186319589614868 \n",
      "     Training Step: 212 Training Loss: 0.610713541507721 \n",
      "     Training Step: 213 Training Loss: 0.6176016330718994 \n",
      "     Training Step: 214 Training Loss: 0.6168026924133301 \n",
      "     Training Step: 215 Training Loss: 0.6137714982032776 \n",
      "     Training Step: 216 Training Loss: 0.6169198751449585 \n",
      "     Training Step: 217 Training Loss: 0.612657904624939 \n",
      "     Training Step: 218 Training Loss: 0.6185256242752075 \n",
      "     Training Step: 219 Training Loss: 0.6125669479370117 \n",
      "     Training Step: 220 Training Loss: 0.6156951189041138 \n",
      "     Training Step: 221 Training Loss: 0.615527868270874 \n",
      "     Training Step: 222 Training Loss: 0.6181010603904724 \n",
      "     Training Step: 223 Training Loss: 0.6132493019104004 \n",
      "     Training Step: 224 Training Loss: 0.6172145009040833 \n",
      "     Training Step: 225 Training Loss: 0.6166574954986572 \n",
      "     Training Step: 226 Training Loss: 0.615450918674469 \n",
      "     Training Step: 227 Training Loss: 0.6107242703437805 \n",
      "     Training Step: 228 Training Loss: 0.6130997538566589 \n",
      "     Training Step: 229 Training Loss: 0.6161088347434998 \n",
      "     Training Step: 230 Training Loss: 0.6149154305458069 \n",
      "     Training Step: 231 Training Loss: 0.6152178049087524 \n",
      "     Training Step: 232 Training Loss: 0.6175248622894287 \n",
      "     Training Step: 233 Training Loss: 0.6145516633987427 \n",
      "     Training Step: 234 Training Loss: 0.6138147711753845 \n",
      "     Training Step: 235 Training Loss: 0.6146503686904907 \n",
      "     Training Step: 236 Training Loss: 0.6140750646591187 \n",
      "     Training Step: 237 Training Loss: 0.6128000020980835 \n",
      "     Training Step: 238 Training Loss: 0.6118852496147156 \n",
      "     Training Step: 239 Training Loss: 0.6122241020202637 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6178269386291504 \n",
      "     Validation Step: 1 Validation Loss: 0.615915060043335 \n",
      "     Validation Step: 2 Validation Loss: 0.6143104434013367 \n",
      "     Validation Step: 3 Validation Loss: 0.6101171374320984 \n",
      "     Validation Step: 4 Validation Loss: 0.615300178527832 \n",
      "     Validation Step: 5 Validation Loss: 0.6163784265518188 \n",
      "     Validation Step: 6 Validation Loss: 0.6116519570350647 \n",
      "     Validation Step: 7 Validation Loss: 0.6174076199531555 \n",
      "     Validation Step: 8 Validation Loss: 0.6145921945571899 \n",
      "     Validation Step: 9 Validation Loss: 0.613650381565094 \n",
      "     Validation Step: 10 Validation Loss: 0.6149382591247559 \n",
      "     Validation Step: 11 Validation Loss: 0.6186566352844238 \n",
      "     Validation Step: 12 Validation Loss: 0.6121419668197632 \n",
      "     Validation Step: 13 Validation Loss: 0.6106330156326294 \n",
      "     Validation Step: 14 Validation Loss: 0.6101608872413635 \n",
      "     Validation Step: 15 Validation Loss: 0.6146382093429565 \n",
      "     Validation Step: 16 Validation Loss: 0.6185076236724854 \n",
      "     Validation Step: 17 Validation Loss: 0.6151536107063293 \n",
      "     Validation Step: 18 Validation Loss: 0.6130296587944031 \n",
      "     Validation Step: 19 Validation Loss: 0.6186151504516602 \n",
      "     Validation Step: 20 Validation Loss: 0.6142759919166565 \n",
      "     Validation Step: 21 Validation Loss: 0.6184159517288208 \n",
      "     Validation Step: 22 Validation Loss: 0.6161434650421143 \n",
      "     Validation Step: 23 Validation Loss: 0.6149001121520996 \n",
      "     Validation Step: 24 Validation Loss: 0.6141402721405029 \n",
      "     Validation Step: 25 Validation Loss: 0.613760769367218 \n",
      "     Validation Step: 26 Validation Loss: 0.6111953258514404 \n",
      "     Validation Step: 27 Validation Loss: 0.6157171726226807 \n",
      "     Validation Step: 28 Validation Loss: 0.6115727424621582 \n",
      "     Validation Step: 29 Validation Loss: 0.6118828654289246 \n",
      "     Validation Step: 30 Validation Loss: 0.6137346625328064 \n",
      "     Validation Step: 31 Validation Loss: 0.6133496165275574 \n",
      "     Validation Step: 32 Validation Loss: 0.6142005920410156 \n",
      "     Validation Step: 33 Validation Loss: 0.6111757755279541 \n",
      "     Validation Step: 34 Validation Loss: 0.6105005145072937 \n",
      "     Validation Step: 35 Validation Loss: 0.6128350496292114 \n",
      "     Validation Step: 36 Validation Loss: 0.61821049451828 \n",
      "     Validation Step: 37 Validation Loss: 0.6177090406417847 \n",
      "     Validation Step: 38 Validation Loss: 0.6101461052894592 \n",
      "     Validation Step: 39 Validation Loss: 0.6074916124343872 \n",
      "     Validation Step: 40 Validation Loss: 0.6105306148529053 \n",
      "     Validation Step: 41 Validation Loss: 0.6171373724937439 \n",
      "     Validation Step: 42 Validation Loss: 0.6147310137748718 \n",
      "     Validation Step: 43 Validation Loss: 0.6154112815856934 \n",
      "     Validation Step: 44 Validation Loss: 0.6157177686691284 \n",
      "Epoch: 33\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6140502691268921 \n",
      "     Training Step: 1 Training Loss: 0.613773763179779 \n",
      "     Training Step: 2 Training Loss: 0.6162948608398438 \n",
      "     Training Step: 3 Training Loss: 0.611900269985199 \n",
      "     Training Step: 4 Training Loss: 0.6147393584251404 \n",
      "     Training Step: 5 Training Loss: 0.618042528629303 \n",
      "     Training Step: 6 Training Loss: 0.6160982847213745 \n",
      "     Training Step: 7 Training Loss: 0.6132453680038452 \n",
      "     Training Step: 8 Training Loss: 0.613381028175354 \n",
      "     Training Step: 9 Training Loss: 0.6148144006729126 \n",
      "     Training Step: 10 Training Loss: 0.6118159294128418 \n",
      "     Training Step: 11 Training Loss: 0.6136711239814758 \n",
      "     Training Step: 12 Training Loss: 0.6116503477096558 \n",
      "     Training Step: 13 Training Loss: 0.6094067692756653 \n",
      "     Training Step: 14 Training Loss: 0.6133190989494324 \n",
      "     Training Step: 15 Training Loss: 0.6144658923149109 \n",
      "     Training Step: 16 Training Loss: 0.6154877543449402 \n",
      "     Training Step: 17 Training Loss: 0.6114999055862427 \n",
      "     Training Step: 18 Training Loss: 0.615716814994812 \n",
      "     Training Step: 19 Training Loss: 0.6149801015853882 \n",
      "     Training Step: 20 Training Loss: 0.6112421154975891 \n",
      "     Training Step: 21 Training Loss: 0.6139370799064636 \n",
      "     Training Step: 22 Training Loss: 0.6154252290725708 \n",
      "     Training Step: 23 Training Loss: 0.6118524670600891 \n",
      "     Training Step: 24 Training Loss: 0.6117169857025146 \n",
      "     Training Step: 25 Training Loss: 0.6100693345069885 \n",
      "     Training Step: 26 Training Loss: 0.6121867895126343 \n",
      "     Training Step: 27 Training Loss: 0.6147657632827759 \n",
      "     Training Step: 28 Training Loss: 0.6094503998756409 \n",
      "     Training Step: 29 Training Loss: 0.614388644695282 \n",
      "     Training Step: 30 Training Loss: 0.6149673461914062 \n",
      "     Training Step: 31 Training Loss: 0.6153962016105652 \n",
      "     Training Step: 32 Training Loss: 0.6168337464332581 \n",
      "     Training Step: 33 Training Loss: 0.6171594858169556 \n",
      "     Training Step: 34 Training Loss: 0.6123384833335876 \n",
      "     Training Step: 35 Training Loss: 0.6146731376647949 \n",
      "     Training Step: 36 Training Loss: 0.6152007579803467 \n",
      "     Training Step: 37 Training Loss: 0.616812527179718 \n",
      "     Training Step: 38 Training Loss: 0.6149618625640869 \n",
      "     Training Step: 39 Training Loss: 0.6133663654327393 \n",
      "     Training Step: 40 Training Loss: 0.6110024452209473 \n",
      "     Training Step: 41 Training Loss: 0.6153106689453125 \n",
      "     Training Step: 42 Training Loss: 0.6155005693435669 \n",
      "     Training Step: 43 Training Loss: 0.6129424571990967 \n",
      "     Training Step: 44 Training Loss: 0.6118555665016174 \n",
      "     Training Step: 45 Training Loss: 0.6117834448814392 \n",
      "     Training Step: 46 Training Loss: 0.6168802976608276 \n",
      "     Training Step: 47 Training Loss: 0.6107260584831238 \n",
      "     Training Step: 48 Training Loss: 0.6140751838684082 \n",
      "     Training Step: 49 Training Loss: 0.6136295199394226 \n",
      "     Training Step: 50 Training Loss: 0.6194998025894165 \n",
      "     Training Step: 51 Training Loss: 0.6101670861244202 \n",
      "     Training Step: 52 Training Loss: 0.6153574585914612 \n",
      "     Training Step: 53 Training Loss: 0.6104517579078674 \n",
      "     Training Step: 54 Training Loss: 0.6133311986923218 \n",
      "     Training Step: 55 Training Loss: 0.6118484735488892 \n",
      "     Training Step: 56 Training Loss: 0.6123290061950684 \n",
      "     Training Step: 57 Training Loss: 0.6134876608848572 \n",
      "     Training Step: 58 Training Loss: 0.614192545413971 \n",
      "     Training Step: 59 Training Loss: 0.6147955656051636 \n",
      "     Training Step: 60 Training Loss: 0.6115114688873291 \n",
      "     Training Step: 61 Training Loss: 0.6106284856796265 \n",
      "     Training Step: 62 Training Loss: 0.61307293176651 \n",
      "     Training Step: 63 Training Loss: 0.6082659363746643 \n",
      "     Training Step: 64 Training Loss: 0.6159347295761108 \n",
      "     Training Step: 65 Training Loss: 0.6133986115455627 \n",
      "     Training Step: 66 Training Loss: 0.6125497221946716 \n",
      "     Training Step: 67 Training Loss: 0.6153613924980164 \n",
      "     Training Step: 68 Training Loss: 0.6167182326316833 \n",
      "     Training Step: 69 Training Loss: 0.6171589493751526 \n",
      "     Training Step: 70 Training Loss: 0.6133314371109009 \n",
      "     Training Step: 71 Training Loss: 0.6101779341697693 \n",
      "     Training Step: 72 Training Loss: 0.6115959882736206 \n",
      "     Training Step: 73 Training Loss: 0.6168297529220581 \n",
      "     Training Step: 74 Training Loss: 0.614820122718811 \n",
      "     Training Step: 75 Training Loss: 0.6121585369110107 \n",
      "     Training Step: 76 Training Loss: 0.6186724901199341 \n",
      "     Training Step: 77 Training Loss: 0.6099127531051636 \n",
      "     Training Step: 78 Training Loss: 0.6155602931976318 \n",
      "     Training Step: 79 Training Loss: 0.6129092574119568 \n",
      "     Training Step: 80 Training Loss: 0.6135192513465881 \n",
      "     Training Step: 81 Training Loss: 0.6200059652328491 \n",
      "     Training Step: 82 Training Loss: 0.6114684343338013 \n",
      "     Training Step: 83 Training Loss: 0.6126789450645447 \n",
      "     Training Step: 84 Training Loss: 0.6164364814758301 \n",
      "     Training Step: 85 Training Loss: 0.618110716342926 \n",
      "     Training Step: 86 Training Loss: 0.6098617911338806 \n",
      "     Training Step: 87 Training Loss: 0.6167345643043518 \n",
      "     Training Step: 88 Training Loss: 0.6152517795562744 \n",
      "     Training Step: 89 Training Loss: 0.6129823923110962 \n",
      "     Training Step: 90 Training Loss: 0.6153459548950195 \n",
      "     Training Step: 91 Training Loss: 0.6128280758857727 \n",
      "     Training Step: 92 Training Loss: 0.6106794476509094 \n",
      "     Training Step: 93 Training Loss: 0.6146567463874817 \n",
      "     Training Step: 94 Training Loss: 0.6144474744796753 \n",
      "     Training Step: 95 Training Loss: 0.6105939149856567 \n",
      "     Training Step: 96 Training Loss: 0.6178624033927917 \n",
      "     Training Step: 97 Training Loss: 0.6174326539039612 \n",
      "     Training Step: 98 Training Loss: 0.6104869246482849 \n",
      "     Training Step: 99 Training Loss: 0.6146334409713745 \n",
      "     Training Step: 100 Training Loss: 0.6154800057411194 \n",
      "     Training Step: 101 Training Loss: 0.6129312515258789 \n",
      "     Training Step: 102 Training Loss: 0.611503541469574 \n",
      "     Training Step: 103 Training Loss: 0.6137511134147644 \n",
      "     Training Step: 104 Training Loss: 0.6186676621437073 \n",
      "     Training Step: 105 Training Loss: 0.6167552471160889 \n",
      "     Training Step: 106 Training Loss: 0.6166523694992065 \n",
      "     Training Step: 107 Training Loss: 0.6131503582000732 \n",
      "     Training Step: 108 Training Loss: 0.615980327129364 \n",
      "     Training Step: 109 Training Loss: 0.6124736666679382 \n",
      "     Training Step: 110 Training Loss: 0.6129449605941772 \n",
      "     Training Step: 111 Training Loss: 0.6101517081260681 \n",
      "     Training Step: 112 Training Loss: 0.6146124005317688 \n",
      "     Training Step: 113 Training Loss: 0.6121575832366943 \n",
      "     Training Step: 114 Training Loss: 0.6182748675346375 \n",
      "     Training Step: 115 Training Loss: 0.6167812347412109 \n",
      "     Training Step: 116 Training Loss: 0.6149774193763733 \n",
      "     Training Step: 117 Training Loss: 0.6175554394721985 \n",
      "     Training Step: 118 Training Loss: 0.6134448647499084 \n",
      "     Training Step: 119 Training Loss: 0.6110362410545349 \n",
      "     Training Step: 120 Training Loss: 0.6137322187423706 \n",
      "     Training Step: 121 Training Loss: 0.6141366958618164 \n",
      "     Training Step: 122 Training Loss: 0.6155641078948975 \n",
      "     Training Step: 123 Training Loss: 0.6128376722335815 \n",
      "     Training Step: 124 Training Loss: 0.6151140928268433 \n",
      "     Training Step: 125 Training Loss: 0.6147410273551941 \n",
      "     Training Step: 126 Training Loss: 0.6125228404998779 \n",
      "     Training Step: 127 Training Loss: 0.6184136867523193 \n",
      "     Training Step: 128 Training Loss: 0.6164451241493225 \n",
      "     Training Step: 129 Training Loss: 0.6142336130142212 \n",
      "     Training Step: 130 Training Loss: 0.6148278117179871 \n",
      "     Training Step: 131 Training Loss: 0.6182861924171448 \n",
      "     Training Step: 132 Training Loss: 0.6155511140823364 \n",
      "     Training Step: 133 Training Loss: 0.6160179972648621 \n",
      "     Training Step: 134 Training Loss: 0.6177470088005066 \n",
      "     Training Step: 135 Training Loss: 0.6183639168739319 \n",
      "     Training Step: 136 Training Loss: 0.6104016304016113 \n",
      "     Training Step: 137 Training Loss: 0.6166642904281616 \n",
      "     Training Step: 138 Training Loss: 0.6141656041145325 \n",
      "     Training Step: 139 Training Loss: 0.611198902130127 \n",
      "     Training Step: 140 Training Loss: 0.613169252872467 \n",
      "     Training Step: 141 Training Loss: 0.6168870329856873 \n",
      "     Training Step: 142 Training Loss: 0.6155650615692139 \n",
      "     Training Step: 143 Training Loss: 0.618478536605835 \n",
      "     Training Step: 144 Training Loss: 0.6178013682365417 \n",
      "     Training Step: 145 Training Loss: 0.6140381097793579 \n",
      "     Training Step: 146 Training Loss: 0.6135762929916382 \n",
      "     Training Step: 147 Training Loss: 0.6177923679351807 \n",
      "     Training Step: 148 Training Loss: 0.6158282160758972 \n",
      "     Training Step: 149 Training Loss: 0.6170003414154053 \n",
      "     Training Step: 150 Training Loss: 0.6147155165672302 \n",
      "     Training Step: 151 Training Loss: 0.6144958138465881 \n",
      "     Training Step: 152 Training Loss: 0.6163783669471741 \n",
      "     Training Step: 153 Training Loss: 0.6162879467010498 \n",
      "     Training Step: 154 Training Loss: 0.6177037358283997 \n",
      "     Training Step: 155 Training Loss: 0.6151158809661865 \n",
      "     Training Step: 156 Training Loss: 0.6197122931480408 \n",
      "     Training Step: 157 Training Loss: 0.6126804947853088 \n",
      "     Training Step: 158 Training Loss: 0.6188188195228577 \n",
      "     Training Step: 159 Training Loss: 0.6125286817550659 \n",
      "     Training Step: 160 Training Loss: 0.6115781664848328 \n",
      "     Training Step: 161 Training Loss: 0.6136371493339539 \n",
      "     Training Step: 162 Training Loss: 0.6159818768501282 \n",
      "     Training Step: 163 Training Loss: 0.6122724413871765 \n",
      "     Training Step: 164 Training Loss: 0.6183146834373474 \n",
      "     Training Step: 165 Training Loss: 0.613214373588562 \n",
      "     Training Step: 166 Training Loss: 0.6151304244995117 \n",
      "     Training Step: 167 Training Loss: 0.6107202172279358 \n",
      "     Training Step: 168 Training Loss: 0.6114819645881653 \n",
      "     Training Step: 169 Training Loss: 0.6161946058273315 \n",
      "     Training Step: 170 Training Loss: 0.6158224940299988 \n",
      "     Training Step: 171 Training Loss: 0.6116440296173096 \n",
      "     Training Step: 172 Training Loss: 0.614139974117279 \n",
      "     Training Step: 173 Training Loss: 0.6146190166473389 \n",
      "     Training Step: 174 Training Loss: 0.610659122467041 \n",
      "     Training Step: 175 Training Loss: 0.6178252696990967 \n",
      "     Training Step: 176 Training Loss: 0.6137763261795044 \n",
      "     Training Step: 177 Training Loss: 0.6107357144355774 \n",
      "     Training Step: 178 Training Loss: 0.6125787496566772 \n",
      "     Training Step: 179 Training Loss: 0.6152120232582092 \n",
      "     Training Step: 180 Training Loss: 0.6146752834320068 \n",
      "     Training Step: 181 Training Loss: 0.6162395477294922 \n",
      "     Training Step: 182 Training Loss: 0.6168321967124939 \n",
      "     Training Step: 183 Training Loss: 0.6128199100494385 \n",
      "     Training Step: 184 Training Loss: 0.6147964000701904 \n",
      "     Training Step: 185 Training Loss: 0.6128429174423218 \n",
      "     Training Step: 186 Training Loss: 0.6189296841621399 \n",
      "     Training Step: 187 Training Loss: 0.6123203039169312 \n",
      "     Training Step: 188 Training Loss: 0.6181552410125732 \n",
      "     Training Step: 189 Training Loss: 0.6106171607971191 \n",
      "     Training Step: 190 Training Loss: 0.617096483707428 \n",
      "     Training Step: 191 Training Loss: 0.6155077219009399 \n",
      "     Training Step: 192 Training Loss: 0.6141926050186157 \n",
      "     Training Step: 193 Training Loss: 0.6171958446502686 \n",
      "     Training Step: 194 Training Loss: 0.6120826005935669 \n",
      "     Training Step: 195 Training Loss: 0.6135444045066833 \n",
      "     Training Step: 196 Training Loss: 0.613961398601532 \n",
      "     Training Step: 197 Training Loss: 0.6144313216209412 \n",
      "     Training Step: 198 Training Loss: 0.6197605729103088 \n",
      "     Training Step: 199 Training Loss: 0.6172358393669128 \n",
      "     Training Step: 200 Training Loss: 0.6169531941413879 \n",
      "     Training Step: 201 Training Loss: 0.6130712628364563 \n",
      "     Training Step: 202 Training Loss: 0.6156920194625854 \n",
      "     Training Step: 203 Training Loss: 0.6126194596290588 \n",
      "     Training Step: 204 Training Loss: 0.6116440892219543 \n",
      "     Training Step: 205 Training Loss: 0.6123460531234741 \n",
      "     Training Step: 206 Training Loss: 0.6122815012931824 \n",
      "     Training Step: 207 Training Loss: 0.6139604449272156 \n",
      "     Training Step: 208 Training Loss: 0.6145169734954834 \n",
      "     Training Step: 209 Training Loss: 0.6177284121513367 \n",
      "     Training Step: 210 Training Loss: 0.6146296262741089 \n",
      "     Training Step: 211 Training Loss: 0.6104392409324646 \n",
      "     Training Step: 212 Training Loss: 0.6122680306434631 \n",
      "     Training Step: 213 Training Loss: 0.6105959415435791 \n",
      "     Training Step: 214 Training Loss: 0.6116880178451538 \n",
      "     Training Step: 215 Training Loss: 0.6098371148109436 \n",
      "     Training Step: 216 Training Loss: 0.6093061566352844 \n",
      "     Training Step: 217 Training Loss: 0.6148574352264404 \n",
      "     Training Step: 218 Training Loss: 0.6108392477035522 \n",
      "     Training Step: 219 Training Loss: 0.6136929988861084 \n",
      "     Training Step: 220 Training Loss: 0.6125584244728088 \n",
      "     Training Step: 221 Training Loss: 0.6168158054351807 \n",
      "     Training Step: 222 Training Loss: 0.6112979650497437 \n",
      "     Training Step: 223 Training Loss: 0.6144497394561768 \n",
      "     Training Step: 224 Training Loss: 0.6166831851005554 \n",
      "     Training Step: 225 Training Loss: 0.6157703995704651 \n",
      "     Training Step: 226 Training Loss: 0.6184850335121155 \n",
      "     Training Step: 227 Training Loss: 0.6209801435470581 \n",
      "     Training Step: 228 Training Loss: 0.6122105121612549 \n",
      "     Training Step: 229 Training Loss: 0.6144067645072937 \n",
      "     Training Step: 230 Training Loss: 0.6158427596092224 \n",
      "     Training Step: 231 Training Loss: 0.6153273582458496 \n",
      "     Training Step: 232 Training Loss: 0.611696720123291 \n",
      "     Training Step: 233 Training Loss: 0.6117771863937378 \n",
      "     Training Step: 234 Training Loss: 0.6117186546325684 \n",
      "     Training Step: 235 Training Loss: 0.6131697297096252 \n",
      "     Training Step: 236 Training Loss: 0.6143404245376587 \n",
      "     Training Step: 237 Training Loss: 0.6145802140235901 \n",
      "     Training Step: 238 Training Loss: 0.6115702390670776 \n",
      "     Training Step: 239 Training Loss: 0.6203160285949707 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6101749539375305 \n",
      "     Validation Step: 1 Validation Loss: 0.6185336709022522 \n",
      "     Validation Step: 2 Validation Loss: 0.6142818927764893 \n",
      "     Validation Step: 3 Validation Loss: 0.6150859594345093 \n",
      "     Validation Step: 4 Validation Loss: 0.6076042652130127 \n",
      "     Validation Step: 5 Validation Loss: 0.6112198233604431 \n",
      "     Validation Step: 6 Validation Loss: 0.6156581044197083 \n",
      "     Validation Step: 7 Validation Loss: 0.6133320331573486 \n",
      "     Validation Step: 8 Validation Loss: 0.6128753423690796 \n",
      "     Validation Step: 9 Validation Loss: 0.6116040945053101 \n",
      "     Validation Step: 10 Validation Loss: 0.6116827726364136 \n",
      "     Validation Step: 11 Validation Loss: 0.6105661392211914 \n",
      "     Validation Step: 12 Validation Loss: 0.6105202436447144 \n",
      "     Validation Step: 13 Validation Loss: 0.6181135177612305 \n",
      "     Validation Step: 14 Validation Loss: 0.6163068413734436 \n",
      "     Validation Step: 15 Validation Loss: 0.6160726547241211 \n",
      "     Validation Step: 16 Validation Loss: 0.6141838431358337 \n",
      "     Validation Step: 17 Validation Loss: 0.6152639985084534 \n",
      "     Validation Step: 18 Validation Loss: 0.6112286448478699 \n",
      "     Validation Step: 19 Validation Loss: 0.610191822052002 \n",
      "     Validation Step: 20 Validation Loss: 0.6170561909675598 \n",
      "     Validation Step: 21 Validation Loss: 0.6121878027915955 \n",
      "     Validation Step: 22 Validation Loss: 0.6136988401412964 \n",
      "     Validation Step: 23 Validation Loss: 0.6106982827186584 \n",
      "     Validation Step: 24 Validation Loss: 0.6158899068832397 \n",
      "     Validation Step: 25 Validation Loss: 0.6145614385604858 \n",
      "     Validation Step: 26 Validation Loss: 0.6173477172851562 \n",
      "     Validation Step: 27 Validation Loss: 0.6119515895843506 \n",
      "     Validation Step: 28 Validation Loss: 0.6176416873931885 \n",
      "     Validation Step: 29 Validation Loss: 0.6148896813392639 \n",
      "     Validation Step: 30 Validation Loss: 0.6177580952644348 \n",
      "     Validation Step: 31 Validation Loss: 0.6137309074401855 \n",
      "     Validation Step: 32 Validation Loss: 0.6146041750907898 \n",
      "     Validation Step: 33 Validation Loss: 0.6102120876312256 \n",
      "     Validation Step: 34 Validation Loss: 0.6156172752380371 \n",
      "     Validation Step: 35 Validation Loss: 0.6141680479049683 \n",
      "     Validation Step: 36 Validation Loss: 0.6149407029151917 \n",
      "     Validation Step: 37 Validation Loss: 0.6183069944381714 \n",
      "     Validation Step: 38 Validation Loss: 0.6184019446372986 \n",
      "     Validation Step: 39 Validation Loss: 0.6130394339561462 \n",
      "     Validation Step: 40 Validation Loss: 0.6153699159622192 \n",
      "     Validation Step: 41 Validation Loss: 0.6185516715049744 \n",
      "     Validation Step: 42 Validation Loss: 0.6136710047721863 \n",
      "     Validation Step: 43 Validation Loss: 0.6146857738494873 \n",
      "     Validation Step: 44 Validation Loss: 0.6142499446868896 \n",
      "Epoch: 34\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.616980791091919 \n",
      "     Training Step: 1 Training Loss: 0.6166612505912781 \n",
      "     Training Step: 2 Training Loss: 0.6135945916175842 \n",
      "     Training Step: 3 Training Loss: 0.6134496331214905 \n",
      "     Training Step: 4 Training Loss: 0.614719569683075 \n",
      "     Training Step: 5 Training Loss: 0.6145569086074829 \n",
      "     Training Step: 6 Training Loss: 0.6172680258750916 \n",
      "     Training Step: 7 Training Loss: 0.6153284907341003 \n",
      "     Training Step: 8 Training Loss: 0.6147976517677307 \n",
      "     Training Step: 9 Training Loss: 0.6147210001945496 \n",
      "     Training Step: 10 Training Loss: 0.6155620813369751 \n",
      "     Training Step: 11 Training Loss: 0.610621988773346 \n",
      "     Training Step: 12 Training Loss: 0.6115780472755432 \n",
      "     Training Step: 13 Training Loss: 0.6140645146369934 \n",
      "     Training Step: 14 Training Loss: 0.6136550307273865 \n",
      "     Training Step: 15 Training Loss: 0.613247811794281 \n",
      "     Training Step: 16 Training Loss: 0.611808180809021 \n",
      "     Training Step: 17 Training Loss: 0.6154629588127136 \n",
      "     Training Step: 18 Training Loss: 0.6141471862792969 \n",
      "     Training Step: 19 Training Loss: 0.6210099458694458 \n",
      "     Training Step: 20 Training Loss: 0.6197084188461304 \n",
      "     Training Step: 21 Training Loss: 0.6182125806808472 \n",
      "     Training Step: 22 Training Loss: 0.6184136867523193 \n",
      "     Training Step: 23 Training Loss: 0.6118596196174622 \n",
      "     Training Step: 24 Training Loss: 0.6143741011619568 \n",
      "     Training Step: 25 Training Loss: 0.6106547117233276 \n",
      "     Training Step: 26 Training Loss: 0.6134405732154846 \n",
      "     Training Step: 27 Training Loss: 0.6148642301559448 \n",
      "     Training Step: 28 Training Loss: 0.610800564289093 \n",
      "     Training Step: 29 Training Loss: 0.6116029024124146 \n",
      "     Training Step: 30 Training Loss: 0.6178684830665588 \n",
      "     Training Step: 31 Training Loss: 0.6184585094451904 \n",
      "     Training Step: 32 Training Loss: 0.6186856031417847 \n",
      "     Training Step: 33 Training Loss: 0.6168652176856995 \n",
      "     Training Step: 34 Training Loss: 0.6121705770492554 \n",
      "     Training Step: 35 Training Loss: 0.6143240928649902 \n",
      "     Training Step: 36 Training Loss: 0.6154204607009888 \n",
      "     Training Step: 37 Training Loss: 0.6182460188865662 \n",
      "     Training Step: 38 Training Loss: 0.61102694272995 \n",
      "     Training Step: 39 Training Loss: 0.6169744729995728 \n",
      "     Training Step: 40 Training Loss: 0.6198142170906067 \n",
      "     Training Step: 41 Training Loss: 0.6154208183288574 \n",
      "     Training Step: 42 Training Loss: 0.6113564968109131 \n",
      "     Training Step: 43 Training Loss: 0.616719126701355 \n",
      "     Training Step: 44 Training Loss: 0.6159697771072388 \n",
      "     Training Step: 45 Training Loss: 0.6172515749931335 \n",
      "     Training Step: 46 Training Loss: 0.6134018898010254 \n",
      "     Training Step: 47 Training Loss: 0.615204930305481 \n",
      "     Training Step: 48 Training Loss: 0.6155734658241272 \n",
      "     Training Step: 49 Training Loss: 0.610975444316864 \n",
      "     Training Step: 50 Training Loss: 0.6203393936157227 \n",
      "     Training Step: 51 Training Loss: 0.613689661026001 \n",
      "     Training Step: 52 Training Loss: 0.6140415668487549 \n",
      "     Training Step: 53 Training Loss: 0.611436128616333 \n",
      "     Training Step: 54 Training Loss: 0.6145280003547668 \n",
      "     Training Step: 55 Training Loss: 0.6150086522102356 \n",
      "     Training Step: 56 Training Loss: 0.6119245290756226 \n",
      "     Training Step: 57 Training Loss: 0.6139317750930786 \n",
      "     Training Step: 58 Training Loss: 0.6168509125709534 \n",
      "     Training Step: 59 Training Loss: 0.6177744269371033 \n",
      "     Training Step: 60 Training Loss: 0.6107174158096313 \n",
      "     Training Step: 61 Training Loss: 0.6135398149490356 \n",
      "     Training Step: 62 Training Loss: 0.6103994250297546 \n",
      "     Training Step: 63 Training Loss: 0.6158959269523621 \n",
      "     Training Step: 64 Training Loss: 0.6153339147567749 \n",
      "     Training Step: 65 Training Loss: 0.6147565841674805 \n",
      "     Training Step: 66 Training Loss: 0.6168142557144165 \n",
      "     Training Step: 67 Training Loss: 0.6144181489944458 \n",
      "     Training Step: 68 Training Loss: 0.6161849498748779 \n",
      "     Training Step: 69 Training Loss: 0.6152134537696838 \n",
      "     Training Step: 70 Training Loss: 0.6132005453109741 \n",
      "     Training Step: 71 Training Loss: 0.6137780547142029 \n",
      "     Training Step: 72 Training Loss: 0.6127439141273499 \n",
      "     Training Step: 73 Training Loss: 0.6119100451469421 \n",
      "     Training Step: 74 Training Loss: 0.6117352247238159 \n",
      "     Training Step: 75 Training Loss: 0.6151986718177795 \n",
      "     Training Step: 76 Training Loss: 0.614619255065918 \n",
      "     Training Step: 77 Training Loss: 0.6163557767868042 \n",
      "     Training Step: 78 Training Loss: 0.6125377416610718 \n",
      "     Training Step: 79 Training Loss: 0.6176338791847229 \n",
      "     Training Step: 80 Training Loss: 0.6128350496292114 \n",
      "     Training Step: 81 Training Loss: 0.6136950254440308 \n",
      "     Training Step: 82 Training Loss: 0.6154314279556274 \n",
      "     Training Step: 83 Training Loss: 0.6101013422012329 \n",
      "     Training Step: 84 Training Loss: 0.6177797913551331 \n",
      "     Training Step: 85 Training Loss: 0.6155392527580261 \n",
      "     Training Step: 86 Training Loss: 0.6137874722480774 \n",
      "     Training Step: 87 Training Loss: 0.6122274398803711 \n",
      "     Training Step: 88 Training Loss: 0.6112185120582581 \n",
      "     Training Step: 89 Training Loss: 0.6114574670791626 \n",
      "     Training Step: 90 Training Loss: 0.616884708404541 \n",
      "     Training Step: 91 Training Loss: 0.612097978591919 \n",
      "     Training Step: 92 Training Loss: 0.6121621131896973 \n",
      "     Training Step: 93 Training Loss: 0.6178686022758484 \n",
      "     Training Step: 94 Training Loss: 0.6135888695716858 \n",
      "     Training Step: 95 Training Loss: 0.6131201386451721 \n",
      "     Training Step: 96 Training Loss: 0.6154507398605347 \n",
      "     Training Step: 97 Training Loss: 0.613916277885437 \n",
      "     Training Step: 98 Training Loss: 0.6118656992912292 \n",
      "     Training Step: 99 Training Loss: 0.6143919229507446 \n",
      "     Training Step: 100 Training Loss: 0.6187348365783691 \n",
      "     Training Step: 101 Training Loss: 0.6171789765357971 \n",
      "     Training Step: 102 Training Loss: 0.6124529242515564 \n",
      "     Training Step: 103 Training Loss: 0.6117812395095825 \n",
      "     Training Step: 104 Training Loss: 0.6115418076515198 \n",
      "     Training Step: 105 Training Loss: 0.6134417653083801 \n",
      "     Training Step: 106 Training Loss: 0.6185821294784546 \n",
      "     Training Step: 107 Training Loss: 0.6170380115509033 \n",
      "     Training Step: 108 Training Loss: 0.612169086933136 \n",
      "     Training Step: 109 Training Loss: 0.6156232953071594 \n",
      "     Training Step: 110 Training Loss: 0.6108099818229675 \n",
      "     Training Step: 111 Training Loss: 0.6148937940597534 \n",
      "     Training Step: 112 Training Loss: 0.6164347529411316 \n",
      "     Training Step: 113 Training Loss: 0.6116899847984314 \n",
      "     Training Step: 114 Training Loss: 0.6107998490333557 \n",
      "     Training Step: 115 Training Loss: 0.6116028428077698 \n",
      "     Training Step: 116 Training Loss: 0.6133405566215515 \n",
      "     Training Step: 117 Training Loss: 0.6151532530784607 \n",
      "     Training Step: 118 Training Loss: 0.6141489744186401 \n",
      "     Training Step: 119 Training Loss: 0.6168690323829651 \n",
      "     Training Step: 120 Training Loss: 0.6153705716133118 \n",
      "     Training Step: 121 Training Loss: 0.6118742227554321 \n",
      "     Training Step: 122 Training Loss: 0.6102016568183899 \n",
      "     Training Step: 123 Training Loss: 0.6120719313621521 \n",
      "     Training Step: 124 Training Loss: 0.6158707141876221 \n",
      "     Training Step: 125 Training Loss: 0.61586594581604 \n",
      "     Training Step: 126 Training Loss: 0.6092439293861389 \n",
      "     Training Step: 127 Training Loss: 0.6125757098197937 \n",
      "     Training Step: 128 Training Loss: 0.617861807346344 \n",
      "     Training Step: 129 Training Loss: 0.6160429120063782 \n",
      "     Training Step: 130 Training Loss: 0.6180537343025208 \n",
      "     Training Step: 131 Training Loss: 0.6167351007461548 \n",
      "     Training Step: 132 Training Loss: 0.6154621243476868 \n",
      "     Training Step: 133 Training Loss: 0.6158473491668701 \n",
      "     Training Step: 134 Training Loss: 0.6099355816841125 \n",
      "     Training Step: 135 Training Loss: 0.611706554889679 \n",
      "     Training Step: 136 Training Loss: 0.6106929779052734 \n",
      "     Training Step: 137 Training Loss: 0.6102427840232849 \n",
      "     Training Step: 138 Training Loss: 0.614027202129364 \n",
      "     Training Step: 139 Training Loss: 0.6156346201896667 \n",
      "     Training Step: 140 Training Loss: 0.6157493591308594 \n",
      "     Training Step: 141 Training Loss: 0.6124154925346375 \n",
      "     Training Step: 142 Training Loss: 0.6150287389755249 \n",
      "     Training Step: 143 Training Loss: 0.6147753596305847 \n",
      "     Training Step: 144 Training Loss: 0.6115798950195312 \n",
      "     Training Step: 145 Training Loss: 0.6099317073822021 \n",
      "     Training Step: 146 Training Loss: 0.6126196384429932 \n",
      "     Training Step: 147 Training Loss: 0.6132256984710693 \n",
      "     Training Step: 148 Training Loss: 0.61222904920578 \n",
      "     Training Step: 149 Training Loss: 0.6150158047676086 \n",
      "     Training Step: 150 Training Loss: 0.6144490838050842 \n",
      "     Training Step: 151 Training Loss: 0.6124234795570374 \n",
      "     Training Step: 152 Training Loss: 0.6125656366348267 \n",
      "     Training Step: 153 Training Loss: 0.6117002367973328 \n",
      "     Training Step: 154 Training Loss: 0.6123414039611816 \n",
      "     Training Step: 155 Training Loss: 0.6201644539833069 \n",
      "     Training Step: 156 Training Loss: 0.6129658818244934 \n",
      "     Training Step: 157 Training Loss: 0.6105162501335144 \n",
      "     Training Step: 158 Training Loss: 0.6163636445999146 \n",
      "     Training Step: 159 Training Loss: 0.6141307353973389 \n",
      "     Training Step: 160 Training Loss: 0.6133062839508057 \n",
      "     Training Step: 161 Training Loss: 0.6174739599227905 \n",
      "     Training Step: 162 Training Loss: 0.6122766733169556 \n",
      "     Training Step: 163 Training Loss: 0.6162744164466858 \n",
      "     Training Step: 164 Training Loss: 0.6131014823913574 \n",
      "     Training Step: 165 Training Loss: 0.6118981838226318 \n",
      "     Training Step: 166 Training Loss: 0.6142322421073914 \n",
      "     Training Step: 167 Training Loss: 0.6101155877113342 \n",
      "     Training Step: 168 Training Loss: 0.6142722368240356 \n",
      "     Training Step: 169 Training Loss: 0.6180617809295654 \n",
      "     Training Step: 170 Training Loss: 0.6083515882492065 \n",
      "     Training Step: 171 Training Loss: 0.6160709857940674 \n",
      "     Training Step: 172 Training Loss: 0.6167728304862976 \n",
      "     Training Step: 173 Training Loss: 0.616535484790802 \n",
      "     Training Step: 174 Training Loss: 0.6188972592353821 \n",
      "     Training Step: 175 Training Loss: 0.6116683483123779 \n",
      "     Training Step: 176 Training Loss: 0.617128312587738 \n",
      "     Training Step: 177 Training Loss: 0.6159687638282776 \n",
      "     Training Step: 178 Training Loss: 0.6107690930366516 \n",
      "     Training Step: 179 Training Loss: 0.6138512492179871 \n",
      "     Training Step: 180 Training Loss: 0.6147682070732117 \n",
      "     Training Step: 181 Training Loss: 0.6134821772575378 \n",
      "     Training Step: 182 Training Loss: 0.6172451972961426 \n",
      "     Training Step: 183 Training Loss: 0.6123137474060059 \n",
      "     Training Step: 184 Training Loss: 0.6162611842155457 \n",
      "     Training Step: 185 Training Loss: 0.6098374724388123 \n",
      "     Training Step: 186 Training Loss: 0.6167671084403992 \n",
      "     Training Step: 187 Training Loss: 0.6195009350776672 \n",
      "     Training Step: 188 Training Loss: 0.6133341193199158 \n",
      "     Training Step: 189 Training Loss: 0.6148051023483276 \n",
      "     Training Step: 190 Training Loss: 0.6177008748054504 \n",
      "     Training Step: 191 Training Loss: 0.6164249777793884 \n",
      "     Training Step: 192 Training Loss: 0.6129100918769836 \n",
      "     Training Step: 193 Training Loss: 0.6132381558418274 \n",
      "     Training Step: 194 Training Loss: 0.6095086932182312 \n",
      "     Training Step: 195 Training Loss: 0.6142172813415527 \n",
      "     Training Step: 196 Training Loss: 0.6157777905464172 \n",
      "     Training Step: 197 Training Loss: 0.6129534244537354 \n",
      "     Training Step: 198 Training Loss: 0.6116421818733215 \n",
      "     Training Step: 199 Training Loss: 0.6127845048904419 \n",
      "     Training Step: 200 Training Loss: 0.6115078330039978 \n",
      "     Training Step: 201 Training Loss: 0.6143912076950073 \n",
      "     Training Step: 202 Training Loss: 0.6154460906982422 \n",
      "     Training Step: 203 Training Loss: 0.6146372556686401 \n",
      "     Training Step: 204 Training Loss: 0.616714596748352 \n",
      "     Training Step: 205 Training Loss: 0.6101410388946533 \n",
      "     Training Step: 206 Training Loss: 0.6167450547218323 \n",
      "     Training Step: 207 Training Loss: 0.6147093176841736 \n",
      "     Training Step: 208 Training Loss: 0.6147922277450562 \n",
      "     Training Step: 209 Training Loss: 0.6182511448860168 \n",
      "     Training Step: 210 Training Loss: 0.6174914836883545 \n",
      "     Training Step: 211 Training Loss: 0.6145016551017761 \n",
      "     Training Step: 212 Training Loss: 0.6102634072303772 \n",
      "     Training Step: 213 Training Loss: 0.6148308515548706 \n",
      "     Training Step: 214 Training Loss: 0.6126685738563538 \n",
      "     Training Step: 215 Training Loss: 0.6144192814826965 \n",
      "     Training Step: 216 Training Loss: 0.6125397086143494 \n",
      "     Training Step: 217 Training Loss: 0.6125360131263733 \n",
      "     Training Step: 218 Training Loss: 0.6136788725852966 \n",
      "     Training Step: 219 Training Loss: 0.6145252585411072 \n",
      "     Training Step: 220 Training Loss: 0.6181564927101135 \n",
      "     Training Step: 221 Training Loss: 0.6133692264556885 \n",
      "     Training Step: 222 Training Loss: 0.61531662940979 \n",
      "     Training Step: 223 Training Loss: 0.6106692552566528 \n",
      "     Training Step: 224 Training Loss: 0.6189031600952148 \n",
      "     Training Step: 225 Training Loss: 0.6157127022743225 \n",
      "     Training Step: 226 Training Loss: 0.6137767434120178 \n",
      "     Training Step: 227 Training Loss: 0.6144583821296692 \n",
      "     Training Step: 228 Training Loss: 0.6124582290649414 \n",
      "     Training Step: 229 Training Loss: 0.6151213645935059 \n",
      "     Training Step: 230 Training Loss: 0.610602080821991 \n",
      "     Training Step: 231 Training Loss: 0.618527352809906 \n",
      "     Training Step: 232 Training Loss: 0.6129520535469055 \n",
      "     Training Step: 233 Training Loss: 0.6146724820137024 \n",
      "     Training Step: 234 Training Loss: 0.6129744052886963 \n",
      "     Training Step: 235 Training Loss: 0.6146952509880066 \n",
      "     Training Step: 236 Training Loss: 0.6094733476638794 \n",
      "     Training Step: 237 Training Loss: 0.6112100481987 \n",
      "     Training Step: 238 Training Loss: 0.612898588180542 \n",
      "     Training Step: 239 Training Loss: 0.612857460975647 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6128481030464172 \n",
      "     Validation Step: 1 Validation Loss: 0.6147350072860718 \n",
      "     Validation Step: 2 Validation Loss: 0.6141714453697205 \n",
      "     Validation Step: 3 Validation Loss: 0.610507607460022 \n",
      "     Validation Step: 4 Validation Loss: 0.617863655090332 \n",
      "     Validation Step: 5 Validation Loss: 0.6137383580207825 \n",
      "     Validation Step: 6 Validation Loss: 0.6141941547393799 \n",
      "     Validation Step: 7 Validation Loss: 0.6177619099617004 \n",
      "     Validation Step: 8 Validation Loss: 0.6101207733154297 \n",
      "     Validation Step: 9 Validation Loss: 0.6143293976783752 \n",
      "     Validation Step: 10 Validation Loss: 0.6111647486686707 \n",
      "     Validation Step: 11 Validation Loss: 0.6100866794586182 \n",
      "     Validation Step: 12 Validation Loss: 0.6074530482292175 \n",
      "     Validation Step: 13 Validation Loss: 0.618709921836853 \n",
      "     Validation Step: 14 Validation Loss: 0.610164225101471 \n",
      "     Validation Step: 15 Validation Loss: 0.6106311678886414 \n",
      "     Validation Step: 16 Validation Loss: 0.6137573719024658 \n",
      "     Validation Step: 17 Validation Loss: 0.6184524297714233 \n",
      "     Validation Step: 18 Validation Loss: 0.6151571273803711 \n",
      "     Validation Step: 19 Validation Loss: 0.6161613464355469 \n",
      "     Validation Step: 20 Validation Loss: 0.6157353520393372 \n",
      "     Validation Step: 21 Validation Loss: 0.6115657091140747 \n",
      "     Validation Step: 22 Validation Loss: 0.6116545796394348 \n",
      "     Validation Step: 23 Validation Loss: 0.612142026424408 \n",
      "     Validation Step: 24 Validation Loss: 0.6130309104919434 \n",
      "     Validation Step: 25 Validation Loss: 0.6104865074157715 \n",
      "     Validation Step: 26 Validation Loss: 0.6111970543861389 \n",
      "     Validation Step: 27 Validation Loss: 0.6157330274581909 \n",
      "     Validation Step: 28 Validation Loss: 0.6182507276535034 \n",
      "     Validation Step: 29 Validation Loss: 0.6149718165397644 \n",
      "     Validation Step: 30 Validation Loss: 0.6154255867004395 \n",
      "     Validation Step: 31 Validation Loss: 0.617460310459137 \n",
      "     Validation Step: 32 Validation Loss: 0.6146653294563293 \n",
      "     Validation Step: 33 Validation Loss: 0.6185345649719238 \n",
      "     Validation Step: 34 Validation Loss: 0.6118845343589783 \n",
      "     Validation Step: 35 Validation Loss: 0.6186704635620117 \n",
      "     Validation Step: 36 Validation Loss: 0.6142827272415161 \n",
      "     Validation Step: 37 Validation Loss: 0.6133598685264587 \n",
      "     Validation Step: 38 Validation Loss: 0.6153193712234497 \n",
      "     Validation Step: 39 Validation Loss: 0.6149150133132935 \n",
      "     Validation Step: 40 Validation Loss: 0.6171828508377075 \n",
      "     Validation Step: 41 Validation Loss: 0.6146079301834106 \n",
      "     Validation Step: 42 Validation Loss: 0.6159403324127197 \n",
      "     Validation Step: 43 Validation Loss: 0.6164024472236633 \n",
      "     Validation Step: 44 Validation Loss: 0.6136773228645325 \n",
      "Epoch: 35\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6154229640960693 \n",
      "     Training Step: 1 Training Loss: 0.6178556680679321 \n",
      "     Training Step: 2 Training Loss: 0.6164059042930603 \n",
      "     Training Step: 3 Training Loss: 0.611570417881012 \n",
      "     Training Step: 4 Training Loss: 0.6139582395553589 \n",
      "     Training Step: 5 Training Loss: 0.6156064867973328 \n",
      "     Training Step: 6 Training Loss: 0.6118199825286865 \n",
      "     Training Step: 7 Training Loss: 0.6195530295372009 \n",
      "     Training Step: 8 Training Loss: 0.6166835427284241 \n",
      "     Training Step: 9 Training Loss: 0.6182341575622559 \n",
      "     Training Step: 10 Training Loss: 0.615138828754425 \n",
      "     Training Step: 11 Training Loss: 0.6142476201057434 \n",
      "     Training Step: 12 Training Loss: 0.6155682802200317 \n",
      "     Training Step: 13 Training Loss: 0.6180509328842163 \n",
      "     Training Step: 14 Training Loss: 0.6133389472961426 \n",
      "     Training Step: 15 Training Loss: 0.6136971712112427 \n",
      "     Training Step: 16 Training Loss: 0.6114712953567505 \n",
      "     Training Step: 17 Training Loss: 0.6156336069107056 \n",
      "     Training Step: 18 Training Loss: 0.6172292828559875 \n",
      "     Training Step: 19 Training Loss: 0.612886369228363 \n",
      "     Training Step: 20 Training Loss: 0.6147362589836121 \n",
      "     Training Step: 21 Training Loss: 0.6177419424057007 \n",
      "     Training Step: 22 Training Loss: 0.6167645454406738 \n",
      "     Training Step: 23 Training Loss: 0.6177926659584045 \n",
      "     Training Step: 24 Training Loss: 0.6141625642776489 \n",
      "     Training Step: 25 Training Loss: 0.6102327108383179 \n",
      "     Training Step: 26 Training Loss: 0.6133471131324768 \n",
      "     Training Step: 27 Training Loss: 0.6132017970085144 \n",
      "     Training Step: 28 Training Loss: 0.6118938326835632 \n",
      "     Training Step: 29 Training Loss: 0.6120598316192627 \n",
      "     Training Step: 30 Training Loss: 0.6138185262680054 \n",
      "     Training Step: 31 Training Loss: 0.6137336492538452 \n",
      "     Training Step: 32 Training Loss: 0.6160882115364075 \n",
      "     Training Step: 33 Training Loss: 0.6175633072853088 \n",
      "     Training Step: 34 Training Loss: 0.6176507472991943 \n",
      "     Training Step: 35 Training Loss: 0.6197082996368408 \n",
      "     Training Step: 36 Training Loss: 0.6148715019226074 \n",
      "     Training Step: 37 Training Loss: 0.6168494820594788 \n",
      "     Training Step: 38 Training Loss: 0.6116893291473389 \n",
      "     Training Step: 39 Training Loss: 0.6098564863204956 \n",
      "     Training Step: 40 Training Loss: 0.6115544438362122 \n",
      "     Training Step: 41 Training Loss: 0.6124866604804993 \n",
      "     Training Step: 42 Training Loss: 0.6109742522239685 \n",
      "     Training Step: 43 Training Loss: 0.6167010068893433 \n",
      "     Training Step: 44 Training Loss: 0.6182177066802979 \n",
      "     Training Step: 45 Training Loss: 0.6143569350242615 \n",
      "     Training Step: 46 Training Loss: 0.6203113198280334 \n",
      "     Training Step: 47 Training Loss: 0.6196982264518738 \n",
      "     Training Step: 48 Training Loss: 0.6156156659126282 \n",
      "     Training Step: 49 Training Loss: 0.6189436912536621 \n",
      "     Training Step: 50 Training Loss: 0.6108238101005554 \n",
      "     Training Step: 51 Training Loss: 0.6169137954711914 \n",
      "     Training Step: 52 Training Loss: 0.6122573614120483 \n",
      "     Training Step: 53 Training Loss: 0.6126439571380615 \n",
      "     Training Step: 54 Training Loss: 0.6129292249679565 \n",
      "     Training Step: 55 Training Loss: 0.6128417253494263 \n",
      "     Training Step: 56 Training Loss: 0.6094985008239746 \n",
      "     Training Step: 57 Training Loss: 0.6138827204704285 \n",
      "     Training Step: 58 Training Loss: 0.6145398020744324 \n",
      "     Training Step: 59 Training Loss: 0.6141502261161804 \n",
      "     Training Step: 60 Training Loss: 0.6186926960945129 \n",
      "     Training Step: 61 Training Loss: 0.6101040840148926 \n",
      "     Training Step: 62 Training Loss: 0.6169478297233582 \n",
      "     Training Step: 63 Training Loss: 0.6184184551239014 \n",
      "     Training Step: 64 Training Loss: 0.6186218857765198 \n",
      "     Training Step: 65 Training Loss: 0.6123420000076294 \n",
      "     Training Step: 66 Training Loss: 0.614705502986908 \n",
      "     Training Step: 67 Training Loss: 0.6115537881851196 \n",
      "     Training Step: 68 Training Loss: 0.6144285202026367 \n",
      "     Training Step: 69 Training Loss: 0.6105574369430542 \n",
      "     Training Step: 70 Training Loss: 0.6160361170768738 \n",
      "     Training Step: 71 Training Loss: 0.6129822731018066 \n",
      "     Training Step: 72 Training Loss: 0.6200067400932312 \n",
      "     Training Step: 73 Training Loss: 0.6152530312538147 \n",
      "     Training Step: 74 Training Loss: 0.6120933890342712 \n",
      "     Training Step: 75 Training Loss: 0.6162087321281433 \n",
      "     Training Step: 76 Training Loss: 0.6158259510993958 \n",
      "     Training Step: 77 Training Loss: 0.6143742203712463 \n",
      "     Training Step: 78 Training Loss: 0.6172423958778381 \n",
      "     Training Step: 79 Training Loss: 0.6162897348403931 \n",
      "     Training Step: 80 Training Loss: 0.6148256659507751 \n",
      "     Training Step: 81 Training Loss: 0.6109980344772339 \n",
      "     Training Step: 82 Training Loss: 0.6153091788291931 \n",
      "     Training Step: 83 Training Loss: 0.6144933104515076 \n",
      "     Training Step: 84 Training Loss: 0.6149465441703796 \n",
      "     Training Step: 85 Training Loss: 0.6121937036514282 \n",
      "     Training Step: 86 Training Loss: 0.6140646934509277 \n",
      "     Training Step: 87 Training Loss: 0.6163142919540405 \n",
      "     Training Step: 88 Training Loss: 0.6126620173454285 \n",
      "     Training Step: 89 Training Loss: 0.6132928729057312 \n",
      "     Training Step: 90 Training Loss: 0.6152700781822205 \n",
      "     Training Step: 91 Training Loss: 0.6119083762168884 \n",
      "     Training Step: 92 Training Loss: 0.6116248965263367 \n",
      "     Training Step: 93 Training Loss: 0.6178898215293884 \n",
      "     Training Step: 94 Training Loss: 0.6122910976409912 \n",
      "     Training Step: 95 Training Loss: 0.6136221289634705 \n",
      "     Training Step: 96 Training Loss: 0.6177422404289246 \n",
      "     Training Step: 97 Training Loss: 0.615314245223999 \n",
      "     Training Step: 98 Training Loss: 0.6146832704544067 \n",
      "     Training Step: 99 Training Loss: 0.6154924631118774 \n",
      "     Training Step: 100 Training Loss: 0.6151701807975769 \n",
      "     Training Step: 101 Training Loss: 0.6116174459457397 \n",
      "     Training Step: 102 Training Loss: 0.6124091148376465 \n",
      "     Training Step: 103 Training Loss: 0.6122589111328125 \n",
      "     Training Step: 104 Training Loss: 0.6134639978408813 \n",
      "     Training Step: 105 Training Loss: 0.6106004118919373 \n",
      "     Training Step: 106 Training Loss: 0.6125345230102539 \n",
      "     Training Step: 107 Training Loss: 0.6134567260742188 \n",
      "     Training Step: 108 Training Loss: 0.6138157248497009 \n",
      "     Training Step: 109 Training Loss: 0.6168553233146667 \n",
      "     Training Step: 110 Training Loss: 0.6142030358314514 \n",
      "     Training Step: 111 Training Loss: 0.6136150360107422 \n",
      "     Training Step: 112 Training Loss: 0.6147820353507996 \n",
      "     Training Step: 113 Training Loss: 0.6119213700294495 \n",
      "     Training Step: 114 Training Loss: 0.6102293133735657 \n",
      "     Training Step: 115 Training Loss: 0.6167805194854736 \n",
      "     Training Step: 116 Training Loss: 0.6139904856681824 \n",
      "     Training Step: 117 Training Loss: 0.6142416596412659 \n",
      "     Training Step: 118 Training Loss: 0.6154710650444031 \n",
      "     Training Step: 119 Training Loss: 0.6124274134635925 \n",
      "     Training Step: 120 Training Loss: 0.6105967164039612 \n",
      "     Training Step: 121 Training Loss: 0.6104328036308289 \n",
      "     Training Step: 122 Training Loss: 0.6106066107749939 \n",
      "     Training Step: 123 Training Loss: 0.6148392558097839 \n",
      "     Training Step: 124 Training Loss: 0.6116737127304077 \n",
      "     Training Step: 125 Training Loss: 0.611639142036438 \n",
      "     Training Step: 126 Training Loss: 0.6131446361541748 \n",
      "     Training Step: 127 Training Loss: 0.6118320822715759 \n",
      "     Training Step: 128 Training Loss: 0.6111864447593689 \n",
      "     Training Step: 129 Training Loss: 0.6148149967193604 \n",
      "     Training Step: 130 Training Loss: 0.6185032725334167 \n",
      "     Training Step: 131 Training Loss: 0.6117111444473267 \n",
      "     Training Step: 132 Training Loss: 0.6143626570701599 \n",
      "     Training Step: 133 Training Loss: 0.616729199886322 \n",
      "     Training Step: 134 Training Loss: 0.6142134070396423 \n",
      "     Training Step: 135 Training Loss: 0.615400493144989 \n",
      "     Training Step: 136 Training Loss: 0.6099764704704285 \n",
      "     Training Step: 137 Training Loss: 0.6152960062026978 \n",
      "     Training Step: 138 Training Loss: 0.6136214137077332 \n",
      "     Training Step: 139 Training Loss: 0.6154347658157349 \n",
      "     Training Step: 140 Training Loss: 0.6184338927268982 \n",
      "     Training Step: 141 Training Loss: 0.6129558682441711 \n",
      "     Training Step: 142 Training Loss: 0.6167697906494141 \n",
      "     Training Step: 143 Training Loss: 0.6166570782661438 \n",
      "     Training Step: 144 Training Loss: 0.6149364113807678 \n",
      "     Training Step: 145 Training Loss: 0.6145581603050232 \n",
      "     Training Step: 146 Training Loss: 0.6122251152992249 \n",
      "     Training Step: 147 Training Loss: 0.6101177930831909 \n",
      "     Training Step: 148 Training Loss: 0.6114553809165955 \n",
      "     Training Step: 149 Training Loss: 0.6130617260932922 \n",
      "     Training Step: 150 Training Loss: 0.6179674863815308 \n",
      "     Training Step: 151 Training Loss: 0.6157956719398499 \n",
      "     Training Step: 152 Training Loss: 0.6133029460906982 \n",
      "     Training Step: 153 Training Loss: 0.6141365170478821 \n",
      "     Training Step: 154 Training Loss: 0.6102902889251709 \n",
      "     Training Step: 155 Training Loss: 0.6123005747795105 \n",
      "     Training Step: 156 Training Loss: 0.6162069439888 \n",
      "     Training Step: 157 Training Loss: 0.6136022806167603 \n",
      "     Training Step: 158 Training Loss: 0.6156200766563416 \n",
      "     Training Step: 159 Training Loss: 0.6127959489822388 \n",
      "     Training Step: 160 Training Loss: 0.6123997569084167 \n",
      "     Training Step: 161 Training Loss: 0.6182852983474731 \n",
      "     Training Step: 162 Training Loss: 0.6135340332984924 \n",
      "     Training Step: 163 Training Loss: 0.6135087609291077 \n",
      "     Training Step: 164 Training Loss: 0.6106082797050476 \n",
      "     Training Step: 165 Training Loss: 0.6151726841926575 \n",
      "     Training Step: 166 Training Loss: 0.6107333898544312 \n",
      "     Training Step: 167 Training Loss: 0.6147187352180481 \n",
      "     Training Step: 168 Training Loss: 0.6147154569625854 \n",
      "     Training Step: 169 Training Loss: 0.6097885966300964 \n",
      "     Training Step: 170 Training Loss: 0.6129236221313477 \n",
      "     Training Step: 171 Training Loss: 0.6146750450134277 \n",
      "     Training Step: 172 Training Loss: 0.6092389225959778 \n",
      "     Training Step: 173 Training Loss: 0.6150209903717041 \n",
      "     Training Step: 174 Training Loss: 0.6147410273551941 \n",
      "     Training Step: 175 Training Loss: 0.6112106442451477 \n",
      "     Training Step: 176 Training Loss: 0.6167182922363281 \n",
      "     Training Step: 177 Training Loss: 0.6154147982597351 \n",
      "     Training Step: 178 Training Loss: 0.6117160320281982 \n",
      "     Training Step: 179 Training Loss: 0.618456244468689 \n",
      "     Training Step: 180 Training Loss: 0.61163330078125 \n",
      "     Training Step: 181 Training Loss: 0.6115949749946594 \n",
      "     Training Step: 182 Training Loss: 0.6144438982009888 \n",
      "     Training Step: 183 Training Loss: 0.6100671291351318 \n",
      "     Training Step: 184 Training Loss: 0.6148731112480164 \n",
      "     Training Step: 185 Training Loss: 0.6158002018928528 \n",
      "     Training Step: 186 Training Loss: 0.6210479736328125 \n",
      "     Training Step: 187 Training Loss: 0.6131937503814697 \n",
      "     Training Step: 188 Training Loss: 0.613383412361145 \n",
      "     Training Step: 189 Training Loss: 0.612643837928772 \n",
      "     Training Step: 190 Training Loss: 0.6114885210990906 \n",
      "     Training Step: 191 Training Loss: 0.6163639426231384 \n",
      "     Training Step: 192 Training Loss: 0.6107597351074219 \n",
      "     Training Step: 193 Training Loss: 0.6165481209754944 \n",
      "     Training Step: 194 Training Loss: 0.6154659986495972 \n",
      "     Training Step: 195 Training Loss: 0.6112350225448608 \n",
      "     Training Step: 196 Training Loss: 0.6171212196350098 \n",
      "     Training Step: 197 Training Loss: 0.615827739238739 \n",
      "     Training Step: 198 Training Loss: 0.6153245568275452 \n",
      "     Training Step: 199 Training Loss: 0.6147311329841614 \n",
      "     Training Step: 200 Training Loss: 0.612518310546875 \n",
      "     Training Step: 201 Training Loss: 0.6131241917610168 \n",
      "     Training Step: 202 Training Loss: 0.6132705807685852 \n",
      "     Training Step: 203 Training Loss: 0.612820565700531 \n",
      "     Training Step: 204 Training Loss: 0.6127229928970337 \n",
      "     Training Step: 205 Training Loss: 0.6143062114715576 \n",
      "     Training Step: 206 Training Loss: 0.6133055090904236 \n",
      "     Training Step: 207 Training Loss: 0.6181291937828064 \n",
      "     Training Step: 208 Training Loss: 0.6125361323356628 \n",
      "     Training Step: 209 Training Loss: 0.6121072173118591 \n",
      "     Training Step: 210 Training Loss: 0.6151121854782104 \n",
      "     Training Step: 211 Training Loss: 0.6174728870391846 \n",
      "     Training Step: 212 Training Loss: 0.6118634343147278 \n",
      "     Training Step: 213 Training Loss: 0.6144441366195679 \n",
      "     Training Step: 214 Training Loss: 0.6180328726768494 \n",
      "     Training Step: 215 Training Loss: 0.608477771282196 \n",
      "     Training Step: 216 Training Loss: 0.6108030676841736 \n",
      "     Training Step: 217 Training Loss: 0.6103992462158203 \n",
      "     Training Step: 218 Training Loss: 0.6169798970222473 \n",
      "     Training Step: 219 Training Loss: 0.6094529032707214 \n",
      "     Training Step: 220 Training Loss: 0.6158130168914795 \n",
      "     Training Step: 221 Training Loss: 0.6144729256629944 \n",
      "     Training Step: 222 Training Loss: 0.6168040633201599 \n",
      "     Training Step: 223 Training Loss: 0.6122282147407532 \n",
      "     Training Step: 224 Training Loss: 0.6150308847427368 \n",
      "     Training Step: 225 Training Loss: 0.6157902479171753 \n",
      "     Training Step: 226 Training Loss: 0.6133719086647034 \n",
      "     Training Step: 227 Training Loss: 0.617247998714447 \n",
      "     Training Step: 228 Training Loss: 0.6167137622833252 \n",
      "     Training Step: 229 Training Loss: 0.6146399974822998 \n",
      "     Training Step: 230 Training Loss: 0.6171796917915344 \n",
      "     Training Step: 231 Training Loss: 0.6138356924057007 \n",
      "     Training Step: 232 Training Loss: 0.6140123009681702 \n",
      "     Training Step: 233 Training Loss: 0.6117364168167114 \n",
      "     Training Step: 234 Training Loss: 0.6164512634277344 \n",
      "     Training Step: 235 Training Loss: 0.616998016834259 \n",
      "     Training Step: 236 Training Loss: 0.6107978224754333 \n",
      "     Training Step: 237 Training Loss: 0.6140567660331726 \n",
      "     Training Step: 238 Training Loss: 0.6159712672233582 \n",
      "     Training Step: 239 Training Loss: 0.6189201474189758 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6153588891029358 \n",
      "     Validation Step: 1 Validation Loss: 0.6156474351882935 \n",
      "     Validation Step: 2 Validation Loss: 0.6156149506568909 \n",
      "     Validation Step: 3 Validation Loss: 0.6128942370414734 \n",
      "     Validation Step: 4 Validation Loss: 0.6170249581336975 \n",
      "     Validation Step: 5 Validation Loss: 0.6173031330108643 \n",
      "     Validation Step: 6 Validation Loss: 0.61335289478302 \n",
      "     Validation Step: 7 Validation Loss: 0.6130618453025818 \n",
      "     Validation Step: 8 Validation Loss: 0.6112682819366455 \n",
      "     Validation Step: 9 Validation Loss: 0.6106136441230774 \n",
      "     Validation Step: 10 Validation Loss: 0.6149356365203857 \n",
      "     Validation Step: 11 Validation Loss: 0.6117236614227295 \n",
      "     Validation Step: 12 Validation Loss: 0.6143044829368591 \n",
      "     Validation Step: 13 Validation Loss: 0.6142588257789612 \n",
      "     Validation Step: 14 Validation Loss: 0.6176076531410217 \n",
      "     Validation Step: 15 Validation Loss: 0.6136770844459534 \n",
      "     Validation Step: 16 Validation Loss: 0.6146945357322693 \n",
      "     Validation Step: 17 Validation Loss: 0.6112669706344604 \n",
      "     Validation Step: 18 Validation Loss: 0.6162890791893005 \n",
      "     Validation Step: 19 Validation Loss: 0.613743007183075 \n",
      "     Validation Step: 20 Validation Loss: 0.6160435080528259 \n",
      "     Validation Step: 21 Validation Loss: 0.6152570843696594 \n",
      "     Validation Step: 22 Validation Loss: 0.6141763925552368 \n",
      "     Validation Step: 23 Validation Loss: 0.6102616190910339 \n",
      "     Validation Step: 24 Validation Loss: 0.6142043471336365 \n",
      "     Validation Step: 25 Validation Loss: 0.6145918369293213 \n",
      "     Validation Step: 26 Validation Loss: 0.6107481122016907 \n",
      "     Validation Step: 27 Validation Loss: 0.610284149646759 \n",
      "     Validation Step: 28 Validation Loss: 0.6177168488502502 \n",
      "     Validation Step: 29 Validation Loss: 0.6148862838745117 \n",
      "     Validation Step: 30 Validation Loss: 0.6185001134872437 \n",
      "     Validation Step: 31 Validation Loss: 0.6183664202690125 \n",
      "     Validation Step: 32 Validation Loss: 0.6119765043258667 \n",
      "     Validation Step: 33 Validation Loss: 0.6105943322181702 \n",
      "     Validation Step: 34 Validation Loss: 0.614583432674408 \n",
      "     Validation Step: 35 Validation Loss: 0.6122188568115234 \n",
      "     Validation Step: 36 Validation Loss: 0.6137088537216187 \n",
      "     Validation Step: 37 Validation Loss: 0.6158495545387268 \n",
      "     Validation Step: 38 Validation Loss: 0.6150935888290405 \n",
      "     Validation Step: 39 Validation Loss: 0.6102794408798218 \n",
      "     Validation Step: 40 Validation Loss: 0.6180770993232727 \n",
      "     Validation Step: 41 Validation Loss: 0.6182599663734436 \n",
      "     Validation Step: 42 Validation Loss: 0.6077136397361755 \n",
      "     Validation Step: 43 Validation Loss: 0.6184802055358887 \n",
      "     Validation Step: 44 Validation Loss: 0.6116508841514587 \n",
      "Epoch: 36\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6155658960342407 \n",
      "     Training Step: 1 Training Loss: 0.6185881495475769 \n",
      "     Training Step: 2 Training Loss: 0.6117022633552551 \n",
      "     Training Step: 3 Training Loss: 0.6143566966056824 \n",
      "     Training Step: 4 Training Loss: 0.6153185367584229 \n",
      "     Training Step: 5 Training Loss: 0.6167866587638855 \n",
      "     Training Step: 6 Training Loss: 0.6162577867507935 \n",
      "     Training Step: 7 Training Loss: 0.6128519773483276 \n",
      "     Training Step: 8 Training Loss: 0.611790120601654 \n",
      "     Training Step: 9 Training Loss: 0.6128213405609131 \n",
      "     Training Step: 10 Training Loss: 0.6124387979507446 \n",
      "     Training Step: 11 Training Loss: 0.6082895994186401 \n",
      "     Training Step: 12 Training Loss: 0.6156905293464661 \n",
      "     Training Step: 13 Training Loss: 0.6156497001647949 \n",
      "     Training Step: 14 Training Loss: 0.6169626116752625 \n",
      "     Training Step: 15 Training Loss: 0.6142937541007996 \n",
      "     Training Step: 16 Training Loss: 0.6147134304046631 \n",
      "     Training Step: 17 Training Loss: 0.6140183806419373 \n",
      "     Training Step: 18 Training Loss: 0.6136792898178101 \n",
      "     Training Step: 19 Training Loss: 0.6150646805763245 \n",
      "     Training Step: 20 Training Loss: 0.6164660453796387 \n",
      "     Training Step: 21 Training Loss: 0.6148073673248291 \n",
      "     Training Step: 22 Training Loss: 0.6136913299560547 \n",
      "     Training Step: 23 Training Loss: 0.6153497695922852 \n",
      "     Training Step: 24 Training Loss: 0.6184508204460144 \n",
      "     Training Step: 25 Training Loss: 0.612362265586853 \n",
      "     Training Step: 26 Training Loss: 0.6098670363426208 \n",
      "     Training Step: 27 Training Loss: 0.6171336770057678 \n",
      "     Training Step: 28 Training Loss: 0.6129066348075867 \n",
      "     Training Step: 29 Training Loss: 0.613580584526062 \n",
      "     Training Step: 30 Training Loss: 0.6178187131881714 \n",
      "     Training Step: 31 Training Loss: 0.6140539646148682 \n",
      "     Training Step: 32 Training Loss: 0.6132290363311768 \n",
      "     Training Step: 33 Training Loss: 0.6180519461631775 \n",
      "     Training Step: 34 Training Loss: 0.6130606532096863 \n",
      "     Training Step: 35 Training Loss: 0.6129072904586792 \n",
      "     Training Step: 36 Training Loss: 0.6133021116256714 \n",
      "     Training Step: 37 Training Loss: 0.6203377842903137 \n",
      "     Training Step: 38 Training Loss: 0.6169424653053284 \n",
      "     Training Step: 39 Training Loss: 0.6184442043304443 \n",
      "     Training Step: 40 Training Loss: 0.614538848400116 \n",
      "     Training Step: 41 Training Loss: 0.6140648126602173 \n",
      "     Training Step: 42 Training Loss: 0.6155120134353638 \n",
      "     Training Step: 43 Training Loss: 0.6131842732429504 \n",
      "     Training Step: 44 Training Loss: 0.6112390756607056 \n",
      "     Training Step: 45 Training Loss: 0.6105329990386963 \n",
      "     Training Step: 46 Training Loss: 0.6180599927902222 \n",
      "     Training Step: 47 Training Loss: 0.6137061715126038 \n",
      "     Training Step: 48 Training Loss: 0.6122652888298035 \n",
      "     Training Step: 49 Training Loss: 0.614404022693634 \n",
      "     Training Step: 50 Training Loss: 0.6159968972206116 \n",
      "     Training Step: 51 Training Loss: 0.6165235042572021 \n",
      "     Training Step: 52 Training Loss: 0.6116331219673157 \n",
      "     Training Step: 53 Training Loss: 0.6118533611297607 \n",
      "     Training Step: 54 Training Loss: 0.6135144233703613 \n",
      "     Training Step: 55 Training Loss: 0.6157848834991455 \n",
      "     Training Step: 56 Training Loss: 0.6097583770751953 \n",
      "     Training Step: 57 Training Loss: 0.6146540641784668 \n",
      "     Training Step: 58 Training Loss: 0.6115617156028748 \n",
      "     Training Step: 59 Training Loss: 0.6164847612380981 \n",
      "     Training Step: 60 Training Loss: 0.6147077083587646 \n",
      "     Training Step: 61 Training Loss: 0.6098250150680542 \n",
      "     Training Step: 62 Training Loss: 0.6142572164535522 \n",
      "     Training Step: 63 Training Loss: 0.6127297878265381 \n",
      "     Training Step: 64 Training Loss: 0.6151483654975891 \n",
      "     Training Step: 65 Training Loss: 0.614494264125824 \n",
      "     Training Step: 66 Training Loss: 0.6151091456413269 \n",
      "     Training Step: 67 Training Loss: 0.6181109547615051 \n",
      "     Training Step: 68 Training Loss: 0.6141036152839661 \n",
      "     Training Step: 69 Training Loss: 0.6108958125114441 \n",
      "     Training Step: 70 Training Loss: 0.6182120442390442 \n",
      "     Training Step: 71 Training Loss: 0.6177342534065247 \n",
      "     Training Step: 72 Training Loss: 0.6166431903839111 \n",
      "     Training Step: 73 Training Loss: 0.6102517247200012 \n",
      "     Training Step: 74 Training Loss: 0.6182936429977417 \n",
      "     Training Step: 75 Training Loss: 0.6167574524879456 \n",
      "     Training Step: 76 Training Loss: 0.6148393154144287 \n",
      "     Training Step: 77 Training Loss: 0.6134307980537415 \n",
      "     Training Step: 78 Training Loss: 0.6196269989013672 \n",
      "     Training Step: 79 Training Loss: 0.6137751340866089 \n",
      "     Training Step: 80 Training Loss: 0.6116631031036377 \n",
      "     Training Step: 81 Training Loss: 0.610104501247406 \n",
      "     Training Step: 82 Training Loss: 0.6106842756271362 \n",
      "     Training Step: 83 Training Loss: 0.6142004728317261 \n",
      "     Training Step: 84 Training Loss: 0.6199604272842407 \n",
      "     Training Step: 85 Training Loss: 0.6100296974182129 \n",
      "     Training Step: 86 Training Loss: 0.612614631652832 \n",
      "     Training Step: 87 Training Loss: 0.6147342324256897 \n",
      "     Training Step: 88 Training Loss: 0.6121334433555603 \n",
      "     Training Step: 89 Training Loss: 0.6123970150947571 \n",
      "     Training Step: 90 Training Loss: 0.6156837940216064 \n",
      "     Training Step: 91 Training Loss: 0.61548912525177 \n",
      "     Training Step: 92 Training Loss: 0.6125441789627075 \n",
      "     Training Step: 93 Training Loss: 0.6115072965621948 \n",
      "     Training Step: 94 Training Loss: 0.6210192441940308 \n",
      "     Training Step: 95 Training Loss: 0.6144282817840576 \n",
      "     Training Step: 96 Training Loss: 0.614182710647583 \n",
      "     Training Step: 97 Training Loss: 0.6137189865112305 \n",
      "     Training Step: 98 Training Loss: 0.6121569275856018 \n",
      "     Training Step: 99 Training Loss: 0.6149172782897949 \n",
      "     Training Step: 100 Training Loss: 0.6132780909538269 \n",
      "     Training Step: 101 Training Loss: 0.612803041934967 \n",
      "     Training Step: 102 Training Loss: 0.6094039678573608 \n",
      "     Training Step: 103 Training Loss: 0.6126593351364136 \n",
      "     Training Step: 104 Training Loss: 0.6116837859153748 \n",
      "     Training Step: 105 Training Loss: 0.6169129014015198 \n",
      "     Training Step: 106 Training Loss: 0.6173041462898254 \n",
      "     Training Step: 107 Training Loss: 0.6129442453384399 \n",
      "     Training Step: 108 Training Loss: 0.6188825368881226 \n",
      "     Training Step: 109 Training Loss: 0.6153002381324768 \n",
      "     Training Step: 110 Training Loss: 0.6136018633842468 \n",
      "     Training Step: 111 Training Loss: 0.6184856295585632 \n",
      "     Training Step: 112 Training Loss: 0.614007294178009 \n",
      "     Training Step: 113 Training Loss: 0.6133683919906616 \n",
      "     Training Step: 114 Training Loss: 0.6148101091384888 \n",
      "     Training Step: 115 Training Loss: 0.6147032380104065 \n",
      "     Training Step: 116 Training Loss: 0.6152588129043579 \n",
      "     Training Step: 117 Training Loss: 0.614756166934967 \n",
      "     Training Step: 118 Training Loss: 0.6107746958732605 \n",
      "     Training Step: 119 Training Loss: 0.6153271794319153 \n",
      "     Training Step: 120 Training Loss: 0.6134986281394958 \n",
      "     Training Step: 121 Training Loss: 0.6149486303329468 \n",
      "     Training Step: 122 Training Loss: 0.614682674407959 \n",
      "     Training Step: 123 Training Loss: 0.6161239147186279 \n",
      "     Training Step: 124 Training Loss: 0.6171944737434387 \n",
      "     Training Step: 125 Training Loss: 0.614328145980835 \n",
      "     Training Step: 126 Training Loss: 0.6199108362197876 \n",
      "     Training Step: 127 Training Loss: 0.6153849363327026 \n",
      "     Training Step: 128 Training Loss: 0.6115419864654541 \n",
      "     Training Step: 129 Training Loss: 0.6122181415557861 \n",
      "     Training Step: 130 Training Loss: 0.6131646633148193 \n",
      "     Training Step: 131 Training Loss: 0.6155645251274109 \n",
      "     Training Step: 132 Training Loss: 0.6178123354911804 \n",
      "     Training Step: 133 Training Loss: 0.6133264899253845 \n",
      "     Training Step: 134 Training Loss: 0.6118475794792175 \n",
      "     Training Step: 135 Training Loss: 0.612212598323822 \n",
      "     Training Step: 136 Training Loss: 0.6116674542427063 \n",
      "     Training Step: 137 Training Loss: 0.6092790365219116 \n",
      "     Training Step: 138 Training Loss: 0.6141403317451477 \n",
      "     Training Step: 139 Training Loss: 0.6143191456794739 \n",
      "     Training Step: 140 Training Loss: 0.6148132681846619 \n",
      "     Training Step: 141 Training Loss: 0.6132559776306152 \n",
      "     Training Step: 142 Training Loss: 0.6138620376586914 \n",
      "     Training Step: 143 Training Loss: 0.6152054667472839 \n",
      "     Training Step: 144 Training Loss: 0.6142792701721191 \n",
      "     Training Step: 145 Training Loss: 0.6188701391220093 \n",
      "     Training Step: 146 Training Loss: 0.6146258115768433 \n",
      "     Training Step: 147 Training Loss: 0.6176517009735107 \n",
      "     Training Step: 148 Training Loss: 0.6112768054008484 \n",
      "     Training Step: 149 Training Loss: 0.618588387966156 \n",
      "     Training Step: 150 Training Loss: 0.6171456575393677 \n",
      "     Training Step: 151 Training Loss: 0.6147515773773193 \n",
      "     Training Step: 152 Training Loss: 0.6158657073974609 \n",
      "     Training Step: 153 Training Loss: 0.6124070882797241 \n",
      "     Training Step: 154 Training Loss: 0.6166937947273254 \n",
      "     Training Step: 155 Training Loss: 0.6128696203231812 \n",
      "     Training Step: 156 Training Loss: 0.612069845199585 \n",
      "     Training Step: 157 Training Loss: 0.6162736415863037 \n",
      "     Training Step: 158 Training Loss: 0.6158283352851868 \n",
      "     Training Step: 159 Training Loss: 0.6166864633560181 \n",
      "     Training Step: 160 Training Loss: 0.6174322366714478 \n",
      "     Training Step: 161 Training Loss: 0.6183768510818481 \n",
      "     Training Step: 162 Training Loss: 0.616665780544281 \n",
      "     Training Step: 163 Training Loss: 0.6103894710540771 \n",
      "     Training Step: 164 Training Loss: 0.6157040596008301 \n",
      "     Training Step: 165 Training Loss: 0.6095225811004639 \n",
      "     Training Step: 166 Training Loss: 0.6109408736228943 \n",
      "     Training Step: 167 Training Loss: 0.6119483709335327 \n",
      "     Training Step: 168 Training Loss: 0.61476069688797 \n",
      "     Training Step: 169 Training Loss: 0.6145823001861572 \n",
      "     Training Step: 170 Training Loss: 0.6155441403388977 \n",
      "     Training Step: 171 Training Loss: 0.6123901009559631 \n",
      "     Training Step: 172 Training Loss: 0.6126610636711121 \n",
      "     Training Step: 173 Training Loss: 0.6112134456634521 \n",
      "     Training Step: 174 Training Loss: 0.616516649723053 \n",
      "     Training Step: 175 Training Loss: 0.6146621108055115 \n",
      "     Training Step: 176 Training Loss: 0.6158702373504639 \n",
      "     Training Step: 177 Training Loss: 0.6116631031036377 \n",
      "     Training Step: 178 Training Loss: 0.6106802821159363 \n",
      "     Training Step: 179 Training Loss: 0.6106379628181458 \n",
      "     Training Step: 180 Training Loss: 0.6125518083572388 \n",
      "     Training Step: 181 Training Loss: 0.6131875514984131 \n",
      "     Training Step: 182 Training Loss: 0.610481321811676 \n",
      "     Training Step: 183 Training Loss: 0.6134865283966064 \n",
      "     Training Step: 184 Training Loss: 0.6118599772453308 \n",
      "     Training Step: 185 Training Loss: 0.6169461607933044 \n",
      "     Training Step: 186 Training Loss: 0.6153259873390198 \n",
      "     Training Step: 187 Training Loss: 0.6150078177452087 \n",
      "     Training Step: 188 Training Loss: 0.6125121712684631 \n",
      "     Training Step: 189 Training Loss: 0.6148185133934021 \n",
      "     Training Step: 190 Training Loss: 0.6161289811134338 \n",
      "     Training Step: 191 Training Loss: 0.6116123199462891 \n",
      "     Training Step: 192 Training Loss: 0.613784670829773 \n",
      "     Training Step: 193 Training Loss: 0.6114634871482849 \n",
      "     Training Step: 194 Training Loss: 0.6182218194007874 \n",
      "     Training Step: 195 Training Loss: 0.6102127432823181 \n",
      "     Training Step: 196 Training Loss: 0.6114926934242249 \n",
      "     Training Step: 197 Training Loss: 0.6101124882698059 \n",
      "     Training Step: 198 Training Loss: 0.6178086400032043 \n",
      "     Training Step: 199 Training Loss: 0.61604905128479 \n",
      "     Training Step: 200 Training Loss: 0.6107077598571777 \n",
      "     Training Step: 201 Training Loss: 0.6132925152778625 \n",
      "     Training Step: 202 Training Loss: 0.6105824708938599 \n",
      "     Training Step: 203 Training Loss: 0.6155892610549927 \n",
      "     Training Step: 204 Training Loss: 0.6104061603546143 \n",
      "     Training Step: 205 Training Loss: 0.6122936606407166 \n",
      "     Training Step: 206 Training Loss: 0.6118032932281494 \n",
      "     Training Step: 207 Training Loss: 0.6154912114143372 \n",
      "     Training Step: 208 Training Loss: 0.6129434704780579 \n",
      "     Training Step: 209 Training Loss: 0.6153936386108398 \n",
      "     Training Step: 210 Training Loss: 0.6178141236305237 \n",
      "     Training Step: 211 Training Loss: 0.6167253851890564 \n",
      "     Training Step: 212 Training Loss: 0.6125507354736328 \n",
      "     Training Step: 213 Training Loss: 0.6157559156417847 \n",
      "     Training Step: 214 Training Loss: 0.6106656193733215 \n",
      "     Training Step: 215 Training Loss: 0.6141910552978516 \n",
      "     Training Step: 216 Training Loss: 0.6181687116622925 \n",
      "     Training Step: 217 Training Loss: 0.6169981360435486 \n",
      "     Training Step: 218 Training Loss: 0.6144356727600098 \n",
      "     Training Step: 219 Training Loss: 0.6115242838859558 \n",
      "     Training Step: 220 Training Loss: 0.6171547770500183 \n",
      "     Training Step: 221 Training Loss: 0.6131299734115601 \n",
      "     Training Step: 222 Training Loss: 0.6194469928741455 \n",
      "     Training Step: 223 Training Loss: 0.6167391538619995 \n",
      "     Training Step: 224 Training Loss: 0.6145146489143372 \n",
      "     Training Step: 225 Training Loss: 0.6167209148406982 \n",
      "     Training Step: 226 Training Loss: 0.61100834608078 \n",
      "     Training Step: 227 Training Loss: 0.611728310585022 \n",
      "     Training Step: 228 Training Loss: 0.6157611012458801 \n",
      "     Training Step: 229 Training Loss: 0.6167075037956238 \n",
      "     Training Step: 230 Training Loss: 0.6114577651023865 \n",
      "     Training Step: 231 Training Loss: 0.6151796579360962 \n",
      "     Training Step: 232 Training Loss: 0.6175520420074463 \n",
      "     Training Step: 233 Training Loss: 0.6104252338409424 \n",
      "     Training Step: 234 Training Loss: 0.6133041977882385 \n",
      "     Training Step: 235 Training Loss: 0.6162757873535156 \n",
      "     Training Step: 236 Training Loss: 0.6138553023338318 \n",
      "     Training Step: 237 Training Loss: 0.6122137904167175 \n",
      "     Training Step: 238 Training Loss: 0.6118761897087097 \n",
      "     Training Step: 239 Training Loss: 0.6119146347045898 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6145926713943481 \n",
      "     Validation Step: 1 Validation Loss: 0.6156547665596008 \n",
      "     Validation Step: 2 Validation Loss: 0.6163045167922974 \n",
      "     Validation Step: 3 Validation Loss: 0.6142335534095764 \n",
      "     Validation Step: 4 Validation Loss: 0.6142887473106384 \n",
      "     Validation Step: 5 Validation Loss: 0.6184051036834717 \n",
      "     Validation Step: 6 Validation Loss: 0.6141542196273804 \n",
      "     Validation Step: 7 Validation Loss: 0.6173436641693115 \n",
      "     Validation Step: 8 Validation Loss: 0.6118912696838379 \n",
      "     Validation Step: 9 Validation Loss: 0.617741584777832 \n",
      "     Validation Step: 10 Validation Loss: 0.6130117774009705 \n",
      "     Validation Step: 11 Validation Loss: 0.6181224584579468 \n",
      "     Validation Step: 12 Validation Loss: 0.611177384853363 \n",
      "     Validation Step: 13 Validation Loss: 0.6116546392440796 \n",
      "     Validation Step: 14 Validation Loss: 0.6170670390129089 \n",
      "     Validation Step: 15 Validation Loss: 0.614861249923706 \n",
      "     Validation Step: 16 Validation Loss: 0.616062581539154 \n",
      "     Validation Step: 17 Validation Loss: 0.6146717667579651 \n",
      "     Validation Step: 18 Validation Loss: 0.615349531173706 \n",
      "     Validation Step: 19 Validation Loss: 0.6133235096931458 \n",
      "     Validation Step: 20 Validation Loss: 0.6183125972747803 \n",
      "     Validation Step: 21 Validation Loss: 0.6105167865753174 \n",
      "     Validation Step: 22 Validation Loss: 0.6152547001838684 \n",
      "     Validation Step: 23 Validation Loss: 0.6185654997825623 \n",
      "     Validation Step: 24 Validation Loss: 0.6106548309326172 \n",
      "     Validation Step: 25 Validation Loss: 0.6101426482200623 \n",
      "     Validation Step: 26 Validation Loss: 0.6101975440979004 \n",
      "     Validation Step: 27 Validation Loss: 0.6136423945426941 \n",
      "     Validation Step: 28 Validation Loss: 0.6075494289398193 \n",
      "     Validation Step: 29 Validation Loss: 0.6176425814628601 \n",
      "     Validation Step: 30 Validation Loss: 0.6145643591880798 \n",
      "     Validation Step: 31 Validation Loss: 0.618531346321106 \n",
      "     Validation Step: 32 Validation Loss: 0.6136873364448547 \n",
      "     Validation Step: 33 Validation Loss: 0.6149206161499023 \n",
      "     Validation Step: 34 Validation Loss: 0.6101692318916321 \n",
      "     Validation Step: 35 Validation Loss: 0.6121443510055542 \n",
      "     Validation Step: 36 Validation Loss: 0.6141359210014343 \n",
      "     Validation Step: 37 Validation Loss: 0.6105307340621948 \n",
      "     Validation Step: 38 Validation Loss: 0.6158424615859985 \n",
      "     Validation Step: 39 Validation Loss: 0.615086555480957 \n",
      "     Validation Step: 40 Validation Loss: 0.6128376722335815 \n",
      "     Validation Step: 41 Validation Loss: 0.6156356930732727 \n",
      "     Validation Step: 42 Validation Loss: 0.6111966967582703 \n",
      "     Validation Step: 43 Validation Loss: 0.6137124300003052 \n",
      "     Validation Step: 44 Validation Loss: 0.6115749478340149 \n",
      "Epoch: 37\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6125351786613464 \n",
      "     Training Step: 1 Training Loss: 0.6143531203269958 \n",
      "     Training Step: 2 Training Loss: 0.616916835308075 \n",
      "     Training Step: 3 Training Loss: 0.613260805606842 \n",
      "     Training Step: 4 Training Loss: 0.616815984249115 \n",
      "     Training Step: 5 Training Loss: 0.6123551726341248 \n",
      "     Training Step: 6 Training Loss: 0.6142898201942444 \n",
      "     Training Step: 7 Training Loss: 0.6147819757461548 \n",
      "     Training Step: 8 Training Loss: 0.6124188303947449 \n",
      "     Training Step: 9 Training Loss: 0.6164451241493225 \n",
      "     Training Step: 10 Training Loss: 0.6147156953811646 \n",
      "     Training Step: 11 Training Loss: 0.6133896708488464 \n",
      "     Training Step: 12 Training Loss: 0.6168155670166016 \n",
      "     Training Step: 13 Training Loss: 0.6148270964622498 \n",
      "     Training Step: 14 Training Loss: 0.6115758419036865 \n",
      "     Training Step: 15 Training Loss: 0.6159650683403015 \n",
      "     Training Step: 16 Training Loss: 0.6136037111282349 \n",
      "     Training Step: 17 Training Loss: 0.6132311820983887 \n",
      "     Training Step: 18 Training Loss: 0.613782525062561 \n",
      "     Training Step: 19 Training Loss: 0.615552544593811 \n",
      "     Training Step: 20 Training Loss: 0.6183557510375977 \n",
      "     Training Step: 21 Training Loss: 0.6151782274246216 \n",
      "     Training Step: 22 Training Loss: 0.6122496128082275 \n",
      "     Training Step: 23 Training Loss: 0.6181021928787231 \n",
      "     Training Step: 24 Training Loss: 0.6101557612419128 \n",
      "     Training Step: 25 Training Loss: 0.6115900874137878 \n",
      "     Training Step: 26 Training Loss: 0.6107767224311829 \n",
      "     Training Step: 27 Training Loss: 0.6154764890670776 \n",
      "     Training Step: 28 Training Loss: 0.614507794380188 \n",
      "     Training Step: 29 Training Loss: 0.6178139448165894 \n",
      "     Training Step: 30 Training Loss: 0.6147581338882446 \n",
      "     Training Step: 31 Training Loss: 0.6130049228668213 \n",
      "     Training Step: 32 Training Loss: 0.6178430914878845 \n",
      "     Training Step: 33 Training Loss: 0.6167742609977722 \n",
      "     Training Step: 34 Training Loss: 0.6112946271896362 \n",
      "     Training Step: 35 Training Loss: 0.6128342747688293 \n",
      "     Training Step: 36 Training Loss: 0.6155561208724976 \n",
      "     Training Step: 37 Training Loss: 0.6116467714309692 \n",
      "     Training Step: 38 Training Loss: 0.62034672498703 \n",
      "     Training Step: 39 Training Loss: 0.618981122970581 \n",
      "     Training Step: 40 Training Loss: 0.6099595427513123 \n",
      "     Training Step: 41 Training Loss: 0.6149377822875977 \n",
      "     Training Step: 42 Training Loss: 0.6129056811332703 \n",
      "     Training Step: 43 Training Loss: 0.6148176789283752 \n",
      "     Training Step: 44 Training Loss: 0.6145400404930115 \n",
      "     Training Step: 45 Training Loss: 0.6094872355461121 \n",
      "     Training Step: 46 Training Loss: 0.6175061464309692 \n",
      "     Training Step: 47 Training Loss: 0.6114622950553894 \n",
      "     Training Step: 48 Training Loss: 0.6105269193649292 \n",
      "     Training Step: 49 Training Loss: 0.6195492744445801 \n",
      "     Training Step: 50 Training Loss: 0.6115738153457642 \n",
      "     Training Step: 51 Training Loss: 0.6116987466812134 \n",
      "     Training Step: 52 Training Loss: 0.6136406064033508 \n",
      "     Training Step: 53 Training Loss: 0.6133368015289307 \n",
      "     Training Step: 54 Training Loss: 0.6111564040184021 \n",
      "     Training Step: 55 Training Loss: 0.6131830811500549 \n",
      "     Training Step: 56 Training Loss: 0.6154835820198059 \n",
      "     Training Step: 57 Training Loss: 0.6101099848747253 \n",
      "     Training Step: 58 Training Loss: 0.6109451055526733 \n",
      "     Training Step: 59 Training Loss: 0.6104376912117004 \n",
      "     Training Step: 60 Training Loss: 0.6137126088142395 \n",
      "     Training Step: 61 Training Loss: 0.6168727874755859 \n",
      "     Training Step: 62 Training Loss: 0.6120930314064026 \n",
      "     Training Step: 63 Training Loss: 0.6175850629806519 \n",
      "     Training Step: 64 Training Loss: 0.6130092740058899 \n",
      "     Training Step: 65 Training Loss: 0.6107168197631836 \n",
      "     Training Step: 66 Training Loss: 0.611984372138977 \n",
      "     Training Step: 67 Training Loss: 0.6119012832641602 \n",
      "     Training Step: 68 Training Loss: 0.6140584945678711 \n",
      "     Training Step: 69 Training Loss: 0.6114345192909241 \n",
      "     Training Step: 70 Training Loss: 0.6161452531814575 \n",
      "     Training Step: 71 Training Loss: 0.610065221786499 \n",
      "     Training Step: 72 Training Loss: 0.6179123520851135 \n",
      "     Training Step: 73 Training Loss: 0.6132007837295532 \n",
      "     Training Step: 74 Training Loss: 0.6142164468765259 \n",
      "     Training Step: 75 Training Loss: 0.6116294860839844 \n",
      "     Training Step: 76 Training Loss: 0.6180793046951294 \n",
      "     Training Step: 77 Training Loss: 0.6155600547790527 \n",
      "     Training Step: 78 Training Loss: 0.6128423810005188 \n",
      "     Training Step: 79 Training Loss: 0.612230122089386 \n",
      "     Training Step: 80 Training Loss: 0.6209883689880371 \n",
      "     Training Step: 81 Training Loss: 0.6171051859855652 \n",
      "     Training Step: 82 Training Loss: 0.6138589382171631 \n",
      "     Training Step: 83 Training Loss: 0.612362802028656 \n",
      "     Training Step: 84 Training Loss: 0.6116419434547424 \n",
      "     Training Step: 85 Training Loss: 0.610751211643219 \n",
      "     Training Step: 86 Training Loss: 0.6124910712242126 \n",
      "     Training Step: 87 Training Loss: 0.615936815738678 \n",
      "     Training Step: 88 Training Loss: 0.6144179105758667 \n",
      "     Training Step: 89 Training Loss: 0.6154063940048218 \n",
      "     Training Step: 90 Training Loss: 0.6129993200302124 \n",
      "     Training Step: 91 Training Loss: 0.6113741397857666 \n",
      "     Training Step: 92 Training Loss: 0.6148155927658081 \n",
      "     Training Step: 93 Training Loss: 0.619712233543396 \n",
      "     Training Step: 94 Training Loss: 0.6154707670211792 \n",
      "     Training Step: 95 Training Loss: 0.6172712445259094 \n",
      "     Training Step: 96 Training Loss: 0.6122575998306274 \n",
      "     Training Step: 97 Training Loss: 0.6149580478668213 \n",
      "     Training Step: 98 Training Loss: 0.6133351922035217 \n",
      "     Training Step: 99 Training Loss: 0.6114766597747803 \n",
      "     Training Step: 100 Training Loss: 0.6139430403709412 \n",
      "     Training Step: 101 Training Loss: 0.6147648692131042 \n",
      "     Training Step: 102 Training Loss: 0.6178953051567078 \n",
      "     Training Step: 103 Training Loss: 0.6118343472480774 \n",
      "     Training Step: 104 Training Loss: 0.6162465810775757 \n",
      "     Training Step: 105 Training Loss: 0.6136805415153503 \n",
      "     Training Step: 106 Training Loss: 0.611739456653595 \n",
      "     Training Step: 107 Training Loss: 0.6146264672279358 \n",
      "     Training Step: 108 Training Loss: 0.6154026389122009 \n",
      "     Training Step: 109 Training Loss: 0.6167714595794678 \n",
      "     Training Step: 110 Training Loss: 0.6186254024505615 \n",
      "     Training Step: 111 Training Loss: 0.6122701168060303 \n",
      "     Training Step: 112 Training Loss: 0.6140945553779602 \n",
      "     Training Step: 113 Training Loss: 0.6147391200065613 \n",
      "     Training Step: 114 Training Loss: 0.6138443946838379 \n",
      "     Training Step: 115 Training Loss: 0.6129394173622131 \n",
      "     Training Step: 116 Training Loss: 0.6167317032814026 \n",
      "     Training Step: 117 Training Loss: 0.6153348088264465 \n",
      "     Training Step: 118 Training Loss: 0.6168089509010315 \n",
      "     Training Step: 119 Training Loss: 0.6162441372871399 \n",
      "     Training Step: 120 Training Loss: 0.6157811284065247 \n",
      "     Training Step: 121 Training Loss: 0.6146969795227051 \n",
      "     Training Step: 122 Training Loss: 0.6154229044914246 \n",
      "     Training Step: 123 Training Loss: 0.6106175780296326 \n",
      "     Training Step: 124 Training Loss: 0.6182840466499329 \n",
      "     Training Step: 125 Training Loss: 0.6118592023849487 \n",
      "     Training Step: 126 Training Loss: 0.611545741558075 \n",
      "     Training Step: 127 Training Loss: 0.6114245057106018 \n",
      "     Training Step: 128 Training Loss: 0.6165288686752319 \n",
      "     Training Step: 129 Training Loss: 0.6106060743331909 \n",
      "     Training Step: 130 Training Loss: 0.6120246648788452 \n",
      "     Training Step: 131 Training Loss: 0.6103893518447876 \n",
      "     Training Step: 132 Training Loss: 0.6165596842765808 \n",
      "     Training Step: 133 Training Loss: 0.6149746775627136 \n",
      "     Training Step: 134 Training Loss: 0.6125574111938477 \n",
      "     Training Step: 135 Training Loss: 0.6171941161155701 \n",
      "     Training Step: 136 Training Loss: 0.6186129450798035 \n",
      "     Training Step: 137 Training Loss: 0.6119284629821777 \n",
      "     Training Step: 138 Training Loss: 0.613152801990509 \n",
      "     Training Step: 139 Training Loss: 0.6180644035339355 \n",
      "     Training Step: 140 Training Loss: 0.6157675385475159 \n",
      "     Training Step: 141 Training Loss: 0.6167330145835876 \n",
      "     Training Step: 142 Training Loss: 0.6102880835533142 \n",
      "     Training Step: 143 Training Loss: 0.6162455677986145 \n",
      "     Training Step: 144 Training Loss: 0.6116921305656433 \n",
      "     Training Step: 145 Training Loss: 0.617742657661438 \n",
      "     Training Step: 146 Training Loss: 0.6135277152061462 \n",
      "     Training Step: 147 Training Loss: 0.6177101135253906 \n",
      "     Training Step: 148 Training Loss: 0.6146324276924133 \n",
      "     Training Step: 149 Training Loss: 0.6156092882156372 \n",
      "     Training Step: 150 Training Loss: 0.6123435497283936 \n",
      "     Training Step: 151 Training Loss: 0.6126605272293091 \n",
      "     Training Step: 152 Training Loss: 0.6144662499427795 \n",
      "     Training Step: 153 Training Loss: 0.6141446828842163 \n",
      "     Training Step: 154 Training Loss: 0.6157998442649841 \n",
      "     Training Step: 155 Training Loss: 0.6161538362503052 \n",
      "     Training Step: 156 Training Loss: 0.6122052669525146 \n",
      "     Training Step: 157 Training Loss: 0.6140621304512024 \n",
      "     Training Step: 158 Training Loss: 0.6147059202194214 \n",
      "     Training Step: 159 Training Loss: 0.6139464974403381 \n",
      "     Training Step: 160 Training Loss: 0.6125620007514954 \n",
      "     Training Step: 161 Training Loss: 0.6132016181945801 \n",
      "     Training Step: 162 Training Loss: 0.6169795989990234 \n",
      "     Training Step: 163 Training Loss: 0.6143833994865417 \n",
      "     Training Step: 164 Training Loss: 0.6118893027305603 \n",
      "     Training Step: 165 Training Loss: 0.6184418797492981 \n",
      "     Training Step: 166 Training Loss: 0.6150792241096497 \n",
      "     Training Step: 167 Training Loss: 0.6133392453193665 \n",
      "     Training Step: 168 Training Loss: 0.6146683692932129 \n",
      "     Training Step: 169 Training Loss: 0.619935929775238 \n",
      "     Training Step: 170 Training Loss: 0.6169326305389404 \n",
      "     Training Step: 171 Training Loss: 0.6151131987571716 \n",
      "     Training Step: 172 Training Loss: 0.6115396618843079 \n",
      "     Training Step: 173 Training Loss: 0.6102237701416016 \n",
      "     Training Step: 174 Training Loss: 0.6184601783752441 \n",
      "     Training Step: 175 Training Loss: 0.6152100563049316 \n",
      "     Training Step: 176 Training Loss: 0.6128169894218445 \n",
      "     Training Step: 177 Training Loss: 0.6153035759925842 \n",
      "     Training Step: 178 Training Loss: 0.6197659373283386 \n",
      "     Training Step: 179 Training Loss: 0.6153134703636169 \n",
      "     Training Step: 180 Training Loss: 0.614542543888092 \n",
      "     Training Step: 181 Training Loss: 0.6107071042060852 \n",
      "     Training Step: 182 Training Loss: 0.615298867225647 \n",
      "     Training Step: 183 Training Loss: 0.6154676675796509 \n",
      "     Training Step: 184 Training Loss: 0.6156138181686401 \n",
      "     Training Step: 185 Training Loss: 0.6167404651641846 \n",
      "     Training Step: 186 Training Loss: 0.6146766543388367 \n",
      "     Training Step: 187 Training Loss: 0.6122385859489441 \n",
      "     Training Step: 188 Training Loss: 0.6133526563644409 \n",
      "     Training Step: 189 Training Loss: 0.6135464906692505 \n",
      "     Training Step: 190 Training Loss: 0.6123963594436646 \n",
      "     Training Step: 191 Training Loss: 0.6146702170372009 \n",
      "     Training Step: 192 Training Loss: 0.6093853712081909 \n",
      "     Training Step: 193 Training Loss: 0.6145784854888916 \n",
      "     Training Step: 194 Training Loss: 0.6159285306930542 \n",
      "     Training Step: 195 Training Loss: 0.6142513155937195 \n",
      "     Training Step: 196 Training Loss: 0.6118882894515991 \n",
      "     Training Step: 197 Training Loss: 0.6133752465248108 \n",
      "     Training Step: 198 Training Loss: 0.6180523037910461 \n",
      "     Training Step: 199 Training Loss: 0.6106922030448914 \n",
      "     Training Step: 200 Training Loss: 0.6182815432548523 \n",
      "     Training Step: 201 Training Loss: 0.6134443283081055 \n",
      "     Training Step: 202 Training Loss: 0.6106228828430176 \n",
      "     Training Step: 203 Training Loss: 0.6097490787506104 \n",
      "     Training Step: 204 Training Loss: 0.6097026467323303 \n",
      "     Training Step: 205 Training Loss: 0.6173366904258728 \n",
      "     Training Step: 206 Training Loss: 0.6157705783843994 \n",
      "     Training Step: 207 Training Loss: 0.6152832508087158 \n",
      "     Training Step: 208 Training Loss: 0.6128225326538086 \n",
      "     Training Step: 209 Training Loss: 0.6160319447517395 \n",
      "     Training Step: 210 Training Loss: 0.6094066500663757 \n",
      "     Training Step: 211 Training Loss: 0.6176674365997314 \n",
      "     Training Step: 212 Training Loss: 0.6142264008522034 \n",
      "     Training Step: 213 Training Loss: 0.6163902878761292 \n",
      "     Training Step: 214 Training Loss: 0.6107640862464905 \n",
      "     Training Step: 215 Training Loss: 0.6144746541976929 \n",
      "     Training Step: 216 Training Loss: 0.6140558123588562 \n",
      "     Training Step: 217 Training Loss: 0.6083583235740662 \n",
      "     Training Step: 218 Training Loss: 0.6144123673439026 \n",
      "     Training Step: 219 Training Loss: 0.6189603805541992 \n",
      "     Training Step: 220 Training Loss: 0.6168655157089233 \n",
      "     Training Step: 221 Training Loss: 0.6141799092292786 \n",
      "     Training Step: 222 Training Loss: 0.6171934604644775 \n",
      "     Training Step: 223 Training Loss: 0.6158237457275391 \n",
      "     Training Step: 224 Training Loss: 0.6110455989837646 \n",
      "     Training Step: 225 Training Loss: 0.6166458129882812 \n",
      "     Training Step: 226 Training Loss: 0.6135061979293823 \n",
      "     Training Step: 227 Training Loss: 0.6135303378105164 \n",
      "     Training Step: 228 Training Loss: 0.6124964356422424 \n",
      "     Training Step: 229 Training Loss: 0.6151228547096252 \n",
      "     Training Step: 230 Training Loss: 0.6138193607330322 \n",
      "     Training Step: 231 Training Loss: 0.6166936755180359 \n",
      "     Training Step: 232 Training Loss: 0.6125714182853699 \n",
      "     Training Step: 233 Training Loss: 0.6139191389083862 \n",
      "     Training Step: 234 Training Loss: 0.6100333333015442 \n",
      "     Training Step: 235 Training Loss: 0.6150062084197998 \n",
      "     Training Step: 236 Training Loss: 0.6186007261276245 \n",
      "     Training Step: 237 Training Loss: 0.6129451990127563 \n",
      "     Training Step: 238 Training Loss: 0.6117172837257385 \n",
      "     Training Step: 239 Training Loss: 0.6131526231765747 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6183977723121643 \n",
      "     Validation Step: 1 Validation Loss: 0.6152433156967163 \n",
      "     Validation Step: 2 Validation Loss: 0.6136374473571777 \n",
      "     Validation Step: 3 Validation Loss: 0.6111969947814941 \n",
      "     Validation Step: 4 Validation Loss: 0.6141760349273682 \n",
      "     Validation Step: 5 Validation Loss: 0.6128354668617249 \n",
      "     Validation Step: 6 Validation Loss: 0.6185092329978943 \n",
      "     Validation Step: 7 Validation Loss: 0.6101934313774109 \n",
      "     Validation Step: 8 Validation Loss: 0.6181048154830933 \n",
      "     Validation Step: 9 Validation Loss: 0.6101661920547485 \n",
      "     Validation Step: 10 Validation Loss: 0.6115818619728088 \n",
      "     Validation Step: 11 Validation Loss: 0.6145577430725098 \n",
      "     Validation Step: 12 Validation Loss: 0.6162932515144348 \n",
      "     Validation Step: 13 Validation Loss: 0.6158484220504761 \n",
      "     Validation Step: 14 Validation Loss: 0.6146821975708008 \n",
      "     Validation Step: 15 Validation Loss: 0.6148627400398254 \n",
      "     Validation Step: 16 Validation Loss: 0.6106635928153992 \n",
      "     Validation Step: 17 Validation Loss: 0.6116651892662048 \n",
      "     Validation Step: 18 Validation Loss: 0.6105208992958069 \n",
      "     Validation Step: 19 Validation Loss: 0.6101933717727661 \n",
      "     Validation Step: 20 Validation Loss: 0.6156454682350159 \n",
      "     Validation Step: 21 Validation Loss: 0.6105430126190186 \n",
      "     Validation Step: 22 Validation Loss: 0.6170459985733032 \n",
      "     Validation Step: 23 Validation Loss: 0.6130156517028809 \n",
      "     Validation Step: 24 Validation Loss: 0.6112024784088135 \n",
      "     Validation Step: 25 Validation Loss: 0.6133198142051697 \n",
      "     Validation Step: 26 Validation Loss: 0.6173145771026611 \n",
      "     Validation Step: 27 Validation Loss: 0.613718569278717 \n",
      "     Validation Step: 28 Validation Loss: 0.6185334324836731 \n",
      "     Validation Step: 29 Validation Loss: 0.617740273475647 \n",
      "     Validation Step: 30 Validation Loss: 0.6119034886360168 \n",
      "     Validation Step: 31 Validation Loss: 0.6176227331161499 \n",
      "     Validation Step: 32 Validation Loss: 0.6182970404624939 \n",
      "     Validation Step: 33 Validation Loss: 0.6142768859863281 \n",
      "     Validation Step: 34 Validation Loss: 0.6141299605369568 \n",
      "     Validation Step: 35 Validation Loss: 0.6156265139579773 \n",
      "     Validation Step: 36 Validation Loss: 0.6149006485939026 \n",
      "     Validation Step: 37 Validation Loss: 0.6136887669563293 \n",
      "     Validation Step: 38 Validation Loss: 0.6150913834571838 \n",
      "     Validation Step: 39 Validation Loss: 0.6142317652702332 \n",
      "     Validation Step: 40 Validation Loss: 0.6153513789176941 \n",
      "     Validation Step: 41 Validation Loss: 0.6145820617675781 \n",
      "     Validation Step: 42 Validation Loss: 0.6075896620750427 \n",
      "     Validation Step: 43 Validation Loss: 0.612156331539154 \n",
      "     Validation Step: 44 Validation Loss: 0.6160522103309631 \n",
      "Epoch: 38\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6129356622695923 \n",
      "     Training Step: 1 Training Loss: 0.6145062446594238 \n",
      "     Training Step: 2 Training Loss: 0.6135028004646301 \n",
      "     Training Step: 3 Training Loss: 0.6189346313476562 \n",
      "     Training Step: 4 Training Loss: 0.6167104840278625 \n",
      "     Training Step: 5 Training Loss: 0.6154282689094543 \n",
      "     Training Step: 6 Training Loss: 0.6152960658073425 \n",
      "     Training Step: 7 Training Loss: 0.616713285446167 \n",
      "     Training Step: 8 Training Loss: 0.6125726699829102 \n",
      "     Training Step: 9 Training Loss: 0.6147872805595398 \n",
      "     Training Step: 10 Training Loss: 0.610607922077179 \n",
      "     Training Step: 11 Training Loss: 0.6145226359367371 \n",
      "     Training Step: 12 Training Loss: 0.6137402057647705 \n",
      "     Training Step: 13 Training Loss: 0.6145210862159729 \n",
      "     Training Step: 14 Training Loss: 0.6190931797027588 \n",
      "     Training Step: 15 Training Loss: 0.6168801784515381 \n",
      "     Training Step: 16 Training Loss: 0.6152766942977905 \n",
      "     Training Step: 17 Training Loss: 0.6134338974952698 \n",
      "     Training Step: 18 Training Loss: 0.6115806698799133 \n",
      "     Training Step: 19 Training Loss: 0.6116697788238525 \n",
      "     Training Step: 20 Training Loss: 0.6186038851737976 \n",
      "     Training Step: 21 Training Loss: 0.6138623356819153 \n",
      "     Training Step: 22 Training Loss: 0.6139714121818542 \n",
      "     Training Step: 23 Training Loss: 0.6148284077644348 \n",
      "     Training Step: 24 Training Loss: 0.6119346618652344 \n",
      "     Training Step: 25 Training Loss: 0.6107061505317688 \n",
      "     Training Step: 26 Training Loss: 0.6172717809677124 \n",
      "     Training Step: 27 Training Loss: 0.6167770624160767 \n",
      "     Training Step: 28 Training Loss: 0.6108086705207825 \n",
      "     Training Step: 29 Training Loss: 0.6167957186698914 \n",
      "     Training Step: 30 Training Loss: 0.6102206707000732 \n",
      "     Training Step: 31 Training Loss: 0.6123876571655273 \n",
      "     Training Step: 32 Training Loss: 0.6149724721908569 \n",
      "     Training Step: 33 Training Loss: 0.6165044903755188 \n",
      "     Training Step: 34 Training Loss: 0.6148368716239929 \n",
      "     Training Step: 35 Training Loss: 0.6108240485191345 \n",
      "     Training Step: 36 Training Loss: 0.616766095161438 \n",
      "     Training Step: 37 Training Loss: 0.618106484413147 \n",
      "     Training Step: 38 Training Loss: 0.614626407623291 \n",
      "     Training Step: 39 Training Loss: 0.6173878312110901 \n",
      "     Training Step: 40 Training Loss: 0.6130083203315735 \n",
      "     Training Step: 41 Training Loss: 0.6131718158721924 \n",
      "     Training Step: 42 Training Loss: 0.6123558282852173 \n",
      "     Training Step: 43 Training Loss: 0.6133381724357605 \n",
      "     Training Step: 44 Training Loss: 0.6151171326637268 \n",
      "     Training Step: 45 Training Loss: 0.6125670075416565 \n",
      "     Training Step: 46 Training Loss: 0.6120561957359314 \n",
      "     Training Step: 47 Training Loss: 0.6148977279663086 \n",
      "     Training Step: 48 Training Loss: 0.6186020970344543 \n",
      "     Training Step: 49 Training Loss: 0.6178707480430603 \n",
      "     Training Step: 50 Training Loss: 0.6157292127609253 \n",
      "     Training Step: 51 Training Loss: 0.6117932200431824 \n",
      "     Training Step: 52 Training Loss: 0.6123694777488708 \n",
      "     Training Step: 53 Training Loss: 0.6185395121574402 \n",
      "     Training Step: 54 Training Loss: 0.6129047870635986 \n",
      "     Training Step: 55 Training Loss: 0.6121813058853149 \n",
      "     Training Step: 56 Training Loss: 0.6121888160705566 \n",
      "     Training Step: 57 Training Loss: 0.6117110252380371 \n",
      "     Training Step: 58 Training Loss: 0.6107322573661804 \n",
      "     Training Step: 59 Training Loss: 0.6122074127197266 \n",
      "     Training Step: 60 Training Loss: 0.6141170263290405 \n",
      "     Training Step: 61 Training Loss: 0.6165546178817749 \n",
      "     Training Step: 62 Training Loss: 0.6144187450408936 \n",
      "     Training Step: 63 Training Loss: 0.6142507791519165 \n",
      "     Training Step: 64 Training Loss: 0.6149736046791077 \n",
      "     Training Step: 65 Training Loss: 0.6155438423156738 \n",
      "     Training Step: 66 Training Loss: 0.6154851317405701 \n",
      "     Training Step: 67 Training Loss: 0.613214910030365 \n",
      "     Training Step: 68 Training Loss: 0.614176869392395 \n",
      "     Training Step: 69 Training Loss: 0.6164873838424683 \n",
      "     Training Step: 70 Training Loss: 0.6130441427230835 \n",
      "     Training Step: 71 Training Loss: 0.61182701587677 \n",
      "     Training Step: 72 Training Loss: 0.6104299426078796 \n",
      "     Training Step: 73 Training Loss: 0.6152289509773254 \n",
      "     Training Step: 74 Training Loss: 0.6155978441238403 \n",
      "     Training Step: 75 Training Loss: 0.6139143109321594 \n",
      "     Training Step: 76 Training Loss: 0.6135181188583374 \n",
      "     Training Step: 77 Training Loss: 0.6084207892417908 \n",
      "     Training Step: 78 Training Loss: 0.6141366362571716 \n",
      "     Training Step: 79 Training Loss: 0.6168321967124939 \n",
      "     Training Step: 80 Training Loss: 0.6129088997840881 \n",
      "     Training Step: 81 Training Loss: 0.6177766919136047 \n",
      "     Training Step: 82 Training Loss: 0.613628625869751 \n",
      "     Training Step: 83 Training Loss: 0.6122839450836182 \n",
      "     Training Step: 84 Training Loss: 0.6133604645729065 \n",
      "     Training Step: 85 Training Loss: 0.6143535375595093 \n",
      "     Training Step: 86 Training Loss: 0.6167923808097839 \n",
      "     Training Step: 87 Training Loss: 0.6106665134429932 \n",
      "     Training Step: 88 Training Loss: 0.6107792854309082 \n",
      "     Training Step: 89 Training Loss: 0.6181751489639282 \n",
      "     Training Step: 90 Training Loss: 0.6128233075141907 \n",
      "     Training Step: 91 Training Loss: 0.6155463457107544 \n",
      "     Training Step: 92 Training Loss: 0.6131414771080017 \n",
      "     Training Step: 93 Training Loss: 0.6125742793083191 \n",
      "     Training Step: 94 Training Loss: 0.6132153272628784 \n",
      "     Training Step: 95 Training Loss: 0.6124770045280457 \n",
      "     Training Step: 96 Training Loss: 0.6138574481010437 \n",
      "     Training Step: 97 Training Loss: 0.6123013496398926 \n",
      "     Training Step: 98 Training Loss: 0.6125457882881165 \n",
      "     Training Step: 99 Training Loss: 0.610578179359436 \n",
      "     Training Step: 100 Training Loss: 0.6115350723266602 \n",
      "     Training Step: 101 Training Loss: 0.6143272519111633 \n",
      "     Training Step: 102 Training Loss: 0.6135208606719971 \n",
      "     Training Step: 103 Training Loss: 0.6181378960609436 \n",
      "     Training Step: 104 Training Loss: 0.6114537119865417 \n",
      "     Training Step: 105 Training Loss: 0.6105476021766663 \n",
      "     Training Step: 106 Training Loss: 0.6154126524925232 \n",
      "     Training Step: 107 Training Loss: 0.6114432215690613 \n",
      "     Training Step: 108 Training Loss: 0.6153558492660522 \n",
      "     Training Step: 109 Training Loss: 0.6126590371131897 \n",
      "     Training Step: 110 Training Loss: 0.6159627437591553 \n",
      "     Training Step: 111 Training Loss: 0.6142644286155701 \n",
      "     Training Step: 112 Training Loss: 0.6115628480911255 \n",
      "     Training Step: 113 Training Loss: 0.6123949885368347 \n",
      "     Training Step: 114 Training Loss: 0.6176689267158508 \n",
      "     Training Step: 115 Training Loss: 0.6094686985015869 \n",
      "     Training Step: 116 Training Loss: 0.6147024631500244 \n",
      "     Training Step: 117 Training Loss: 0.6183002591133118 \n",
      "     Training Step: 118 Training Loss: 0.6196579933166504 \n",
      "     Training Step: 119 Training Loss: 0.6169326305389404 \n",
      "     Training Step: 120 Training Loss: 0.6161463260650635 \n",
      "     Training Step: 121 Training Loss: 0.613810122013092 \n",
      "     Training Step: 122 Training Loss: 0.614759087562561 \n",
      "     Training Step: 123 Training Loss: 0.6121093034744263 \n",
      "     Training Step: 124 Training Loss: 0.6112199425697327 \n",
      "     Training Step: 125 Training Loss: 0.6164838671684265 \n",
      "     Training Step: 126 Training Loss: 0.6119231581687927 \n",
      "     Training Step: 127 Training Loss: 0.6158525347709656 \n",
      "     Training Step: 128 Training Loss: 0.6182401776313782 \n",
      "     Training Step: 129 Training Loss: 0.6113713383674622 \n",
      "     Training Step: 130 Training Loss: 0.616901695728302 \n",
      "     Training Step: 131 Training Loss: 0.6099117398262024 \n",
      "     Training Step: 132 Training Loss: 0.615646481513977 \n",
      "     Training Step: 133 Training Loss: 0.6097736358642578 \n",
      "     Training Step: 134 Training Loss: 0.614849865436554 \n",
      "     Training Step: 135 Training Loss: 0.6109648942947388 \n",
      "     Training Step: 136 Training Loss: 0.6100311279296875 \n",
      "     Training Step: 137 Training Loss: 0.6118544340133667 \n",
      "     Training Step: 138 Training Loss: 0.6187386512756348 \n",
      "     Training Step: 139 Training Loss: 0.6158000826835632 \n",
      "     Training Step: 140 Training Loss: 0.6202415227890015 \n",
      "     Training Step: 141 Training Loss: 0.61439049243927 \n",
      "     Training Step: 142 Training Loss: 0.6152946352958679 \n",
      "     Training Step: 143 Training Loss: 0.6168393492698669 \n",
      "     Training Step: 144 Training Loss: 0.6158755421638489 \n",
      "     Training Step: 145 Training Loss: 0.6181408166885376 \n",
      "     Training Step: 146 Training Loss: 0.6174973249435425 \n",
      "     Training Step: 147 Training Loss: 0.6119177341461182 \n",
      "     Training Step: 148 Training Loss: 0.6117939352989197 \n",
      "     Training Step: 149 Training Loss: 0.614784836769104 \n",
      "     Training Step: 150 Training Loss: 0.6145607829093933 \n",
      "     Training Step: 151 Training Loss: 0.6129123568534851 \n",
      "     Training Step: 152 Training Loss: 0.6117229461669922 \n",
      "     Training Step: 153 Training Loss: 0.6136406660079956 \n",
      "     Training Step: 154 Training Loss: 0.6137681603431702 \n",
      "     Training Step: 155 Training Loss: 0.6146981716156006 \n",
      "     Training Step: 156 Training Loss: 0.6155837178230286 \n",
      "     Training Step: 157 Training Loss: 0.6102471351623535 \n",
      "     Training Step: 158 Training Loss: 0.6162446737289429 \n",
      "     Training Step: 159 Training Loss: 0.6140552163124084 \n",
      "     Training Step: 160 Training Loss: 0.6163899898529053 \n",
      "     Training Step: 161 Training Loss: 0.6194953918457031 \n",
      "     Training Step: 162 Training Loss: 0.6139442920684814 \n",
      "     Training Step: 163 Training Loss: 0.6093751192092896 \n",
      "     Training Step: 164 Training Loss: 0.6186255812644958 \n",
      "     Training Step: 165 Training Loss: 0.6171619892120361 \n",
      "     Training Step: 166 Training Loss: 0.6151076555252075 \n",
      "     Training Step: 167 Training Loss: 0.6119576692581177 \n",
      "     Training Step: 168 Training Loss: 0.6128315925598145 \n",
      "     Training Step: 169 Training Loss: 0.6133305430412292 \n",
      "     Training Step: 170 Training Loss: 0.614944338798523 \n",
      "     Training Step: 171 Training Loss: 0.6154996156692505 \n",
      "     Training Step: 172 Training Loss: 0.6100012063980103 \n",
      "     Training Step: 173 Training Loss: 0.6103775501251221 \n",
      "     Training Step: 174 Training Loss: 0.612496018409729 \n",
      "     Training Step: 175 Training Loss: 0.6105771660804749 \n",
      "     Training Step: 176 Training Loss: 0.6158678531646729 \n",
      "     Training Step: 177 Training Loss: 0.6119014024734497 \n",
      "     Training Step: 178 Training Loss: 0.6112135648727417 \n",
      "     Training Step: 179 Training Loss: 0.6134309768676758 \n",
      "     Training Step: 180 Training Loss: 0.6116804480552673 \n",
      "     Training Step: 181 Training Loss: 0.6146156787872314 \n",
      "     Training Step: 182 Training Loss: 0.6116116642951965 \n",
      "     Training Step: 183 Training Loss: 0.6185020208358765 \n",
      "     Training Step: 184 Training Loss: 0.620996356010437 \n",
      "     Training Step: 185 Training Loss: 0.6127784848213196 \n",
      "     Training Step: 186 Training Loss: 0.617692768573761 \n",
      "     Training Step: 187 Training Loss: 0.6110800504684448 \n",
      "     Training Step: 188 Training Loss: 0.6171514987945557 \n",
      "     Training Step: 189 Training Loss: 0.614098072052002 \n",
      "     Training Step: 190 Training Loss: 0.6153682470321655 \n",
      "     Training Step: 191 Training Loss: 0.6177222728729248 \n",
      "     Training Step: 192 Training Loss: 0.6150684952735901 \n",
      "     Training Step: 193 Training Loss: 0.616689920425415 \n",
      "     Training Step: 194 Training Loss: 0.6134436130523682 \n",
      "     Training Step: 195 Training Loss: 0.614252507686615 \n",
      "     Training Step: 196 Training Loss: 0.6132163405418396 \n",
      "     Training Step: 197 Training Loss: 0.6161608695983887 \n",
      "     Training Step: 198 Training Loss: 0.6172180771827698 \n",
      "     Training Step: 199 Training Loss: 0.616044282913208 \n",
      "     Training Step: 200 Training Loss: 0.6098178625106812 \n",
      "     Training Step: 201 Training Loss: 0.6124233603477478 \n",
      "     Training Step: 202 Training Loss: 0.6115483641624451 \n",
      "     Training Step: 203 Training Loss: 0.6133192181587219 \n",
      "     Training Step: 204 Training Loss: 0.6144528985023499 \n",
      "     Training Step: 205 Training Loss: 0.6147670745849609 \n",
      "     Training Step: 206 Training Loss: 0.6146971583366394 \n",
      "     Training Step: 207 Training Loss: 0.6140769124031067 \n",
      "     Training Step: 208 Training Loss: 0.616921067237854 \n",
      "     Training Step: 209 Training Loss: 0.6147345900535583 \n",
      "     Training Step: 210 Training Loss: 0.6170992851257324 \n",
      "     Training Step: 211 Training Loss: 0.615401029586792 \n",
      "     Training Step: 212 Training Loss: 0.6154630184173584 \n",
      "     Training Step: 213 Training Loss: 0.6144435405731201 \n",
      "     Training Step: 214 Training Loss: 0.6128697991371155 \n",
      "     Training Step: 215 Training Loss: 0.6162689924240112 \n",
      "     Training Step: 216 Training Loss: 0.615787923336029 \n",
      "     Training Step: 217 Training Loss: 0.6132216453552246 \n",
      "     Training Step: 218 Training Loss: 0.6116625666618347 \n",
      "     Training Step: 219 Training Loss: 0.6101469397544861 \n",
      "     Training Step: 220 Training Loss: 0.6114360094070435 \n",
      "     Training Step: 221 Training Loss: 0.6168736219406128 \n",
      "     Training Step: 222 Training Loss: 0.6093730330467224 \n",
      "     Training Step: 223 Training Loss: 0.6131367683410645 \n",
      "     Training Step: 224 Training Loss: 0.6135678887367249 \n",
      "     Training Step: 225 Training Loss: 0.6121776700019836 \n",
      "     Training Step: 226 Training Loss: 0.6101750731468201 \n",
      "     Training Step: 227 Training Loss: 0.6150083541870117 \n",
      "     Training Step: 228 Training Loss: 0.6137956380844116 \n",
      "     Training Step: 229 Training Loss: 0.6197479367256165 \n",
      "     Training Step: 230 Training Loss: 0.616671621799469 \n",
      "     Training Step: 231 Training Loss: 0.6157317757606506 \n",
      "     Training Step: 232 Training Loss: 0.6177961230278015 \n",
      "     Training Step: 233 Training Loss: 0.6115314960479736 \n",
      "     Training Step: 234 Training Loss: 0.6153271794319153 \n",
      "     Training Step: 235 Training Loss: 0.6178027391433716 \n",
      "     Training Step: 236 Training Loss: 0.6142016649246216 \n",
      "     Training Step: 237 Training Loss: 0.6153767108917236 \n",
      "     Training Step: 238 Training Loss: 0.6199386119842529 \n",
      "     Training Step: 239 Training Loss: 0.6146531701087952 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.614713728427887 \n",
      "     Validation Step: 1 Validation Loss: 0.6184706091880798 \n",
      "     Validation Step: 2 Validation Loss: 0.6106663346290588 \n",
      "     Validation Step: 3 Validation Loss: 0.6153742074966431 \n",
      "     Validation Step: 4 Validation Loss: 0.6143590807914734 \n",
      "     Validation Step: 5 Validation Loss: 0.6120288372039795 \n",
      "     Validation Step: 6 Validation Loss: 0.6117900013923645 \n",
      "     Validation Step: 7 Validation Loss: 0.6134058833122253 \n",
      "     Validation Step: 8 Validation Loss: 0.6160195469856262 \n",
      "     Validation Step: 9 Validation Loss: 0.6149020791053772 \n",
      "     Validation Step: 10 Validation Loss: 0.6150986552238464 \n",
      "     Validation Step: 11 Validation Loss: 0.6176203489303589 \n",
      "     Validation Step: 12 Validation Loss: 0.6156198382377625 \n",
      "     Validation Step: 13 Validation Loss: 0.6122763156890869 \n",
      "     Validation Step: 14 Validation Loss: 0.6173073053359985 \n",
      "     Validation Step: 15 Validation Loss: 0.6180695295333862 \n",
      "     Validation Step: 16 Validation Loss: 0.6103516817092896 \n",
      "     Validation Step: 17 Validation Loss: 0.6108288168907166 \n",
      "     Validation Step: 18 Validation Loss: 0.6142305135726929 \n",
      "     Validation Step: 19 Validation Loss: 0.6146199703216553 \n",
      "     Validation Step: 20 Validation Loss: 0.6137214303016663 \n",
      "     Validation Step: 21 Validation Loss: 0.6137531995773315 \n",
      "     Validation Step: 22 Validation Loss: 0.6078223586082458 \n",
      "     Validation Step: 23 Validation Loss: 0.6156617403030396 \n",
      "     Validation Step: 24 Validation Loss: 0.6103823184967041 \n",
      "     Validation Step: 25 Validation Loss: 0.6152839064598083 \n",
      "     Validation Step: 26 Validation Loss: 0.610380232334137 \n",
      "     Validation Step: 27 Validation Loss: 0.6142837405204773 \n",
      "     Validation Step: 28 Validation Loss: 0.6149768233299255 \n",
      "     Validation Step: 29 Validation Loss: 0.6113353967666626 \n",
      "     Validation Step: 30 Validation Loss: 0.6183508634567261 \n",
      "     Validation Step: 31 Validation Loss: 0.6162856817245483 \n",
      "     Validation Step: 32 Validation Loss: 0.6137216687202454 \n",
      "     Validation Step: 33 Validation Loss: 0.6142251491546631 \n",
      "     Validation Step: 34 Validation Loss: 0.6106622219085693 \n",
      "     Validation Step: 35 Validation Loss: 0.6117037534713745 \n",
      "     Validation Step: 36 Validation Loss: 0.6158271431922913 \n",
      "     Validation Step: 37 Validation Loss: 0.6146308183670044 \n",
      "     Validation Step: 38 Validation Loss: 0.6170281767845154 \n",
      "     Validation Step: 39 Validation Loss: 0.6130913496017456 \n",
      "     Validation Step: 40 Validation Loss: 0.6113203763961792 \n",
      "     Validation Step: 41 Validation Loss: 0.6182363033294678 \n",
      "     Validation Step: 42 Validation Loss: 0.6184911131858826 \n",
      "     Validation Step: 43 Validation Loss: 0.6129417419433594 \n",
      "     Validation Step: 44 Validation Loss: 0.6176989078521729 \n",
      "Epoch: 39\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6148476600646973 \n",
      "     Training Step: 1 Training Loss: 0.6123165488243103 \n",
      "     Training Step: 2 Training Loss: 0.6141297817230225 \n",
      "     Training Step: 3 Training Loss: 0.6116337180137634 \n",
      "     Training Step: 4 Training Loss: 0.617820143699646 \n",
      "     Training Step: 5 Training Loss: 0.6153903007507324 \n",
      "     Training Step: 6 Training Loss: 0.6174565553665161 \n",
      "     Training Step: 7 Training Loss: 0.6123914122581482 \n",
      "     Training Step: 8 Training Loss: 0.6167051792144775 \n",
      "     Training Step: 9 Training Loss: 0.6135836839675903 \n",
      "     Training Step: 10 Training Loss: 0.6147056818008423 \n",
      "     Training Step: 11 Training Loss: 0.6137939095497131 \n",
      "     Training Step: 12 Training Loss: 0.6122803688049316 \n",
      "     Training Step: 13 Training Loss: 0.6153569221496582 \n",
      "     Training Step: 14 Training Loss: 0.6122878193855286 \n",
      "     Training Step: 15 Training Loss: 0.6135026812553406 \n",
      "     Training Step: 16 Training Loss: 0.6133074164390564 \n",
      "     Training Step: 17 Training Loss: 0.6142382025718689 \n",
      "     Training Step: 18 Training Loss: 0.6138234734535217 \n",
      "     Training Step: 19 Training Loss: 0.6153188347816467 \n",
      "     Training Step: 20 Training Loss: 0.6137924194335938 \n",
      "     Training Step: 21 Training Loss: 0.6097323894500732 \n",
      "     Training Step: 22 Training Loss: 0.61333829164505 \n",
      "     Training Step: 23 Training Loss: 0.6120914816856384 \n",
      "     Training Step: 24 Training Loss: 0.610080361366272 \n",
      "     Training Step: 25 Training Loss: 0.6154783368110657 \n",
      "     Training Step: 26 Training Loss: 0.6157844066619873 \n",
      "     Training Step: 27 Training Loss: 0.6157747507095337 \n",
      "     Training Step: 28 Training Loss: 0.6116306185722351 \n",
      "     Training Step: 29 Training Loss: 0.6154605150222778 \n",
      "     Training Step: 30 Training Loss: 0.6139277815818787 \n",
      "     Training Step: 31 Training Loss: 0.6146637797355652 \n",
      "     Training Step: 32 Training Loss: 0.6127377152442932 \n",
      "     Training Step: 33 Training Loss: 0.6147871017456055 \n",
      "     Training Step: 34 Training Loss: 0.6111817359924316 \n",
      "     Training Step: 35 Training Loss: 0.6136652827262878 \n",
      "     Training Step: 36 Training Loss: 0.6171677708625793 \n",
      "     Training Step: 37 Training Loss: 0.6102571487426758 \n",
      "     Training Step: 38 Training Loss: 0.6178082823753357 \n",
      "     Training Step: 39 Training Loss: 0.6140502691268921 \n",
      "     Training Step: 40 Training Loss: 0.6155418753623962 \n",
      "     Training Step: 41 Training Loss: 0.6150993704795837 \n",
      "     Training Step: 42 Training Loss: 0.6126604676246643 \n",
      "     Training Step: 43 Training Loss: 0.6147212982177734 \n",
      "     Training Step: 44 Training Loss: 0.614630937576294 \n",
      "     Training Step: 45 Training Loss: 0.6164416670799255 \n",
      "     Training Step: 46 Training Loss: 0.6184108257293701 \n",
      "     Training Step: 47 Training Loss: 0.613015353679657 \n",
      "     Training Step: 48 Training Loss: 0.6194270849227905 \n",
      "     Training Step: 49 Training Loss: 0.6147894859313965 \n",
      "     Training Step: 50 Training Loss: 0.6106415390968323 \n",
      "     Training Step: 51 Training Loss: 0.6197672486305237 \n",
      "     Training Step: 52 Training Loss: 0.6181488037109375 \n",
      "     Training Step: 53 Training Loss: 0.6131632328033447 \n",
      "     Training Step: 54 Training Loss: 0.6132421493530273 \n",
      "     Training Step: 55 Training Loss: 0.6142567992210388 \n",
      "     Training Step: 56 Training Loss: 0.6167394518852234 \n",
      "     Training Step: 57 Training Loss: 0.6159537434577942 \n",
      "     Training Step: 58 Training Loss: 0.6144458055496216 \n",
      "     Training Step: 59 Training Loss: 0.6168092489242554 \n",
      "     Training Step: 60 Training Loss: 0.6144704222679138 \n",
      "     Training Step: 61 Training Loss: 0.6180266737937927 \n",
      "     Training Step: 62 Training Loss: 0.6141669750213623 \n",
      "     Training Step: 63 Training Loss: 0.6198700666427612 \n",
      "     Training Step: 64 Training Loss: 0.6177841424942017 \n",
      "     Training Step: 65 Training Loss: 0.6151074767112732 \n",
      "     Training Step: 66 Training Loss: 0.6153187155723572 \n",
      "     Training Step: 67 Training Loss: 0.6165019869804382 \n",
      "     Training Step: 68 Training Loss: 0.6125653982162476 \n",
      "     Training Step: 69 Training Loss: 0.611465573310852 \n",
      "     Training Step: 70 Training Loss: 0.6133389472961426 \n",
      "     Training Step: 71 Training Loss: 0.6162449717521667 \n",
      "     Training Step: 72 Training Loss: 0.610042929649353 \n",
      "     Training Step: 73 Training Loss: 0.6145493388175964 \n",
      "     Training Step: 74 Training Loss: 0.6182965040206909 \n",
      "     Training Step: 75 Training Loss: 0.6154311895370483 \n",
      "     Training Step: 76 Training Loss: 0.6129288673400879 \n",
      "     Training Step: 77 Training Loss: 0.616798996925354 \n",
      "     Training Step: 78 Training Loss: 0.612937331199646 \n",
      "     Training Step: 79 Training Loss: 0.6155396103858948 \n",
      "     Training Step: 80 Training Loss: 0.61811363697052 \n",
      "     Training Step: 81 Training Loss: 0.610961377620697 \n",
      "     Training Step: 82 Training Loss: 0.6136072874069214 \n",
      "     Training Step: 83 Training Loss: 0.6107738614082336 \n",
      "     Training Step: 84 Training Loss: 0.6166733503341675 \n",
      "     Training Step: 85 Training Loss: 0.6133338809013367 \n",
      "     Training Step: 86 Training Loss: 0.6169344782829285 \n",
      "     Training Step: 87 Training Loss: 0.6106216907501221 \n",
      "     Training Step: 88 Training Loss: 0.6121394634246826 \n",
      "     Training Step: 89 Training Loss: 0.6125458478927612 \n",
      "     Training Step: 90 Training Loss: 0.6124776005744934 \n",
      "     Training Step: 91 Training Loss: 0.6153687238693237 \n",
      "     Training Step: 92 Training Loss: 0.6139115691184998 \n",
      "     Training Step: 93 Training Loss: 0.611786961555481 \n",
      "     Training Step: 94 Training Loss: 0.6097233891487122 \n",
      "     Training Step: 95 Training Loss: 0.6127631664276123 \n",
      "     Training Step: 96 Training Loss: 0.6103659272193909 \n",
      "     Training Step: 97 Training Loss: 0.6135427355766296 \n",
      "     Training Step: 98 Training Loss: 0.6169039607048035 \n",
      "     Training Step: 99 Training Loss: 0.6167814135551453 \n",
      "     Training Step: 100 Training Loss: 0.6131123304367065 \n",
      "     Training Step: 101 Training Loss: 0.6117404699325562 \n",
      "     Training Step: 102 Training Loss: 0.6140508055686951 \n",
      "     Training Step: 103 Training Loss: 0.6140550971031189 \n",
      "     Training Step: 104 Training Loss: 0.6171635985374451 \n",
      "     Training Step: 105 Training Loss: 0.6157841682434082 \n",
      "     Training Step: 106 Training Loss: 0.6132489442825317 \n",
      "     Training Step: 107 Training Loss: 0.6188860535621643 \n",
      "     Training Step: 108 Training Loss: 0.617250919342041 \n",
      "     Training Step: 109 Training Loss: 0.6160426735877991 \n",
      "     Training Step: 110 Training Loss: 0.61192387342453 \n",
      "     Training Step: 111 Training Loss: 0.6168110370635986 \n",
      "     Training Step: 112 Training Loss: 0.6210016012191772 \n",
      "     Training Step: 113 Training Loss: 0.6146501898765564 \n",
      "     Training Step: 114 Training Loss: 0.6108468174934387 \n",
      "     Training Step: 115 Training Loss: 0.613017737865448 \n",
      "     Training Step: 116 Training Loss: 0.6146276593208313 \n",
      "     Training Step: 117 Training Loss: 0.6112191081047058 \n",
      "     Training Step: 118 Training Loss: 0.6108866333961487 \n",
      "     Training Step: 119 Training Loss: 0.6133743524551392 \n",
      "     Training Step: 120 Training Loss: 0.6105919480323792 \n",
      "     Training Step: 121 Training Loss: 0.6115820407867432 \n",
      "     Training Step: 122 Training Loss: 0.6168955564498901 \n",
      "     Training Step: 123 Training Loss: 0.6114782691001892 \n",
      "     Training Step: 124 Training Loss: 0.6154317855834961 \n",
      "     Training Step: 125 Training Loss: 0.610850989818573 \n",
      "     Training Step: 126 Training Loss: 0.6142379641532898 \n",
      "     Training Step: 127 Training Loss: 0.6154220700263977 \n",
      "     Training Step: 128 Training Loss: 0.6114810705184937 \n",
      "     Training Step: 129 Training Loss: 0.6145126223564148 \n",
      "     Training Step: 130 Training Loss: 0.6111940145492554 \n",
      "     Training Step: 131 Training Loss: 0.6129197478294373 \n",
      "     Training Step: 132 Training Loss: 0.6131705045700073 \n",
      "     Training Step: 133 Training Loss: 0.6144902110099792 \n",
      "     Training Step: 134 Training Loss: 0.6142904758453369 \n",
      "     Training Step: 135 Training Loss: 0.6114912629127502 \n",
      "     Training Step: 136 Training Loss: 0.6153104305267334 \n",
      "     Training Step: 137 Training Loss: 0.608396589756012 \n",
      "     Training Step: 138 Training Loss: 0.6148396134376526 \n",
      "     Training Step: 139 Training Loss: 0.6147028207778931 \n",
      "     Training Step: 140 Training Loss: 0.6133868098258972 \n",
      "     Training Step: 141 Training Loss: 0.6163089871406555 \n",
      "     Training Step: 142 Training Loss: 0.6106058955192566 \n",
      "     Training Step: 143 Training Loss: 0.6186859607696533 \n",
      "     Training Step: 144 Training Loss: 0.6117464900016785 \n",
      "     Training Step: 145 Training Loss: 0.6128767132759094 \n",
      "     Training Step: 146 Training Loss: 0.6120917797088623 \n",
      "     Training Step: 147 Training Loss: 0.614740788936615 \n",
      "     Training Step: 148 Training Loss: 0.6138572692871094 \n",
      "     Training Step: 149 Training Loss: 0.6100791692733765 \n",
      "     Training Step: 150 Training Loss: 0.6118426322937012 \n",
      "     Training Step: 151 Training Loss: 0.6125784516334534 \n",
      "     Training Step: 152 Training Loss: 0.6181936264038086 \n",
      "     Training Step: 153 Training Loss: 0.6094309687614441 \n",
      "     Training Step: 154 Training Loss: 0.614727795124054 \n",
      "     Training Step: 155 Training Loss: 0.6148902773857117 \n",
      "     Training Step: 156 Training Loss: 0.6116323471069336 \n",
      "     Training Step: 157 Training Loss: 0.61495441198349 \n",
      "     Training Step: 158 Training Loss: 0.6136353611946106 \n",
      "     Training Step: 159 Training Loss: 0.6143134832382202 \n",
      "     Training Step: 160 Training Loss: 0.6164604425430298 \n",
      "     Training Step: 161 Training Loss: 0.6128838062286377 \n",
      "     Training Step: 162 Training Loss: 0.616722583770752 \n",
      "     Training Step: 163 Training Loss: 0.610066831111908 \n",
      "     Training Step: 164 Training Loss: 0.6158041954040527 \n",
      "     Training Step: 165 Training Loss: 0.6146541237831116 \n",
      "     Training Step: 166 Training Loss: 0.6184654235839844 \n",
      "     Training Step: 167 Training Loss: 0.6124081611633301 \n",
      "     Training Step: 168 Training Loss: 0.6143908500671387 \n",
      "     Training Step: 169 Training Loss: 0.6188567280769348 \n",
      "     Training Step: 170 Training Loss: 0.6115117073059082 \n",
      "     Training Step: 171 Training Loss: 0.6107226610183716 \n",
      "     Training Step: 172 Training Loss: 0.6135229468345642 \n",
      "     Training Step: 173 Training Loss: 0.6170324683189392 \n",
      "     Training Step: 174 Training Loss: 0.6122950911521912 \n",
      "     Training Step: 175 Training Loss: 0.61607825756073 \n",
      "     Training Step: 176 Training Loss: 0.6143628358840942 \n",
      "     Training Step: 177 Training Loss: 0.6136910915374756 \n",
      "     Training Step: 178 Training Loss: 0.6106981635093689 \n",
      "     Training Step: 179 Training Loss: 0.6151376962661743 \n",
      "     Training Step: 180 Training Loss: 0.6121665835380554 \n",
      "     Training Step: 181 Training Loss: 0.611913800239563 \n",
      "     Training Step: 182 Training Loss: 0.6101561188697815 \n",
      "     Training Step: 183 Training Loss: 0.6172482371330261 \n",
      "     Training Step: 184 Training Loss: 0.6172525882720947 \n",
      "     Training Step: 185 Training Loss: 0.6151911616325378 \n",
      "     Training Step: 186 Training Loss: 0.6123995184898376 \n",
      "     Training Step: 187 Training Loss: 0.6132814884185791 \n",
      "     Training Step: 188 Training Loss: 0.614076554775238 \n",
      "     Training Step: 189 Training Loss: 0.6183225512504578 \n",
      "     Training Step: 190 Training Loss: 0.6164215207099915 \n",
      "     Training Step: 191 Training Loss: 0.6122076511383057 \n",
      "     Training Step: 192 Training Loss: 0.6134941577911377 \n",
      "     Training Step: 193 Training Loss: 0.6116342544555664 \n",
      "     Training Step: 194 Training Loss: 0.6115046739578247 \n",
      "     Training Step: 195 Training Loss: 0.6092536449432373 \n",
      "     Training Step: 196 Training Loss: 0.6144683361053467 \n",
      "     Training Step: 197 Training Loss: 0.6104890704154968 \n",
      "     Training Step: 198 Training Loss: 0.617885410785675 \n",
      "     Training Step: 199 Training Loss: 0.6176320314407349 \n",
      "     Training Step: 200 Training Loss: 0.6098150014877319 \n",
      "     Training Step: 201 Training Loss: 0.6147682666778564 \n",
      "     Training Step: 202 Training Loss: 0.6128640174865723 \n",
      "     Training Step: 203 Training Loss: 0.616217851638794 \n",
      "     Training Step: 204 Training Loss: 0.6155532002449036 \n",
      "     Training Step: 205 Training Loss: 0.6149552464485168 \n",
      "     Training Step: 206 Training Loss: 0.6177299618721008 \n",
      "     Training Step: 207 Training Loss: 0.617706298828125 \n",
      "     Training Step: 208 Training Loss: 0.6156757473945618 \n",
      "     Training Step: 209 Training Loss: 0.6195770502090454 \n",
      "     Training Step: 210 Training Loss: 0.6128166317939758 \n",
      "     Training Step: 211 Training Loss: 0.6132463216781616 \n",
      "     Training Step: 212 Training Loss: 0.61171954870224 \n",
      "     Training Step: 213 Training Loss: 0.6157464981079102 \n",
      "     Training Step: 214 Training Loss: 0.6169261336326599 \n",
      "     Training Step: 215 Training Loss: 0.6118787527084351 \n",
      "     Training Step: 216 Training Loss: 0.6123889088630676 \n",
      "     Training Step: 217 Training Loss: 0.6094958782196045 \n",
      "     Training Step: 218 Training Loss: 0.6177979707717896 \n",
      "     Training Step: 219 Training Loss: 0.6167119145393372 \n",
      "     Training Step: 220 Training Loss: 0.6144381761550903 \n",
      "     Training Step: 221 Training Loss: 0.6149871945381165 \n",
      "     Training Step: 222 Training Loss: 0.6140910387039185 \n",
      "     Training Step: 223 Training Loss: 0.6184963583946228 \n",
      "     Training Step: 224 Training Loss: 0.61588454246521 \n",
      "     Training Step: 225 Training Loss: 0.6118707656860352 \n",
      "     Training Step: 226 Training Loss: 0.615449070930481 \n",
      "     Training Step: 227 Training Loss: 0.6151921153068542 \n",
      "     Training Step: 228 Training Loss: 0.6185779571533203 \n",
      "     Training Step: 229 Training Loss: 0.6183171272277832 \n",
      "     Training Step: 230 Training Loss: 0.6201717257499695 \n",
      "     Training Step: 231 Training Loss: 0.6168323159217834 \n",
      "     Training Step: 232 Training Loss: 0.611919641494751 \n",
      "     Training Step: 233 Training Loss: 0.6116679310798645 \n",
      "     Training Step: 234 Training Loss: 0.6162943243980408 \n",
      "     Training Step: 235 Training Loss: 0.615760862827301 \n",
      "     Training Step: 236 Training Loss: 0.6104287505149841 \n",
      "     Training Step: 237 Training Loss: 0.6118953824043274 \n",
      "     Training Step: 238 Training Loss: 0.6123932003974915 \n",
      "     Training Step: 239 Training Loss: 0.6156126260757446 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6183683276176453 \n",
      "     Validation Step: 1 Validation Loss: 0.6137329339981079 \n",
      "     Validation Step: 2 Validation Loss: 0.6136475801467896 \n",
      "     Validation Step: 3 Validation Loss: 0.6105304956436157 \n",
      "     Validation Step: 4 Validation Loss: 0.6147233247756958 \n",
      "     Validation Step: 5 Validation Loss: 0.613051176071167 \n",
      "     Validation Step: 6 Validation Loss: 0.6105595827102661 \n",
      "     Validation Step: 7 Validation Loss: 0.6112263798713684 \n",
      "     Validation Step: 8 Validation Loss: 0.6112205982208252 \n",
      "     Validation Step: 9 Validation Loss: 0.6184624433517456 \n",
      "     Validation Step: 10 Validation Loss: 0.6119235157966614 \n",
      "     Validation Step: 11 Validation Loss: 0.6146079897880554 \n",
      "     Validation Step: 12 Validation Loss: 0.6141502261161804 \n",
      "     Validation Step: 13 Validation Loss: 0.6116671562194824 \n",
      "     Validation Step: 14 Validation Loss: 0.6176596283912659 \n",
      "     Validation Step: 15 Validation Loss: 0.6101788878440857 \n",
      "     Validation Step: 16 Validation Loss: 0.6133435964584351 \n",
      "     Validation Step: 17 Validation Loss: 0.6137623190879822 \n",
      "     Validation Step: 18 Validation Loss: 0.6149322986602783 \n",
      "     Validation Step: 19 Validation Loss: 0.6185959577560425 \n",
      "     Validation Step: 20 Validation Loss: 0.6173548102378845 \n",
      "     Validation Step: 21 Validation Loss: 0.6101890802383423 \n",
      "     Validation Step: 22 Validation Loss: 0.6156922578811646 \n",
      "     Validation Step: 23 Validation Loss: 0.615279495716095 \n",
      "     Validation Step: 24 Validation Loss: 0.6142727732658386 \n",
      "     Validation Step: 25 Validation Loss: 0.6148895025253296 \n",
      "     Validation Step: 26 Validation Loss: 0.6170837879180908 \n",
      "     Validation Step: 27 Validation Loss: 0.612170934677124 \n",
      "     Validation Step: 28 Validation Loss: 0.6154026985168457 \n",
      "     Validation Step: 29 Validation Loss: 0.6142964363098145 \n",
      "     Validation Step: 30 Validation Loss: 0.6159086227416992 \n",
      "     Validation Step: 31 Validation Loss: 0.6142117977142334 \n",
      "     Validation Step: 32 Validation Loss: 0.6106672883033752 \n",
      "     Validation Step: 33 Validation Loss: 0.6128524541854858 \n",
      "     Validation Step: 34 Validation Loss: 0.611599326133728 \n",
      "     Validation Step: 35 Validation Loss: 0.6185577511787415 \n",
      "     Validation Step: 36 Validation Loss: 0.607580304145813 \n",
      "     Validation Step: 37 Validation Loss: 0.6101875901222229 \n",
      "     Validation Step: 38 Validation Loss: 0.6181632876396179 \n",
      "     Validation Step: 39 Validation Loss: 0.6163406372070312 \n",
      "     Validation Step: 40 Validation Loss: 0.6161128878593445 \n",
      "     Validation Step: 41 Validation Loss: 0.6151424050331116 \n",
      "     Validation Step: 42 Validation Loss: 0.6145792007446289 \n",
      "     Validation Step: 43 Validation Loss: 0.6156686544418335 \n",
      "     Validation Step: 44 Validation Loss: 0.6177897453308105 \n",
      "Epoch: 40\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6105343699455261 \n",
      "     Training Step: 1 Training Loss: 0.6106337904930115 \n",
      "     Training Step: 2 Training Loss: 0.6179215908050537 \n",
      "     Training Step: 3 Training Loss: 0.615515410900116 \n",
      "     Training Step: 4 Training Loss: 0.6131473779678345 \n",
      "     Training Step: 5 Training Loss: 0.6155665516853333 \n",
      "     Training Step: 6 Training Loss: 0.6137681603431702 \n",
      "     Training Step: 7 Training Loss: 0.6171594858169556 \n",
      "     Training Step: 8 Training Loss: 0.6151213049888611 \n",
      "     Training Step: 9 Training Loss: 0.6115320920944214 \n",
      "     Training Step: 10 Training Loss: 0.6118522882461548 \n",
      "     Training Step: 11 Training Loss: 0.6176926493644714 \n",
      "     Training Step: 12 Training Loss: 0.6184237003326416 \n",
      "     Training Step: 13 Training Loss: 0.6134985685348511 \n",
      "     Training Step: 14 Training Loss: 0.6157121658325195 \n",
      "     Training Step: 15 Training Loss: 0.6179981231689453 \n",
      "     Training Step: 16 Training Loss: 0.6119582056999207 \n",
      "     Training Step: 17 Training Loss: 0.6132559180259705 \n",
      "     Training Step: 18 Training Loss: 0.6164463758468628 \n",
      "     Training Step: 19 Training Loss: 0.6142740845680237 \n",
      "     Training Step: 20 Training Loss: 0.6151916980743408 \n",
      "     Training Step: 21 Training Loss: 0.6140373349189758 \n",
      "     Training Step: 22 Training Loss: 0.618626594543457 \n",
      "     Training Step: 23 Training Loss: 0.6150990128517151 \n",
      "     Training Step: 24 Training Loss: 0.61542147397995 \n",
      "     Training Step: 25 Training Loss: 0.6167205572128296 \n",
      "     Training Step: 26 Training Loss: 0.6146467328071594 \n",
      "     Training Step: 27 Training Loss: 0.6095370054244995 \n",
      "     Training Step: 28 Training Loss: 0.6177677512168884 \n",
      "     Training Step: 29 Training Loss: 0.6167805194854736 \n",
      "     Training Step: 30 Training Loss: 0.6145002841949463 \n",
      "     Training Step: 31 Training Loss: 0.6111882925033569 \n",
      "     Training Step: 32 Training Loss: 0.6134843230247498 \n",
      "     Training Step: 33 Training Loss: 0.6209940910339355 \n",
      "     Training Step: 34 Training Loss: 0.6154187321662903 \n",
      "     Training Step: 35 Training Loss: 0.6144779324531555 \n",
      "     Training Step: 36 Training Loss: 0.6157985329627991 \n",
      "     Training Step: 37 Training Loss: 0.6121692657470703 \n",
      "     Training Step: 38 Training Loss: 0.6136178374290466 \n",
      "     Training Step: 39 Training Loss: 0.6117026805877686 \n",
      "     Training Step: 40 Training Loss: 0.6128808856010437 \n",
      "     Training Step: 41 Training Loss: 0.6123872399330139 \n",
      "     Training Step: 42 Training Loss: 0.6168679594993591 \n",
      "     Training Step: 43 Training Loss: 0.618708610534668 \n",
      "     Training Step: 44 Training Loss: 0.6194630265235901 \n",
      "     Training Step: 45 Training Loss: 0.6117918491363525 \n",
      "     Training Step: 46 Training Loss: 0.610768735408783 \n",
      "     Training Step: 47 Training Loss: 0.6129836440086365 \n",
      "     Training Step: 48 Training Loss: 0.6186002492904663 \n",
      "     Training Step: 49 Training Loss: 0.6120995879173279 \n",
      "     Training Step: 50 Training Loss: 0.6155439019203186 \n",
      "     Training Step: 51 Training Loss: 0.6100674867630005 \n",
      "     Training Step: 52 Training Loss: 0.6094546318054199 \n",
      "     Training Step: 53 Training Loss: 0.6082910299301147 \n",
      "     Training Step: 54 Training Loss: 0.6158396601676941 \n",
      "     Training Step: 55 Training Loss: 0.610758364200592 \n",
      "     Training Step: 56 Training Loss: 0.6105889678001404 \n",
      "     Training Step: 57 Training Loss: 0.614736795425415 \n",
      "     Training Step: 58 Training Loss: 0.6125003695487976 \n",
      "     Training Step: 59 Training Loss: 0.6167377233505249 \n",
      "     Training Step: 60 Training Loss: 0.6112499833106995 \n",
      "     Training Step: 61 Training Loss: 0.6136730909347534 \n",
      "     Training Step: 62 Training Loss: 0.6154571771621704 \n",
      "     Training Step: 63 Training Loss: 0.6147683262825012 \n",
      "     Training Step: 64 Training Loss: 0.613214373588562 \n",
      "     Training Step: 65 Training Loss: 0.6196638345718384 \n",
      "     Training Step: 66 Training Loss: 0.6167230010032654 \n",
      "     Training Step: 67 Training Loss: 0.6110893487930298 \n",
      "     Training Step: 68 Training Loss: 0.6144942045211792 \n",
      "     Training Step: 69 Training Loss: 0.6153026223182678 \n",
      "     Training Step: 70 Training Loss: 0.6123326420783997 \n",
      "     Training Step: 71 Training Loss: 0.6189350485801697 \n",
      "     Training Step: 72 Training Loss: 0.6143205761909485 \n",
      "     Training Step: 73 Training Loss: 0.6097451448440552 \n",
      "     Training Step: 74 Training Loss: 0.6177896857261658 \n",
      "     Training Step: 75 Training Loss: 0.6122746467590332 \n",
      "     Training Step: 76 Training Loss: 0.6156167984008789 \n",
      "     Training Step: 77 Training Loss: 0.6184825897216797 \n",
      "     Training Step: 78 Training Loss: 0.6149066090583801 \n",
      "     Training Step: 79 Training Loss: 0.6102791428565979 \n",
      "     Training Step: 80 Training Loss: 0.6168200969696045 \n",
      "     Training Step: 81 Training Loss: 0.614231526851654 \n",
      "     Training Step: 82 Training Loss: 0.6127908825874329 \n",
      "     Training Step: 83 Training Loss: 0.6169517040252686 \n",
      "     Training Step: 84 Training Loss: 0.617139458656311 \n",
      "     Training Step: 85 Training Loss: 0.6128686666488647 \n",
      "     Training Step: 86 Training Loss: 0.6139793395996094 \n",
      "     Training Step: 87 Training Loss: 0.609862744808197 \n",
      "     Training Step: 88 Training Loss: 0.6167594790458679 \n",
      "     Training Step: 89 Training Loss: 0.614715039730072 \n",
      "     Training Step: 90 Training Loss: 0.6103789806365967 \n",
      "     Training Step: 91 Training Loss: 0.6111742258071899 \n",
      "     Training Step: 92 Training Loss: 0.615327000617981 \n",
      "     Training Step: 93 Training Loss: 0.6114022135734558 \n",
      "     Training Step: 94 Training Loss: 0.6143780946731567 \n",
      "     Training Step: 95 Training Loss: 0.6184252500534058 \n",
      "     Training Step: 96 Training Loss: 0.6145313382148743 \n",
      "     Training Step: 97 Training Loss: 0.6180395483970642 \n",
      "     Training Step: 98 Training Loss: 0.611870527267456 \n",
      "     Training Step: 99 Training Loss: 0.61395263671875 \n",
      "     Training Step: 100 Training Loss: 0.6114592552185059 \n",
      "     Training Step: 101 Training Loss: 0.6125693321228027 \n",
      "     Training Step: 102 Training Loss: 0.6123045682907104 \n",
      "     Training Step: 103 Training Loss: 0.6151816844940186 \n",
      "     Training Step: 104 Training Loss: 0.6175426840782166 \n",
      "     Training Step: 105 Training Loss: 0.6146610379219055 \n",
      "     Training Step: 106 Training Loss: 0.6180738806724548 \n",
      "     Training Step: 107 Training Loss: 0.6116547584533691 \n",
      "     Training Step: 108 Training Loss: 0.6146907210350037 \n",
      "     Training Step: 109 Training Loss: 0.614752471446991 \n",
      "     Training Step: 110 Training Loss: 0.6138274073600769 \n",
      "     Training Step: 111 Training Loss: 0.6178467273712158 \n",
      "     Training Step: 112 Training Loss: 0.6144772171974182 \n",
      "     Training Step: 113 Training Loss: 0.6138398051261902 \n",
      "     Training Step: 114 Training Loss: 0.6153256893157959 \n",
      "     Training Step: 115 Training Loss: 0.6115955710411072 \n",
      "     Training Step: 116 Training Loss: 0.6118890643119812 \n",
      "     Training Step: 117 Training Loss: 0.6099967360496521 \n",
      "     Training Step: 118 Training Loss: 0.6127524971961975 \n",
      "     Training Step: 119 Training Loss: 0.6116008162498474 \n",
      "     Training Step: 120 Training Loss: 0.6129794716835022 \n",
      "     Training Step: 121 Training Loss: 0.6124036312103271 \n",
      "     Training Step: 122 Training Loss: 0.6125313639640808 \n",
      "     Training Step: 123 Training Loss: 0.6133555769920349 \n",
      "     Training Step: 124 Training Loss: 0.6129410862922668 \n",
      "     Training Step: 125 Training Loss: 0.6115700006484985 \n",
      "     Training Step: 126 Training Loss: 0.6135187745094299 \n",
      "     Training Step: 127 Training Loss: 0.6131816506385803 \n",
      "     Training Step: 128 Training Loss: 0.6136738061904907 \n",
      "     Training Step: 129 Training Loss: 0.6124852895736694 \n",
      "     Training Step: 130 Training Loss: 0.6121475696563721 \n",
      "     Training Step: 131 Training Loss: 0.6106857657432556 \n",
      "     Training Step: 132 Training Loss: 0.6172458529472351 \n",
      "     Training Step: 133 Training Loss: 0.6118685007095337 \n",
      "     Training Step: 134 Training Loss: 0.6135382056236267 \n",
      "     Training Step: 135 Training Loss: 0.6106134057044983 \n",
      "     Training Step: 136 Training Loss: 0.6164912581443787 \n",
      "     Training Step: 137 Training Loss: 0.615929901599884 \n",
      "     Training Step: 138 Training Loss: 0.6123131513595581 \n",
      "     Training Step: 139 Training Loss: 0.617190420627594 \n",
      "     Training Step: 140 Training Loss: 0.6129593849182129 \n",
      "     Training Step: 141 Training Loss: 0.6133819818496704 \n",
      "     Training Step: 142 Training Loss: 0.6166887879371643 \n",
      "     Training Step: 143 Training Loss: 0.619935154914856 \n",
      "     Training Step: 144 Training Loss: 0.6102126836776733 \n",
      "     Training Step: 145 Training Loss: 0.6173925399780273 \n",
      "     Training Step: 146 Training Loss: 0.6159577965736389 \n",
      "     Training Step: 147 Training Loss: 0.614106297492981 \n",
      "     Training Step: 148 Training Loss: 0.6146681308746338 \n",
      "     Training Step: 149 Training Loss: 0.6142463088035583 \n",
      "     Training Step: 150 Training Loss: 0.6160535216331482 \n",
      "     Training Step: 151 Training Loss: 0.6149888038635254 \n",
      "     Training Step: 152 Training Loss: 0.6202510595321655 \n",
      "     Training Step: 153 Training Loss: 0.6109650135040283 \n",
      "     Training Step: 154 Training Loss: 0.6143995523452759 \n",
      "     Training Step: 155 Training Loss: 0.6118473410606384 \n",
      "     Training Step: 156 Training Loss: 0.6198126077651978 \n",
      "     Training Step: 157 Training Loss: 0.6101104617118835 \n",
      "     Training Step: 158 Training Loss: 0.6147102117538452 \n",
      "     Training Step: 159 Training Loss: 0.6182510256767273 \n",
      "     Training Step: 160 Training Loss: 0.6117106676101685 \n",
      "     Training Step: 161 Training Loss: 0.616215705871582 \n",
      "     Training Step: 162 Training Loss: 0.6155482530593872 \n",
      "     Training Step: 163 Training Loss: 0.6160130500793457 \n",
      "     Training Step: 164 Training Loss: 0.6148186922073364 \n",
      "     Training Step: 165 Training Loss: 0.6177935004234314 \n",
      "     Training Step: 166 Training Loss: 0.6162059903144836 \n",
      "     Training Step: 167 Training Loss: 0.6143493056297302 \n",
      "     Training Step: 168 Training Loss: 0.6147378087043762 \n",
      "     Training Step: 169 Training Loss: 0.6120034456253052 \n",
      "     Training Step: 170 Training Loss: 0.6133444309234619 \n",
      "     Training Step: 171 Training Loss: 0.615323543548584 \n",
      "     Training Step: 172 Training Loss: 0.6122639775276184 \n",
      "     Training Step: 173 Training Loss: 0.6107140183448792 \n",
      "     Training Step: 174 Training Loss: 0.6097419857978821 \n",
      "     Training Step: 175 Training Loss: 0.611592710018158 \n",
      "     Training Step: 176 Training Loss: 0.6148828268051147 \n",
      "     Training Step: 177 Training Loss: 0.6169407963752747 \n",
      "     Training Step: 178 Training Loss: 0.6149828433990479 \n",
      "     Training Step: 179 Training Loss: 0.6139509081840515 \n",
      "     Training Step: 180 Training Loss: 0.6131552457809448 \n",
      "     Training Step: 181 Training Loss: 0.613432765007019 \n",
      "     Training Step: 182 Training Loss: 0.6143816113471985 \n",
      "     Training Step: 183 Training Loss: 0.6125847101211548 \n",
      "     Training Step: 184 Training Loss: 0.6147957444190979 \n",
      "     Training Step: 185 Training Loss: 0.6133021712303162 \n",
      "     Training Step: 186 Training Loss: 0.6141959428787231 \n",
      "     Training Step: 187 Training Loss: 0.6165350079536438 \n",
      "     Training Step: 188 Training Loss: 0.6134539246559143 \n",
      "     Training Step: 189 Training Loss: 0.6116257905960083 \n",
      "     Training Step: 190 Training Loss: 0.6133561730384827 \n",
      "     Training Step: 191 Training Loss: 0.6167495250701904 \n",
      "     Training Step: 192 Training Loss: 0.6101357340812683 \n",
      "     Training Step: 193 Training Loss: 0.6154952645301819 \n",
      "     Training Step: 194 Training Loss: 0.6161714792251587 \n",
      "     Training Step: 195 Training Loss: 0.6140891313552856 \n",
      "     Training Step: 196 Training Loss: 0.6171069145202637 \n",
      "     Training Step: 197 Training Loss: 0.6123102307319641 \n",
      "     Training Step: 198 Training Loss: 0.6163746118545532 \n",
      "     Training Step: 199 Training Loss: 0.6166658401489258 \n",
      "     Training Step: 200 Training Loss: 0.6153837442398071 \n",
      "     Training Step: 201 Training Loss: 0.6107473969459534 \n",
      "     Training Step: 202 Training Loss: 0.6132263541221619 \n",
      "     Training Step: 203 Training Loss: 0.612674355506897 \n",
      "     Training Step: 204 Training Loss: 0.6149548292160034 \n",
      "     Training Step: 205 Training Loss: 0.6130662560462952 \n",
      "     Training Step: 206 Training Loss: 0.6116562485694885 \n",
      "     Training Step: 207 Training Loss: 0.6120988726615906 \n",
      "     Training Step: 208 Training Loss: 0.6151940822601318 \n",
      "     Training Step: 209 Training Loss: 0.6140480637550354 \n",
      "     Training Step: 210 Training Loss: 0.6121740341186523 \n",
      "     Training Step: 211 Training Loss: 0.6146807670593262 \n",
      "     Training Step: 212 Training Loss: 0.6157683730125427 \n",
      "     Training Step: 213 Training Loss: 0.6117129921913147 \n",
      "     Training Step: 214 Training Loss: 0.6115713715553284 \n",
      "     Training Step: 215 Training Loss: 0.6182863116264343 \n",
      "     Training Step: 216 Training Loss: 0.6141382455825806 \n",
      "     Training Step: 217 Training Loss: 0.6181331276893616 \n",
      "     Training Step: 218 Training Loss: 0.6093292236328125 \n",
      "     Training Step: 219 Training Loss: 0.6169109344482422 \n",
      "     Training Step: 220 Training Loss: 0.617704451084137 \n",
      "     Training Step: 221 Training Loss: 0.6157637238502502 \n",
      "     Training Step: 222 Training Loss: 0.613777756690979 \n",
      "     Training Step: 223 Training Loss: 0.6156764626502991 \n",
      "     Training Step: 224 Training Loss: 0.6136954426765442 \n",
      "     Training Step: 225 Training Loss: 0.6146200299263 \n",
      "     Training Step: 226 Training Loss: 0.6115158796310425 \n",
      "     Training Step: 227 Training Loss: 0.6168203353881836 \n",
      "     Training Step: 228 Training Loss: 0.6105347871780396 \n",
      "     Training Step: 229 Training Loss: 0.612778902053833 \n",
      "     Training Step: 230 Training Loss: 0.6189299821853638 \n",
      "     Training Step: 231 Training Loss: 0.6104455590248108 \n",
      "     Training Step: 232 Training Loss: 0.6129299998283386 \n",
      "     Training Step: 233 Training Loss: 0.6152544021606445 \n",
      "     Training Step: 234 Training Loss: 0.614165186882019 \n",
      "     Training Step: 235 Training Loss: 0.6153497695922852 \n",
      "     Training Step: 236 Training Loss: 0.6123802661895752 \n",
      "     Training Step: 237 Training Loss: 0.6163033843040466 \n",
      "     Training Step: 238 Training Loss: 0.616827130317688 \n",
      "     Training Step: 239 Training Loss: 0.6155384182929993 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6142970323562622 \n",
      "     Validation Step: 1 Validation Loss: 0.6117234826087952 \n",
      "     Validation Step: 2 Validation Loss: 0.6150527000427246 \n",
      "     Validation Step: 3 Validation Loss: 0.6141657829284668 \n",
      "     Validation Step: 4 Validation Loss: 0.6141785979270935 \n",
      "     Validation Step: 5 Validation Loss: 0.611259937286377 \n",
      "     Validation Step: 6 Validation Loss: 0.6175978779792786 \n",
      "     Validation Step: 7 Validation Loss: 0.6184757351875305 \n",
      "     Validation Step: 8 Validation Loss: 0.6136738657951355 \n",
      "     Validation Step: 9 Validation Loss: 0.6076998114585876 \n",
      "     Validation Step: 10 Validation Loss: 0.6130361557006836 \n",
      "     Validation Step: 11 Validation Loss: 0.6137075424194336 \n",
      "     Validation Step: 12 Validation Loss: 0.6128897666931152 \n",
      "     Validation Step: 13 Validation Loss: 0.6116276979446411 \n",
      "     Validation Step: 14 Validation Loss: 0.6148653626441956 \n",
      "     Validation Step: 15 Validation Loss: 0.6136831641197205 \n",
      "     Validation Step: 16 Validation Loss: 0.6145851016044617 \n",
      "     Validation Step: 17 Validation Loss: 0.6119627952575684 \n",
      "     Validation Step: 18 Validation Loss: 0.6162590980529785 \n",
      "     Validation Step: 19 Validation Loss: 0.6182228326797485 \n",
      "     Validation Step: 20 Validation Loss: 0.6102710962295532 \n",
      "     Validation Step: 21 Validation Loss: 0.6102907061576843 \n",
      "     Validation Step: 22 Validation Loss: 0.617296576499939 \n",
      "     Validation Step: 23 Validation Loss: 0.6149308085441589 \n",
      "     Validation Step: 24 Validation Loss: 0.6142289042472839 \n",
      "     Validation Step: 25 Validation Loss: 0.6184597611427307 \n",
      "     Validation Step: 26 Validation Loss: 0.6158068776130676 \n",
      "     Validation Step: 27 Validation Loss: 0.6156203746795654 \n",
      "     Validation Step: 28 Validation Loss: 0.6146600842475891 \n",
      "     Validation Step: 29 Validation Loss: 0.6180476546287537 \n",
      "     Validation Step: 30 Validation Loss: 0.612212061882019 \n",
      "     Validation Step: 31 Validation Loss: 0.6160006523132324 \n",
      "     Validation Step: 32 Validation Loss: 0.6155833005905151 \n",
      "     Validation Step: 33 Validation Loss: 0.617684543132782 \n",
      "     Validation Step: 34 Validation Loss: 0.6102400422096252 \n",
      "     Validation Step: 35 Validation Loss: 0.6107414960861206 \n",
      "     Validation Step: 36 Validation Loss: 0.6133478283882141 \n",
      "     Validation Step: 37 Validation Loss: 0.6112425327301025 \n",
      "     Validation Step: 38 Validation Loss: 0.6153354644775391 \n",
      "     Validation Step: 39 Validation Loss: 0.6105930209159851 \n",
      "     Validation Step: 40 Validation Loss: 0.6152414083480835 \n",
      "     Validation Step: 41 Validation Loss: 0.6170119047164917 \n",
      "     Validation Step: 42 Validation Loss: 0.6105782389640808 \n",
      "     Validation Step: 43 Validation Loss: 0.6145761609077454 \n",
      "     Validation Step: 44 Validation Loss: 0.6183260083198547 \n",
      "Epoch: 41\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6168858408927917 \n",
      "     Training Step: 1 Training Loss: 0.6116088628768921 \n",
      "     Training Step: 2 Training Loss: 0.6154215931892395 \n",
      "     Training Step: 3 Training Loss: 0.6131755113601685 \n",
      "     Training Step: 4 Training Loss: 0.6147469282150269 \n",
      "     Training Step: 5 Training Loss: 0.6167029738426208 \n",
      "     Training Step: 6 Training Loss: 0.6155579090118408 \n",
      "     Training Step: 7 Training Loss: 0.6109306812286377 \n",
      "     Training Step: 8 Training Loss: 0.6128696799278259 \n",
      "     Training Step: 9 Training Loss: 0.6182589530944824 \n",
      "     Training Step: 10 Training Loss: 0.6105917096138 \n",
      "     Training Step: 11 Training Loss: 0.6164624691009521 \n",
      "     Training Step: 12 Training Loss: 0.6168394088745117 \n",
      "     Training Step: 13 Training Loss: 0.6107094287872314 \n",
      "     Training Step: 14 Training Loss: 0.6171826720237732 \n",
      "     Training Step: 15 Training Loss: 0.6117415428161621 \n",
      "     Training Step: 16 Training Loss: 0.6133117079734802 \n",
      "     Training Step: 17 Training Loss: 0.6121748089790344 \n",
      "     Training Step: 18 Training Loss: 0.6142229437828064 \n",
      "     Training Step: 19 Training Loss: 0.6151143908500671 \n",
      "     Training Step: 20 Training Loss: 0.6168100833892822 \n",
      "     Training Step: 21 Training Loss: 0.615468442440033 \n",
      "     Training Step: 22 Training Loss: 0.6140665411949158 \n",
      "     Training Step: 23 Training Loss: 0.6152021884918213 \n",
      "     Training Step: 24 Training Loss: 0.6156814098358154 \n",
      "     Training Step: 25 Training Loss: 0.612442135810852 \n",
      "     Training Step: 26 Training Loss: 0.6104398369789124 \n",
      "     Training Step: 27 Training Loss: 0.6132434606552124 \n",
      "     Training Step: 28 Training Loss: 0.6112157702445984 \n",
      "     Training Step: 29 Training Loss: 0.6165761351585388 \n",
      "     Training Step: 30 Training Loss: 0.6133363246917725 \n",
      "     Training Step: 31 Training Loss: 0.6139461994171143 \n",
      "     Training Step: 32 Training Loss: 0.6184982657432556 \n",
      "     Training Step: 33 Training Loss: 0.6146275997161865 \n",
      "     Training Step: 34 Training Loss: 0.6117292046546936 \n",
      "     Training Step: 35 Training Loss: 0.616823673248291 \n",
      "     Training Step: 36 Training Loss: 0.61490398645401 \n",
      "     Training Step: 37 Training Loss: 0.6098292469978333 \n",
      "     Training Step: 38 Training Loss: 0.6158443093299866 \n",
      "     Training Step: 39 Training Loss: 0.6141758561134338 \n",
      "     Training Step: 40 Training Loss: 0.6177753210067749 \n",
      "     Training Step: 41 Training Loss: 0.6105054616928101 \n",
      "     Training Step: 42 Training Loss: 0.6155743598937988 \n",
      "     Training Step: 43 Training Loss: 0.6115790605545044 \n",
      "     Training Step: 44 Training Loss: 0.6175311803817749 \n",
      "     Training Step: 45 Training Loss: 0.6157729029655457 \n",
      "     Training Step: 46 Training Loss: 0.6106305718421936 \n",
      "     Training Step: 47 Training Loss: 0.6133176684379578 \n",
      "     Training Step: 48 Training Loss: 0.6111685037612915 \n",
      "     Training Step: 49 Training Loss: 0.6147177815437317 \n",
      "     Training Step: 50 Training Loss: 0.6136066913604736 \n",
      "     Training Step: 51 Training Loss: 0.618483304977417 \n",
      "     Training Step: 52 Training Loss: 0.6140767931938171 \n",
      "     Training Step: 53 Training Loss: 0.6131049394607544 \n",
      "     Training Step: 54 Training Loss: 0.6181065440177917 \n",
      "     Training Step: 55 Training Loss: 0.6147140264511108 \n",
      "     Training Step: 56 Training Loss: 0.6152925491333008 \n",
      "     Training Step: 57 Training Loss: 0.6173906326293945 \n",
      "     Training Step: 58 Training Loss: 0.609510600566864 \n",
      "     Training Step: 59 Training Loss: 0.617826521396637 \n",
      "     Training Step: 60 Training Loss: 0.6149791479110718 \n",
      "     Training Step: 61 Training Loss: 0.6185688376426697 \n",
      "     Training Step: 62 Training Loss: 0.6106148958206177 \n",
      "     Training Step: 63 Training Loss: 0.6106562614440918 \n",
      "     Training Step: 64 Training Loss: 0.6097060441970825 \n",
      "     Training Step: 65 Training Loss: 0.6165229082107544 \n",
      "     Training Step: 66 Training Loss: 0.6116463541984558 \n",
      "     Training Step: 67 Training Loss: 0.6114644408226013 \n",
      "     Training Step: 68 Training Loss: 0.6136818528175354 \n",
      "     Training Step: 69 Training Loss: 0.6152760982513428 \n",
      "     Training Step: 70 Training Loss: 0.6135372519493103 \n",
      "     Training Step: 71 Training Loss: 0.6160802245140076 \n",
      "     Training Step: 72 Training Loss: 0.6130273342132568 \n",
      "     Training Step: 73 Training Loss: 0.6116045713424683 \n",
      "     Training Step: 74 Training Loss: 0.617275059223175 \n",
      "     Training Step: 75 Training Loss: 0.6167566776275635 \n",
      "     Training Step: 76 Training Loss: 0.6135707497596741 \n",
      "     Training Step: 77 Training Loss: 0.6182265281677246 \n",
      "     Training Step: 78 Training Loss: 0.6132526993751526 \n",
      "     Training Step: 79 Training Loss: 0.6126474142074585 \n",
      "     Training Step: 80 Training Loss: 0.61287522315979 \n",
      "     Training Step: 81 Training Loss: 0.6157050728797913 \n",
      "     Training Step: 82 Training Loss: 0.6134889125823975 \n",
      "     Training Step: 83 Training Loss: 0.6125191450119019 \n",
      "     Training Step: 84 Training Loss: 0.6123043894767761 \n",
      "     Training Step: 85 Training Loss: 0.6132469177246094 \n",
      "     Training Step: 86 Training Loss: 0.6165204048156738 \n",
      "     Training Step: 87 Training Loss: 0.6146933436393738 \n",
      "     Training Step: 88 Training Loss: 0.6145198941230774 \n",
      "     Training Step: 89 Training Loss: 0.6121808886528015 \n",
      "     Training Step: 90 Training Loss: 0.610223114490509 \n",
      "     Training Step: 91 Training Loss: 0.6121132373809814 \n",
      "     Training Step: 92 Training Loss: 0.6101176738739014 \n",
      "     Training Step: 93 Training Loss: 0.6115045547485352 \n",
      "     Training Step: 94 Training Loss: 0.6148156523704529 \n",
      "     Training Step: 95 Training Loss: 0.6144771575927734 \n",
      "     Training Step: 96 Training Loss: 0.6125402450561523 \n",
      "     Training Step: 97 Training Loss: 0.614446222782135 \n",
      "     Training Step: 98 Training Loss: 0.6180850267410278 \n",
      "     Training Step: 99 Training Loss: 0.6119563579559326 \n",
      "     Training Step: 100 Training Loss: 0.611681342124939 \n",
      "     Training Step: 101 Training Loss: 0.6126999855041504 \n",
      "     Training Step: 102 Training Loss: 0.6138321757316589 \n",
      "     Training Step: 103 Training Loss: 0.6120972633361816 \n",
      "     Training Step: 104 Training Loss: 0.6114203929901123 \n",
      "     Training Step: 105 Training Loss: 0.613824725151062 \n",
      "     Training Step: 106 Training Loss: 0.6147506237030029 \n",
      "     Training Step: 107 Training Loss: 0.6171333193778992 \n",
      "     Training Step: 108 Training Loss: 0.6144231557846069 \n",
      "     Training Step: 109 Training Loss: 0.6168133616447449 \n",
      "     Training Step: 110 Training Loss: 0.6118800044059753 \n",
      "     Training Step: 111 Training Loss: 0.6122768521308899 \n",
      "     Training Step: 112 Training Loss: 0.6133291721343994 \n",
      "     Training Step: 113 Training Loss: 0.6121601462364197 \n",
      "     Training Step: 114 Training Loss: 0.6141942739486694 \n",
      "     Training Step: 115 Training Loss: 0.6172122359275818 \n",
      "     Training Step: 116 Training Loss: 0.6146676540374756 \n",
      "     Training Step: 117 Training Loss: 0.6119372248649597 \n",
      "     Training Step: 118 Training Loss: 0.61891108751297 \n",
      "     Training Step: 119 Training Loss: 0.6180208921432495 \n",
      "     Training Step: 120 Training Loss: 0.6143531799316406 \n",
      "     Training Step: 121 Training Loss: 0.612374484539032 \n",
      "     Training Step: 122 Training Loss: 0.6094129085540771 \n",
      "     Training Step: 123 Training Loss: 0.6107239723205566 \n",
      "     Training Step: 124 Training Loss: 0.6129512786865234 \n",
      "     Training Step: 125 Training Loss: 0.6153623461723328 \n",
      "     Training Step: 126 Training Loss: 0.6136941909790039 \n",
      "     Training Step: 127 Training Loss: 0.6168122887611389 \n",
      "     Training Step: 128 Training Loss: 0.6108546257019043 \n",
      "     Training Step: 129 Training Loss: 0.6124155521392822 \n",
      "     Training Step: 130 Training Loss: 0.6140556931495667 \n",
      "     Training Step: 131 Training Loss: 0.6131190061569214 \n",
      "     Training Step: 132 Training Loss: 0.6142824292182922 \n",
      "     Training Step: 133 Training Loss: 0.6163031458854675 \n",
      "     Training Step: 134 Training Loss: 0.6139428615570068 \n",
      "     Training Step: 135 Training Loss: 0.6123257875442505 \n",
      "     Training Step: 136 Training Loss: 0.6155218482017517 \n",
      "     Training Step: 137 Training Loss: 0.610228419303894 \n",
      "     Training Step: 138 Training Loss: 0.6210193634033203 \n",
      "     Training Step: 139 Training Loss: 0.6154375076293945 \n",
      "     Training Step: 140 Training Loss: 0.6139684319496155 \n",
      "     Training Step: 141 Training Loss: 0.6127377152442932 \n",
      "     Training Step: 142 Training Loss: 0.6154458522796631 \n",
      "     Training Step: 143 Training Loss: 0.615622341632843 \n",
      "     Training Step: 144 Training Loss: 0.6118961572647095 \n",
      "     Training Step: 145 Training Loss: 0.6136443018913269 \n",
      "     Training Step: 146 Training Loss: 0.608311116695404 \n",
      "     Training Step: 147 Training Loss: 0.6118371486663818 \n",
      "     Training Step: 148 Training Loss: 0.6142402291297913 \n",
      "     Training Step: 149 Training Loss: 0.6199471950531006 \n",
      "     Training Step: 150 Training Loss: 0.6167686581611633 \n",
      "     Training Step: 151 Training Loss: 0.6108748912811279 \n",
      "     Training Step: 152 Training Loss: 0.6150256395339966 \n",
      "     Training Step: 153 Training Loss: 0.6148080229759216 \n",
      "     Training Step: 154 Training Loss: 0.6144874095916748 \n",
      "     Training Step: 155 Training Loss: 0.6122546195983887 \n",
      "     Training Step: 156 Training Loss: 0.6185290813446045 \n",
      "     Training Step: 157 Training Loss: 0.6153585910797119 \n",
      "     Training Step: 158 Training Loss: 0.6095208525657654 \n",
      "     Training Step: 159 Training Loss: 0.6101655960083008 \n",
      "     Training Step: 160 Training Loss: 0.6202664375305176 \n",
      "     Training Step: 161 Training Loss: 0.614822268486023 \n",
      "     Training Step: 162 Training Loss: 0.6166835427284241 \n",
      "     Training Step: 163 Training Loss: 0.6167547106742859 \n",
      "     Training Step: 164 Training Loss: 0.6122074127197266 \n",
      "     Training Step: 165 Training Loss: 0.6167454123497009 \n",
      "     Training Step: 166 Training Loss: 0.6154002547264099 \n",
      "     Training Step: 167 Training Loss: 0.611194372177124 \n",
      "     Training Step: 168 Training Loss: 0.6197484135627747 \n",
      "     Training Step: 169 Training Loss: 0.6101386547088623 \n",
      "     Training Step: 170 Training Loss: 0.6166428327560425 \n",
      "     Training Step: 171 Training Loss: 0.6146578192710876 \n",
      "     Training Step: 172 Training Loss: 0.6171686053276062 \n",
      "     Training Step: 173 Training Loss: 0.6169329285621643 \n",
      "     Training Step: 174 Training Loss: 0.6129351854324341 \n",
      "     Training Step: 175 Training Loss: 0.6176853179931641 \n",
      "     Training Step: 176 Training Loss: 0.6151712536811829 \n",
      "     Training Step: 177 Training Loss: 0.615950882434845 \n",
      "     Training Step: 178 Training Loss: 0.6147849559783936 \n",
      "     Training Step: 179 Training Loss: 0.6133061051368713 \n",
      "     Training Step: 180 Training Loss: 0.6138240098953247 \n",
      "     Training Step: 181 Training Loss: 0.6131296753883362 \n",
      "     Training Step: 182 Training Loss: 0.612541675567627 \n",
      "     Training Step: 183 Training Loss: 0.6185300350189209 \n",
      "     Training Step: 184 Training Loss: 0.6129279732704163 \n",
      "     Training Step: 185 Training Loss: 0.6146968007087708 \n",
      "     Training Step: 186 Training Loss: 0.6162348389625549 \n",
      "     Training Step: 187 Training Loss: 0.6134974360466003 \n",
      "     Training Step: 188 Training Loss: 0.6155325174331665 \n",
      "     Training Step: 189 Training Loss: 0.6188731789588928 \n",
      "     Training Step: 190 Training Loss: 0.6152732968330383 \n",
      "     Training Step: 191 Training Loss: 0.6114912629127502 \n",
      "     Training Step: 192 Training Loss: 0.6157593131065369 \n",
      "     Training Step: 193 Training Loss: 0.6158581972122192 \n",
      "     Training Step: 194 Training Loss: 0.6160153746604919 \n",
      "     Training Step: 195 Training Loss: 0.6098141074180603 \n",
      "     Training Step: 196 Training Loss: 0.612790584564209 \n",
      "     Training Step: 197 Training Loss: 0.613754391670227 \n",
      "     Training Step: 198 Training Loss: 0.6125438809394836 \n",
      "     Training Step: 199 Training Loss: 0.6163474917411804 \n",
      "     Training Step: 200 Training Loss: 0.6181043982505798 \n",
      "     Training Step: 201 Training Loss: 0.6157998442649841 \n",
      "     Training Step: 202 Training Loss: 0.6151334047317505 \n",
      "     Training Step: 203 Training Loss: 0.6117072105407715 \n",
      "     Training Step: 204 Training Loss: 0.6133944392204285 \n",
      "     Training Step: 205 Training Loss: 0.6117985248565674 \n",
      "     Training Step: 206 Training Loss: 0.6178109049797058 \n",
      "     Training Step: 207 Training Loss: 0.610732913017273 \n",
      "     Training Step: 208 Training Loss: 0.6161799430847168 \n",
      "     Training Step: 209 Training Loss: 0.6167996525764465 \n",
      "     Training Step: 210 Training Loss: 0.6143952012062073 \n",
      "     Training Step: 211 Training Loss: 0.6110985279083252 \n",
      "     Training Step: 212 Training Loss: 0.6146714091300964 \n",
      "     Training Step: 213 Training Loss: 0.6135233640670776 \n",
      "     Training Step: 214 Training Loss: 0.6178911328315735 \n",
      "     Training Step: 215 Training Loss: 0.6200584173202515 \n",
      "     Training Step: 216 Training Loss: 0.6177281141281128 \n",
      "     Training Step: 217 Training Loss: 0.6145738363265991 \n",
      "     Training Step: 218 Training Loss: 0.6108648180961609 \n",
      "     Training Step: 219 Training Loss: 0.6154803037643433 \n",
      "     Training Step: 220 Training Loss: 0.617715060710907 \n",
      "     Training Step: 221 Training Loss: 0.6116415858268738 \n",
      "     Training Step: 222 Training Loss: 0.6149963736534119 \n",
      "     Training Step: 223 Training Loss: 0.611617922782898 \n",
      "     Training Step: 224 Training Loss: 0.6114336252212524 \n",
      "     Training Step: 225 Training Loss: 0.6142983436584473 \n",
      "     Training Step: 226 Training Loss: 0.6115520000457764 \n",
      "     Training Step: 227 Training Loss: 0.6154072284698486 \n",
      "     Training Step: 228 Training Loss: 0.6141491532325745 \n",
      "     Training Step: 229 Training Loss: 0.6186771988868713 \n",
      "     Training Step: 230 Training Loss: 0.6129797697067261 \n",
      "     Training Step: 231 Training Loss: 0.6100809574127197 \n",
      "     Training Step: 232 Training Loss: 0.6146677732467651 \n",
      "     Training Step: 233 Training Loss: 0.612396776676178 \n",
      "     Training Step: 234 Training Loss: 0.6151233911514282 \n",
      "     Training Step: 235 Training Loss: 0.619530439376831 \n",
      "     Training Step: 236 Training Loss: 0.6145091652870178 \n",
      "     Training Step: 237 Training Loss: 0.6128643155097961 \n",
      "     Training Step: 238 Training Loss: 0.614342451095581 \n",
      "     Training Step: 239 Training Loss: 0.6119012832641602 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6141446232795715 \n",
      "     Validation Step: 1 Validation Loss: 0.6152474284172058 \n",
      "     Validation Step: 2 Validation Loss: 0.6156372427940369 \n",
      "     Validation Step: 3 Validation Loss: 0.617617666721344 \n",
      "     Validation Step: 4 Validation Loss: 0.6119071841239929 \n",
      "     Validation Step: 5 Validation Loss: 0.6182687878608704 \n",
      "     Validation Step: 6 Validation Loss: 0.6121460199356079 \n",
      "     Validation Step: 7 Validation Loss: 0.6156030893325806 \n",
      "     Validation Step: 8 Validation Loss: 0.6141307950019836 \n",
      "     Validation Step: 9 Validation Loss: 0.610205352306366 \n",
      "     Validation Step: 10 Validation Loss: 0.6128396391868591 \n",
      "     Validation Step: 11 Validation Loss: 0.6145507097244263 \n",
      "     Validation Step: 12 Validation Loss: 0.6149112582206726 \n",
      "     Validation Step: 13 Validation Loss: 0.6111974120140076 \n",
      "     Validation Step: 14 Validation Loss: 0.6170403361320496 \n",
      "     Validation Step: 15 Validation Loss: 0.6105042099952698 \n",
      "     Validation Step: 16 Validation Loss: 0.6180886030197144 \n",
      "     Validation Step: 17 Validation Loss: 0.6142697334289551 \n",
      "     Validation Step: 18 Validation Loss: 0.6101770997047424 \n",
      "     Validation Step: 19 Validation Loss: 0.6162792444229126 \n",
      "     Validation Step: 20 Validation Loss: 0.6106710433959961 \n",
      "     Validation Step: 21 Validation Loss: 0.6185024380683899 \n",
      "     Validation Step: 22 Validation Loss: 0.6105388402938843 \n",
      "     Validation Step: 23 Validation Loss: 0.6133126616477966 \n",
      "     Validation Step: 24 Validation Loss: 0.6142163872718811 \n",
      "     Validation Step: 25 Validation Loss: 0.6101434230804443 \n",
      "     Validation Step: 26 Validation Loss: 0.6177253127098083 \n",
      "     Validation Step: 27 Validation Loss: 0.6116682887077332 \n",
      "     Validation Step: 28 Validation Loss: 0.6160305142402649 \n",
      "     Validation Step: 29 Validation Loss: 0.6173248291015625 \n",
      "     Validation Step: 30 Validation Loss: 0.614592969417572 \n",
      "     Validation Step: 31 Validation Loss: 0.6136510968208313 \n",
      "     Validation Step: 32 Validation Loss: 0.6150598526000977 \n",
      "     Validation Step: 33 Validation Loss: 0.6158316731452942 \n",
      "     Validation Step: 34 Validation Loss: 0.6136940121650696 \n",
      "     Validation Step: 35 Validation Loss: 0.6115758419036865 \n",
      "     Validation Step: 36 Validation Loss: 0.618367612361908 \n",
      "     Validation Step: 37 Validation Loss: 0.607571005821228 \n",
      "     Validation Step: 38 Validation Loss: 0.6153327226638794 \n",
      "     Validation Step: 39 Validation Loss: 0.6185213327407837 \n",
      "     Validation Step: 40 Validation Loss: 0.6111887097358704 \n",
      "     Validation Step: 41 Validation Loss: 0.6129918694496155 \n",
      "     Validation Step: 42 Validation Loss: 0.6146603226661682 \n",
      "     Validation Step: 43 Validation Loss: 0.6136718988418579 \n",
      "     Validation Step: 44 Validation Loss: 0.6148561835289001 \n",
      "Epoch: 42\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6196796298027039 \n",
      "     Training Step: 1 Training Loss: 0.6122688055038452 \n",
      "     Training Step: 2 Training Loss: 0.6135492324829102 \n",
      "     Training Step: 3 Training Loss: 0.6140950918197632 \n",
      "     Training Step: 4 Training Loss: 0.6112634539604187 \n",
      "     Training Step: 5 Training Loss: 0.6147283911705017 \n",
      "     Training Step: 6 Training Loss: 0.614682137966156 \n",
      "     Training Step: 7 Training Loss: 0.6132912635803223 \n",
      "     Training Step: 8 Training Loss: 0.6131084561347961 \n",
      "     Training Step: 9 Training Loss: 0.6157900094985962 \n",
      "     Training Step: 10 Training Loss: 0.6155945062637329 \n",
      "     Training Step: 11 Training Loss: 0.6118232011795044 \n",
      "     Training Step: 12 Training Loss: 0.6202735304832458 \n",
      "     Training Step: 13 Training Loss: 0.6116194725036621 \n",
      "     Training Step: 14 Training Loss: 0.6115387082099915 \n",
      "     Training Step: 15 Training Loss: 0.6133711338043213 \n",
      "     Training Step: 16 Training Loss: 0.6123185157775879 \n",
      "     Training Step: 17 Training Loss: 0.6132169365882874 \n",
      "     Training Step: 18 Training Loss: 0.6105829477310181 \n",
      "     Training Step: 19 Training Loss: 0.6153851747512817 \n",
      "     Training Step: 20 Training Loss: 0.6141369938850403 \n",
      "     Training Step: 21 Training Loss: 0.6184489727020264 \n",
      "     Training Step: 22 Training Loss: 0.6169750690460205 \n",
      "     Training Step: 23 Training Loss: 0.6178433299064636 \n",
      "     Training Step: 24 Training Loss: 0.6164736151695251 \n",
      "     Training Step: 25 Training Loss: 0.6108190417289734 \n",
      "     Training Step: 26 Training Loss: 0.6135200262069702 \n",
      "     Training Step: 27 Training Loss: 0.6189833283424377 \n",
      "     Training Step: 28 Training Loss: 0.6173192858695984 \n",
      "     Training Step: 29 Training Loss: 0.6174202561378479 \n",
      "     Training Step: 30 Training Loss: 0.6102691292762756 \n",
      "     Training Step: 31 Training Loss: 0.6123976707458496 \n",
      "     Training Step: 32 Training Loss: 0.6178290843963623 \n",
      "     Training Step: 33 Training Loss: 0.6177600622177124 \n",
      "     Training Step: 34 Training Loss: 0.6131510734558105 \n",
      "     Training Step: 35 Training Loss: 0.6125898361206055 \n",
      "     Training Step: 36 Training Loss: 0.6118381023406982 \n",
      "     Training Step: 37 Training Loss: 0.6101455092430115 \n",
      "     Training Step: 38 Training Loss: 0.611556351184845 \n",
      "     Training Step: 39 Training Loss: 0.6114304661750793 \n",
      "     Training Step: 40 Training Loss: 0.6142435073852539 \n",
      "     Training Step: 41 Training Loss: 0.6155561804771423 \n",
      "     Training Step: 42 Training Loss: 0.6112156510353088 \n",
      "     Training Step: 43 Training Loss: 0.6116838455200195 \n",
      "     Training Step: 44 Training Loss: 0.6162863373756409 \n",
      "     Training Step: 45 Training Loss: 0.612933337688446 \n",
      "     Training Step: 46 Training Loss: 0.6157297492027283 \n",
      "     Training Step: 47 Training Loss: 0.6165305376052856 \n",
      "     Training Step: 48 Training Loss: 0.6154019236564636 \n",
      "     Training Step: 49 Training Loss: 0.6167240738868713 \n",
      "     Training Step: 50 Training Loss: 0.6166462898254395 \n",
      "     Training Step: 51 Training Loss: 0.6196778416633606 \n",
      "     Training Step: 52 Training Loss: 0.6153560876846313 \n",
      "     Training Step: 53 Training Loss: 0.6099466681480408 \n",
      "     Training Step: 54 Training Loss: 0.611841082572937 \n",
      "     Training Step: 55 Training Loss: 0.6153554916381836 \n",
      "     Training Step: 56 Training Loss: 0.613847017288208 \n",
      "     Training Step: 57 Training Loss: 0.6167585253715515 \n",
      "     Training Step: 58 Training Loss: 0.6154917478561401 \n",
      "     Training Step: 59 Training Loss: 0.6168325543403625 \n",
      "     Training Step: 60 Training Loss: 0.6128144860267639 \n",
      "     Training Step: 61 Training Loss: 0.6135159730911255 \n",
      "     Training Step: 62 Training Loss: 0.6125162243843079 \n",
      "     Training Step: 63 Training Loss: 0.6123923063278198 \n",
      "     Training Step: 64 Training Loss: 0.6107401251792908 \n",
      "     Training Step: 65 Training Loss: 0.6108422875404358 \n",
      "     Training Step: 66 Training Loss: 0.6123769879341125 \n",
      "     Training Step: 67 Training Loss: 0.6160993576049805 \n",
      "     Training Step: 68 Training Loss: 0.6116929054260254 \n",
      "     Training Step: 69 Training Loss: 0.6149556636810303 \n",
      "     Training Step: 70 Training Loss: 0.6108969449996948 \n",
      "     Training Step: 71 Training Loss: 0.6156905293464661 \n",
      "     Training Step: 72 Training Loss: 0.6145198345184326 \n",
      "     Training Step: 73 Training Loss: 0.6167558431625366 \n",
      "     Training Step: 74 Training Loss: 0.6146448850631714 \n",
      "     Training Step: 75 Training Loss: 0.6146880388259888 \n",
      "     Training Step: 76 Training Loss: 0.6133902668952942 \n",
      "     Training Step: 77 Training Loss: 0.6122840642929077 \n",
      "     Training Step: 78 Training Loss: 0.6157567501068115 \n",
      "     Training Step: 79 Training Loss: 0.6139383316040039 \n",
      "     Training Step: 80 Training Loss: 0.6195083260536194 \n",
      "     Training Step: 81 Training Loss: 0.6147982478141785 \n",
      "     Training Step: 82 Training Loss: 0.6127971410751343 \n",
      "     Training Step: 83 Training Loss: 0.6167477369308472 \n",
      "     Training Step: 84 Training Loss: 0.6150819063186646 \n",
      "     Training Step: 85 Training Loss: 0.6128852963447571 \n",
      "     Training Step: 86 Training Loss: 0.617211103439331 \n",
      "     Training Step: 87 Training Loss: 0.6098325252532959 \n",
      "     Training Step: 88 Training Loss: 0.6104605197906494 \n",
      "     Training Step: 89 Training Loss: 0.6151653528213501 \n",
      "     Training Step: 90 Training Loss: 0.6167699098587036 \n",
      "     Training Step: 91 Training Loss: 0.6189556121826172 \n",
      "     Training Step: 92 Training Loss: 0.6144794225692749 \n",
      "     Training Step: 93 Training Loss: 0.6142385601997375 \n",
      "     Training Step: 94 Training Loss: 0.6122843623161316 \n",
      "     Training Step: 95 Training Loss: 0.6101394891738892 \n",
      "     Training Step: 96 Training Loss: 0.6168813109397888 \n",
      "     Training Step: 97 Training Loss: 0.6143083572387695 \n",
      "     Training Step: 98 Training Loss: 0.6185684204101562 \n",
      "     Training Step: 99 Training Loss: 0.61210697889328 \n",
      "     Training Step: 100 Training Loss: 0.6129084825515747 \n",
      "     Training Step: 101 Training Loss: 0.6177033185958862 \n",
      "     Training Step: 102 Training Loss: 0.6169082522392273 \n",
      "     Training Step: 103 Training Loss: 0.6177535057067871 \n",
      "     Training Step: 104 Training Loss: 0.614678144454956 \n",
      "     Training Step: 105 Training Loss: 0.6167610287666321 \n",
      "     Training Step: 106 Training Loss: 0.6115932464599609 \n",
      "     Training Step: 107 Training Loss: 0.6098045110702515 \n",
      "     Training Step: 108 Training Loss: 0.6144163012504578 \n",
      "     Training Step: 109 Training Loss: 0.6187827587127686 \n",
      "     Training Step: 110 Training Loss: 0.615189790725708 \n",
      "     Training Step: 111 Training Loss: 0.6154510974884033 \n",
      "     Training Step: 112 Training Loss: 0.6153400540351868 \n",
      "     Training Step: 113 Training Loss: 0.6108096241950989 \n",
      "     Training Step: 114 Training Loss: 0.6142773628234863 \n",
      "     Training Step: 115 Training Loss: 0.6175846457481384 \n",
      "     Training Step: 116 Training Loss: 0.6128178834915161 \n",
      "     Training Step: 117 Training Loss: 0.6181278228759766 \n",
      "     Training Step: 118 Training Loss: 0.6133161783218384 \n",
      "     Training Step: 119 Training Loss: 0.6129721403121948 \n",
      "     Training Step: 120 Training Loss: 0.6147668957710266 \n",
      "     Training Step: 121 Training Loss: 0.6125301122665405 \n",
      "     Training Step: 122 Training Loss: 0.6140525937080383 \n",
      "     Training Step: 123 Training Loss: 0.6105849146842957 \n",
      "     Training Step: 124 Training Loss: 0.6111879348754883 \n",
      "     Training Step: 125 Training Loss: 0.6182844042778015 \n",
      "     Training Step: 126 Training Loss: 0.6130855679512024 \n",
      "     Training Step: 127 Training Loss: 0.618131160736084 \n",
      "     Training Step: 128 Training Loss: 0.6199262142181396 \n",
      "     Training Step: 129 Training Loss: 0.6117128729820251 \n",
      "     Training Step: 130 Training Loss: 0.6147279739379883 \n",
      "     Training Step: 131 Training Loss: 0.6110050678253174 \n",
      "     Training Step: 132 Training Loss: 0.6181014776229858 \n",
      "     Training Step: 133 Training Loss: 0.6172471046447754 \n",
      "     Training Step: 134 Training Loss: 0.6148006319999695 \n",
      "     Training Step: 135 Training Loss: 0.6155489087104797 \n",
      "     Training Step: 136 Training Loss: 0.613595187664032 \n",
      "     Training Step: 137 Training Loss: 0.6118397116661072 \n",
      "     Training Step: 138 Training Loss: 0.6116670370101929 \n",
      "     Training Step: 139 Training Loss: 0.6114305853843689 \n",
      "     Training Step: 140 Training Loss: 0.6119450330734253 \n",
      "     Training Step: 141 Training Loss: 0.6187732219696045 \n",
      "     Training Step: 142 Training Loss: 0.6145302653312683 \n",
      "     Training Step: 143 Training Loss: 0.6136688590049744 \n",
      "     Training Step: 144 Training Loss: 0.6153308153152466 \n",
      "     Training Step: 145 Training Loss: 0.6096904873847961 \n",
      "     Training Step: 146 Training Loss: 0.6147593259811401 \n",
      "     Training Step: 147 Training Loss: 0.6177098155021667 \n",
      "     Training Step: 148 Training Loss: 0.621078610420227 \n",
      "     Training Step: 149 Training Loss: 0.610023021697998 \n",
      "     Training Step: 150 Training Loss: 0.6141834259033203 \n",
      "     Training Step: 151 Training Loss: 0.6083638668060303 \n",
      "     Training Step: 152 Training Loss: 0.6106888651847839 \n",
      "     Training Step: 153 Training Loss: 0.6123946905136108 \n",
      "     Training Step: 154 Training Loss: 0.6146692633628845 \n",
      "     Training Step: 155 Training Loss: 0.616855263710022 \n",
      "     Training Step: 156 Training Loss: 0.615777313709259 \n",
      "     Training Step: 157 Training Loss: 0.6159737706184387 \n",
      "     Training Step: 158 Training Loss: 0.6158674955368042 \n",
      "     Training Step: 159 Training Loss: 0.6150335669517517 \n",
      "     Training Step: 160 Training Loss: 0.6117676496505737 \n",
      "     Training Step: 161 Training Loss: 0.6138390898704529 \n",
      "     Training Step: 162 Training Loss: 0.6133320927619934 \n",
      "     Training Step: 163 Training Loss: 0.6144487261772156 \n",
      "     Training Step: 164 Training Loss: 0.6167533993721008 \n",
      "     Training Step: 165 Training Loss: 0.6131574511528015 \n",
      "     Training Step: 166 Training Loss: 0.6125692129135132 \n",
      "     Training Step: 167 Training Loss: 0.6133454442024231 \n",
      "     Training Step: 168 Training Loss: 0.611443281173706 \n",
      "     Training Step: 169 Training Loss: 0.6149746775627136 \n",
      "     Training Step: 170 Training Loss: 0.61686772108078 \n",
      "     Training Step: 171 Training Loss: 0.6158146262168884 \n",
      "     Training Step: 172 Training Loss: 0.6171836853027344 \n",
      "     Training Step: 173 Training Loss: 0.6139917969703674 \n",
      "     Training Step: 174 Training Loss: 0.6137111186981201 \n",
      "     Training Step: 175 Training Loss: 0.6176131963729858 \n",
      "     Training Step: 176 Training Loss: 0.6148175001144409 \n",
      "     Training Step: 177 Training Loss: 0.6151642203330994 \n",
      "     Training Step: 178 Training Loss: 0.6139112710952759 \n",
      "     Training Step: 179 Training Loss: 0.6171757578849792 \n",
      "     Training Step: 180 Training Loss: 0.6152570843696594 \n",
      "     Training Step: 181 Training Loss: 0.6143551468849182 \n",
      "     Training Step: 182 Training Loss: 0.6156047582626343 \n",
      "     Training Step: 183 Training Loss: 0.6162193417549133 \n",
      "     Training Step: 184 Training Loss: 0.6143208146095276 \n",
      "     Training Step: 185 Training Loss: 0.6137917637825012 \n",
      "     Training Step: 186 Training Loss: 0.6182469129562378 \n",
      "     Training Step: 187 Training Loss: 0.6129305362701416 \n",
      "     Training Step: 188 Training Loss: 0.6163644194602966 \n",
      "     Training Step: 189 Training Loss: 0.6122459173202515 \n",
      "     Training Step: 190 Training Loss: 0.6186316609382629 \n",
      "     Training Step: 191 Training Loss: 0.6104153990745544 \n",
      "     Training Step: 192 Training Loss: 0.610139787197113 \n",
      "     Training Step: 193 Training Loss: 0.6164835095405579 \n",
      "     Training Step: 194 Training Loss: 0.6114374399185181 \n",
      "     Training Step: 195 Training Loss: 0.6140563488006592 \n",
      "     Training Step: 196 Training Loss: 0.6094542145729065 \n",
      "     Training Step: 197 Training Loss: 0.6144363284111023 \n",
      "     Training Step: 198 Training Loss: 0.6118373274803162 \n",
      "     Training Step: 199 Training Loss: 0.6120388507843018 \n",
      "     Training Step: 200 Training Loss: 0.6183294653892517 \n",
      "     Training Step: 201 Training Loss: 0.6151231527328491 \n",
      "     Training Step: 202 Training Loss: 0.6149444580078125 \n",
      "     Training Step: 203 Training Loss: 0.6146758198738098 \n",
      "     Training Step: 204 Training Loss: 0.6154544949531555 \n",
      "     Training Step: 205 Training Loss: 0.6132440567016602 \n",
      "     Training Step: 206 Training Loss: 0.6092928051948547 \n",
      "     Training Step: 207 Training Loss: 0.6185480952262878 \n",
      "     Training Step: 208 Training Loss: 0.6130027770996094 \n",
      "     Training Step: 209 Training Loss: 0.6161603927612305 \n",
      "     Training Step: 210 Training Loss: 0.6146802306175232 \n",
      "     Training Step: 211 Training Loss: 0.6115931868553162 \n",
      "     Training Step: 212 Training Loss: 0.6121859550476074 \n",
      "     Training Step: 213 Training Loss: 0.6123988628387451 \n",
      "     Training Step: 214 Training Loss: 0.6132169961929321 \n",
      "     Training Step: 215 Training Loss: 0.6154520511627197 \n",
      "     Training Step: 216 Training Loss: 0.6117076873779297 \n",
      "     Training Step: 217 Training Loss: 0.6145339608192444 \n",
      "     Training Step: 218 Training Loss: 0.6136571764945984 \n",
      "     Training Step: 219 Training Loss: 0.6118725538253784 \n",
      "     Training Step: 220 Training Loss: 0.6125687956809998 \n",
      "     Training Step: 221 Training Loss: 0.6140261292457581 \n",
      "     Training Step: 222 Training Loss: 0.6105244159698486 \n",
      "     Training Step: 223 Training Loss: 0.6101863384246826 \n",
      "     Training Step: 224 Training Loss: 0.6148640513420105 \n",
      "     Training Step: 225 Training Loss: 0.6127912402153015 \n",
      "     Training Step: 226 Training Loss: 0.6136400103569031 \n",
      "     Training Step: 227 Training Loss: 0.6167305111885071 \n",
      "     Training Step: 228 Training Loss: 0.6122386455535889 \n",
      "     Training Step: 229 Training Loss: 0.6110007166862488 \n",
      "     Training Step: 230 Training Loss: 0.6127138137817383 \n",
      "     Training Step: 231 Training Loss: 0.6105025410652161 \n",
      "     Training Step: 232 Training Loss: 0.6145510673522949 \n",
      "     Training Step: 233 Training Loss: 0.6135355830192566 \n",
      "     Training Step: 234 Training Loss: 0.6118669509887695 \n",
      "     Training Step: 235 Training Loss: 0.6155704855918884 \n",
      "     Training Step: 236 Training Loss: 0.6154459714889526 \n",
      "     Training Step: 237 Training Loss: 0.6163030862808228 \n",
      "     Training Step: 238 Training Loss: 0.6138782501220703 \n",
      "     Training Step: 239 Training Loss: 0.6161277890205383 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6117364168167114 \n",
      "     Validation Step: 1 Validation Loss: 0.6076996326446533 \n",
      "     Validation Step: 2 Validation Loss: 0.6152549386024475 \n",
      "     Validation Step: 3 Validation Loss: 0.6122254133224487 \n",
      "     Validation Step: 4 Validation Loss: 0.6116303205490112 \n",
      "     Validation Step: 5 Validation Loss: 0.6105865240097046 \n",
      "     Validation Step: 6 Validation Loss: 0.6180574893951416 \n",
      "     Validation Step: 7 Validation Loss: 0.6136842966079712 \n",
      "     Validation Step: 8 Validation Loss: 0.6102318167686462 \n",
      "     Validation Step: 9 Validation Loss: 0.6155776977539062 \n",
      "     Validation Step: 10 Validation Loss: 0.6183202266693115 \n",
      "     Validation Step: 11 Validation Loss: 0.6184929013252258 \n",
      "     Validation Step: 12 Validation Loss: 0.6145905256271362 \n",
      "     Validation Step: 13 Validation Loss: 0.614205002784729 \n",
      "     Validation Step: 14 Validation Loss: 0.6173282861709595 \n",
      "     Validation Step: 15 Validation Loss: 0.6103134751319885 \n",
      "     Validation Step: 16 Validation Loss: 0.6112378835678101 \n",
      "     Validation Step: 17 Validation Loss: 0.611971378326416 \n",
      "     Validation Step: 18 Validation Loss: 0.6141548156738281 \n",
      "     Validation Step: 19 Validation Loss: 0.6182133555412292 \n",
      "     Validation Step: 20 Validation Loss: 0.6129056811332703 \n",
      "     Validation Step: 21 Validation Loss: 0.6105756163597107 \n",
      "     Validation Step: 22 Validation Loss: 0.6184820532798767 \n",
      "     Validation Step: 23 Validation Loss: 0.614321768283844 \n",
      "     Validation Step: 24 Validation Loss: 0.6148700714111328 \n",
      "     Validation Step: 25 Validation Loss: 0.6142324209213257 \n",
      "     Validation Step: 26 Validation Loss: 0.6150355935096741 \n",
      "     Validation Step: 27 Validation Loss: 0.6133597493171692 \n",
      "     Validation Step: 28 Validation Loss: 0.6149543523788452 \n",
      "     Validation Step: 29 Validation Loss: 0.61027592420578 \n",
      "     Validation Step: 30 Validation Loss: 0.6176868677139282 \n",
      "     Validation Step: 31 Validation Loss: 0.6137111186981201 \n",
      "     Validation Step: 32 Validation Loss: 0.6136558055877686 \n",
      "     Validation Step: 33 Validation Loss: 0.6130310297012329 \n",
      "     Validation Step: 34 Validation Loss: 0.6153275966644287 \n",
      "     Validation Step: 35 Validation Loss: 0.6176252961158752 \n",
      "     Validation Step: 36 Validation Loss: 0.6157969236373901 \n",
      "     Validation Step: 37 Validation Loss: 0.6162497997283936 \n",
      "     Validation Step: 38 Validation Loss: 0.6146506667137146 \n",
      "     Validation Step: 39 Validation Loss: 0.61598140001297 \n",
      "     Validation Step: 40 Validation Loss: 0.6146071553230286 \n",
      "     Validation Step: 41 Validation Loss: 0.6156213879585266 \n",
      "     Validation Step: 42 Validation Loss: 0.6107641458511353 \n",
      "     Validation Step: 43 Validation Loss: 0.6170250177383423 \n",
      "     Validation Step: 44 Validation Loss: 0.611266016960144 \n",
      "Epoch: 43\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6147056818008423 \n",
      "     Training Step: 1 Training Loss: 0.6144790649414062 \n",
      "     Training Step: 2 Training Loss: 0.6119298934936523 \n",
      "     Training Step: 3 Training Loss: 0.6138384938240051 \n",
      "     Training Step: 4 Training Loss: 0.6135550737380981 \n",
      "     Training Step: 5 Training Loss: 0.610159158706665 \n",
      "     Training Step: 6 Training Loss: 0.6163532137870789 \n",
      "     Training Step: 7 Training Loss: 0.6128791570663452 \n",
      "     Training Step: 8 Training Loss: 0.6133354306221008 \n",
      "     Training Step: 9 Training Loss: 0.6104665994644165 \n",
      "     Training Step: 10 Training Loss: 0.6184073090553284 \n",
      "     Training Step: 11 Training Loss: 0.6118118762969971 \n",
      "     Training Step: 12 Training Loss: 0.6135998964309692 \n",
      "     Training Step: 13 Training Loss: 0.6154434680938721 \n",
      "     Training Step: 14 Training Loss: 0.6143724322319031 \n",
      "     Training Step: 15 Training Loss: 0.611431896686554 \n",
      "     Training Step: 16 Training Loss: 0.6154036521911621 \n",
      "     Training Step: 17 Training Loss: 0.6144924163818359 \n",
      "     Training Step: 18 Training Loss: 0.612522542476654 \n",
      "     Training Step: 19 Training Loss: 0.6159008145332336 \n",
      "     Training Step: 20 Training Loss: 0.6139211654663086 \n",
      "     Training Step: 21 Training Loss: 0.6107036471366882 \n",
      "     Training Step: 22 Training Loss: 0.6149562001228333 \n",
      "     Training Step: 23 Training Loss: 0.6177699565887451 \n",
      "     Training Step: 24 Training Loss: 0.6182507276535034 \n",
      "     Training Step: 25 Training Loss: 0.6137099862098694 \n",
      "     Training Step: 26 Training Loss: 0.6168342232704163 \n",
      "     Training Step: 27 Training Loss: 0.6133632659912109 \n",
      "     Training Step: 28 Training Loss: 0.6154142618179321 \n",
      "     Training Step: 29 Training Loss: 0.616735577583313 \n",
      "     Training Step: 30 Training Loss: 0.6169427037239075 \n",
      "     Training Step: 31 Training Loss: 0.6118864417076111 \n",
      "     Training Step: 32 Training Loss: 0.6106418371200562 \n",
      "     Training Step: 33 Training Loss: 0.6142285466194153 \n",
      "     Training Step: 34 Training Loss: 0.6094918251037598 \n",
      "     Training Step: 35 Training Loss: 0.6100708246231079 \n",
      "     Training Step: 36 Training Loss: 0.6161185503005981 \n",
      "     Training Step: 37 Training Loss: 0.6151581406593323 \n",
      "     Training Step: 38 Training Loss: 0.6134804487228394 \n",
      "     Training Step: 39 Training Loss: 0.6198648810386658 \n",
      "     Training Step: 40 Training Loss: 0.6168566346168518 \n",
      "     Training Step: 41 Training Loss: 0.6154401302337646 \n",
      "     Training Step: 42 Training Loss: 0.6108900308609009 \n",
      "     Training Step: 43 Training Loss: 0.6107140779495239 \n",
      "     Training Step: 44 Training Loss: 0.6176021695137024 \n",
      "     Training Step: 45 Training Loss: 0.6115373969078064 \n",
      "     Training Step: 46 Training Loss: 0.6114712357521057 \n",
      "     Training Step: 47 Training Loss: 0.614753782749176 \n",
      "     Training Step: 48 Training Loss: 0.6094321608543396 \n",
      "     Training Step: 49 Training Loss: 0.6122452020645142 \n",
      "     Training Step: 50 Training Loss: 0.6116852164268494 \n",
      "     Training Step: 51 Training Loss: 0.6151205897331238 \n",
      "     Training Step: 52 Training Loss: 0.615520715713501 \n",
      "     Training Step: 53 Training Loss: 0.6118777990341187 \n",
      "     Training Step: 54 Training Loss: 0.615350604057312 \n",
      "     Training Step: 55 Training Loss: 0.6157327890396118 \n",
      "     Training Step: 56 Training Loss: 0.6112573146820068 \n",
      "     Training Step: 57 Training Loss: 0.6133025288581848 \n",
      "     Training Step: 58 Training Loss: 0.6121607422828674 \n",
      "     Training Step: 59 Training Loss: 0.6145099401473999 \n",
      "     Training Step: 60 Training Loss: 0.615604817867279 \n",
      "     Training Step: 61 Training Loss: 0.6148161292076111 \n",
      "     Training Step: 62 Training Loss: 0.6115612387657166 \n",
      "     Training Step: 63 Training Loss: 0.6166955232620239 \n",
      "     Training Step: 64 Training Loss: 0.6098117828369141 \n",
      "     Training Step: 65 Training Loss: 0.6167644262313843 \n",
      "     Training Step: 66 Training Loss: 0.6177968382835388 \n",
      "     Training Step: 67 Training Loss: 0.6164462566375732 \n",
      "     Training Step: 68 Training Loss: 0.6152610182762146 \n",
      "     Training Step: 69 Training Loss: 0.6127896904945374 \n",
      "     Training Step: 70 Training Loss: 0.620941162109375 \n",
      "     Training Step: 71 Training Loss: 0.6128169894218445 \n",
      "     Training Step: 72 Training Loss: 0.6147816777229309 \n",
      "     Training Step: 73 Training Loss: 0.6163609027862549 \n",
      "     Training Step: 74 Training Loss: 0.6114996075630188 \n",
      "     Training Step: 75 Training Loss: 0.6143659949302673 \n",
      "     Training Step: 76 Training Loss: 0.6139088273048401 \n",
      "     Training Step: 77 Training Loss: 0.6137492656707764 \n",
      "     Training Step: 78 Training Loss: 0.6131263375282288 \n",
      "     Training Step: 79 Training Loss: 0.6134718060493469 \n",
      "     Training Step: 80 Training Loss: 0.6162357926368713 \n",
      "     Training Step: 81 Training Loss: 0.6166903376579285 \n",
      "     Training Step: 82 Training Loss: 0.6146636605262756 \n",
      "     Training Step: 83 Training Loss: 0.6084889769554138 \n",
      "     Training Step: 84 Training Loss: 0.6148423552513123 \n",
      "     Training Step: 85 Training Loss: 0.6147418022155762 \n",
      "     Training Step: 86 Training Loss: 0.6109153032302856 \n",
      "     Training Step: 87 Training Loss: 0.6140458583831787 \n",
      "     Training Step: 88 Training Loss: 0.6173524856567383 \n",
      "     Training Step: 89 Training Loss: 0.6152420043945312 \n",
      "     Training Step: 90 Training Loss: 0.613243043422699 \n",
      "     Training Step: 91 Training Loss: 0.6140779256820679 \n",
      "     Training Step: 92 Training Loss: 0.6202496886253357 \n",
      "     Training Step: 93 Training Loss: 0.6119117140769958 \n",
      "     Training Step: 94 Training Loss: 0.6141817569732666 \n",
      "     Training Step: 95 Training Loss: 0.6159613728523254 \n",
      "     Training Step: 96 Training Loss: 0.6136071681976318 \n",
      "     Training Step: 97 Training Loss: 0.6124111413955688 \n",
      "     Training Step: 98 Training Loss: 0.6151617169380188 \n",
      "     Training Step: 99 Training Loss: 0.6123149394989014 \n",
      "     Training Step: 100 Training Loss: 0.6106043457984924 \n",
      "     Training Step: 101 Training Loss: 0.6165444254875183 \n",
      "     Training Step: 102 Training Loss: 0.6174840331077576 \n",
      "     Training Step: 103 Training Loss: 0.6171694993972778 \n",
      "     Training Step: 104 Training Loss: 0.6133661866188049 \n",
      "     Training Step: 105 Training Loss: 0.6154724359512329 \n",
      "     Training Step: 106 Training Loss: 0.6147468090057373 \n",
      "     Training Step: 107 Training Loss: 0.6149163246154785 \n",
      "     Training Step: 108 Training Loss: 0.6177207231521606 \n",
      "     Training Step: 109 Training Loss: 0.6166738271713257 \n",
      "     Training Step: 110 Training Loss: 0.6100983023643494 \n",
      "     Training Step: 111 Training Loss: 0.6162189245223999 \n",
      "     Training Step: 112 Training Loss: 0.6118316054344177 \n",
      "     Training Step: 113 Training Loss: 0.6144227981567383 \n",
      "     Training Step: 114 Training Loss: 0.6136682629585266 \n",
      "     Training Step: 115 Training Loss: 0.6167917847633362 \n",
      "     Training Step: 116 Training Loss: 0.6124273538589478 \n",
      "     Training Step: 117 Training Loss: 0.6115943789482117 \n",
      "     Training Step: 118 Training Loss: 0.613775372505188 \n",
      "     Training Step: 119 Training Loss: 0.6125741600990295 \n",
      "     Training Step: 120 Training Loss: 0.6111592054367065 \n",
      "     Training Step: 121 Training Loss: 0.6155673861503601 \n",
      "     Training Step: 122 Training Loss: 0.6187179684638977 \n",
      "     Training Step: 123 Training Loss: 0.6098309755325317 \n",
      "     Training Step: 124 Training Loss: 0.6130880117416382 \n",
      "     Training Step: 125 Training Loss: 0.6147332191467285 \n",
      "     Training Step: 126 Training Loss: 0.6172285676002502 \n",
      "     Training Step: 127 Training Loss: 0.6141356825828552 \n",
      "     Training Step: 128 Training Loss: 0.612223744392395 \n",
      "     Training Step: 129 Training Loss: 0.612332820892334 \n",
      "     Training Step: 130 Training Loss: 0.6136541366577148 \n",
      "     Training Step: 131 Training Loss: 0.6100592613220215 \n",
      "     Training Step: 132 Training Loss: 0.615378737449646 \n",
      "     Training Step: 133 Training Loss: 0.6183679699897766 \n",
      "     Training Step: 134 Training Loss: 0.6125055551528931 \n",
      "     Training Step: 135 Training Loss: 0.6132511496543884 \n",
      "     Training Step: 136 Training Loss: 0.6114727854728699 \n",
      "     Training Step: 137 Training Loss: 0.6146867275238037 \n",
      "     Training Step: 138 Training Loss: 0.6143192052841187 \n",
      "     Training Step: 139 Training Loss: 0.617017388343811 \n",
      "     Training Step: 140 Training Loss: 0.6166671514511108 \n",
      "     Training Step: 141 Training Loss: 0.6117091774940491 \n",
      "     Training Step: 142 Training Loss: 0.612301230430603 \n",
      "     Training Step: 143 Training Loss: 0.614707350730896 \n",
      "     Training Step: 144 Training Loss: 0.6143662333488464 \n",
      "     Training Step: 145 Training Loss: 0.6107483506202698 \n",
      "     Training Step: 146 Training Loss: 0.6151522994041443 \n",
      "     Training Step: 147 Training Loss: 0.6154486536979675 \n",
      "     Training Step: 148 Training Loss: 0.6144669651985168 \n",
      "     Training Step: 149 Training Loss: 0.6180774569511414 \n",
      "     Training Step: 150 Training Loss: 0.6119087934494019 \n",
      "     Training Step: 151 Training Loss: 0.6118128299713135 \n",
      "     Training Step: 152 Training Loss: 0.6181246042251587 \n",
      "     Training Step: 153 Training Loss: 0.6130809783935547 \n",
      "     Training Step: 154 Training Loss: 0.6184973120689392 \n",
      "     Training Step: 155 Training Loss: 0.6122570037841797 \n",
      "     Training Step: 156 Training Loss: 0.6116769313812256 \n",
      "     Training Step: 157 Training Loss: 0.6105273962020874 \n",
      "     Training Step: 158 Training Loss: 0.6167972683906555 \n",
      "     Training Step: 159 Training Loss: 0.6157509088516235 \n",
      "     Training Step: 160 Training Loss: 0.61578768491745 \n",
      "     Training Step: 161 Training Loss: 0.6134251952171326 \n",
      "     Training Step: 162 Training Loss: 0.6180698275566101 \n",
      "     Training Step: 163 Training Loss: 0.612409234046936 \n",
      "     Training Step: 164 Training Loss: 0.6196428537368774 \n",
      "     Training Step: 165 Training Loss: 0.6156350374221802 \n",
      "     Training Step: 166 Training Loss: 0.6138718128204346 \n",
      "     Training Step: 167 Training Loss: 0.6154077053070068 \n",
      "     Training Step: 168 Training Loss: 0.6142682433128357 \n",
      "     Training Step: 169 Training Loss: 0.6184611320495605 \n",
      "     Training Step: 170 Training Loss: 0.6115086674690247 \n",
      "     Training Step: 171 Training Loss: 0.6189517974853516 \n",
      "     Training Step: 172 Training Loss: 0.6129499077796936 \n",
      "     Training Step: 173 Training Loss: 0.618037223815918 \n",
      "     Training Step: 174 Training Loss: 0.6171388030052185 \n",
      "     Training Step: 175 Training Loss: 0.6141997575759888 \n",
      "     Training Step: 176 Training Loss: 0.6125569939613342 \n",
      "     Training Step: 177 Training Loss: 0.6115868091583252 \n",
      "     Training Step: 178 Training Loss: 0.6134800314903259 \n",
      "     Training Step: 179 Training Loss: 0.6146776080131531 \n",
      "     Training Step: 180 Training Loss: 0.6179584860801697 \n",
      "     Training Step: 181 Training Loss: 0.6125742793083191 \n",
      "     Training Step: 182 Training Loss: 0.618915319442749 \n",
      "     Training Step: 183 Training Loss: 0.6106376647949219 \n",
      "     Training Step: 184 Training Loss: 0.6098352670669556 \n",
      "     Training Step: 185 Training Loss: 0.6103854179382324 \n",
      "     Training Step: 186 Training Loss: 0.6101273894309998 \n",
      "     Training Step: 187 Training Loss: 0.6121904850006104 \n",
      "     Training Step: 188 Training Loss: 0.61553555727005 \n",
      "     Training Step: 189 Training Loss: 0.6165461540222168 \n",
      "     Training Step: 190 Training Loss: 0.616849422454834 \n",
      "     Training Step: 191 Training Loss: 0.6150519251823425 \n",
      "     Training Step: 192 Training Loss: 0.6124127507209778 \n",
      "     Training Step: 193 Training Loss: 0.6141306757926941 \n",
      "     Training Step: 194 Training Loss: 0.6184285283088684 \n",
      "     Training Step: 195 Training Loss: 0.6107381582260132 \n",
      "     Training Step: 196 Training Loss: 0.6141883730888367 \n",
      "     Training Step: 197 Training Loss: 0.6092926263809204 \n",
      "     Training Step: 198 Training Loss: 0.6129145622253418 \n",
      "     Training Step: 199 Training Loss: 0.6146672368049622 \n",
      "     Training Step: 200 Training Loss: 0.614980936050415 \n",
      "     Training Step: 201 Training Loss: 0.6130626797676086 \n",
      "     Training Step: 202 Training Loss: 0.6194551587104797 \n",
      "     Training Step: 203 Training Loss: 0.6155675649642944 \n",
      "     Training Step: 204 Training Loss: 0.6132360696792603 \n",
      "     Training Step: 205 Training Loss: 0.61161208152771 \n",
      "     Training Step: 206 Training Loss: 0.6139841675758362 \n",
      "     Training Step: 207 Training Loss: 0.6129127144813538 \n",
      "     Training Step: 208 Training Loss: 0.6128397583961487 \n",
      "     Training Step: 209 Training Loss: 0.6157830357551575 \n",
      "     Training Step: 210 Training Loss: 0.6127675771713257 \n",
      "     Training Step: 211 Training Loss: 0.6168371438980103 \n",
      "     Training Step: 212 Training Loss: 0.6161547899246216 \n",
      "     Training Step: 213 Training Loss: 0.6112536787986755 \n",
      "     Training Step: 214 Training Loss: 0.6109384894371033 \n",
      "     Training Step: 215 Training Loss: 0.6145471930503845 \n",
      "     Training Step: 216 Training Loss: 0.6143116354942322 \n",
      "     Training Step: 217 Training Loss: 0.614763081073761 \n",
      "     Training Step: 218 Training Loss: 0.6176953315734863 \n",
      "     Training Step: 219 Training Loss: 0.6166835427284241 \n",
      "     Training Step: 220 Training Loss: 0.6186110973358154 \n",
      "     Training Step: 221 Training Loss: 0.6177746653556824 \n",
      "     Training Step: 222 Training Loss: 0.6118305921554565 \n",
      "     Training Step: 223 Training Loss: 0.6146643757820129 \n",
      "     Training Step: 224 Training Loss: 0.6132022142410278 \n",
      "     Training Step: 225 Training Loss: 0.6102507710456848 \n",
      "     Training Step: 226 Training Loss: 0.6159226894378662 \n",
      "     Training Step: 227 Training Loss: 0.6201762557029724 \n",
      "     Training Step: 228 Training Loss: 0.6178447604179382 \n",
      "     Training Step: 229 Training Loss: 0.6122399568557739 \n",
      "     Training Step: 230 Training Loss: 0.612501859664917 \n",
      "     Training Step: 231 Training Loss: 0.6127631068229675 \n",
      "     Training Step: 232 Training Loss: 0.6160446405410767 \n",
      "     Training Step: 233 Training Loss: 0.6116493344306946 \n",
      "     Training Step: 234 Training Loss: 0.6160154938697815 \n",
      "     Training Step: 235 Training Loss: 0.6172749996185303 \n",
      "     Training Step: 236 Training Loss: 0.6132896542549133 \n",
      "     Training Step: 237 Training Loss: 0.6129813194274902 \n",
      "     Training Step: 238 Training Loss: 0.615355908870697 \n",
      "     Training Step: 239 Training Loss: 0.6107687950134277 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6136815547943115 \n",
      "     Validation Step: 1 Validation Loss: 0.6105243563652039 \n",
      "     Validation Step: 2 Validation Loss: 0.6145506501197815 \n",
      "     Validation Step: 3 Validation Loss: 0.6156071424484253 \n",
      "     Validation Step: 4 Validation Loss: 0.6146626472473145 \n",
      "     Validation Step: 5 Validation Loss: 0.6156352758407593 \n",
      "     Validation Step: 6 Validation Loss: 0.6141560673713684 \n",
      "     Validation Step: 7 Validation Loss: 0.6184823513031006 \n",
      "     Validation Step: 8 Validation Loss: 0.6128386855125427 \n",
      "     Validation Step: 9 Validation Loss: 0.6176046133041382 \n",
      "     Validation Step: 10 Validation Loss: 0.6102083921432495 \n",
      "     Validation Step: 11 Validation Loss: 0.6142688393592834 \n",
      "     Validation Step: 12 Validation Loss: 0.6119062304496765 \n",
      "     Validation Step: 13 Validation Loss: 0.611195981502533 \n",
      "     Validation Step: 14 Validation Loss: 0.616035521030426 \n",
      "     Validation Step: 15 Validation Loss: 0.6121572256088257 \n",
      "     Validation Step: 16 Validation Loss: 0.6173036098480225 \n",
      "     Validation Step: 17 Validation Loss: 0.6158234477043152 \n",
      "     Validation Step: 18 Validation Loss: 0.6136314272880554 \n",
      "     Validation Step: 19 Validation Loss: 0.6182745695114136 \n",
      "     Validation Step: 20 Validation Loss: 0.6141307950019836 \n",
      "     Validation Step: 21 Validation Loss: 0.6148493885993958 \n",
      "     Validation Step: 22 Validation Loss: 0.6115847229957581 \n",
      "     Validation Step: 23 Validation Loss: 0.6180835366249084 \n",
      "     Validation Step: 24 Validation Loss: 0.6105472445487976 \n",
      "     Validation Step: 25 Validation Loss: 0.6162840127944946 \n",
      "     Validation Step: 26 Validation Loss: 0.61370849609375 \n",
      "     Validation Step: 27 Validation Loss: 0.6106705069541931 \n",
      "     Validation Step: 28 Validation Loss: 0.6150716543197632 \n",
      "     Validation Step: 29 Validation Loss: 0.6153438687324524 \n",
      "     Validation Step: 30 Validation Loss: 0.6142246127128601 \n",
      "     Validation Step: 31 Validation Loss: 0.6101745367050171 \n",
      "     Validation Step: 32 Validation Loss: 0.6130131483078003 \n",
      "     Validation Step: 33 Validation Loss: 0.6152374148368835 \n",
      "     Validation Step: 34 Validation Loss: 0.6112099885940552 \n",
      "     Validation Step: 35 Validation Loss: 0.6101995706558228 \n",
      "     Validation Step: 36 Validation Loss: 0.6183714866638184 \n",
      "     Validation Step: 37 Validation Loss: 0.6170270442962646 \n",
      "     Validation Step: 38 Validation Loss: 0.6185141801834106 \n",
      "     Validation Step: 39 Validation Loss: 0.6133177280426025 \n",
      "     Validation Step: 40 Validation Loss: 0.6145726442337036 \n",
      "     Validation Step: 41 Validation Loss: 0.6149045825004578 \n",
      "     Validation Step: 42 Validation Loss: 0.6177123188972473 \n",
      "     Validation Step: 43 Validation Loss: 0.6075920462608337 \n",
      "     Validation Step: 44 Validation Loss: 0.6116630434989929 \n",
      "Epoch: 44\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6109091639518738 \n",
      "     Training Step: 1 Training Loss: 0.6140891909599304 \n",
      "     Training Step: 2 Training Loss: 0.6158297657966614 \n",
      "     Training Step: 3 Training Loss: 0.6100782155990601 \n",
      "     Training Step: 4 Training Loss: 0.6199566125869751 \n",
      "     Training Step: 5 Training Loss: 0.6160356998443604 \n",
      "     Training Step: 6 Training Loss: 0.6160765886306763 \n",
      "     Training Step: 7 Training Loss: 0.6130200624465942 \n",
      "     Training Step: 8 Training Loss: 0.6181131601333618 \n",
      "     Training Step: 9 Training Loss: 0.6145011782646179 \n",
      "     Training Step: 10 Training Loss: 0.6163755655288696 \n",
      "     Training Step: 11 Training Loss: 0.609808623790741 \n",
      "     Training Step: 12 Training Loss: 0.6138933300971985 \n",
      "     Training Step: 13 Training Loss: 0.6156585216522217 \n",
      "     Training Step: 14 Training Loss: 0.6185411214828491 \n",
      "     Training Step: 15 Training Loss: 0.6186474561691284 \n",
      "     Training Step: 16 Training Loss: 0.6154912114143372 \n",
      "     Training Step: 17 Training Loss: 0.6139041185379028 \n",
      "     Training Step: 18 Training Loss: 0.6167536377906799 \n",
      "     Training Step: 19 Training Loss: 0.6118354201316833 \n",
      "     Training Step: 20 Training Loss: 0.617046594619751 \n",
      "     Training Step: 21 Training Loss: 0.6151506304740906 \n",
      "     Training Step: 22 Training Loss: 0.6119265556335449 \n",
      "     Training Step: 23 Training Loss: 0.612815260887146 \n",
      "     Training Step: 24 Training Loss: 0.6106149554252625 \n",
      "     Training Step: 25 Training Loss: 0.6125671863555908 \n",
      "     Training Step: 26 Training Loss: 0.6146942973136902 \n",
      "     Training Step: 27 Training Loss: 0.6118786931037903 \n",
      "     Training Step: 28 Training Loss: 0.6168498396873474 \n",
      "     Training Step: 29 Training Loss: 0.6121876835823059 \n",
      "     Training Step: 30 Training Loss: 0.6164950728416443 \n",
      "     Training Step: 31 Training Loss: 0.611638069152832 \n",
      "     Training Step: 32 Training Loss: 0.611892580986023 \n",
      "     Training Step: 33 Training Loss: 0.6152738332748413 \n",
      "     Training Step: 34 Training Loss: 0.6114662885665894 \n",
      "     Training Step: 35 Training Loss: 0.6118593215942383 \n",
      "     Training Step: 36 Training Loss: 0.6147122979164124 \n",
      "     Training Step: 37 Training Loss: 0.6157798171043396 \n",
      "     Training Step: 38 Training Loss: 0.6171393990516663 \n",
      "     Training Step: 39 Training Loss: 0.6153042316436768 \n",
      "     Training Step: 40 Training Loss: 0.6122267842292786 \n",
      "     Training Step: 41 Training Loss: 0.6117548942565918 \n",
      "     Training Step: 42 Training Loss: 0.6134956479072571 \n",
      "     Training Step: 43 Training Loss: 0.6131796836853027 \n",
      "     Training Step: 44 Training Loss: 0.6129030585289001 \n",
      "     Training Step: 45 Training Loss: 0.6122620105743408 \n",
      "     Training Step: 46 Training Loss: 0.617891252040863 \n",
      "     Training Step: 47 Training Loss: 0.6094964742660522 \n",
      "     Training Step: 48 Training Loss: 0.6105562448501587 \n",
      "     Training Step: 49 Training Loss: 0.6112183332443237 \n",
      "     Training Step: 50 Training Loss: 0.6148267984390259 \n",
      "     Training Step: 51 Training Loss: 0.6106533408164978 \n",
      "     Training Step: 52 Training Loss: 0.6176409125328064 \n",
      "     Training Step: 53 Training Loss: 0.6114293932914734 \n",
      "     Training Step: 54 Training Loss: 0.6141659021377563 \n",
      "     Training Step: 55 Training Loss: 0.6132076382637024 \n",
      "     Training Step: 56 Training Loss: 0.6106133460998535 \n",
      "     Training Step: 57 Training Loss: 0.611498236656189 \n",
      "     Training Step: 58 Training Loss: 0.614682137966156 \n",
      "     Training Step: 59 Training Loss: 0.6147488355636597 \n",
      "     Training Step: 60 Training Loss: 0.6092406511306763 \n",
      "     Training Step: 61 Training Loss: 0.614708423614502 \n",
      "     Training Step: 62 Training Loss: 0.6116299033164978 \n",
      "     Training Step: 63 Training Loss: 0.6144360303878784 \n",
      "     Training Step: 64 Training Loss: 0.6106034517288208 \n",
      "     Training Step: 65 Training Loss: 0.6129875183105469 \n",
      "     Training Step: 66 Training Loss: 0.6172472238540649 \n",
      "     Training Step: 67 Training Loss: 0.6155641078948975 \n",
      "     Training Step: 68 Training Loss: 0.611592710018158 \n",
      "     Training Step: 69 Training Loss: 0.6135042309761047 \n",
      "     Training Step: 70 Training Loss: 0.6133282780647278 \n",
      "     Training Step: 71 Training Loss: 0.6172285676002502 \n",
      "     Training Step: 72 Training Loss: 0.6141563057899475 \n",
      "     Training Step: 73 Training Loss: 0.6132250428199768 \n",
      "     Training Step: 74 Training Loss: 0.6118475198745728 \n",
      "     Training Step: 75 Training Loss: 0.6181368827819824 \n",
      "     Training Step: 76 Training Loss: 0.6128981113433838 \n",
      "     Training Step: 77 Training Loss: 0.615409255027771 \n",
      "     Training Step: 78 Training Loss: 0.615455687046051 \n",
      "     Training Step: 79 Training Loss: 0.6138976216316223 \n",
      "     Training Step: 80 Training Loss: 0.6146801114082336 \n",
      "     Training Step: 81 Training Loss: 0.6104332208633423 \n",
      "     Training Step: 82 Training Loss: 0.6135648488998413 \n",
      "     Training Step: 83 Training Loss: 0.6195666193962097 \n",
      "     Training Step: 84 Training Loss: 0.6114364266395569 \n",
      "     Training Step: 85 Training Loss: 0.6167495250701904 \n",
      "     Training Step: 86 Training Loss: 0.6209822297096252 \n",
      "     Training Step: 87 Training Loss: 0.610784649848938 \n",
      "     Training Step: 88 Training Loss: 0.6183871626853943 \n",
      "     Training Step: 89 Training Loss: 0.6142323613166809 \n",
      "     Training Step: 90 Training Loss: 0.6177791357040405 \n",
      "     Training Step: 91 Training Loss: 0.6165181398391724 \n",
      "     Training Step: 92 Training Loss: 0.6137849688529968 \n",
      "     Training Step: 93 Training Loss: 0.6176908612251282 \n",
      "     Training Step: 94 Training Loss: 0.6176977157592773 \n",
      "     Training Step: 95 Training Loss: 0.6102649569511414 \n",
      "     Training Step: 96 Training Loss: 0.6149381995201111 \n",
      "     Training Step: 97 Training Loss: 0.6167552471160889 \n",
      "     Training Step: 98 Training Loss: 0.6153151988983154 \n",
      "     Training Step: 99 Training Loss: 0.6168246865272522 \n",
      "     Training Step: 100 Training Loss: 0.6133325099945068 \n",
      "     Training Step: 101 Training Loss: 0.614474892616272 \n",
      "     Training Step: 102 Training Loss: 0.6147235035896301 \n",
      "     Training Step: 103 Training Loss: 0.611598014831543 \n",
      "     Training Step: 104 Training Loss: 0.6167144775390625 \n",
      "     Training Step: 105 Training Loss: 0.6168155074119568 \n",
      "     Training Step: 106 Training Loss: 0.6124864816665649 \n",
      "     Training Step: 107 Training Loss: 0.6123158931732178 \n",
      "     Training Step: 108 Training Loss: 0.6186184287071228 \n",
      "     Training Step: 109 Training Loss: 0.6184566617012024 \n",
      "     Training Step: 110 Training Loss: 0.6119571924209595 \n",
      "     Training Step: 111 Training Loss: 0.614728569984436 \n",
      "     Training Step: 112 Training Loss: 0.6136662364006042 \n",
      "     Training Step: 113 Training Loss: 0.6153572797775269 \n",
      "     Training Step: 114 Training Loss: 0.610216498374939 \n",
      "     Training Step: 115 Training Loss: 0.6142328381538391 \n",
      "     Training Step: 116 Training Loss: 0.6190330386161804 \n",
      "     Training Step: 117 Training Loss: 0.6158390045166016 \n",
      "     Training Step: 118 Training Loss: 0.6144349575042725 \n",
      "     Training Step: 119 Training Loss: 0.6151854991912842 \n",
      "     Training Step: 120 Training Loss: 0.6196074485778809 \n",
      "     Training Step: 121 Training Loss: 0.6147650480270386 \n",
      "     Training Step: 122 Training Loss: 0.6135150790214539 \n",
      "     Training Step: 123 Training Loss: 0.6108128428459167 \n",
      "     Training Step: 124 Training Loss: 0.6148375272750854 \n",
      "     Training Step: 125 Training Loss: 0.6177719831466675 \n",
      "     Training Step: 126 Training Loss: 0.6083025932312012 \n",
      "     Training Step: 127 Training Loss: 0.6125244498252869 \n",
      "     Training Step: 128 Training Loss: 0.6114261746406555 \n",
      "     Training Step: 129 Training Loss: 0.6157800555229187 \n",
      "     Training Step: 130 Training Loss: 0.620345950126648 \n",
      "     Training Step: 131 Training Loss: 0.6140524744987488 \n",
      "     Training Step: 132 Training Loss: 0.6155109405517578 \n",
      "     Training Step: 133 Training Loss: 0.6116986870765686 \n",
      "     Training Step: 134 Training Loss: 0.6125591397285461 \n",
      "     Training Step: 135 Training Loss: 0.6122699975967407 \n",
      "     Training Step: 136 Training Loss: 0.6101225018501282 \n",
      "     Training Step: 137 Training Loss: 0.6107168197631836 \n",
      "     Training Step: 138 Training Loss: 0.6178221106529236 \n",
      "     Training Step: 139 Training Loss: 0.614910900592804 \n",
      "     Training Step: 140 Training Loss: 0.6159403324127197 \n",
      "     Training Step: 141 Training Loss: 0.6134771108627319 \n",
      "     Training Step: 142 Training Loss: 0.6117120385169983 \n",
      "     Training Step: 143 Training Loss: 0.6142955422401428 \n",
      "     Training Step: 144 Training Loss: 0.6168367862701416 \n",
      "     Training Step: 145 Training Loss: 0.6124249696731567 \n",
      "     Training Step: 146 Training Loss: 0.6115144491195679 \n",
      "     Training Step: 147 Training Loss: 0.6115391850471497 \n",
      "     Training Step: 148 Training Loss: 0.6136729717254639 \n",
      "     Training Step: 149 Training Loss: 0.6111794710159302 \n",
      "     Training Step: 150 Training Loss: 0.6154889464378357 \n",
      "     Training Step: 151 Training Loss: 0.614799439907074 \n",
      "     Training Step: 152 Training Loss: 0.6180808544158936 \n",
      "     Training Step: 153 Training Loss: 0.6153305172920227 \n",
      "     Training Step: 154 Training Loss: 0.6167370080947876 \n",
      "     Training Step: 155 Training Loss: 0.6124753952026367 \n",
      "     Training Step: 156 Training Loss: 0.6158226132392883 \n",
      "     Training Step: 157 Training Loss: 0.6100862622261047 \n",
      "     Training Step: 158 Training Loss: 0.610951840877533 \n",
      "     Training Step: 159 Training Loss: 0.6165786981582642 \n",
      "     Training Step: 160 Training Loss: 0.6135948300361633 \n",
      "     Training Step: 161 Training Loss: 0.6094132661819458 \n",
      "     Training Step: 162 Training Loss: 0.6129591464996338 \n",
      "     Training Step: 163 Training Loss: 0.6121892333030701 \n",
      "     Training Step: 164 Training Loss: 0.6116121411323547 \n",
      "     Training Step: 165 Training Loss: 0.612145721912384 \n",
      "     Training Step: 166 Training Loss: 0.6151244044303894 \n",
      "     Training Step: 167 Training Loss: 0.614987313747406 \n",
      "     Training Step: 168 Training Loss: 0.6182774901390076 \n",
      "     Training Step: 169 Training Loss: 0.6154744625091553 \n",
      "     Training Step: 170 Training Loss: 0.6098992824554443 \n",
      "     Training Step: 171 Training Loss: 0.6155443787574768 \n",
      "     Training Step: 172 Training Loss: 0.6097803115844727 \n",
      "     Training Step: 173 Training Loss: 0.615800678730011 \n",
      "     Training Step: 174 Training Loss: 0.6181837320327759 \n",
      "     Training Step: 175 Training Loss: 0.6136391162872314 \n",
      "     Training Step: 176 Training Loss: 0.6105731129646301 \n",
      "     Training Step: 177 Training Loss: 0.6111885905265808 \n",
      "     Training Step: 178 Training Loss: 0.6139432787895203 \n",
      "     Training Step: 179 Training Loss: 0.6130906343460083 \n",
      "     Training Step: 180 Training Loss: 0.6184348464012146 \n",
      "     Training Step: 181 Training Loss: 0.6122727394104004 \n",
      "     Training Step: 182 Training Loss: 0.6137738227844238 \n",
      "     Training Step: 183 Training Loss: 0.6143563389778137 \n",
      "     Training Step: 184 Training Loss: 0.613322377204895 \n",
      "     Training Step: 185 Training Loss: 0.613969624042511 \n",
      "     Training Step: 186 Training Loss: 0.6171613931655884 \n",
      "     Training Step: 187 Training Loss: 0.6161222457885742 \n",
      "     Training Step: 188 Training Loss: 0.6131764650344849 \n",
      "     Training Step: 189 Training Loss: 0.6146141886711121 \n",
      "     Training Step: 190 Training Loss: 0.6104550361633301 \n",
      "     Training Step: 191 Training Loss: 0.6174689531326294 \n",
      "     Training Step: 192 Training Loss: 0.6133154630661011 \n",
      "     Training Step: 193 Training Loss: 0.6159833669662476 \n",
      "     Training Step: 194 Training Loss: 0.6169127821922302 \n",
      "     Training Step: 195 Training Loss: 0.6145268082618713 \n",
      "     Training Step: 196 Training Loss: 0.6132721900939941 \n",
      "     Training Step: 197 Training Loss: 0.6166598200798035 \n",
      "     Training Step: 198 Training Loss: 0.6138508915901184 \n",
      "     Training Step: 199 Training Loss: 0.6167304515838623 \n",
      "     Training Step: 200 Training Loss: 0.6151076555252075 \n",
      "     Training Step: 201 Training Loss: 0.6152952909469604 \n",
      "     Training Step: 202 Training Loss: 0.6128523945808411 \n",
      "     Training Step: 203 Training Loss: 0.6140275001525879 \n",
      "     Training Step: 204 Training Loss: 0.6162213087081909 \n",
      "     Training Step: 205 Training Loss: 0.6188678741455078 \n",
      "     Training Step: 206 Training Loss: 0.6177928447723389 \n",
      "     Training Step: 207 Training Loss: 0.6182377934455872 \n",
      "     Training Step: 208 Training Loss: 0.6158177852630615 \n",
      "     Training Step: 209 Training Loss: 0.6127567887306213 \n",
      "     Training Step: 210 Training Loss: 0.6133396625518799 \n",
      "     Training Step: 211 Training Loss: 0.6167285442352295 \n",
      "     Training Step: 212 Training Loss: 0.6101545691490173 \n",
      "     Training Step: 213 Training Loss: 0.6107273697853088 \n",
      "     Training Step: 214 Training Loss: 0.6130945682525635 \n",
      "     Training Step: 215 Training Loss: 0.6200450658798218 \n",
      "     Training Step: 216 Training Loss: 0.6162331104278564 \n",
      "     Training Step: 217 Training Loss: 0.6141647696495056 \n",
      "     Training Step: 218 Training Loss: 0.6118206977844238 \n",
      "     Training Step: 219 Training Loss: 0.6121704578399658 \n",
      "     Training Step: 220 Training Loss: 0.6124083399772644 \n",
      "     Training Step: 221 Training Loss: 0.6145933866500854 \n",
      "     Training Step: 222 Training Loss: 0.6148159503936768 \n",
      "     Training Step: 223 Training Loss: 0.615568220615387 \n",
      "     Training Step: 224 Training Loss: 0.6142851710319519 \n",
      "     Training Step: 225 Training Loss: 0.6121724843978882 \n",
      "     Training Step: 226 Training Loss: 0.6151822209358215 \n",
      "     Training Step: 227 Training Loss: 0.6141886115074158 \n",
      "     Training Step: 228 Training Loss: 0.6129328012466431 \n",
      "     Training Step: 229 Training Loss: 0.612548828125 \n",
      "     Training Step: 230 Training Loss: 0.6148428916931152 \n",
      "     Training Step: 231 Training Loss: 0.6144242882728577 \n",
      "     Training Step: 232 Training Loss: 0.6172022223472595 \n",
      "     Training Step: 233 Training Loss: 0.6127339601516724 \n",
      "     Training Step: 234 Training Loss: 0.6163200736045837 \n",
      "     Training Step: 235 Training Loss: 0.6168561577796936 \n",
      "     Training Step: 236 Training Loss: 0.6128779053688049 \n",
      "     Training Step: 237 Training Loss: 0.6143657565116882 \n",
      "     Training Step: 238 Training Loss: 0.6123281717300415 \n",
      "     Training Step: 239 Training Loss: 0.615252673625946 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6137251257896423 \n",
      "     Validation Step: 1 Validation Loss: 0.6112217903137207 \n",
      "     Validation Step: 2 Validation Loss: 0.613375723361969 \n",
      "     Validation Step: 3 Validation Loss: 0.6105451583862305 \n",
      "     Validation Step: 4 Validation Loss: 0.6106775999069214 \n",
      "     Validation Step: 5 Validation Loss: 0.6137117147445679 \n",
      "     Validation Step: 6 Validation Loss: 0.61159348487854 \n",
      "     Validation Step: 7 Validation Loss: 0.6171624660491943 \n",
      "     Validation Step: 8 Validation Loss: 0.6163665056228638 \n",
      "     Validation Step: 9 Validation Loss: 0.6111895442008972 \n",
      "     Validation Step: 10 Validation Loss: 0.6147188544273376 \n",
      "     Validation Step: 11 Validation Loss: 0.6157165169715881 \n",
      "     Validation Step: 12 Validation Loss: 0.6137411594390869 \n",
      "     Validation Step: 13 Validation Loss: 0.6177341938018799 \n",
      "     Validation Step: 14 Validation Loss: 0.6154055595397949 \n",
      "     Validation Step: 15 Validation Loss: 0.6119166016578674 \n",
      "     Validation Step: 16 Validation Loss: 0.6142784953117371 \n",
      "     Validation Step: 17 Validation Loss: 0.6141838431358337 \n",
      "     Validation Step: 18 Validation Loss: 0.6159012317657471 \n",
      "     Validation Step: 19 Validation Loss: 0.6102224588394165 \n",
      "     Validation Step: 20 Validation Loss: 0.6149190068244934 \n",
      "     Validation Step: 21 Validation Loss: 0.6178249716758728 \n",
      "     Validation Step: 22 Validation Loss: 0.6143463253974915 \n",
      "     Validation Step: 23 Validation Loss: 0.6161226630210876 \n",
      "     Validation Step: 24 Validation Loss: 0.6130332350730896 \n",
      "     Validation Step: 25 Validation Loss: 0.6153237223625183 \n",
      "     Validation Step: 26 Validation Loss: 0.6149835586547852 \n",
      "     Validation Step: 27 Validation Loss: 0.6101620197296143 \n",
      "     Validation Step: 28 Validation Loss: 0.6186349987983704 \n",
      "     Validation Step: 29 Validation Loss: 0.6184826493263245 \n",
      "     Validation Step: 30 Validation Loss: 0.6101220846176147 \n",
      "     Validation Step: 31 Validation Loss: 0.6116942167282104 \n",
      "     Validation Step: 32 Validation Loss: 0.6174455285072327 \n",
      "     Validation Step: 33 Validation Loss: 0.6183916926383972 \n",
      "     Validation Step: 34 Validation Loss: 0.6146222352981567 \n",
      "     Validation Step: 35 Validation Loss: 0.6128789186477661 \n",
      "     Validation Step: 36 Validation Loss: 0.6105203032493591 \n",
      "     Validation Step: 37 Validation Loss: 0.6146741509437561 \n",
      "     Validation Step: 38 Validation Loss: 0.6157031655311584 \n",
      "     Validation Step: 39 Validation Loss: 0.6182073354721069 \n",
      "     Validation Step: 40 Validation Loss: 0.6075166463851929 \n",
      "     Validation Step: 41 Validation Loss: 0.6121718883514404 \n",
      "     Validation Step: 42 Validation Loss: 0.6151303052902222 \n",
      "     Validation Step: 43 Validation Loss: 0.6141892075538635 \n",
      "     Validation Step: 44 Validation Loss: 0.6186602115631104 \n",
      "Epoch: 45\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.611932098865509 \n",
      "     Training Step: 1 Training Loss: 0.6153929233551025 \n",
      "     Training Step: 2 Training Loss: 0.6101389527320862 \n",
      "     Training Step: 3 Training Loss: 0.6125208139419556 \n",
      "     Training Step: 4 Training Loss: 0.6135472655296326 \n",
      "     Training Step: 5 Training Loss: 0.6171938180923462 \n",
      "     Training Step: 6 Training Loss: 0.6118577122688293 \n",
      "     Training Step: 7 Training Loss: 0.6174248456954956 \n",
      "     Training Step: 8 Training Loss: 0.6133064031600952 \n",
      "     Training Step: 9 Training Loss: 0.61562579870224 \n",
      "     Training Step: 10 Training Loss: 0.6131601333618164 \n",
      "     Training Step: 11 Training Loss: 0.6186005473136902 \n",
      "     Training Step: 12 Training Loss: 0.6209239363670349 \n",
      "     Training Step: 13 Training Loss: 0.6130760908126831 \n",
      "     Training Step: 14 Training Loss: 0.6154695749282837 \n",
      "     Training Step: 15 Training Loss: 0.6155399084091187 \n",
      "     Training Step: 16 Training Loss: 0.6154024004936218 \n",
      "     Training Step: 17 Training Loss: 0.6106724739074707 \n",
      "     Training Step: 18 Training Loss: 0.6112324595451355 \n",
      "     Training Step: 19 Training Loss: 0.6176105737686157 \n",
      "     Training Step: 20 Training Loss: 0.6132347583770752 \n",
      "     Training Step: 21 Training Loss: 0.6082831621170044 \n",
      "     Training Step: 22 Training Loss: 0.6118407845497131 \n",
      "     Training Step: 23 Training Loss: 0.6122898459434509 \n",
      "     Training Step: 24 Training Loss: 0.613487720489502 \n",
      "     Training Step: 25 Training Loss: 0.6178683638572693 \n",
      "     Training Step: 26 Training Loss: 0.616729199886322 \n",
      "     Training Step: 27 Training Loss: 0.6143860220909119 \n",
      "     Training Step: 28 Training Loss: 0.6162716746330261 \n",
      "     Training Step: 29 Training Loss: 0.6147884130477905 \n",
      "     Training Step: 30 Training Loss: 0.6166643500328064 \n",
      "     Training Step: 31 Training Loss: 0.6115948557853699 \n",
      "     Training Step: 32 Training Loss: 0.6141219735145569 \n",
      "     Training Step: 33 Training Loss: 0.6128864288330078 \n",
      "     Training Step: 34 Training Loss: 0.6142212152481079 \n",
      "     Training Step: 35 Training Loss: 0.6112253665924072 \n",
      "     Training Step: 36 Training Loss: 0.6131190061569214 \n",
      "     Training Step: 37 Training Loss: 0.6146889328956604 \n",
      "     Training Step: 38 Training Loss: 0.6128816604614258 \n",
      "     Training Step: 39 Training Loss: 0.6165176033973694 \n",
      "     Training Step: 40 Training Loss: 0.6147175431251526 \n",
      "     Training Step: 41 Training Loss: 0.6132599711418152 \n",
      "     Training Step: 42 Training Loss: 0.6135523915290833 \n",
      "     Training Step: 43 Training Loss: 0.6118725538253784 \n",
      "     Training Step: 44 Training Loss: 0.6154390573501587 \n",
      "     Training Step: 45 Training Loss: 0.6169593930244446 \n",
      "     Training Step: 46 Training Loss: 0.6147804856300354 \n",
      "     Training Step: 47 Training Loss: 0.6104689240455627 \n",
      "     Training Step: 48 Training Loss: 0.6124202013015747 \n",
      "     Training Step: 49 Training Loss: 0.6168519854545593 \n",
      "     Training Step: 50 Training Loss: 0.6202735900878906 \n",
      "     Training Step: 51 Training Loss: 0.6131651997566223 \n",
      "     Training Step: 52 Training Loss: 0.6125800013542175 \n",
      "     Training Step: 53 Training Loss: 0.6140361428260803 \n",
      "     Training Step: 54 Training Loss: 0.6128767132759094 \n",
      "     Training Step: 55 Training Loss: 0.6154918074607849 \n",
      "     Training Step: 56 Training Loss: 0.614113986492157 \n",
      "     Training Step: 57 Training Loss: 0.61213219165802 \n",
      "     Training Step: 58 Training Loss: 0.613310694694519 \n",
      "     Training Step: 59 Training Loss: 0.6162360310554504 \n",
      "     Training Step: 60 Training Loss: 0.6105992197990417 \n",
      "     Training Step: 61 Training Loss: 0.6184304356575012 \n",
      "     Training Step: 62 Training Loss: 0.6094978451728821 \n",
      "     Training Step: 63 Training Loss: 0.6133663058280945 \n",
      "     Training Step: 64 Training Loss: 0.6100883483886719 \n",
      "     Training Step: 65 Training Loss: 0.6115332841873169 \n",
      "     Training Step: 66 Training Loss: 0.6129206418991089 \n",
      "     Training Step: 67 Training Loss: 0.6183900833129883 \n",
      "     Training Step: 68 Training Loss: 0.6121671199798584 \n",
      "     Training Step: 69 Training Loss: 0.6198126673698425 \n",
      "     Training Step: 70 Training Loss: 0.6145811080932617 \n",
      "     Training Step: 71 Training Loss: 0.6143203973770142 \n",
      "     Training Step: 72 Training Loss: 0.610192596912384 \n",
      "     Training Step: 73 Training Loss: 0.6127945184707642 \n",
      "     Training Step: 74 Training Loss: 0.6153855323791504 \n",
      "     Training Step: 75 Training Loss: 0.6107476949691772 \n",
      "     Training Step: 76 Training Loss: 0.6186483502388 \n",
      "     Training Step: 77 Training Loss: 0.612786054611206 \n",
      "     Training Step: 78 Training Loss: 0.6117372512817383 \n",
      "     Training Step: 79 Training Loss: 0.6098861694335938 \n",
      "     Training Step: 80 Training Loss: 0.613672137260437 \n",
      "     Training Step: 81 Training Loss: 0.6124742031097412 \n",
      "     Training Step: 82 Training Loss: 0.6097278594970703 \n",
      "     Training Step: 83 Training Loss: 0.6197157502174377 \n",
      "     Training Step: 84 Training Loss: 0.6158381700515747 \n",
      "     Training Step: 85 Training Loss: 0.6108816266059875 \n",
      "     Training Step: 86 Training Loss: 0.6142731308937073 \n",
      "     Training Step: 87 Training Loss: 0.6153166890144348 \n",
      "     Training Step: 88 Training Loss: 0.6157826781272888 \n",
      "     Training Step: 89 Training Loss: 0.61673903465271 \n",
      "     Training Step: 90 Training Loss: 0.611638605594635 \n",
      "     Training Step: 91 Training Loss: 0.6094300150871277 \n",
      "     Training Step: 92 Training Loss: 0.6178711652755737 \n",
      "     Training Step: 93 Training Loss: 0.6167868375778198 \n",
      "     Training Step: 94 Training Loss: 0.6140715479850769 \n",
      "     Training Step: 95 Training Loss: 0.6146121025085449 \n",
      "     Training Step: 96 Training Loss: 0.6155931949615479 \n",
      "     Training Step: 97 Training Loss: 0.612372875213623 \n",
      "     Training Step: 98 Training Loss: 0.6098152995109558 \n",
      "     Training Step: 99 Training Loss: 0.6121688485145569 \n",
      "     Training Step: 100 Training Loss: 0.6116452217102051 \n",
      "     Training Step: 101 Training Loss: 0.6199923157691956 \n",
      "     Training Step: 102 Training Loss: 0.6143706440925598 \n",
      "     Training Step: 103 Training Loss: 0.6105394959449768 \n",
      "     Training Step: 104 Training Loss: 0.6149482131004333 \n",
      "     Training Step: 105 Training Loss: 0.61723393201828 \n",
      "     Training Step: 106 Training Loss: 0.6180890202522278 \n",
      "     Training Step: 107 Training Loss: 0.6147250533103943 \n",
      "     Training Step: 108 Training Loss: 0.6121839284896851 \n",
      "     Training Step: 109 Training Loss: 0.6136901378631592 \n",
      "     Training Step: 110 Training Loss: 0.6151493787765503 \n",
      "     Training Step: 111 Training Loss: 0.6138057708740234 \n",
      "     Training Step: 112 Training Loss: 0.6145294904708862 \n",
      "     Training Step: 113 Training Loss: 0.61846923828125 \n",
      "     Training Step: 114 Training Loss: 0.6199100613594055 \n",
      "     Training Step: 115 Training Loss: 0.6149536967277527 \n",
      "     Training Step: 116 Training Loss: 0.6145409941673279 \n",
      "     Training Step: 117 Training Loss: 0.6177720427513123 \n",
      "     Training Step: 118 Training Loss: 0.6180112361907959 \n",
      "     Training Step: 119 Training Loss: 0.6147488951683044 \n",
      "     Training Step: 120 Training Loss: 0.6168509125709534 \n",
      "     Training Step: 121 Training Loss: 0.618061900138855 \n",
      "     Training Step: 122 Training Loss: 0.6141695380210876 \n",
      "     Training Step: 123 Training Loss: 0.6106979846954346 \n",
      "     Training Step: 124 Training Loss: 0.6157501339912415 \n",
      "     Training Step: 125 Training Loss: 0.6181180477142334 \n",
      "     Training Step: 126 Training Loss: 0.614720344543457 \n",
      "     Training Step: 127 Training Loss: 0.6177060604095459 \n",
      "     Training Step: 128 Training Loss: 0.6164484024047852 \n",
      "     Training Step: 129 Training Loss: 0.6144599318504333 \n",
      "     Training Step: 130 Training Loss: 0.6154136061668396 \n",
      "     Training Step: 131 Training Loss: 0.6117332577705383 \n",
      "     Training Step: 132 Training Loss: 0.6127572059631348 \n",
      "     Training Step: 133 Training Loss: 0.6159751415252686 \n",
      "     Training Step: 134 Training Loss: 0.6150955557823181 \n",
      "     Training Step: 135 Training Loss: 0.6139081120491028 \n",
      "     Training Step: 136 Training Loss: 0.6115014553070068 \n",
      "     Training Step: 137 Training Loss: 0.6135143637657166 \n",
      "     Training Step: 138 Training Loss: 0.6123092174530029 \n",
      "     Training Step: 139 Training Loss: 0.6113994717597961 \n",
      "     Training Step: 140 Training Loss: 0.610030472278595 \n",
      "     Training Step: 141 Training Loss: 0.6154948472976685 \n",
      "     Training Step: 142 Training Loss: 0.6146979928016663 \n",
      "     Training Step: 143 Training Loss: 0.6166841983795166 \n",
      "     Training Step: 144 Training Loss: 0.6164361238479614 \n",
      "     Training Step: 145 Training Loss: 0.6178359985351562 \n",
      "     Training Step: 146 Training Loss: 0.6183029413223267 \n",
      "     Training Step: 147 Training Loss: 0.6113583445549011 \n",
      "     Training Step: 148 Training Loss: 0.6182188391685486 \n",
      "     Training Step: 149 Training Loss: 0.6150182485580444 \n",
      "     Training Step: 150 Training Loss: 0.6132470369338989 \n",
      "     Training Step: 151 Training Loss: 0.6122717261314392 \n",
      "     Training Step: 152 Training Loss: 0.6115658283233643 \n",
      "     Training Step: 153 Training Loss: 0.6114873886108398 \n",
      "     Training Step: 154 Training Loss: 0.614784300327301 \n",
      "     Training Step: 155 Training Loss: 0.6118507981300354 \n",
      "     Training Step: 156 Training Loss: 0.6104875206947327 \n",
      "     Training Step: 157 Training Loss: 0.6156091690063477 \n",
      "     Training Step: 158 Training Loss: 0.6124062538146973 \n",
      "     Training Step: 159 Training Loss: 0.6158557534217834 \n",
      "     Training Step: 160 Training Loss: 0.6124176979064941 \n",
      "     Training Step: 161 Training Loss: 0.6100564002990723 \n",
      "     Training Step: 162 Training Loss: 0.6152227520942688 \n",
      "     Training Step: 163 Training Loss: 0.6151096224784851 \n",
      "     Training Step: 164 Training Loss: 0.6152504682540894 \n",
      "     Training Step: 165 Training Loss: 0.6186025738716125 \n",
      "     Training Step: 166 Training Loss: 0.6139522194862366 \n",
      "     Training Step: 167 Training Loss: 0.6107704639434814 \n",
      "     Training Step: 168 Training Loss: 0.6116967797279358 \n",
      "     Training Step: 169 Training Loss: 0.6149082779884338 \n",
      "     Training Step: 170 Training Loss: 0.6136772632598877 \n",
      "     Training Step: 171 Training Loss: 0.61613929271698 \n",
      "     Training Step: 172 Training Loss: 0.6103888154029846 \n",
      "     Training Step: 173 Training Loss: 0.6120924949645996 \n",
      "     Training Step: 174 Training Loss: 0.6129303574562073 \n",
      "     Training Step: 175 Training Loss: 0.6118349432945251 \n",
      "     Training Step: 176 Training Loss: 0.6133329272270203 \n",
      "     Training Step: 177 Training Loss: 0.611656129360199 \n",
      "     Training Step: 178 Training Loss: 0.6154474020004272 \n",
      "     Training Step: 179 Training Loss: 0.6138656735420227 \n",
      "     Training Step: 180 Training Loss: 0.6132956147193909 \n",
      "     Training Step: 181 Training Loss: 0.6136274337768555 \n",
      "     Training Step: 182 Training Loss: 0.61394202709198 \n",
      "     Training Step: 183 Training Loss: 0.616228461265564 \n",
      "     Training Step: 184 Training Loss: 0.612562358379364 \n",
      "     Training Step: 185 Training Loss: 0.6156730651855469 \n",
      "     Training Step: 186 Training Loss: 0.6130922436714172 \n",
      "     Training Step: 187 Training Loss: 0.6166396141052246 \n",
      "     Training Step: 188 Training Loss: 0.6153075098991394 \n",
      "     Training Step: 189 Training Loss: 0.6160271167755127 \n",
      "     Training Step: 190 Training Loss: 0.6163545250892639 \n",
      "     Training Step: 191 Training Loss: 0.6108183860778809 \n",
      "     Training Step: 192 Training Loss: 0.6167046427726746 \n",
      "     Training Step: 193 Training Loss: 0.6167219281196594 \n",
      "     Training Step: 194 Training Loss: 0.6134878396987915 \n",
      "     Training Step: 195 Training Loss: 0.6125501394271851 \n",
      "     Training Step: 196 Training Loss: 0.61890709400177 \n",
      "     Training Step: 197 Training Loss: 0.6101943850517273 \n",
      "     Training Step: 198 Training Loss: 0.6144210696220398 \n",
      "     Training Step: 199 Training Loss: 0.6092584729194641 \n",
      "     Training Step: 200 Training Loss: 0.6122384667396545 \n",
      "     Training Step: 201 Training Loss: 0.6146573424339294 \n",
      "     Training Step: 202 Training Loss: 0.6144149303436279 \n",
      "     Training Step: 203 Training Loss: 0.6158888339996338 \n",
      "     Training Step: 204 Training Loss: 0.6126900911331177 \n",
      "     Training Step: 205 Training Loss: 0.615318775177002 \n",
      "     Training Step: 206 Training Loss: 0.6147320866584778 \n",
      "     Training Step: 207 Training Loss: 0.6151701211929321 \n",
      "     Training Step: 208 Training Loss: 0.6107903122901917 \n",
      "     Training Step: 209 Training Loss: 0.6142890453338623 \n",
      "     Training Step: 210 Training Loss: 0.6137779355049133 \n",
      "     Training Step: 211 Training Loss: 0.61333829164505 \n",
      "     Training Step: 212 Training Loss: 0.6138179898262024 \n",
      "     Training Step: 213 Training Loss: 0.6115623712539673 \n",
      "     Training Step: 214 Training Loss: 0.615693986415863 \n",
      "     Training Step: 215 Training Loss: 0.61409592628479 \n",
      "     Training Step: 216 Training Loss: 0.6149632930755615 \n",
      "     Training Step: 217 Training Loss: 0.6167774796485901 \n",
      "     Training Step: 218 Training Loss: 0.6176334023475647 \n",
      "     Training Step: 219 Training Loss: 0.616922914981842 \n",
      "     Training Step: 220 Training Loss: 0.6118566393852234 \n",
      "     Training Step: 221 Training Loss: 0.6171059012413025 \n",
      "     Training Step: 222 Training Loss: 0.6146247982978821 \n",
      "     Training Step: 223 Training Loss: 0.6143847703933716 \n",
      "     Training Step: 224 Training Loss: 0.6161332726478577 \n",
      "     Training Step: 225 Training Loss: 0.6147922277450562 \n",
      "     Training Step: 226 Training Loss: 0.6188432574272156 \n",
      "     Training Step: 227 Training Loss: 0.6116563081741333 \n",
      "     Training Step: 228 Training Loss: 0.6115275621414185 \n",
      "     Training Step: 229 Training Loss: 0.6120341420173645 \n",
      "     Training Step: 230 Training Loss: 0.6129749417304993 \n",
      "     Training Step: 231 Training Loss: 0.6172972917556763 \n",
      "     Training Step: 232 Training Loss: 0.6173092126846313 \n",
      "     Training Step: 233 Training Loss: 0.6144965291023254 \n",
      "     Training Step: 234 Training Loss: 0.6107080578804016 \n",
      "     Training Step: 235 Training Loss: 0.6168392300605774 \n",
      "     Training Step: 236 Training Loss: 0.6115160584449768 \n",
      "     Training Step: 237 Training Loss: 0.616763174533844 \n",
      "     Training Step: 238 Training Loss: 0.6177994608879089 \n",
      "     Training Step: 239 Training Loss: 0.6109435558319092 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6112129092216492 \n",
      "     Validation Step: 1 Validation Loss: 0.6149262189865112 \n",
      "     Validation Step: 2 Validation Loss: 0.6141839027404785 \n",
      "     Validation Step: 3 Validation Loss: 0.6176377534866333 \n",
      "     Validation Step: 4 Validation Loss: 0.6156753897666931 \n",
      "     Validation Step: 5 Validation Loss: 0.6141402125358582 \n",
      "     Validation Step: 6 Validation Loss: 0.6142852902412415 \n",
      "     Validation Step: 7 Validation Loss: 0.610540509223938 \n",
      "     Validation Step: 8 Validation Loss: 0.6163415908813477 \n",
      "     Validation Step: 9 Validation Loss: 0.6185327172279358 \n",
      "     Validation Step: 10 Validation Loss: 0.6136518120765686 \n",
      "     Validation Step: 11 Validation Loss: 0.6106700301170349 \n",
      "     Validation Step: 12 Validation Loss: 0.6161041855812073 \n",
      "     Validation Step: 13 Validation Loss: 0.6112104654312134 \n",
      "     Validation Step: 14 Validation Loss: 0.6184169054031372 \n",
      "     Validation Step: 15 Validation Loss: 0.6128576993942261 \n",
      "     Validation Step: 16 Validation Loss: 0.6142552495002747 \n",
      "     Validation Step: 17 Validation Loss: 0.6151164174079895 \n",
      "     Validation Step: 18 Validation Loss: 0.6152718663215637 \n",
      "     Validation Step: 19 Validation Loss: 0.6153709888458252 \n",
      "     Validation Step: 20 Validation Loss: 0.6181283593177795 \n",
      "     Validation Step: 21 Validation Loss: 0.6146066784858704 \n",
      "     Validation Step: 22 Validation Loss: 0.6145712733268738 \n",
      "     Validation Step: 23 Validation Loss: 0.6137291193008423 \n",
      "     Validation Step: 24 Validation Loss: 0.6130406856536865 \n",
      "     Validation Step: 25 Validation Loss: 0.6119177937507629 \n",
      "     Validation Step: 26 Validation Loss: 0.6170811057090759 \n",
      "     Validation Step: 27 Validation Loss: 0.6075643301010132 \n",
      "     Validation Step: 28 Validation Loss: 0.6116752028465271 \n",
      "     Validation Step: 29 Validation Loss: 0.6156619191169739 \n",
      "     Validation Step: 30 Validation Loss: 0.6183313131332397 \n",
      "     Validation Step: 31 Validation Loss: 0.6173502802848816 \n",
      "     Validation Step: 32 Validation Loss: 0.6146934032440186 \n",
      "     Validation Step: 33 Validation Loss: 0.6116060018539429 \n",
      "     Validation Step: 34 Validation Loss: 0.6137502789497375 \n",
      "     Validation Step: 35 Validation Loss: 0.610188901424408 \n",
      "     Validation Step: 36 Validation Loss: 0.6101619601249695 \n",
      "     Validation Step: 37 Validation Loss: 0.6102010607719421 \n",
      "     Validation Step: 38 Validation Loss: 0.6158733367919922 \n",
      "     Validation Step: 39 Validation Loss: 0.6177638173103333 \n",
      "     Validation Step: 40 Validation Loss: 0.6148920059204102 \n",
      "     Validation Step: 41 Validation Loss: 0.6121652722358704 \n",
      "     Validation Step: 42 Validation Loss: 0.6185575723648071 \n",
      "     Validation Step: 43 Validation Loss: 0.6133325695991516 \n",
      "     Validation Step: 44 Validation Loss: 0.6105692982673645 \n",
      "Epoch: 46\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6132521033287048 \n",
      "     Training Step: 1 Training Loss: 0.610094428062439 \n",
      "     Training Step: 2 Training Loss: 0.6125099658966064 \n",
      "     Training Step: 3 Training Loss: 0.6154136657714844 \n",
      "     Training Step: 4 Training Loss: 0.6178543567657471 \n",
      "     Training Step: 5 Training Loss: 0.6129373908042908 \n",
      "     Training Step: 6 Training Loss: 0.6135693192481995 \n",
      "     Training Step: 7 Training Loss: 0.6197051405906677 \n",
      "     Training Step: 8 Training Loss: 0.6167665123939514 \n",
      "     Training Step: 9 Training Loss: 0.614226222038269 \n",
      "     Training Step: 10 Training Loss: 0.6098893284797668 \n",
      "     Training Step: 11 Training Loss: 0.6125782132148743 \n",
      "     Training Step: 12 Training Loss: 0.61335289478302 \n",
      "     Training Step: 13 Training Loss: 0.6132043600082397 \n",
      "     Training Step: 14 Training Loss: 0.6155803799629211 \n",
      "     Training Step: 15 Training Loss: 0.614093005657196 \n",
      "     Training Step: 16 Training Loss: 0.617270827293396 \n",
      "     Training Step: 17 Training Loss: 0.6132258772850037 \n",
      "     Training Step: 18 Training Loss: 0.6112852692604065 \n",
      "     Training Step: 19 Training Loss: 0.6150056719779968 \n",
      "     Training Step: 20 Training Loss: 0.6123297810554504 \n",
      "     Training Step: 21 Training Loss: 0.6146004796028137 \n",
      "     Training Step: 22 Training Loss: 0.6167855262756348 \n",
      "     Training Step: 23 Training Loss: 0.612167239189148 \n",
      "     Training Step: 24 Training Loss: 0.6128913760185242 \n",
      "     Training Step: 25 Training Loss: 0.6166895031929016 \n",
      "     Training Step: 26 Training Loss: 0.6155561804771423 \n",
      "     Training Step: 27 Training Loss: 0.6168171763420105 \n",
      "     Training Step: 28 Training Loss: 0.6133487224578857 \n",
      "     Training Step: 29 Training Loss: 0.6184358596801758 \n",
      "     Training Step: 30 Training Loss: 0.6167261600494385 \n",
      "     Training Step: 31 Training Loss: 0.6202147603034973 \n",
      "     Training Step: 32 Training Loss: 0.6117095947265625 \n",
      "     Training Step: 33 Training Loss: 0.6134123206138611 \n",
      "     Training Step: 34 Training Loss: 0.6182169318199158 \n",
      "     Training Step: 35 Training Loss: 0.6164888143539429 \n",
      "     Training Step: 36 Training Loss: 0.6107423901557922 \n",
      "     Training Step: 37 Training Loss: 0.6132147908210754 \n",
      "     Training Step: 38 Training Loss: 0.6153098940849304 \n",
      "     Training Step: 39 Training Loss: 0.6103992462158203 \n",
      "     Training Step: 40 Training Loss: 0.6167647242546082 \n",
      "     Training Step: 41 Training Loss: 0.6129751205444336 \n",
      "     Training Step: 42 Training Loss: 0.6157336831092834 \n",
      "     Training Step: 43 Training Loss: 0.610026478767395 \n",
      "     Training Step: 44 Training Loss: 0.6134938597679138 \n",
      "     Training Step: 45 Training Loss: 0.6151039004325867 \n",
      "     Training Step: 46 Training Loss: 0.6144641041755676 \n",
      "     Training Step: 47 Training Loss: 0.6146367192268372 \n",
      "     Training Step: 48 Training Loss: 0.6138327121734619 \n",
      "     Training Step: 49 Training Loss: 0.6157888770103455 \n",
      "     Training Step: 50 Training Loss: 0.6133114099502563 \n",
      "     Training Step: 51 Training Loss: 0.6132296919822693 \n",
      "     Training Step: 52 Training Loss: 0.6144896745681763 \n",
      "     Training Step: 53 Training Loss: 0.6116633415222168 \n",
      "     Training Step: 54 Training Loss: 0.6092499494552612 \n",
      "     Training Step: 55 Training Loss: 0.6173121929168701 \n",
      "     Training Step: 56 Training Loss: 0.615386962890625 \n",
      "     Training Step: 57 Training Loss: 0.6159785985946655 \n",
      "     Training Step: 58 Training Loss: 0.6131864786148071 \n",
      "     Training Step: 59 Training Loss: 0.6188609004020691 \n",
      "     Training Step: 60 Training Loss: 0.618471086025238 \n",
      "     Training Step: 61 Training Loss: 0.6181170344352722 \n",
      "     Training Step: 62 Training Loss: 0.6103522777557373 \n",
      "     Training Step: 63 Training Loss: 0.611872136592865 \n",
      "     Training Step: 64 Training Loss: 0.6154752373695374 \n",
      "     Training Step: 65 Training Loss: 0.6122340559959412 \n",
      "     Training Step: 66 Training Loss: 0.6170661449432373 \n",
      "     Training Step: 67 Training Loss: 0.6172337532043457 \n",
      "     Training Step: 68 Training Loss: 0.6154974102973938 \n",
      "     Training Step: 69 Training Loss: 0.6150248050689697 \n",
      "     Training Step: 70 Training Loss: 0.6183543801307678 \n",
      "     Training Step: 71 Training Loss: 0.6113528609275818 \n",
      "     Training Step: 72 Training Loss: 0.6107290387153625 \n",
      "     Training Step: 73 Training Loss: 0.6152024269104004 \n",
      "     Training Step: 74 Training Loss: 0.6134575009346008 \n",
      "     Training Step: 75 Training Loss: 0.6128014922142029 \n",
      "     Training Step: 76 Training Loss: 0.6119072437286377 \n",
      "     Training Step: 77 Training Loss: 0.6178293228149414 \n",
      "     Training Step: 78 Training Loss: 0.6158392429351807 \n",
      "     Training Step: 79 Training Loss: 0.6117269992828369 \n",
      "     Training Step: 80 Training Loss: 0.6113098859786987 \n",
      "     Training Step: 81 Training Loss: 0.6107311844825745 \n",
      "     Training Step: 82 Training Loss: 0.6115000247955322 \n",
      "     Training Step: 83 Training Loss: 0.6180536150932312 \n",
      "     Training Step: 84 Training Loss: 0.6138034462928772 \n",
      "     Training Step: 85 Training Loss: 0.6116597056388855 \n",
      "     Training Step: 86 Training Loss: 0.613656759262085 \n",
      "     Training Step: 87 Training Loss: 0.6146889925003052 \n",
      "     Training Step: 88 Training Loss: 0.6107459664344788 \n",
      "     Training Step: 89 Training Loss: 0.6141139268875122 \n",
      "     Training Step: 90 Training Loss: 0.6178317070007324 \n",
      "     Training Step: 91 Training Loss: 0.6148185133934021 \n",
      "     Training Step: 92 Training Loss: 0.6124143600463867 \n",
      "     Training Step: 93 Training Loss: 0.6134769916534424 \n",
      "     Training Step: 94 Training Loss: 0.6101545095443726 \n",
      "     Training Step: 95 Training Loss: 0.6144280433654785 \n",
      "     Training Step: 96 Training Loss: 0.6121792793273926 \n",
      "     Training Step: 97 Training Loss: 0.6143078207969666 \n",
      "     Training Step: 98 Training Loss: 0.6161393523216248 \n",
      "     Training Step: 99 Training Loss: 0.6142224669456482 \n",
      "     Training Step: 100 Training Loss: 0.6123937964439392 \n",
      "     Training Step: 101 Training Loss: 0.6178434491157532 \n",
      "     Training Step: 102 Training Loss: 0.6149677634239197 \n",
      "     Training Step: 103 Training Loss: 0.6153966784477234 \n",
      "     Training Step: 104 Training Loss: 0.6140915751457214 \n",
      "     Training Step: 105 Training Loss: 0.6209333539009094 \n",
      "     Training Step: 106 Training Loss: 0.6166730523109436 \n",
      "     Training Step: 107 Training Loss: 0.6185439229011536 \n",
      "     Training Step: 108 Training Loss: 0.6117362380027771 \n",
      "     Training Step: 109 Training Loss: 0.6157874464988708 \n",
      "     Training Step: 110 Training Loss: 0.6115877628326416 \n",
      "     Training Step: 111 Training Loss: 0.6135565638542175 \n",
      "     Training Step: 112 Training Loss: 0.6147821545600891 \n",
      "     Training Step: 113 Training Loss: 0.6164200305938721 \n",
      "     Training Step: 114 Training Loss: 0.6148086190223694 \n",
      "     Training Step: 115 Training Loss: 0.6148471832275391 \n",
      "     Training Step: 116 Training Loss: 0.6194550395011902 \n",
      "     Training Step: 117 Training Loss: 0.6118022799491882 \n",
      "     Training Step: 118 Training Loss: 0.6154081225395203 \n",
      "     Training Step: 119 Training Loss: 0.6125667095184326 \n",
      "     Training Step: 120 Training Loss: 0.6118462085723877 \n",
      "     Training Step: 121 Training Loss: 0.6163102984428406 \n",
      "     Training Step: 122 Training Loss: 0.6128019094467163 \n",
      "     Training Step: 123 Training Loss: 0.611788809299469 \n",
      "     Training Step: 124 Training Loss: 0.6094788908958435 \n",
      "     Training Step: 125 Training Loss: 0.6094153523445129 \n",
      "     Training Step: 126 Training Loss: 0.6177459955215454 \n",
      "     Training Step: 127 Training Loss: 0.6187343001365662 \n",
      "     Training Step: 128 Training Loss: 0.6199601888656616 \n",
      "     Training Step: 129 Training Loss: 0.6161072850227356 \n",
      "     Training Step: 130 Training Loss: 0.6151623129844666 \n",
      "     Training Step: 131 Training Loss: 0.6116892099380493 \n",
      "     Training Step: 132 Training Loss: 0.6106669306755066 \n",
      "     Training Step: 133 Training Loss: 0.6177697777748108 \n",
      "     Training Step: 134 Training Loss: 0.615295946598053 \n",
      "     Training Step: 135 Training Loss: 0.6184202432632446 \n",
      "     Training Step: 136 Training Loss: 0.6128581762313843 \n",
      "     Training Step: 137 Training Loss: 0.6147223114967346 \n",
      "     Training Step: 138 Training Loss: 0.6151943802833557 \n",
      "     Training Step: 139 Training Loss: 0.6137089133262634 \n",
      "     Training Step: 140 Training Loss: 0.6109656095504761 \n",
      "     Training Step: 141 Training Loss: 0.6144337058067322 \n",
      "     Training Step: 142 Training Loss: 0.6122584939002991 \n",
      "     Training Step: 143 Training Loss: 0.6106274127960205 \n",
      "     Training Step: 144 Training Loss: 0.6176817417144775 \n",
      "     Training Step: 145 Training Loss: 0.612069308757782 \n",
      "     Training Step: 146 Training Loss: 0.6114972233772278 \n",
      "     Training Step: 147 Training Loss: 0.6160295605659485 \n",
      "     Training Step: 148 Training Loss: 0.6146669983863831 \n",
      "     Training Step: 149 Training Loss: 0.6180380582809448 \n",
      "     Training Step: 150 Training Loss: 0.6183913946151733 \n",
      "     Training Step: 151 Training Loss: 0.6125187873840332 \n",
      "     Training Step: 152 Training Loss: 0.6118877530097961 \n",
      "     Training Step: 153 Training Loss: 0.6147192716598511 \n",
      "     Training Step: 154 Training Loss: 0.6171661615371704 \n",
      "     Training Step: 155 Training Loss: 0.6106160283088684 \n",
      "     Training Step: 156 Training Loss: 0.6153524518013 \n",
      "     Training Step: 157 Training Loss: 0.6125262975692749 \n",
      "     Training Step: 158 Training Loss: 0.6152834892272949 \n",
      "     Training Step: 159 Training Loss: 0.6114688515663147 \n",
      "     Training Step: 160 Training Loss: 0.6137802004814148 \n",
      "     Training Step: 161 Training Loss: 0.6165171265602112 \n",
      "     Training Step: 162 Training Loss: 0.6139309406280518 \n",
      "     Training Step: 163 Training Loss: 0.6116456389427185 \n",
      "     Training Step: 164 Training Loss: 0.6137800812721252 \n",
      "     Training Step: 165 Training Loss: 0.6174222826957703 \n",
      "     Training Step: 166 Training Loss: 0.6171554327011108 \n",
      "     Training Step: 167 Training Loss: 0.6167623996734619 \n",
      "     Training Step: 168 Training Loss: 0.6120137572288513 \n",
      "     Training Step: 169 Training Loss: 0.6167246103286743 \n",
      "     Training Step: 170 Training Loss: 0.6164295673370361 \n",
      "     Training Step: 171 Training Loss: 0.6107048988342285 \n",
      "     Training Step: 172 Training Loss: 0.6097995638847351 \n",
      "     Training Step: 173 Training Loss: 0.6141179800033569 \n",
      "     Training Step: 174 Training Loss: 0.6182749271392822 \n",
      "     Training Step: 175 Training Loss: 0.6168625354766846 \n",
      "     Training Step: 176 Training Loss: 0.6158307790756226 \n",
      "     Training Step: 177 Training Loss: 0.6155433058738708 \n",
      "     Training Step: 178 Training Loss: 0.6180789470672607 \n",
      "     Training Step: 179 Training Loss: 0.6129835844039917 \n",
      "     Training Step: 180 Training Loss: 0.6129913926124573 \n",
      "     Training Step: 181 Training Loss: 0.6158834099769592 \n",
      "     Training Step: 182 Training Loss: 0.6123954057693481 \n",
      "     Training Step: 183 Training Loss: 0.6097257733345032 \n",
      "     Training Step: 184 Training Loss: 0.6129423975944519 \n",
      "     Training Step: 185 Training Loss: 0.6190103888511658 \n",
      "     Training Step: 186 Training Loss: 0.6144824624061584 \n",
      "     Training Step: 187 Training Loss: 0.6123819947242737 \n",
      "     Training Step: 188 Training Loss: 0.6146566867828369 \n",
      "     Training Step: 189 Training Loss: 0.6124298572540283 \n",
      "     Training Step: 190 Training Loss: 0.6138951182365417 \n",
      "     Training Step: 191 Training Loss: 0.6139655709266663 \n",
      "     Training Step: 192 Training Loss: 0.6158427000045776 \n",
      "     Training Step: 193 Training Loss: 0.6143031120300293 \n",
      "     Training Step: 194 Training Loss: 0.6154879331588745 \n",
      "     Training Step: 195 Training Loss: 0.6156007051467896 \n",
      "     Training Step: 196 Training Loss: 0.6115652918815613 \n",
      "     Training Step: 197 Training Loss: 0.6123178601264954 \n",
      "     Training Step: 198 Training Loss: 0.6166566610336304 \n",
      "     Training Step: 199 Training Loss: 0.6152964234352112 \n",
      "     Training Step: 200 Training Loss: 0.6128235459327698 \n",
      "     Training Step: 201 Training Loss: 0.6141866445541382 \n",
      "     Training Step: 202 Training Loss: 0.6121344566345215 \n",
      "     Training Step: 203 Training Loss: 0.613606870174408 \n",
      "     Training Step: 204 Training Loss: 0.6106411218643188 \n",
      "     Training Step: 205 Training Loss: 0.6101025342941284 \n",
      "     Training Step: 206 Training Loss: 0.6147434711456299 \n",
      "     Training Step: 207 Training Loss: 0.6164482831954956 \n",
      "     Training Step: 208 Training Loss: 0.61419677734375 \n",
      "     Training Step: 209 Training Loss: 0.6155573129653931 \n",
      "     Training Step: 210 Training Loss: 0.6146833896636963 \n",
      "     Training Step: 211 Training Loss: 0.6147524118423462 \n",
      "     Training Step: 212 Training Loss: 0.614368736743927 \n",
      "     Training Step: 213 Training Loss: 0.6162106394767761 \n",
      "     Training Step: 214 Training Loss: 0.6121113300323486 \n",
      "     Training Step: 215 Training Loss: 0.6143659949302673 \n",
      "     Training Step: 216 Training Loss: 0.6134788393974304 \n",
      "     Training Step: 217 Training Loss: 0.6107493042945862 \n",
      "     Training Step: 218 Training Loss: 0.6168272495269775 \n",
      "     Training Step: 219 Training Loss: 0.616985023021698 \n",
      "     Training Step: 220 Training Loss: 0.614396333694458 \n",
      "     Training Step: 221 Training Loss: 0.6155451536178589 \n",
      "     Training Step: 222 Training Loss: 0.6142488121986389 \n",
      "     Training Step: 223 Training Loss: 0.6115394234657288 \n",
      "     Training Step: 224 Training Loss: 0.6146864891052246 \n",
      "     Training Step: 225 Training Loss: 0.6147817373275757 \n",
      "     Training Step: 226 Training Loss: 0.6137852668762207 \n",
      "     Training Step: 227 Training Loss: 0.610929012298584 \n",
      "     Training Step: 228 Training Loss: 0.6104040145874023 \n",
      "     Training Step: 229 Training Loss: 0.6116082668304443 \n",
      "     Training Step: 230 Training Loss: 0.6149924397468567 \n",
      "     Training Step: 231 Training Loss: 0.6169654726982117 \n",
      "     Training Step: 232 Training Loss: 0.613304078578949 \n",
      "     Training Step: 233 Training Loss: 0.6117214560508728 \n",
      "     Training Step: 234 Training Loss: 0.6101165413856506 \n",
      "     Training Step: 235 Training Loss: 0.6083145141601562 \n",
      "     Training Step: 236 Training Loss: 0.6127024292945862 \n",
      "     Training Step: 237 Training Loss: 0.6199602484703064 \n",
      "     Training Step: 238 Training Loss: 0.611869752407074 \n",
      "     Training Step: 239 Training Loss: 0.6130927205085754 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6149444580078125 \n",
      "     Validation Step: 1 Validation Loss: 0.6156261563301086 \n",
      "     Validation Step: 2 Validation Loss: 0.6102587580680847 \n",
      "     Validation Step: 3 Validation Loss: 0.6105749607086182 \n",
      "     Validation Step: 4 Validation Loss: 0.6160355806350708 \n",
      "     Validation Step: 5 Validation Loss: 0.6182924509048462 \n",
      "     Validation Step: 6 Validation Loss: 0.6152644157409668 \n",
      "     Validation Step: 7 Validation Loss: 0.6173251867294312 \n",
      "     Validation Step: 8 Validation Loss: 0.6156565546989441 \n",
      "     Validation Step: 9 Validation Loss: 0.6117005348205566 \n",
      "     Validation Step: 10 Validation Loss: 0.6141922473907471 \n",
      "     Validation Step: 11 Validation Loss: 0.6107205748558044 \n",
      "     Validation Step: 12 Validation Loss: 0.617725670337677 \n",
      "     Validation Step: 13 Validation Loss: 0.61460280418396 \n",
      "     Validation Step: 14 Validation Loss: 0.6143215298652649 \n",
      "     Validation Step: 15 Validation Loss: 0.6136995553970337 \n",
      "     Validation Step: 16 Validation Loss: 0.6116228699684143 \n",
      "     Validation Step: 17 Validation Loss: 0.6128701567649841 \n",
      "     Validation Step: 18 Validation Loss: 0.610264003276825 \n",
      "     Validation Step: 19 Validation Loss: 0.6119381189346313 \n",
      "     Validation Step: 20 Validation Loss: 0.6112366914749146 \n",
      "     Validation Step: 21 Validation Loss: 0.6158270835876465 \n",
      "     Validation Step: 22 Validation Loss: 0.6105612516403198 \n",
      "     Validation Step: 23 Validation Loss: 0.6185346245765686 \n",
      "     Validation Step: 24 Validation Loss: 0.6146931052207947 \n",
      "     Validation Step: 25 Validation Loss: 0.613722026348114 \n",
      "     Validation Step: 26 Validation Loss: 0.613032877445221 \n",
      "     Validation Step: 27 Validation Loss: 0.6176367402076721 \n",
      "     Validation Step: 28 Validation Loss: 0.6121995449066162 \n",
      "     Validation Step: 29 Validation Loss: 0.6148708462715149 \n",
      "     Validation Step: 30 Validation Loss: 0.6162979006767273 \n",
      "     Validation Step: 31 Validation Loss: 0.6136578321456909 \n",
      "     Validation Step: 32 Validation Loss: 0.6145919561386108 \n",
      "     Validation Step: 33 Validation Loss: 0.610231876373291 \n",
      "     Validation Step: 34 Validation Loss: 0.61839359998703 \n",
      "     Validation Step: 35 Validation Loss: 0.618109941482544 \n",
      "     Validation Step: 36 Validation Loss: 0.6153737306594849 \n",
      "     Validation Step: 37 Validation Loss: 0.6112489700317383 \n",
      "     Validation Step: 38 Validation Loss: 0.6142585277557373 \n",
      "     Validation Step: 39 Validation Loss: 0.6150997281074524 \n",
      "     Validation Step: 40 Validation Loss: 0.6170546412467957 \n",
      "     Validation Step: 41 Validation Loss: 0.6185094714164734 \n",
      "     Validation Step: 42 Validation Loss: 0.6141709685325623 \n",
      "     Validation Step: 43 Validation Loss: 0.6076562404632568 \n",
      "     Validation Step: 44 Validation Loss: 0.6133567690849304 \n",
      "Epoch: 47\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6209840774536133 \n",
      "     Training Step: 1 Training Loss: 0.6113097667694092 \n",
      "     Training Step: 2 Training Loss: 0.6178216934204102 \n",
      "     Training Step: 3 Training Loss: 0.6123127937316895 \n",
      "     Training Step: 4 Training Loss: 0.6144702434539795 \n",
      "     Training Step: 5 Training Loss: 0.6183246970176697 \n",
      "     Training Step: 6 Training Loss: 0.6142223477363586 \n",
      "     Training Step: 7 Training Loss: 0.6115811467170715 \n",
      "     Training Step: 8 Training Loss: 0.6104423999786377 \n",
      "     Training Step: 9 Training Loss: 0.6152277588844299 \n",
      "     Training Step: 10 Training Loss: 0.6168737411499023 \n",
      "     Training Step: 11 Training Loss: 0.6114491820335388 \n",
      "     Training Step: 12 Training Loss: 0.6146780252456665 \n",
      "     Training Step: 13 Training Loss: 0.6166868805885315 \n",
      "     Training Step: 14 Training Loss: 0.6153296232223511 \n",
      "     Training Step: 15 Training Loss: 0.6168419718742371 \n",
      "     Training Step: 16 Training Loss: 0.6106733083724976 \n",
      "     Training Step: 17 Training Loss: 0.6127916574478149 \n",
      "     Training Step: 18 Training Loss: 0.6180933713912964 \n",
      "     Training Step: 19 Training Loss: 0.6114212870597839 \n",
      "     Training Step: 20 Training Loss: 0.6122579574584961 \n",
      "     Training Step: 21 Training Loss: 0.6123929619789124 \n",
      "     Training Step: 22 Training Loss: 0.6100603342056274 \n",
      "     Training Step: 23 Training Loss: 0.6155454516410828 \n",
      "     Training Step: 24 Training Loss: 0.6157984733581543 \n",
      "     Training Step: 25 Training Loss: 0.6162582635879517 \n",
      "     Training Step: 26 Training Loss: 0.6154698133468628 \n",
      "     Training Step: 27 Training Loss: 0.6140127778053284 \n",
      "     Training Step: 28 Training Loss: 0.6141029000282288 \n",
      "     Training Step: 29 Training Loss: 0.6151250600814819 \n",
      "     Training Step: 30 Training Loss: 0.6156959533691406 \n",
      "     Training Step: 31 Training Loss: 0.6168438792228699 \n",
      "     Training Step: 32 Training Loss: 0.6159877181053162 \n",
      "     Training Step: 33 Training Loss: 0.6182346940040588 \n",
      "     Training Step: 34 Training Loss: 0.6157671213150024 \n",
      "     Training Step: 35 Training Loss: 0.6142056584358215 \n",
      "     Training Step: 36 Training Loss: 0.6152932047843933 \n",
      "     Training Step: 37 Training Loss: 0.6146291494369507 \n",
      "     Training Step: 38 Training Loss: 0.6136633157730103 \n",
      "     Training Step: 39 Training Loss: 0.6104030609130859 \n",
      "     Training Step: 40 Training Loss: 0.6158332228660583 \n",
      "     Training Step: 41 Training Loss: 0.6094434857368469 \n",
      "     Training Step: 42 Training Loss: 0.6185383796691895 \n",
      "     Training Step: 43 Training Loss: 0.6146062612533569 \n",
      "     Training Step: 44 Training Loss: 0.6119142770767212 \n",
      "     Training Step: 45 Training Loss: 0.6167727112770081 \n",
      "     Training Step: 46 Training Loss: 0.6149604320526123 \n",
      "     Training Step: 47 Training Loss: 0.6147866249084473 \n",
      "     Training Step: 48 Training Loss: 0.6109871864318848 \n",
      "     Training Step: 49 Training Loss: 0.6122372150421143 \n",
      "     Training Step: 50 Training Loss: 0.6115633249282837 \n",
      "     Training Step: 51 Training Loss: 0.6151751279830933 \n",
      "     Training Step: 52 Training Loss: 0.6120864152908325 \n",
      "     Training Step: 53 Training Loss: 0.6146866679191589 \n",
      "     Training Step: 54 Training Loss: 0.6154800653457642 \n",
      "     Training Step: 55 Training Loss: 0.6143963932991028 \n",
      "     Training Step: 56 Training Loss: 0.6093610525131226 \n",
      "     Training Step: 57 Training Loss: 0.6124870777130127 \n",
      "     Training Step: 58 Training Loss: 0.6168724894523621 \n",
      "     Training Step: 59 Training Loss: 0.6116477251052856 \n",
      "     Training Step: 60 Training Loss: 0.6107023358345032 \n",
      "     Training Step: 61 Training Loss: 0.6170171499252319 \n",
      "     Training Step: 62 Training Loss: 0.6181325316429138 \n",
      "     Training Step: 63 Training Loss: 0.6135578155517578 \n",
      "     Training Step: 64 Training Loss: 0.6102123856544495 \n",
      "     Training Step: 65 Training Loss: 0.6125773191452026 \n",
      "     Training Step: 66 Training Loss: 0.6132239103317261 \n",
      "     Training Step: 67 Training Loss: 0.6169133186340332 \n",
      "     Training Step: 68 Training Loss: 0.610053539276123 \n",
      "     Training Step: 69 Training Loss: 0.6101526618003845 \n",
      "     Training Step: 70 Training Loss: 0.615041196346283 \n",
      "     Training Step: 71 Training Loss: 0.6147425770759583 \n",
      "     Training Step: 72 Training Loss: 0.6101105213165283 \n",
      "     Training Step: 73 Training Loss: 0.6139274835586548 \n",
      "     Training Step: 74 Training Loss: 0.6164075136184692 \n",
      "     Training Step: 75 Training Loss: 0.6129403710365295 \n",
      "     Training Step: 76 Training Loss: 0.6121810078620911 \n",
      "     Training Step: 77 Training Loss: 0.6133100986480713 \n",
      "     Training Step: 78 Training Loss: 0.6114813685417175 \n",
      "     Training Step: 79 Training Loss: 0.6179279088973999 \n",
      "     Training Step: 80 Training Loss: 0.6156457662582397 \n",
      "     Training Step: 81 Training Loss: 0.6134938597679138 \n",
      "     Training Step: 82 Training Loss: 0.6129063367843628 \n",
      "     Training Step: 83 Training Loss: 0.6084467172622681 \n",
      "     Training Step: 84 Training Loss: 0.6153079867362976 \n",
      "     Training Step: 85 Training Loss: 0.6171590089797974 \n",
      "     Training Step: 86 Training Loss: 0.6177473664283752 \n",
      "     Training Step: 87 Training Loss: 0.6155473589897156 \n",
      "     Training Step: 88 Training Loss: 0.6180286407470703 \n",
      "     Training Step: 89 Training Loss: 0.6147456765174866 \n",
      "     Training Step: 90 Training Loss: 0.6133882403373718 \n",
      "     Training Step: 91 Training Loss: 0.6124699711799622 \n",
      "     Training Step: 92 Training Loss: 0.6153644323348999 \n",
      "     Training Step: 93 Training Loss: 0.6147512793540955 \n",
      "     Training Step: 94 Training Loss: 0.6187018752098083 \n",
      "     Training Step: 95 Training Loss: 0.6157152652740479 \n",
      "     Training Step: 96 Training Loss: 0.614436149597168 \n",
      "     Training Step: 97 Training Loss: 0.6132457256317139 \n",
      "     Training Step: 98 Training Loss: 0.6154115796089172 \n",
      "     Training Step: 99 Training Loss: 0.6132019758224487 \n",
      "     Training Step: 100 Training Loss: 0.6102197170257568 \n",
      "     Training Step: 101 Training Loss: 0.6158902049064636 \n",
      "     Training Step: 102 Training Loss: 0.6125591993331909 \n",
      "     Training Step: 103 Training Loss: 0.6172948479652405 \n",
      "     Training Step: 104 Training Loss: 0.6114211082458496 \n",
      "     Training Step: 105 Training Loss: 0.6143581867218018 \n",
      "     Training Step: 106 Training Loss: 0.610582709312439 \n",
      "     Training Step: 107 Training Loss: 0.6129499673843384 \n",
      "     Training Step: 108 Training Loss: 0.6147832274436951 \n",
      "     Training Step: 109 Training Loss: 0.6154050230979919 \n",
      "     Training Step: 110 Training Loss: 0.6117802858352661 \n",
      "     Training Step: 111 Training Loss: 0.6135135293006897 \n",
      "     Training Step: 112 Training Loss: 0.6131365299224854 \n",
      "     Training Step: 113 Training Loss: 0.6126910448074341 \n",
      "     Training Step: 114 Training Loss: 0.6130824685096741 \n",
      "     Training Step: 115 Training Loss: 0.6155726909637451 \n",
      "     Training Step: 116 Training Loss: 0.6166775226593018 \n",
      "     Training Step: 117 Training Loss: 0.6166760325431824 \n",
      "     Training Step: 118 Training Loss: 0.6184906959533691 \n",
      "     Training Step: 119 Training Loss: 0.612548291683197 \n",
      "     Training Step: 120 Training Loss: 0.609650731086731 \n",
      "     Training Step: 121 Training Loss: 0.6133691668510437 \n",
      "     Training Step: 122 Training Loss: 0.6125946044921875 \n",
      "     Training Step: 123 Training Loss: 0.6116015315055847 \n",
      "     Training Step: 124 Training Loss: 0.6168365478515625 \n",
      "     Training Step: 125 Training Loss: 0.609744668006897 \n",
      "     Training Step: 126 Training Loss: 0.6188921332359314 \n",
      "     Training Step: 127 Training Loss: 0.6117538809776306 \n",
      "     Training Step: 128 Training Loss: 0.6155899167060852 \n",
      "     Training Step: 129 Training Loss: 0.6189106106758118 \n",
      "     Training Step: 130 Training Loss: 0.6119933724403381 \n",
      "     Training Step: 131 Training Loss: 0.6109968423843384 \n",
      "     Training Step: 132 Training Loss: 0.6131667494773865 \n",
      "     Training Step: 133 Training Loss: 0.6106194853782654 \n",
      "     Training Step: 134 Training Loss: 0.6128706932067871 \n",
      "     Training Step: 135 Training Loss: 0.6141338348388672 \n",
      "     Training Step: 136 Training Loss: 0.6144897937774658 \n",
      "     Training Step: 137 Training Loss: 0.6146751046180725 \n",
      "     Training Step: 138 Training Loss: 0.6098442077636719 \n",
      "     Training Step: 139 Training Loss: 0.6123858094215393 \n",
      "     Training Step: 140 Training Loss: 0.6165174245834351 \n",
      "     Training Step: 141 Training Loss: 0.616772472858429 \n",
      "     Training Step: 142 Training Loss: 0.6184722185134888 \n",
      "     Training Step: 143 Training Loss: 0.6149078011512756 \n",
      "     Training Step: 144 Training Loss: 0.6108447909355164 \n",
      "     Training Step: 145 Training Loss: 0.6151119470596313 \n",
      "     Training Step: 146 Training Loss: 0.6123398542404175 \n",
      "     Training Step: 147 Training Loss: 0.6122586131095886 \n",
      "     Training Step: 148 Training Loss: 0.6128966808319092 \n",
      "     Training Step: 149 Training Loss: 0.6167650818824768 \n",
      "     Training Step: 150 Training Loss: 0.6133410930633545 \n",
      "     Training Step: 151 Training Loss: 0.6160632371902466 \n",
      "     Training Step: 152 Training Loss: 0.6146858930587769 \n",
      "     Training Step: 153 Training Loss: 0.6155185699462891 \n",
      "     Training Step: 154 Training Loss: 0.6115403175354004 \n",
      "     Training Step: 155 Training Loss: 0.6146979928016663 \n",
      "     Training Step: 156 Training Loss: 0.6148167252540588 \n",
      "     Training Step: 157 Training Loss: 0.6137660145759583 \n",
      "     Training Step: 158 Training Loss: 0.6107527017593384 \n",
      "     Training Step: 159 Training Loss: 0.613445520401001 \n",
      "     Training Step: 160 Training Loss: 0.6097347736358643 \n",
      "     Training Step: 161 Training Loss: 0.614669680595398 \n",
      "     Training Step: 162 Training Loss: 0.6125196218490601 \n",
      "     Training Step: 163 Training Loss: 0.6134942770004272 \n",
      "     Training Step: 164 Training Loss: 0.61147141456604 \n",
      "     Training Step: 165 Training Loss: 0.6115291118621826 \n",
      "     Training Step: 166 Training Loss: 0.6117972731590271 \n",
      "     Training Step: 167 Training Loss: 0.6138200163841248 \n",
      "     Training Step: 168 Training Loss: 0.6170046329498291 \n",
      "     Training Step: 169 Training Loss: 0.6120724678039551 \n",
      "     Training Step: 170 Training Loss: 0.6107463240623474 \n",
      "     Training Step: 171 Training Loss: 0.6136782765388489 \n",
      "     Training Step: 172 Training Loss: 0.6119060516357422 \n",
      "     Training Step: 173 Training Loss: 0.616472601890564 \n",
      "     Training Step: 174 Training Loss: 0.6118159294128418 \n",
      "     Training Step: 175 Training Loss: 0.6121636033058167 \n",
      "     Training Step: 176 Training Loss: 0.6195406913757324 \n",
      "     Training Step: 177 Training Loss: 0.6177322864532471 \n",
      "     Training Step: 178 Training Loss: 0.6107503771781921 \n",
      "     Training Step: 179 Training Loss: 0.6106908321380615 \n",
      "     Training Step: 180 Training Loss: 0.6149816513061523 \n",
      "     Training Step: 181 Training Loss: 0.6116209030151367 \n",
      "     Training Step: 182 Training Loss: 0.6185377240180969 \n",
      "     Training Step: 183 Training Loss: 0.6132140755653381 \n",
      "     Training Step: 184 Training Loss: 0.6116204857826233 \n",
      "     Training Step: 185 Training Loss: 0.6161603331565857 \n",
      "     Training Step: 186 Training Loss: 0.6144979000091553 \n",
      "     Training Step: 187 Training Loss: 0.616249680519104 \n",
      "     Training Step: 188 Training Loss: 0.6199381351470947 \n",
      "     Training Step: 189 Training Loss: 0.6177690029144287 \n",
      "     Training Step: 190 Training Loss: 0.612265408039093 \n",
      "     Training Step: 191 Training Loss: 0.6171503663063049 \n",
      "     Training Step: 192 Training Loss: 0.6202258467674255 \n",
      "     Training Step: 193 Training Loss: 0.6162474751472473 \n",
      "     Training Step: 194 Training Loss: 0.6164137125015259 \n",
      "     Training Step: 195 Training Loss: 0.615314245223999 \n",
      "     Training Step: 196 Training Loss: 0.614356517791748 \n",
      "     Training Step: 197 Training Loss: 0.617376446723938 \n",
      "     Training Step: 198 Training Loss: 0.6176073551177979 \n",
      "     Training Step: 199 Training Loss: 0.6133893132209778 \n",
      "     Training Step: 200 Training Loss: 0.6143080592155457 \n",
      "     Training Step: 201 Training Loss: 0.6158049702644348 \n",
      "     Training Step: 202 Training Loss: 0.6146979928016663 \n",
      "     Training Step: 203 Training Loss: 0.6138342022895813 \n",
      "     Training Step: 204 Training Loss: 0.6141065955162048 \n",
      "     Training Step: 205 Training Loss: 0.6137659549713135 \n",
      "     Training Step: 206 Training Loss: 0.6128723621368408 \n",
      "     Training Step: 207 Training Loss: 0.6196931600570679 \n",
      "     Training Step: 208 Training Loss: 0.6188668012619019 \n",
      "     Training Step: 209 Training Loss: 0.6171652674674988 \n",
      "     Training Step: 210 Training Loss: 0.6167952418327332 \n",
      "     Training Step: 211 Training Loss: 0.6136797070503235 \n",
      "     Training Step: 212 Training Loss: 0.6112777590751648 \n",
      "     Training Step: 213 Training Loss: 0.6106032729148865 \n",
      "     Training Step: 214 Training Loss: 0.6140393018722534 \n",
      "     Training Step: 215 Training Loss: 0.6144070625305176 \n",
      "     Training Step: 216 Training Loss: 0.6127252578735352 \n",
      "     Training Step: 217 Training Loss: 0.6161267757415771 \n",
      "     Training Step: 218 Training Loss: 0.6154109835624695 \n",
      "     Training Step: 219 Training Loss: 0.614250659942627 \n",
      "     Training Step: 220 Training Loss: 0.6154665350914001 \n",
      "     Training Step: 221 Training Loss: 0.6180533766746521 \n",
      "     Training Step: 222 Training Loss: 0.614354133605957 \n",
      "     Training Step: 223 Training Loss: 0.6140841245651245 \n",
      "     Training Step: 224 Training Loss: 0.6117742657661438 \n",
      "     Training Step: 225 Training Loss: 0.6130425333976746 \n",
      "     Training Step: 226 Training Loss: 0.6128133535385132 \n",
      "     Training Step: 227 Training Loss: 0.6134101152420044 \n",
      "     Training Step: 228 Training Loss: 0.6117280721664429 \n",
      "     Training Step: 229 Training Loss: 0.61189204454422 \n",
      "     Training Step: 230 Training Loss: 0.617313027381897 \n",
      "     Training Step: 231 Training Loss: 0.6112397313117981 \n",
      "     Training Step: 232 Training Loss: 0.6144645810127258 \n",
      "     Training Step: 233 Training Loss: 0.6136685013771057 \n",
      "     Training Step: 234 Training Loss: 0.6142306923866272 \n",
      "     Training Step: 235 Training Loss: 0.6145489811897278 \n",
      "     Training Step: 236 Training Loss: 0.6197445392608643 \n",
      "     Training Step: 237 Training Loss: 0.6177023649215698 \n",
      "     Training Step: 238 Training Loss: 0.6175061464309692 \n",
      "     Training Step: 239 Training Loss: 0.6152515411376953 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.613426148891449 \n",
      "     Validation Step: 1 Validation Loss: 0.618229329586029 \n",
      "     Validation Step: 2 Validation Loss: 0.6170328259468079 \n",
      "     Validation Step: 3 Validation Loss: 0.6113554239273071 \n",
      "     Validation Step: 4 Validation Loss: 0.6143017411231995 \n",
      "     Validation Step: 5 Validation Loss: 0.6129826903343201 \n",
      "     Validation Step: 6 Validation Loss: 0.6151130199432373 \n",
      "     Validation Step: 7 Validation Loss: 0.6137565970420837 \n",
      "     Validation Step: 8 Validation Loss: 0.6146351099014282 \n",
      "     Validation Step: 9 Validation Loss: 0.6147279143333435 \n",
      "     Validation Step: 10 Validation Loss: 0.6158425807952881 \n",
      "     Validation Step: 11 Validation Loss: 0.610382080078125 \n",
      "     Validation Step: 12 Validation Loss: 0.6123074293136597 \n",
      "     Validation Step: 13 Validation Loss: 0.6137763261795044 \n",
      "     Validation Step: 14 Validation Loss: 0.6183450818061829 \n",
      "     Validation Step: 15 Validation Loss: 0.6113590002059937 \n",
      "     Validation Step: 16 Validation Loss: 0.6149948239326477 \n",
      "     Validation Step: 17 Validation Loss: 0.6118231415748596 \n",
      "     Validation Step: 18 Validation Loss: 0.6152971386909485 \n",
      "     Validation Step: 19 Validation Loss: 0.6117403507232666 \n",
      "     Validation Step: 20 Validation Loss: 0.6184679865837097 \n",
      "     Validation Step: 21 Validation Loss: 0.6137422323226929 \n",
      "     Validation Step: 22 Validation Loss: 0.6107059717178345 \n",
      "     Validation Step: 23 Validation Loss: 0.6176236867904663 \n",
      "     Validation Step: 24 Validation Loss: 0.6142522096633911 \n",
      "     Validation Step: 25 Validation Loss: 0.610701858997345 \n",
      "     Validation Step: 26 Validation Loss: 0.6156231164932251 \n",
      "     Validation Step: 27 Validation Loss: 0.6146511435508728 \n",
      "     Validation Step: 28 Validation Loss: 0.6104145050048828 \n",
      "     Validation Step: 29 Validation Loss: 0.6131216883659363 \n",
      "     Validation Step: 30 Validation Loss: 0.6104115843772888 \n",
      "     Validation Step: 31 Validation Loss: 0.6108609437942505 \n",
      "     Validation Step: 32 Validation Loss: 0.6160297393798828 \n",
      "     Validation Step: 33 Validation Loss: 0.6162930727005005 \n",
      "     Validation Step: 34 Validation Loss: 0.615382969379425 \n",
      "     Validation Step: 35 Validation Loss: 0.6156688332557678 \n",
      "     Validation Step: 36 Validation Loss: 0.6173132061958313 \n",
      "     Validation Step: 37 Validation Loss: 0.6143797636032104 \n",
      "     Validation Step: 38 Validation Loss: 0.6184806823730469 \n",
      "     Validation Step: 39 Validation Loss: 0.6180657744407654 \n",
      "     Validation Step: 40 Validation Loss: 0.6120660901069641 \n",
      "     Validation Step: 41 Validation Loss: 0.6177055239677429 \n",
      "     Validation Step: 42 Validation Loss: 0.6142529845237732 \n",
      "     Validation Step: 43 Validation Loss: 0.61492919921875 \n",
      "     Validation Step: 44 Validation Loss: 0.6078716516494751 \n",
      "Epoch: 48\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.615650475025177 \n",
      "     Training Step: 1 Training Loss: 0.6161426305770874 \n",
      "     Training Step: 2 Training Loss: 0.6133201718330383 \n",
      "     Training Step: 3 Training Loss: 0.609440267086029 \n",
      "     Training Step: 4 Training Loss: 0.6114814281463623 \n",
      "     Training Step: 5 Training Loss: 0.6152870655059814 \n",
      "     Training Step: 6 Training Loss: 0.6124348044395447 \n",
      "     Training Step: 7 Training Loss: 0.616439700126648 \n",
      "     Training Step: 8 Training Loss: 0.6114704608917236 \n",
      "     Training Step: 9 Training Loss: 0.6093688607215881 \n",
      "     Training Step: 10 Training Loss: 0.6168034076690674 \n",
      "     Training Step: 11 Training Loss: 0.6109087467193604 \n",
      "     Training Step: 12 Training Loss: 0.6178067326545715 \n",
      "     Training Step: 13 Training Loss: 0.6154107451438904 \n",
      "     Training Step: 14 Training Loss: 0.6155310869216919 \n",
      "     Training Step: 15 Training Loss: 0.6120818257331848 \n",
      "     Training Step: 16 Training Loss: 0.6154218912124634 \n",
      "     Training Step: 17 Training Loss: 0.6144887804985046 \n",
      "     Training Step: 18 Training Loss: 0.618430495262146 \n",
      "     Training Step: 19 Training Loss: 0.6181147694587708 \n",
      "     Training Step: 20 Training Loss: 0.6149111986160278 \n",
      "     Training Step: 21 Training Loss: 0.6157816648483276 \n",
      "     Training Step: 22 Training Loss: 0.6153029203414917 \n",
      "     Training Step: 23 Training Loss: 0.6122748851776123 \n",
      "     Training Step: 24 Training Loss: 0.6102494597434998 \n",
      "     Training Step: 25 Training Loss: 0.6119318604469299 \n",
      "     Training Step: 26 Training Loss: 0.613499641418457 \n",
      "     Training Step: 27 Training Loss: 0.6115437746047974 \n",
      "     Training Step: 28 Training Loss: 0.6116965413093567 \n",
      "     Training Step: 29 Training Loss: 0.6163695454597473 \n",
      "     Training Step: 30 Training Loss: 0.6111703515052795 \n",
      "     Training Step: 31 Training Loss: 0.6168531179428101 \n",
      "     Training Step: 32 Training Loss: 0.6177610158920288 \n",
      "     Training Step: 33 Training Loss: 0.6098212003707886 \n",
      "     Training Step: 34 Training Loss: 0.6166855692863464 \n",
      "     Training Step: 35 Training Loss: 0.6116454005241394 \n",
      "     Training Step: 36 Training Loss: 0.6167986989021301 \n",
      "     Training Step: 37 Training Loss: 0.612400233745575 \n",
      "     Training Step: 38 Training Loss: 0.6169260144233704 \n",
      "     Training Step: 39 Training Loss: 0.6188752055168152 \n",
      "     Training Step: 40 Training Loss: 0.6101256012916565 \n",
      "     Training Step: 41 Training Loss: 0.6132750511169434 \n",
      "     Training Step: 42 Training Loss: 0.6155630946159363 \n",
      "     Training Step: 43 Training Loss: 0.6121776103973389 \n",
      "     Training Step: 44 Training Loss: 0.6185349822044373 \n",
      "     Training Step: 45 Training Loss: 0.6122491359710693 \n",
      "     Training Step: 46 Training Loss: 0.6119239330291748 \n",
      "     Training Step: 47 Training Loss: 0.6180652976036072 \n",
      "     Training Step: 48 Training Loss: 0.6118870973587036 \n",
      "     Training Step: 49 Training Loss: 0.6140543222427368 \n",
      "     Training Step: 50 Training Loss: 0.6139388084411621 \n",
      "     Training Step: 51 Training Loss: 0.6147252917289734 \n",
      "     Training Step: 52 Training Loss: 0.6157631278038025 \n",
      "     Training Step: 53 Training Loss: 0.6145211458206177 \n",
      "     Training Step: 54 Training Loss: 0.6141740679740906 \n",
      "     Training Step: 55 Training Loss: 0.6121359467506409 \n",
      "     Training Step: 56 Training Loss: 0.6122428178787231 \n",
      "     Training Step: 57 Training Loss: 0.6132844686508179 \n",
      "     Training Step: 58 Training Loss: 0.609463632106781 \n",
      "     Training Step: 59 Training Loss: 0.6160569787025452 \n",
      "     Training Step: 60 Training Loss: 0.6166893243789673 \n",
      "     Training Step: 61 Training Loss: 0.6134622097015381 \n",
      "     Training Step: 62 Training Loss: 0.6101693511009216 \n",
      "     Training Step: 63 Training Loss: 0.6097748875617981 \n",
      "     Training Step: 64 Training Loss: 0.6174480319023132 \n",
      "     Training Step: 65 Training Loss: 0.6116141676902771 \n",
      "     Training Step: 66 Training Loss: 0.6135106086730957 \n",
      "     Training Step: 67 Training Loss: 0.611538827419281 \n",
      "     Training Step: 68 Training Loss: 0.6103672981262207 \n",
      "     Training Step: 69 Training Loss: 0.6128873229026794 \n",
      "     Training Step: 70 Training Loss: 0.6118238568305969 \n",
      "     Training Step: 71 Training Loss: 0.6156004071235657 \n",
      "     Training Step: 72 Training Loss: 0.6114352345466614 \n",
      "     Training Step: 73 Training Loss: 0.61248379945755 \n",
      "     Training Step: 74 Training Loss: 0.6130689978599548 \n",
      "     Training Step: 75 Training Loss: 0.6134514808654785 \n",
      "     Training Step: 76 Training Loss: 0.6105757355690002 \n",
      "     Training Step: 77 Training Loss: 0.6147437691688538 \n",
      "     Training Step: 78 Training Loss: 0.6150050759315491 \n",
      "     Training Step: 79 Training Loss: 0.6146354079246521 \n",
      "     Training Step: 80 Training Loss: 0.6123867630958557 \n",
      "     Training Step: 81 Training Loss: 0.6131604313850403 \n",
      "     Training Step: 82 Training Loss: 0.6114729642868042 \n",
      "     Training Step: 83 Training Loss: 0.6162431240081787 \n",
      "     Training Step: 84 Training Loss: 0.6178219318389893 \n",
      "     Training Step: 85 Training Loss: 0.6144304275512695 \n",
      "     Training Step: 86 Training Loss: 0.614262044429779 \n",
      "     Training Step: 87 Training Loss: 0.6116734147071838 \n",
      "     Training Step: 88 Training Loss: 0.6156956553459167 \n",
      "     Training Step: 89 Training Loss: 0.6121727228164673 \n",
      "     Training Step: 90 Training Loss: 0.6114811897277832 \n",
      "     Training Step: 91 Training Loss: 0.6160111427307129 \n",
      "     Training Step: 92 Training Loss: 0.6160916686058044 \n",
      "     Training Step: 93 Training Loss: 0.6136337518692017 \n",
      "     Training Step: 94 Training Loss: 0.6138551235198975 \n",
      "     Training Step: 95 Training Loss: 0.6122004985809326 \n",
      "     Training Step: 96 Training Loss: 0.615409255027771 \n",
      "     Training Step: 97 Training Loss: 0.61058109998703 \n",
      "     Training Step: 98 Training Loss: 0.6146617531776428 \n",
      "     Training Step: 99 Training Loss: 0.6129778623580933 \n",
      "     Training Step: 100 Training Loss: 0.618178129196167 \n",
      "     Training Step: 101 Training Loss: 0.6182578802108765 \n",
      "     Training Step: 102 Training Loss: 0.613798201084137 \n",
      "     Training Step: 103 Training Loss: 0.6128411889076233 \n",
      "     Training Step: 104 Training Loss: 0.6149502992630005 \n",
      "     Training Step: 105 Training Loss: 0.6168095469474792 \n",
      "     Training Step: 106 Training Loss: 0.6177729368209839 \n",
      "     Training Step: 107 Training Loss: 0.6168140172958374 \n",
      "     Training Step: 108 Training Loss: 0.619623064994812 \n",
      "     Training Step: 109 Training Loss: 0.6171481013298035 \n",
      "     Training Step: 110 Training Loss: 0.6113272309303284 \n",
      "     Training Step: 111 Training Loss: 0.6153191328048706 \n",
      "     Training Step: 112 Training Loss: 0.6107293963432312 \n",
      "     Training Step: 113 Training Loss: 0.6127735376358032 \n",
      "     Training Step: 114 Training Loss: 0.6147608757019043 \n",
      "     Training Step: 115 Training Loss: 0.6147021055221558 \n",
      "     Training Step: 116 Training Loss: 0.6144589185714722 \n",
      "     Training Step: 117 Training Loss: 0.6131243109703064 \n",
      "     Training Step: 118 Training Loss: 0.6165256500244141 \n",
      "     Training Step: 119 Training Loss: 0.6194774508476257 \n",
      "     Training Step: 120 Training Loss: 0.6189113259315491 \n",
      "     Training Step: 121 Training Loss: 0.6105982661247253 \n",
      "     Training Step: 122 Training Loss: 0.6136248111724854 \n",
      "     Training Step: 123 Training Loss: 0.6155372858047485 \n",
      "     Training Step: 124 Training Loss: 0.6156577467918396 \n",
      "     Training Step: 125 Training Loss: 0.6135856509208679 \n",
      "     Training Step: 126 Training Loss: 0.610698401927948 \n",
      "     Training Step: 127 Training Loss: 0.6116245985031128 \n",
      "     Training Step: 128 Training Loss: 0.614405632019043 \n",
      "     Training Step: 129 Training Loss: 0.6141868829727173 \n",
      "     Training Step: 130 Training Loss: 0.6146916747093201 \n",
      "     Training Step: 131 Training Loss: 0.6162461042404175 \n",
      "     Training Step: 132 Training Loss: 0.6166698336601257 \n",
      "     Training Step: 133 Training Loss: 0.6118836998939514 \n",
      "     Training Step: 134 Training Loss: 0.6107601523399353 \n",
      "     Training Step: 135 Training Loss: 0.6154316067695618 \n",
      "     Training Step: 136 Training Loss: 0.6149889826774597 \n",
      "     Training Step: 137 Training Loss: 0.612558901309967 \n",
      "     Training Step: 138 Training Loss: 0.6158763766288757 \n",
      "     Training Step: 139 Training Loss: 0.617834746837616 \n",
      "     Training Step: 140 Training Loss: 0.6133376955986023 \n",
      "     Training Step: 141 Training Loss: 0.6184468269348145 \n",
      "     Training Step: 142 Training Loss: 0.6142706274986267 \n",
      "     Training Step: 143 Training Loss: 0.6127005219459534 \n",
      "     Training Step: 144 Training Loss: 0.6176223754882812 \n",
      "     Training Step: 145 Training Loss: 0.6141313910484314 \n",
      "     Training Step: 146 Training Loss: 0.614289402961731 \n",
      "     Training Step: 147 Training Loss: 0.6153836846351624 \n",
      "     Training Step: 148 Training Loss: 0.6140424013137817 \n",
      "     Training Step: 149 Training Loss: 0.6105558276176453 \n",
      "     Training Step: 150 Training Loss: 0.6200414299964905 \n",
      "     Training Step: 151 Training Loss: 0.6125743985176086 \n",
      "     Training Step: 152 Training Loss: 0.6132264137268066 \n",
      "     Training Step: 153 Training Loss: 0.6107592582702637 \n",
      "     Training Step: 154 Training Loss: 0.6158304214477539 \n",
      "     Training Step: 155 Training Loss: 0.6116737127304077 \n",
      "     Training Step: 156 Training Loss: 0.6139450073242188 \n",
      "     Training Step: 157 Training Loss: 0.6175395250320435 \n",
      "     Training Step: 158 Training Loss: 0.6100945472717285 \n",
      "     Training Step: 159 Training Loss: 0.614682674407959 \n",
      "     Training Step: 160 Training Loss: 0.6100879311561584 \n",
      "     Training Step: 161 Training Loss: 0.6147458553314209 \n",
      "     Training Step: 162 Training Loss: 0.6134390234947205 \n",
      "     Training Step: 163 Training Loss: 0.6117727756500244 \n",
      "     Training Step: 164 Training Loss: 0.6172414422035217 \n",
      "     Training Step: 165 Training Loss: 0.6202989220619202 \n",
      "     Training Step: 166 Training Loss: 0.6147197484970093 \n",
      "     Training Step: 167 Training Loss: 0.6196883320808411 \n",
      "     Training Step: 168 Training Loss: 0.6149411201477051 \n",
      "     Training Step: 169 Training Loss: 0.6145548820495605 \n",
      "     Training Step: 170 Training Loss: 0.6137431263923645 \n",
      "     Training Step: 171 Training Loss: 0.6169742345809937 \n",
      "     Training Step: 172 Training Loss: 0.6132532358169556 \n",
      "     Training Step: 173 Training Loss: 0.612959086894989 \n",
      "     Training Step: 174 Training Loss: 0.6125659942626953 \n",
      "     Training Step: 175 Training Loss: 0.6123287081718445 \n",
      "     Training Step: 176 Training Loss: 0.61810302734375 \n",
      "     Training Step: 177 Training Loss: 0.6167159676551819 \n",
      "     Training Step: 178 Training Loss: 0.6164680123329163 \n",
      "     Training Step: 179 Training Loss: 0.6209366321563721 \n",
      "     Training Step: 180 Training Loss: 0.6129130125045776 \n",
      "     Training Step: 181 Training Loss: 0.6185675859451294 \n",
      "     Training Step: 182 Training Loss: 0.6183215975761414 \n",
      "     Training Step: 183 Training Loss: 0.6134504675865173 \n",
      "     Training Step: 184 Training Loss: 0.6143872737884521 \n",
      "     Training Step: 185 Training Loss: 0.6108241081237793 \n",
      "     Training Step: 186 Training Loss: 0.6148115992546082 \n",
      "     Training Step: 187 Training Loss: 0.6133219003677368 \n",
      "     Training Step: 188 Training Loss: 0.6158011555671692 \n",
      "     Training Step: 189 Training Loss: 0.6151403188705444 \n",
      "     Training Step: 190 Training Loss: 0.615320086479187 \n",
      "     Training Step: 191 Training Loss: 0.6125392913818359 \n",
      "     Training Step: 192 Training Loss: 0.6139111518859863 \n",
      "     Training Step: 193 Training Loss: 0.6166760921478271 \n",
      "     Training Step: 194 Training Loss: 0.6136655211448669 \n",
      "     Training Step: 195 Training Loss: 0.6167511940002441 \n",
      "     Training Step: 196 Training Loss: 0.6118228435516357 \n",
      "     Training Step: 197 Training Loss: 0.6152887940406799 \n",
      "     Training Step: 198 Training Loss: 0.6177160739898682 \n",
      "     Training Step: 199 Training Loss: 0.6140876412391663 \n",
      "     Training Step: 200 Training Loss: 0.6129542589187622 \n",
      "     Training Step: 201 Training Loss: 0.6144173741340637 \n",
      "     Training Step: 202 Training Loss: 0.6097624897956848 \n",
      "     Training Step: 203 Training Loss: 0.6148348450660706 \n",
      "     Training Step: 204 Training Loss: 0.6147127151489258 \n",
      "     Training Step: 205 Training Loss: 0.6104899048805237 \n",
      "     Training Step: 206 Training Loss: 0.6168535351753235 \n",
      "     Training Step: 207 Training Loss: 0.6150871515274048 \n",
      "     Training Step: 208 Training Loss: 0.6186195611953735 \n",
      "     Training Step: 209 Training Loss: 0.614372968673706 \n",
      "     Training Step: 210 Training Loss: 0.6148349046707153 \n",
      "     Training Step: 211 Training Loss: 0.61455237865448 \n",
      "     Training Step: 212 Training Loss: 0.6115921139717102 \n",
      "     Training Step: 213 Training Loss: 0.6123398542404175 \n",
      "     Training Step: 214 Training Loss: 0.616564929485321 \n",
      "     Training Step: 215 Training Loss: 0.6131861209869385 \n",
      "     Training Step: 216 Training Loss: 0.6114309430122375 \n",
      "     Training Step: 217 Training Loss: 0.617301344871521 \n",
      "     Training Step: 218 Training Loss: 0.6128091812133789 \n",
      "     Training Step: 219 Training Loss: 0.6117348670959473 \n",
      "     Training Step: 220 Training Loss: 0.6109856367111206 \n",
      "     Training Step: 221 Training Loss: 0.61517733335495 \n",
      "     Training Step: 222 Training Loss: 0.6157504320144653 \n",
      "     Training Step: 223 Training Loss: 0.6183120012283325 \n",
      "     Training Step: 224 Training Loss: 0.6137784123420715 \n",
      "     Training Step: 225 Training Loss: 0.612127423286438 \n",
      "     Training Step: 226 Training Loss: 0.6167212724685669 \n",
      "     Training Step: 227 Training Loss: 0.6141855120658875 \n",
      "     Training Step: 228 Training Loss: 0.613768994808197 \n",
      "     Training Step: 229 Training Loss: 0.6153770685195923 \n",
      "     Training Step: 230 Training Loss: 0.6112383008003235 \n",
      "     Training Step: 231 Training Loss: 0.6154471635818481 \n",
      "     Training Step: 232 Training Loss: 0.6083197593688965 \n",
      "     Training Step: 233 Training Loss: 0.6128610968589783 \n",
      "     Training Step: 234 Training Loss: 0.6172406673431396 \n",
      "     Training Step: 235 Training Loss: 0.6152026653289795 \n",
      "     Training Step: 236 Training Loss: 0.6106048226356506 \n",
      "     Training Step: 237 Training Loss: 0.6128897070884705 \n",
      "     Training Step: 238 Training Loss: 0.617117702960968 \n",
      "     Training Step: 239 Training Loss: 0.6100931167602539 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6105252504348755 \n",
      "     Validation Step: 1 Validation Loss: 0.6141443848609924 \n",
      "     Validation Step: 2 Validation Loss: 0.6150680780410767 \n",
      "     Validation Step: 3 Validation Loss: 0.6160474419593811 \n",
      "     Validation Step: 4 Validation Loss: 0.6130017042160034 \n",
      "     Validation Step: 5 Validation Loss: 0.6170621514320374 \n",
      "     Validation Step: 6 Validation Loss: 0.6162832379341125 \n",
      "     Validation Step: 7 Validation Loss: 0.611661970615387 \n",
      "     Validation Step: 8 Validation Loss: 0.6133213043212891 \n",
      "     Validation Step: 9 Validation Loss: 0.6111749410629272 \n",
      "     Validation Step: 10 Validation Loss: 0.6146050095558167 \n",
      "     Validation Step: 11 Validation Loss: 0.6136682033538818 \n",
      "     Validation Step: 12 Validation Loss: 0.6156129837036133 \n",
      "     Validation Step: 13 Validation Loss: 0.6185370087623596 \n",
      "     Validation Step: 14 Validation Loss: 0.615345299243927 \n",
      "     Validation Step: 15 Validation Loss: 0.6115641593933105 \n",
      "     Validation Step: 16 Validation Loss: 0.6181144714355469 \n",
      "     Validation Step: 17 Validation Loss: 0.6176460981369019 \n",
      "     Validation Step: 18 Validation Loss: 0.6101440787315369 \n",
      "     Validation Step: 19 Validation Loss: 0.6106576919555664 \n",
      "     Validation Step: 20 Validation Loss: 0.6128477454185486 \n",
      "     Validation Step: 21 Validation Loss: 0.6121489405632019 \n",
      "     Validation Step: 22 Validation Loss: 0.6156476140022278 \n",
      "     Validation Step: 23 Validation Loss: 0.6136708855628967 \n",
      "     Validation Step: 24 Validation Loss: 0.6185552477836609 \n",
      "     Validation Step: 25 Validation Loss: 0.6177521347999573 \n",
      "     Validation Step: 26 Validation Loss: 0.6136941909790039 \n",
      "     Validation Step: 27 Validation Loss: 0.6104886531829834 \n",
      "     Validation Step: 28 Validation Loss: 0.6101832389831543 \n",
      "     Validation Step: 29 Validation Loss: 0.611907958984375 \n",
      "     Validation Step: 30 Validation Loss: 0.6101149916648865 \n",
      "     Validation Step: 31 Validation Loss: 0.6142219305038452 \n",
      "     Validation Step: 32 Validation Loss: 0.6183953881263733 \n",
      "     Validation Step: 33 Validation Loss: 0.6182963252067566 \n",
      "     Validation Step: 34 Validation Loss: 0.6111948490142822 \n",
      "     Validation Step: 35 Validation Loss: 0.6075419187545776 \n",
      "     Validation Step: 36 Validation Loss: 0.6141495108604431 \n",
      "     Validation Step: 37 Validation Loss: 0.6145548224449158 \n",
      "     Validation Step: 38 Validation Loss: 0.6173526644706726 \n",
      "     Validation Step: 39 Validation Loss: 0.6142773628234863 \n",
      "     Validation Step: 40 Validation Loss: 0.6158583164215088 \n",
      "     Validation Step: 41 Validation Loss: 0.6149207949638367 \n",
      "     Validation Step: 42 Validation Loss: 0.6152553558349609 \n",
      "     Validation Step: 43 Validation Loss: 0.6146672368049622 \n",
      "     Validation Step: 44 Validation Loss: 0.6148676872253418 \n",
      "Epoch: 49\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6083077192306519 \n",
      "     Training Step: 1 Training Loss: 0.6198272705078125 \n",
      "     Training Step: 2 Training Loss: 0.61469566822052 \n",
      "     Training Step: 3 Training Loss: 0.6123772859573364 \n",
      "     Training Step: 4 Training Loss: 0.6121683120727539 \n",
      "     Training Step: 5 Training Loss: 0.6160264611244202 \n",
      "     Training Step: 6 Training Loss: 0.6174319386482239 \n",
      "     Training Step: 7 Training Loss: 0.6142479181289673 \n",
      "     Training Step: 8 Training Loss: 0.6150734424591064 \n",
      "     Training Step: 9 Training Loss: 0.6146966218948364 \n",
      "     Training Step: 10 Training Loss: 0.6117008924484253 \n",
      "     Training Step: 11 Training Loss: 0.6183156371116638 \n",
      "     Training Step: 12 Training Loss: 0.6171769499778748 \n",
      "     Training Step: 13 Training Loss: 0.6125861406326294 \n",
      "     Training Step: 14 Training Loss: 0.6133512854576111 \n",
      "     Training Step: 15 Training Loss: 0.6148300766944885 \n",
      "     Training Step: 16 Training Loss: 0.609792947769165 \n",
      "     Training Step: 17 Training Loss: 0.6154237985610962 \n",
      "     Training Step: 18 Training Loss: 0.6142103672027588 \n",
      "     Training Step: 19 Training Loss: 0.615501344203949 \n",
      "     Training Step: 20 Training Loss: 0.6154130697250366 \n",
      "     Training Step: 21 Training Loss: 0.6186419725418091 \n",
      "     Training Step: 22 Training Loss: 0.6095949411392212 \n",
      "     Training Step: 23 Training Loss: 0.613934338092804 \n",
      "     Training Step: 24 Training Loss: 0.61163729429245 \n",
      "     Training Step: 25 Training Loss: 0.6172322630882263 \n",
      "     Training Step: 26 Training Loss: 0.61405348777771 \n",
      "     Training Step: 27 Training Loss: 0.6151279211044312 \n",
      "     Training Step: 28 Training Loss: 0.6142612099647522 \n",
      "     Training Step: 29 Training Loss: 0.6144538521766663 \n",
      "     Training Step: 30 Training Loss: 0.610492467880249 \n",
      "     Training Step: 31 Training Loss: 0.612682580947876 \n",
      "     Training Step: 32 Training Loss: 0.6163204908370972 \n",
      "     Training Step: 33 Training Loss: 0.616854190826416 \n",
      "     Training Step: 34 Training Loss: 0.6141144037246704 \n",
      "     Training Step: 35 Training Loss: 0.6162332892417908 \n",
      "     Training Step: 36 Training Loss: 0.6146134734153748 \n",
      "     Training Step: 37 Training Loss: 0.6144774556159973 \n",
      "     Training Step: 38 Training Loss: 0.6180088520050049 \n",
      "     Training Step: 39 Training Loss: 0.6176074147224426 \n",
      "     Training Step: 40 Training Loss: 0.6151238679885864 \n",
      "     Training Step: 41 Training Loss: 0.6149113178253174 \n",
      "     Training Step: 42 Training Loss: 0.6128199100494385 \n",
      "     Training Step: 43 Training Loss: 0.6114370822906494 \n",
      "     Training Step: 44 Training Loss: 0.6132569313049316 \n",
      "     Training Step: 45 Training Loss: 0.6116378307342529 \n",
      "     Training Step: 46 Training Loss: 0.616883397102356 \n",
      "     Training Step: 47 Training Loss: 0.6138241291046143 \n",
      "     Training Step: 48 Training Loss: 0.6116828918457031 \n",
      "     Training Step: 49 Training Loss: 0.6153565645217896 \n",
      "     Training Step: 50 Training Loss: 0.6183868050575256 \n",
      "     Training Step: 51 Training Loss: 0.613986074924469 \n",
      "     Training Step: 52 Training Loss: 0.6133719086647034 \n",
      "     Training Step: 53 Training Loss: 0.615551769733429 \n",
      "     Training Step: 54 Training Loss: 0.6129860877990723 \n",
      "     Training Step: 55 Training Loss: 0.615836501121521 \n",
      "     Training Step: 56 Training Loss: 0.6178464889526367 \n",
      "     Training Step: 57 Training Loss: 0.6199371814727783 \n",
      "     Training Step: 58 Training Loss: 0.6115407347679138 \n",
      "     Training Step: 59 Training Loss: 0.6167079210281372 \n",
      "     Training Step: 60 Training Loss: 0.6177449822425842 \n",
      "     Training Step: 61 Training Loss: 0.6125261783599854 \n",
      "     Training Step: 62 Training Loss: 0.614959180355072 \n",
      "     Training Step: 63 Training Loss: 0.6100865006446838 \n",
      "     Training Step: 64 Training Loss: 0.6147521734237671 \n",
      "     Training Step: 65 Training Loss: 0.6105946898460388 \n",
      "     Training Step: 66 Training Loss: 0.6153683066368103 \n",
      "     Training Step: 67 Training Loss: 0.6144503951072693 \n",
      "     Training Step: 68 Training Loss: 0.6122739911079407 \n",
      "     Training Step: 69 Training Loss: 0.6168020963668823 \n",
      "     Training Step: 70 Training Loss: 0.6147015690803528 \n",
      "     Training Step: 71 Training Loss: 0.616741418838501 \n",
      "     Training Step: 72 Training Loss: 0.6158609390258789 \n",
      "     Training Step: 73 Training Loss: 0.6146804690361023 \n",
      "     Training Step: 74 Training Loss: 0.6138498783111572 \n",
      "     Training Step: 75 Training Loss: 0.6122631430625916 \n",
      "     Training Step: 76 Training Loss: 0.6125536561012268 \n",
      "     Training Step: 77 Training Loss: 0.6161715388298035 \n",
      "     Training Step: 78 Training Loss: 0.6168259382247925 \n",
      "     Training Step: 79 Training Loss: 0.6136874556541443 \n",
      "     Training Step: 80 Training Loss: 0.6181163787841797 \n",
      "     Training Step: 81 Training Loss: 0.6125766038894653 \n",
      "     Training Step: 82 Training Loss: 0.6119393706321716 \n",
      "     Training Step: 83 Training Loss: 0.6134507060050964 \n",
      "     Training Step: 84 Training Loss: 0.6152077913284302 \n",
      "     Training Step: 85 Training Loss: 0.6118007302284241 \n",
      "     Training Step: 86 Training Loss: 0.6179094314575195 \n",
      "     Training Step: 87 Training Loss: 0.6146836280822754 \n",
      "     Training Step: 88 Training Loss: 0.6143785715103149 \n",
      "     Training Step: 89 Training Loss: 0.6141204833984375 \n",
      "     Training Step: 90 Training Loss: 0.6138078570365906 \n",
      "     Training Step: 91 Training Loss: 0.6107311248779297 \n",
      "     Training Step: 92 Training Loss: 0.6097316145896912 \n",
      "     Training Step: 93 Training Loss: 0.614452064037323 \n",
      "     Training Step: 94 Training Loss: 0.611603856086731 \n",
      "     Training Step: 95 Training Loss: 0.6190733909606934 \n",
      "     Training Step: 96 Training Loss: 0.613627016544342 \n",
      "     Training Step: 97 Training Loss: 0.6107028126716614 \n",
      "     Training Step: 98 Training Loss: 0.6163226962089539 \n",
      "     Training Step: 99 Training Loss: 0.6172621250152588 \n",
      "     Training Step: 100 Training Loss: 0.6133050322532654 \n",
      "     Training Step: 101 Training Loss: 0.6101498007774353 \n",
      "     Training Step: 102 Training Loss: 0.6136312484741211 \n",
      "     Training Step: 103 Training Loss: 0.6135381460189819 \n",
      "     Training Step: 104 Training Loss: 0.618566632270813 \n",
      "     Training Step: 105 Training Loss: 0.6114482879638672 \n",
      "     Training Step: 106 Training Loss: 0.6143776774406433 \n",
      "     Training Step: 107 Training Loss: 0.6108595132827759 \n",
      "     Training Step: 108 Training Loss: 0.6121314764022827 \n",
      "     Training Step: 109 Training Loss: 0.6132944226264954 \n",
      "     Training Step: 110 Training Loss: 0.6167486310005188 \n",
      "     Training Step: 111 Training Loss: 0.6101346015930176 \n",
      "     Training Step: 112 Training Loss: 0.6142456531524658 \n",
      "     Training Step: 113 Training Loss: 0.6169525980949402 \n",
      "     Training Step: 114 Training Loss: 0.6197678446769714 \n",
      "     Training Step: 115 Training Loss: 0.6094492077827454 \n",
      "     Training Step: 116 Training Loss: 0.6122228503227234 \n",
      "     Training Step: 117 Training Loss: 0.6159507036209106 \n",
      "     Training Step: 118 Training Loss: 0.6118282675743103 \n",
      "     Training Step: 119 Training Loss: 0.6125531792640686 \n",
      "     Training Step: 120 Training Loss: 0.6118570566177368 \n",
      "     Training Step: 121 Training Loss: 0.615391194820404 \n",
      "     Training Step: 122 Training Loss: 0.6105771064758301 \n",
      "     Training Step: 123 Training Loss: 0.6164597272872925 \n",
      "     Training Step: 124 Training Loss: 0.6115809679031372 \n",
      "     Training Step: 125 Training Loss: 0.6164640784263611 \n",
      "     Training Step: 126 Training Loss: 0.6182342171669006 \n",
      "     Training Step: 127 Training Loss: 0.6127448678016663 \n",
      "     Training Step: 128 Training Loss: 0.6140554547309875 \n",
      "     Training Step: 129 Training Loss: 0.6152512431144714 \n",
      "     Training Step: 130 Training Loss: 0.616654634475708 \n",
      "     Training Step: 131 Training Loss: 0.6137651205062866 \n",
      "     Training Step: 132 Training Loss: 0.6105562448501587 \n",
      "     Training Step: 133 Training Loss: 0.6167270541191101 \n",
      "     Training Step: 134 Training Loss: 0.6111655235290527 \n",
      "     Training Step: 135 Training Loss: 0.6130748987197876 \n",
      "     Training Step: 136 Training Loss: 0.6171342134475708 \n",
      "     Training Step: 137 Training Loss: 0.6128517389297485 \n",
      "     Training Step: 138 Training Loss: 0.6114540100097656 \n",
      "     Training Step: 139 Training Loss: 0.6127679944038391 \n",
      "     Training Step: 140 Training Loss: 0.6153198480606079 \n",
      "     Training Step: 141 Training Loss: 0.621035099029541 \n",
      "     Training Step: 142 Training Loss: 0.6120816469192505 \n",
      "     Training Step: 143 Training Loss: 0.6095455884933472 \n",
      "     Training Step: 144 Training Loss: 0.6107719540596008 \n",
      "     Training Step: 145 Training Loss: 0.615198016166687 \n",
      "     Training Step: 146 Training Loss: 0.6131622195243835 \n",
      "     Training Step: 147 Training Loss: 0.6155242919921875 \n",
      "     Training Step: 148 Training Loss: 0.6114789843559265 \n",
      "     Training Step: 149 Training Loss: 0.6163984537124634 \n",
      "     Training Step: 150 Training Loss: 0.6132195591926575 \n",
      "     Training Step: 151 Training Loss: 0.6131076216697693 \n",
      "     Training Step: 152 Training Loss: 0.6157481074333191 \n",
      "     Training Step: 153 Training Loss: 0.6185830235481262 \n",
      "     Training Step: 154 Training Loss: 0.6104512214660645 \n",
      "     Training Step: 155 Training Loss: 0.6157617568969727 \n",
      "     Training Step: 156 Training Loss: 0.6147440075874329 \n",
      "     Training Step: 157 Training Loss: 0.6156208515167236 \n",
      "     Training Step: 158 Training Loss: 0.6123141050338745 \n",
      "     Training Step: 159 Training Loss: 0.6171508431434631 \n",
      "     Training Step: 160 Training Loss: 0.6184490919113159 \n",
      "     Training Step: 161 Training Loss: 0.6149867177009583 \n",
      "     Training Step: 162 Training Loss: 0.6177154183387756 \n",
      "     Training Step: 163 Training Loss: 0.6168013215065002 \n",
      "     Training Step: 164 Training Loss: 0.6123518943786621 \n",
      "     Training Step: 165 Training Loss: 0.6166380047798157 \n",
      "     Training Step: 166 Training Loss: 0.6168359518051147 \n",
      "     Training Step: 167 Training Loss: 0.6121676564216614 \n",
      "     Training Step: 168 Training Loss: 0.6154505014419556 \n",
      "     Training Step: 169 Training Loss: 0.6122499108314514 \n",
      "     Training Step: 170 Training Loss: 0.613372266292572 \n",
      "     Training Step: 171 Training Loss: 0.610603392124176 \n",
      "     Training Step: 172 Training Loss: 0.612879753112793 \n",
      "     Training Step: 173 Training Loss: 0.612825334072113 \n",
      "     Training Step: 174 Training Loss: 0.6123883128166199 \n",
      "     Training Step: 175 Training Loss: 0.6141926050186157 \n",
      "     Training Step: 176 Training Loss: 0.6181395053863525 \n",
      "     Training Step: 177 Training Loss: 0.6155602335929871 \n",
      "     Training Step: 178 Training Loss: 0.6116546392440796 \n",
      "     Training Step: 179 Training Loss: 0.612257182598114 \n",
      "     Training Step: 180 Training Loss: 0.6114556193351746 \n",
      "     Training Step: 181 Training Loss: 0.6168063879013062 \n",
      "     Training Step: 182 Training Loss: 0.6131529808044434 \n",
      "     Training Step: 183 Training Loss: 0.6143755316734314 \n",
      "     Training Step: 184 Training Loss: 0.6112174987792969 \n",
      "     Training Step: 185 Training Loss: 0.6147655248641968 \n",
      "     Training Step: 186 Training Loss: 0.6180669069290161 \n",
      "     Training Step: 187 Training Loss: 0.6117941737174988 \n",
      "     Training Step: 188 Training Loss: 0.6119474172592163 \n",
      "     Training Step: 189 Training Loss: 0.6176896095275879 \n",
      "     Training Step: 190 Training Loss: 0.6153031587600708 \n",
      "     Training Step: 191 Training Loss: 0.6105204820632935 \n",
      "     Training Step: 192 Training Loss: 0.6185564398765564 \n",
      "     Training Step: 193 Training Loss: 0.610686182975769 \n",
      "     Training Step: 194 Training Loss: 0.6165158152580261 \n",
      "     Training Step: 195 Training Loss: 0.6157588362693787 \n",
      "     Training Step: 196 Training Loss: 0.6098276972770691 \n",
      "     Training Step: 197 Training Loss: 0.6109253168106079 \n",
      "     Training Step: 198 Training Loss: 0.6101818084716797 \n",
      "     Training Step: 199 Training Loss: 0.6157894134521484 \n",
      "     Training Step: 200 Training Loss: 0.6130062937736511 \n",
      "     Training Step: 201 Training Loss: 0.6135689616203308 \n",
      "     Training Step: 202 Training Loss: 0.6154931783676147 \n",
      "     Training Step: 203 Training Loss: 0.6139352917671204 \n",
      "     Training Step: 204 Training Loss: 0.6135174036026001 \n",
      "     Training Step: 205 Training Loss: 0.6101796627044678 \n",
      "     Training Step: 206 Training Loss: 0.618876039981842 \n",
      "     Training Step: 207 Training Loss: 0.6100324988365173 \n",
      "     Training Step: 208 Training Loss: 0.615431010723114 \n",
      "     Training Step: 209 Training Loss: 0.6135339140892029 \n",
      "     Training Step: 210 Training Loss: 0.6169729232788086 \n",
      "     Training Step: 211 Training Loss: 0.6156736016273499 \n",
      "     Training Step: 212 Training Loss: 0.6145442128181458 \n",
      "     Training Step: 213 Training Loss: 0.619421124458313 \n",
      "     Training Step: 214 Training Loss: 0.6134145855903625 \n",
      "     Training Step: 215 Training Loss: 0.6130048036575317 \n",
      "     Training Step: 216 Training Loss: 0.6132055521011353 \n",
      "     Training Step: 217 Training Loss: 0.6147715449333191 \n",
      "     Training Step: 218 Training Loss: 0.6147015690803528 \n",
      "     Training Step: 219 Training Loss: 0.6145466566085815 \n",
      "     Training Step: 220 Training Loss: 0.6147505044937134 \n",
      "     Training Step: 221 Training Loss: 0.6118590831756592 \n",
      "     Training Step: 222 Training Loss: 0.6109588146209717 \n",
      "     Training Step: 223 Training Loss: 0.6124350428581238 \n",
      "     Training Step: 224 Training Loss: 0.6149551868438721 \n",
      "     Training Step: 225 Training Loss: 0.6116493940353394 \n",
      "     Training Step: 226 Training Loss: 0.6129153966903687 \n",
      "     Training Step: 227 Training Loss: 0.6176398396492004 \n",
      "     Training Step: 228 Training Loss: 0.6144434213638306 \n",
      "     Training Step: 229 Training Loss: 0.6160668134689331 \n",
      "     Training Step: 230 Training Loss: 0.6155752539634705 \n",
      "     Training Step: 231 Training Loss: 0.6137548089027405 \n",
      "     Training Step: 232 Training Loss: 0.6147371530532837 \n",
      "     Training Step: 233 Training Loss: 0.6202051639556885 \n",
      "     Training Step: 234 Training Loss: 0.6118666529655457 \n",
      "     Training Step: 235 Training Loss: 0.6115673184394836 \n",
      "     Training Step: 236 Training Loss: 0.611201822757721 \n",
      "     Training Step: 237 Training Loss: 0.6178410053253174 \n",
      "     Training Step: 238 Training Loss: 0.6140602827072144 \n",
      "     Training Step: 239 Training Loss: 0.6147862672805786 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6177282333374023 \n",
      "     Validation Step: 1 Validation Loss: 0.6149300932884216 \n",
      "     Validation Step: 2 Validation Loss: 0.6183704733848572 \n",
      "     Validation Step: 3 Validation Loss: 0.6122074127197266 \n",
      "     Validation Step: 4 Validation Loss: 0.615363359451294 \n",
      "     Validation Step: 5 Validation Loss: 0.6076910495758057 \n",
      "     Validation Step: 6 Validation Loss: 0.6137104630470276 \n",
      "     Validation Step: 7 Validation Loss: 0.6119682788848877 \n",
      "     Validation Step: 8 Validation Loss: 0.6176079511642456 \n",
      "     Validation Step: 9 Validation Loss: 0.6156131029129028 \n",
      "     Validation Step: 10 Validation Loss: 0.618083655834198 \n",
      "     Validation Step: 11 Validation Loss: 0.6117165088653564 \n",
      "     Validation Step: 12 Validation Loss: 0.6141654849052429 \n",
      "     Validation Step: 13 Validation Loss: 0.6184746026992798 \n",
      "     Validation Step: 14 Validation Loss: 0.6102609038352966 \n",
      "     Validation Step: 15 Validation Loss: 0.6112611889839172 \n",
      "     Validation Step: 16 Validation Loss: 0.6133467555046082 \n",
      "     Validation Step: 17 Validation Loss: 0.6150977611541748 \n",
      "     Validation Step: 18 Validation Loss: 0.6116416454315186 \n",
      "     Validation Step: 19 Validation Loss: 0.6158505082130432 \n",
      "     Validation Step: 20 Validation Loss: 0.6185016632080078 \n",
      "     Validation Step: 21 Validation Loss: 0.6170265078544617 \n",
      "     Validation Step: 22 Validation Loss: 0.6160415410995483 \n",
      "     Validation Step: 23 Validation Loss: 0.6148779392242432 \n",
      "     Validation Step: 24 Validation Loss: 0.6162861585617065 \n",
      "     Validation Step: 25 Validation Loss: 0.6173037886619568 \n",
      "     Validation Step: 26 Validation Loss: 0.6182618141174316 \n",
      "     Validation Step: 27 Validation Loss: 0.6128837466239929 \n",
      "     Validation Step: 28 Validation Loss: 0.611263632774353 \n",
      "     Validation Step: 29 Validation Loss: 0.6152624487876892 \n",
      "     Validation Step: 30 Validation Loss: 0.6107361912727356 \n",
      "     Validation Step: 31 Validation Loss: 0.6105796098709106 \n",
      "     Validation Step: 32 Validation Loss: 0.6102709770202637 \n",
      "     Validation Step: 33 Validation Loss: 0.6146935820579529 \n",
      "     Validation Step: 34 Validation Loss: 0.6137341260910034 \n",
      "     Validation Step: 35 Validation Loss: 0.6145995855331421 \n",
      "     Validation Step: 36 Validation Loss: 0.614576518535614 \n",
      "     Validation Step: 37 Validation Loss: 0.613052248954773 \n",
      "     Validation Step: 38 Validation Loss: 0.6142505407333374 \n",
      "     Validation Step: 39 Validation Loss: 0.6136709451675415 \n",
      "     Validation Step: 40 Validation Loss: 0.6142941117286682 \n",
      "     Validation Step: 41 Validation Loss: 0.6142011284828186 \n",
      "     Validation Step: 42 Validation Loss: 0.6102467179298401 \n",
      "     Validation Step: 43 Validation Loss: 0.6156522035598755 \n",
      "     Validation Step: 44 Validation Loss: 0.6106052994728088 \n",
      "Epoch: 50\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6178145408630371 \n",
      "     Training Step: 1 Training Loss: 0.6116429567337036 \n",
      "     Training Step: 2 Training Loss: 0.6160395741462708 \n",
      "     Training Step: 3 Training Loss: 0.6116194725036621 \n",
      "     Training Step: 4 Training Loss: 0.6155887842178345 \n",
      "     Training Step: 5 Training Loss: 0.6149184107780457 \n",
      "     Training Step: 6 Training Loss: 0.6133363842964172 \n",
      "     Training Step: 7 Training Loss: 0.6168570518493652 \n",
      "     Training Step: 8 Training Loss: 0.611717700958252 \n",
      "     Training Step: 9 Training Loss: 0.6153014302253723 \n",
      "     Training Step: 10 Training Loss: 0.6124443411827087 \n",
      "     Training Step: 11 Training Loss: 0.6101858615875244 \n",
      "     Training Step: 12 Training Loss: 0.6143695116043091 \n",
      "     Training Step: 13 Training Loss: 0.6203466653823853 \n",
      "     Training Step: 14 Training Loss: 0.6127846240997314 \n",
      "     Training Step: 15 Training Loss: 0.619753897190094 \n",
      "     Training Step: 16 Training Loss: 0.6107032299041748 \n",
      "     Training Step: 17 Training Loss: 0.6098487377166748 \n",
      "     Training Step: 18 Training Loss: 0.6209549903869629 \n",
      "     Training Step: 19 Training Loss: 0.6167206168174744 \n",
      "     Training Step: 20 Training Loss: 0.6136766076087952 \n",
      "     Training Step: 21 Training Loss: 0.6153790354728699 \n",
      "     Training Step: 22 Training Loss: 0.6135232448577881 \n",
      "     Training Step: 23 Training Loss: 0.6196200847625732 \n",
      "     Training Step: 24 Training Loss: 0.6136647462844849 \n",
      "     Training Step: 25 Training Loss: 0.6142706274986267 \n",
      "     Training Step: 26 Training Loss: 0.6134874224662781 \n",
      "     Training Step: 27 Training Loss: 0.6151938438415527 \n",
      "     Training Step: 28 Training Loss: 0.6158735752105713 \n",
      "     Training Step: 29 Training Loss: 0.6123860478401184 \n",
      "     Training Step: 30 Training Loss: 0.616458535194397 \n",
      "     Training Step: 31 Training Loss: 0.6122947931289673 \n",
      "     Training Step: 32 Training Loss: 0.6147196888923645 \n",
      "     Training Step: 33 Training Loss: 0.613551676273346 \n",
      "     Training Step: 34 Training Loss: 0.6169084906578064 \n",
      "     Training Step: 35 Training Loss: 0.610226035118103 \n",
      "     Training Step: 36 Training Loss: 0.6114597916603088 \n",
      "     Training Step: 37 Training Loss: 0.6082546710968018 \n",
      "     Training Step: 38 Training Loss: 0.6116451621055603 \n",
      "     Training Step: 39 Training Loss: 0.6148545742034912 \n",
      "     Training Step: 40 Training Loss: 0.6173108816146851 \n",
      "     Training Step: 41 Training Loss: 0.614493191242218 \n",
      "     Training Step: 42 Training Loss: 0.6155449748039246 \n",
      "     Training Step: 43 Training Loss: 0.6127746105194092 \n",
      "     Training Step: 44 Training Loss: 0.6105717420578003 \n",
      "     Training Step: 45 Training Loss: 0.6143153309822083 \n",
      "     Training Step: 46 Training Loss: 0.6167942881584167 \n",
      "     Training Step: 47 Training Loss: 0.6161965727806091 \n",
      "     Training Step: 48 Training Loss: 0.6132404804229736 \n",
      "     Training Step: 49 Training Loss: 0.6122649908065796 \n",
      "     Training Step: 50 Training Loss: 0.6097943782806396 \n",
      "     Training Step: 51 Training Loss: 0.6162746548652649 \n",
      "     Training Step: 52 Training Loss: 0.6144003868103027 \n",
      "     Training Step: 53 Training Loss: 0.6181290149688721 \n",
      "     Training Step: 54 Training Loss: 0.6141780614852905 \n",
      "     Training Step: 55 Training Loss: 0.611615777015686 \n",
      "     Training Step: 56 Training Loss: 0.6177988052368164 \n",
      "     Training Step: 57 Training Loss: 0.6118152141571045 \n",
      "     Training Step: 58 Training Loss: 0.61070317029953 \n",
      "     Training Step: 59 Training Loss: 0.6151432394981384 \n",
      "     Training Step: 60 Training Loss: 0.612888514995575 \n",
      "     Training Step: 61 Training Loss: 0.6144033670425415 \n",
      "     Training Step: 62 Training Loss: 0.6149328947067261 \n",
      "     Training Step: 63 Training Loss: 0.6134836077690125 \n",
      "     Training Step: 64 Training Loss: 0.6116973757743835 \n",
      "     Training Step: 65 Training Loss: 0.6157615780830383 \n",
      "     Training Step: 66 Training Loss: 0.6182529926300049 \n",
      "     Training Step: 67 Training Loss: 0.6160125136375427 \n",
      "     Training Step: 68 Training Loss: 0.618027925491333 \n",
      "     Training Step: 69 Training Loss: 0.6129399538040161 \n",
      "     Training Step: 70 Training Loss: 0.6154218912124634 \n",
      "     Training Step: 71 Training Loss: 0.6112152934074402 \n",
      "     Training Step: 72 Training Loss: 0.6125252842903137 \n",
      "     Training Step: 73 Training Loss: 0.6114344596862793 \n",
      "     Training Step: 74 Training Loss: 0.6129693388938904 \n",
      "     Training Step: 75 Training Loss: 0.61417156457901 \n",
      "     Training Step: 76 Training Loss: 0.6103838086128235 \n",
      "     Training Step: 77 Training Loss: 0.6116062998771667 \n",
      "     Training Step: 78 Training Loss: 0.6154708862304688 \n",
      "     Training Step: 79 Training Loss: 0.614962100982666 \n",
      "     Training Step: 80 Training Loss: 0.6118810176849365 \n",
      "     Training Step: 81 Training Loss: 0.6114424467086792 \n",
      "     Training Step: 82 Training Loss: 0.6118410229682922 \n",
      "     Training Step: 83 Training Loss: 0.6147321462631226 \n",
      "     Training Step: 84 Training Loss: 0.6167514324188232 \n",
      "     Training Step: 85 Training Loss: 0.6136346459388733 \n",
      "     Training Step: 86 Training Loss: 0.6147374510765076 \n",
      "     Training Step: 87 Training Loss: 0.6167264580726624 \n",
      "     Training Step: 88 Training Loss: 0.6184235215187073 \n",
      "     Training Step: 89 Training Loss: 0.6158302426338196 \n",
      "     Training Step: 90 Training Loss: 0.6134030818939209 \n",
      "     Training Step: 91 Training Loss: 0.6125153303146362 \n",
      "     Training Step: 92 Training Loss: 0.6112403273582458 \n",
      "     Training Step: 93 Training Loss: 0.6137952208518982 \n",
      "     Training Step: 94 Training Loss: 0.6142662167549133 \n",
      "     Training Step: 95 Training Loss: 0.618790864944458 \n",
      "     Training Step: 96 Training Loss: 0.612852156162262 \n",
      "     Training Step: 97 Training Loss: 0.6146824955940247 \n",
      "     Training Step: 98 Training Loss: 0.6147506833076477 \n",
      "     Training Step: 99 Training Loss: 0.6131777763366699 \n",
      "     Training Step: 100 Training Loss: 0.6106277704238892 \n",
      "     Training Step: 101 Training Loss: 0.6114347577095032 \n",
      "     Training Step: 102 Training Loss: 0.610615074634552 \n",
      "     Training Step: 103 Training Loss: 0.6183933019638062 \n",
      "     Training Step: 104 Training Loss: 0.6166421175003052 \n",
      "     Training Step: 105 Training Loss: 0.6172453165054321 \n",
      "     Training Step: 106 Training Loss: 0.6169719696044922 \n",
      "     Training Step: 107 Training Loss: 0.6169110536575317 \n",
      "     Training Step: 108 Training Loss: 0.6144189238548279 \n",
      "     Training Step: 109 Training Loss: 0.6112266778945923 \n",
      "     Training Step: 110 Training Loss: 0.6130598783493042 \n",
      "     Training Step: 111 Training Loss: 0.610234260559082 \n",
      "     Training Step: 112 Training Loss: 0.6106696128845215 \n",
      "     Training Step: 113 Training Loss: 0.6204562187194824 \n",
      "     Training Step: 114 Training Loss: 0.6145235896110535 \n",
      "     Training Step: 115 Training Loss: 0.6147755980491638 \n",
      "     Training Step: 116 Training Loss: 0.6104120016098022 \n",
      "     Training Step: 117 Training Loss: 0.6134369969367981 \n",
      "     Training Step: 118 Training Loss: 0.614179790019989 \n",
      "     Training Step: 119 Training Loss: 0.615104615688324 \n",
      "     Training Step: 120 Training Loss: 0.617779552936554 \n",
      "     Training Step: 121 Training Loss: 0.6136188507080078 \n",
      "     Training Step: 122 Training Loss: 0.6131128072738647 \n",
      "     Training Step: 123 Training Loss: 0.6106223464012146 \n",
      "     Training Step: 124 Training Loss: 0.6140475273132324 \n",
      "     Training Step: 125 Training Loss: 0.6132392883300781 \n",
      "     Training Step: 126 Training Loss: 0.6194785237312317 \n",
      "     Training Step: 127 Training Loss: 0.6120803356170654 \n",
      "     Training Step: 128 Training Loss: 0.6116003394126892 \n",
      "     Training Step: 129 Training Loss: 0.6115375757217407 \n",
      "     Training Step: 130 Training Loss: 0.6111865639686584 \n",
      "     Training Step: 131 Training Loss: 0.6121841669082642 \n",
      "     Training Step: 132 Training Loss: 0.6094370484352112 \n",
      "     Training Step: 133 Training Loss: 0.6169429421424866 \n",
      "     Training Step: 134 Training Loss: 0.6153884530067444 \n",
      "     Training Step: 135 Training Loss: 0.615125298500061 \n",
      "     Training Step: 136 Training Loss: 0.613372802734375 \n",
      "     Training Step: 137 Training Loss: 0.6122504472732544 \n",
      "     Training Step: 138 Training Loss: 0.6137654781341553 \n",
      "     Training Step: 139 Training Loss: 0.6127423048019409 \n",
      "     Training Step: 140 Training Loss: 0.6181849837303162 \n",
      "     Training Step: 141 Training Loss: 0.6114814877510071 \n",
      "     Training Step: 142 Training Loss: 0.618939995765686 \n",
      "     Training Step: 143 Training Loss: 0.6128234267234802 \n",
      "     Training Step: 144 Training Loss: 0.6156900525093079 \n",
      "     Training Step: 145 Training Loss: 0.617134690284729 \n",
      "     Training Step: 146 Training Loss: 0.6147472262382507 \n",
      "     Training Step: 147 Training Loss: 0.6134136319160461 \n",
      "     Training Step: 148 Training Loss: 0.614806592464447 \n",
      "     Training Step: 149 Training Loss: 0.60926353931427 \n",
      "     Training Step: 150 Training Loss: 0.6147167682647705 \n",
      "     Training Step: 151 Training Loss: 0.6155404448509216 \n",
      "     Training Step: 152 Training Loss: 0.615344226360321 \n",
      "     Training Step: 153 Training Loss: 0.6114505529403687 \n",
      "     Training Step: 154 Training Loss: 0.6147730350494385 \n",
      "     Training Step: 155 Training Loss: 0.6129699349403381 \n",
      "     Training Step: 156 Training Loss: 0.6094933152198792 \n",
      "     Training Step: 157 Training Loss: 0.616697371006012 \n",
      "     Training Step: 158 Training Loss: 0.6131237745285034 \n",
      "     Training Step: 159 Training Loss: 0.6118643879890442 \n",
      "     Training Step: 160 Training Loss: 0.6153480410575867 \n",
      "     Training Step: 161 Training Loss: 0.6156516671180725 \n",
      "     Training Step: 162 Training Loss: 0.6125345826148987 \n",
      "     Training Step: 163 Training Loss: 0.6172372698783875 \n",
      "     Training Step: 164 Training Loss: 0.6153266429901123 \n",
      "     Training Step: 165 Training Loss: 0.6145386099815369 \n",
      "     Training Step: 166 Training Loss: 0.6139580011367798 \n",
      "     Training Step: 167 Training Loss: 0.6174941062927246 \n",
      "     Training Step: 168 Training Loss: 0.6123990416526794 \n",
      "     Training Step: 169 Training Loss: 0.611831545829773 \n",
      "     Training Step: 170 Training Loss: 0.6182853579521179 \n",
      "     Training Step: 171 Training Loss: 0.6147857308387756 \n",
      "     Training Step: 172 Training Loss: 0.6142536401748657 \n",
      "     Training Step: 173 Training Loss: 0.6185968518257141 \n",
      "     Training Step: 174 Training Loss: 0.6139665246009827 \n",
      "     Training Step: 175 Training Loss: 0.6140801906585693 \n",
      "     Training Step: 176 Training Loss: 0.616227388381958 \n",
      "     Training Step: 177 Training Loss: 0.6127670407295227 \n",
      "     Training Step: 178 Training Loss: 0.6138278841972351 \n",
      "     Training Step: 179 Training Loss: 0.6143671870231628 \n",
      "     Training Step: 180 Training Loss: 0.6131907105445862 \n",
      "     Training Step: 181 Training Loss: 0.6131476163864136 \n",
      "     Training Step: 182 Training Loss: 0.6166837811470032 \n",
      "     Training Step: 183 Training Loss: 0.6164335608482361 \n",
      "     Training Step: 184 Training Loss: 0.616264283657074 \n",
      "     Training Step: 185 Training Loss: 0.6108030080795288 \n",
      "     Training Step: 186 Training Loss: 0.6157846450805664 \n",
      "     Training Step: 187 Training Loss: 0.6159650087356567 \n",
      "     Training Step: 188 Training Loss: 0.6122439503669739 \n",
      "     Training Step: 189 Training Loss: 0.6177316308021545 \n",
      "     Training Step: 190 Training Loss: 0.6155328750610352 \n",
      "     Training Step: 191 Training Loss: 0.6121218800544739 \n",
      "     Training Step: 192 Training Loss: 0.6166215538978577 \n",
      "     Training Step: 193 Training Loss: 0.6101039052009583 \n",
      "     Training Step: 194 Training Loss: 0.6155350208282471 \n",
      "     Training Step: 195 Training Loss: 0.6153985857963562 \n",
      "     Training Step: 196 Training Loss: 0.6137776374816895 \n",
      "     Training Step: 197 Training Loss: 0.6140373945236206 \n",
      "     Training Step: 198 Training Loss: 0.6188954710960388 \n",
      "     Training Step: 199 Training Loss: 0.6167744994163513 \n",
      "     Training Step: 200 Training Loss: 0.6123703718185425 \n",
      "     Training Step: 201 Training Loss: 0.6123026013374329 \n",
      "     Training Step: 202 Training Loss: 0.6171552538871765 \n",
      "     Training Step: 203 Training Loss: 0.6156816482543945 \n",
      "     Training Step: 204 Training Loss: 0.6145125031471252 \n",
      "     Training Step: 205 Training Loss: 0.6118507981300354 \n",
      "     Training Step: 206 Training Loss: 0.6149689555168152 \n",
      "     Training Step: 207 Training Loss: 0.6157608032226562 \n",
      "     Training Step: 208 Training Loss: 0.6109129786491394 \n",
      "     Training Step: 209 Training Loss: 0.6184502840042114 \n",
      "     Training Step: 210 Training Loss: 0.6136404275894165 \n",
      "     Training Step: 211 Training Loss: 0.6152599453926086 \n",
      "     Training Step: 212 Training Loss: 0.6107206344604492 \n",
      "     Training Step: 213 Training Loss: 0.6107508540153503 \n",
      "     Training Step: 214 Training Loss: 0.6178565621376038 \n",
      "     Training Step: 215 Training Loss: 0.6100692749023438 \n",
      "     Training Step: 216 Training Loss: 0.6116271615028381 \n",
      "     Training Step: 217 Training Loss: 0.6121622323989868 \n",
      "     Training Step: 218 Training Loss: 0.6154810786247253 \n",
      "     Training Step: 219 Training Loss: 0.6144875884056091 \n",
      "     Training Step: 220 Training Loss: 0.6097540855407715 \n",
      "     Training Step: 221 Training Loss: 0.6125510334968567 \n",
      "     Training Step: 222 Training Loss: 0.6146290302276611 \n",
      "     Training Step: 223 Training Loss: 0.6104859113693237 \n",
      "     Training Step: 224 Training Loss: 0.6174890398979187 \n",
      "     Training Step: 225 Training Loss: 0.6151790022850037 \n",
      "     Training Step: 226 Training Loss: 0.6121814250946045 \n",
      "     Training Step: 227 Training Loss: 0.6176964640617371 \n",
      "     Training Step: 228 Training Loss: 0.6184561252593994 \n",
      "     Training Step: 229 Training Loss: 0.6168248057365417 \n",
      "     Training Step: 230 Training Loss: 0.6147295236587524 \n",
      "     Training Step: 231 Training Loss: 0.613918662071228 \n",
      "     Training Step: 232 Training Loss: 0.6146190762519836 \n",
      "     Training Step: 233 Training Loss: 0.6119304895401001 \n",
      "     Training Step: 234 Training Loss: 0.6167352795600891 \n",
      "     Training Step: 235 Training Loss: 0.6163841485977173 \n",
      "     Training Step: 236 Training Loss: 0.6183297634124756 \n",
      "     Training Step: 237 Training Loss: 0.6126317977905273 \n",
      "     Training Step: 238 Training Loss: 0.617690920829773 \n",
      "     Training Step: 239 Training Loss: 0.6134535670280457 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6182150840759277 \n",
      "     Validation Step: 1 Validation Loss: 0.6102904677391052 \n",
      "     Validation Step: 2 Validation Loss: 0.6130478978157043 \n",
      "     Validation Step: 3 Validation Loss: 0.6133634448051453 \n",
      "     Validation Step: 4 Validation Loss: 0.6142464280128479 \n",
      "     Validation Step: 5 Validation Loss: 0.6162490844726562 \n",
      "     Validation Step: 6 Validation Loss: 0.6170075535774231 \n",
      "     Validation Step: 7 Validation Loss: 0.6158085465431213 \n",
      "     Validation Step: 8 Validation Loss: 0.6176789999008179 \n",
      "     Validation Step: 9 Validation Loss: 0.6176016330718994 \n",
      "     Validation Step: 10 Validation Loss: 0.6150559186935425 \n",
      "     Validation Step: 11 Validation Loss: 0.6129103302955627 \n",
      "     Validation Step: 12 Validation Loss: 0.6112730503082275 \n",
      "     Validation Step: 13 Validation Loss: 0.6184599995613098 \n",
      "     Validation Step: 14 Validation Loss: 0.613696277141571 \n",
      "     Validation Step: 15 Validation Loss: 0.6122260093688965 \n",
      "     Validation Step: 16 Validation Loss: 0.6153391003608704 \n",
      "     Validation Step: 17 Validation Loss: 0.614182710647583 \n",
      "     Validation Step: 18 Validation Loss: 0.6156236529350281 \n",
      "     Validation Step: 19 Validation Loss: 0.6184737682342529 \n",
      "     Validation Step: 20 Validation Loss: 0.614874541759491 \n",
      "     Validation Step: 21 Validation Loss: 0.6116475462913513 \n",
      "     Validation Step: 22 Validation Loss: 0.6159927248954773 \n",
      "     Validation Step: 23 Validation Loss: 0.614589512348175 \n",
      "     Validation Step: 24 Validation Loss: 0.6103065609931946 \n",
      "     Validation Step: 25 Validation Loss: 0.6117345094680786 \n",
      "     Validation Step: 26 Validation Loss: 0.6107585430145264 \n",
      "     Validation Step: 27 Validation Loss: 0.6105908155441284 \n",
      "     Validation Step: 28 Validation Loss: 0.6077269911766052 \n",
      "     Validation Step: 29 Validation Loss: 0.611979067325592 \n",
      "     Validation Step: 30 Validation Loss: 0.61061030626297 \n",
      "     Validation Step: 31 Validation Loss: 0.6136746406555176 \n",
      "     Validation Step: 32 Validation Loss: 0.6141884922981262 \n",
      "     Validation Step: 33 Validation Loss: 0.6149448752403259 \n",
      "     Validation Step: 34 Validation Loss: 0.6146714091300964 \n",
      "     Validation Step: 35 Validation Loss: 0.6183262467384338 \n",
      "     Validation Step: 36 Validation Loss: 0.6180465817451477 \n",
      "     Validation Step: 37 Validation Loss: 0.6152470707893372 \n",
      "     Validation Step: 38 Validation Loss: 0.6155787110328674 \n",
      "     Validation Step: 39 Validation Loss: 0.6137080192565918 \n",
      "     Validation Step: 40 Validation Loss: 0.6143190264701843 \n",
      "     Validation Step: 41 Validation Loss: 0.6102587580680847 \n",
      "     Validation Step: 42 Validation Loss: 0.6112561821937561 \n",
      "     Validation Step: 43 Validation Loss: 0.6172935366630554 \n",
      "     Validation Step: 44 Validation Loss: 0.6145873069763184 \n",
      "Epoch: 51\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6145316362380981 \n",
      "     Training Step: 1 Training Loss: 0.6104507446289062 \n",
      "     Training Step: 2 Training Loss: 0.6186847686767578 \n",
      "     Training Step: 3 Training Loss: 0.6097341179847717 \n",
      "     Training Step: 4 Training Loss: 0.6134570240974426 \n",
      "     Training Step: 5 Training Loss: 0.614761471748352 \n",
      "     Training Step: 6 Training Loss: 0.6132422089576721 \n",
      "     Training Step: 7 Training Loss: 0.6154626607894897 \n",
      "     Training Step: 8 Training Loss: 0.6165390610694885 \n",
      "     Training Step: 9 Training Loss: 0.6138181686401367 \n",
      "     Training Step: 10 Training Loss: 0.6161348223686218 \n",
      "     Training Step: 11 Training Loss: 0.6116531491279602 \n",
      "     Training Step: 12 Training Loss: 0.6145144104957581 \n",
      "     Training Step: 13 Training Loss: 0.6167641282081604 \n",
      "     Training Step: 14 Training Loss: 0.6142361760139465 \n",
      "     Training Step: 15 Training Loss: 0.6181092262268066 \n",
      "     Training Step: 16 Training Loss: 0.6154417395591736 \n",
      "     Training Step: 17 Training Loss: 0.6113889217376709 \n",
      "     Training Step: 18 Training Loss: 0.6134519577026367 \n",
      "     Training Step: 19 Training Loss: 0.6105667948722839 \n",
      "     Training Step: 20 Training Loss: 0.6117438077926636 \n",
      "     Training Step: 21 Training Loss: 0.614456057548523 \n",
      "     Training Step: 22 Training Loss: 0.6183362007141113 \n",
      "     Training Step: 23 Training Loss: 0.6116659045219421 \n",
      "     Training Step: 24 Training Loss: 0.6101757884025574 \n",
      "     Training Step: 25 Training Loss: 0.6202549338340759 \n",
      "     Training Step: 26 Training Loss: 0.6126013994216919 \n",
      "     Training Step: 27 Training Loss: 0.6119375228881836 \n",
      "     Training Step: 28 Training Loss: 0.613138735294342 \n",
      "     Training Step: 29 Training Loss: 0.6124028563499451 \n",
      "     Training Step: 30 Training Loss: 0.6121893525123596 \n",
      "     Training Step: 31 Training Loss: 0.6129481196403503 \n",
      "     Training Step: 32 Training Loss: 0.6094316840171814 \n",
      "     Training Step: 33 Training Loss: 0.6129370331764221 \n",
      "     Training Step: 34 Training Loss: 0.6178409457206726 \n",
      "     Training Step: 35 Training Loss: 0.6177274584770203 \n",
      "     Training Step: 36 Training Loss: 0.6142358779907227 \n",
      "     Training Step: 37 Training Loss: 0.6125174164772034 \n",
      "     Training Step: 38 Training Loss: 0.6181937456130981 \n",
      "     Training Step: 39 Training Loss: 0.6171998381614685 \n",
      "     Training Step: 40 Training Loss: 0.6123520135879517 \n",
      "     Training Step: 41 Training Loss: 0.6135351657867432 \n",
      "     Training Step: 42 Training Loss: 0.6133184432983398 \n",
      "     Training Step: 43 Training Loss: 0.6157990097999573 \n",
      "     Training Step: 44 Training Loss: 0.6142545342445374 \n",
      "     Training Step: 45 Training Loss: 0.6147512793540955 \n",
      "     Training Step: 46 Training Loss: 0.6135432124137878 \n",
      "     Training Step: 47 Training Loss: 0.6148203015327454 \n",
      "     Training Step: 48 Training Loss: 0.6130782961845398 \n",
      "     Training Step: 49 Training Loss: 0.6194970607757568 \n",
      "     Training Step: 50 Training Loss: 0.6146427989006042 \n",
      "     Training Step: 51 Training Loss: 0.6169366836547852 \n",
      "     Training Step: 52 Training Loss: 0.6109530925750732 \n",
      "     Training Step: 53 Training Loss: 0.6168978214263916 \n",
      "     Training Step: 54 Training Loss: 0.6114537119865417 \n",
      "     Training Step: 55 Training Loss: 0.6137862801551819 \n",
      "     Training Step: 56 Training Loss: 0.6148863434791565 \n",
      "     Training Step: 57 Training Loss: 0.6185135245323181 \n",
      "     Training Step: 58 Training Loss: 0.6083120703697205 \n",
      "     Training Step: 59 Training Loss: 0.6105802059173584 \n",
      "     Training Step: 60 Training Loss: 0.6174854636192322 \n",
      "     Training Step: 61 Training Loss: 0.6178027391433716 \n",
      "     Training Step: 62 Training Loss: 0.6139333844184875 \n",
      "     Training Step: 63 Training Loss: 0.6156078577041626 \n",
      "     Training Step: 64 Training Loss: 0.6154782176017761 \n",
      "     Training Step: 65 Training Loss: 0.6140692830085754 \n",
      "     Training Step: 66 Training Loss: 0.6188677549362183 \n",
      "     Training Step: 67 Training Loss: 0.6152898669242859 \n",
      "     Training Step: 68 Training Loss: 0.6176369786262512 \n",
      "     Training Step: 69 Training Loss: 0.6101925373077393 \n",
      "     Training Step: 70 Training Loss: 0.611181378364563 \n",
      "     Training Step: 71 Training Loss: 0.6116263270378113 \n",
      "     Training Step: 72 Training Loss: 0.6148015260696411 \n",
      "     Training Step: 73 Training Loss: 0.6168226003646851 \n",
      "     Training Step: 74 Training Loss: 0.6162585020065308 \n",
      "     Training Step: 75 Training Loss: 0.6155547499656677 \n",
      "     Training Step: 76 Training Loss: 0.6177136898040771 \n",
      "     Training Step: 77 Training Loss: 0.6166515946388245 \n",
      "     Training Step: 78 Training Loss: 0.6146881580352783 \n",
      "     Training Step: 79 Training Loss: 0.6178116202354431 \n",
      "     Training Step: 80 Training Loss: 0.6116036176681519 \n",
      "     Training Step: 81 Training Loss: 0.614229142665863 \n",
      "     Training Step: 82 Training Loss: 0.6117053627967834 \n",
      "     Training Step: 83 Training Loss: 0.6153271198272705 \n",
      "     Training Step: 84 Training Loss: 0.6114375591278076 \n",
      "     Training Step: 85 Training Loss: 0.6151462197303772 \n",
      "     Training Step: 86 Training Loss: 0.6128231883049011 \n",
      "     Training Step: 87 Training Loss: 0.6152007579803467 \n",
      "     Training Step: 88 Training Loss: 0.6146637201309204 \n",
      "     Training Step: 89 Training Loss: 0.6172072291374207 \n",
      "     Training Step: 90 Training Loss: 0.614369809627533 \n",
      "     Training Step: 91 Training Loss: 0.6138259768486023 \n",
      "     Training Step: 92 Training Loss: 0.6209891438484192 \n",
      "     Training Step: 93 Training Loss: 0.6159819960594177 \n",
      "     Training Step: 94 Training Loss: 0.6149736642837524 \n",
      "     Training Step: 95 Training Loss: 0.6155309677124023 \n",
      "     Training Step: 96 Training Loss: 0.6157761216163635 \n",
      "     Training Step: 97 Training Loss: 0.6129407286643982 \n",
      "     Training Step: 98 Training Loss: 0.6121237874031067 \n",
      "     Training Step: 99 Training Loss: 0.6197603940963745 \n",
      "     Training Step: 100 Training Loss: 0.6164764761924744 \n",
      "     Training Step: 101 Training Loss: 0.618468701839447 \n",
      "     Training Step: 102 Training Loss: 0.6171398162841797 \n",
      "     Training Step: 103 Training Loss: 0.6144254207611084 \n",
      "     Training Step: 104 Training Loss: 0.6155818700790405 \n",
      "     Training Step: 105 Training Loss: 0.6167700886726379 \n",
      "     Training Step: 106 Training Loss: 0.6185864210128784 \n",
      "     Training Step: 107 Training Loss: 0.6157759428024292 \n",
      "     Training Step: 108 Training Loss: 0.6144540309906006 \n",
      "     Training Step: 109 Training Loss: 0.6167177557945251 \n",
      "     Training Step: 110 Training Loss: 0.6152009963989258 \n",
      "     Training Step: 111 Training Loss: 0.6146893501281738 \n",
      "     Training Step: 112 Training Loss: 0.6136762499809265 \n",
      "     Training Step: 113 Training Loss: 0.6122700572013855 \n",
      "     Training Step: 114 Training Loss: 0.6100297570228577 \n",
      "     Training Step: 115 Training Loss: 0.6129404902458191 \n",
      "     Training Step: 116 Training Loss: 0.6140645146369934 \n",
      "     Training Step: 117 Training Loss: 0.6105812788009644 \n",
      "     Training Step: 118 Training Loss: 0.6168238520622253 \n",
      "     Training Step: 119 Training Loss: 0.6108642816543579 \n",
      "     Training Step: 120 Training Loss: 0.6149865388870239 \n",
      "     Training Step: 121 Training Loss: 0.611476719379425 \n",
      "     Training Step: 122 Training Loss: 0.6147518754005432 \n",
      "     Training Step: 123 Training Loss: 0.6160397529602051 \n",
      "     Training Step: 124 Training Loss: 0.6132959723472595 \n",
      "     Training Step: 125 Training Loss: 0.6139010787010193 \n",
      "     Training Step: 126 Training Loss: 0.6153209805488586 \n",
      "     Training Step: 127 Training Loss: 0.6122496724128723 \n",
      "     Training Step: 128 Training Loss: 0.6168099641799927 \n",
      "     Training Step: 129 Training Loss: 0.6098024845123291 \n",
      "     Training Step: 130 Training Loss: 0.6118550300598145 \n",
      "     Training Step: 131 Training Loss: 0.614109456539154 \n",
      "     Training Step: 132 Training Loss: 0.6153149604797363 \n",
      "     Training Step: 133 Training Loss: 0.6184503436088562 \n",
      "     Training Step: 134 Training Loss: 0.6136000752449036 \n",
      "     Training Step: 135 Training Loss: 0.6182994842529297 \n",
      "     Training Step: 136 Training Loss: 0.6163723468780518 \n",
      "     Training Step: 137 Training Loss: 0.6123542189598083 \n",
      "     Training Step: 138 Training Loss: 0.6116623282432556 \n",
      "     Training Step: 139 Training Loss: 0.6107930541038513 \n",
      "     Training Step: 140 Training Loss: 0.6144372820854187 \n",
      "     Training Step: 141 Training Loss: 0.6155324578285217 \n",
      "     Training Step: 142 Training Loss: 0.6096866726875305 \n",
      "     Training Step: 143 Training Loss: 0.61553955078125 \n",
      "     Training Step: 144 Training Loss: 0.6114417314529419 \n",
      "     Training Step: 145 Training Loss: 0.6136555075645447 \n",
      "     Training Step: 146 Training Loss: 0.615227460861206 \n",
      "     Training Step: 147 Training Loss: 0.6177538633346558 \n",
      "     Training Step: 148 Training Loss: 0.6175299882888794 \n",
      "     Training Step: 149 Training Loss: 0.6166941523551941 \n",
      "     Training Step: 150 Training Loss: 0.612575888633728 \n",
      "     Training Step: 151 Training Loss: 0.6147326231002808 \n",
      "     Training Step: 152 Training Loss: 0.6135038137435913 \n",
      "     Training Step: 153 Training Loss: 0.6155604720115662 \n",
      "     Training Step: 154 Training Loss: 0.6101005673408508 \n",
      "     Training Step: 155 Training Loss: 0.6172202229499817 \n",
      "     Training Step: 156 Training Loss: 0.6127845644950867 \n",
      "     Training Step: 157 Training Loss: 0.6197855472564697 \n",
      "     Training Step: 158 Training Loss: 0.6095372438430786 \n",
      "     Training Step: 159 Training Loss: 0.6189066171646118 \n",
      "     Training Step: 160 Training Loss: 0.6121787428855896 \n",
      "     Training Step: 161 Training Loss: 0.6101396679878235 \n",
      "     Training Step: 162 Training Loss: 0.6146683096885681 \n",
      "     Training Step: 163 Training Loss: 0.6105432510375977 \n",
      "     Training Step: 164 Training Loss: 0.6128378510475159 \n",
      "     Training Step: 165 Training Loss: 0.6200730800628662 \n",
      "     Training Step: 166 Training Loss: 0.6162727475166321 \n",
      "     Training Step: 167 Training Loss: 0.6119422912597656 \n",
      "     Training Step: 168 Training Loss: 0.6134037971496582 \n",
      "     Training Step: 169 Training Loss: 0.6147115230560303 \n",
      "     Training Step: 170 Training Loss: 0.6133425235748291 \n",
      "     Training Step: 171 Training Loss: 0.6123166084289551 \n",
      "     Training Step: 172 Training Loss: 0.6125896573066711 \n",
      "     Training Step: 173 Training Loss: 0.6169242858886719 \n",
      "     Training Step: 174 Training Loss: 0.6126877665519714 \n",
      "     Training Step: 175 Training Loss: 0.6158665418624878 \n",
      "     Training Step: 176 Training Loss: 0.6112374067306519 \n",
      "     Training Step: 177 Training Loss: 0.6158167719841003 \n",
      "     Training Step: 178 Training Loss: 0.6157325506210327 \n",
      "     Training Step: 179 Training Loss: 0.6144822835922241 \n",
      "     Training Step: 180 Training Loss: 0.6180446147918701 \n",
      "     Training Step: 181 Training Loss: 0.6180457472801208 \n",
      "     Training Step: 182 Training Loss: 0.6140561103820801 \n",
      "     Training Step: 183 Training Loss: 0.614503026008606 \n",
      "     Training Step: 184 Training Loss: 0.6118397116661072 \n",
      "     Training Step: 185 Training Loss: 0.6150863170623779 \n",
      "     Training Step: 186 Training Loss: 0.614662230014801 \n",
      "     Training Step: 187 Training Loss: 0.6166712641716003 \n",
      "     Training Step: 188 Training Loss: 0.610623836517334 \n",
      "     Training Step: 189 Training Loss: 0.6114525198936462 \n",
      "     Training Step: 190 Training Loss: 0.6115850210189819 \n",
      "     Training Step: 191 Training Loss: 0.6165791749954224 \n",
      "     Training Step: 192 Training Loss: 0.6133207082748413 \n",
      "     Training Step: 193 Training Loss: 0.6116578578948975 \n",
      "     Training Step: 194 Training Loss: 0.6129965782165527 \n",
      "     Training Step: 195 Training Loss: 0.6136587262153625 \n",
      "     Training Step: 196 Training Loss: 0.6162583827972412 \n",
      "     Training Step: 197 Training Loss: 0.6118253469467163 \n",
      "     Training Step: 198 Training Loss: 0.6106119751930237 \n",
      "     Training Step: 199 Training Loss: 0.6123881340026855 \n",
      "     Training Step: 200 Training Loss: 0.6135508418083191 \n",
      "     Training Step: 201 Training Loss: 0.6125256419181824 \n",
      "     Training Step: 202 Training Loss: 0.6141489744186401 \n",
      "     Training Step: 203 Training Loss: 0.6115273237228394 \n",
      "     Training Step: 204 Training Loss: 0.6118883490562439 \n",
      "     Training Step: 205 Training Loss: 0.6138610243797302 \n",
      "     Training Step: 206 Training Loss: 0.6153350472450256 \n",
      "     Training Step: 207 Training Loss: 0.609265148639679 \n",
      "     Training Step: 208 Training Loss: 0.612542986869812 \n",
      "     Training Step: 209 Training Loss: 0.6139336228370667 \n",
      "     Training Step: 210 Training Loss: 0.6127108335494995 \n",
      "     Training Step: 211 Training Loss: 0.6153715252876282 \n",
      "     Training Step: 212 Training Loss: 0.6101760864257812 \n",
      "     Training Step: 213 Training Loss: 0.612162172794342 \n",
      "     Training Step: 214 Training Loss: 0.6171390414237976 \n",
      "     Training Step: 215 Training Loss: 0.6168288588523865 \n",
      "     Training Step: 216 Training Loss: 0.6120779514312744 \n",
      "     Training Step: 217 Training Loss: 0.6182445287704468 \n",
      "     Training Step: 218 Training Loss: 0.6132360100746155 \n",
      "     Training Step: 219 Training Loss: 0.6153804063796997 \n",
      "     Training Step: 220 Training Loss: 0.6104347109794617 \n",
      "     Training Step: 221 Training Loss: 0.6143892407417297 \n",
      "     Training Step: 222 Training Loss: 0.6168413162231445 \n",
      "     Training Step: 223 Training Loss: 0.6168325543403625 \n",
      "     Training Step: 224 Training Loss: 0.6122072339057922 \n",
      "     Training Step: 225 Training Loss: 0.6131078600883484 \n",
      "     Training Step: 226 Training Loss: 0.613167405128479 \n",
      "     Training Step: 227 Training Loss: 0.6107573509216309 \n",
      "     Training Step: 228 Training Loss: 0.6146818995475769 \n",
      "     Training Step: 229 Training Loss: 0.6118259429931641 \n",
      "     Training Step: 230 Training Loss: 0.6132261157035828 \n",
      "     Training Step: 231 Training Loss: 0.6106940507888794 \n",
      "     Training Step: 232 Training Loss: 0.6157991290092468 \n",
      "     Training Step: 233 Training Loss: 0.6148005723953247 \n",
      "     Training Step: 234 Training Loss: 0.6160680055618286 \n",
      "     Training Step: 235 Training Loss: 0.6142674088478088 \n",
      "     Training Step: 236 Training Loss: 0.6116191744804382 \n",
      "     Training Step: 237 Training Loss: 0.6128690838813782 \n",
      "     Training Step: 238 Training Loss: 0.6106675863265991 \n",
      "     Training Step: 239 Training Loss: 0.6150485277175903 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6171827912330627 \n",
      "     Validation Step: 1 Validation Loss: 0.6146166920661926 \n",
      "     Validation Step: 2 Validation Loss: 0.6112154126167297 \n",
      "     Validation Step: 3 Validation Loss: 0.6101940870285034 \n",
      "     Validation Step: 4 Validation Loss: 0.6074823141098022 \n",
      "     Validation Step: 5 Validation Loss: 0.615425705909729 \n",
      "     Validation Step: 6 Validation Loss: 0.6161460876464844 \n",
      "     Validation Step: 7 Validation Loss: 0.6121651530265808 \n",
      "     Validation Step: 8 Validation Loss: 0.61639004945755 \n",
      "     Validation Step: 9 Validation Loss: 0.6105058193206787 \n",
      "     Validation Step: 10 Validation Loss: 0.6146882176399231 \n",
      "     Validation Step: 11 Validation Loss: 0.6178506016731262 \n",
      "     Validation Step: 12 Validation Loss: 0.6119141578674316 \n",
      "     Validation Step: 13 Validation Loss: 0.6137042045593262 \n",
      "     Validation Step: 14 Validation Loss: 0.6157155632972717 \n",
      "     Validation Step: 15 Validation Loss: 0.6185105443000793 \n",
      "     Validation Step: 16 Validation Loss: 0.6105359196662903 \n",
      "     Validation Step: 17 Validation Loss: 0.6141877174377441 \n",
      "     Validation Step: 18 Validation Loss: 0.6147279739379883 \n",
      "     Validation Step: 19 Validation Loss: 0.6137498617172241 \n",
      "     Validation Step: 20 Validation Loss: 0.6149908900260925 \n",
      "     Validation Step: 21 Validation Loss: 0.6116845011711121 \n",
      "     Validation Step: 22 Validation Loss: 0.6149213910102844 \n",
      "     Validation Step: 23 Validation Loss: 0.618683397769928 \n",
      "     Validation Step: 24 Validation Loss: 0.6128720641136169 \n",
      "     Validation Step: 25 Validation Loss: 0.6133726835250854 \n",
      "     Validation Step: 26 Validation Loss: 0.6159195303916931 \n",
      "     Validation Step: 27 Validation Loss: 0.6101074814796448 \n",
      "     Validation Step: 28 Validation Loss: 0.6151447892189026 \n",
      "     Validation Step: 29 Validation Loss: 0.6111820340156555 \n",
      "     Validation Step: 30 Validation Loss: 0.6182343363761902 \n",
      "     Validation Step: 31 Validation Loss: 0.6106634736061096 \n",
      "     Validation Step: 32 Validation Loss: 0.6130334138870239 \n",
      "     Validation Step: 33 Validation Loss: 0.6153334975242615 \n",
      "     Validation Step: 34 Validation Loss: 0.6157349348068237 \n",
      "     Validation Step: 35 Validation Loss: 0.6115866303443909 \n",
      "     Validation Step: 36 Validation Loss: 0.6186535954475403 \n",
      "     Validation Step: 37 Validation Loss: 0.6101446151733398 \n",
      "     Validation Step: 38 Validation Loss: 0.6143376231193542 \n",
      "     Validation Step: 39 Validation Loss: 0.6141855716705322 \n",
      "     Validation Step: 40 Validation Loss: 0.6184224486351013 \n",
      "     Validation Step: 41 Validation Loss: 0.6177526116371155 \n",
      "     Validation Step: 42 Validation Loss: 0.6137418746948242 \n",
      "     Validation Step: 43 Validation Loss: 0.6142799258232117 \n",
      "     Validation Step: 44 Validation Loss: 0.6174643635749817 \n",
      "Epoch: 52\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6168871521949768 \n",
      "     Training Step: 1 Training Loss: 0.61067795753479 \n",
      "     Training Step: 2 Training Loss: 0.6104227900505066 \n",
      "     Training Step: 3 Training Loss: 0.6118534803390503 \n",
      "     Training Step: 4 Training Loss: 0.6199533343315125 \n",
      "     Training Step: 5 Training Loss: 0.612436056137085 \n",
      "     Training Step: 6 Training Loss: 0.6154094934463501 \n",
      "     Training Step: 7 Training Loss: 0.6125950813293457 \n",
      "     Training Step: 8 Training Loss: 0.6114488840103149 \n",
      "     Training Step: 9 Training Loss: 0.6116220951080322 \n",
      "     Training Step: 10 Training Loss: 0.6178770661354065 \n",
      "     Training Step: 11 Training Loss: 0.6150119304656982 \n",
      "     Training Step: 12 Training Loss: 0.6114163994789124 \n",
      "     Training Step: 13 Training Loss: 0.615349292755127 \n",
      "     Training Step: 14 Training Loss: 0.6167816519737244 \n",
      "     Training Step: 15 Training Loss: 0.619655430316925 \n",
      "     Training Step: 16 Training Loss: 0.6201989054679871 \n",
      "     Training Step: 17 Training Loss: 0.6169182658195496 \n",
      "     Training Step: 18 Training Loss: 0.6143688559532166 \n",
      "     Training Step: 19 Training Loss: 0.614778459072113 \n",
      "     Training Step: 20 Training Loss: 0.6188068985939026 \n",
      "     Training Step: 21 Training Loss: 0.6151142716407776 \n",
      "     Training Step: 22 Training Loss: 0.6147921085357666 \n",
      "     Training Step: 23 Training Loss: 0.6157932877540588 \n",
      "     Training Step: 24 Training Loss: 0.6116162538528442 \n",
      "     Training Step: 25 Training Loss: 0.616263747215271 \n",
      "     Training Step: 26 Training Loss: 0.6112400889396667 \n",
      "     Training Step: 27 Training Loss: 0.613523006439209 \n",
      "     Training Step: 28 Training Loss: 0.6149519681930542 \n",
      "     Training Step: 29 Training Loss: 0.6121749877929688 \n",
      "     Training Step: 30 Training Loss: 0.6132203936576843 \n",
      "     Training Step: 31 Training Loss: 0.6157236099243164 \n",
      "     Training Step: 32 Training Loss: 0.6133087277412415 \n",
      "     Training Step: 33 Training Loss: 0.6118007898330688 \n",
      "     Training Step: 34 Training Loss: 0.6154735088348389 \n",
      "     Training Step: 35 Training Loss: 0.6101333498954773 \n",
      "     Training Step: 36 Training Loss: 0.614047646522522 \n",
      "     Training Step: 37 Training Loss: 0.6167392730712891 \n",
      "     Training Step: 38 Training Loss: 0.6101757287979126 \n",
      "     Training Step: 39 Training Loss: 0.6182858347892761 \n",
      "     Training Step: 40 Training Loss: 0.6155536770820618 \n",
      "     Training Step: 41 Training Loss: 0.6148203015327454 \n",
      "     Training Step: 42 Training Loss: 0.611607015132904 \n",
      "     Training Step: 43 Training Loss: 0.6166579723358154 \n",
      "     Training Step: 44 Training Loss: 0.6130900979042053 \n",
      "     Training Step: 45 Training Loss: 0.6186133027076721 \n",
      "     Training Step: 46 Training Loss: 0.6142152547836304 \n",
      "     Training Step: 47 Training Loss: 0.618311882019043 \n",
      "     Training Step: 48 Training Loss: 0.6124610900878906 \n",
      "     Training Step: 49 Training Loss: 0.6129685640335083 \n",
      "     Training Step: 50 Training Loss: 0.6098493337631226 \n",
      "     Training Step: 51 Training Loss: 0.6175264716148376 \n",
      "     Training Step: 52 Training Loss: 0.6180852055549622 \n",
      "     Training Step: 53 Training Loss: 0.6136185526847839 \n",
      "     Training Step: 54 Training Loss: 0.6105281710624695 \n",
      "     Training Step: 55 Training Loss: 0.6141639351844788 \n",
      "     Training Step: 56 Training Loss: 0.6184975504875183 \n",
      "     Training Step: 57 Training Loss: 0.6147823333740234 \n",
      "     Training Step: 58 Training Loss: 0.6188710331916809 \n",
      "     Training Step: 59 Training Loss: 0.6127669215202332 \n",
      "     Training Step: 60 Training Loss: 0.6147559285163879 \n",
      "     Training Step: 61 Training Loss: 0.612672746181488 \n",
      "     Training Step: 62 Training Loss: 0.6107006669044495 \n",
      "     Training Step: 63 Training Loss: 0.6131711006164551 \n",
      "     Training Step: 64 Training Loss: 0.6135240197181702 \n",
      "     Training Step: 65 Training Loss: 0.6145028471946716 \n",
      "     Training Step: 66 Training Loss: 0.6100508570671082 \n",
      "     Training Step: 67 Training Loss: 0.6186494827270508 \n",
      "     Training Step: 68 Training Loss: 0.6133257150650024 \n",
      "     Training Step: 69 Training Loss: 0.6153094172477722 \n",
      "     Training Step: 70 Training Loss: 0.614396333694458 \n",
      "     Training Step: 71 Training Loss: 0.6164151430130005 \n",
      "     Training Step: 72 Training Loss: 0.6122682094573975 \n",
      "     Training Step: 73 Training Loss: 0.6146344542503357 \n",
      "     Training Step: 74 Training Loss: 0.6177166104316711 \n",
      "     Training Step: 75 Training Loss: 0.6209970712661743 \n",
      "     Training Step: 76 Training Loss: 0.6117006540298462 \n",
      "     Training Step: 77 Training Loss: 0.6120105385780334 \n",
      "     Training Step: 78 Training Loss: 0.6108366847038269 \n",
      "     Training Step: 79 Training Loss: 0.6172317266464233 \n",
      "     Training Step: 80 Training Loss: 0.6082882285118103 \n",
      "     Training Step: 81 Training Loss: 0.6150051355361938 \n",
      "     Training Step: 82 Training Loss: 0.615337073802948 \n",
      "     Training Step: 83 Training Loss: 0.6111748218536377 \n",
      "     Training Step: 84 Training Loss: 0.6118479371070862 \n",
      "     Training Step: 85 Training Loss: 0.6116237640380859 \n",
      "     Training Step: 86 Training Loss: 0.613935112953186 \n",
      "     Training Step: 87 Training Loss: 0.6155574321746826 \n",
      "     Training Step: 88 Training Loss: 0.6100220084190369 \n",
      "     Training Step: 89 Training Loss: 0.6128842234611511 \n",
      "     Training Step: 90 Training Loss: 0.611530601978302 \n",
      "     Training Step: 91 Training Loss: 0.6147292852401733 \n",
      "     Training Step: 92 Training Loss: 0.6123861074447632 \n",
      "     Training Step: 93 Training Loss: 0.6122902631759644 \n",
      "     Training Step: 94 Training Loss: 0.6145071983337402 \n",
      "     Training Step: 95 Training Loss: 0.6137580275535583 \n",
      "     Training Step: 96 Training Loss: 0.609457790851593 \n",
      "     Training Step: 97 Training Loss: 0.6167331337928772 \n",
      "     Training Step: 98 Training Loss: 0.6106064915657043 \n",
      "     Training Step: 99 Training Loss: 0.6154225468635559 \n",
      "     Training Step: 100 Training Loss: 0.617741048336029 \n",
      "     Training Step: 101 Training Loss: 0.616832971572876 \n",
      "     Training Step: 102 Training Loss: 0.6132254600524902 \n",
      "     Training Step: 103 Training Loss: 0.6133334636688232 \n",
      "     Training Step: 104 Training Loss: 0.6155515313148499 \n",
      "     Training Step: 105 Training Loss: 0.6154373288154602 \n",
      "     Training Step: 106 Training Loss: 0.6144571900367737 \n",
      "     Training Step: 107 Training Loss: 0.6122481226921082 \n",
      "     Training Step: 108 Training Loss: 0.6147019863128662 \n",
      "     Training Step: 109 Training Loss: 0.6138383150100708 \n",
      "     Training Step: 110 Training Loss: 0.6111388802528381 \n",
      "     Training Step: 111 Training Loss: 0.610379695892334 \n",
      "     Training Step: 112 Training Loss: 0.615412712097168 \n",
      "     Training Step: 113 Training Loss: 0.6134654879570007 \n",
      "     Training Step: 114 Training Loss: 0.6114881634712219 \n",
      "     Training Step: 115 Training Loss: 0.6136669516563416 \n",
      "     Training Step: 116 Training Loss: 0.6108633279800415 \n",
      "     Training Step: 117 Training Loss: 0.6163365840911865 \n",
      "     Training Step: 118 Training Loss: 0.6151872873306274 \n",
      "     Training Step: 119 Training Loss: 0.6168249249458313 \n",
      "     Training Step: 120 Training Loss: 0.6180350184440613 \n",
      "     Training Step: 121 Training Loss: 0.6131607890129089 \n",
      "     Training Step: 122 Training Loss: 0.6168245077133179 \n",
      "     Training Step: 123 Training Loss: 0.6129027605056763 \n",
      "     Training Step: 124 Training Loss: 0.6122766733169556 \n",
      "     Training Step: 125 Training Loss: 0.6121003031730652 \n",
      "     Training Step: 126 Training Loss: 0.6133373379707336 \n",
      "     Training Step: 127 Training Loss: 0.6167758107185364 \n",
      "     Training Step: 128 Training Loss: 0.6147024631500244 \n",
      "     Training Step: 129 Training Loss: 0.6163830757141113 \n",
      "     Training Step: 130 Training Loss: 0.6117910742759705 \n",
      "     Training Step: 131 Training Loss: 0.6137077808380127 \n",
      "     Training Step: 132 Training Loss: 0.614730179309845 \n",
      "     Training Step: 133 Training Loss: 0.61673903465271 \n",
      "     Training Step: 134 Training Loss: 0.616814374923706 \n",
      "     Training Step: 135 Training Loss: 0.6125822067260742 \n",
      "     Training Step: 136 Training Loss: 0.6171370148658752 \n",
      "     Training Step: 137 Training Loss: 0.6142485737800598 \n",
      "     Training Step: 138 Training Loss: 0.616020143032074 \n",
      "     Training Step: 139 Training Loss: 0.6137779355049133 \n",
      "     Training Step: 140 Training Loss: 0.6129133105278015 \n",
      "     Training Step: 141 Training Loss: 0.6116846799850464 \n",
      "     Training Step: 142 Training Loss: 0.619533896446228 \n",
      "     Training Step: 143 Training Loss: 0.6157761216163635 \n",
      "     Training Step: 144 Training Loss: 0.615623950958252 \n",
      "     Training Step: 145 Training Loss: 0.6139326691627502 \n",
      "     Training Step: 146 Training Loss: 0.6098518967628479 \n",
      "     Training Step: 147 Training Loss: 0.6146088242530823 \n",
      "     Training Step: 148 Training Loss: 0.6133756041526794 \n",
      "     Training Step: 149 Training Loss: 0.6139045357704163 \n",
      "     Training Step: 150 Training Loss: 0.6114354729652405 \n",
      "     Training Step: 151 Training Loss: 0.614206850528717 \n",
      "     Training Step: 152 Training Loss: 0.6146575212478638 \n",
      "     Training Step: 153 Training Loss: 0.6129725575447083 \n",
      "     Training Step: 154 Training Loss: 0.6114905476570129 \n",
      "     Training Step: 155 Training Loss: 0.615294337272644 \n",
      "     Training Step: 156 Training Loss: 0.613538920879364 \n",
      "     Training Step: 157 Training Loss: 0.6151907444000244 \n",
      "     Training Step: 158 Training Loss: 0.6120336651802063 \n",
      "     Training Step: 159 Training Loss: 0.6162470579147339 \n",
      "     Training Step: 160 Training Loss: 0.618486762046814 \n",
      "     Training Step: 161 Training Loss: 0.609510600566864 \n",
      "     Training Step: 162 Training Loss: 0.6180974245071411 \n",
      "     Training Step: 163 Training Loss: 0.6109665632247925 \n",
      "     Training Step: 164 Training Loss: 0.6125591993331909 \n",
      "     Training Step: 165 Training Loss: 0.6140305399894714 \n",
      "     Training Step: 166 Training Loss: 0.6124963164329529 \n",
      "     Training Step: 167 Training Loss: 0.6097274422645569 \n",
      "     Training Step: 168 Training Loss: 0.6148271560668945 \n",
      "     Training Step: 169 Training Loss: 0.6155183911323547 \n",
      "     Training Step: 170 Training Loss: 0.6131248474121094 \n",
      "     Training Step: 171 Training Loss: 0.6197519898414612 \n",
      "     Training Step: 172 Training Loss: 0.6118820309638977 \n",
      "     Training Step: 173 Training Loss: 0.6106979250907898 \n",
      "     Training Step: 174 Training Loss: 0.6164175271987915 \n",
      "     Training Step: 175 Training Loss: 0.6153153777122498 \n",
      "     Training Step: 176 Training Loss: 0.6128024458885193 \n",
      "     Training Step: 177 Training Loss: 0.6140565872192383 \n",
      "     Training Step: 178 Training Loss: 0.6134572625160217 \n",
      "     Training Step: 179 Training Loss: 0.6115592122077942 \n",
      "     Training Step: 180 Training Loss: 0.6169479489326477 \n",
      "     Training Step: 181 Training Loss: 0.6121840476989746 \n",
      "     Training Step: 182 Training Loss: 0.61415696144104 \n",
      "     Training Step: 183 Training Loss: 0.6159763932228088 \n",
      "     Training Step: 184 Training Loss: 0.6129379272460938 \n",
      "     Training Step: 185 Training Loss: 0.6143802404403687 \n",
      "     Training Step: 186 Training Loss: 0.616148054599762 \n",
      "     Training Step: 187 Training Loss: 0.6117019057273865 \n",
      "     Training Step: 188 Training Loss: 0.6121463179588318 \n",
      "     Training Step: 189 Training Loss: 0.6137699484825134 \n",
      "     Training Step: 190 Training Loss: 0.6153048872947693 \n",
      "     Training Step: 191 Training Loss: 0.6178333163261414 \n",
      "     Training Step: 192 Training Loss: 0.6144333481788635 \n",
      "     Training Step: 193 Training Loss: 0.6148855686187744 \n",
      "     Training Step: 194 Training Loss: 0.6140962243080139 \n",
      "     Training Step: 195 Training Loss: 0.6180565357208252 \n",
      "     Training Step: 196 Training Loss: 0.6133558750152588 \n",
      "     Training Step: 197 Training Loss: 0.6150965690612793 \n",
      "     Training Step: 198 Training Loss: 0.6147117018699646 \n",
      "     Training Step: 199 Training Loss: 0.6160677671432495 \n",
      "     Training Step: 200 Training Loss: 0.6101634502410889 \n",
      "     Training Step: 201 Training Loss: 0.6177926659584045 \n",
      "     Training Step: 202 Training Loss: 0.6143599152565002 \n",
      "     Training Step: 203 Training Loss: 0.6143711805343628 \n",
      "     Training Step: 204 Training Loss: 0.61668461561203 \n",
      "     Training Step: 205 Training Loss: 0.615384042263031 \n",
      "     Training Step: 206 Training Loss: 0.615838885307312 \n",
      "     Training Step: 207 Training Loss: 0.617821991443634 \n",
      "     Training Step: 208 Training Loss: 0.6093679070472717 \n",
      "     Training Step: 209 Training Loss: 0.6132488250732422 \n",
      "     Training Step: 210 Training Loss: 0.6151090860366821 \n",
      "     Training Step: 211 Training Loss: 0.6114270687103271 \n",
      "     Training Step: 212 Training Loss: 0.617287278175354 \n",
      "     Training Step: 213 Training Loss: 0.6107305884361267 \n",
      "     Training Step: 214 Training Loss: 0.611790120601654 \n",
      "     Training Step: 215 Training Loss: 0.6157719492912292 \n",
      "     Training Step: 216 Training Loss: 0.6105127334594727 \n",
      "     Training Step: 217 Training Loss: 0.6165448427200317 \n",
      "     Training Step: 218 Training Loss: 0.6136607527732849 \n",
      "     Training Step: 219 Training Loss: 0.6174324154853821 \n",
      "     Training Step: 220 Training Loss: 0.6182555556297302 \n",
      "     Training Step: 221 Training Loss: 0.617699921131134 \n",
      "     Training Step: 222 Training Loss: 0.614571750164032 \n",
      "     Training Step: 223 Training Loss: 0.612288236618042 \n",
      "     Training Step: 224 Training Loss: 0.6146804094314575 \n",
      "     Training Step: 225 Training Loss: 0.6123369336128235 \n",
      "     Training Step: 226 Training Loss: 0.6106077432632446 \n",
      "     Training Step: 227 Training Loss: 0.6125435829162598 \n",
      "     Training Step: 228 Training Loss: 0.6128613352775574 \n",
      "     Training Step: 229 Training Loss: 0.6143301725387573 \n",
      "     Training Step: 230 Training Loss: 0.6158145070075989 \n",
      "     Training Step: 231 Training Loss: 0.6118912696838379 \n",
      "     Training Step: 232 Training Loss: 0.6168016195297241 \n",
      "     Training Step: 233 Training Loss: 0.6128448843955994 \n",
      "     Training Step: 234 Training Loss: 0.6101371049880981 \n",
      "     Training Step: 235 Training Loss: 0.6106002926826477 \n",
      "     Training Step: 236 Training Loss: 0.6186533570289612 \n",
      "     Training Step: 237 Training Loss: 0.6172937154769897 \n",
      "     Training Step: 238 Training Loss: 0.6171839833259583 \n",
      "     Training Step: 239 Training Loss: 0.6157084107398987 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6184412837028503 \n",
      "     Validation Step: 1 Validation Loss: 0.6105025410652161 \n",
      "     Validation Step: 2 Validation Loss: 0.6119034290313721 \n",
      "     Validation Step: 3 Validation Loss: 0.6104906797409058 \n",
      "     Validation Step: 4 Validation Loss: 0.6107876300811768 \n",
      "     Validation Step: 5 Validation Loss: 0.614713728427887 \n",
      "     Validation Step: 6 Validation Loss: 0.6121736168861389 \n",
      "     Validation Step: 7 Validation Loss: 0.6130703091621399 \n",
      "     Validation Step: 8 Validation Loss: 0.6143630743026733 \n",
      "     Validation Step: 9 Validation Loss: 0.6144332885742188 \n",
      "     Validation Step: 10 Validation Loss: 0.613898515701294 \n",
      "     Validation Step: 11 Validation Loss: 0.6185520887374878 \n",
      "     Validation Step: 12 Validation Loss: 0.6150610446929932 \n",
      "     Validation Step: 13 Validation Loss: 0.6135021448135376 \n",
      "     Validation Step: 14 Validation Loss: 0.6132374405860901 \n",
      "     Validation Step: 15 Validation Loss: 0.6118399500846863 \n",
      "     Validation Step: 16 Validation Loss: 0.6143248081207275 \n",
      "     Validation Step: 17 Validation Loss: 0.6176831126213074 \n",
      "     Validation Step: 18 Validation Loss: 0.6114690899848938 \n",
      "     Validation Step: 19 Validation Loss: 0.616396963596344 \n",
      "     Validation Step: 20 Validation Loss: 0.6114526987075806 \n",
      "     Validation Step: 21 Validation Loss: 0.6159721612930298 \n",
      "     Validation Step: 22 Validation Loss: 0.6183321475982666 \n",
      "     Validation Step: 23 Validation Loss: 0.6154853701591492 \n",
      "     Validation Step: 24 Validation Loss: 0.6181477308273315 \n",
      "     Validation Step: 25 Validation Loss: 0.6150285601615906 \n",
      "     Validation Step: 26 Validation Loss: 0.6152245998382568 \n",
      "     Validation Step: 27 Validation Loss: 0.6178099513053894 \n",
      "     Validation Step: 28 Validation Loss: 0.6185314655303955 \n",
      "     Validation Step: 29 Validation Loss: 0.6143950819969177 \n",
      "     Validation Step: 30 Validation Loss: 0.6173795461654663 \n",
      "     Validation Step: 31 Validation Loss: 0.6153861880302429 \n",
      "     Validation Step: 32 Validation Loss: 0.6109431385993958 \n",
      "     Validation Step: 33 Validation Loss: 0.6157110333442688 \n",
      "     Validation Step: 34 Validation Loss: 0.6124029755592346 \n",
      "     Validation Step: 35 Validation Loss: 0.6108214259147644 \n",
      "     Validation Step: 36 Validation Loss: 0.6157739162445068 \n",
      "     Validation Step: 37 Validation Loss: 0.6148327589035034 \n",
      "     Validation Step: 38 Validation Loss: 0.6138233542442322 \n",
      "     Validation Step: 39 Validation Loss: 0.6161492466926575 \n",
      "     Validation Step: 40 Validation Loss: 0.6147137880325317 \n",
      "     Validation Step: 41 Validation Loss: 0.6170976161956787 \n",
      "     Validation Step: 42 Validation Loss: 0.6079656481742859 \n",
      "     Validation Step: 43 Validation Loss: 0.6104677319526672 \n",
      "     Validation Step: 44 Validation Loss: 0.6138607263565063 \n",
      "Epoch: 53\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6108697056770325 \n",
      "     Training Step: 1 Training Loss: 0.6148777604103088 \n",
      "     Training Step: 2 Training Loss: 0.6156168580055237 \n",
      "     Training Step: 3 Training Loss: 0.6210235357284546 \n",
      "     Training Step: 4 Training Loss: 0.6122525930404663 \n",
      "     Training Step: 5 Training Loss: 0.6143867373466492 \n",
      "     Training Step: 6 Training Loss: 0.6131063103675842 \n",
      "     Training Step: 7 Training Loss: 0.613238513469696 \n",
      "     Training Step: 8 Training Loss: 0.6130110025405884 \n",
      "     Training Step: 9 Training Loss: 0.6122364401817322 \n",
      "     Training Step: 10 Training Loss: 0.6155493855476379 \n",
      "     Training Step: 11 Training Loss: 0.6184683442115784 \n",
      "     Training Step: 12 Training Loss: 0.6115800142288208 \n",
      "     Training Step: 13 Training Loss: 0.6202622056007385 \n",
      "     Training Step: 14 Training Loss: 0.6172291040420532 \n",
      "     Training Step: 15 Training Loss: 0.6150956749916077 \n",
      "     Training Step: 16 Training Loss: 0.6177496910095215 \n",
      "     Training Step: 17 Training Loss: 0.6095321774482727 \n",
      "     Training Step: 18 Training Loss: 0.6167673468589783 \n",
      "     Training Step: 19 Training Loss: 0.618289589881897 \n",
      "     Training Step: 20 Training Loss: 0.6125779151916504 \n",
      "     Training Step: 21 Training Loss: 0.6162147521972656 \n",
      "     Training Step: 22 Training Loss: 0.6131389737129211 \n",
      "     Training Step: 23 Training Loss: 0.6174024939537048 \n",
      "     Training Step: 24 Training Loss: 0.6133315563201904 \n",
      "     Training Step: 25 Training Loss: 0.6146506071090698 \n",
      "     Training Step: 26 Training Loss: 0.6146183609962463 \n",
      "     Training Step: 27 Training Loss: 0.6146780848503113 \n",
      "     Training Step: 28 Training Loss: 0.6157634854316711 \n",
      "     Training Step: 29 Training Loss: 0.6167165637016296 \n",
      "     Training Step: 30 Training Loss: 0.6163490414619446 \n",
      "     Training Step: 31 Training Loss: 0.6127774715423584 \n",
      "     Training Step: 32 Training Loss: 0.610480010509491 \n",
      "     Training Step: 33 Training Loss: 0.6117008328437805 \n",
      "     Training Step: 34 Training Loss: 0.6154234409332275 \n",
      "     Training Step: 35 Training Loss: 0.6129004955291748 \n",
      "     Training Step: 36 Training Loss: 0.6097313761711121 \n",
      "     Training Step: 37 Training Loss: 0.6129393577575684 \n",
      "     Training Step: 38 Training Loss: 0.6167260408401489 \n",
      "     Training Step: 39 Training Loss: 0.6132053732872009 \n",
      "     Training Step: 40 Training Loss: 0.6169412732124329 \n",
      "     Training Step: 41 Training Loss: 0.6146335005760193 \n",
      "     Training Step: 42 Training Loss: 0.6140614748001099 \n",
      "     Training Step: 43 Training Loss: 0.6181067824363708 \n",
      "     Training Step: 44 Training Loss: 0.6149529814720154 \n",
      "     Training Step: 45 Training Loss: 0.6167299747467041 \n",
      "     Training Step: 46 Training Loss: 0.6134843826293945 \n",
      "     Training Step: 47 Training Loss: 0.6184642314910889 \n",
      "     Training Step: 48 Training Loss: 0.6182259321212769 \n",
      "     Training Step: 49 Training Loss: 0.6109821200370789 \n",
      "     Training Step: 50 Training Loss: 0.617476224899292 \n",
      "     Training Step: 51 Training Loss: 0.6114792227745056 \n",
      "     Training Step: 52 Training Loss: 0.6155258417129517 \n",
      "     Training Step: 53 Training Loss: 0.610744297504425 \n",
      "     Training Step: 54 Training Loss: 0.6100091934204102 \n",
      "     Training Step: 55 Training Loss: 0.6153613924980164 \n",
      "     Training Step: 56 Training Loss: 0.6121761202812195 \n",
      "     Training Step: 57 Training Loss: 0.6155198812484741 \n",
      "     Training Step: 58 Training Loss: 0.613217830657959 \n",
      "     Training Step: 59 Training Loss: 0.617243230342865 \n",
      "     Training Step: 60 Training Loss: 0.6145033836364746 \n",
      "     Training Step: 61 Training Loss: 0.6138022541999817 \n",
      "     Training Step: 62 Training Loss: 0.6147185564041138 \n",
      "     Training Step: 63 Training Loss: 0.6176608800888062 \n",
      "     Training Step: 64 Training Loss: 0.6106797456741333 \n",
      "     Training Step: 65 Training Loss: 0.6115422248840332 \n",
      "     Training Step: 66 Training Loss: 0.6148139834403992 \n",
      "     Training Step: 67 Training Loss: 0.6118047833442688 \n",
      "     Training Step: 68 Training Loss: 0.6189244985580444 \n",
      "     Training Step: 69 Training Loss: 0.6139292120933533 \n",
      "     Training Step: 70 Training Loss: 0.6128157377243042 \n",
      "     Training Step: 71 Training Loss: 0.614709198474884 \n",
      "     Training Step: 72 Training Loss: 0.6133085489273071 \n",
      "     Training Step: 73 Training Loss: 0.6199301481246948 \n",
      "     Training Step: 74 Training Loss: 0.6146957874298096 \n",
      "     Training Step: 75 Training Loss: 0.6124180555343628 \n",
      "     Training Step: 76 Training Loss: 0.6171561479568481 \n",
      "     Training Step: 77 Training Loss: 0.6148777008056641 \n",
      "     Training Step: 78 Training Loss: 0.6102327704429626 \n",
      "     Training Step: 79 Training Loss: 0.615382730960846 \n",
      "     Training Step: 80 Training Loss: 0.6147335767745972 \n",
      "     Training Step: 81 Training Loss: 0.6097419261932373 \n",
      "     Training Step: 82 Training Loss: 0.611423909664154 \n",
      "     Training Step: 83 Training Loss: 0.6140543818473816 \n",
      "     Training Step: 84 Training Loss: 0.6149935126304626 \n",
      "     Training Step: 85 Training Loss: 0.6139245629310608 \n",
      "     Training Step: 86 Training Loss: 0.6115392446517944 \n",
      "     Training Step: 87 Training Loss: 0.6166774034500122 \n",
      "     Training Step: 88 Training Loss: 0.6154392957687378 \n",
      "     Training Step: 89 Training Loss: 0.6157678365707397 \n",
      "     Training Step: 90 Training Loss: 0.6135109663009644 \n",
      "     Training Step: 91 Training Loss: 0.6180810928344727 \n",
      "     Training Step: 92 Training Loss: 0.6133729815483093 \n",
      "     Training Step: 93 Training Loss: 0.6185811161994934 \n",
      "     Training Step: 94 Training Loss: 0.6107314825057983 \n",
      "     Training Step: 95 Training Loss: 0.6123939156532288 \n",
      "     Training Step: 96 Training Loss: 0.6180795431137085 \n",
      "     Training Step: 97 Training Loss: 0.6196534037590027 \n",
      "     Training Step: 98 Training Loss: 0.6117235422134399 \n",
      "     Training Step: 99 Training Loss: 0.6121605634689331 \n",
      "     Training Step: 100 Training Loss: 0.6136033535003662 \n",
      "     Training Step: 101 Training Loss: 0.6119168996810913 \n",
      "     Training Step: 102 Training Loss: 0.6165640354156494 \n",
      "     Training Step: 103 Training Loss: 0.6114370822906494 \n",
      "     Training Step: 104 Training Loss: 0.612129271030426 \n",
      "     Training Step: 105 Training Loss: 0.6157109141349792 \n",
      "     Training Step: 106 Training Loss: 0.6101417541503906 \n",
      "     Training Step: 107 Training Loss: 0.6116406321525574 \n",
      "     Training Step: 108 Training Loss: 0.6184824109077454 \n",
      "     Training Step: 109 Training Loss: 0.6126548051834106 \n",
      "     Training Step: 110 Training Loss: 0.6178458333015442 \n",
      "     Training Step: 111 Training Loss: 0.6139432191848755 \n",
      "     Training Step: 112 Training Loss: 0.6120854616165161 \n",
      "     Training Step: 113 Training Loss: 0.6116129159927368 \n",
      "     Training Step: 114 Training Loss: 0.6157448291778564 \n",
      "     Training Step: 115 Training Loss: 0.6177942752838135 \n",
      "     Training Step: 116 Training Loss: 0.6151027083396912 \n",
      "     Training Step: 117 Training Loss: 0.6142722964286804 \n",
      "     Training Step: 118 Training Loss: 0.6144921183586121 \n",
      "     Training Step: 119 Training Loss: 0.6131935119628906 \n",
      "     Training Step: 120 Training Loss: 0.6129159927368164 \n",
      "     Training Step: 121 Training Loss: 0.6184462904930115 \n",
      "     Training Step: 122 Training Loss: 0.6136505007743835 \n",
      "     Training Step: 123 Training Loss: 0.6160628199577332 \n",
      "     Training Step: 124 Training Loss: 0.6125350594520569 \n",
      "     Training Step: 125 Training Loss: 0.6114684343338013 \n",
      "     Training Step: 126 Training Loss: 0.6159358024597168 \n",
      "     Training Step: 127 Training Loss: 0.615332841873169 \n",
      "     Training Step: 128 Training Loss: 0.6118079423904419 \n",
      "     Training Step: 129 Training Loss: 0.6157797574996948 \n",
      "     Training Step: 130 Training Loss: 0.6115376949310303 \n",
      "     Training Step: 131 Training Loss: 0.6122376918792725 \n",
      "     Training Step: 132 Training Loss: 0.6137776374816895 \n",
      "     Training Step: 133 Training Loss: 0.6162768602371216 \n",
      "     Training Step: 134 Training Loss: 0.6129180192947388 \n",
      "     Training Step: 135 Training Loss: 0.6105798482894897 \n",
      "     Training Step: 136 Training Loss: 0.6151949167251587 \n",
      "     Training Step: 137 Training Loss: 0.6106272339820862 \n",
      "     Training Step: 138 Training Loss: 0.6151400804519653 \n",
      "     Training Step: 139 Training Loss: 0.6177328824996948 \n",
      "     Training Step: 140 Training Loss: 0.6123038530349731 \n",
      "     Training Step: 141 Training Loss: 0.6151678562164307 \n",
      "     Training Step: 142 Training Loss: 0.6112030148506165 \n",
      "     Training Step: 143 Training Loss: 0.6130741834640503 \n",
      "     Training Step: 144 Training Loss: 0.6144985556602478 \n",
      "     Training Step: 145 Training Loss: 0.6136603951454163 \n",
      "     Training Step: 146 Training Loss: 0.6100924015045166 \n",
      "     Training Step: 147 Training Loss: 0.6171509027481079 \n",
      "     Training Step: 148 Training Loss: 0.6154640316963196 \n",
      "     Training Step: 149 Training Loss: 0.6118091940879822 \n",
      "     Training Step: 150 Training Loss: 0.6164559721946716 \n",
      "     Training Step: 151 Training Loss: 0.6171413064002991 \n",
      "     Training Step: 152 Training Loss: 0.6152686476707458 \n",
      "     Training Step: 153 Training Loss: 0.619434654712677 \n",
      "     Training Step: 154 Training Loss: 0.6110303997993469 \n",
      "     Training Step: 155 Training Loss: 0.6186001300811768 \n",
      "     Training Step: 156 Training Loss: 0.6167190074920654 \n",
      "     Training Step: 157 Training Loss: 0.6116482615470886 \n",
      "     Training Step: 158 Training Loss: 0.6147153377532959 \n",
      "     Training Step: 159 Training Loss: 0.6154634356498718 \n",
      "     Training Step: 160 Training Loss: 0.6141171455383301 \n",
      "     Training Step: 161 Training Loss: 0.6147889494895935 \n",
      "     Training Step: 162 Training Loss: 0.6146529316902161 \n",
      "     Training Step: 163 Training Loss: 0.6164132952690125 \n",
      "     Training Step: 164 Training Loss: 0.6115041971206665 \n",
      "     Training Step: 165 Training Loss: 0.6166568994522095 \n",
      "     Training Step: 166 Training Loss: 0.6143136024475098 \n",
      "     Training Step: 167 Training Loss: 0.614345908164978 \n",
      "     Training Step: 168 Training Loss: 0.6169254183769226 \n",
      "     Training Step: 169 Training Loss: 0.6143783330917358 \n",
      "     Training Step: 170 Training Loss: 0.617780327796936 \n",
      "     Training Step: 171 Training Loss: 0.6122395396232605 \n",
      "     Training Step: 172 Training Loss: 0.6101861596107483 \n",
      "     Training Step: 173 Training Loss: 0.6142329573631287 \n",
      "     Training Step: 174 Training Loss: 0.6116250157356262 \n",
      "     Training Step: 175 Training Loss: 0.6169511079788208 \n",
      "     Training Step: 176 Training Loss: 0.6112204194068909 \n",
      "     Training Step: 177 Training Loss: 0.6167432069778442 \n",
      "     Training Step: 178 Training Loss: 0.6125189065933228 \n",
      "     Training Step: 179 Training Loss: 0.6153190732002258 \n",
      "     Training Step: 180 Training Loss: 0.6149935722351074 \n",
      "     Training Step: 181 Training Loss: 0.6141034364700317 \n",
      "     Training Step: 182 Training Loss: 0.6177019476890564 \n",
      "     Training Step: 183 Training Loss: 0.6188392043113708 \n",
      "     Training Step: 184 Training Loss: 0.6135677099227905 \n",
      "     Training Step: 185 Training Loss: 0.6144638061523438 \n",
      "     Training Step: 186 Training Loss: 0.6196936964988708 \n",
      "     Training Step: 187 Training Loss: 0.6146448254585266 \n",
      "     Training Step: 188 Training Loss: 0.6157964468002319 \n",
      "     Training Step: 189 Training Loss: 0.6141761541366577 \n",
      "     Training Step: 190 Training Loss: 0.615391731262207 \n",
      "     Training Step: 191 Training Loss: 0.6180245280265808 \n",
      "     Training Step: 192 Training Loss: 0.6168009042739868 \n",
      "     Training Step: 193 Training Loss: 0.6153677701950073 \n",
      "     Training Step: 194 Training Loss: 0.6166631579399109 \n",
      "     Training Step: 195 Training Loss: 0.6136865615844727 \n",
      "     Training Step: 196 Training Loss: 0.6142147183418274 \n",
      "     Training Step: 197 Training Loss: 0.6145134568214417 \n",
      "     Training Step: 198 Training Loss: 0.6124708652496338 \n",
      "     Training Step: 199 Training Loss: 0.6159768104553223 \n",
      "     Training Step: 200 Training Loss: 0.6118303537368774 \n",
      "     Training Step: 201 Training Loss: 0.6160311698913574 \n",
      "     Training Step: 202 Training Loss: 0.6128519177436829 \n",
      "     Training Step: 203 Training Loss: 0.6155508756637573 \n",
      "     Training Step: 204 Training Loss: 0.610597550868988 \n",
      "     Training Step: 205 Training Loss: 0.6111451387405396 \n",
      "     Training Step: 206 Training Loss: 0.6082683801651001 \n",
      "     Training Step: 207 Training Loss: 0.6169655323028564 \n",
      "     Training Step: 208 Training Loss: 0.6153630614280701 \n",
      "     Training Step: 209 Training Loss: 0.6133404970169067 \n",
      "     Training Step: 210 Training Loss: 0.6137564778327942 \n",
      "     Training Step: 211 Training Loss: 0.6122840642929077 \n",
      "     Training Step: 212 Training Loss: 0.6093204021453857 \n",
      "     Training Step: 213 Training Loss: 0.6140609979629517 \n",
      "     Training Step: 214 Training Loss: 0.6133046746253967 \n",
      "     Training Step: 215 Training Loss: 0.6142075657844543 \n",
      "     Training Step: 216 Training Loss: 0.6118617057800293 \n",
      "     Training Step: 217 Training Loss: 0.6161697506904602 \n",
      "     Training Step: 218 Training Loss: 0.6106119155883789 \n",
      "     Training Step: 219 Training Loss: 0.6146724224090576 \n",
      "     Training Step: 220 Training Loss: 0.612578809261322 \n",
      "     Training Step: 221 Training Loss: 0.6127594709396362 \n",
      "     Training Step: 222 Training Loss: 0.6100468635559082 \n",
      "     Training Step: 223 Training Loss: 0.6138488054275513 \n",
      "     Training Step: 224 Training Loss: 0.616895854473114 \n",
      "     Training Step: 225 Training Loss: 0.6143721342086792 \n",
      "     Training Step: 226 Training Loss: 0.6114357113838196 \n",
      "     Training Step: 227 Training Loss: 0.6128403544425964 \n",
      "     Training Step: 228 Training Loss: 0.6134525537490845 \n",
      "     Training Step: 229 Training Loss: 0.6103965044021606 \n",
      "     Training Step: 230 Training Loss: 0.6107306480407715 \n",
      "     Training Step: 231 Training Loss: 0.6135512590408325 \n",
      "     Training Step: 232 Training Loss: 0.6118491888046265 \n",
      "     Training Step: 233 Training Loss: 0.6104647517204285 \n",
      "     Training Step: 234 Training Loss: 0.6124040484428406 \n",
      "     Training Step: 235 Training Loss: 0.6163604855537415 \n",
      "     Training Step: 236 Training Loss: 0.6132996678352356 \n",
      "     Training Step: 237 Training Loss: 0.6123350858688354 \n",
      "     Training Step: 238 Training Loss: 0.6097553968429565 \n",
      "     Training Step: 239 Training Loss: 0.6094303131103516 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.614210844039917 \n",
      "     Validation Step: 1 Validation Loss: 0.6121863722801208 \n",
      "     Validation Step: 2 Validation Loss: 0.6175127029418945 \n",
      "     Validation Step: 3 Validation Loss: 0.6117052435874939 \n",
      "     Validation Step: 4 Validation Loss: 0.6115957498550415 \n",
      "     Validation Step: 5 Validation Loss: 0.6172269582748413 \n",
      "     Validation Step: 6 Validation Loss: 0.611193835735321 \n",
      "     Validation Step: 7 Validation Loss: 0.6102218627929688 \n",
      "     Validation Step: 8 Validation Loss: 0.6178873181343079 \n",
      "     Validation Step: 9 Validation Loss: 0.6106804609298706 \n",
      "     Validation Step: 10 Validation Loss: 0.6161604523658752 \n",
      "     Validation Step: 11 Validation Loss: 0.6178059577941895 \n",
      "     Validation Step: 12 Validation Loss: 0.6112390756607056 \n",
      "     Validation Step: 13 Validation Loss: 0.6137476563453674 \n",
      "     Validation Step: 14 Validation Loss: 0.6187418699264526 \n",
      "     Validation Step: 15 Validation Loss: 0.6142256855964661 \n",
      "     Validation Step: 16 Validation Loss: 0.6154518127441406 \n",
      "     Validation Step: 17 Validation Loss: 0.613409161567688 \n",
      "     Validation Step: 18 Validation Loss: 0.6150311827659607 \n",
      "     Validation Step: 19 Validation Loss: 0.6157456636428833 \n",
      "     Validation Step: 20 Validation Loss: 0.6164007186889648 \n",
      "     Validation Step: 21 Validation Loss: 0.6137567162513733 \n",
      "     Validation Step: 22 Validation Loss: 0.6159498691558838 \n",
      "     Validation Step: 23 Validation Loss: 0.6182786822319031 \n",
      "     Validation Step: 24 Validation Loss: 0.6129010319709778 \n",
      "     Validation Step: 25 Validation Loss: 0.6143864393234253 \n",
      "     Validation Step: 26 Validation Loss: 0.607489824295044 \n",
      "     Validation Step: 27 Validation Loss: 0.6119268536567688 \n",
      "     Validation Step: 28 Validation Loss: 0.6146565675735474 \n",
      "     Validation Step: 29 Validation Loss: 0.6153665781021118 \n",
      "     Validation Step: 30 Validation Loss: 0.6151672601699829 \n",
      "     Validation Step: 31 Validation Loss: 0.614721953868866 \n",
      "     Validation Step: 32 Validation Loss: 0.6187137961387634 \n",
      "     Validation Step: 33 Validation Loss: 0.6157609820365906 \n",
      "     Validation Step: 34 Validation Loss: 0.610537052154541 \n",
      "     Validation Step: 35 Validation Loss: 0.6137473583221436 \n",
      "     Validation Step: 36 Validation Loss: 0.6130464673042297 \n",
      "     Validation Step: 37 Validation Loss: 0.6105136871337891 \n",
      "     Validation Step: 38 Validation Loss: 0.6184603571891785 \n",
      "     Validation Step: 39 Validation Loss: 0.6185510754585266 \n",
      "     Validation Step: 40 Validation Loss: 0.6101070642471313 \n",
      "     Validation Step: 41 Validation Loss: 0.6143085956573486 \n",
      "     Validation Step: 42 Validation Loss: 0.6101484894752502 \n",
      "     Validation Step: 43 Validation Loss: 0.6147545576095581 \n",
      "     Validation Step: 44 Validation Loss: 0.6149476170539856 \n",
      "Epoch: 54\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.616968035697937 \n",
      "     Training Step: 1 Training Loss: 0.6147874593734741 \n",
      "     Training Step: 2 Training Loss: 0.6174731254577637 \n",
      "     Training Step: 3 Training Loss: 0.6158504486083984 \n",
      "     Training Step: 4 Training Loss: 0.6178033351898193 \n",
      "     Training Step: 5 Training Loss: 0.6169718503952026 \n",
      "     Training Step: 6 Training Loss: 0.6135028004646301 \n",
      "     Training Step: 7 Training Loss: 0.6143340468406677 \n",
      "     Training Step: 8 Training Loss: 0.6172049641609192 \n",
      "     Training Step: 9 Training Loss: 0.6154800653457642 \n",
      "     Training Step: 10 Training Loss: 0.6158636808395386 \n",
      "     Training Step: 11 Training Loss: 0.610797643661499 \n",
      "     Training Step: 12 Training Loss: 0.6146606206893921 \n",
      "     Training Step: 13 Training Loss: 0.6135692000389099 \n",
      "     Training Step: 14 Training Loss: 0.618649959564209 \n",
      "     Training Step: 15 Training Loss: 0.6159770488739014 \n",
      "     Training Step: 16 Training Loss: 0.6129670143127441 \n",
      "     Training Step: 17 Training Loss: 0.611240565776825 \n",
      "     Training Step: 18 Training Loss: 0.6107026934623718 \n",
      "     Training Step: 19 Training Loss: 0.6142539978027344 \n",
      "     Training Step: 20 Training Loss: 0.6147657036781311 \n",
      "     Training Step: 21 Training Loss: 0.6118898987770081 \n",
      "     Training Step: 22 Training Loss: 0.6145230531692505 \n",
      "     Training Step: 23 Training Loss: 0.6122152805328369 \n",
      "     Training Step: 24 Training Loss: 0.6151759028434753 \n",
      "     Training Step: 25 Training Loss: 0.61328125 \n",
      "     Training Step: 26 Training Loss: 0.6134148240089417 \n",
      "     Training Step: 27 Training Loss: 0.6105717420578003 \n",
      "     Training Step: 28 Training Loss: 0.6117907762527466 \n",
      "     Training Step: 29 Training Loss: 0.6168474555015564 \n",
      "     Training Step: 30 Training Loss: 0.6181055903434753 \n",
      "     Training Step: 31 Training Loss: 0.6143406629562378 \n",
      "     Training Step: 32 Training Loss: 0.6164507865905762 \n",
      "     Training Step: 33 Training Loss: 0.6118597388267517 \n",
      "     Training Step: 34 Training Loss: 0.6188536882400513 \n",
      "     Training Step: 35 Training Loss: 0.6154570579528809 \n",
      "     Training Step: 36 Training Loss: 0.6127580404281616 \n",
      "     Training Step: 37 Training Loss: 0.6100678443908691 \n",
      "     Training Step: 38 Training Loss: 0.6186074614524841 \n",
      "     Training Step: 39 Training Loss: 0.6158027052879333 \n",
      "     Training Step: 40 Training Loss: 0.6200218200683594 \n",
      "     Training Step: 41 Training Loss: 0.6112663745880127 \n",
      "     Training Step: 42 Training Loss: 0.6116837859153748 \n",
      "     Training Step: 43 Training Loss: 0.6151285171508789 \n",
      "     Training Step: 44 Training Loss: 0.6166786551475525 \n",
      "     Training Step: 45 Training Loss: 0.6177554130554199 \n",
      "     Training Step: 46 Training Loss: 0.6116477251052856 \n",
      "     Training Step: 47 Training Loss: 0.6118640899658203 \n",
      "     Training Step: 48 Training Loss: 0.616716742515564 \n",
      "     Training Step: 49 Training Loss: 0.6186038851737976 \n",
      "     Training Step: 50 Training Loss: 0.6196883320808411 \n",
      "     Training Step: 51 Training Loss: 0.6139866709709167 \n",
      "     Training Step: 52 Training Loss: 0.6116600632667542 \n",
      "     Training Step: 53 Training Loss: 0.6133960485458374 \n",
      "     Training Step: 54 Training Loss: 0.6171978712081909 \n",
      "     Training Step: 55 Training Loss: 0.6179095506668091 \n",
      "     Training Step: 56 Training Loss: 0.618144690990448 \n",
      "     Training Step: 57 Training Loss: 0.617131769657135 \n",
      "     Training Step: 58 Training Loss: 0.610866129398346 \n",
      "     Training Step: 59 Training Loss: 0.6128484606742859 \n",
      "     Training Step: 60 Training Loss: 0.6140680909156799 \n",
      "     Training Step: 61 Training Loss: 0.6141183972358704 \n",
      "     Training Step: 62 Training Loss: 0.615657389163971 \n",
      "     Training Step: 63 Training Loss: 0.6142604351043701 \n",
      "     Training Step: 64 Training Loss: 0.6144784092903137 \n",
      "     Training Step: 65 Training Loss: 0.6172265410423279 \n",
      "     Training Step: 66 Training Loss: 0.6167274117469788 \n",
      "     Training Step: 67 Training Loss: 0.6131632328033447 \n",
      "     Training Step: 68 Training Loss: 0.6140251755714417 \n",
      "     Training Step: 69 Training Loss: 0.6169157028198242 \n",
      "     Training Step: 70 Training Loss: 0.6115584969520569 \n",
      "     Training Step: 71 Training Loss: 0.6162530183792114 \n",
      "     Training Step: 72 Training Loss: 0.6158432960510254 \n",
      "     Training Step: 73 Training Loss: 0.614457368850708 \n",
      "     Training Step: 74 Training Loss: 0.611445426940918 \n",
      "     Training Step: 75 Training Loss: 0.6104208827018738 \n",
      "     Training Step: 76 Training Loss: 0.6122497320175171 \n",
      "     Training Step: 77 Training Loss: 0.6125224828720093 \n",
      "     Training Step: 78 Training Loss: 0.6123874187469482 \n",
      "     Training Step: 79 Training Loss: 0.6101205945014954 \n",
      "     Training Step: 80 Training Loss: 0.6176108717918396 \n",
      "     Training Step: 81 Training Loss: 0.6125269532203674 \n",
      "     Training Step: 82 Training Loss: 0.6116732358932495 \n",
      "     Training Step: 83 Training Loss: 0.6157758235931396 \n",
      "     Training Step: 84 Training Loss: 0.6105316281318665 \n",
      "     Training Step: 85 Training Loss: 0.6155555248260498 \n",
      "     Training Step: 86 Training Loss: 0.6109040975570679 \n",
      "     Training Step: 87 Training Loss: 0.6114304661750793 \n",
      "     Training Step: 88 Training Loss: 0.6165487170219421 \n",
      "     Training Step: 89 Training Loss: 0.6148014068603516 \n",
      "     Training Step: 90 Training Loss: 0.6154488325119019 \n",
      "     Training Step: 91 Training Loss: 0.6106691956520081 \n",
      "     Training Step: 92 Training Loss: 0.6153031587600708 \n",
      "     Training Step: 93 Training Loss: 0.61469566822052 \n",
      "     Training Step: 94 Training Loss: 0.619451642036438 \n",
      "     Training Step: 95 Training Loss: 0.612178385257721 \n",
      "     Training Step: 96 Training Loss: 0.6184563040733337 \n",
      "     Training Step: 97 Training Loss: 0.6125262975692749 \n",
      "     Training Step: 98 Training Loss: 0.6121900081634521 \n",
      "     Training Step: 99 Training Loss: 0.6117051839828491 \n",
      "     Training Step: 100 Training Loss: 0.6120485067367554 \n",
      "     Training Step: 101 Training Loss: 0.6177635192871094 \n",
      "     Training Step: 102 Training Loss: 0.6171424984931946 \n",
      "     Training Step: 103 Training Loss: 0.6196540594100952 \n",
      "     Training Step: 104 Training Loss: 0.6102364659309387 \n",
      "     Training Step: 105 Training Loss: 0.6137269735336304 \n",
      "     Training Step: 106 Training Loss: 0.6105806827545166 \n",
      "     Training Step: 107 Training Loss: 0.6125555634498596 \n",
      "     Training Step: 108 Training Loss: 0.615867555141449 \n",
      "     Training Step: 109 Training Loss: 0.609381377696991 \n",
      "     Training Step: 110 Training Loss: 0.6129979491233826 \n",
      "     Training Step: 111 Training Loss: 0.6144124865531921 \n",
      "     Training Step: 112 Training Loss: 0.6162341237068176 \n",
      "     Training Step: 113 Training Loss: 0.612311601638794 \n",
      "     Training Step: 114 Training Loss: 0.6124936938285828 \n",
      "     Training Step: 115 Training Loss: 0.6153836250305176 \n",
      "     Training Step: 116 Training Loss: 0.6151615977287292 \n",
      "     Training Step: 117 Training Loss: 0.6120818257331848 \n",
      "     Training Step: 118 Training Loss: 0.6128242611885071 \n",
      "     Training Step: 119 Training Loss: 0.6144518852233887 \n",
      "     Training Step: 120 Training Loss: 0.6104074716567993 \n",
      "     Training Step: 121 Training Loss: 0.6122433543205261 \n",
      "     Training Step: 122 Training Loss: 0.6151900291442871 \n",
      "     Training Step: 123 Training Loss: 0.6114433407783508 \n",
      "     Training Step: 124 Training Loss: 0.6166701316833496 \n",
      "     Training Step: 125 Training Loss: 0.6164956092834473 \n",
      "     Training Step: 126 Training Loss: 0.6149535775184631 \n",
      "     Training Step: 127 Training Loss: 0.6147185564041138 \n",
      "     Training Step: 128 Training Loss: 0.612990140914917 \n",
      "     Training Step: 129 Training Loss: 0.609728991985321 \n",
      "     Training Step: 130 Training Loss: 0.6123056411743164 \n",
      "     Training Step: 131 Training Loss: 0.6101933121681213 \n",
      "     Training Step: 132 Training Loss: 0.6169670820236206 \n",
      "     Training Step: 133 Training Loss: 0.6114109754562378 \n",
      "     Training Step: 134 Training Loss: 0.613204300403595 \n",
      "     Training Step: 135 Training Loss: 0.6118462681770325 \n",
      "     Training Step: 136 Training Loss: 0.6153349876403809 \n",
      "     Training Step: 137 Training Loss: 0.6095258593559265 \n",
      "     Training Step: 138 Training Loss: 0.6146075129508972 \n",
      "     Training Step: 139 Training Loss: 0.6138383746147156 \n",
      "     Training Step: 140 Training Loss: 0.6143332719802856 \n",
      "     Training Step: 141 Training Loss: 0.6131466031074524 \n",
      "     Training Step: 142 Training Loss: 0.6161404252052307 \n",
      "     Training Step: 143 Training Loss: 0.6132129430770874 \n",
      "     Training Step: 144 Training Loss: 0.6168068647384644 \n",
      "     Training Step: 145 Training Loss: 0.620919942855835 \n",
      "     Training Step: 146 Training Loss: 0.6134087443351746 \n",
      "     Training Step: 147 Training Loss: 0.6131211519241333 \n",
      "     Training Step: 148 Training Loss: 0.6140811443328857 \n",
      "     Training Step: 149 Training Loss: 0.6136845946311951 \n",
      "     Training Step: 150 Training Loss: 0.6116504073143005 \n",
      "     Training Step: 151 Training Loss: 0.6150413751602173 \n",
      "     Training Step: 152 Training Loss: 0.611538290977478 \n",
      "     Training Step: 153 Training Loss: 0.6203029751777649 \n",
      "     Training Step: 154 Training Loss: 0.6115087866783142 \n",
      "     Training Step: 155 Training Loss: 0.6106978058815002 \n",
      "     Training Step: 156 Training Loss: 0.6129366755485535 \n",
      "     Training Step: 157 Training Loss: 0.6154174208641052 \n",
      "     Training Step: 158 Training Loss: 0.6155269742012024 \n",
      "     Training Step: 159 Training Loss: 0.6107129454612732 \n",
      "     Training Step: 160 Training Loss: 0.6146612763404846 \n",
      "     Training Step: 161 Training Loss: 0.6128492951393127 \n",
      "     Training Step: 162 Training Loss: 0.6155462861061096 \n",
      "     Training Step: 163 Training Loss: 0.6166922450065613 \n",
      "     Training Step: 164 Training Loss: 0.61553955078125 \n",
      "     Training Step: 165 Training Loss: 0.6099018454551697 \n",
      "     Training Step: 166 Training Loss: 0.6182330250740051 \n",
      "     Training Step: 167 Training Loss: 0.6168330311775208 \n",
      "     Training Step: 168 Training Loss: 0.6126586198806763 \n",
      "     Training Step: 169 Training Loss: 0.6124798059463501 \n",
      "     Training Step: 170 Training Loss: 0.6131302714347839 \n",
      "     Training Step: 171 Training Loss: 0.6134660840034485 \n",
      "     Training Step: 172 Training Loss: 0.6146214008331299 \n",
      "     Training Step: 173 Training Loss: 0.6150831580162048 \n",
      "     Training Step: 174 Training Loss: 0.6123208999633789 \n",
      "     Training Step: 175 Training Loss: 0.6177647709846497 \n",
      "     Training Step: 176 Training Loss: 0.6133121252059937 \n",
      "     Training Step: 177 Training Loss: 0.618414580821991 \n",
      "     Training Step: 178 Training Loss: 0.6145070791244507 \n",
      "     Training Step: 179 Training Loss: 0.6154391169548035 \n",
      "     Training Step: 180 Training Loss: 0.6097697615623474 \n",
      "     Training Step: 181 Training Loss: 0.6147433519363403 \n",
      "     Training Step: 182 Training Loss: 0.613608717918396 \n",
      "     Training Step: 183 Training Loss: 0.6147992610931396 \n",
      "     Training Step: 184 Training Loss: 0.6137439608573914 \n",
      "     Training Step: 185 Training Loss: 0.6116083264350891 \n",
      "     Training Step: 186 Training Loss: 0.6163602471351624 \n",
      "     Training Step: 187 Training Loss: 0.6083347797393799 \n",
      "     Training Step: 188 Training Loss: 0.6092385649681091 \n",
      "     Training Step: 189 Training Loss: 0.6153855323791504 \n",
      "     Training Step: 190 Training Loss: 0.6135419011116028 \n",
      "     Training Step: 191 Training Loss: 0.6137787103652954 \n",
      "     Training Step: 192 Training Loss: 0.6147422194480896 \n",
      "     Training Step: 193 Training Loss: 0.6112239956855774 \n",
      "     Training Step: 194 Training Loss: 0.612900972366333 \n",
      "     Training Step: 195 Training Loss: 0.6143651604652405 \n",
      "     Training Step: 196 Training Loss: 0.6182389259338379 \n",
      "     Training Step: 197 Training Loss: 0.6183372139930725 \n",
      "     Training Step: 198 Training Loss: 0.6166602969169617 \n",
      "     Training Step: 199 Training Loss: 0.6119500994682312 \n",
      "     Training Step: 200 Training Loss: 0.6149296760559082 \n",
      "     Training Step: 201 Training Loss: 0.6176256537437439 \n",
      "     Training Step: 202 Training Loss: 0.6135499477386475 \n",
      "     Training Step: 203 Training Loss: 0.6137990355491638 \n",
      "     Training Step: 204 Training Loss: 0.6101656556129456 \n",
      "     Training Step: 205 Training Loss: 0.6154423356056213 \n",
      "     Training Step: 206 Training Loss: 0.614838719367981 \n",
      "     Training Step: 207 Training Loss: 0.6146981716156006 \n",
      "     Training Step: 208 Training Loss: 0.6149458885192871 \n",
      "     Training Step: 209 Training Loss: 0.6177342534065247 \n",
      "     Training Step: 210 Training Loss: 0.610654354095459 \n",
      "     Training Step: 211 Training Loss: 0.6139642596244812 \n",
      "     Training Step: 212 Training Loss: 0.6109307408332825 \n",
      "     Training Step: 213 Training Loss: 0.6167798042297363 \n",
      "     Training Step: 214 Training Loss: 0.6160943508148193 \n",
      "     Training Step: 215 Training Loss: 0.616033136844635 \n",
      "     Training Step: 216 Training Loss: 0.6115821003913879 \n",
      "     Training Step: 217 Training Loss: 0.6180711388587952 \n",
      "     Training Step: 218 Training Loss: 0.6128284931182861 \n",
      "     Training Step: 219 Training Loss: 0.6118758916854858 \n",
      "     Training Step: 220 Training Loss: 0.6147041916847229 \n",
      "     Training Step: 221 Training Loss: 0.6133192777633667 \n",
      "     Training Step: 222 Training Loss: 0.6157233119010925 \n",
      "     Training Step: 223 Training Loss: 0.6141288876533508 \n",
      "     Training Step: 224 Training Loss: 0.6124036312103271 \n",
      "     Training Step: 225 Training Loss: 0.6162509918212891 \n",
      "     Training Step: 226 Training Loss: 0.6144311428070068 \n",
      "     Training Step: 227 Training Loss: 0.6141640543937683 \n",
      "     Training Step: 228 Training Loss: 0.6153054237365723 \n",
      "     Training Step: 229 Training Loss: 0.6140309572219849 \n",
      "     Training Step: 230 Training Loss: 0.6136415600776672 \n",
      "     Training Step: 231 Training Loss: 0.6188892126083374 \n",
      "     Training Step: 232 Training Loss: 0.6118561029434204 \n",
      "     Training Step: 233 Training Loss: 0.6133007407188416 \n",
      "     Training Step: 234 Training Loss: 0.6166589856147766 \n",
      "     Training Step: 235 Training Loss: 0.6152437925338745 \n",
      "     Training Step: 236 Training Loss: 0.6121472716331482 \n",
      "     Training Step: 237 Training Loss: 0.6142491102218628 \n",
      "     Training Step: 238 Training Loss: 0.6100181341171265 \n",
      "     Training Step: 239 Training Loss: 0.61815345287323 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6183132529258728 \n",
      "     Validation Step: 1 Validation Loss: 0.6100983023643494 \n",
      "     Validation Step: 2 Validation Loss: 0.6160669922828674 \n",
      "     Validation Step: 3 Validation Loss: 0.6136417984962463 \n",
      "     Validation Step: 4 Validation Loss: 0.6133004426956177 \n",
      "     Validation Step: 5 Validation Loss: 0.6118877530097961 \n",
      "     Validation Step: 6 Validation Loss: 0.6121338605880737 \n",
      "     Validation Step: 7 Validation Loss: 0.6184040307998657 \n",
      "     Validation Step: 8 Validation Loss: 0.6149104833602905 \n",
      "     Validation Step: 9 Validation Loss: 0.6128295063972473 \n",
      "     Validation Step: 10 Validation Loss: 0.6156533360481262 \n",
      "     Validation Step: 11 Validation Loss: 0.6111652255058289 \n",
      "     Validation Step: 12 Validation Loss: 0.6101234555244446 \n",
      "     Validation Step: 13 Validation Loss: 0.617067277431488 \n",
      "     Validation Step: 14 Validation Loss: 0.6115536093711853 \n",
      "     Validation Step: 15 Validation Loss: 0.614124059677124 \n",
      "     Validation Step: 16 Validation Loss: 0.6145934462547302 \n",
      "     Validation Step: 17 Validation Loss: 0.6111781001091003 \n",
      "     Validation Step: 18 Validation Loss: 0.6181187033653259 \n",
      "     Validation Step: 19 Validation Loss: 0.6105160713195801 \n",
      "     Validation Step: 20 Validation Loss: 0.6129975914955139 \n",
      "     Validation Step: 21 Validation Loss: 0.6101561188697815 \n",
      "     Validation Step: 22 Validation Loss: 0.6146575212478638 \n",
      "     Validation Step: 23 Validation Loss: 0.6156172752380371 \n",
      "     Validation Step: 24 Validation Loss: 0.6148624420166016 \n",
      "     Validation Step: 25 Validation Loss: 0.6162968277931213 \n",
      "     Validation Step: 26 Validation Loss: 0.617351770401001 \n",
      "     Validation Step: 27 Validation Loss: 0.6106305122375488 \n",
      "     Validation Step: 28 Validation Loss: 0.613686740398407 \n",
      "     Validation Step: 29 Validation Loss: 0.6185583472251892 \n",
      "     Validation Step: 30 Validation Loss: 0.6153524518013 \n",
      "     Validation Step: 31 Validation Loss: 0.6141427755355835 \n",
      "     Validation Step: 32 Validation Loss: 0.6152522563934326 \n",
      "     Validation Step: 33 Validation Loss: 0.6142573952674866 \n",
      "     Validation Step: 34 Validation Loss: 0.6176335215568542 \n",
      "     Validation Step: 35 Validation Loss: 0.6145405769348145 \n",
      "     Validation Step: 36 Validation Loss: 0.6075025796890259 \n",
      "     Validation Step: 37 Validation Loss: 0.6116387248039246 \n",
      "     Validation Step: 38 Validation Loss: 0.615077555179596 \n",
      "     Validation Step: 39 Validation Loss: 0.6142184138298035 \n",
      "     Validation Step: 40 Validation Loss: 0.610476016998291 \n",
      "     Validation Step: 41 Validation Loss: 0.6177588701248169 \n",
      "     Validation Step: 42 Validation Loss: 0.6158590912818909 \n",
      "     Validation Step: 43 Validation Loss: 0.6137035489082336 \n",
      "     Validation Step: 44 Validation Loss: 0.6185296177864075 \n",
      "Epoch: 55\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6129639744758606 \n",
      "     Training Step: 1 Training Loss: 0.6151372790336609 \n",
      "     Training Step: 2 Training Loss: 0.6125245690345764 \n",
      "     Training Step: 3 Training Loss: 0.6100990772247314 \n",
      "     Training Step: 4 Training Loss: 0.6148045063018799 \n",
      "     Training Step: 5 Training Loss: 0.6157764792442322 \n",
      "     Training Step: 6 Training Loss: 0.6182960867881775 \n",
      "     Training Step: 7 Training Loss: 0.6137964129447937 \n",
      "     Training Step: 8 Training Loss: 0.6115939021110535 \n",
      "     Training Step: 9 Training Loss: 0.6102223992347717 \n",
      "     Training Step: 10 Training Loss: 0.6126478314399719 \n",
      "     Training Step: 11 Training Loss: 0.6120452880859375 \n",
      "     Training Step: 12 Training Loss: 0.6123189926147461 \n",
      "     Training Step: 13 Training Loss: 0.6147186756134033 \n",
      "     Training Step: 14 Training Loss: 0.6105684638023376 \n",
      "     Training Step: 15 Training Loss: 0.6154595613479614 \n",
      "     Training Step: 16 Training Loss: 0.6093193888664246 \n",
      "     Training Step: 17 Training Loss: 0.6162819862365723 \n",
      "     Training Step: 18 Training Loss: 0.6127752661705017 \n",
      "     Training Step: 19 Training Loss: 0.6177546381950378 \n",
      "     Training Step: 20 Training Loss: 0.6150808930397034 \n",
      "     Training Step: 21 Training Loss: 0.6158432364463806 \n",
      "     Training Step: 22 Training Loss: 0.6171608567237854 \n",
      "     Training Step: 23 Training Loss: 0.6132898926734924 \n",
      "     Training Step: 24 Training Loss: 0.6154709458351135 \n",
      "     Training Step: 25 Training Loss: 0.6166393756866455 \n",
      "     Training Step: 26 Training Loss: 0.6145054697990417 \n",
      "     Training Step: 27 Training Loss: 0.6166977286338806 \n",
      "     Training Step: 28 Training Loss: 0.6181082129478455 \n",
      "     Training Step: 29 Training Loss: 0.6132517457008362 \n",
      "     Training Step: 30 Training Loss: 0.6157956123352051 \n",
      "     Training Step: 31 Training Loss: 0.6128719449043274 \n",
      "     Training Step: 32 Training Loss: 0.6149330735206604 \n",
      "     Training Step: 33 Training Loss: 0.6140217781066895 \n",
      "     Training Step: 34 Training Loss: 0.6182748079299927 \n",
      "     Training Step: 35 Training Loss: 0.6154245138168335 \n",
      "     Training Step: 36 Training Loss: 0.6194462776184082 \n",
      "     Training Step: 37 Training Loss: 0.6153975129127502 \n",
      "     Training Step: 38 Training Loss: 0.6147288084030151 \n",
      "     Training Step: 39 Training Loss: 0.6106308102607727 \n",
      "     Training Step: 40 Training Loss: 0.6137465834617615 \n",
      "     Training Step: 41 Training Loss: 0.6131429076194763 \n",
      "     Training Step: 42 Training Loss: 0.6171301603317261 \n",
      "     Training Step: 43 Training Loss: 0.6147302985191345 \n",
      "     Training Step: 44 Training Loss: 0.6097503304481506 \n",
      "     Training Step: 45 Training Loss: 0.614711582660675 \n",
      "     Training Step: 46 Training Loss: 0.6210035085678101 \n",
      "     Training Step: 47 Training Loss: 0.6110458970069885 \n",
      "     Training Step: 48 Training Loss: 0.6139388084411621 \n",
      "     Training Step: 49 Training Loss: 0.6115537285804749 \n",
      "     Training Step: 50 Training Loss: 0.6186589002609253 \n",
      "     Training Step: 51 Training Loss: 0.615431547164917 \n",
      "     Training Step: 52 Training Loss: 0.6144745945930481 \n",
      "     Training Step: 53 Training Loss: 0.6152795553207397 \n",
      "     Training Step: 54 Training Loss: 0.6166408658027649 \n",
      "     Training Step: 55 Training Loss: 0.6106732487678528 \n",
      "     Training Step: 56 Training Loss: 0.6139702796936035 \n",
      "     Training Step: 57 Training Loss: 0.615529477596283 \n",
      "     Training Step: 58 Training Loss: 0.6180685758590698 \n",
      "     Training Step: 59 Training Loss: 0.6115404963493347 \n",
      "     Training Step: 60 Training Loss: 0.6141676902770996 \n",
      "     Training Step: 61 Training Loss: 0.6127140522003174 \n",
      "     Training Step: 62 Training Loss: 0.610062301158905 \n",
      "     Training Step: 63 Training Loss: 0.6202911734580994 \n",
      "     Training Step: 64 Training Loss: 0.6136250495910645 \n",
      "     Training Step: 65 Training Loss: 0.6133297085762024 \n",
      "     Training Step: 66 Training Loss: 0.6100058555603027 \n",
      "     Training Step: 67 Training Loss: 0.6118173003196716 \n",
      "     Training Step: 68 Training Loss: 0.6174913644790649 \n",
      "     Training Step: 69 Training Loss: 0.6154471635818481 \n",
      "     Training Step: 70 Training Loss: 0.6166670322418213 \n",
      "     Training Step: 71 Training Loss: 0.6104642152786255 \n",
      "     Training Step: 72 Training Loss: 0.6167576909065247 \n",
      "     Training Step: 73 Training Loss: 0.6180785894393921 \n",
      "     Training Step: 74 Training Loss: 0.6098520755767822 \n",
      "     Training Step: 75 Training Loss: 0.6167124509811401 \n",
      "     Training Step: 76 Training Loss: 0.6111984252929688 \n",
      "     Training Step: 77 Training Loss: 0.6160426735877991 \n",
      "     Training Step: 78 Training Loss: 0.6118541359901428 \n",
      "     Training Step: 79 Training Loss: 0.6121740341186523 \n",
      "     Training Step: 80 Training Loss: 0.6160025596618652 \n",
      "     Training Step: 81 Training Loss: 0.6144382953643799 \n",
      "     Training Step: 82 Training Loss: 0.6101095080375671 \n",
      "     Training Step: 83 Training Loss: 0.6171043515205383 \n",
      "     Training Step: 84 Training Loss: 0.610420286655426 \n",
      "     Training Step: 85 Training Loss: 0.6177536845207214 \n",
      "     Training Step: 86 Training Loss: 0.615626335144043 \n",
      "     Training Step: 87 Training Loss: 0.6142158508300781 \n",
      "     Training Step: 88 Training Loss: 0.6116721034049988 \n",
      "     Training Step: 89 Training Loss: 0.6155475378036499 \n",
      "     Training Step: 90 Training Loss: 0.6117261648178101 \n",
      "     Training Step: 91 Training Loss: 0.610715389251709 \n",
      "     Training Step: 92 Training Loss: 0.6162404417991638 \n",
      "     Training Step: 93 Training Loss: 0.6114205121994019 \n",
      "     Training Step: 94 Training Loss: 0.6142697930335999 \n",
      "     Training Step: 95 Training Loss: 0.6121753454208374 \n",
      "     Training Step: 96 Training Loss: 0.6118468046188354 \n",
      "     Training Step: 97 Training Loss: 0.6116304993629456 \n",
      "     Training Step: 98 Training Loss: 0.6128999590873718 \n",
      "     Training Step: 99 Training Loss: 0.6107116341590881 \n",
      "     Training Step: 100 Training Loss: 0.6157652139663696 \n",
      "     Training Step: 101 Training Loss: 0.6097015738487244 \n",
      "     Training Step: 102 Training Loss: 0.6153215169906616 \n",
      "     Training Step: 103 Training Loss: 0.6164535880088806 \n",
      "     Training Step: 104 Training Loss: 0.6168383359909058 \n",
      "     Training Step: 105 Training Loss: 0.6177039742469788 \n",
      "     Training Step: 106 Training Loss: 0.6126574277877808 \n",
      "     Training Step: 107 Training Loss: 0.6153106093406677 \n",
      "     Training Step: 108 Training Loss: 0.611934244632721 \n",
      "     Training Step: 109 Training Loss: 0.613534688949585 \n",
      "     Training Step: 110 Training Loss: 0.6172693371772766 \n",
      "     Training Step: 111 Training Loss: 0.611577033996582 \n",
      "     Training Step: 112 Training Loss: 0.6144453287124634 \n",
      "     Training Step: 113 Training Loss: 0.611217200756073 \n",
      "     Training Step: 114 Training Loss: 0.6176497936248779 \n",
      "     Training Step: 115 Training Loss: 0.61334228515625 \n",
      "     Training Step: 116 Training Loss: 0.6125417947769165 \n",
      "     Training Step: 117 Training Loss: 0.6135056018829346 \n",
      "     Training Step: 118 Training Loss: 0.6172260642051697 \n",
      "     Training Step: 119 Training Loss: 0.6178230047225952 \n",
      "     Training Step: 120 Training Loss: 0.6168239712715149 \n",
      "     Training Step: 121 Training Loss: 0.6136860847473145 \n",
      "     Training Step: 122 Training Loss: 0.6152650713920593 \n",
      "     Training Step: 123 Training Loss: 0.6115443706512451 \n",
      "     Training Step: 124 Training Loss: 0.6136471033096313 \n",
      "     Training Step: 125 Training Loss: 0.6154190301895142 \n",
      "     Training Step: 126 Training Loss: 0.6172246932983398 \n",
      "     Training Step: 127 Training Loss: 0.6146576404571533 \n",
      "     Training Step: 128 Training Loss: 0.614783525466919 \n",
      "     Training Step: 129 Training Loss: 0.6147163510322571 \n",
      "     Training Step: 130 Training Loss: 0.6153128147125244 \n",
      "     Training Step: 131 Training Loss: 0.6147335171699524 \n",
      "     Training Step: 132 Training Loss: 0.614346444606781 \n",
      "     Training Step: 133 Training Loss: 0.6196421980857849 \n",
      "     Training Step: 134 Training Loss: 0.6135099530220032 \n",
      "     Training Step: 135 Training Loss: 0.6121566891670227 \n",
      "     Training Step: 136 Training Loss: 0.6106428503990173 \n",
      "     Training Step: 137 Training Loss: 0.613477349281311 \n",
      "     Training Step: 138 Training Loss: 0.6139578223228455 \n",
      "     Training Step: 139 Training Loss: 0.6141440272331238 \n",
      "     Training Step: 140 Training Loss: 0.6108991503715515 \n",
      "     Training Step: 141 Training Loss: 0.6132920980453491 \n",
      "     Training Step: 142 Training Loss: 0.6138745546340942 \n",
      "     Training Step: 143 Training Loss: 0.6112015247344971 \n",
      "     Training Step: 144 Training Loss: 0.614473283290863 \n",
      "     Training Step: 145 Training Loss: 0.6167138814926147 \n",
      "     Training Step: 146 Training Loss: 0.6160939335823059 \n",
      "     Training Step: 147 Training Loss: 0.6168374419212341 \n",
      "     Training Step: 148 Training Loss: 0.6125609278678894 \n",
      "     Training Step: 149 Training Loss: 0.6116824150085449 \n",
      "     Training Step: 150 Training Loss: 0.6122655272483826 \n",
      "     Training Step: 151 Training Loss: 0.6122390031814575 \n",
      "     Training Step: 152 Training Loss: 0.6164860725402832 \n",
      "     Training Step: 153 Training Loss: 0.6184452772140503 \n",
      "     Training Step: 154 Training Loss: 0.6143893003463745 \n",
      "     Training Step: 155 Training Loss: 0.6131150126457214 \n",
      "     Training Step: 156 Training Loss: 0.612339437007904 \n",
      "     Training Step: 157 Training Loss: 0.6149217486381531 \n",
      "     Training Step: 158 Training Loss: 0.6128304600715637 \n",
      "     Training Step: 159 Training Loss: 0.6118566989898682 \n",
      "     Training Step: 160 Training Loss: 0.613560140132904 \n",
      "     Training Step: 161 Training Loss: 0.6168519854545593 \n",
      "     Training Step: 162 Training Loss: 0.6185322999954224 \n",
      "     Training Step: 163 Training Loss: 0.6169039011001587 \n",
      "     Training Step: 164 Training Loss: 0.6107590198516846 \n",
      "     Training Step: 165 Training Loss: 0.6168333292007446 \n",
      "     Training Step: 166 Training Loss: 0.6184574961662292 \n",
      "     Training Step: 167 Training Loss: 0.6124451160430908 \n",
      "     Training Step: 168 Training Loss: 0.6152002811431885 \n",
      "     Training Step: 169 Training Loss: 0.6121582984924316 \n",
      "     Training Step: 170 Training Loss: 0.6132321357727051 \n",
      "     Training Step: 171 Training Loss: 0.6124022006988525 \n",
      "     Training Step: 172 Training Loss: 0.6117915511131287 \n",
      "     Training Step: 173 Training Loss: 0.6147447824478149 \n",
      "     Training Step: 174 Training Loss: 0.613784670829773 \n",
      "     Training Step: 175 Training Loss: 0.6165375709533691 \n",
      "     Training Step: 176 Training Loss: 0.6145322322845459 \n",
      "     Training Step: 177 Training Loss: 0.6124170422554016 \n",
      "     Training Step: 178 Training Loss: 0.6140478849411011 \n",
      "     Training Step: 179 Training Loss: 0.6149942874908447 \n",
      "     Training Step: 180 Training Loss: 0.6149847507476807 \n",
      "     Training Step: 181 Training Loss: 0.6189048886299133 \n",
      "     Training Step: 182 Training Loss: 0.6116438508033752 \n",
      "     Training Step: 183 Training Loss: 0.6140635013580322 \n",
      "     Training Step: 184 Training Loss: 0.6167523264884949 \n",
      "     Training Step: 185 Training Loss: 0.6115084290504456 \n",
      "     Training Step: 186 Training Loss: 0.6146153807640076 \n",
      "     Training Step: 187 Training Loss: 0.6114535331726074 \n",
      "     Training Step: 188 Training Loss: 0.6129447817802429 \n",
      "     Training Step: 189 Training Loss: 0.6179236173629761 \n",
      "     Training Step: 190 Training Loss: 0.6164203882217407 \n",
      "     Training Step: 191 Training Loss: 0.6128877997398376 \n",
      "     Training Step: 192 Training Loss: 0.6133367419242859 \n",
      "     Training Step: 193 Training Loss: 0.6142439246177673 \n",
      "     Training Step: 194 Training Loss: 0.610167384147644 \n",
      "     Training Step: 195 Training Loss: 0.614173412322998 \n",
      "     Training Step: 196 Training Loss: 0.6125485897064209 \n",
      "     Training Step: 197 Training Loss: 0.6190071105957031 \n",
      "     Training Step: 198 Training Loss: 0.6155546307563782 \n",
      "     Training Step: 199 Training Loss: 0.6130788326263428 \n",
      "     Training Step: 200 Training Loss: 0.6155129075050354 \n",
      "     Training Step: 201 Training Loss: 0.6116994023323059 \n",
      "     Training Step: 202 Training Loss: 0.6131641864776611 \n",
      "     Training Step: 203 Training Loss: 0.6161394715309143 \n",
      "     Training Step: 204 Training Loss: 0.6082771420478821 \n",
      "     Training Step: 205 Training Loss: 0.6147341132164001 \n",
      "     Training Step: 206 Training Loss: 0.6093705296516418 \n",
      "     Training Step: 207 Training Loss: 0.6114130020141602 \n",
      "     Training Step: 208 Training Loss: 0.6147000193595886 \n",
      "     Training Step: 209 Training Loss: 0.6107391119003296 \n",
      "     Training Step: 210 Training Loss: 0.6200307011604309 \n",
      "     Training Step: 211 Training Loss: 0.6157809495925903 \n",
      "     Training Step: 212 Training Loss: 0.6119025945663452 \n",
      "     Training Step: 213 Training Loss: 0.6184062957763672 \n",
      "     Training Step: 214 Training Loss: 0.6129424571990967 \n",
      "     Training Step: 215 Training Loss: 0.609542727470398 \n",
      "     Training Step: 216 Training Loss: 0.619830846786499 \n",
      "     Training Step: 217 Training Loss: 0.6133280396461487 \n",
      "     Training Step: 218 Training Loss: 0.6136021018028259 \n",
      "     Training Step: 219 Training Loss: 0.6127927899360657 \n",
      "     Training Step: 220 Training Loss: 0.6152056455612183 \n",
      "     Training Step: 221 Training Loss: 0.6114758253097534 \n",
      "     Training Step: 222 Training Loss: 0.6157124042510986 \n",
      "     Training Step: 223 Training Loss: 0.617850124835968 \n",
      "     Training Step: 224 Training Loss: 0.618061900138855 \n",
      "     Training Step: 225 Training Loss: 0.6123315095901489 \n",
      "     Training Step: 226 Training Loss: 0.614631712436676 \n",
      "     Training Step: 227 Training Loss: 0.6151179075241089 \n",
      "     Training Step: 228 Training Loss: 0.6143198013305664 \n",
      "     Training Step: 229 Training Loss: 0.6175243854522705 \n",
      "     Training Step: 230 Training Loss: 0.6120947599411011 \n",
      "     Training Step: 231 Training Loss: 0.6147633194923401 \n",
      "     Training Step: 232 Training Loss: 0.6186356544494629 \n",
      "     Training Step: 233 Training Loss: 0.6133912205696106 \n",
      "     Training Step: 234 Training Loss: 0.6162113547325134 \n",
      "     Training Step: 235 Training Loss: 0.6107114553451538 \n",
      "     Training Step: 236 Training Loss: 0.6106285452842712 \n",
      "     Training Step: 237 Training Loss: 0.6143993735313416 \n",
      "     Training Step: 238 Training Loss: 0.6157974600791931 \n",
      "     Training Step: 239 Training Loss: 0.6141095161437988 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.61189866065979 \n",
      "     Validation Step: 1 Validation Loss: 0.6116543412208557 \n",
      "     Validation Step: 2 Validation Loss: 0.6129950881004333 \n",
      "     Validation Step: 3 Validation Loss: 0.6145440936088562 \n",
      "     Validation Step: 4 Validation Loss: 0.611176609992981 \n",
      "     Validation Step: 5 Validation Loss: 0.6170401573181152 \n",
      "     Validation Step: 6 Validation Loss: 0.6121479868888855 \n",
      "     Validation Step: 7 Validation Loss: 0.6105242967605591 \n",
      "     Validation Step: 8 Validation Loss: 0.6101711392402649 \n",
      "     Validation Step: 9 Validation Loss: 0.6115679740905762 \n",
      "     Validation Step: 10 Validation Loss: 0.6136677861213684 \n",
      "     Validation Step: 11 Validation Loss: 0.6145866513252258 \n",
      "     Validation Step: 12 Validation Loss: 0.6184974312782288 \n",
      "     Validation Step: 13 Validation Loss: 0.6182740926742554 \n",
      "     Validation Step: 14 Validation Loss: 0.6133100390434265 \n",
      "     Validation Step: 15 Validation Loss: 0.6158210039138794 \n",
      "     Validation Step: 16 Validation Loss: 0.6146498918533325 \n",
      "     Validation Step: 17 Validation Loss: 0.6106588840484619 \n",
      "     Validation Step: 18 Validation Loss: 0.616273820400238 \n",
      "     Validation Step: 19 Validation Loss: 0.6111955642700195 \n",
      "     Validation Step: 20 Validation Loss: 0.6128363013267517 \n",
      "     Validation Step: 21 Validation Loss: 0.6142131686210632 \n",
      "     Validation Step: 22 Validation Loss: 0.6177226901054382 \n",
      "     Validation Step: 23 Validation Loss: 0.617325484752655 \n",
      "     Validation Step: 24 Validation Loss: 0.6183735132217407 \n",
      "     Validation Step: 25 Validation Loss: 0.6104956269264221 \n",
      "     Validation Step: 26 Validation Loss: 0.6153422594070435 \n",
      "     Validation Step: 27 Validation Loss: 0.6101892590522766 \n",
      "     Validation Step: 28 Validation Loss: 0.6156327128410339 \n",
      "     Validation Step: 29 Validation Loss: 0.6136394739151001 \n",
      "     Validation Step: 30 Validation Loss: 0.6155951023101807 \n",
      "     Validation Step: 31 Validation Loss: 0.6148442029953003 \n",
      "     Validation Step: 32 Validation Loss: 0.615058183670044 \n",
      "     Validation Step: 33 Validation Loss: 0.6149148941040039 \n",
      "     Validation Step: 34 Validation Loss: 0.6075548529624939 \n",
      "     Validation Step: 35 Validation Loss: 0.6180934309959412 \n",
      "     Validation Step: 36 Validation Loss: 0.6160247921943665 \n",
      "     Validation Step: 37 Validation Loss: 0.6136842966079712 \n",
      "     Validation Step: 38 Validation Loss: 0.6152421236038208 \n",
      "     Validation Step: 39 Validation Loss: 0.6101405024528503 \n",
      "     Validation Step: 40 Validation Loss: 0.6142656207084656 \n",
      "     Validation Step: 41 Validation Loss: 0.6176226735115051 \n",
      "     Validation Step: 42 Validation Loss: 0.6141414046287537 \n",
      "     Validation Step: 43 Validation Loss: 0.6141321063041687 \n",
      "     Validation Step: 44 Validation Loss: 0.6185277700424194 \n",
      "Epoch: 56\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6152998805046082 \n",
      "     Training Step: 1 Training Loss: 0.6173999905586243 \n",
      "     Training Step: 2 Training Loss: 0.615962028503418 \n",
      "     Training Step: 3 Training Loss: 0.6135438680648804 \n",
      "     Training Step: 4 Training Loss: 0.6136854887008667 \n",
      "     Training Step: 5 Training Loss: 0.613073468208313 \n",
      "     Training Step: 6 Training Loss: 0.6160607933998108 \n",
      "     Training Step: 7 Training Loss: 0.6111716628074646 \n",
      "     Training Step: 8 Training Loss: 0.613347053527832 \n",
      "     Training Step: 9 Training Loss: 0.6115253567695618 \n",
      "     Training Step: 10 Training Loss: 0.6167204976081848 \n",
      "     Training Step: 11 Training Loss: 0.6180803775787354 \n",
      "     Training Step: 12 Training Loss: 0.6119638085365295 \n",
      "     Training Step: 13 Training Loss: 0.6125834584236145 \n",
      "     Training Step: 14 Training Loss: 0.6144347786903381 \n",
      "     Training Step: 15 Training Loss: 0.6147804260253906 \n",
      "     Training Step: 16 Training Loss: 0.6210628151893616 \n",
      "     Training Step: 17 Training Loss: 0.6121504306793213 \n",
      "     Training Step: 18 Training Loss: 0.6176989078521729 \n",
      "     Training Step: 19 Training Loss: 0.6171873211860657 \n",
      "     Training Step: 20 Training Loss: 0.6128671169281006 \n",
      "     Training Step: 21 Training Loss: 0.611928403377533 \n",
      "     Training Step: 22 Training Loss: 0.6188297867774963 \n",
      "     Training Step: 23 Training Loss: 0.6142141819000244 \n",
      "     Training Step: 24 Training Loss: 0.6143786311149597 \n",
      "     Training Step: 25 Training Loss: 0.615557849407196 \n",
      "     Training Step: 26 Training Loss: 0.6121889352798462 \n",
      "     Training Step: 27 Training Loss: 0.6094697713851929 \n",
      "     Training Step: 28 Training Loss: 0.6177732348442078 \n",
      "     Training Step: 29 Training Loss: 0.6144070625305176 \n",
      "     Training Step: 30 Training Loss: 0.6126465201377869 \n",
      "     Training Step: 31 Training Loss: 0.6166684031486511 \n",
      "     Training Step: 32 Training Loss: 0.613211452960968 \n",
      "     Training Step: 33 Training Loss: 0.6115685105323792 \n",
      "     Training Step: 34 Training Loss: 0.6105861663818359 \n",
      "     Training Step: 35 Training Loss: 0.6123924255371094 \n",
      "     Training Step: 36 Training Loss: 0.6137736439704895 \n",
      "     Training Step: 37 Training Loss: 0.6094242930412292 \n",
      "     Training Step: 38 Training Loss: 0.6156902313232422 \n",
      "     Training Step: 39 Training Loss: 0.611789882183075 \n",
      "     Training Step: 40 Training Loss: 0.6147042512893677 \n",
      "     Training Step: 41 Training Loss: 0.6121013164520264 \n",
      "     Training Step: 42 Training Loss: 0.6146859526634216 \n",
      "     Training Step: 43 Training Loss: 0.6131356954574585 \n",
      "     Training Step: 44 Training Loss: 0.6167441010475159 \n",
      "     Training Step: 45 Training Loss: 0.610899806022644 \n",
      "     Training Step: 46 Training Loss: 0.6103984117507935 \n",
      "     Training Step: 47 Training Loss: 0.6154204607009888 \n",
      "     Training Step: 48 Training Loss: 0.6106725931167603 \n",
      "     Training Step: 49 Training Loss: 0.6146889328956604 \n",
      "     Training Step: 50 Training Loss: 0.6104031801223755 \n",
      "     Training Step: 51 Training Loss: 0.6114760637283325 \n",
      "     Training Step: 52 Training Loss: 0.6187090873718262 \n",
      "     Training Step: 53 Training Loss: 0.614775538444519 \n",
      "     Training Step: 54 Training Loss: 0.6148845553398132 \n",
      "     Training Step: 55 Training Loss: 0.6174969673156738 \n",
      "     Training Step: 56 Training Loss: 0.6106730103492737 \n",
      "     Training Step: 57 Training Loss: 0.612213134765625 \n",
      "     Training Step: 58 Training Loss: 0.6181207299232483 \n",
      "     Training Step: 59 Training Loss: 0.6140931248664856 \n",
      "     Training Step: 60 Training Loss: 0.6167425513267517 \n",
      "     Training Step: 61 Training Loss: 0.6123203635215759 \n",
      "     Training Step: 62 Training Loss: 0.6151608824729919 \n",
      "     Training Step: 63 Training Loss: 0.617612361907959 \n",
      "     Training Step: 64 Training Loss: 0.615191638469696 \n",
      "     Training Step: 65 Training Loss: 0.6115085482597351 \n",
      "     Training Step: 66 Training Loss: 0.6153492331504822 \n",
      "     Training Step: 67 Training Loss: 0.6137582659721375 \n",
      "     Training Step: 68 Training Loss: 0.614955723285675 \n",
      "     Training Step: 69 Training Loss: 0.6169893741607666 \n",
      "     Training Step: 70 Training Loss: 0.6178321838378906 \n",
      "     Training Step: 71 Training Loss: 0.6147259473800659 \n",
      "     Training Step: 72 Training Loss: 0.6129891872406006 \n",
      "     Training Step: 73 Training Loss: 0.6176910996437073 \n",
      "     Training Step: 74 Training Loss: 0.6157712340354919 \n",
      "     Training Step: 75 Training Loss: 0.6141633987426758 \n",
      "     Training Step: 76 Training Loss: 0.6122432351112366 \n",
      "     Training Step: 77 Training Loss: 0.6092393398284912 \n",
      "     Training Step: 78 Training Loss: 0.6167546510696411 \n",
      "     Training Step: 79 Training Loss: 0.6132067441940308 \n",
      "     Training Step: 80 Training Loss: 0.6167329549789429 \n",
      "     Training Step: 81 Training Loss: 0.6146242618560791 \n",
      "     Training Step: 82 Training Loss: 0.6147732138633728 \n",
      "     Training Step: 83 Training Loss: 0.6162195205688477 \n",
      "     Training Step: 84 Training Loss: 0.6129037737846375 \n",
      "     Training Step: 85 Training Loss: 0.6133316159248352 \n",
      "     Training Step: 86 Training Loss: 0.6124054193496704 \n",
      "     Training Step: 87 Training Loss: 0.6129555702209473 \n",
      "     Training Step: 88 Training Loss: 0.6100465655326843 \n",
      "     Training Step: 89 Training Loss: 0.6137056350708008 \n",
      "     Training Step: 90 Training Loss: 0.6145166754722595 \n",
      "     Training Step: 91 Training Loss: 0.6147270202636719 \n",
      "     Training Step: 92 Training Loss: 0.6112223863601685 \n",
      "     Training Step: 93 Training Loss: 0.6199675798416138 \n",
      "     Training Step: 94 Training Loss: 0.6182453036308289 \n",
      "     Training Step: 95 Training Loss: 0.6136494278907776 \n",
      "     Training Step: 96 Training Loss: 0.6108472347259521 \n",
      "     Training Step: 97 Training Loss: 0.612169086933136 \n",
      "     Training Step: 98 Training Loss: 0.6145881414413452 \n",
      "     Training Step: 99 Training Loss: 0.6158533096313477 \n",
      "     Training Step: 100 Training Loss: 0.6146805882453918 \n",
      "     Training Step: 101 Training Loss: 0.6171106696128845 \n",
      "     Training Step: 102 Training Loss: 0.6178529262542725 \n",
      "     Training Step: 103 Training Loss: 0.6119012236595154 \n",
      "     Training Step: 104 Training Loss: 0.61554354429245 \n",
      "     Training Step: 105 Training Loss: 0.6098176836967468 \n",
      "     Training Step: 106 Training Loss: 0.6130082011222839 \n",
      "     Training Step: 107 Training Loss: 0.6168650388717651 \n",
      "     Training Step: 108 Training Loss: 0.6131880879402161 \n",
      "     Training Step: 109 Training Loss: 0.6195777654647827 \n",
      "     Training Step: 110 Training Loss: 0.6116601824760437 \n",
      "     Training Step: 111 Training Loss: 0.6156129240989685 \n",
      "     Training Step: 112 Training Loss: 0.6184872984886169 \n",
      "     Training Step: 113 Training Loss: 0.6196799874305725 \n",
      "     Training Step: 114 Training Loss: 0.6141089797019958 \n",
      "     Training Step: 115 Training Loss: 0.611846923828125 \n",
      "     Training Step: 116 Training Loss: 0.6158624887466431 \n",
      "     Training Step: 117 Training Loss: 0.6173068284988403 \n",
      "     Training Step: 118 Training Loss: 0.6133242845535278 \n",
      "     Training Step: 119 Training Loss: 0.6137773394584656 \n",
      "     Training Step: 120 Training Loss: 0.6122742891311646 \n",
      "     Training Step: 121 Training Loss: 0.6107650399208069 \n",
      "     Training Step: 122 Training Loss: 0.6112366914749146 \n",
      "     Training Step: 123 Training Loss: 0.6135240793228149 \n",
      "     Training Step: 124 Training Loss: 0.6144073605537415 \n",
      "     Training Step: 125 Training Loss: 0.6132408380508423 \n",
      "     Training Step: 126 Training Loss: 0.6128876209259033 \n",
      "     Training Step: 127 Training Loss: 0.6146165132522583 \n",
      "     Training Step: 128 Training Loss: 0.6141329407691956 \n",
      "     Training Step: 129 Training Loss: 0.6133381724357605 \n",
      "     Training Step: 130 Training Loss: 0.6107849478721619 \n",
      "     Training Step: 131 Training Loss: 0.616217851638794 \n",
      "     Training Step: 132 Training Loss: 0.6118674278259277 \n",
      "     Training Step: 133 Training Loss: 0.6140508055686951 \n",
      "     Training Step: 134 Training Loss: 0.617802083492279 \n",
      "     Training Step: 135 Training Loss: 0.6154342889785767 \n",
      "     Training Step: 136 Training Loss: 0.6106948852539062 \n",
      "     Training Step: 137 Training Loss: 0.6167290210723877 \n",
      "     Training Step: 138 Training Loss: 0.6164131760597229 \n",
      "     Training Step: 139 Training Loss: 0.6154334545135498 \n",
      "     Training Step: 140 Training Loss: 0.6110650897026062 \n",
      "     Training Step: 141 Training Loss: 0.6150795221328735 \n",
      "     Training Step: 142 Training Loss: 0.6134704947471619 \n",
      "     Training Step: 143 Training Loss: 0.6122507452964783 \n",
      "     Training Step: 144 Training Loss: 0.6153069138526917 \n",
      "     Training Step: 145 Training Loss: 0.6184056997299194 \n",
      "     Training Step: 146 Training Loss: 0.6135114431381226 \n",
      "     Training Step: 147 Training Loss: 0.612844705581665 \n",
      "     Training Step: 148 Training Loss: 0.6164113879203796 \n",
      "     Training Step: 149 Training Loss: 0.6114903688430786 \n",
      "     Training Step: 150 Training Loss: 0.6186053156852722 \n",
      "     Training Step: 151 Training Loss: 0.6161518096923828 \n",
      "     Training Step: 152 Training Loss: 0.6153095364570618 \n",
      "     Training Step: 153 Training Loss: 0.6139487028121948 \n",
      "     Training Step: 154 Training Loss: 0.6182110905647278 \n",
      "     Training Step: 155 Training Loss: 0.6115064024925232 \n",
      "     Training Step: 156 Training Loss: 0.6171474456787109 \n",
      "     Training Step: 157 Training Loss: 0.6116594672203064 \n",
      "     Training Step: 158 Training Loss: 0.6097047328948975 \n",
      "     Training Step: 159 Training Loss: 0.6154667735099792 \n",
      "     Training Step: 160 Training Loss: 0.6158393621444702 \n",
      "     Training Step: 161 Training Loss: 0.6148311495780945 \n",
      "     Training Step: 162 Training Loss: 0.6167324185371399 \n",
      "     Training Step: 163 Training Loss: 0.6163977980613708 \n",
      "     Training Step: 164 Training Loss: 0.6147897243499756 \n",
      "     Training Step: 165 Training Loss: 0.6147226095199585 \n",
      "     Training Step: 166 Training Loss: 0.6183841824531555 \n",
      "     Training Step: 167 Training Loss: 0.6167545914649963 \n",
      "     Training Step: 168 Training Loss: 0.6162331700325012 \n",
      "     Training Step: 169 Training Loss: 0.6150239109992981 \n",
      "     Training Step: 170 Training Loss: 0.612629234790802 \n",
      "     Training Step: 171 Training Loss: 0.6100984811782837 \n",
      "     Training Step: 172 Training Loss: 0.6153063774108887 \n",
      "     Training Step: 173 Training Loss: 0.6180978417396545 \n",
      "     Training Step: 174 Training Loss: 0.6101019382476807 \n",
      "     Training Step: 175 Training Loss: 0.6156113743782043 \n",
      "     Training Step: 176 Training Loss: 0.6168698668479919 \n",
      "     Training Step: 177 Training Loss: 0.6138421893119812 \n",
      "     Training Step: 178 Training Loss: 0.6139397621154785 \n",
      "     Training Step: 179 Training Loss: 0.6124874353408813 \n",
      "     Training Step: 180 Training Loss: 0.6158560514450073 \n",
      "     Training Step: 181 Training Loss: 0.6136690378189087 \n",
      "     Training Step: 182 Training Loss: 0.6168292164802551 \n",
      "     Training Step: 183 Training Loss: 0.6128095388412476 \n",
      "     Training Step: 184 Training Loss: 0.6145089268684387 \n",
      "     Training Step: 185 Training Loss: 0.6133034825325012 \n",
      "     Training Step: 186 Training Loss: 0.6118512749671936 \n",
      "     Training Step: 187 Training Loss: 0.6117196083068848 \n",
      "     Training Step: 188 Training Loss: 0.61142897605896 \n",
      "     Training Step: 189 Training Loss: 0.6154614090919495 \n",
      "     Training Step: 190 Training Loss: 0.6082366704940796 \n",
      "     Training Step: 191 Training Loss: 0.6125248670578003 \n",
      "     Training Step: 192 Training Loss: 0.6100485324859619 \n",
      "     Training Step: 193 Training Loss: 0.617012619972229 \n",
      "     Training Step: 194 Training Loss: 0.6118558645248413 \n",
      "     Training Step: 195 Training Loss: 0.6134009957313538 \n",
      "     Training Step: 196 Training Loss: 0.6157211065292358 \n",
      "     Training Step: 197 Training Loss: 0.6116337180137634 \n",
      "     Training Step: 198 Training Loss: 0.6127358675003052 \n",
      "     Training Step: 199 Training Loss: 0.6144734621047974 \n",
      "     Training Step: 200 Training Loss: 0.6114150285720825 \n",
      "     Training Step: 201 Training Loss: 0.6104913949966431 \n",
      "     Training Step: 202 Training Loss: 0.6123989224433899 \n",
      "     Training Step: 203 Training Loss: 0.6105733513832092 \n",
      "     Training Step: 204 Training Loss: 0.6097575426101685 \n",
      "     Training Step: 205 Training Loss: 0.6101877689361572 \n",
      "     Training Step: 206 Training Loss: 0.6143465042114258 \n",
      "     Training Step: 207 Training Loss: 0.6134740710258484 \n",
      "     Training Step: 208 Training Loss: 0.6128820776939392 \n",
      "     Training Step: 209 Training Loss: 0.6151211261749268 \n",
      "     Training Step: 210 Training Loss: 0.612556517124176 \n",
      "     Training Step: 211 Training Loss: 0.6140328049659729 \n",
      "     Training Step: 212 Training Loss: 0.6142564415931702 \n",
      "     Training Step: 213 Training Loss: 0.6165128946304321 \n",
      "     Training Step: 214 Training Loss: 0.6181208491325378 \n",
      "     Training Step: 215 Training Loss: 0.6101856231689453 \n",
      "     Training Step: 216 Training Loss: 0.6188650727272034 \n",
      "     Training Step: 217 Training Loss: 0.6131806969642639 \n",
      "     Training Step: 218 Training Loss: 0.6197404861450195 \n",
      "     Training Step: 219 Training Loss: 0.6105721592903137 \n",
      "     Training Step: 220 Training Loss: 0.6151143908500671 \n",
      "     Training Step: 221 Training Loss: 0.6115834712982178 \n",
      "     Training Step: 222 Training Loss: 0.6120694875717163 \n",
      "     Training Step: 223 Training Loss: 0.6152896881103516 \n",
      "     Training Step: 224 Training Loss: 0.6171817183494568 \n",
      "     Training Step: 225 Training Loss: 0.6156982779502869 \n",
      "     Training Step: 226 Training Loss: 0.6160524487495422 \n",
      "     Training Step: 227 Training Loss: 0.6142250299453735 \n",
      "     Training Step: 228 Training Loss: 0.6144317984580994 \n",
      "     Training Step: 229 Training Loss: 0.6202424764633179 \n",
      "     Training Step: 230 Training Loss: 0.6142528653144836 \n",
      "     Training Step: 231 Training Loss: 0.6123121380805969 \n",
      "     Training Step: 232 Training Loss: 0.614938497543335 \n",
      "     Training Step: 233 Training Loss: 0.6185008883476257 \n",
      "     Training Step: 234 Training Loss: 0.611666202545166 \n",
      "     Training Step: 235 Training Loss: 0.6167960166931152 \n",
      "     Training Step: 236 Training Loss: 0.6146339774131775 \n",
      "     Training Step: 237 Training Loss: 0.6139063835144043 \n",
      "     Training Step: 238 Training Loss: 0.6154462099075317 \n",
      "     Training Step: 239 Training Loss: 0.6116471886634827 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6075413823127747 \n",
      "     Validation Step: 1 Validation Loss: 0.6115661263465881 \n",
      "     Validation Step: 2 Validation Loss: 0.6183630228042603 \n",
      "     Validation Step: 3 Validation Loss: 0.6111910343170166 \n",
      "     Validation Step: 4 Validation Loss: 0.6185259222984314 \n",
      "     Validation Step: 5 Validation Loss: 0.6150493621826172 \n",
      "     Validation Step: 6 Validation Loss: 0.6148537993431091 \n",
      "     Validation Step: 7 Validation Loss: 0.6116564273834229 \n",
      "     Validation Step: 8 Validation Loss: 0.6153252124786377 \n",
      "     Validation Step: 9 Validation Loss: 0.6160269379615784 \n",
      "     Validation Step: 10 Validation Loss: 0.6101543307304382 \n",
      "     Validation Step: 11 Validation Loss: 0.6185004115104675 \n",
      "     Validation Step: 12 Validation Loss: 0.6133109331130981 \n",
      "     Validation Step: 13 Validation Loss: 0.6145445704460144 \n",
      "     Validation Step: 14 Validation Loss: 0.6121414303779602 \n",
      "     Validation Step: 15 Validation Loss: 0.6158241033554077 \n",
      "     Validation Step: 16 Validation Loss: 0.615599513053894 \n",
      "     Validation Step: 17 Validation Loss: 0.6118966937065125 \n",
      "     Validation Step: 18 Validation Loss: 0.6173316240310669 \n",
      "     Validation Step: 19 Validation Loss: 0.613653302192688 \n",
      "     Validation Step: 20 Validation Loss: 0.6101189851760864 \n",
      "     Validation Step: 21 Validation Loss: 0.6136830449104309 \n",
      "     Validation Step: 22 Validation Loss: 0.6149098873138428 \n",
      "     Validation Step: 23 Validation Loss: 0.6128427386283875 \n",
      "     Validation Step: 24 Validation Loss: 0.6176199316978455 \n",
      "     Validation Step: 25 Validation Loss: 0.6142117977142334 \n",
      "     Validation Step: 26 Validation Loss: 0.6152438521385193 \n",
      "     Validation Step: 27 Validation Loss: 0.614129900932312 \n",
      "     Validation Step: 28 Validation Loss: 0.6104946136474609 \n",
      "     Validation Step: 29 Validation Loss: 0.6145873069763184 \n",
      "     Validation Step: 30 Validation Loss: 0.6156330108642578 \n",
      "     Validation Step: 31 Validation Loss: 0.6106568574905396 \n",
      "     Validation Step: 32 Validation Loss: 0.6180843710899353 \n",
      "     Validation Step: 33 Validation Loss: 0.6105272769927979 \n",
      "     Validation Step: 34 Validation Loss: 0.6162688136100769 \n",
      "     Validation Step: 35 Validation Loss: 0.6136627793312073 \n",
      "     Validation Step: 36 Validation Loss: 0.6170390844345093 \n",
      "     Validation Step: 37 Validation Loss: 0.6177211403846741 \n",
      "     Validation Step: 38 Validation Loss: 0.6182644367218018 \n",
      "     Validation Step: 39 Validation Loss: 0.614644467830658 \n",
      "     Validation Step: 40 Validation Loss: 0.6101928949356079 \n",
      "     Validation Step: 41 Validation Loss: 0.6129935383796692 \n",
      "     Validation Step: 42 Validation Loss: 0.6142666339874268 \n",
      "     Validation Step: 43 Validation Loss: 0.6141330599784851 \n",
      "     Validation Step: 44 Validation Loss: 0.6111719012260437 \n",
      "Epoch: 57\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6114647388458252 \n",
      "     Training Step: 1 Training Loss: 0.6165007948875427 \n",
      "     Training Step: 2 Training Loss: 0.6154152154922485 \n",
      "     Training Step: 3 Training Loss: 0.6137833595275879 \n",
      "     Training Step: 4 Training Loss: 0.6146291494369507 \n",
      "     Training Step: 5 Training Loss: 0.6125572919845581 \n",
      "     Training Step: 6 Training Loss: 0.6124098300933838 \n",
      "     Training Step: 7 Training Loss: 0.6100344657897949 \n",
      "     Training Step: 8 Training Loss: 0.6185300350189209 \n",
      "     Training Step: 9 Training Loss: 0.6122918128967285 \n",
      "     Training Step: 10 Training Loss: 0.6170063018798828 \n",
      "     Training Step: 11 Training Loss: 0.6139379739761353 \n",
      "     Training Step: 12 Training Loss: 0.6162128448486328 \n",
      "     Training Step: 13 Training Loss: 0.6209204792976379 \n",
      "     Training Step: 14 Training Loss: 0.6162515878677368 \n",
      "     Training Step: 15 Training Loss: 0.6122905611991882 \n",
      "     Training Step: 16 Training Loss: 0.6147592067718506 \n",
      "     Training Step: 17 Training Loss: 0.6098706722259521 \n",
      "     Training Step: 18 Training Loss: 0.6130318641662598 \n",
      "     Training Step: 19 Training Loss: 0.6165376901626587 \n",
      "     Training Step: 20 Training Loss: 0.6116710901260376 \n",
      "     Training Step: 21 Training Loss: 0.6155251264572144 \n",
      "     Training Step: 22 Training Loss: 0.6109254360198975 \n",
      "     Training Step: 23 Training Loss: 0.6128314733505249 \n",
      "     Training Step: 24 Training Loss: 0.6181427240371704 \n",
      "     Training Step: 25 Training Loss: 0.6126859188079834 \n",
      "     Training Step: 26 Training Loss: 0.6097294092178345 \n",
      "     Training Step: 27 Training Loss: 0.6114111542701721 \n",
      "     Training Step: 28 Training Loss: 0.6154279708862305 \n",
      "     Training Step: 29 Training Loss: 0.615746259689331 \n",
      "     Training Step: 30 Training Loss: 0.613371729850769 \n",
      "     Training Step: 31 Training Loss: 0.61417156457901 \n",
      "     Training Step: 32 Training Loss: 0.6157869100570679 \n",
      "     Training Step: 33 Training Loss: 0.6147372722625732 \n",
      "     Training Step: 34 Training Loss: 0.6108258366584778 \n",
      "     Training Step: 35 Training Loss: 0.6177156567573547 \n",
      "     Training Step: 36 Training Loss: 0.6145059466362 \n",
      "     Training Step: 37 Training Loss: 0.6155099868774414 \n",
      "     Training Step: 38 Training Loss: 0.6114458441734314 \n",
      "     Training Step: 39 Training Loss: 0.6147462129592896 \n",
      "     Training Step: 40 Training Loss: 0.6127186417579651 \n",
      "     Training Step: 41 Training Loss: 0.6171521544456482 \n",
      "     Training Step: 42 Training Loss: 0.6165000200271606 \n",
      "     Training Step: 43 Training Loss: 0.6188955307006836 \n",
      "     Training Step: 44 Training Loss: 0.6115243434906006 \n",
      "     Training Step: 45 Training Loss: 0.6123404502868652 \n",
      "     Training Step: 46 Training Loss: 0.6135282516479492 \n",
      "     Training Step: 47 Training Loss: 0.6111881136894226 \n",
      "     Training Step: 48 Training Loss: 0.6125382781028748 \n",
      "     Training Step: 49 Training Loss: 0.6124253273010254 \n",
      "     Training Step: 50 Training Loss: 0.6147273778915405 \n",
      "     Training Step: 51 Training Loss: 0.611820638179779 \n",
      "     Training Step: 52 Training Loss: 0.6157181859016418 \n",
      "     Training Step: 53 Training Loss: 0.6144052743911743 \n",
      "     Training Step: 54 Training Loss: 0.6116053462028503 \n",
      "     Training Step: 55 Training Loss: 0.6149742007255554 \n",
      "     Training Step: 56 Training Loss: 0.6197726130485535 \n",
      "     Training Step: 57 Training Loss: 0.6102281808853149 \n",
      "     Training Step: 58 Training Loss: 0.6133104562759399 \n",
      "     Training Step: 59 Training Loss: 0.6127790808677673 \n",
      "     Training Step: 60 Training Loss: 0.615344226360321 \n",
      "     Training Step: 61 Training Loss: 0.6108934879302979 \n",
      "     Training Step: 62 Training Loss: 0.6115496158599854 \n",
      "     Training Step: 63 Training Loss: 0.6185498237609863 \n",
      "     Training Step: 64 Training Loss: 0.6147633194923401 \n",
      "     Training Step: 65 Training Loss: 0.6167793869972229 \n",
      "     Training Step: 66 Training Loss: 0.6121647953987122 \n",
      "     Training Step: 67 Training Loss: 0.6166184544563293 \n",
      "     Training Step: 68 Training Loss: 0.6167877912521362 \n",
      "     Training Step: 69 Training Loss: 0.6107233762741089 \n",
      "     Training Step: 70 Training Loss: 0.6173844337463379 \n",
      "     Training Step: 71 Training Loss: 0.6118724942207336 \n",
      "     Training Step: 72 Training Loss: 0.6181180477142334 \n",
      "     Training Step: 73 Training Loss: 0.6100895404815674 \n",
      "     Training Step: 74 Training Loss: 0.61673504114151 \n",
      "     Training Step: 75 Training Loss: 0.6145064830780029 \n",
      "     Training Step: 76 Training Loss: 0.6161744594573975 \n",
      "     Training Step: 77 Training Loss: 0.6115314960479736 \n",
      "     Training Step: 78 Training Loss: 0.6132855415344238 \n",
      "     Training Step: 79 Training Loss: 0.6146294474601746 \n",
      "     Training Step: 80 Training Loss: 0.6175571084022522 \n",
      "     Training Step: 81 Training Loss: 0.6115820407867432 \n",
      "     Training Step: 82 Training Loss: 0.6180378794670105 \n",
      "     Training Step: 83 Training Loss: 0.6146717667579651 \n",
      "     Training Step: 84 Training Loss: 0.6176084280014038 \n",
      "     Training Step: 85 Training Loss: 0.6182409524917603 \n",
      "     Training Step: 86 Training Loss: 0.6153210997581482 \n",
      "     Training Step: 87 Training Loss: 0.6143858432769775 \n",
      "     Training Step: 88 Training Loss: 0.6148187518119812 \n",
      "     Training Step: 89 Training Loss: 0.6151300668716431 \n",
      "     Training Step: 90 Training Loss: 0.6097591519355774 \n",
      "     Training Step: 91 Training Loss: 0.6101768016815186 \n",
      "     Training Step: 92 Training Loss: 0.6173285841941833 \n",
      "     Training Step: 93 Training Loss: 0.6133217811584473 \n",
      "     Training Step: 94 Training Loss: 0.6159674525260925 \n",
      "     Training Step: 95 Training Loss: 0.6106396317481995 \n",
      "     Training Step: 96 Training Loss: 0.6139269471168518 \n",
      "     Training Step: 97 Training Loss: 0.6154451370239258 \n",
      "     Training Step: 98 Training Loss: 0.6116572618484497 \n",
      "     Training Step: 99 Training Loss: 0.6149551868438721 \n",
      "     Training Step: 100 Training Loss: 0.6118593811988831 \n",
      "     Training Step: 101 Training Loss: 0.6155843734741211 \n",
      "     Training Step: 102 Training Loss: 0.6093894243240356 \n",
      "     Training Step: 103 Training Loss: 0.6189184188842773 \n",
      "     Training Step: 104 Training Loss: 0.6142153143882751 \n",
      "     Training Step: 105 Training Loss: 0.6115102171897888 \n",
      "     Training Step: 106 Training Loss: 0.6167411208152771 \n",
      "     Training Step: 107 Training Loss: 0.61041659116745 \n",
      "     Training Step: 108 Training Loss: 0.6129330396652222 \n",
      "     Training Step: 109 Training Loss: 0.6100519299507141 \n",
      "     Training Step: 110 Training Loss: 0.6179520487785339 \n",
      "     Training Step: 111 Training Loss: 0.615209698677063 \n",
      "     Training Step: 112 Training Loss: 0.6136255264282227 \n",
      "     Training Step: 113 Training Loss: 0.6125708222389221 \n",
      "     Training Step: 114 Training Loss: 0.6157200932502747 \n",
      "     Training Step: 115 Training Loss: 0.6153298616409302 \n",
      "     Training Step: 116 Training Loss: 0.6131447553634644 \n",
      "     Training Step: 117 Training Loss: 0.6128804087638855 \n",
      "     Training Step: 118 Training Loss: 0.6125488877296448 \n",
      "     Training Step: 119 Training Loss: 0.6159154772758484 \n",
      "     Training Step: 120 Training Loss: 0.6155957579612732 \n",
      "     Training Step: 121 Training Loss: 0.6158004999160767 \n",
      "     Training Step: 122 Training Loss: 0.616804301738739 \n",
      "     Training Step: 123 Training Loss: 0.6123329401016235 \n",
      "     Training Step: 124 Training Loss: 0.6155626773834229 \n",
      "     Training Step: 125 Training Loss: 0.6123071908950806 \n",
      "     Training Step: 126 Training Loss: 0.6138247847557068 \n",
      "     Training Step: 127 Training Loss: 0.6129124164581299 \n",
      "     Training Step: 128 Training Loss: 0.6132852435112 \n",
      "     Training Step: 129 Training Loss: 0.6131045818328857 \n",
      "     Training Step: 130 Training Loss: 0.6153261065483093 \n",
      "     Training Step: 131 Training Loss: 0.6120808720588684 \n",
      "     Training Step: 132 Training Loss: 0.6142613291740417 \n",
      "     Training Step: 133 Training Loss: 0.615097165107727 \n",
      "     Training Step: 134 Training Loss: 0.613503634929657 \n",
      "     Training Step: 135 Training Loss: 0.6139346361160278 \n",
      "     Training Step: 136 Training Loss: 0.6104108095169067 \n",
      "     Training Step: 137 Training Loss: 0.6160908341407776 \n",
      "     Training Step: 138 Training Loss: 0.6169170141220093 \n",
      "     Training Step: 139 Training Loss: 0.6131422519683838 \n",
      "     Training Step: 140 Training Loss: 0.6172023415565491 \n",
      "     Training Step: 141 Training Loss: 0.6112703680992126 \n",
      "     Training Step: 142 Training Loss: 0.6144943833351135 \n",
      "     Training Step: 143 Training Loss: 0.6140564680099487 \n",
      "     Training Step: 144 Training Loss: 0.6106211543083191 \n",
      "     Training Step: 145 Training Loss: 0.6132341623306274 \n",
      "     Training Step: 146 Training Loss: 0.6117159128189087 \n",
      "     Training Step: 147 Training Loss: 0.616802990436554 \n",
      "     Training Step: 148 Training Loss: 0.6106697916984558 \n",
      "     Training Step: 149 Training Loss: 0.6157445311546326 \n",
      "     Training Step: 150 Training Loss: 0.6115629076957703 \n",
      "     Training Step: 151 Training Loss: 0.613335132598877 \n",
      "     Training Step: 152 Training Loss: 0.6133125424385071 \n",
      "     Training Step: 153 Training Loss: 0.616796612739563 \n",
      "     Training Step: 154 Training Loss: 0.6171247363090515 \n",
      "     Training Step: 155 Training Loss: 0.6123957633972168 \n",
      "     Training Step: 156 Training Loss: 0.6136423945426941 \n",
      "     Training Step: 157 Training Loss: 0.6186020374298096 \n",
      "     Training Step: 158 Training Loss: 0.6111794114112854 \n",
      "     Training Step: 159 Training Loss: 0.6147579550743103 \n",
      "     Training Step: 160 Training Loss: 0.6121793389320374 \n",
      "     Training Step: 161 Training Loss: 0.6122429966926575 \n",
      "     Training Step: 162 Training Loss: 0.6149389147758484 \n",
      "     Training Step: 163 Training Loss: 0.6131970286369324 \n",
      "     Training Step: 164 Training Loss: 0.6194969415664673 \n",
      "     Training Step: 165 Training Loss: 0.6144309639930725 \n",
      "     Training Step: 166 Training Loss: 0.6136288642883301 \n",
      "     Training Step: 167 Training Loss: 0.614786684513092 \n",
      "     Training Step: 168 Training Loss: 0.6083818078041077 \n",
      "     Training Step: 169 Training Loss: 0.6178028583526611 \n",
      "     Training Step: 170 Training Loss: 0.6137780547142029 \n",
      "     Training Step: 171 Training Loss: 0.6129338145256042 \n",
      "     Training Step: 172 Training Loss: 0.6151074767112732 \n",
      "     Training Step: 173 Training Loss: 0.6100848913192749 \n",
      "     Training Step: 174 Training Loss: 0.6182597279548645 \n",
      "     Training Step: 175 Training Loss: 0.6196666359901428 \n",
      "     Training Step: 176 Training Loss: 0.6095783710479736 \n",
      "     Training Step: 177 Training Loss: 0.6166806221008301 \n",
      "     Training Step: 178 Training Loss: 0.6140282154083252 \n",
      "     Training Step: 179 Training Loss: 0.61076819896698 \n",
      "     Training Step: 180 Training Loss: 0.6118477582931519 \n",
      "     Training Step: 181 Training Loss: 0.6118531823158264 \n",
      "     Training Step: 182 Training Loss: 0.6137012839317322 \n",
      "     Training Step: 183 Training Loss: 0.6143168210983276 \n",
      "     Training Step: 184 Training Loss: 0.6154642701148987 \n",
      "     Training Step: 185 Training Loss: 0.6106989979743958 \n",
      "     Training Step: 186 Training Loss: 0.6142715215682983 \n",
      "     Training Step: 187 Training Loss: 0.6147041916847229 \n",
      "     Training Step: 188 Training Loss: 0.6092587113380432 \n",
      "     Training Step: 189 Training Loss: 0.6147842407226562 \n",
      "     Training Step: 190 Training Loss: 0.6118964552879333 \n",
      "     Training Step: 191 Training Loss: 0.6141501069068909 \n",
      "     Training Step: 192 Training Loss: 0.6116083860397339 \n",
      "     Training Step: 193 Training Loss: 0.6177988648414612 \n",
      "     Training Step: 194 Training Loss: 0.6138013005256653 \n",
      "     Training Step: 195 Training Loss: 0.6168039441108704 \n",
      "     Training Step: 196 Training Loss: 0.6157662272453308 \n",
      "     Training Step: 197 Training Loss: 0.614529550075531 \n",
      "     Training Step: 198 Training Loss: 0.6143535375595093 \n",
      "     Training Step: 199 Training Loss: 0.6186169981956482 \n",
      "     Training Step: 200 Training Loss: 0.6183111071586609 \n",
      "     Training Step: 201 Training Loss: 0.6184725165367126 \n",
      "     Training Step: 202 Training Loss: 0.6136259436607361 \n",
      "     Training Step: 203 Training Loss: 0.6201744675636292 \n",
      "     Training Step: 204 Training Loss: 0.6178027987480164 \n",
      "     Training Step: 205 Training Loss: 0.6163107752799988 \n",
      "     Training Step: 206 Training Loss: 0.6140937805175781 \n",
      "     Training Step: 207 Training Loss: 0.6107785105705261 \n",
      "     Training Step: 208 Training Loss: 0.6154702305793762 \n",
      "     Training Step: 209 Training Loss: 0.6125082969665527 \n",
      "     Training Step: 210 Training Loss: 0.6119104623794556 \n",
      "     Training Step: 211 Training Loss: 0.6154455542564392 \n",
      "     Training Step: 212 Training Loss: 0.6143895387649536 \n",
      "     Training Step: 213 Training Loss: 0.6167187094688416 \n",
      "     Training Step: 214 Training Loss: 0.614231526851654 \n",
      "     Training Step: 215 Training Loss: 0.6152917742729187 \n",
      "     Training Step: 216 Training Loss: 0.6146663427352905 \n",
      "     Training Step: 217 Training Loss: 0.6177228093147278 \n",
      "     Training Step: 218 Training Loss: 0.6141097545623779 \n",
      "     Training Step: 219 Training Loss: 0.6199657917022705 \n",
      "     Training Step: 220 Training Loss: 0.612822949886322 \n",
      "     Training Step: 221 Training Loss: 0.6129074692726135 \n",
      "     Training Step: 222 Training Loss: 0.6131026148796082 \n",
      "     Training Step: 223 Training Loss: 0.612116277217865 \n",
      "     Training Step: 224 Training Loss: 0.6171825528144836 \n",
      "     Training Step: 225 Training Loss: 0.610619843006134 \n",
      "     Training Step: 226 Training Loss: 0.6135231852531433 \n",
      "     Training Step: 227 Training Loss: 0.6146425604820251 \n",
      "     Training Step: 228 Training Loss: 0.6105185747146606 \n",
      "     Training Step: 229 Training Loss: 0.6149348020553589 \n",
      "     Training Step: 230 Training Loss: 0.6181570291519165 \n",
      "     Training Step: 231 Training Loss: 0.6160476207733154 \n",
      "     Training Step: 232 Training Loss: 0.6152143478393555 \n",
      "     Training Step: 233 Training Loss: 0.6164671182632446 \n",
      "     Training Step: 234 Training Loss: 0.6169174313545227 \n",
      "     Training Step: 235 Training Loss: 0.612307071685791 \n",
      "     Training Step: 236 Training Loss: 0.6117140054702759 \n",
      "     Training Step: 237 Training Loss: 0.6116383671760559 \n",
      "     Training Step: 238 Training Loss: 0.6135327219963074 \n",
      "     Training Step: 239 Training Loss: 0.616930365562439 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.614617109298706 \n",
      "     Validation Step: 1 Validation Loss: 0.6151056289672852 \n",
      "     Validation Step: 2 Validation Loss: 0.6141396760940552 \n",
      "     Validation Step: 3 Validation Loss: 0.6173728704452515 \n",
      "     Validation Step: 4 Validation Loss: 0.6185517907142639 \n",
      "     Validation Step: 5 Validation Loss: 0.610503077507019 \n",
      "     Validation Step: 6 Validation Loss: 0.6101633906364441 \n",
      "     Validation Step: 7 Validation Loss: 0.6145716905593872 \n",
      "     Validation Step: 8 Validation Loss: 0.6148721575737 \n",
      "     Validation Step: 9 Validation Loss: 0.6101852655410767 \n",
      "     Validation Step: 10 Validation Loss: 0.6181490421295166 \n",
      "     Validation Step: 11 Validation Loss: 0.6163298487663269 \n",
      "     Validation Step: 12 Validation Loss: 0.6105272173881531 \n",
      "     Validation Step: 13 Validation Loss: 0.6106481552124023 \n",
      "     Validation Step: 14 Validation Loss: 0.6101278066635132 \n",
      "     Validation Step: 15 Validation Loss: 0.6156598329544067 \n",
      "     Validation Step: 16 Validation Loss: 0.6142403483390808 \n",
      "     Validation Step: 17 Validation Loss: 0.6156782507896423 \n",
      "     Validation Step: 18 Validation Loss: 0.6146817803382874 \n",
      "     Validation Step: 19 Validation Loss: 0.6118873357772827 \n",
      "     Validation Step: 20 Validation Loss: 0.618343710899353 \n",
      "     Validation Step: 21 Validation Loss: 0.6130092144012451 \n",
      "     Validation Step: 22 Validation Loss: 0.6152769327163696 \n",
      "     Validation Step: 23 Validation Loss: 0.6133323907852173 \n",
      "     Validation Step: 24 Validation Loss: 0.6075153350830078 \n",
      "     Validation Step: 25 Validation Loss: 0.6121488213539124 \n",
      "     Validation Step: 26 Validation Loss: 0.6137224435806274 \n",
      "     Validation Step: 27 Validation Loss: 0.6136488914489746 \n",
      "     Validation Step: 28 Validation Loss: 0.6142902374267578 \n",
      "     Validation Step: 29 Validation Loss: 0.6160836219787598 \n",
      "     Validation Step: 30 Validation Loss: 0.6171003580093384 \n",
      "     Validation Step: 31 Validation Loss: 0.6184349656105042 \n",
      "     Validation Step: 32 Validation Loss: 0.6158523559570312 \n",
      "     Validation Step: 33 Validation Loss: 0.6149301528930664 \n",
      "     Validation Step: 34 Validation Loss: 0.6111774444580078 \n",
      "     Validation Step: 35 Validation Loss: 0.6116604208946228 \n",
      "     Validation Step: 36 Validation Loss: 0.6141620874404907 \n",
      "     Validation Step: 37 Validation Loss: 0.6112001538276672 \n",
      "     Validation Step: 38 Validation Loss: 0.6115712523460388 \n",
      "     Validation Step: 39 Validation Loss: 0.6176626682281494 \n",
      "     Validation Step: 40 Validation Loss: 0.6153798699378967 \n",
      "     Validation Step: 41 Validation Loss: 0.6177732944488525 \n",
      "     Validation Step: 42 Validation Loss: 0.6185885667800903 \n",
      "     Validation Step: 43 Validation Loss: 0.6137114763259888 \n",
      "     Validation Step: 44 Validation Loss: 0.6128373146057129 \n",
      "Epoch: 58\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6131631731987 \n",
      "     Training Step: 1 Training Loss: 0.6153929233551025 \n",
      "     Training Step: 2 Training Loss: 0.6131465435028076 \n",
      "     Training Step: 3 Training Loss: 0.6148243546485901 \n",
      "     Training Step: 4 Training Loss: 0.6107683777809143 \n",
      "     Training Step: 5 Training Loss: 0.6111501455307007 \n",
      "     Training Step: 6 Training Loss: 0.6094170808792114 \n",
      "     Training Step: 7 Training Loss: 0.6169680953025818 \n",
      "     Training Step: 8 Training Loss: 0.6134144067764282 \n",
      "     Training Step: 9 Training Loss: 0.6158275604248047 \n",
      "     Training Step: 10 Training Loss: 0.6101565957069397 \n",
      "     Training Step: 11 Training Loss: 0.614569902420044 \n",
      "     Training Step: 12 Training Loss: 0.6145695447921753 \n",
      "     Training Step: 13 Training Loss: 0.6118751168251038 \n",
      "     Training Step: 14 Training Loss: 0.6122421622276306 \n",
      "     Training Step: 15 Training Loss: 0.6153474450111389 \n",
      "     Training Step: 16 Training Loss: 0.6122370958328247 \n",
      "     Training Step: 17 Training Loss: 0.6124463081359863 \n",
      "     Training Step: 18 Training Loss: 0.6129727959632874 \n",
      "     Training Step: 19 Training Loss: 0.6121888756752014 \n",
      "     Training Step: 20 Training Loss: 0.6118727326393127 \n",
      "     Training Step: 21 Training Loss: 0.6112489700317383 \n",
      "     Training Step: 22 Training Loss: 0.6144031286239624 \n",
      "     Training Step: 23 Training Loss: 0.6124861836433411 \n",
      "     Training Step: 24 Training Loss: 0.6178410053253174 \n",
      "     Training Step: 25 Training Loss: 0.6171456575393677 \n",
      "     Training Step: 26 Training Loss: 0.6106022000312805 \n",
      "     Training Step: 27 Training Loss: 0.6160848736763 \n",
      "     Training Step: 28 Training Loss: 0.6152045726776123 \n",
      "     Training Step: 29 Training Loss: 0.6140606999397278 \n",
      "     Training Step: 30 Training Loss: 0.6114697456359863 \n",
      "     Training Step: 31 Training Loss: 0.6178228259086609 \n",
      "     Training Step: 32 Training Loss: 0.61531001329422 \n",
      "     Training Step: 33 Training Loss: 0.6100766062736511 \n",
      "     Training Step: 34 Training Loss: 0.6131301522254944 \n",
      "     Training Step: 35 Training Loss: 0.6129481196403503 \n",
      "     Training Step: 36 Training Loss: 0.6142635345458984 \n",
      "     Training Step: 37 Training Loss: 0.6132004857063293 \n",
      "     Training Step: 38 Training Loss: 0.6153448224067688 \n",
      "     Training Step: 39 Training Loss: 0.6118555068969727 \n",
      "     Training Step: 40 Training Loss: 0.6101142168045044 \n",
      "     Training Step: 41 Training Loss: 0.6153364777565002 \n",
      "     Training Step: 42 Training Loss: 0.6115297079086304 \n",
      "     Training Step: 43 Training Loss: 0.6122838258743286 \n",
      "     Training Step: 44 Training Loss: 0.6103618741035461 \n",
      "     Training Step: 45 Training Loss: 0.6157343983650208 \n",
      "     Training Step: 46 Training Loss: 0.6128824949264526 \n",
      "     Training Step: 47 Training Loss: 0.6133179068565369 \n",
      "     Training Step: 48 Training Loss: 0.6128206849098206 \n",
      "     Training Step: 49 Training Loss: 0.6100711226463318 \n",
      "     Training Step: 50 Training Loss: 0.6116874814033508 \n",
      "     Training Step: 51 Training Loss: 0.6203230619430542 \n",
      "     Training Step: 52 Training Loss: 0.6128436923027039 \n",
      "     Training Step: 53 Training Loss: 0.612136960029602 \n",
      "     Training Step: 54 Training Loss: 0.6166735291481018 \n",
      "     Training Step: 55 Training Loss: 0.6124047040939331 \n",
      "     Training Step: 56 Training Loss: 0.6137827634811401 \n",
      "     Training Step: 57 Training Loss: 0.6122485995292664 \n",
      "     Training Step: 58 Training Loss: 0.6121494770050049 \n",
      "     Training Step: 59 Training Loss: 0.6137593388557434 \n",
      "     Training Step: 60 Training Loss: 0.6144055128097534 \n",
      "     Training Step: 61 Training Loss: 0.6139254570007324 \n",
      "     Training Step: 62 Training Loss: 0.6162293553352356 \n",
      "     Training Step: 63 Training Loss: 0.6139673590660095 \n",
      "     Training Step: 64 Training Loss: 0.6183932423591614 \n",
      "     Training Step: 65 Training Loss: 0.6128205060958862 \n",
      "     Training Step: 66 Training Loss: 0.6133479475975037 \n",
      "     Training Step: 67 Training Loss: 0.6133104562759399 \n",
      "     Training Step: 68 Training Loss: 0.6136404871940613 \n",
      "     Training Step: 69 Training Loss: 0.6187964081764221 \n",
      "     Training Step: 70 Training Loss: 0.6118397116661072 \n",
      "     Training Step: 71 Training Loss: 0.6180902719497681 \n",
      "     Training Step: 72 Training Loss: 0.6125829815864563 \n",
      "     Training Step: 73 Training Loss: 0.6184260845184326 \n",
      "     Training Step: 74 Training Loss: 0.6164562702178955 \n",
      "     Training Step: 75 Training Loss: 0.6150230765342712 \n",
      "     Training Step: 76 Training Loss: 0.6182920932769775 \n",
      "     Training Step: 77 Training Loss: 0.6114951968193054 \n",
      "     Training Step: 78 Training Loss: 0.6134839057922363 \n",
      "     Training Step: 79 Training Loss: 0.6141200065612793 \n",
      "     Training Step: 80 Training Loss: 0.617781400680542 \n",
      "     Training Step: 81 Training Loss: 0.6150839924812317 \n",
      "     Training Step: 82 Training Loss: 0.6196197271347046 \n",
      "     Training Step: 83 Training Loss: 0.6108173131942749 \n",
      "     Training Step: 84 Training Loss: 0.6144772171974182 \n",
      "     Training Step: 85 Training Loss: 0.6194405555725098 \n",
      "     Training Step: 86 Training Loss: 0.617142915725708 \n",
      "     Training Step: 87 Training Loss: 0.6159536242485046 \n",
      "     Training Step: 88 Training Loss: 0.6106384992599487 \n",
      "     Training Step: 89 Training Loss: 0.6149256825447083 \n",
      "     Training Step: 90 Training Loss: 0.6128685474395752 \n",
      "     Training Step: 91 Training Loss: 0.6133605241775513 \n",
      "     Training Step: 92 Training Loss: 0.6166850328445435 \n",
      "     Training Step: 93 Training Loss: 0.6136527061462402 \n",
      "     Training Step: 94 Training Loss: 0.6104443669319153 \n",
      "     Training Step: 95 Training Loss: 0.6167334914207458 \n",
      "     Training Step: 96 Training Loss: 0.6152697205543518 \n",
      "     Training Step: 97 Training Loss: 0.6154323220252991 \n",
      "     Training Step: 98 Training Loss: 0.6141694784164429 \n",
      "     Training Step: 99 Training Loss: 0.616649866104126 \n",
      "     Training Step: 100 Training Loss: 0.6107866168022156 \n",
      "     Training Step: 101 Training Loss: 0.614803671836853 \n",
      "     Training Step: 102 Training Loss: 0.6199437975883484 \n",
      "     Training Step: 103 Training Loss: 0.6146581172943115 \n",
      "     Training Step: 104 Training Loss: 0.6184448003768921 \n",
      "     Training Step: 105 Training Loss: 0.6174754500389099 \n",
      "     Training Step: 106 Training Loss: 0.6154818534851074 \n",
      "     Training Step: 107 Training Loss: 0.6163622736930847 \n",
      "     Training Step: 108 Training Loss: 0.6116989850997925 \n",
      "     Training Step: 109 Training Loss: 0.6125699877738953 \n",
      "     Training Step: 110 Training Loss: 0.6147956252098083 \n",
      "     Training Step: 111 Training Loss: 0.6115028858184814 \n",
      "     Training Step: 112 Training Loss: 0.6179662346839905 \n",
      "     Training Step: 113 Training Loss: 0.6167933940887451 \n",
      "     Training Step: 114 Training Loss: 0.6127952933311462 \n",
      "     Training Step: 115 Training Loss: 0.6124813556671143 \n",
      "     Training Step: 116 Training Loss: 0.6149975061416626 \n",
      "     Training Step: 117 Training Loss: 0.6132373809814453 \n",
      "     Training Step: 118 Training Loss: 0.6146750450134277 \n",
      "     Training Step: 119 Training Loss: 0.616735577583313 \n",
      "     Training Step: 120 Training Loss: 0.614700973033905 \n",
      "     Training Step: 121 Training Loss: 0.6147252917289734 \n",
      "     Training Step: 122 Training Loss: 0.6123188138008118 \n",
      "     Training Step: 123 Training Loss: 0.6164835095405579 \n",
      "     Training Step: 124 Training Loss: 0.6119304299354553 \n",
      "     Training Step: 125 Training Loss: 0.6097986102104187 \n",
      "     Training Step: 126 Training Loss: 0.6142266988754272 \n",
      "     Training Step: 127 Training Loss: 0.6132604479789734 \n",
      "     Training Step: 128 Training Loss: 0.6169384717941284 \n",
      "     Training Step: 129 Training Loss: 0.6122658252716064 \n",
      "     Training Step: 130 Training Loss: 0.6116288900375366 \n",
      "     Training Step: 131 Training Loss: 0.6109480857849121 \n",
      "     Training Step: 132 Training Loss: 0.6145142316818237 \n",
      "     Training Step: 133 Training Loss: 0.6146597266197205 \n",
      "     Training Step: 134 Training Loss: 0.6105118989944458 \n",
      "     Training Step: 135 Training Loss: 0.6106604933738708 \n",
      "     Training Step: 136 Training Loss: 0.6082410216331482 \n",
      "     Training Step: 137 Training Loss: 0.610484778881073 \n",
      "     Training Step: 138 Training Loss: 0.6149005889892578 \n",
      "     Training Step: 139 Training Loss: 0.6184601187705994 \n",
      "     Training Step: 140 Training Loss: 0.6147316098213196 \n",
      "     Training Step: 141 Training Loss: 0.6156015396118164 \n",
      "     Training Step: 142 Training Loss: 0.6147920489311218 \n",
      "     Training Step: 143 Training Loss: 0.6110948920249939 \n",
      "     Training Step: 144 Training Loss: 0.6166593432426453 \n",
      "     Training Step: 145 Training Loss: 0.6158700585365295 \n",
      "     Training Step: 146 Training Loss: 0.6114746928215027 \n",
      "     Training Step: 147 Training Loss: 0.6112125515937805 \n",
      "     Training Step: 148 Training Loss: 0.6155067682266235 \n",
      "     Training Step: 149 Training Loss: 0.6161473393440247 \n",
      "     Training Step: 150 Training Loss: 0.6185941696166992 \n",
      "     Training Step: 151 Training Loss: 0.6143518090248108 \n",
      "     Training Step: 152 Training Loss: 0.6117599606513977 \n",
      "     Training Step: 153 Training Loss: 0.6155575513839722 \n",
      "     Training Step: 154 Training Loss: 0.6097613573074341 \n",
      "     Training Step: 155 Training Loss: 0.6135650277137756 \n",
      "     Training Step: 156 Training Loss: 0.6094707250595093 \n",
      "     Training Step: 157 Training Loss: 0.6166778206825256 \n",
      "     Training Step: 158 Training Loss: 0.6099909543991089 \n",
      "     Training Step: 159 Training Loss: 0.614399254322052 \n",
      "     Training Step: 160 Training Loss: 0.6092817187309265 \n",
      "     Training Step: 161 Training Loss: 0.6135205626487732 \n",
      "     Training Step: 162 Training Loss: 0.6162703037261963 \n",
      "     Training Step: 163 Training Loss: 0.6182602643966675 \n",
      "     Training Step: 164 Training Loss: 0.6160141825675964 \n",
      "     Training Step: 165 Training Loss: 0.6154299974441528 \n",
      "     Training Step: 166 Training Loss: 0.6126909852027893 \n",
      "     Training Step: 167 Training Loss: 0.6142079830169678 \n",
      "     Training Step: 168 Training Loss: 0.6128076910972595 \n",
      "     Training Step: 169 Training Loss: 0.6173970699310303 \n",
      "     Training Step: 170 Training Loss: 0.6130824685096741 \n",
      "     Training Step: 171 Training Loss: 0.6171619892120361 \n",
      "     Training Step: 172 Training Loss: 0.6157631278038025 \n",
      "     Training Step: 173 Training Loss: 0.6188387274742126 \n",
      "     Training Step: 174 Training Loss: 0.6169111728668213 \n",
      "     Training Step: 175 Training Loss: 0.6176928877830505 \n",
      "     Training Step: 176 Training Loss: 0.6116610169410706 \n",
      "     Training Step: 177 Training Loss: 0.6114766597747803 \n",
      "     Training Step: 178 Training Loss: 0.6137778759002686 \n",
      "     Training Step: 179 Training Loss: 0.6151760220527649 \n",
      "     Training Step: 180 Training Loss: 0.6168994307518005 \n",
      "     Training Step: 181 Training Loss: 0.6118380427360535 \n",
      "     Training Step: 182 Training Loss: 0.613517701625824 \n",
      "     Training Step: 183 Training Loss: 0.6125628352165222 \n",
      "     Training Step: 184 Training Loss: 0.6163180470466614 \n",
      "     Training Step: 185 Training Loss: 0.6180548667907715 \n",
      "     Training Step: 186 Training Loss: 0.6155027151107788 \n",
      "     Training Step: 187 Training Loss: 0.616725742816925 \n",
      "     Training Step: 188 Training Loss: 0.6168190836906433 \n",
      "     Training Step: 189 Training Loss: 0.6181215643882751 \n",
      "     Training Step: 190 Training Loss: 0.6152874827384949 \n",
      "     Training Step: 191 Training Loss: 0.6121436357498169 \n",
      "     Training Step: 192 Training Loss: 0.6151025295257568 \n",
      "     Training Step: 193 Training Loss: 0.610238254070282 \n",
      "     Training Step: 194 Training Loss: 0.6158692240715027 \n",
      "     Training Step: 195 Training Loss: 0.6147081851959229 \n",
      "     Training Step: 196 Training Loss: 0.6116710305213928 \n",
      "     Training Step: 197 Training Loss: 0.6146305203437805 \n",
      "     Training Step: 198 Training Loss: 0.6115477085113525 \n",
      "     Training Step: 199 Training Loss: 0.61891108751297 \n",
      "     Training Step: 200 Training Loss: 0.6197301745414734 \n",
      "     Training Step: 201 Training Loss: 0.6144112348556519 \n",
      "     Training Step: 202 Training Loss: 0.615663468837738 \n",
      "     Training Step: 203 Training Loss: 0.6149356365203857 \n",
      "     Training Step: 204 Training Loss: 0.6167697310447693 \n",
      "     Training Step: 205 Training Loss: 0.6106736063957214 \n",
      "     Training Step: 206 Training Loss: 0.6129949688911438 \n",
      "     Training Step: 207 Training Loss: 0.6142111420631409 \n",
      "     Training Step: 208 Training Loss: 0.6116621494293213 \n",
      "     Training Step: 209 Training Loss: 0.6141513586044312 \n",
      "     Training Step: 210 Training Loss: 0.6116505861282349 \n",
      "     Training Step: 211 Training Loss: 0.6172533631324768 \n",
      "     Training Step: 212 Training Loss: 0.6107630133628845 \n",
      "     Training Step: 213 Training Loss: 0.615693986415863 \n",
      "     Training Step: 214 Training Loss: 0.6153849363327026 \n",
      "     Training Step: 215 Training Loss: 0.6157854795455933 \n",
      "     Training Step: 216 Training Loss: 0.6180977821350098 \n",
      "     Training Step: 217 Training Loss: 0.6133639812469482 \n",
      "     Training Step: 218 Training Loss: 0.614765465259552 \n",
      "     Training Step: 219 Training Loss: 0.617638111114502 \n",
      "     Training Step: 220 Training Loss: 0.6139398813247681 \n",
      "     Training Step: 221 Training Loss: 0.6146872043609619 \n",
      "     Training Step: 222 Training Loss: 0.6151816844940186 \n",
      "     Training Step: 223 Training Loss: 0.6138483285903931 \n",
      "     Training Step: 224 Training Loss: 0.6125050783157349 \n",
      "     Training Step: 225 Training Loss: 0.6155896782875061 \n",
      "     Training Step: 226 Training Loss: 0.6097390055656433 \n",
      "     Training Step: 227 Training Loss: 0.6171647310256958 \n",
      "     Training Step: 228 Training Loss: 0.6140769720077515 \n",
      "     Training Step: 229 Training Loss: 0.6134747862815857 \n",
      "     Training Step: 230 Training Loss: 0.6177617907524109 \n",
      "     Training Step: 231 Training Loss: 0.611496090888977 \n",
      "     Training Step: 232 Training Loss: 0.6140549778938293 \n",
      "     Training Step: 233 Training Loss: 0.6210594177246094 \n",
      "     Training Step: 234 Training Loss: 0.6169397234916687 \n",
      "     Training Step: 235 Training Loss: 0.6118482351303101 \n",
      "     Training Step: 236 Training Loss: 0.6136532425880432 \n",
      "     Training Step: 237 Training Loss: 0.6144559979438782 \n",
      "     Training Step: 238 Training Loss: 0.6120958924293518 \n",
      "     Training Step: 239 Training Loss: 0.6136553287506104 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6136642694473267 \n",
      "     Validation Step: 1 Validation Loss: 0.6158274412155151 \n",
      "     Validation Step: 2 Validation Loss: 0.6136793494224548 \n",
      "     Validation Step: 3 Validation Loss: 0.6183019876480103 \n",
      "     Validation Step: 4 Validation Loss: 0.616287350654602 \n",
      "     Validation Step: 5 Validation Loss: 0.6184011101722717 \n",
      "     Validation Step: 6 Validation Loss: 0.6160354018211365 \n",
      "     Validation Step: 7 Validation Loss: 0.6176661252975464 \n",
      "     Validation Step: 8 Validation Loss: 0.6116588711738586 \n",
      "     Validation Step: 9 Validation Loss: 0.6128425002098083 \n",
      "     Validation Step: 10 Validation Loss: 0.6129884123802185 \n",
      "     Validation Step: 11 Validation Loss: 0.614295244216919 \n",
      "     Validation Step: 12 Validation Loss: 0.6133321523666382 \n",
      "     Validation Step: 13 Validation Loss: 0.6145704984664917 \n",
      "     Validation Step: 14 Validation Loss: 0.6101983785629272 \n",
      "     Validation Step: 15 Validation Loss: 0.6111615896224976 \n",
      "     Validation Step: 16 Validation Loss: 0.6148558855056763 \n",
      "     Validation Step: 17 Validation Loss: 0.6101109981536865 \n",
      "     Validation Step: 18 Validation Loss: 0.6146131753921509 \n",
      "     Validation Step: 19 Validation Loss: 0.6142232418060303 \n",
      "     Validation Step: 20 Validation Loss: 0.6118854284286499 \n",
      "     Validation Step: 21 Validation Loss: 0.6075196266174316 \n",
      "     Validation Step: 22 Validation Loss: 0.6141474843025208 \n",
      "     Validation Step: 23 Validation Loss: 0.6101522445678711 \n",
      "     Validation Step: 24 Validation Loss: 0.6141367554664612 \n",
      "     Validation Step: 25 Validation Loss: 0.6105080842971802 \n",
      "     Validation Step: 26 Validation Loss: 0.6185467839241028 \n",
      "     Validation Step: 27 Validation Loss: 0.6111950278282166 \n",
      "     Validation Step: 28 Validation Loss: 0.615264892578125 \n",
      "     Validation Step: 29 Validation Loss: 0.6115598678588867 \n",
      "     Validation Step: 30 Validation Loss: 0.6181297898292542 \n",
      "     Validation Step: 31 Validation Loss: 0.6104902625083923 \n",
      "     Validation Step: 32 Validation Loss: 0.6170806884765625 \n",
      "     Validation Step: 33 Validation Loss: 0.6153479218482971 \n",
      "     Validation Step: 34 Validation Loss: 0.6185779571533203 \n",
      "     Validation Step: 35 Validation Loss: 0.6156269311904907 \n",
      "     Validation Step: 36 Validation Loss: 0.6177471876144409 \n",
      "     Validation Step: 37 Validation Loss: 0.6156509518623352 \n",
      "     Validation Step: 38 Validation Loss: 0.6149352192878723 \n",
      "     Validation Step: 39 Validation Loss: 0.6106528639793396 \n",
      "     Validation Step: 40 Validation Loss: 0.617369532585144 \n",
      "     Validation Step: 41 Validation Loss: 0.6146603226661682 \n",
      "     Validation Step: 42 Validation Loss: 0.6136654615402222 \n",
      "     Validation Step: 43 Validation Loss: 0.6150658130645752 \n",
      "     Validation Step: 44 Validation Loss: 0.6121394038200378 \n",
      "Epoch: 59\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.613646924495697 \n",
      "     Training Step: 1 Training Loss: 0.6155198216438293 \n",
      "     Training Step: 2 Training Loss: 0.6133031249046326 \n",
      "     Training Step: 3 Training Loss: 0.6106005907058716 \n",
      "     Training Step: 4 Training Loss: 0.6135187745094299 \n",
      "     Training Step: 5 Training Loss: 0.6144941449165344 \n",
      "     Training Step: 6 Training Loss: 0.6140344142913818 \n",
      "     Training Step: 7 Training Loss: 0.6130673289299011 \n",
      "     Training Step: 8 Training Loss: 0.6131381988525391 \n",
      "     Training Step: 9 Training Loss: 0.616157054901123 \n",
      "     Training Step: 10 Training Loss: 0.6164426803588867 \n",
      "     Training Step: 11 Training Loss: 0.6141751408576965 \n",
      "     Training Step: 12 Training Loss: 0.6133415699005127 \n",
      "     Training Step: 13 Training Loss: 0.6143038272857666 \n",
      "     Training Step: 14 Training Loss: 0.6157817840576172 \n",
      "     Training Step: 15 Training Loss: 0.6164175868034363 \n",
      "     Training Step: 16 Training Loss: 0.6151847839355469 \n",
      "     Training Step: 17 Training Loss: 0.6166479587554932 \n",
      "     Training Step: 18 Training Loss: 0.6133167743682861 \n",
      "     Training Step: 19 Training Loss: 0.6177120208740234 \n",
      "     Training Step: 20 Training Loss: 0.6149721145629883 \n",
      "     Training Step: 21 Training Loss: 0.6166826486587524 \n",
      "     Training Step: 22 Training Loss: 0.6119787096977234 \n",
      "     Training Step: 23 Training Loss: 0.6152947545051575 \n",
      "     Training Step: 24 Training Loss: 0.614224910736084 \n",
      "     Training Step: 25 Training Loss: 0.6156324744224548 \n",
      "     Training Step: 26 Training Loss: 0.6184329986572266 \n",
      "     Training Step: 27 Training Loss: 0.6125447750091553 \n",
      "     Training Step: 28 Training Loss: 0.6146851778030396 \n",
      "     Training Step: 29 Training Loss: 0.6180281639099121 \n",
      "     Training Step: 30 Training Loss: 0.6139912605285645 \n",
      "     Training Step: 31 Training Loss: 0.6185826063156128 \n",
      "     Training Step: 32 Training Loss: 0.6122744083404541 \n",
      "     Training Step: 33 Training Loss: 0.6147497296333313 \n",
      "     Training Step: 34 Training Loss: 0.6132276654243469 \n",
      "     Training Step: 35 Training Loss: 0.6111723780632019 \n",
      "     Training Step: 36 Training Loss: 0.6160879135131836 \n",
      "     Training Step: 37 Training Loss: 0.6115246415138245 \n",
      "     Training Step: 38 Training Loss: 0.610751211643219 \n",
      "     Training Step: 39 Training Loss: 0.6141681671142578 \n",
      "     Training Step: 40 Training Loss: 0.6162546873092651 \n",
      "     Training Step: 41 Training Loss: 0.6155586838722229 \n",
      "     Training Step: 42 Training Loss: 0.6124043464660645 \n",
      "     Training Step: 43 Training Loss: 0.6134781837463379 \n",
      "     Training Step: 44 Training Loss: 0.6181046366691589 \n",
      "     Training Step: 45 Training Loss: 0.6127914786338806 \n",
      "     Training Step: 46 Training Loss: 0.6146929860115051 \n",
      "     Training Step: 47 Training Loss: 0.6142513751983643 \n",
      "     Training Step: 48 Training Loss: 0.6137875318527222 \n",
      "     Training Step: 49 Training Loss: 0.6154173016548157 \n",
      "     Training Step: 50 Training Loss: 0.6125361323356628 \n",
      "     Training Step: 51 Training Loss: 0.6157563924789429 \n",
      "     Training Step: 52 Training Loss: 0.61165452003479 \n",
      "     Training Step: 53 Training Loss: 0.616263747215271 \n",
      "     Training Step: 54 Training Loss: 0.6166841387748718 \n",
      "     Training Step: 55 Training Loss: 0.6184703707695007 \n",
      "     Training Step: 56 Training Loss: 0.618391215801239 \n",
      "     Training Step: 57 Training Loss: 0.6167715787887573 \n",
      "     Training Step: 58 Training Loss: 0.6185610890388489 \n",
      "     Training Step: 59 Training Loss: 0.6180046796798706 \n",
      "     Training Step: 60 Training Loss: 0.6138309240341187 \n",
      "     Training Step: 61 Training Loss: 0.61255943775177 \n",
      "     Training Step: 62 Training Loss: 0.6202816367149353 \n",
      "     Training Step: 63 Training Loss: 0.6100949645042419 \n",
      "     Training Step: 64 Training Loss: 0.6123122572898865 \n",
      "     Training Step: 65 Training Loss: 0.6118348240852356 \n",
      "     Training Step: 66 Training Loss: 0.6148294806480408 \n",
      "     Training Step: 67 Training Loss: 0.6146571040153503 \n",
      "     Training Step: 68 Training Loss: 0.6144452095031738 \n",
      "     Training Step: 69 Training Loss: 0.6165139675140381 \n",
      "     Training Step: 70 Training Loss: 0.617829442024231 \n",
      "     Training Step: 71 Training Loss: 0.6169438362121582 \n",
      "     Training Step: 72 Training Loss: 0.6141434907913208 \n",
      "     Training Step: 73 Training Loss: 0.6194515228271484 \n",
      "     Training Step: 74 Training Loss: 0.6116222143173218 \n",
      "     Training Step: 75 Training Loss: 0.6171705722808838 \n",
      "     Training Step: 76 Training Loss: 0.6146658062934875 \n",
      "     Training Step: 77 Training Loss: 0.6146224141120911 \n",
      "     Training Step: 78 Training Loss: 0.6154451370239258 \n",
      "     Training Step: 79 Training Loss: 0.610924482345581 \n",
      "     Training Step: 80 Training Loss: 0.6159678101539612 \n",
      "     Training Step: 81 Training Loss: 0.6166646480560303 \n",
      "     Training Step: 82 Training Loss: 0.6170887351036072 \n",
      "     Training Step: 83 Training Loss: 0.614708662033081 \n",
      "     Training Step: 84 Training Loss: 0.6124388575553894 \n",
      "     Training Step: 85 Training Loss: 0.6083763241767883 \n",
      "     Training Step: 86 Training Loss: 0.61332768201828 \n",
      "     Training Step: 87 Training Loss: 0.6124244332313538 \n",
      "     Training Step: 88 Training Loss: 0.6144572496414185 \n",
      "     Training Step: 89 Training Loss: 0.6116599440574646 \n",
      "     Training Step: 90 Training Loss: 0.6105657815933228 \n",
      "     Training Step: 91 Training Loss: 0.6107152700424194 \n",
      "     Training Step: 92 Training Loss: 0.6146156787872314 \n",
      "     Training Step: 93 Training Loss: 0.6114434599876404 \n",
      "     Training Step: 94 Training Loss: 0.6097270250320435 \n",
      "     Training Step: 95 Training Loss: 0.6177676916122437 \n",
      "     Training Step: 96 Training Loss: 0.6149590015411377 \n",
      "     Training Step: 97 Training Loss: 0.6128239035606384 \n",
      "     Training Step: 98 Training Loss: 0.6102287769317627 \n",
      "     Training Step: 99 Training Loss: 0.615449845790863 \n",
      "     Training Step: 100 Training Loss: 0.6189002394676208 \n",
      "     Training Step: 101 Training Loss: 0.6188596487045288 \n",
      "     Training Step: 102 Training Loss: 0.6142272353172302 \n",
      "     Training Step: 103 Training Loss: 0.6156794428825378 \n",
      "     Training Step: 104 Training Loss: 0.6120136380195618 \n",
      "     Training Step: 105 Training Loss: 0.615386962890625 \n",
      "     Training Step: 106 Training Loss: 0.6167393922805786 \n",
      "     Training Step: 107 Training Loss: 0.6135334372520447 \n",
      "     Training Step: 108 Training Loss: 0.6167508959770203 \n",
      "     Training Step: 109 Training Loss: 0.614936351776123 \n",
      "     Training Step: 110 Training Loss: 0.6196327805519104 \n",
      "     Training Step: 111 Training Loss: 0.6129269003868103 \n",
      "     Training Step: 112 Training Loss: 0.61440110206604 \n",
      "     Training Step: 113 Training Loss: 0.6116926074028015 \n",
      "     Training Step: 114 Training Loss: 0.6121528744697571 \n",
      "     Training Step: 115 Training Loss: 0.6094205975532532 \n",
      "     Training Step: 116 Training Loss: 0.6130480766296387 \n",
      "     Training Step: 117 Training Loss: 0.6094550490379333 \n",
      "     Training Step: 118 Training Loss: 0.6173686981201172 \n",
      "     Training Step: 119 Training Loss: 0.6117008924484253 \n",
      "     Training Step: 120 Training Loss: 0.616298496723175 \n",
      "     Training Step: 121 Training Loss: 0.6117244958877563 \n",
      "     Training Step: 122 Training Loss: 0.6154821515083313 \n",
      "     Training Step: 123 Training Loss: 0.6153132319450378 \n",
      "     Training Step: 124 Training Loss: 0.6122445464134216 \n",
      "     Training Step: 125 Training Loss: 0.6110019087791443 \n",
      "     Training Step: 126 Training Loss: 0.6114712953567505 \n",
      "     Training Step: 127 Training Loss: 0.6135309934616089 \n",
      "     Training Step: 128 Training Loss: 0.6141589879989624 \n",
      "     Training Step: 129 Training Loss: 0.6200024485588074 \n",
      "     Training Step: 130 Training Loss: 0.614658772945404 \n",
      "     Training Step: 131 Training Loss: 0.6119818091392517 \n",
      "     Training Step: 132 Training Loss: 0.611565113067627 \n",
      "     Training Step: 133 Training Loss: 0.612806499004364 \n",
      "     Training Step: 134 Training Loss: 0.6145517230033875 \n",
      "     Training Step: 135 Training Loss: 0.6157695055007935 \n",
      "     Training Step: 136 Training Loss: 0.6148282885551453 \n",
      "     Training Step: 137 Training Loss: 0.6128904223442078 \n",
      "     Training Step: 138 Training Loss: 0.6155545115470886 \n",
      "     Training Step: 139 Training Loss: 0.6151262521743774 \n",
      "     Training Step: 140 Training Loss: 0.6127929091453552 \n",
      "     Training Step: 141 Training Loss: 0.615109384059906 \n",
      "     Training Step: 142 Training Loss: 0.6152924299240112 \n",
      "     Training Step: 143 Training Loss: 0.6106074452400208 \n",
      "     Training Step: 144 Training Loss: 0.6139363646507263 \n",
      "     Training Step: 145 Training Loss: 0.6174761056900024 \n",
      "     Training Step: 146 Training Loss: 0.610079288482666 \n",
      "     Training Step: 147 Training Loss: 0.612173855304718 \n",
      "     Training Step: 148 Training Loss: 0.6111704707145691 \n",
      "     Training Step: 149 Training Loss: 0.6103702783584595 \n",
      "     Training Step: 150 Training Loss: 0.6114024519920349 \n",
      "     Training Step: 151 Training Loss: 0.6122366189956665 \n",
      "     Training Step: 152 Training Loss: 0.6118376851081848 \n",
      "     Training Step: 153 Training Loss: 0.6183463931083679 \n",
      "     Training Step: 154 Training Loss: 0.6121720671653748 \n",
      "     Training Step: 155 Training Loss: 0.6155527830123901 \n",
      "     Training Step: 156 Training Loss: 0.609380841255188 \n",
      "     Training Step: 157 Training Loss: 0.6158634424209595 \n",
      "     Training Step: 158 Training Loss: 0.6171643733978271 \n",
      "     Training Step: 159 Training Loss: 0.6107398867607117 \n",
      "     Training Step: 160 Training Loss: 0.6112356185913086 \n",
      "     Training Step: 161 Training Loss: 0.6129454374313354 \n",
      "     Training Step: 162 Training Loss: 0.6178821325302124 \n",
      "     Training Step: 163 Training Loss: 0.6141624450683594 \n",
      "     Training Step: 164 Training Loss: 0.6147913336753845 \n",
      "     Training Step: 165 Training Loss: 0.6116466522216797 \n",
      "     Training Step: 166 Training Loss: 0.6168271899223328 \n",
      "     Training Step: 167 Training Loss: 0.6107083559036255 \n",
      "     Training Step: 168 Training Loss: 0.6131924390792847 \n",
      "     Training Step: 169 Training Loss: 0.6104117631912231 \n",
      "     Training Step: 170 Training Loss: 0.6152718663215637 \n",
      "     Training Step: 171 Training Loss: 0.6099796295166016 \n",
      "     Training Step: 172 Training Loss: 0.6178406476974487 \n",
      "     Training Step: 173 Training Loss: 0.613109290599823 \n",
      "     Training Step: 174 Training Loss: 0.6115401983261108 \n",
      "     Training Step: 175 Training Loss: 0.6175229549407959 \n",
      "     Training Step: 176 Training Loss: 0.6144750118255615 \n",
      "     Training Step: 177 Training Loss: 0.614879310131073 \n",
      "     Training Step: 178 Training Loss: 0.6168258190155029 \n",
      "     Training Step: 179 Training Loss: 0.6147473454475403 \n",
      "     Training Step: 180 Training Loss: 0.6139482855796814 \n",
      "     Training Step: 181 Training Loss: 0.6137487292289734 \n",
      "     Training Step: 182 Training Loss: 0.6118671298027039 \n",
      "     Training Step: 183 Training Loss: 0.6197502017021179 \n",
      "     Training Step: 184 Training Loss: 0.6147274971008301 \n",
      "     Training Step: 185 Training Loss: 0.6101264953613281 \n",
      "     Training Step: 186 Training Loss: 0.6152466535568237 \n",
      "     Training Step: 187 Training Loss: 0.6120293140411377 \n",
      "     Training Step: 188 Training Loss: 0.6097291111946106 \n",
      "     Training Step: 189 Training Loss: 0.6129058003425598 \n",
      "     Training Step: 190 Training Loss: 0.6145619750022888 \n",
      "     Training Step: 191 Training Loss: 0.610482931137085 \n",
      "     Training Step: 192 Training Loss: 0.6132758855819702 \n",
      "     Training Step: 193 Training Loss: 0.6136727333068848 \n",
      "     Training Step: 194 Training Loss: 0.6104918122291565 \n",
      "     Training Step: 195 Training Loss: 0.6153523325920105 \n",
      "     Training Step: 196 Training Loss: 0.6131443977355957 \n",
      "     Training Step: 197 Training Loss: 0.6114721894264221 \n",
      "     Training Step: 198 Training Loss: 0.6101343631744385 \n",
      "     Training Step: 199 Training Loss: 0.6138469576835632 \n",
      "     Training Step: 200 Training Loss: 0.6151506304740906 \n",
      "     Training Step: 201 Training Loss: 0.6181430220603943 \n",
      "     Training Step: 202 Training Loss: 0.611670196056366 \n",
      "     Training Step: 203 Training Loss: 0.6182060241699219 \n",
      "     Training Step: 204 Training Loss: 0.6143855452537537 \n",
      "     Training Step: 205 Training Loss: 0.6122223734855652 \n",
      "     Training Step: 206 Training Loss: 0.6153860688209534 \n",
      "     Training Step: 207 Training Loss: 0.6157900094985962 \n",
      "     Training Step: 208 Training Loss: 0.6172499656677246 \n",
      "     Training Step: 209 Training Loss: 0.6163666844367981 \n",
      "     Training Step: 210 Training Loss: 0.6124941110610962 \n",
      "     Training Step: 211 Training Loss: 0.6116389036178589 \n",
      "     Training Step: 212 Training Loss: 0.61681067943573 \n",
      "     Training Step: 213 Training Loss: 0.6166532039642334 \n",
      "     Training Step: 214 Training Loss: 0.6106445789337158 \n",
      "     Training Step: 215 Training Loss: 0.6140301823616028 \n",
      "     Training Step: 216 Training Loss: 0.6122880578041077 \n",
      "     Training Step: 217 Training Loss: 0.6126532554626465 \n",
      "     Training Step: 218 Training Loss: 0.6096668243408203 \n",
      "     Training Step: 219 Training Loss: 0.6154550909996033 \n",
      "     Training Step: 220 Training Loss: 0.6157931089401245 \n",
      "     Training Step: 221 Training Loss: 0.6132373213768005 \n",
      "     Training Step: 222 Training Loss: 0.6168274879455566 \n",
      "     Training Step: 223 Training Loss: 0.6160310506820679 \n",
      "     Training Step: 224 Training Loss: 0.6209350228309631 \n",
      "     Training Step: 225 Training Loss: 0.6135534644126892 \n",
      "     Training Step: 226 Training Loss: 0.6166954040527344 \n",
      "     Training Step: 227 Training Loss: 0.6177122592926025 \n",
      "     Training Step: 228 Training Loss: 0.6126487255096436 \n",
      "     Training Step: 229 Training Loss: 0.6119000911712646 \n",
      "     Training Step: 230 Training Loss: 0.6143775582313538 \n",
      "     Training Step: 231 Training Loss: 0.6177484393119812 \n",
      "     Training Step: 232 Training Loss: 0.6130037307739258 \n",
      "     Training Step: 233 Training Loss: 0.6136255264282227 \n",
      "     Training Step: 234 Training Loss: 0.6184400320053101 \n",
      "     Training Step: 235 Training Loss: 0.6134052276611328 \n",
      "     Training Step: 236 Training Loss: 0.6146945357322693 \n",
      "     Training Step: 237 Training Loss: 0.612087070941925 \n",
      "     Training Step: 238 Training Loss: 0.6169375777244568 \n",
      "     Training Step: 239 Training Loss: 0.6136543154716492 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6136780977249146 \n",
      "     Validation Step: 1 Validation Loss: 0.6101076602935791 \n",
      "     Validation Step: 2 Validation Loss: 0.6104891300201416 \n",
      "     Validation Step: 3 Validation Loss: 0.6118828654289246 \n",
      "     Validation Step: 4 Validation Loss: 0.612136960029602 \n",
      "     Validation Step: 5 Validation Loss: 0.6142156720161438 \n",
      "     Validation Step: 6 Validation Loss: 0.618553876876831 \n",
      "     Validation Step: 7 Validation Loss: 0.6116499900817871 \n",
      "     Validation Step: 8 Validation Loss: 0.617070198059082 \n",
      "     Validation Step: 9 Validation Loss: 0.614280641078949 \n",
      "     Validation Step: 10 Validation Loss: 0.6153391003608704 \n",
      "     Validation Step: 11 Validation Loss: 0.6148517727851868 \n",
      "     Validation Step: 12 Validation Loss: 0.6160357594490051 \n",
      "     Validation Step: 13 Validation Loss: 0.615249752998352 \n",
      "     Validation Step: 14 Validation Loss: 0.6177372932434082 \n",
      "     Validation Step: 15 Validation Loss: 0.6145576238632202 \n",
      "     Validation Step: 16 Validation Loss: 0.6176469326019287 \n",
      "     Validation Step: 17 Validation Loss: 0.611183762550354 \n",
      "     Validation Step: 18 Validation Loss: 0.6141375303268433 \n",
      "     Validation Step: 19 Validation Loss: 0.6162818074226379 \n",
      "     Validation Step: 20 Validation Loss: 0.6101447343826294 \n",
      "     Validation Step: 21 Validation Loss: 0.6136656999588013 \n",
      "     Validation Step: 22 Validation Loss: 0.6156156659126282 \n",
      "     Validation Step: 23 Validation Loss: 0.6141340136528015 \n",
      "     Validation Step: 24 Validation Loss: 0.618293046951294 \n",
      "     Validation Step: 25 Validation Loss: 0.615064799785614 \n",
      "     Validation Step: 26 Validation Loss: 0.6146525144577026 \n",
      "     Validation Step: 27 Validation Loss: 0.6101818084716797 \n",
      "     Validation Step: 28 Validation Loss: 0.6105079054832458 \n",
      "     Validation Step: 29 Validation Loss: 0.611160397529602 \n",
      "     Validation Step: 30 Validation Loss: 0.6183891892433167 \n",
      "     Validation Step: 31 Validation Loss: 0.6158219575881958 \n",
      "     Validation Step: 32 Validation Loss: 0.6145985722541809 \n",
      "     Validation Step: 33 Validation Loss: 0.6133180856704712 \n",
      "     Validation Step: 34 Validation Loss: 0.6149190068244934 \n",
      "     Validation Step: 35 Validation Loss: 0.618531346321106 \n",
      "     Validation Step: 36 Validation Loss: 0.6075200438499451 \n",
      "     Validation Step: 37 Validation Loss: 0.6136520504951477 \n",
      "     Validation Step: 38 Validation Loss: 0.6106446981430054 \n",
      "     Validation Step: 39 Validation Loss: 0.6115554571151733 \n",
      "     Validation Step: 40 Validation Loss: 0.6156399846076965 \n",
      "     Validation Step: 41 Validation Loss: 0.6129899621009827 \n",
      "     Validation Step: 42 Validation Loss: 0.6128329634666443 \n",
      "     Validation Step: 43 Validation Loss: 0.6181144714355469 \n",
      "     Validation Step: 44 Validation Loss: 0.617353081703186 \n",
      "Epoch: 60\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6094504594802856 \n",
      "     Training Step: 1 Training Loss: 0.6181098222732544 \n",
      "     Training Step: 2 Training Loss: 0.6149343252182007 \n",
      "     Training Step: 3 Training Loss: 0.6100974082946777 \n",
      "     Training Step: 4 Training Loss: 0.6128599047660828 \n",
      "     Training Step: 5 Training Loss: 0.6131423115730286 \n",
      "     Training Step: 6 Training Loss: 0.6142165660858154 \n",
      "     Training Step: 7 Training Loss: 0.6097176671028137 \n",
      "     Training Step: 8 Training Loss: 0.6153562068939209 \n",
      "     Training Step: 9 Training Loss: 0.6116464734077454 \n",
      "     Training Step: 10 Training Loss: 0.6133016347885132 \n",
      "     Training Step: 11 Training Loss: 0.609247088432312 \n",
      "     Training Step: 12 Training Loss: 0.6111626029014587 \n",
      "     Training Step: 13 Training Loss: 0.6137615442276001 \n",
      "     Training Step: 14 Training Loss: 0.6121694445610046 \n",
      "     Training Step: 15 Training Loss: 0.6114353537559509 \n",
      "     Training Step: 16 Training Loss: 0.612140417098999 \n",
      "     Training Step: 17 Training Loss: 0.6141437292098999 \n",
      "     Training Step: 18 Training Loss: 0.613514244556427 \n",
      "     Training Step: 19 Training Loss: 0.6139082908630371 \n",
      "     Training Step: 20 Training Loss: 0.6164584755897522 \n",
      "     Training Step: 21 Training Loss: 0.6097531318664551 \n",
      "     Training Step: 22 Training Loss: 0.6116164326667786 \n",
      "     Training Step: 23 Training Loss: 0.615188717842102 \n",
      "     Training Step: 24 Training Loss: 0.6162822246551514 \n",
      "     Training Step: 25 Training Loss: 0.6167371273040771 \n",
      "     Training Step: 26 Training Loss: 0.6147838234901428 \n",
      "     Training Step: 27 Training Loss: 0.6101973056793213 \n",
      "     Training Step: 28 Training Loss: 0.6152966618537903 \n",
      "     Training Step: 29 Training Loss: 0.6182407140731812 \n",
      "     Training Step: 30 Training Loss: 0.6142913103103638 \n",
      "     Training Step: 31 Training Loss: 0.6134772896766663 \n",
      "     Training Step: 32 Training Loss: 0.6123175024986267 \n",
      "     Training Step: 33 Training Loss: 0.6153587698936462 \n",
      "     Training Step: 34 Training Loss: 0.6160683035850525 \n",
      "     Training Step: 35 Training Loss: 0.616023600101471 \n",
      "     Training Step: 36 Training Loss: 0.6147002577781677 \n",
      "     Training Step: 37 Training Loss: 0.6154549717903137 \n",
      "     Training Step: 38 Training Loss: 0.6154433488845825 \n",
      "     Training Step: 39 Training Loss: 0.6177953481674194 \n",
      "     Training Step: 40 Training Loss: 0.614666223526001 \n",
      "     Training Step: 41 Training Loss: 0.617689847946167 \n",
      "     Training Step: 42 Training Loss: 0.6115847826004028 \n",
      "     Training Step: 43 Training Loss: 0.6178252696990967 \n",
      "     Training Step: 44 Training Loss: 0.6114491820335388 \n",
      "     Training Step: 45 Training Loss: 0.6116206645965576 \n",
      "     Training Step: 46 Training Loss: 0.6103839874267578 \n",
      "     Training Step: 47 Training Loss: 0.6125187873840332 \n",
      "     Training Step: 48 Training Loss: 0.6115280389785767 \n",
      "     Training Step: 49 Training Loss: 0.6114091277122498 \n",
      "     Training Step: 50 Training Loss: 0.6172613501548767 \n",
      "     Training Step: 51 Training Loss: 0.6195247173309326 \n",
      "     Training Step: 52 Training Loss: 0.6140924096107483 \n",
      "     Training Step: 53 Training Loss: 0.6129815578460693 \n",
      "     Training Step: 54 Training Loss: 0.618440568447113 \n",
      "     Training Step: 55 Training Loss: 0.6123573184013367 \n",
      "     Training Step: 56 Training Loss: 0.6151092052459717 \n",
      "     Training Step: 57 Training Loss: 0.6136100888252258 \n",
      "     Training Step: 58 Training Loss: 0.6177228689193726 \n",
      "     Training Step: 59 Training Loss: 0.6115104556083679 \n",
      "     Training Step: 60 Training Loss: 0.6128856539726257 \n",
      "     Training Step: 61 Training Loss: 0.6188589930534363 \n",
      "     Training Step: 62 Training Loss: 0.614721953868866 \n",
      "     Training Step: 63 Training Loss: 0.6121584177017212 \n",
      "     Training Step: 64 Training Loss: 0.6124145984649658 \n",
      "     Training Step: 65 Training Loss: 0.6106805205345154 \n",
      "     Training Step: 66 Training Loss: 0.6183335185050964 \n",
      "     Training Step: 67 Training Loss: 0.6131359338760376 \n",
      "     Training Step: 68 Training Loss: 0.6128092408180237 \n",
      "     Training Step: 69 Training Loss: 0.6151236295700073 \n",
      "     Training Step: 70 Training Loss: 0.6153899431228638 \n",
      "     Training Step: 71 Training Loss: 0.6132338047027588 \n",
      "     Training Step: 72 Training Loss: 0.6118927597999573 \n",
      "     Training Step: 73 Training Loss: 0.6154218912124634 \n",
      "     Training Step: 74 Training Loss: 0.6118558645248413 \n",
      "     Training Step: 75 Training Loss: 0.6132960319519043 \n",
      "     Training Step: 76 Training Loss: 0.6186978816986084 \n",
      "     Training Step: 77 Training Loss: 0.615111768245697 \n",
      "     Training Step: 78 Training Loss: 0.6157030463218689 \n",
      "     Training Step: 79 Training Loss: 0.6180748343467712 \n",
      "     Training Step: 80 Training Loss: 0.6124723553657532 \n",
      "     Training Step: 81 Training Loss: 0.6152758002281189 \n",
      "     Training Step: 82 Training Loss: 0.6135578155517578 \n",
      "     Training Step: 83 Training Loss: 0.6157199740409851 \n",
      "     Training Step: 84 Training Loss: 0.6105915307998657 \n",
      "     Training Step: 85 Training Loss: 0.6129403114318848 \n",
      "     Training Step: 86 Training Loss: 0.612263023853302 \n",
      "     Training Step: 87 Training Loss: 0.6168431043624878 \n",
      "     Training Step: 88 Training Loss: 0.6199272274971008 \n",
      "     Training Step: 89 Training Loss: 0.6121088862419128 \n",
      "     Training Step: 90 Training Loss: 0.6103315353393555 \n",
      "     Training Step: 91 Training Loss: 0.6133822202682495 \n",
      "     Training Step: 92 Training Loss: 0.614357054233551 \n",
      "     Training Step: 93 Training Loss: 0.6156163215637207 \n",
      "     Training Step: 94 Training Loss: 0.6185623407363892 \n",
      "     Training Step: 95 Training Loss: 0.6175168752670288 \n",
      "     Training Step: 96 Training Loss: 0.6167215704917908 \n",
      "     Training Step: 97 Training Loss: 0.6144572496414185 \n",
      "     Training Step: 98 Training Loss: 0.6165263056755066 \n",
      "     Training Step: 99 Training Loss: 0.6132835745811462 \n",
      "     Training Step: 100 Training Loss: 0.613758385181427 \n",
      "     Training Step: 101 Training Loss: 0.6107448935508728 \n",
      "     Training Step: 102 Training Loss: 0.6162843704223633 \n",
      "     Training Step: 103 Training Loss: 0.6119000911712646 \n",
      "     Training Step: 104 Training Loss: 0.613344132900238 \n",
      "     Training Step: 105 Training Loss: 0.6103989481925964 \n",
      "     Training Step: 106 Training Loss: 0.6123006343841553 \n",
      "     Training Step: 107 Training Loss: 0.6134262681007385 \n",
      "     Training Step: 108 Training Loss: 0.6109129786491394 \n",
      "     Training Step: 109 Training Loss: 0.6153444647789001 \n",
      "     Training Step: 110 Training Loss: 0.6164928078651428 \n",
      "     Training Step: 111 Training Loss: 0.6141875386238098 \n",
      "     Training Step: 112 Training Loss: 0.6171609163284302 \n",
      "     Training Step: 113 Training Loss: 0.6111703515052795 \n",
      "     Training Step: 114 Training Loss: 0.6147663593292236 \n",
      "     Training Step: 115 Training Loss: 0.6128052473068237 \n",
      "     Training Step: 116 Training Loss: 0.6118732690811157 \n",
      "     Training Step: 117 Training Loss: 0.6106429696083069 \n",
      "     Training Step: 118 Training Loss: 0.6156665086746216 \n",
      "     Training Step: 119 Training Loss: 0.6167546510696411 \n",
      "     Training Step: 120 Training Loss: 0.6140629649162292 \n",
      "     Training Step: 121 Training Loss: 0.6169832348823547 \n",
      "     Training Step: 122 Training Loss: 0.614703357219696 \n",
      "     Training Step: 123 Training Loss: 0.617705762386322 \n",
      "     Training Step: 124 Training Loss: 0.6155385375022888 \n",
      "     Training Step: 125 Training Loss: 0.6132846474647522 \n",
      "     Training Step: 126 Training Loss: 0.6162127256393433 \n",
      "     Training Step: 127 Training Loss: 0.6155638694763184 \n",
      "     Training Step: 128 Training Loss: 0.6098294854164124 \n",
      "     Training Step: 129 Training Loss: 0.6150273680686951 \n",
      "     Training Step: 130 Training Loss: 0.616730809211731 \n",
      "     Training Step: 131 Training Loss: 0.6136256456375122 \n",
      "     Training Step: 132 Training Loss: 0.6101236939430237 \n",
      "     Training Step: 133 Training Loss: 0.6202875971794128 \n",
      "     Training Step: 134 Training Loss: 0.6158432364463806 \n",
      "     Training Step: 135 Training Loss: 0.610181450843811 \n",
      "     Training Step: 136 Training Loss: 0.6172277927398682 \n",
      "     Training Step: 137 Training Loss: 0.6142645478248596 \n",
      "     Training Step: 138 Training Loss: 0.6118344664573669 \n",
      "     Training Step: 139 Training Loss: 0.6153349280357361 \n",
      "     Training Step: 140 Training Loss: 0.6146116256713867 \n",
      "     Training Step: 141 Training Loss: 0.6162072420120239 \n",
      "     Training Step: 142 Training Loss: 0.6157518625259399 \n",
      "     Training Step: 143 Training Loss: 0.6118566393852234 \n",
      "     Training Step: 144 Training Loss: 0.6083646416664124 \n",
      "     Training Step: 145 Training Loss: 0.6114038825035095 \n",
      "     Training Step: 146 Training Loss: 0.6156561374664307 \n",
      "     Training Step: 147 Training Loss: 0.6168983578681946 \n",
      "     Training Step: 148 Training Loss: 0.6099797487258911 \n",
      "     Training Step: 149 Training Loss: 0.6147213578224182 \n",
      "     Training Step: 150 Training Loss: 0.6105667352676392 \n",
      "     Training Step: 151 Training Loss: 0.6149505972862244 \n",
      "     Training Step: 152 Training Loss: 0.6107606291770935 \n",
      "     Training Step: 153 Training Loss: 0.6121696829795837 \n",
      "     Training Step: 154 Training Loss: 0.6145297288894653 \n",
      "     Training Step: 155 Training Loss: 0.6146346926689148 \n",
      "     Training Step: 156 Training Loss: 0.6166999936103821 \n",
      "     Training Step: 157 Training Loss: 0.6142538785934448 \n",
      "     Training Step: 158 Training Loss: 0.6186097860336304 \n",
      "     Training Step: 159 Training Loss: 0.6141251921653748 \n",
      "     Training Step: 160 Training Loss: 0.6171502470970154 \n",
      "     Training Step: 161 Training Loss: 0.6169140338897705 \n",
      "     Training Step: 162 Training Loss: 0.6111176013946533 \n",
      "     Training Step: 163 Training Loss: 0.6121712327003479 \n",
      "     Training Step: 164 Training Loss: 0.6144854426383972 \n",
      "     Training Step: 165 Training Loss: 0.6158308386802673 \n",
      "     Training Step: 166 Training Loss: 0.612266480922699 \n",
      "     Training Step: 167 Training Loss: 0.6126605272293091 \n",
      "     Training Step: 168 Training Loss: 0.6159981489181519 \n",
      "     Training Step: 169 Training Loss: 0.6112437844276428 \n",
      "     Training Step: 170 Training Loss: 0.6168338060379028 \n",
      "     Training Step: 171 Training Loss: 0.616821825504303 \n",
      "     Training Step: 172 Training Loss: 0.6171714067459106 \n",
      "     Training Step: 173 Training Loss: 0.6117327809333801 \n",
      "     Training Step: 174 Training Loss: 0.6094868779182434 \n",
      "     Training Step: 175 Training Loss: 0.6144754886627197 \n",
      "     Training Step: 176 Training Loss: 0.6115993857383728 \n",
      "     Training Step: 177 Training Loss: 0.6185408234596252 \n",
      "     Training Step: 178 Training Loss: 0.6105660200119019 \n",
      "     Training Step: 179 Training Loss: 0.6139274835586548 \n",
      "     Training Step: 180 Training Loss: 0.6168378591537476 \n",
      "     Training Step: 181 Training Loss: 0.6145144701004028 \n",
      "     Training Step: 182 Training Loss: 0.6131148338317871 \n",
      "     Training Step: 183 Training Loss: 0.6144555807113647 \n",
      "     Training Step: 184 Training Loss: 0.6133139729499817 \n",
      "     Training Step: 185 Training Loss: 0.6130918860435486 \n",
      "     Training Step: 186 Training Loss: 0.6125551462173462 \n",
      "     Training Step: 187 Training Loss: 0.614845871925354 \n",
      "     Training Step: 188 Training Loss: 0.6139475107192993 \n",
      "     Training Step: 189 Training Loss: 0.6129907965660095 \n",
      "     Training Step: 190 Training Loss: 0.6153934001922607 \n",
      "     Training Step: 191 Training Loss: 0.6189713478088379 \n",
      "     Training Step: 192 Training Loss: 0.6209773421287537 \n",
      "     Training Step: 193 Training Loss: 0.6154218316078186 \n",
      "     Training Step: 194 Training Loss: 0.6133718490600586 \n",
      "     Training Step: 195 Training Loss: 0.6119499206542969 \n",
      "     Training Step: 196 Training Loss: 0.6158057451248169 \n",
      "     Training Step: 197 Training Loss: 0.6106387376785278 \n",
      "     Training Step: 198 Training Loss: 0.6125752329826355 \n",
      "     Training Step: 199 Training Loss: 0.6197910904884338 \n",
      "     Training Step: 200 Training Loss: 0.6163923740386963 \n",
      "     Training Step: 201 Training Loss: 0.6177505850791931 \n",
      "     Training Step: 202 Training Loss: 0.614138662815094 \n",
      "     Training Step: 203 Training Loss: 0.6167885065078735 \n",
      "     Training Step: 204 Training Loss: 0.6117019057273865 \n",
      "     Training Step: 205 Training Loss: 0.6142035722732544 \n",
      "     Training Step: 206 Training Loss: 0.613770604133606 \n",
      "     Training Step: 207 Training Loss: 0.6138595342636108 \n",
      "     Training Step: 208 Training Loss: 0.6167133450508118 \n",
      "     Training Step: 209 Training Loss: 0.6114441156387329 \n",
      "     Training Step: 210 Training Loss: 0.6143832206726074 \n",
      "     Training Step: 211 Training Loss: 0.611758291721344 \n",
      "     Training Step: 212 Training Loss: 0.6167465448379517 \n",
      "     Training Step: 213 Training Loss: 0.6151918768882751 \n",
      "     Training Step: 214 Training Loss: 0.6148797273635864 \n",
      "     Training Step: 215 Training Loss: 0.614740252494812 \n",
      "     Training Step: 216 Training Loss: 0.6180594563484192 \n",
      "     Training Step: 217 Training Loss: 0.6115532517433167 \n",
      "     Training Step: 218 Training Loss: 0.6124100089073181 \n",
      "     Training Step: 219 Training Loss: 0.6127772331237793 \n",
      "     Training Step: 220 Training Loss: 0.6125402450561523 \n",
      "     Training Step: 221 Training Loss: 0.6177917122840881 \n",
      "     Training Step: 222 Training Loss: 0.6174328327178955 \n",
      "     Training Step: 223 Training Loss: 0.6183605790138245 \n",
      "     Training Step: 224 Training Loss: 0.6138661503791809 \n",
      "     Training Step: 225 Training Loss: 0.614694356918335 \n",
      "     Training Step: 226 Training Loss: 0.6106504797935486 \n",
      "     Training Step: 227 Training Loss: 0.6107243299484253 \n",
      "     Training Step: 228 Training Loss: 0.6127668619155884 \n",
      "     Training Step: 229 Training Loss: 0.6137436628341675 \n",
      "     Training Step: 230 Training Loss: 0.6148266196250916 \n",
      "     Training Step: 231 Training Loss: 0.6146891117095947 \n",
      "     Training Step: 232 Training Loss: 0.613550066947937 \n",
      "     Training Step: 233 Training Loss: 0.6125836372375488 \n",
      "     Training Step: 234 Training Loss: 0.6129840016365051 \n",
      "     Training Step: 235 Training Loss: 0.6146555542945862 \n",
      "     Training Step: 236 Training Loss: 0.61814945936203 \n",
      "     Training Step: 237 Training Loss: 0.6197732090950012 \n",
      "     Training Step: 238 Training Loss: 0.6158387660980225 \n",
      "     Training Step: 239 Training Loss: 0.6143962144851685 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.610338568687439 \n",
      "     Validation Step: 1 Validation Loss: 0.6106478571891785 \n",
      "     Validation Step: 2 Validation Loss: 0.614265501499176 \n",
      "     Validation Step: 3 Validation Loss: 0.6122655272483826 \n",
      "     Validation Step: 4 Validation Loss: 0.6120263934135437 \n",
      "     Validation Step: 5 Validation Loss: 0.6173259615898132 \n",
      "     Validation Step: 6 Validation Loss: 0.6102940440177917 \n",
      "     Validation Step: 7 Validation Loss: 0.6170257329940796 \n",
      "     Validation Step: 8 Validation Loss: 0.6153507232666016 \n",
      "     Validation Step: 9 Validation Loss: 0.6156381368637085 \n",
      "     Validation Step: 10 Validation Loss: 0.6129620671272278 \n",
      "     Validation Step: 11 Validation Loss: 0.6137183904647827 \n",
      "     Validation Step: 12 Validation Loss: 0.6182032227516174 \n",
      "     Validation Step: 13 Validation Loss: 0.6159830093383789 \n",
      "     Validation Step: 14 Validation Loss: 0.6152728796005249 \n",
      "     Validation Step: 15 Validation Loss: 0.6112874746322632 \n",
      "     Validation Step: 16 Validation Loss: 0.6155821681022644 \n",
      "     Validation Step: 17 Validation Loss: 0.6149840354919434 \n",
      "     Validation Step: 18 Validation Loss: 0.6141976118087769 \n",
      "     Validation Step: 19 Validation Loss: 0.6146889328956604 \n",
      "     Validation Step: 20 Validation Loss: 0.6146227717399597 \n",
      "     Validation Step: 21 Validation Loss: 0.6134066581726074 \n",
      "     Validation Step: 22 Validation Loss: 0.6143559813499451 \n",
      "     Validation Step: 23 Validation Loss: 0.610366940498352 \n",
      "     Validation Step: 24 Validation Loss: 0.6184770464897156 \n",
      "     Validation Step: 25 Validation Loss: 0.6113092303276062 \n",
      "     Validation Step: 26 Validation Loss: 0.617684543132782 \n",
      "     Validation Step: 27 Validation Loss: 0.6162625551223755 \n",
      "     Validation Step: 28 Validation Loss: 0.6150547862052917 \n",
      "     Validation Step: 29 Validation Loss: 0.6180538535118103 \n",
      "     Validation Step: 30 Validation Loss: 0.6183180809020996 \n",
      "     Validation Step: 31 Validation Loss: 0.6077789664268494 \n",
      "     Validation Step: 32 Validation Loss: 0.611685574054718 \n",
      "     Validation Step: 33 Validation Loss: 0.6130743026733398 \n",
      "     Validation Step: 34 Validation Loss: 0.6106283068656921 \n",
      "     Validation Step: 35 Validation Loss: 0.610810399055481 \n",
      "     Validation Step: 36 Validation Loss: 0.6158099174499512 \n",
      "     Validation Step: 37 Validation Loss: 0.6184788346290588 \n",
      "     Validation Step: 38 Validation Loss: 0.6142335534095764 \n",
      "     Validation Step: 39 Validation Loss: 0.6176350116729736 \n",
      "     Validation Step: 40 Validation Loss: 0.6136828660964966 \n",
      "     Validation Step: 41 Validation Loss: 0.6137549877166748 \n",
      "     Validation Step: 42 Validation Loss: 0.6117873787879944 \n",
      "     Validation Step: 43 Validation Loss: 0.6146299242973328 \n",
      "     Validation Step: 44 Validation Loss: 0.6149011850357056 \n",
      "Epoch: 61\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6103530526161194 \n",
      "     Training Step: 1 Training Loss: 0.6133602261543274 \n",
      "     Training Step: 2 Training Loss: 0.6147176623344421 \n",
      "     Training Step: 3 Training Loss: 0.6121869087219238 \n",
      "     Training Step: 4 Training Loss: 0.6169731020927429 \n",
      "     Training Step: 5 Training Loss: 0.6143023371696472 \n",
      "     Training Step: 6 Training Loss: 0.6186917424201965 \n",
      "     Training Step: 7 Training Loss: 0.6184030771255493 \n",
      "     Training Step: 8 Training Loss: 0.6146667003631592 \n",
      "     Training Step: 9 Training Loss: 0.6152450442314148 \n",
      "     Training Step: 10 Training Loss: 0.6122227907180786 \n",
      "     Training Step: 11 Training Loss: 0.6141958236694336 \n",
      "     Training Step: 12 Training Loss: 0.6145952343940735 \n",
      "     Training Step: 13 Training Loss: 0.6117405295372009 \n",
      "     Training Step: 14 Training Loss: 0.6158205270767212 \n",
      "     Training Step: 15 Training Loss: 0.6136818528175354 \n",
      "     Training Step: 16 Training Loss: 0.615532398223877 \n",
      "     Training Step: 17 Training Loss: 0.6168102622032166 \n",
      "     Training Step: 18 Training Loss: 0.6167742013931274 \n",
      "     Training Step: 19 Training Loss: 0.6161637306213379 \n",
      "     Training Step: 20 Training Loss: 0.6184550523757935 \n",
      "     Training Step: 21 Training Loss: 0.6180445551872253 \n",
      "     Training Step: 22 Training Loss: 0.6147153973579407 \n",
      "     Training Step: 23 Training Loss: 0.6115080714225769 \n",
      "     Training Step: 24 Training Loss: 0.6177816987037659 \n",
      "     Training Step: 25 Training Loss: 0.6168478727340698 \n",
      "     Training Step: 26 Training Loss: 0.6141590476036072 \n",
      "     Training Step: 27 Training Loss: 0.6115933060646057 \n",
      "     Training Step: 28 Training Loss: 0.6159937977790833 \n",
      "     Training Step: 29 Training Loss: 0.6116817593574524 \n",
      "     Training Step: 30 Training Loss: 0.6140536069869995 \n",
      "     Training Step: 31 Training Loss: 0.618294894695282 \n",
      "     Training Step: 32 Training Loss: 0.6149641871452332 \n",
      "     Training Step: 33 Training Loss: 0.6153567433357239 \n",
      "     Training Step: 34 Training Loss: 0.6128660440444946 \n",
      "     Training Step: 35 Training Loss: 0.6199100613594055 \n",
      "     Training Step: 36 Training Loss: 0.6117238402366638 \n",
      "     Training Step: 37 Training Loss: 0.6140903830528259 \n",
      "     Training Step: 38 Training Loss: 0.6115620732307434 \n",
      "     Training Step: 39 Training Loss: 0.6121513843536377 \n",
      "     Training Step: 40 Training Loss: 0.6203529238700867 \n",
      "     Training Step: 41 Training Loss: 0.6129498481750488 \n",
      "     Training Step: 42 Training Loss: 0.6105585694313049 \n",
      "     Training Step: 43 Training Loss: 0.6115837693214417 \n",
      "     Training Step: 44 Training Loss: 0.6138076186180115 \n",
      "     Training Step: 45 Training Loss: 0.6123887896537781 \n",
      "     Training Step: 46 Training Loss: 0.6154762506484985 \n",
      "     Training Step: 47 Training Loss: 0.6148021817207336 \n",
      "     Training Step: 48 Training Loss: 0.6123045086860657 \n",
      "     Training Step: 49 Training Loss: 0.6146775484085083 \n",
      "     Training Step: 50 Training Loss: 0.6125257611274719 \n",
      "     Training Step: 51 Training Loss: 0.6143571138381958 \n",
      "     Training Step: 52 Training Loss: 0.6118565201759338 \n",
      "     Training Step: 53 Training Loss: 0.6160205006599426 \n",
      "     Training Step: 54 Training Loss: 0.618037223815918 \n",
      "     Training Step: 55 Training Loss: 0.6134852170944214 \n",
      "     Training Step: 56 Training Loss: 0.6146109104156494 \n",
      "     Training Step: 57 Training Loss: 0.6167280673980713 \n",
      "     Training Step: 58 Training Loss: 0.6107885837554932 \n",
      "     Training Step: 59 Training Loss: 0.6141603589057922 \n",
      "     Training Step: 60 Training Loss: 0.6133063435554504 \n",
      "     Training Step: 61 Training Loss: 0.6186853051185608 \n",
      "     Training Step: 62 Training Loss: 0.61644446849823 \n",
      "     Training Step: 63 Training Loss: 0.6144636869430542 \n",
      "     Training Step: 64 Training Loss: 0.6164942979812622 \n",
      "     Training Step: 65 Training Loss: 0.61283940076828 \n",
      "     Training Step: 66 Training Loss: 0.6153067350387573 \n",
      "     Training Step: 67 Training Loss: 0.6083924174308777 \n",
      "     Training Step: 68 Training Loss: 0.615568220615387 \n",
      "     Training Step: 69 Training Loss: 0.6158667802810669 \n",
      "     Training Step: 70 Training Loss: 0.6103938817977905 \n",
      "     Training Step: 71 Training Loss: 0.6195727586746216 \n",
      "     Training Step: 72 Training Loss: 0.6105888485908508 \n",
      "     Training Step: 73 Training Loss: 0.6119015216827393 \n",
      "     Training Step: 74 Training Loss: 0.6133280992507935 \n",
      "     Training Step: 75 Training Loss: 0.6153004765510559 \n",
      "     Training Step: 76 Training Loss: 0.6114200949668884 \n",
      "     Training Step: 77 Training Loss: 0.614728569984436 \n",
      "     Training Step: 78 Training Loss: 0.6133210062980652 \n",
      "     Training Step: 79 Training Loss: 0.6166841387748718 \n",
      "     Training Step: 80 Training Loss: 0.6182378530502319 \n",
      "     Training Step: 81 Training Loss: 0.6180586814880371 \n",
      "     Training Step: 82 Training Loss: 0.6155645251274109 \n",
      "     Training Step: 83 Training Loss: 0.6129917502403259 \n",
      "     Training Step: 84 Training Loss: 0.6115981340408325 \n",
      "     Training Step: 85 Training Loss: 0.6151875853538513 \n",
      "     Training Step: 86 Training Loss: 0.6132497787475586 \n",
      "     Training Step: 87 Training Loss: 0.6116843819618225 \n",
      "     Training Step: 88 Training Loss: 0.6124140024185181 \n",
      "     Training Step: 89 Training Loss: 0.6126716732978821 \n",
      "     Training Step: 90 Training Loss: 0.6144614815711975 \n",
      "     Training Step: 91 Training Loss: 0.6172612309455872 \n",
      "     Training Step: 92 Training Loss: 0.6104966998100281 \n",
      "     Training Step: 93 Training Loss: 0.6147550344467163 \n",
      "     Training Step: 94 Training Loss: 0.6105427145957947 \n",
      "     Training Step: 95 Training Loss: 0.6131604313850403 \n",
      "     Training Step: 96 Training Loss: 0.6150306463241577 \n",
      "     Training Step: 97 Training Loss: 0.6164792776107788 \n",
      "     Training Step: 98 Training Loss: 0.6154935359954834 \n",
      "     Training Step: 99 Training Loss: 0.6109192371368408 \n",
      "     Training Step: 100 Training Loss: 0.6167464256286621 \n",
      "     Training Step: 101 Training Loss: 0.6129572987556458 \n",
      "     Training Step: 102 Training Loss: 0.6168127655982971 \n",
      "     Training Step: 103 Training Loss: 0.6142272353172302 \n",
      "     Training Step: 104 Training Loss: 0.617414653301239 \n",
      "     Training Step: 105 Training Loss: 0.6164180040359497 \n",
      "     Training Step: 106 Training Loss: 0.615421712398529 \n",
      "     Training Step: 107 Training Loss: 0.618821382522583 \n",
      "     Training Step: 108 Training Loss: 0.6178028583526611 \n",
      "     Training Step: 109 Training Loss: 0.6151646971702576 \n",
      "     Training Step: 110 Training Loss: 0.6177563071250916 \n",
      "     Training Step: 111 Training Loss: 0.615408182144165 \n",
      "     Training Step: 112 Training Loss: 0.6115496158599854 \n",
      "     Training Step: 113 Training Loss: 0.6124919652938843 \n",
      "     Training Step: 114 Training Loss: 0.6136536002159119 \n",
      "     Training Step: 115 Training Loss: 0.6169523000717163 \n",
      "     Training Step: 116 Training Loss: 0.6154399514198303 \n",
      "     Training Step: 117 Training Loss: 0.6135177612304688 \n",
      "     Training Step: 118 Training Loss: 0.6136507987976074 \n",
      "     Training Step: 119 Training Loss: 0.611528754234314 \n",
      "     Training Step: 120 Training Loss: 0.6159101724624634 \n",
      "     Training Step: 121 Training Loss: 0.6118007302284241 \n",
      "     Training Step: 122 Training Loss: 0.6162605285644531 \n",
      "     Training Step: 123 Training Loss: 0.6144333481788635 \n",
      "     Training Step: 124 Training Loss: 0.6144880652427673 \n",
      "     Training Step: 125 Training Loss: 0.6116218566894531 \n",
      "     Training Step: 126 Training Loss: 0.6167497634887695 \n",
      "     Training Step: 127 Training Loss: 0.6111779808998108 \n",
      "     Training Step: 128 Training Loss: 0.6133757829666138 \n",
      "     Training Step: 129 Training Loss: 0.61435467004776 \n",
      "     Training Step: 130 Training Loss: 0.6100834012031555 \n",
      "     Training Step: 131 Training Loss: 0.6139256954193115 \n",
      "     Training Step: 132 Training Loss: 0.6146987080574036 \n",
      "     Training Step: 133 Training Loss: 0.6131179928779602 \n",
      "     Training Step: 134 Training Loss: 0.6141849160194397 \n",
      "     Training Step: 135 Training Loss: 0.6172205209732056 \n",
      "     Training Step: 136 Training Loss: 0.6093467473983765 \n",
      "     Training Step: 137 Training Loss: 0.6152569651603699 \n",
      "     Training Step: 138 Training Loss: 0.6097579598426819 \n",
      "     Training Step: 139 Training Loss: 0.6118267774581909 \n",
      "     Training Step: 140 Training Loss: 0.6101135611534119 \n",
      "     Training Step: 141 Training Loss: 0.6138023734092712 \n",
      "     Training Step: 142 Training Loss: 0.6111821532249451 \n",
      "     Training Step: 143 Training Loss: 0.6153589487075806 \n",
      "     Training Step: 144 Training Loss: 0.6168568134307861 \n",
      "     Training Step: 145 Training Loss: 0.6109529733657837 \n",
      "     Training Step: 146 Training Loss: 0.6102270483970642 \n",
      "     Training Step: 147 Training Loss: 0.6154086589813232 \n",
      "     Training Step: 148 Training Loss: 0.6196881532669067 \n",
      "     Training Step: 149 Training Loss: 0.6147501468658447 \n",
      "     Training Step: 150 Training Loss: 0.6175076365470886 \n",
      "     Training Step: 151 Training Loss: 0.612930417060852 \n",
      "     Training Step: 152 Training Loss: 0.610693097114563 \n",
      "     Training Step: 153 Training Loss: 0.6140364408493042 \n",
      "     Training Step: 154 Training Loss: 0.6100919842720032 \n",
      "     Training Step: 155 Training Loss: 0.6163086891174316 \n",
      "     Training Step: 156 Training Loss: 0.6123999953269958 \n",
      "     Training Step: 157 Training Loss: 0.6198163032531738 \n",
      "     Training Step: 158 Training Loss: 0.6134944558143616 \n",
      "     Training Step: 159 Training Loss: 0.6149402260780334 \n",
      "     Training Step: 160 Training Loss: 0.6157292127609253 \n",
      "     Training Step: 161 Training Loss: 0.6098672151565552 \n",
      "     Training Step: 162 Training Loss: 0.6171053051948547 \n",
      "     Training Step: 163 Training Loss: 0.6136640310287476 \n",
      "     Training Step: 164 Training Loss: 0.615125298500061 \n",
      "     Training Step: 165 Training Loss: 0.6094362139701843 \n",
      "     Training Step: 166 Training Loss: 0.6106534004211426 \n",
      "     Training Step: 167 Training Loss: 0.6137775182723999 \n",
      "     Training Step: 168 Training Loss: 0.6105664968490601 \n",
      "     Training Step: 169 Training Loss: 0.6115207076072693 \n",
      "     Training Step: 170 Training Loss: 0.6156412959098816 \n",
      "     Training Step: 171 Training Loss: 0.6130781769752502 \n",
      "     Training Step: 172 Training Loss: 0.6146360635757446 \n",
      "     Training Step: 173 Training Loss: 0.6146939396858215 \n",
      "     Training Step: 174 Training Loss: 0.6131354570388794 \n",
      "     Training Step: 175 Training Loss: 0.6145249605178833 \n",
      "     Training Step: 176 Training Loss: 0.6152939200401306 \n",
      "     Training Step: 177 Training Loss: 0.6107068061828613 \n",
      "     Training Step: 178 Training Loss: 0.6120520234107971 \n",
      "     Training Step: 179 Training Loss: 0.6171826720237732 \n",
      "     Training Step: 180 Training Loss: 0.6138924956321716 \n",
      "     Training Step: 181 Training Loss: 0.6127111911773682 \n",
      "     Training Step: 182 Training Loss: 0.617145836353302 \n",
      "     Training Step: 183 Training Loss: 0.6125595569610596 \n",
      "     Training Step: 184 Training Loss: 0.612265408039093 \n",
      "     Training Step: 185 Training Loss: 0.6166797280311584 \n",
      "     Training Step: 186 Training Loss: 0.6142435073852539 \n",
      "     Training Step: 187 Training Loss: 0.6146954298019409 \n",
      "     Training Step: 188 Training Loss: 0.6132151484489441 \n",
      "     Training Step: 189 Training Loss: 0.6188943982124329 \n",
      "     Training Step: 190 Training Loss: 0.6181114315986633 \n",
      "     Training Step: 191 Training Loss: 0.615778386592865 \n",
      "     Training Step: 192 Training Loss: 0.6148021817207336 \n",
      "     Training Step: 193 Training Loss: 0.60957270860672 \n",
      "     Training Step: 194 Training Loss: 0.6177169680595398 \n",
      "     Training Step: 195 Training Loss: 0.6151163578033447 \n",
      "     Training Step: 196 Training Loss: 0.6139814257621765 \n",
      "     Training Step: 197 Training Loss: 0.6112105846405029 \n",
      "     Training Step: 198 Training Loss: 0.6132789850234985 \n",
      "     Training Step: 199 Training Loss: 0.6114258766174316 \n",
      "     Training Step: 200 Training Loss: 0.6148961186408997 \n",
      "     Training Step: 201 Training Loss: 0.6107489466667175 \n",
      "     Training Step: 202 Training Loss: 0.6097309589385986 \n",
      "     Training Step: 203 Training Loss: 0.6121556758880615 \n",
      "     Training Step: 204 Training Loss: 0.613853931427002 \n",
      "     Training Step: 205 Training Loss: 0.6156086325645447 \n",
      "     Training Step: 206 Training Loss: 0.6143113970756531 \n",
      "     Training Step: 207 Training Loss: 0.6118131279945374 \n",
      "     Training Step: 208 Training Loss: 0.6166912913322449 \n",
      "     Training Step: 209 Training Loss: 0.6209477782249451 \n",
      "     Training Step: 210 Training Loss: 0.6123103499412537 \n",
      "     Training Step: 211 Training Loss: 0.61191725730896 \n",
      "     Training Step: 212 Training Loss: 0.6100534200668335 \n",
      "     Training Step: 213 Training Loss: 0.6184995174407959 \n",
      "     Training Step: 214 Training Loss: 0.6106224060058594 \n",
      "     Training Step: 215 Training Loss: 0.6158236861228943 \n",
      "     Training Step: 216 Training Loss: 0.616066038608551 \n",
      "     Training Step: 217 Training Loss: 0.6178324818611145 \n",
      "     Training Step: 218 Training Loss: 0.612395167350769 \n",
      "     Training Step: 219 Training Loss: 0.6148767471313477 \n",
      "     Training Step: 220 Training Loss: 0.6134559512138367 \n",
      "     Training Step: 221 Training Loss: 0.6125431060791016 \n",
      "     Training Step: 222 Training Loss: 0.617818295955658 \n",
      "     Training Step: 223 Training Loss: 0.617034912109375 \n",
      "     Training Step: 224 Training Loss: 0.6132645010948181 \n",
      "     Training Step: 225 Training Loss: 0.6156834959983826 \n",
      "     Training Step: 226 Training Loss: 0.616214394569397 \n",
      "     Training Step: 227 Training Loss: 0.6167414784431458 \n",
      "     Training Step: 228 Training Loss: 0.6128968000411987 \n",
      "     Training Step: 229 Training Loss: 0.6123538017272949 \n",
      "     Training Step: 230 Training Loss: 0.6125420928001404 \n",
      "     Training Step: 231 Training Loss: 0.6116641759872437 \n",
      "     Training Step: 232 Training Loss: 0.6136170029640198 \n",
      "     Training Step: 233 Training Loss: 0.6129465103149414 \n",
      "     Training Step: 234 Training Loss: 0.6121016144752502 \n",
      "     Training Step: 235 Training Loss: 0.6144101619720459 \n",
      "     Training Step: 236 Training Loss: 0.6118723750114441 \n",
      "     Training Step: 237 Training Loss: 0.6130290031433105 \n",
      "     Training Step: 238 Training Loss: 0.6184783577919006 \n",
      "     Training Step: 239 Training Loss: 0.617636501789093 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6107197403907776 \n",
      "     Validation Step: 1 Validation Loss: 0.6117057800292969 \n",
      "     Validation Step: 2 Validation Loss: 0.6102290153503418 \n",
      "     Validation Step: 3 Validation Loss: 0.6148715615272522 \n",
      "     Validation Step: 4 Validation Loss: 0.6136909127235413 \n",
      "     Validation Step: 5 Validation Loss: 0.6116163730621338 \n",
      "     Validation Step: 6 Validation Loss: 0.6173158288002014 \n",
      "     Validation Step: 7 Validation Loss: 0.6136664152145386 \n",
      "     Validation Step: 8 Validation Loss: 0.6105452179908752 \n",
      "     Validation Step: 9 Validation Loss: 0.6105812191963196 \n",
      "     Validation Step: 10 Validation Loss: 0.6141695380210876 \n",
      "     Validation Step: 11 Validation Loss: 0.6145943999290466 \n",
      "     Validation Step: 12 Validation Loss: 0.6142929196357727 \n",
      "     Validation Step: 13 Validation Loss: 0.6184778213500977 \n",
      "     Validation Step: 14 Validation Loss: 0.6136962175369263 \n",
      "     Validation Step: 15 Validation Loss: 0.607651948928833 \n",
      "     Validation Step: 16 Validation Loss: 0.6141639351844788 \n",
      "     Validation Step: 17 Validation Loss: 0.6101964712142944 \n",
      "     Validation Step: 18 Validation Loss: 0.6130310297012329 \n",
      "     Validation Step: 19 Validation Loss: 0.611238420009613 \n",
      "     Validation Step: 20 Validation Loss: 0.6146607995033264 \n",
      "     Validation Step: 21 Validation Loss: 0.6102535724639893 \n",
      "     Validation Step: 22 Validation Loss: 0.6152478456497192 \n",
      "     Validation Step: 23 Validation Loss: 0.615822970867157 \n",
      "     Validation Step: 24 Validation Loss: 0.616252601146698 \n",
      "     Validation Step: 25 Validation Loss: 0.6160027980804443 \n",
      "     Validation Step: 26 Validation Loss: 0.6183356642723083 \n",
      "     Validation Step: 27 Validation Loss: 0.6150493025779724 \n",
      "     Validation Step: 28 Validation Loss: 0.6119574904441833 \n",
      "     Validation Step: 29 Validation Loss: 0.615575909614563 \n",
      "     Validation Step: 30 Validation Loss: 0.6170168519020081 \n",
      "     Validation Step: 31 Validation Loss: 0.6177005171775818 \n",
      "     Validation Step: 32 Validation Loss: 0.6153327226638794 \n",
      "     Validation Step: 33 Validation Loss: 0.615626871585846 \n",
      "     Validation Step: 34 Validation Loss: 0.6121978759765625 \n",
      "     Validation Step: 35 Validation Loss: 0.6149325370788574 \n",
      "     Validation Step: 36 Validation Loss: 0.617610514163971 \n",
      "     Validation Step: 37 Validation Loss: 0.6145672798156738 \n",
      "     Validation Step: 38 Validation Loss: 0.6182284951210022 \n",
      "     Validation Step: 39 Validation Loss: 0.6133435964584351 \n",
      "     Validation Step: 40 Validation Loss: 0.6128928661346436 \n",
      "     Validation Step: 41 Validation Loss: 0.614232063293457 \n",
      "     Validation Step: 42 Validation Loss: 0.6184903383255005 \n",
      "     Validation Step: 43 Validation Loss: 0.6112269759178162 \n",
      "     Validation Step: 44 Validation Loss: 0.6180599331855774 \n",
      "Epoch: 62\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6152841448783875 \n",
      "     Training Step: 1 Training Loss: 0.6129375100135803 \n",
      "     Training Step: 2 Training Loss: 0.6097959280014038 \n",
      "     Training Step: 3 Training Loss: 0.6111915707588196 \n",
      "     Training Step: 4 Training Loss: 0.6142210364341736 \n",
      "     Training Step: 5 Training Loss: 0.6187275052070618 \n",
      "     Training Step: 6 Training Loss: 0.6140297651290894 \n",
      "     Training Step: 7 Training Loss: 0.610458254814148 \n",
      "     Training Step: 8 Training Loss: 0.6159586310386658 \n",
      "     Training Step: 9 Training Loss: 0.6147706508636475 \n",
      "     Training Step: 10 Training Loss: 0.6124074459075928 \n",
      "     Training Step: 11 Training Loss: 0.6151641011238098 \n",
      "     Training Step: 12 Training Loss: 0.6136682033538818 \n",
      "     Training Step: 13 Training Loss: 0.614363431930542 \n",
      "     Training Step: 14 Training Loss: 0.6123951077461243 \n",
      "     Training Step: 15 Training Loss: 0.6124691367149353 \n",
      "     Training Step: 16 Training Loss: 0.6160188913345337 \n",
      "     Training Step: 17 Training Loss: 0.6180399656295776 \n",
      "     Training Step: 18 Training Loss: 0.6176921725273132 \n",
      "     Training Step: 19 Training Loss: 0.6145321130752563 \n",
      "     Training Step: 20 Training Loss: 0.6146646738052368 \n",
      "     Training Step: 21 Training Loss: 0.6137615442276001 \n",
      "     Training Step: 22 Training Loss: 0.6129781007766724 \n",
      "     Training Step: 23 Training Loss: 0.6143705248832703 \n",
      "     Training Step: 24 Training Loss: 0.6115521788597107 \n",
      "     Training Step: 25 Training Loss: 0.6114449501037598 \n",
      "     Training Step: 26 Training Loss: 0.6116123199462891 \n",
      "     Training Step: 27 Training Loss: 0.6108804941177368 \n",
      "     Training Step: 28 Training Loss: 0.6120862364768982 \n",
      "     Training Step: 29 Training Loss: 0.6116352081298828 \n",
      "     Training Step: 30 Training Loss: 0.6143112778663635 \n",
      "     Training Step: 31 Training Loss: 0.6154602766036987 \n",
      "     Training Step: 32 Training Loss: 0.6154611706733704 \n",
      "     Training Step: 33 Training Loss: 0.6123554110527039 \n",
      "     Training Step: 34 Training Loss: 0.6197307109832764 \n",
      "     Training Step: 35 Training Loss: 0.6098612546920776 \n",
      "     Training Step: 36 Training Loss: 0.6184878349304199 \n",
      "     Training Step: 37 Training Loss: 0.6171462535858154 \n",
      "     Training Step: 38 Training Loss: 0.6129693984985352 \n",
      "     Training Step: 39 Training Loss: 0.6177623867988586 \n",
      "     Training Step: 40 Training Loss: 0.6123042106628418 \n",
      "     Training Step: 41 Training Loss: 0.6169136762619019 \n",
      "     Training Step: 42 Training Loss: 0.6112158894538879 \n",
      "     Training Step: 43 Training Loss: 0.6153244376182556 \n",
      "     Training Step: 44 Training Loss: 0.6133263111114502 \n",
      "     Training Step: 45 Training Loss: 0.6177217364311218 \n",
      "     Training Step: 46 Training Loss: 0.6180827617645264 \n",
      "     Training Step: 47 Training Loss: 0.6153882741928101 \n",
      "     Training Step: 48 Training Loss: 0.615784764289856 \n",
      "     Training Step: 49 Training Loss: 0.61201411485672 \n",
      "     Training Step: 50 Training Loss: 0.6136666536331177 \n",
      "     Training Step: 51 Training Loss: 0.6194940209388733 \n",
      "     Training Step: 52 Training Loss: 0.614221453666687 \n",
      "     Training Step: 53 Training Loss: 0.6163025498390198 \n",
      "     Training Step: 54 Training Loss: 0.6122584939002991 \n",
      "     Training Step: 55 Training Loss: 0.6152895092964172 \n",
      "     Training Step: 56 Training Loss: 0.6138992309570312 \n",
      "     Training Step: 57 Training Loss: 0.6083263754844666 \n",
      "     Training Step: 58 Training Loss: 0.6133694648742676 \n",
      "     Training Step: 59 Training Loss: 0.6131432056427002 \n",
      "     Training Step: 60 Training Loss: 0.6167848110198975 \n",
      "     Training Step: 61 Training Loss: 0.6168643236160278 \n",
      "     Training Step: 62 Training Loss: 0.6165019869804382 \n",
      "     Training Step: 63 Training Loss: 0.6133320331573486 \n",
      "     Training Step: 64 Training Loss: 0.6158161759376526 \n",
      "     Training Step: 65 Training Loss: 0.6131584644317627 \n",
      "     Training Step: 66 Training Loss: 0.6122629046440125 \n",
      "     Training Step: 67 Training Loss: 0.6172275543212891 \n",
      "     Training Step: 68 Training Loss: 0.6167842745780945 \n",
      "     Training Step: 69 Training Loss: 0.6155705451965332 \n",
      "     Training Step: 70 Training Loss: 0.6151057481765747 \n",
      "     Training Step: 71 Training Loss: 0.6138433218002319 \n",
      "     Training Step: 72 Training Loss: 0.6116857528686523 \n",
      "     Training Step: 73 Training Loss: 0.6126905679702759 \n",
      "     Training Step: 74 Training Loss: 0.6122132539749146 \n",
      "     Training Step: 75 Training Loss: 0.6132137775421143 \n",
      "     Training Step: 76 Training Loss: 0.6146928668022156 \n",
      "     Training Step: 77 Training Loss: 0.617830216884613 \n",
      "     Training Step: 78 Training Loss: 0.6135249137878418 \n",
      "     Training Step: 79 Training Loss: 0.6132960319519043 \n",
      "     Training Step: 80 Training Loss: 0.6135096549987793 \n",
      "     Training Step: 81 Training Loss: 0.6121743321418762 \n",
      "     Training Step: 82 Training Loss: 0.6128737330436707 \n",
      "     Training Step: 83 Training Loss: 0.6147240400314331 \n",
      "     Training Step: 84 Training Loss: 0.6144617795944214 \n",
      "     Training Step: 85 Training Loss: 0.615822970867157 \n",
      "     Training Step: 86 Training Loss: 0.6183609962463379 \n",
      "     Training Step: 87 Training Loss: 0.6141301393508911 \n",
      "     Training Step: 88 Training Loss: 0.6184219717979431 \n",
      "     Training Step: 89 Training Loss: 0.6103812456130981 \n",
      "     Training Step: 90 Training Loss: 0.6121090650558472 \n",
      "     Training Step: 91 Training Loss: 0.616141140460968 \n",
      "     Training Step: 92 Training Loss: 0.6100850701332092 \n",
      "     Training Step: 93 Training Loss: 0.6115539073944092 \n",
      "     Training Step: 94 Training Loss: 0.6157098412513733 \n",
      "     Training Step: 95 Training Loss: 0.6134945154190063 \n",
      "     Training Step: 96 Training Loss: 0.6107224225997925 \n",
      "     Training Step: 97 Training Loss: 0.6146078109741211 \n",
      "     Training Step: 98 Training Loss: 0.6121740937232971 \n",
      "     Training Step: 99 Training Loss: 0.6152684092521667 \n",
      "     Training Step: 100 Training Loss: 0.61361163854599 \n",
      "     Training Step: 101 Training Loss: 0.6146335005760193 \n",
      "     Training Step: 102 Training Loss: 0.6144005656242371 \n",
      "     Training Step: 103 Training Loss: 0.6148158311843872 \n",
      "     Training Step: 104 Training Loss: 0.6118333339691162 \n",
      "     Training Step: 105 Training Loss: 0.6147620677947998 \n",
      "     Training Step: 106 Training Loss: 0.6152982115745544 \n",
      "     Training Step: 107 Training Loss: 0.614928662776947 \n",
      "     Training Step: 108 Training Loss: 0.61253422498703 \n",
      "     Training Step: 109 Training Loss: 0.6134324669837952 \n",
      "     Training Step: 110 Training Loss: 0.6118516325950623 \n",
      "     Training Step: 111 Training Loss: 0.613102912902832 \n",
      "     Training Step: 112 Training Loss: 0.6131507158279419 \n",
      "     Training Step: 113 Training Loss: 0.6144322156906128 \n",
      "     Training Step: 114 Training Loss: 0.6186519861221313 \n",
      "     Training Step: 115 Training Loss: 0.6140902042388916 \n",
      "     Training Step: 116 Training Loss: 0.615443229675293 \n",
      "     Training Step: 117 Training Loss: 0.6113330125808716 \n",
      "     Training Step: 118 Training Loss: 0.6153953075408936 \n",
      "     Training Step: 119 Training Loss: 0.6105600595474243 \n",
      "     Training Step: 120 Training Loss: 0.6128510236740112 \n",
      "     Training Step: 121 Training Loss: 0.6151695251464844 \n",
      "     Training Step: 122 Training Loss: 0.6115582585334778 \n",
      "     Training Step: 123 Training Loss: 0.6201168298721313 \n",
      "     Training Step: 124 Training Loss: 0.6115732789039612 \n",
      "     Training Step: 125 Training Loss: 0.6146649122238159 \n",
      "     Training Step: 126 Training Loss: 0.6209287643432617 \n",
      "     Training Step: 127 Training Loss: 0.6188557147979736 \n",
      "     Training Step: 128 Training Loss: 0.616233766078949 \n",
      "     Training Step: 129 Training Loss: 0.6127021312713623 \n",
      "     Training Step: 130 Training Loss: 0.6110140085220337 \n",
      "     Training Step: 131 Training Loss: 0.6157470941543579 \n",
      "     Training Step: 132 Training Loss: 0.6176009178161621 \n",
      "     Training Step: 133 Training Loss: 0.6167188286781311 \n",
      "     Training Step: 134 Training Loss: 0.613369345664978 \n",
      "     Training Step: 135 Training Loss: 0.6154752969741821 \n",
      "     Training Step: 136 Training Loss: 0.6141684651374817 \n",
      "     Training Step: 137 Training Loss: 0.6178308129310608 \n",
      "     Training Step: 138 Training Loss: 0.6171749234199524 \n",
      "     Training Step: 139 Training Loss: 0.6141735911369324 \n",
      "     Training Step: 140 Training Loss: 0.6202511191368103 \n",
      "     Training Step: 141 Training Loss: 0.6166225075721741 \n",
      "     Training Step: 142 Training Loss: 0.6147259473800659 \n",
      "     Training Step: 143 Training Loss: 0.6166417598724365 \n",
      "     Training Step: 144 Training Loss: 0.6099039912223816 \n",
      "     Training Step: 145 Training Loss: 0.6169174909591675 \n",
      "     Training Step: 146 Training Loss: 0.6114310622215271 \n",
      "     Training Step: 147 Training Loss: 0.6132063865661621 \n",
      "     Training Step: 148 Training Loss: 0.6114328503608704 \n",
      "     Training Step: 149 Training Loss: 0.616519570350647 \n",
      "     Training Step: 150 Training Loss: 0.6183433532714844 \n",
      "     Training Step: 151 Training Loss: 0.6125635504722595 \n",
      "     Training Step: 152 Training Loss: 0.6107670068740845 \n",
      "     Training Step: 153 Training Loss: 0.6121562123298645 \n",
      "     Training Step: 154 Training Loss: 0.6180766224861145 \n",
      "     Training Step: 155 Training Loss: 0.6144744753837585 \n",
      "     Training Step: 156 Training Loss: 0.6181692481040955 \n",
      "     Training Step: 157 Training Loss: 0.6184094548225403 \n",
      "     Training Step: 158 Training Loss: 0.6168176531791687 \n",
      "     Training Step: 159 Training Loss: 0.6150001287460327 \n",
      "     Training Step: 160 Training Loss: 0.6096019148826599 \n",
      "     Training Step: 161 Training Loss: 0.6150987148284912 \n",
      "     Training Step: 162 Training Loss: 0.6124117970466614 \n",
      "     Training Step: 163 Training Loss: 0.61650151014328 \n",
      "     Training Step: 164 Training Loss: 0.6107872128486633 \n",
      "     Training Step: 165 Training Loss: 0.6161137223243713 \n",
      "     Training Step: 166 Training Loss: 0.6127954721450806 \n",
      "     Training Step: 167 Training Loss: 0.6148202419281006 \n",
      "     Training Step: 168 Training Loss: 0.6171485781669617 \n",
      "     Training Step: 169 Training Loss: 0.615527331829071 \n",
      "     Training Step: 170 Training Loss: 0.611863374710083 \n",
      "     Training Step: 171 Training Loss: 0.6167373657226562 \n",
      "     Training Step: 172 Training Loss: 0.6149095892906189 \n",
      "     Training Step: 173 Training Loss: 0.6154858469963074 \n",
      "     Training Step: 174 Training Loss: 0.6127986311912537 \n",
      "     Training Step: 175 Training Loss: 0.6118470430374146 \n",
      "     Training Step: 176 Training Loss: 0.6140371561050415 \n",
      "     Training Step: 177 Training Loss: 0.6128910183906555 \n",
      "     Training Step: 178 Training Loss: 0.6100226640701294 \n",
      "     Training Step: 179 Training Loss: 0.6127094030380249 \n",
      "     Training Step: 180 Training Loss: 0.6175543069839478 \n",
      "     Training Step: 181 Training Loss: 0.6157307028770447 \n",
      "     Training Step: 182 Training Loss: 0.6155626177787781 \n",
      "     Training Step: 183 Training Loss: 0.6106032133102417 \n",
      "     Training Step: 184 Training Loss: 0.6147048473358154 \n",
      "     Training Step: 185 Training Loss: 0.6115081906318665 \n",
      "     Training Step: 186 Training Loss: 0.6118889451026917 \n",
      "     Training Step: 187 Training Loss: 0.6104227304458618 \n",
      "     Training Step: 188 Training Loss: 0.6167491674423218 \n",
      "     Training Step: 189 Training Loss: 0.6177856922149658 \n",
      "     Training Step: 190 Training Loss: 0.613821268081665 \n",
      "     Training Step: 191 Training Loss: 0.6133360862731934 \n",
      "     Training Step: 192 Training Loss: 0.6114773154258728 \n",
      "     Training Step: 193 Training Loss: 0.6092972755432129 \n",
      "     Training Step: 194 Training Loss: 0.613935649394989 \n",
      "     Training Step: 195 Training Loss: 0.6179348826408386 \n",
      "     Training Step: 196 Training Loss: 0.614470899105072 \n",
      "     Training Step: 197 Training Loss: 0.6118502616882324 \n",
      "     Training Step: 198 Training Loss: 0.6137444972991943 \n",
      "     Training Step: 199 Training Loss: 0.6132170557975769 \n",
      "     Training Step: 200 Training Loss: 0.6136412024497986 \n",
      "     Training Step: 201 Training Loss: 0.6135225892066956 \n",
      "     Training Step: 202 Training Loss: 0.6106677651405334 \n",
      "     Training Step: 203 Training Loss: 0.614257276058197 \n",
      "     Training Step: 204 Training Loss: 0.6105695962905884 \n",
      "     Training Step: 205 Training Loss: 0.6105571389198303 \n",
      "     Training Step: 206 Training Loss: 0.6183571815490723 \n",
      "     Training Step: 207 Training Loss: 0.6157768368721008 \n",
      "     Training Step: 208 Training Loss: 0.6106307506561279 \n",
      "     Training Step: 209 Training Loss: 0.6168335676193237 \n",
      "     Training Step: 210 Training Loss: 0.6188888549804688 \n",
      "     Training Step: 211 Training Loss: 0.6143113970756531 \n",
      "     Training Step: 212 Training Loss: 0.6152039766311646 \n",
      "     Training Step: 213 Training Loss: 0.6171169281005859 \n",
      "     Training Step: 214 Training Loss: 0.6101649403572083 \n",
      "     Training Step: 215 Training Loss: 0.6105484366416931 \n",
      "     Training Step: 216 Training Loss: 0.6159074902534485 \n",
      "     Training Step: 217 Training Loss: 0.6150362491607666 \n",
      "     Training Step: 218 Training Loss: 0.6117170453071594 \n",
      "     Training Step: 219 Training Loss: 0.614678680896759 \n",
      "     Training Step: 220 Training Loss: 0.6168232560157776 \n",
      "     Training Step: 221 Training Loss: 0.6156222820281982 \n",
      "     Training Step: 222 Training Loss: 0.6167342066764832 \n",
      "     Training Step: 223 Training Loss: 0.6147330403327942 \n",
      "     Training Step: 224 Training Loss: 0.6125867962837219 \n",
      "     Training Step: 225 Training Loss: 0.6101064085960388 \n",
      "     Training Step: 226 Training Loss: 0.6116045117378235 \n",
      "     Training Step: 227 Training Loss: 0.6116857528686523 \n",
      "     Training Step: 228 Training Loss: 0.6123453974723816 \n",
      "     Training Step: 229 Training Loss: 0.6147555112838745 \n",
      "     Training Step: 230 Training Loss: 0.6198070049285889 \n",
      "     Training Step: 231 Training Loss: 0.6162815093994141 \n",
      "     Training Step: 232 Training Loss: 0.611707866191864 \n",
      "     Training Step: 233 Training Loss: 0.6168175935745239 \n",
      "     Training Step: 234 Training Loss: 0.6145458221435547 \n",
      "     Training Step: 235 Training Loss: 0.6095405220985413 \n",
      "     Training Step: 236 Training Loss: 0.614033579826355 \n",
      "     Training Step: 237 Training Loss: 0.6102273464202881 \n",
      "     Training Step: 238 Training Loss: 0.6166200637817383 \n",
      "     Training Step: 239 Training Loss: 0.6129157543182373 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6106346249580383 \n",
      "     Validation Step: 1 Validation Loss: 0.6136940717697144 \n",
      "     Validation Step: 2 Validation Loss: 0.6115624308586121 \n",
      "     Validation Step: 3 Validation Loss: 0.6177458167076111 \n",
      "     Validation Step: 4 Validation Loss: 0.6156185865402222 \n",
      "     Validation Step: 5 Validation Loss: 0.6142221689224243 \n",
      "     Validation Step: 6 Validation Loss: 0.6141209006309509 \n",
      "     Validation Step: 7 Validation Loss: 0.6149113774299622 \n",
      "     Validation Step: 8 Validation Loss: 0.6133083701133728 \n",
      "     Validation Step: 9 Validation Loss: 0.615249514579773 \n",
      "     Validation Step: 10 Validation Loss: 0.6184069514274597 \n",
      "     Validation Step: 11 Validation Loss: 0.6101585626602173 \n",
      "     Validation Step: 12 Validation Loss: 0.6148557066917419 \n",
      "     Validation Step: 13 Validation Loss: 0.6160641312599182 \n",
      "     Validation Step: 14 Validation Loss: 0.6183076500892639 \n",
      "     Validation Step: 15 Validation Loss: 0.6130011677742004 \n",
      "     Validation Step: 16 Validation Loss: 0.6105247735977173 \n",
      "     Validation Step: 17 Validation Loss: 0.6170650124549866 \n",
      "     Validation Step: 18 Validation Loss: 0.6145443916320801 \n",
      "     Validation Step: 19 Validation Loss: 0.6145899295806885 \n",
      "     Validation Step: 20 Validation Loss: 0.6173355579376221 \n",
      "     Validation Step: 21 Validation Loss: 0.6111706495285034 \n",
      "     Validation Step: 22 Validation Loss: 0.618516743183136 \n",
      "     Validation Step: 23 Validation Loss: 0.6101410388946533 \n",
      "     Validation Step: 24 Validation Loss: 0.617623507976532 \n",
      "     Validation Step: 25 Validation Loss: 0.6162926554679871 \n",
      "     Validation Step: 26 Validation Loss: 0.6142638921737671 \n",
      "     Validation Step: 27 Validation Loss: 0.6181147694587708 \n",
      "     Validation Step: 28 Validation Loss: 0.6121360063552856 \n",
      "     Validation Step: 29 Validation Loss: 0.6150857210159302 \n",
      "     Validation Step: 30 Validation Loss: 0.6158440709114075 \n",
      "     Validation Step: 31 Validation Loss: 0.6111851930618286 \n",
      "     Validation Step: 32 Validation Loss: 0.6104894876480103 \n",
      "     Validation Step: 33 Validation Loss: 0.6141499280929565 \n",
      "     Validation Step: 34 Validation Loss: 0.612827479839325 \n",
      "     Validation Step: 35 Validation Loss: 0.6075196266174316 \n",
      "     Validation Step: 36 Validation Loss: 0.6146642565727234 \n",
      "     Validation Step: 37 Validation Loss: 0.61370450258255 \n",
      "     Validation Step: 38 Validation Loss: 0.6156532764434814 \n",
      "     Validation Step: 39 Validation Loss: 0.6153586506843567 \n",
      "     Validation Step: 40 Validation Loss: 0.6118890047073364 \n",
      "     Validation Step: 41 Validation Loss: 0.6136317253112793 \n",
      "     Validation Step: 42 Validation Loss: 0.6185467839241028 \n",
      "     Validation Step: 43 Validation Loss: 0.6101187467575073 \n",
      "     Validation Step: 44 Validation Loss: 0.6116413474082947 \n",
      "Epoch: 63\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6155619621276855 \n",
      "     Training Step: 1 Training Loss: 0.6115337014198303 \n",
      "     Training Step: 2 Training Loss: 0.617786169052124 \n",
      "     Training Step: 3 Training Loss: 0.6134191155433655 \n",
      "     Training Step: 4 Training Loss: 0.6127862930297852 \n",
      "     Training Step: 5 Training Loss: 0.6145135164260864 \n",
      "     Training Step: 6 Training Loss: 0.6121550798416138 \n",
      "     Training Step: 7 Training Loss: 0.6176389455795288 \n",
      "     Training Step: 8 Training Loss: 0.6143373847007751 \n",
      "     Training Step: 9 Training Loss: 0.6114869713783264 \n",
      "     Training Step: 10 Training Loss: 0.6164289712905884 \n",
      "     Training Step: 11 Training Loss: 0.6158012747764587 \n",
      "     Training Step: 12 Training Loss: 0.615542471408844 \n",
      "     Training Step: 13 Training Loss: 0.6124497652053833 \n",
      "     Training Step: 14 Training Loss: 0.6121921539306641 \n",
      "     Training Step: 15 Training Loss: 0.6150955557823181 \n",
      "     Training Step: 16 Training Loss: 0.6118491291999817 \n",
      "     Training Step: 17 Training Loss: 0.6162940263748169 \n",
      "     Training Step: 18 Training Loss: 0.6100710034370422 \n",
      "     Training Step: 19 Training Loss: 0.61496502161026 \n",
      "     Training Step: 20 Training Loss: 0.6119030117988586 \n",
      "     Training Step: 21 Training Loss: 0.6115844249725342 \n",
      "     Training Step: 22 Training Loss: 0.6101576089859009 \n",
      "     Training Step: 23 Training Loss: 0.6105852127075195 \n",
      "     Training Step: 24 Training Loss: 0.6122408509254456 \n",
      "     Training Step: 25 Training Loss: 0.615463137626648 \n",
      "     Training Step: 26 Training Loss: 0.6157210469245911 \n",
      "     Training Step: 27 Training Loss: 0.6105681657791138 \n",
      "     Training Step: 28 Training Loss: 0.6153486967086792 \n",
      "     Training Step: 29 Training Loss: 0.6152045130729675 \n",
      "     Training Step: 30 Training Loss: 0.6102736592292786 \n",
      "     Training Step: 31 Training Loss: 0.6141627430915833 \n",
      "     Training Step: 32 Training Loss: 0.6122322678565979 \n",
      "     Training Step: 33 Training Loss: 0.61786949634552 \n",
      "     Training Step: 34 Training Loss: 0.6111457347869873 \n",
      "     Training Step: 35 Training Loss: 0.6116948127746582 \n",
      "     Training Step: 36 Training Loss: 0.6146661639213562 \n",
      "     Training Step: 37 Training Loss: 0.6123837232589722 \n",
      "     Training Step: 38 Training Loss: 0.6152899265289307 \n",
      "     Training Step: 39 Training Loss: 0.6107609272003174 \n",
      "     Training Step: 40 Training Loss: 0.6143560409545898 \n",
      "     Training Step: 41 Training Loss: 0.6138219237327576 \n",
      "     Training Step: 42 Training Loss: 0.6130778193473816 \n",
      "     Training Step: 43 Training Loss: 0.612647294998169 \n",
      "     Training Step: 44 Training Loss: 0.6130563020706177 \n",
      "     Training Step: 45 Training Loss: 0.6124945282936096 \n",
      "     Training Step: 46 Training Loss: 0.6169681549072266 \n",
      "     Training Step: 47 Training Loss: 0.6167374849319458 \n",
      "     Training Step: 48 Training Loss: 0.6151151061058044 \n",
      "     Training Step: 49 Training Loss: 0.61681067943573 \n",
      "     Training Step: 50 Training Loss: 0.6136335134506226 \n",
      "     Training Step: 51 Training Loss: 0.6141976714134216 \n",
      "     Training Step: 52 Training Loss: 0.6131965517997742 \n",
      "     Training Step: 53 Training Loss: 0.613315224647522 \n",
      "     Training Step: 54 Training Loss: 0.6142513155937195 \n",
      "     Training Step: 55 Training Loss: 0.6160213947296143 \n",
      "     Training Step: 56 Training Loss: 0.6118353009223938 \n",
      "     Training Step: 57 Training Loss: 0.6186699271202087 \n",
      "     Training Step: 58 Training Loss: 0.6154717803001404 \n",
      "     Training Step: 59 Training Loss: 0.611517071723938 \n",
      "     Training Step: 60 Training Loss: 0.6182268261909485 \n",
      "     Training Step: 61 Training Loss: 0.6116737723350525 \n",
      "     Training Step: 62 Training Loss: 0.6145002245903015 \n",
      "     Training Step: 63 Training Loss: 0.6169542670249939 \n",
      "     Training Step: 64 Training Loss: 0.6181625127792358 \n",
      "     Training Step: 65 Training Loss: 0.6167741417884827 \n",
      "     Training Step: 66 Training Loss: 0.6101781129837036 \n",
      "     Training Step: 67 Training Loss: 0.616363525390625 \n",
      "     Training Step: 68 Training Loss: 0.6180142164230347 \n",
      "     Training Step: 69 Training Loss: 0.6183320879936218 \n",
      "     Training Step: 70 Training Loss: 0.6098541617393494 \n",
      "     Training Step: 71 Training Loss: 0.6120914816856384 \n",
      "     Training Step: 72 Training Loss: 0.6133255958557129 \n",
      "     Training Step: 73 Training Loss: 0.6116660833358765 \n",
      "     Training Step: 74 Training Loss: 0.6116294264793396 \n",
      "     Training Step: 75 Training Loss: 0.6142640709877014 \n",
      "     Training Step: 76 Training Loss: 0.6082668304443359 \n",
      "     Training Step: 77 Training Loss: 0.6132107973098755 \n",
      "     Training Step: 78 Training Loss: 0.6151184439659119 \n",
      "     Training Step: 79 Training Loss: 0.6185119152069092 \n",
      "     Training Step: 80 Training Loss: 0.6107826828956604 \n",
      "     Training Step: 81 Training Loss: 0.6209754347801208 \n",
      "     Training Step: 82 Training Loss: 0.6137771606445312 \n",
      "     Training Step: 83 Training Loss: 0.6142585277557373 \n",
      "     Training Step: 84 Training Loss: 0.6109297275543213 \n",
      "     Training Step: 85 Training Loss: 0.6140917539596558 \n",
      "     Training Step: 86 Training Loss: 0.6147274971008301 \n",
      "     Training Step: 87 Training Loss: 0.6167717576026917 \n",
      "     Training Step: 88 Training Loss: 0.6112130880355835 \n",
      "     Training Step: 89 Training Loss: 0.6105469465255737 \n",
      "     Training Step: 90 Training Loss: 0.6157481670379639 \n",
      "     Training Step: 91 Training Loss: 0.6144436001777649 \n",
      "     Training Step: 92 Training Loss: 0.614383339881897 \n",
      "     Training Step: 93 Training Loss: 0.6103898286819458 \n",
      "     Training Step: 94 Training Loss: 0.6143826842308044 \n",
      "     Training Step: 95 Training Loss: 0.6125319004058838 \n",
      "     Training Step: 96 Training Loss: 0.616666853427887 \n",
      "     Training Step: 97 Training Loss: 0.6134765148162842 \n",
      "     Training Step: 98 Training Loss: 0.6140218377113342 \n",
      "     Training Step: 99 Training Loss: 0.6106182336807251 \n",
      "     Training Step: 100 Training Loss: 0.6170985102653503 \n",
      "     Training Step: 101 Training Loss: 0.6125352382659912 \n",
      "     Training Step: 102 Training Loss: 0.6136488318443298 \n",
      "     Training Step: 103 Training Loss: 0.6136115789413452 \n",
      "     Training Step: 104 Training Loss: 0.6136647462844849 \n",
      "     Training Step: 105 Training Loss: 0.616278886795044 \n",
      "     Training Step: 106 Training Loss: 0.6148889660835266 \n",
      "     Training Step: 107 Training Loss: 0.6188754439353943 \n",
      "     Training Step: 108 Training Loss: 0.609358012676239 \n",
      "     Training Step: 109 Training Loss: 0.6180239319801331 \n",
      "     Training Step: 110 Training Loss: 0.6162265539169312 \n",
      "     Training Step: 111 Training Loss: 0.6135385036468506 \n",
      "     Training Step: 112 Training Loss: 0.6107654571533203 \n",
      "     Training Step: 113 Training Loss: 0.6166521310806274 \n",
      "     Training Step: 114 Training Loss: 0.6128069162368774 \n",
      "     Training Step: 115 Training Loss: 0.6167058944702148 \n",
      "     Training Step: 116 Training Loss: 0.6129767894744873 \n",
      "     Training Step: 117 Training Loss: 0.6127110123634338 \n",
      "     Training Step: 118 Training Loss: 0.6094342470169067 \n",
      "     Training Step: 119 Training Loss: 0.6167749762535095 \n",
      "     Training Step: 120 Training Loss: 0.6122837066650391 \n",
      "     Training Step: 121 Training Loss: 0.6149579882621765 \n",
      "     Training Step: 122 Training Loss: 0.6147939562797546 \n",
      "     Training Step: 123 Training Loss: 0.6140333414077759 \n",
      "     Training Step: 124 Training Loss: 0.6135185956954956 \n",
      "     Training Step: 125 Training Loss: 0.6115419268608093 \n",
      "     Training Step: 126 Training Loss: 0.6105051636695862 \n",
      "     Training Step: 127 Training Loss: 0.6160480380058289 \n",
      "     Training Step: 128 Training Loss: 0.6135134100914001 \n",
      "     Training Step: 129 Training Loss: 0.6186974048614502 \n",
      "     Training Step: 130 Training Loss: 0.6196643114089966 \n",
      "     Training Step: 131 Training Loss: 0.6147692799568176 \n",
      "     Training Step: 132 Training Loss: 0.6183861494064331 \n",
      "     Training Step: 133 Training Loss: 0.6138778328895569 \n",
      "     Training Step: 134 Training Loss: 0.6129652857780457 \n",
      "     Training Step: 135 Training Loss: 0.613350510597229 \n",
      "     Training Step: 136 Training Loss: 0.6097809672355652 \n",
      "     Training Step: 137 Training Loss: 0.6179261207580566 \n",
      "     Training Step: 138 Training Loss: 0.6145369410514832 \n",
      "     Training Step: 139 Training Loss: 0.6152876615524292 \n",
      "     Training Step: 140 Training Loss: 0.618264377117157 \n",
      "     Training Step: 141 Training Loss: 0.6116987466812134 \n",
      "     Training Step: 142 Training Loss: 0.6135925650596619 \n",
      "     Training Step: 143 Training Loss: 0.6101680994033813 \n",
      "     Training Step: 144 Training Loss: 0.6157196164131165 \n",
      "     Training Step: 145 Training Loss: 0.6115497946739197 \n",
      "     Training Step: 146 Training Loss: 0.6161943674087524 \n",
      "     Training Step: 147 Training Loss: 0.6158244609832764 \n",
      "     Training Step: 148 Training Loss: 0.6128769516944885 \n",
      "     Training Step: 149 Training Loss: 0.6165047287940979 \n",
      "     Training Step: 150 Training Loss: 0.6122407913208008 \n",
      "     Training Step: 151 Training Loss: 0.612349271774292 \n",
      "     Training Step: 152 Training Loss: 0.6157596707344055 \n",
      "     Training Step: 153 Training Loss: 0.6176667213439941 \n",
      "     Training Step: 154 Training Loss: 0.6132990121841431 \n",
      "     Training Step: 155 Training Loss: 0.6157119870185852 \n",
      "     Training Step: 156 Training Loss: 0.6123140454292297 \n",
      "     Training Step: 157 Training Loss: 0.6133629083633423 \n",
      "     Training Step: 158 Training Loss: 0.6194636821746826 \n",
      "     Training Step: 159 Training Loss: 0.6114534735679626 \n",
      "     Training Step: 160 Training Loss: 0.6201930642127991 \n",
      "     Training Step: 161 Training Loss: 0.6171344518661499 \n",
      "     Training Step: 162 Training Loss: 0.6155146956443787 \n",
      "     Training Step: 163 Training Loss: 0.6173787117004395 \n",
      "     Training Step: 164 Training Loss: 0.6147035360336304 \n",
      "     Training Step: 165 Training Loss: 0.6177926659584045 \n",
      "     Training Step: 166 Training Loss: 0.6118466854095459 \n",
      "     Training Step: 167 Training Loss: 0.6147012114524841 \n",
      "     Training Step: 168 Training Loss: 0.6167088150978088 \n",
      "     Training Step: 169 Training Loss: 0.6143457889556885 \n",
      "     Training Step: 170 Training Loss: 0.6171945929527283 \n",
      "     Training Step: 171 Training Loss: 0.611920177936554 \n",
      "     Training Step: 172 Training Loss: 0.6107326149940491 \n",
      "     Training Step: 173 Training Loss: 0.6161438226699829 \n",
      "     Training Step: 174 Training Loss: 0.6128464341163635 \n",
      "     Training Step: 175 Training Loss: 0.6168458461761475 \n",
      "     Training Step: 176 Training Loss: 0.6139187216758728 \n",
      "     Training Step: 177 Training Loss: 0.6167030334472656 \n",
      "     Training Step: 178 Training Loss: 0.6177003383636475 \n",
      "     Training Step: 179 Training Loss: 0.6197288036346436 \n",
      "     Training Step: 180 Training Loss: 0.6105385422706604 \n",
      "     Training Step: 181 Training Loss: 0.6176964044570923 \n",
      "     Training Step: 182 Training Loss: 0.6118983030319214 \n",
      "     Training Step: 183 Training Loss: 0.6154347658157349 \n",
      "     Training Step: 184 Training Loss: 0.6154184937477112 \n",
      "     Training Step: 185 Training Loss: 0.6139110922813416 \n",
      "     Training Step: 186 Training Loss: 0.6115382313728333 \n",
      "     Training Step: 187 Training Loss: 0.6129295825958252 \n",
      "     Training Step: 188 Training Loss: 0.6147254109382629 \n",
      "     Training Step: 189 Training Loss: 0.6139253973960876 \n",
      "     Training Step: 190 Training Loss: 0.612502932548523 \n",
      "     Training Step: 191 Training Loss: 0.6147140264511108 \n",
      "     Training Step: 192 Training Loss: 0.6149313449859619 \n",
      "     Training Step: 193 Training Loss: 0.6127700209617615 \n",
      "     Training Step: 194 Training Loss: 0.6153051853179932 \n",
      "     Training Step: 195 Training Loss: 0.6116182208061218 \n",
      "     Training Step: 196 Training Loss: 0.6125253438949585 \n",
      "     Training Step: 197 Training Loss: 0.6171858310699463 \n",
      "     Training Step: 198 Training Loss: 0.6097148060798645 \n",
      "     Training Step: 199 Training Loss: 0.6154581308364868 \n",
      "     Training Step: 200 Training Loss: 0.6146255135536194 \n",
      "     Training Step: 201 Training Loss: 0.6155478358268738 \n",
      "     Training Step: 202 Training Loss: 0.6100267171859741 \n",
      "     Training Step: 203 Training Loss: 0.6121291518211365 \n",
      "     Training Step: 204 Training Loss: 0.612392008304596 \n",
      "     Training Step: 205 Training Loss: 0.6111676692962646 \n",
      "     Training Step: 206 Training Loss: 0.6172327995300293 \n",
      "     Training Step: 207 Training Loss: 0.6152928471565247 \n",
      "     Training Step: 208 Training Loss: 0.6151683926582336 \n",
      "     Training Step: 209 Training Loss: 0.6158487796783447 \n",
      "     Training Step: 210 Training Loss: 0.6118518114089966 \n",
      "     Training Step: 211 Training Loss: 0.614436686038971 \n",
      "     Training Step: 212 Training Loss: 0.6131367087364197 \n",
      "     Training Step: 213 Training Loss: 0.6114844083786011 \n",
      "     Training Step: 214 Training Loss: 0.6094456315040588 \n",
      "     Training Step: 215 Training Loss: 0.6147100925445557 \n",
      "     Training Step: 216 Training Loss: 0.6168311834335327 \n",
      "     Training Step: 217 Training Loss: 0.6108928918838501 \n",
      "     Training Step: 218 Training Loss: 0.6184751987457275 \n",
      "     Training Step: 219 Training Loss: 0.6199958920478821 \n",
      "     Training Step: 220 Training Loss: 0.6147021651268005 \n",
      "     Training Step: 221 Training Loss: 0.6116197109222412 \n",
      "     Training Step: 222 Training Loss: 0.6147696375846863 \n",
      "     Training Step: 223 Training Loss: 0.6146814227104187 \n",
      "     Training Step: 224 Training Loss: 0.6169310808181763 \n",
      "     Training Step: 225 Training Loss: 0.6132524609565735 \n",
      "     Training Step: 226 Training Loss: 0.6129353046417236 \n",
      "     Training Step: 227 Training Loss: 0.6120467185974121 \n",
      "     Training Step: 228 Training Loss: 0.6180638074874878 \n",
      "     Training Step: 229 Training Loss: 0.6141696572303772 \n",
      "     Training Step: 230 Training Loss: 0.6138477921485901 \n",
      "     Training Step: 231 Training Loss: 0.6154090762138367 \n",
      "     Training Step: 232 Training Loss: 0.6164262890815735 \n",
      "     Training Step: 233 Training Loss: 0.6188961267471313 \n",
      "     Training Step: 234 Training Loss: 0.6148355603218079 \n",
      "     Training Step: 235 Training Loss: 0.6146887540817261 \n",
      "     Training Step: 236 Training Loss: 0.6154724359512329 \n",
      "     Training Step: 237 Training Loss: 0.6132453083992004 \n",
      "     Training Step: 238 Training Loss: 0.614074170589447 \n",
      "     Training Step: 239 Training Loss: 0.6106154918670654 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.614858865737915 \n",
      "     Validation Step: 1 Validation Loss: 0.6177461743354797 \n",
      "     Validation Step: 2 Validation Loss: 0.6104974746704102 \n",
      "     Validation Step: 3 Validation Loss: 0.6141486763954163 \n",
      "     Validation Step: 4 Validation Loss: 0.6137046813964844 \n",
      "     Validation Step: 5 Validation Loss: 0.6173350214958191 \n",
      "     Validation Step: 6 Validation Loss: 0.614590585231781 \n",
      "     Validation Step: 7 Validation Loss: 0.6111876964569092 \n",
      "     Validation Step: 8 Validation Loss: 0.6153470277786255 \n",
      "     Validation Step: 9 Validation Loss: 0.6160653233528137 \n",
      "     Validation Step: 10 Validation Loss: 0.6136941313743591 \n",
      "     Validation Step: 11 Validation Loss: 0.6156240701675415 \n",
      "     Validation Step: 12 Validation Loss: 0.6128289699554443 \n",
      "     Validation Step: 13 Validation Loss: 0.6185100674629211 \n",
      "     Validation Step: 14 Validation Loss: 0.6185427308082581 \n",
      "     Validation Step: 15 Validation Loss: 0.6182995438575745 \n",
      "     Validation Step: 16 Validation Loss: 0.6111768484115601 \n",
      "     Validation Step: 17 Validation Loss: 0.6133087277412415 \n",
      "     Validation Step: 18 Validation Loss: 0.6121339797973633 \n",
      "     Validation Step: 19 Validation Loss: 0.6152535676956177 \n",
      "     Validation Step: 20 Validation Loss: 0.6150856018066406 \n",
      "     Validation Step: 21 Validation Loss: 0.6130039095878601 \n",
      "     Validation Step: 22 Validation Loss: 0.6158449053764343 \n",
      "     Validation Step: 23 Validation Loss: 0.6116452813148499 \n",
      "     Validation Step: 24 Validation Loss: 0.6075212359428406 \n",
      "     Validation Step: 25 Validation Loss: 0.6101418733596802 \n",
      "     Validation Step: 26 Validation Loss: 0.6118906140327454 \n",
      "     Validation Step: 27 Validation Loss: 0.6106384992599487 \n",
      "     Validation Step: 28 Validation Loss: 0.6183983087539673 \n",
      "     Validation Step: 29 Validation Loss: 0.6156542897224426 \n",
      "     Validation Step: 30 Validation Loss: 0.6176183223724365 \n",
      "     Validation Step: 31 Validation Loss: 0.6142610907554626 \n",
      "     Validation Step: 32 Validation Loss: 0.6162887215614319 \n",
      "     Validation Step: 33 Validation Loss: 0.6136358380317688 \n",
      "     Validation Step: 34 Validation Loss: 0.6105298399925232 \n",
      "     Validation Step: 35 Validation Loss: 0.6142187118530273 \n",
      "     Validation Step: 36 Validation Loss: 0.6115657687187195 \n",
      "     Validation Step: 37 Validation Loss: 0.6101186871528625 \n",
      "     Validation Step: 38 Validation Loss: 0.610162615776062 \n",
      "     Validation Step: 39 Validation Loss: 0.6145448088645935 \n",
      "     Validation Step: 40 Validation Loss: 0.6149036288261414 \n",
      "     Validation Step: 41 Validation Loss: 0.6141194701194763 \n",
      "     Validation Step: 42 Validation Loss: 0.6170592308044434 \n",
      "     Validation Step: 43 Validation Loss: 0.6181091070175171 \n",
      "     Validation Step: 44 Validation Loss: 0.6146629452705383 \n",
      "Epoch: 64\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6131300926208496 \n",
      "     Training Step: 1 Training Loss: 0.6138498187065125 \n",
      "     Training Step: 2 Training Loss: 0.6120296120643616 \n",
      "     Training Step: 3 Training Loss: 0.6158214807510376 \n",
      "     Training Step: 4 Training Loss: 0.6153643727302551 \n",
      "     Training Step: 5 Training Loss: 0.6177300214767456 \n",
      "     Training Step: 6 Training Loss: 0.6133631467819214 \n",
      "     Training Step: 7 Training Loss: 0.6128095388412476 \n",
      "     Training Step: 8 Training Loss: 0.6172160506248474 \n",
      "     Training Step: 9 Training Loss: 0.6166749000549316 \n",
      "     Training Step: 10 Training Loss: 0.6140822172164917 \n",
      "     Training Step: 11 Training Loss: 0.6116816401481628 \n",
      "     Training Step: 12 Training Loss: 0.6123747825622559 \n",
      "     Training Step: 13 Training Loss: 0.6154148578643799 \n",
      "     Training Step: 14 Training Loss: 0.6146653294563293 \n",
      "     Training Step: 15 Training Loss: 0.6132900714874268 \n",
      "     Training Step: 16 Training Loss: 0.6106184720993042 \n",
      "     Training Step: 17 Training Loss: 0.6196865439414978 \n",
      "     Training Step: 18 Training Loss: 0.6180469989776611 \n",
      "     Training Step: 19 Training Loss: 0.6140610575675964 \n",
      "     Training Step: 20 Training Loss: 0.6139122843742371 \n",
      "     Training Step: 21 Training Loss: 0.6184308528900146 \n",
      "     Training Step: 22 Training Loss: 0.6135033965110779 \n",
      "     Training Step: 23 Training Loss: 0.6117292642593384 \n",
      "     Training Step: 24 Training Loss: 0.6154635548591614 \n",
      "     Training Step: 25 Training Loss: 0.6129759550094604 \n",
      "     Training Step: 26 Training Loss: 0.6141259074211121 \n",
      "     Training Step: 27 Training Loss: 0.6114081144332886 \n",
      "     Training Step: 28 Training Loss: 0.6167012453079224 \n",
      "     Training Step: 29 Training Loss: 0.6135234236717224 \n",
      "     Training Step: 30 Training Loss: 0.610000491142273 \n",
      "     Training Step: 31 Training Loss: 0.6151070594787598 \n",
      "     Training Step: 32 Training Loss: 0.6107175946235657 \n",
      "     Training Step: 33 Training Loss: 0.6185429096221924 \n",
      "     Training Step: 34 Training Loss: 0.6122410297393799 \n",
      "     Training Step: 35 Training Loss: 0.6210038065910339 \n",
      "     Training Step: 36 Training Loss: 0.6129052042961121 \n",
      "     Training Step: 37 Training Loss: 0.6164889931678772 \n",
      "     Training Step: 38 Training Loss: 0.615811824798584 \n",
      "     Training Step: 39 Training Loss: 0.6168112754821777 \n",
      "     Training Step: 40 Training Loss: 0.6149813532829285 \n",
      "     Training Step: 41 Training Loss: 0.6102115511894226 \n",
      "     Training Step: 42 Training Loss: 0.6130895614624023 \n",
      "     Training Step: 43 Training Loss: 0.6121941208839417 \n",
      "     Training Step: 44 Training Loss: 0.6124008297920227 \n",
      "     Training Step: 45 Training Loss: 0.6094390153884888 \n",
      "     Training Step: 46 Training Loss: 0.6104759573936462 \n",
      "     Training Step: 47 Training Loss: 0.6142809391021729 \n",
      "     Training Step: 48 Training Loss: 0.6189945936203003 \n",
      "     Training Step: 49 Training Loss: 0.6144660115242004 \n",
      "     Training Step: 50 Training Loss: 0.6158950924873352 \n",
      "     Training Step: 51 Training Loss: 0.6126021146774292 \n",
      "     Training Step: 52 Training Loss: 0.6167358756065369 \n",
      "     Training Step: 53 Training Loss: 0.613970935344696 \n",
      "     Training Step: 54 Training Loss: 0.6121405959129333 \n",
      "     Training Step: 55 Training Loss: 0.6122438311576843 \n",
      "     Training Step: 56 Training Loss: 0.6092312932014465 \n",
      "     Training Step: 57 Training Loss: 0.6153386831283569 \n",
      "     Training Step: 58 Training Loss: 0.6164701581001282 \n",
      "     Training Step: 59 Training Loss: 0.6148056983947754 \n",
      "     Training Step: 60 Training Loss: 0.6145122051239014 \n",
      "     Training Step: 61 Training Loss: 0.6157205104827881 \n",
      "     Training Step: 62 Training Loss: 0.6106976270675659 \n",
      "     Training Step: 63 Training Loss: 0.6153770685195923 \n",
      "     Training Step: 64 Training Loss: 0.6097487211227417 \n",
      "     Training Step: 65 Training Loss: 0.6111443638801575 \n",
      "     Training Step: 66 Training Loss: 0.6144682765007019 \n",
      "     Training Step: 67 Training Loss: 0.6105989813804626 \n",
      "     Training Step: 68 Training Loss: 0.6118546724319458 \n",
      "     Training Step: 69 Training Loss: 0.6146525144577026 \n",
      "     Training Step: 70 Training Loss: 0.6123875379562378 \n",
      "     Training Step: 71 Training Loss: 0.6171749234199524 \n",
      "     Training Step: 72 Training Loss: 0.6167146563529968 \n",
      "     Training Step: 73 Training Loss: 0.6123811602592468 \n",
      "     Training Step: 74 Training Loss: 0.6146117448806763 \n",
      "     Training Step: 75 Training Loss: 0.6147295832633972 \n",
      "     Training Step: 76 Training Loss: 0.6146970987319946 \n",
      "     Training Step: 77 Training Loss: 0.6136731505393982 \n",
      "     Training Step: 78 Training Loss: 0.6132218241691589 \n",
      "     Training Step: 79 Training Loss: 0.6148856282234192 \n",
      "     Training Step: 80 Training Loss: 0.6152504086494446 \n",
      "     Training Step: 81 Training Loss: 0.6128101348876953 \n",
      "     Training Step: 82 Training Loss: 0.6166635751724243 \n",
      "     Training Step: 83 Training Loss: 0.6163460612297058 \n",
      "     Training Step: 84 Training Loss: 0.6146795153617859 \n",
      "     Training Step: 85 Training Loss: 0.6107261776924133 \n",
      "     Training Step: 86 Training Loss: 0.6097568273544312 \n",
      "     Training Step: 87 Training Loss: 0.6154854893684387 \n",
      "     Training Step: 88 Training Loss: 0.6168869137763977 \n",
      "     Training Step: 89 Training Loss: 0.6097313165664673 \n",
      "     Training Step: 90 Training Loss: 0.6118254065513611 \n",
      "     Training Step: 91 Training Loss: 0.6168075799942017 \n",
      "     Training Step: 92 Training Loss: 0.6147034168243408 \n",
      "     Training Step: 93 Training Loss: 0.615321934223175 \n",
      "     Training Step: 94 Training Loss: 0.6161365509033203 \n",
      "     Training Step: 95 Training Loss: 0.6101543307304382 \n",
      "     Training Step: 96 Training Loss: 0.6178238987922668 \n",
      "     Training Step: 97 Training Loss: 0.6123811602592468 \n",
      "     Training Step: 98 Training Loss: 0.611642599105835 \n",
      "     Training Step: 99 Training Loss: 0.6112033724784851 \n",
      "     Training Step: 100 Training Loss: 0.6101695895195007 \n",
      "     Training Step: 101 Training Loss: 0.6133351922035217 \n",
      "     Training Step: 102 Training Loss: 0.6168333292007446 \n",
      "     Training Step: 103 Training Loss: 0.6111704111099243 \n",
      "     Training Step: 104 Training Loss: 0.613757848739624 \n",
      "     Training Step: 105 Training Loss: 0.6136514544487 \n",
      "     Training Step: 106 Training Loss: 0.617393434047699 \n",
      "     Training Step: 107 Training Loss: 0.6181280016899109 \n",
      "     Training Step: 108 Training Loss: 0.6135344505310059 \n",
      "     Training Step: 109 Training Loss: 0.6141901016235352 \n",
      "     Training Step: 110 Training Loss: 0.6108002066612244 \n",
      "     Training Step: 111 Training Loss: 0.6202317476272583 \n",
      "     Training Step: 112 Training Loss: 0.6169084310531616 \n",
      "     Training Step: 113 Training Loss: 0.6121031045913696 \n",
      "     Training Step: 114 Training Loss: 0.6184118986129761 \n",
      "     Training Step: 115 Training Loss: 0.6146519184112549 \n",
      "     Training Step: 116 Training Loss: 0.6141698956489563 \n",
      "     Training Step: 117 Training Loss: 0.6083752512931824 \n",
      "     Training Step: 118 Training Loss: 0.6118079423904419 \n",
      "     Training Step: 119 Training Loss: 0.6183404922485352 \n",
      "     Training Step: 120 Training Loss: 0.6167866587638855 \n",
      "     Training Step: 121 Training Loss: 0.611428439617157 \n",
      "     Training Step: 122 Training Loss: 0.61320561170578 \n",
      "     Training Step: 123 Training Loss: 0.6154669523239136 \n",
      "     Training Step: 124 Training Loss: 0.6122753620147705 \n",
      "     Training Step: 125 Training Loss: 0.6146350502967834 \n",
      "     Training Step: 126 Training Loss: 0.6136022806167603 \n",
      "     Training Step: 127 Training Loss: 0.610707700252533 \n",
      "     Training Step: 128 Training Loss: 0.6154857873916626 \n",
      "     Training Step: 129 Training Loss: 0.6155542135238647 \n",
      "     Training Step: 130 Training Loss: 0.6180956959724426 \n",
      "     Training Step: 131 Training Loss: 0.6182220578193665 \n",
      "     Training Step: 132 Training Loss: 0.6178078055381775 \n",
      "     Training Step: 133 Training Loss: 0.6158503890037537 \n",
      "     Training Step: 134 Training Loss: 0.615609347820282 \n",
      "     Training Step: 135 Training Loss: 0.6144959330558777 \n",
      "     Training Step: 136 Training Loss: 0.6153050661087036 \n",
      "     Training Step: 137 Training Loss: 0.6133968830108643 \n",
      "     Training Step: 138 Training Loss: 0.6116641759872437 \n",
      "     Training Step: 139 Training Loss: 0.6145326495170593 \n",
      "     Training Step: 140 Training Loss: 0.6103971004486084 \n",
      "     Training Step: 141 Training Loss: 0.6183494329452515 \n",
      "     Training Step: 142 Training Loss: 0.6118390560150146 \n",
      "     Training Step: 143 Training Loss: 0.6129109859466553 \n",
      "     Training Step: 144 Training Loss: 0.6129072308540344 \n",
      "     Training Step: 145 Training Loss: 0.6115484237670898 \n",
      "     Training Step: 146 Training Loss: 0.6126465201377869 \n",
      "     Training Step: 147 Training Loss: 0.6155718564987183 \n",
      "     Training Step: 148 Training Loss: 0.6115683317184448 \n",
      "     Training Step: 149 Training Loss: 0.6140671372413635 \n",
      "     Training Step: 150 Training Loss: 0.6147940754890442 \n",
      "     Training Step: 151 Training Loss: 0.6124982833862305 \n",
      "     Training Step: 152 Training Loss: 0.6168296933174133 \n",
      "     Training Step: 153 Training Loss: 0.6109642386436462 \n",
      "     Training Step: 154 Training Loss: 0.6146910190582275 \n",
      "     Training Step: 155 Training Loss: 0.6127193570137024 \n",
      "     Training Step: 156 Training Loss: 0.6186407804489136 \n",
      "     Training Step: 157 Training Loss: 0.6195105314254761 \n",
      "     Training Step: 158 Training Loss: 0.6118361353874207 \n",
      "     Training Step: 159 Training Loss: 0.6159995794296265 \n",
      "     Training Step: 160 Training Loss: 0.6125997304916382 \n",
      "     Training Step: 161 Training Loss: 0.615601658821106 \n",
      "     Training Step: 162 Training Loss: 0.6176985502243042 \n",
      "     Training Step: 163 Training Loss: 0.6143447160720825 \n",
      "     Training Step: 164 Training Loss: 0.6140981912612915 \n",
      "     Training Step: 165 Training Loss: 0.6106365919113159 \n",
      "     Training Step: 166 Training Loss: 0.6147239208221436 \n",
      "     Training Step: 167 Training Loss: 0.6100611090660095 \n",
      "     Training Step: 168 Training Loss: 0.6158013939857483 \n",
      "     Training Step: 169 Training Loss: 0.6152182817459106 \n",
      "     Training Step: 170 Training Loss: 0.617784857749939 \n",
      "     Training Step: 171 Training Loss: 0.6116506457328796 \n",
      "     Training Step: 172 Training Loss: 0.6136548519134521 \n",
      "     Training Step: 173 Training Loss: 0.6109158992767334 \n",
      "     Training Step: 174 Training Loss: 0.6134793162345886 \n",
      "     Training Step: 175 Training Loss: 0.6125054359436035 \n",
      "     Training Step: 176 Training Loss: 0.6168506145477295 \n",
      "     Training Step: 177 Training Loss: 0.6121808886528015 \n",
      "     Training Step: 178 Training Loss: 0.6121421456336975 \n",
      "     Training Step: 179 Training Loss: 0.6100569367408752 \n",
      "     Training Step: 180 Training Loss: 0.6171227097511292 \n",
      "     Training Step: 181 Training Loss: 0.6125525832176208 \n",
      "     Training Step: 182 Training Loss: 0.6166648268699646 \n",
      "     Training Step: 183 Training Loss: 0.6160420179367065 \n",
      "     Training Step: 184 Training Loss: 0.6128587126731873 \n",
      "     Training Step: 185 Training Loss: 0.6144923567771912 \n",
      "     Training Step: 186 Training Loss: 0.6131508350372314 \n",
      "     Training Step: 187 Training Loss: 0.6104137301445007 \n",
      "     Training Step: 188 Training Loss: 0.6147315502166748 \n",
      "     Training Step: 189 Training Loss: 0.6162827610969543 \n",
      "     Training Step: 190 Training Loss: 0.6114497184753418 \n",
      "     Training Step: 191 Training Loss: 0.6137521266937256 \n",
      "     Training Step: 192 Training Loss: 0.6174903512001038 \n",
      "     Training Step: 193 Training Loss: 0.6129546761512756 \n",
      "     Training Step: 194 Training Loss: 0.6133330464363098 \n",
      "     Training Step: 195 Training Loss: 0.6118553876876831 \n",
      "     Training Step: 196 Training Loss: 0.6150773763656616 \n",
      "     Training Step: 197 Training Loss: 0.6134918928146362 \n",
      "     Training Step: 198 Training Loss: 0.6181170344352722 \n",
      "     Training Step: 199 Training Loss: 0.6129139065742493 \n",
      "     Training Step: 200 Training Loss: 0.6133055686950684 \n",
      "     Training Step: 201 Training Loss: 0.6114630699157715 \n",
      "     Training Step: 202 Training Loss: 0.6149281859397888 \n",
      "     Training Step: 203 Training Loss: 0.6169255375862122 \n",
      "     Training Step: 204 Training Loss: 0.6115574240684509 \n",
      "     Training Step: 205 Training Loss: 0.6159541010856628 \n",
      "     Training Step: 206 Training Loss: 0.6130640506744385 \n",
      "     Training Step: 207 Training Loss: 0.6171925663948059 \n",
      "     Training Step: 208 Training Loss: 0.615537166595459 \n",
      "     Training Step: 209 Training Loss: 0.6177502870559692 \n",
      "     Training Step: 210 Training Loss: 0.6115657091140747 \n",
      "     Training Step: 211 Training Loss: 0.6105942130088806 \n",
      "     Training Step: 212 Training Loss: 0.6152995228767395 \n",
      "     Training Step: 213 Training Loss: 0.6143924593925476 \n",
      "     Training Step: 214 Training Loss: 0.6199977397918701 \n",
      "     Training Step: 215 Training Loss: 0.6186327338218689 \n",
      "     Training Step: 216 Training Loss: 0.6119517683982849 \n",
      "     Training Step: 217 Training Loss: 0.6156699657440186 \n",
      "     Training Step: 218 Training Loss: 0.6142482161521912 \n",
      "     Training Step: 219 Training Loss: 0.6149828433990479 \n",
      "     Training Step: 220 Training Loss: 0.6142573952674866 \n",
      "     Training Step: 221 Training Loss: 0.6197315454483032 \n",
      "     Training Step: 222 Training Loss: 0.6094362735748291 \n",
      "     Training Step: 223 Training Loss: 0.616249144077301 \n",
      "     Training Step: 224 Training Loss: 0.6151334643363953 \n",
      "     Training Step: 225 Training Loss: 0.6171622276306152 \n",
      "     Training Step: 226 Training Loss: 0.6151575446128845 \n",
      "     Training Step: 227 Training Loss: 0.6147962212562561 \n",
      "     Training Step: 228 Training Loss: 0.6143737435340881 \n",
      "     Training Step: 229 Training Loss: 0.6137920022010803 \n",
      "     Training Step: 230 Training Loss: 0.6132257580757141 \n",
      "     Training Step: 231 Training Loss: 0.6116490364074707 \n",
      "     Training Step: 232 Training Loss: 0.6139592528343201 \n",
      "     Training Step: 233 Training Loss: 0.6190000772476196 \n",
      "     Training Step: 234 Training Loss: 0.6114072799682617 \n",
      "     Training Step: 235 Training Loss: 0.6176636815071106 \n",
      "     Training Step: 236 Training Loss: 0.6164684891700745 \n",
      "     Training Step: 237 Training Loss: 0.6143544912338257 \n",
      "     Training Step: 238 Training Loss: 0.611659586429596 \n",
      "     Training Step: 239 Training Loss: 0.6162440180778503 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6105083227157593 \n",
      "     Validation Step: 1 Validation Loss: 0.6173378825187683 \n",
      "     Validation Step: 2 Validation Loss: 0.6150367259979248 \n",
      "     Validation Step: 3 Validation Loss: 0.6121683120727539 \n",
      "     Validation Step: 4 Validation Loss: 0.6176307201385498 \n",
      "     Validation Step: 5 Validation Loss: 0.6162547469139099 \n",
      "     Validation Step: 6 Validation Loss: 0.61429363489151 \n",
      "     Validation Step: 7 Validation Loss: 0.6142238974571228 \n",
      "     Validation Step: 8 Validation Loss: 0.6148611307144165 \n",
      "     Validation Step: 9 Validation Loss: 0.6182403564453125 \n",
      "     Validation Step: 10 Validation Loss: 0.6106887459754944 \n",
      "     Validation Step: 11 Validation Loss: 0.6111869812011719 \n",
      "     Validation Step: 12 Validation Loss: 0.6075867414474487 \n",
      "     Validation Step: 13 Validation Loss: 0.6141563653945923 \n",
      "     Validation Step: 14 Validation Loss: 0.6159963011741638 \n",
      "     Validation Step: 15 Validation Loss: 0.6133356690406799 \n",
      "     Validation Step: 16 Validation Loss: 0.6170372366905212 \n",
      "     Validation Step: 17 Validation Loss: 0.6136484146118164 \n",
      "     Validation Step: 18 Validation Loss: 0.6130006313323975 \n",
      "     Validation Step: 19 Validation Loss: 0.6115853190422058 \n",
      "     Validation Step: 20 Validation Loss: 0.6177064180374146 \n",
      "     Validation Step: 21 Validation Loss: 0.6105452179908752 \n",
      "     Validation Step: 22 Validation Loss: 0.6183469891548157 \n",
      "     Validation Step: 23 Validation Loss: 0.6119239330291748 \n",
      "     Validation Step: 24 Validation Loss: 0.6146513819694519 \n",
      "     Validation Step: 25 Validation Loss: 0.6146038174629211 \n",
      "     Validation Step: 26 Validation Loss: 0.6152486801147461 \n",
      "     Validation Step: 27 Validation Loss: 0.6112073063850403 \n",
      "     Validation Step: 28 Validation Loss: 0.6128705143928528 \n",
      "     Validation Step: 29 Validation Loss: 0.610227108001709 \n",
      "     Validation Step: 30 Validation Loss: 0.6116846799850464 \n",
      "     Validation Step: 31 Validation Loss: 0.6101866364479065 \n",
      "     Validation Step: 32 Validation Loss: 0.6149320006370544 \n",
      "     Validation Step: 33 Validation Loss: 0.6185044646263123 \n",
      "     Validation Step: 34 Validation Loss: 0.6158077716827393 \n",
      "     Validation Step: 35 Validation Loss: 0.6101459264755249 \n",
      "     Validation Step: 36 Validation Loss: 0.6136724352836609 \n",
      "     Validation Step: 37 Validation Loss: 0.6141384243965149 \n",
      "     Validation Step: 38 Validation Loss: 0.6185137629508972 \n",
      "     Validation Step: 39 Validation Loss: 0.6136850714683533 \n",
      "     Validation Step: 40 Validation Loss: 0.615626335144043 \n",
      "     Validation Step: 41 Validation Loss: 0.6145656704902649 \n",
      "     Validation Step: 42 Validation Loss: 0.6180810332298279 \n",
      "     Validation Step: 43 Validation Loss: 0.6153273582458496 \n",
      "     Validation Step: 44 Validation Loss: 0.6155780553817749 \n",
      "Epoch: 65\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6143633723258972 \n",
      "     Training Step: 1 Training Loss: 0.6195425987243652 \n",
      "     Training Step: 2 Training Loss: 0.6168562173843384 \n",
      "     Training Step: 3 Training Loss: 0.6122498512268066 \n",
      "     Training Step: 4 Training Loss: 0.6184254288673401 \n",
      "     Training Step: 5 Training Loss: 0.6139926314353943 \n",
      "     Training Step: 6 Training Loss: 0.6153379678726196 \n",
      "     Training Step: 7 Training Loss: 0.6119049787521362 \n",
      "     Training Step: 8 Training Loss: 0.6117132306098938 \n",
      "     Training Step: 9 Training Loss: 0.6147353649139404 \n",
      "     Training Step: 10 Training Loss: 0.6148722767829895 \n",
      "     Training Step: 11 Training Loss: 0.6145486831665039 \n",
      "     Training Step: 12 Training Loss: 0.6136146783828735 \n",
      "     Training Step: 13 Training Loss: 0.616014838218689 \n",
      "     Training Step: 14 Training Loss: 0.6147217154502869 \n",
      "     Training Step: 15 Training Loss: 0.6132442951202393 \n",
      "     Training Step: 16 Training Loss: 0.6166654825210571 \n",
      "     Training Step: 17 Training Loss: 0.6151220798492432 \n",
      "     Training Step: 18 Training Loss: 0.6116063594818115 \n",
      "     Training Step: 19 Training Loss: 0.6180926561355591 \n",
      "     Training Step: 20 Training Loss: 0.6107189655303955 \n",
      "     Training Step: 21 Training Loss: 0.6149242520332336 \n",
      "     Training Step: 22 Training Loss: 0.6210545301437378 \n",
      "     Training Step: 23 Training Loss: 0.6198950409889221 \n",
      "     Training Step: 24 Training Loss: 0.6126795411109924 \n",
      "     Training Step: 25 Training Loss: 0.6122551560401917 \n",
      "     Training Step: 26 Training Loss: 0.6155551671981812 \n",
      "     Training Step: 27 Training Loss: 0.6129297018051147 \n",
      "     Training Step: 28 Training Loss: 0.6112330555915833 \n",
      "     Training Step: 29 Training Loss: 0.6175356507301331 \n",
      "     Training Step: 30 Training Loss: 0.6159154772758484 \n",
      "     Training Step: 31 Training Loss: 0.6104174852371216 \n",
      "     Training Step: 32 Training Loss: 0.6114800572395325 \n",
      "     Training Step: 33 Training Loss: 0.6202582120895386 \n",
      "     Training Step: 34 Training Loss: 0.612572431564331 \n",
      "     Training Step: 35 Training Loss: 0.6147411465644836 \n",
      "     Training Step: 36 Training Loss: 0.6104530096054077 \n",
      "     Training Step: 37 Training Loss: 0.6117861270904541 \n",
      "     Training Step: 38 Training Loss: 0.61308753490448 \n",
      "     Training Step: 39 Training Loss: 0.6176985502243042 \n",
      "     Training Step: 40 Training Loss: 0.6118679642677307 \n",
      "     Training Step: 41 Training Loss: 0.6127790808677673 \n",
      "     Training Step: 42 Training Loss: 0.6107131242752075 \n",
      "     Training Step: 43 Training Loss: 0.6156795024871826 \n",
      "     Training Step: 44 Training Loss: 0.6159586310386658 \n",
      "     Training Step: 45 Training Loss: 0.6147318482398987 \n",
      "     Training Step: 46 Training Loss: 0.6166526675224304 \n",
      "     Training Step: 47 Training Loss: 0.6133131980895996 \n",
      "     Training Step: 48 Training Loss: 0.6152666807174683 \n",
      "     Training Step: 49 Training Loss: 0.6133502721786499 \n",
      "     Training Step: 50 Training Loss: 0.6157153844833374 \n",
      "     Training Step: 51 Training Loss: 0.6119093298912048 \n",
      "     Training Step: 52 Training Loss: 0.6142386794090271 \n",
      "     Training Step: 53 Training Loss: 0.6157486438751221 \n",
      "     Training Step: 54 Training Loss: 0.6167836785316467 \n",
      "     Training Step: 55 Training Loss: 0.6115953326225281 \n",
      "     Training Step: 56 Training Loss: 0.6154311895370483 \n",
      "     Training Step: 57 Training Loss: 0.614094078540802 \n",
      "     Training Step: 58 Training Loss: 0.615454375743866 \n",
      "     Training Step: 59 Training Loss: 0.6162250638008118 \n",
      "     Training Step: 60 Training Loss: 0.61116623878479 \n",
      "     Training Step: 61 Training Loss: 0.6147046089172363 \n",
      "     Training Step: 62 Training Loss: 0.6106029748916626 \n",
      "     Training Step: 63 Training Loss: 0.61540287733078 \n",
      "     Training Step: 64 Training Loss: 0.6168273687362671 \n",
      "     Training Step: 65 Training Loss: 0.6154381036758423 \n",
      "     Training Step: 66 Training Loss: 0.6153738498687744 \n",
      "     Training Step: 67 Training Loss: 0.6108026504516602 \n",
      "     Training Step: 68 Training Loss: 0.6146209836006165 \n",
      "     Training Step: 69 Training Loss: 0.6155418753623962 \n",
      "     Training Step: 70 Training Loss: 0.6132109761238098 \n",
      "     Training Step: 71 Training Loss: 0.6180580854415894 \n",
      "     Training Step: 72 Training Loss: 0.6163517236709595 \n",
      "     Training Step: 73 Training Loss: 0.6125684976577759 \n",
      "     Training Step: 74 Training Loss: 0.6146674752235413 \n",
      "     Training Step: 75 Training Loss: 0.6153076887130737 \n",
      "     Training Step: 76 Training Loss: 0.6196245551109314 \n",
      "     Training Step: 77 Training Loss: 0.612531304359436 \n",
      "     Training Step: 78 Training Loss: 0.6102450489997864 \n",
      "     Training Step: 79 Training Loss: 0.6122990250587463 \n",
      "     Training Step: 80 Training Loss: 0.6133344769477844 \n",
      "     Training Step: 81 Training Loss: 0.614827036857605 \n",
      "     Training Step: 82 Training Loss: 0.610123872756958 \n",
      "     Training Step: 83 Training Loss: 0.6160843968391418 \n",
      "     Training Step: 84 Training Loss: 0.6123751997947693 \n",
      "     Training Step: 85 Training Loss: 0.6137599349021912 \n",
      "     Training Step: 86 Training Loss: 0.6115776300430298 \n",
      "     Training Step: 87 Training Loss: 0.6083009243011475 \n",
      "     Training Step: 88 Training Loss: 0.6114233732223511 \n",
      "     Training Step: 89 Training Loss: 0.6150287389755249 \n",
      "     Training Step: 90 Training Loss: 0.612947404384613 \n",
      "     Training Step: 91 Training Loss: 0.6115431785583496 \n",
      "     Training Step: 92 Training Loss: 0.6147071719169617 \n",
      "     Training Step: 93 Training Loss: 0.6106670498847961 \n",
      "     Training Step: 94 Training Loss: 0.6111973524093628 \n",
      "     Training Step: 95 Training Loss: 0.6141861081123352 \n",
      "     Training Step: 96 Training Loss: 0.6143702268600464 \n",
      "     Training Step: 97 Training Loss: 0.6118459105491638 \n",
      "     Training Step: 98 Training Loss: 0.6114455461502075 \n",
      "     Training Step: 99 Training Loss: 0.6153503060340881 \n",
      "     Training Step: 100 Training Loss: 0.6181508302688599 \n",
      "     Training Step: 101 Training Loss: 0.6164200305938721 \n",
      "     Training Step: 102 Training Loss: 0.6146235466003418 \n",
      "     Training Step: 103 Training Loss: 0.6096236705780029 \n",
      "     Training Step: 104 Training Loss: 0.6145305037498474 \n",
      "     Training Step: 105 Training Loss: 0.6097699403762817 \n",
      "     Training Step: 106 Training Loss: 0.614698052406311 \n",
      "     Training Step: 107 Training Loss: 0.6170077919960022 \n",
      "     Training Step: 108 Training Loss: 0.6131273508071899 \n",
      "     Training Step: 109 Training Loss: 0.6142213344573975 \n",
      "     Training Step: 110 Training Loss: 0.6136317849159241 \n",
      "     Training Step: 111 Training Loss: 0.6166663765907288 \n",
      "     Training Step: 112 Training Loss: 0.6156186461448669 \n",
      "     Training Step: 113 Training Loss: 0.6183432340621948 \n",
      "     Training Step: 114 Training Loss: 0.6136903762817383 \n",
      "     Training Step: 115 Training Loss: 0.6095130443572998 \n",
      "     Training Step: 116 Training Loss: 0.6189103126525879 \n",
      "     Training Step: 117 Training Loss: 0.6172094941139221 \n",
      "     Training Step: 118 Training Loss: 0.6155586242675781 \n",
      "     Training Step: 119 Training Loss: 0.6101177334785461 \n",
      "     Training Step: 120 Training Loss: 0.6121693253517151 \n",
      "     Training Step: 121 Training Loss: 0.6109026670455933 \n",
      "     Training Step: 122 Training Loss: 0.6144781708717346 \n",
      "     Training Step: 123 Training Loss: 0.6161571145057678 \n",
      "     Training Step: 124 Training Loss: 0.6128235459327698 \n",
      "     Training Step: 125 Training Loss: 0.6129258275032043 \n",
      "     Training Step: 126 Training Loss: 0.6197790503501892 \n",
      "     Training Step: 127 Training Loss: 0.614125669002533 \n",
      "     Training Step: 128 Training Loss: 0.616816520690918 \n",
      "     Training Step: 129 Training Loss: 0.6167122721672058 \n",
      "     Training Step: 130 Training Loss: 0.6148347854614258 \n",
      "     Training Step: 131 Training Loss: 0.6128138303756714 \n",
      "     Training Step: 132 Training Loss: 0.612155556678772 \n",
      "     Training Step: 133 Training Loss: 0.6100270748138428 \n",
      "     Training Step: 134 Training Loss: 0.6116458773612976 \n",
      "     Training Step: 135 Training Loss: 0.617938756942749 \n",
      "     Training Step: 136 Training Loss: 0.6168785095214844 \n",
      "     Training Step: 137 Training Loss: 0.6127827167510986 \n",
      "     Training Step: 138 Training Loss: 0.6151957511901855 \n",
      "     Training Step: 139 Training Loss: 0.6138604283332825 \n",
      "     Training Step: 140 Training Loss: 0.6181166172027588 \n",
      "     Training Step: 141 Training Loss: 0.6124324202537537 \n",
      "     Training Step: 142 Training Loss: 0.6132091283798218 \n",
      "     Training Step: 143 Training Loss: 0.6147822737693787 \n",
      "     Training Step: 144 Training Loss: 0.6135024428367615 \n",
      "     Training Step: 145 Training Loss: 0.6185597777366638 \n",
      "     Training Step: 146 Training Loss: 0.6151142716407776 \n",
      "     Training Step: 147 Training Loss: 0.615752637386322 \n",
      "     Training Step: 148 Training Loss: 0.6140814423561096 \n",
      "     Training Step: 149 Training Loss: 0.614963173866272 \n",
      "     Training Step: 150 Training Loss: 0.6116896271705627 \n",
      "     Training Step: 151 Training Loss: 0.6128502488136292 \n",
      "     Training Step: 152 Training Loss: 0.6142908334732056 \n",
      "     Training Step: 153 Training Loss: 0.6158568263053894 \n",
      "     Training Step: 154 Training Loss: 0.6165643334388733 \n",
      "     Training Step: 155 Training Loss: 0.6105091571807861 \n",
      "     Training Step: 156 Training Loss: 0.6118303537368774 \n",
      "     Training Step: 157 Training Loss: 0.6178202033042908 \n",
      "     Training Step: 158 Training Loss: 0.6098046898841858 \n",
      "     Training Step: 159 Training Loss: 0.6182397603988647 \n",
      "     Training Step: 160 Training Loss: 0.6139189600944519 \n",
      "     Training Step: 161 Training Loss: 0.6122320294380188 \n",
      "     Training Step: 162 Training Loss: 0.6178668141365051 \n",
      "     Training Step: 163 Training Loss: 0.6121590733528137 \n",
      "     Training Step: 164 Training Loss: 0.6177607774734497 \n",
      "     Training Step: 165 Training Loss: 0.6106204390525818 \n",
      "     Training Step: 166 Training Loss: 0.6118351817131042 \n",
      "     Training Step: 167 Training Loss: 0.6114270091056824 \n",
      "     Training Step: 168 Training Loss: 0.6130857467651367 \n",
      "     Training Step: 169 Training Loss: 0.6100249886512756 \n",
      "     Training Step: 170 Training Loss: 0.6140598058700562 \n",
      "     Training Step: 171 Training Loss: 0.6153576970100403 \n",
      "     Training Step: 172 Training Loss: 0.6167898774147034 \n",
      "     Training Step: 173 Training Loss: 0.6106315851211548 \n",
      "     Training Step: 174 Training Loss: 0.6132978796958923 \n",
      "     Training Step: 175 Training Loss: 0.6133772134780884 \n",
      "     Training Step: 176 Training Loss: 0.6184279918670654 \n",
      "     Training Step: 177 Training Loss: 0.614289402961731 \n",
      "     Training Step: 178 Training Loss: 0.6151818037033081 \n",
      "     Training Step: 179 Training Loss: 0.618597149848938 \n",
      "     Training Step: 180 Training Loss: 0.6114937663078308 \n",
      "     Training Step: 181 Training Loss: 0.6149606704711914 \n",
      "     Training Step: 182 Training Loss: 0.6120926737785339 \n",
      "     Training Step: 183 Training Loss: 0.61771160364151 \n",
      "     Training Step: 184 Training Loss: 0.6167413592338562 \n",
      "     Training Step: 185 Training Loss: 0.617701530456543 \n",
      "     Training Step: 186 Training Loss: 0.6170935034751892 \n",
      "     Training Step: 187 Training Loss: 0.61368727684021 \n",
      "     Training Step: 188 Training Loss: 0.6129629611968994 \n",
      "     Training Step: 189 Training Loss: 0.6171553134918213 \n",
      "     Training Step: 190 Training Loss: 0.6093072295188904 \n",
      "     Training Step: 191 Training Loss: 0.6169524192810059 \n",
      "     Training Step: 192 Training Loss: 0.6116665005683899 \n",
      "     Training Step: 193 Training Loss: 0.6123069524765015 \n",
      "     Training Step: 194 Training Loss: 0.6138346791267395 \n",
      "     Training Step: 195 Training Loss: 0.6167700886726379 \n",
      "     Training Step: 196 Training Loss: 0.6153952479362488 \n",
      "     Training Step: 197 Training Loss: 0.6132519841194153 \n",
      "     Training Step: 198 Training Loss: 0.6182668805122375 \n",
      "     Training Step: 199 Training Loss: 0.6167038083076477 \n",
      "     Training Step: 200 Training Loss: 0.6138045191764832 \n",
      "     Training Step: 201 Training Loss: 0.6141694784164429 \n",
      "     Training Step: 202 Training Loss: 0.6171743273735046 \n",
      "     Training Step: 203 Training Loss: 0.613494336605072 \n",
      "     Training Step: 204 Training Loss: 0.6144759654998779 \n",
      "     Training Step: 205 Training Loss: 0.6121773719787598 \n",
      "     Training Step: 206 Training Loss: 0.6188533902168274 \n",
      "     Training Step: 207 Training Loss: 0.6135154962539673 \n",
      "     Training Step: 208 Training Loss: 0.6126770973205566 \n",
      "     Training Step: 209 Training Loss: 0.6143633127212524 \n",
      "     Training Step: 210 Training Loss: 0.6122387647628784 \n",
      "     Training Step: 211 Training Loss: 0.6162450909614563 \n",
      "     Training Step: 212 Training Loss: 0.6153749823570251 \n",
      "     Training Step: 213 Training Loss: 0.6129812002182007 \n",
      "     Training Step: 214 Training Loss: 0.6162471771240234 \n",
      "     Training Step: 215 Training Loss: 0.6158363223075867 \n",
      "     Training Step: 216 Training Loss: 0.6098024845123291 \n",
      "     Training Step: 217 Training Loss: 0.6116653084754944 \n",
      "     Training Step: 218 Training Loss: 0.615491509437561 \n",
      "     Training Step: 219 Training Loss: 0.6151204705238342 \n",
      "     Training Step: 220 Training Loss: 0.6164693832397461 \n",
      "     Training Step: 221 Training Loss: 0.6134222745895386 \n",
      "     Training Step: 222 Training Loss: 0.6144327521324158 \n",
      "     Training Step: 223 Training Loss: 0.6109371781349182 \n",
      "     Training Step: 224 Training Loss: 0.6140391826629639 \n",
      "     Training Step: 225 Training Loss: 0.6132749915122986 \n",
      "     Training Step: 226 Training Loss: 0.6177200078964233 \n",
      "     Training Step: 227 Training Loss: 0.6186432838439941 \n",
      "     Training Step: 228 Training Loss: 0.612406849861145 \n",
      "     Training Step: 229 Training Loss: 0.6124926209449768 \n",
      "     Training Step: 230 Training Loss: 0.6101582050323486 \n",
      "     Training Step: 231 Training Loss: 0.6116027235984802 \n",
      "     Training Step: 232 Training Loss: 0.6139274835586548 \n",
      "     Training Step: 233 Training Loss: 0.6144219636917114 \n",
      "     Training Step: 234 Training Loss: 0.6135251522064209 \n",
      "     Training Step: 235 Training Loss: 0.61724454164505 \n",
      "     Training Step: 236 Training Loss: 0.6114373803138733 \n",
      "     Training Step: 237 Training Loss: 0.6146069169044495 \n",
      "     Training Step: 238 Training Loss: 0.6105538010597229 \n",
      "     Training Step: 239 Training Loss: 0.6105919480323792 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6183516383171082 \n",
      "     Validation Step: 1 Validation Loss: 0.6133151650428772 \n",
      "     Validation Step: 2 Validation Loss: 0.6136460304260254 \n",
      "     Validation Step: 3 Validation Loss: 0.6111549735069275 \n",
      "     Validation Step: 4 Validation Loss: 0.6185733079910278 \n",
      "     Validation Step: 5 Validation Loss: 0.6153683066368103 \n",
      "     Validation Step: 6 Validation Loss: 0.6156729459762573 \n",
      "     Validation Step: 7 Validation Loss: 0.6104690432548523 \n",
      "     Validation Step: 8 Validation Loss: 0.6146103739738464 \n",
      "     Validation Step: 9 Validation Loss: 0.614129900932312 \n",
      "     Validation Step: 10 Validation Loss: 0.6106144785881042 \n",
      "     Validation Step: 11 Validation Loss: 0.6146780848503113 \n",
      "     Validation Step: 12 Validation Loss: 0.6177919507026672 \n",
      "     Validation Step: 13 Validation Loss: 0.6101419925689697 \n",
      "     Validation Step: 14 Validation Loss: 0.61427903175354 \n",
      "     Validation Step: 15 Validation Loss: 0.617110013961792 \n",
      "     Validation Step: 16 Validation Loss: 0.6145632863044739 \n",
      "     Validation Step: 17 Validation Loss: 0.6142298579216003 \n",
      "     Validation Step: 18 Validation Loss: 0.6101067662239075 \n",
      "     Validation Step: 19 Validation Loss: 0.6115441918373108 \n",
      "     Validation Step: 20 Validation Loss: 0.6184495091438293 \n",
      "     Validation Step: 21 Validation Loss: 0.6148689985275269 \n",
      "     Validation Step: 22 Validation Loss: 0.6141523718833923 \n",
      "     Validation Step: 23 Validation Loss: 0.6160876154899597 \n",
      "     Validation Step: 24 Validation Loss: 0.6104925870895386 \n",
      "     Validation Step: 25 Validation Loss: 0.6181623339653015 \n",
      "     Validation Step: 26 Validation Loss: 0.611633837223053 \n",
      "     Validation Step: 27 Validation Loss: 0.6111739873886108 \n",
      "     Validation Step: 28 Validation Loss: 0.6121176481246948 \n",
      "     Validation Step: 29 Validation Loss: 0.6156557202339172 \n",
      "     Validation Step: 30 Validation Loss: 0.616312563419342 \n",
      "     Validation Step: 31 Validation Loss: 0.6186052560806274 \n",
      "     Validation Step: 32 Validation Loss: 0.6100753545761108 \n",
      "     Validation Step: 33 Validation Loss: 0.614922285079956 \n",
      "     Validation Step: 34 Validation Loss: 0.6118685007095337 \n",
      "     Validation Step: 35 Validation Loss: 0.61510169506073 \n",
      "     Validation Step: 36 Validation Loss: 0.6137005090713501 \n",
      "     Validation Step: 37 Validation Loss: 0.6129915118217468 \n",
      "     Validation Step: 38 Validation Loss: 0.6128201484680176 \n",
      "     Validation Step: 39 Validation Loss: 0.6158686280250549 \n",
      "     Validation Step: 40 Validation Loss: 0.6136967539787292 \n",
      "     Validation Step: 41 Validation Loss: 0.6074651479721069 \n",
      "     Validation Step: 42 Validation Loss: 0.6173819899559021 \n",
      "     Validation Step: 43 Validation Loss: 0.6176720857620239 \n",
      "     Validation Step: 44 Validation Loss: 0.6152714490890503 \n",
      "Epoch: 66\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6178389191627502 \n",
      "     Training Step: 1 Training Loss: 0.6128890514373779 \n",
      "     Training Step: 2 Training Loss: 0.615388035774231 \n",
      "     Training Step: 3 Training Loss: 0.6132132411003113 \n",
      "     Training Step: 4 Training Loss: 0.612158477306366 \n",
      "     Training Step: 5 Training Loss: 0.6162187457084656 \n",
      "     Training Step: 6 Training Loss: 0.615086555480957 \n",
      "     Training Step: 7 Training Loss: 0.6136782765388489 \n",
      "     Training Step: 8 Training Loss: 0.6155325770378113 \n",
      "     Training Step: 9 Training Loss: 0.6169145107269287 \n",
      "     Training Step: 10 Training Loss: 0.6127819418907166 \n",
      "     Training Step: 11 Training Loss: 0.6154361367225647 \n",
      "     Training Step: 12 Training Loss: 0.6183398962020874 \n",
      "     Training Step: 13 Training Loss: 0.610190749168396 \n",
      "     Training Step: 14 Training Loss: 0.6106234788894653 \n",
      "     Training Step: 15 Training Loss: 0.6141095161437988 \n",
      "     Training Step: 16 Training Loss: 0.6167572140693665 \n",
      "     Training Step: 17 Training Loss: 0.6197768449783325 \n",
      "     Training Step: 18 Training Loss: 0.6166752576828003 \n",
      "     Training Step: 19 Training Loss: 0.6166756749153137 \n",
      "     Training Step: 20 Training Loss: 0.6162164807319641 \n",
      "     Training Step: 21 Training Loss: 0.6155564785003662 \n",
      "     Training Step: 22 Training Loss: 0.6154548525810242 \n",
      "     Training Step: 23 Training Loss: 0.6100934743881226 \n",
      "     Training Step: 24 Training Loss: 0.6139474511146545 \n",
      "     Training Step: 25 Training Loss: 0.6158150434494019 \n",
      "     Training Step: 26 Training Loss: 0.6158208847045898 \n",
      "     Training Step: 27 Training Loss: 0.6125195622444153 \n",
      "     Training Step: 28 Training Loss: 0.6145338416099548 \n",
      "     Training Step: 29 Training Loss: 0.6132676005363464 \n",
      "     Training Step: 30 Training Loss: 0.6125375628471375 \n",
      "     Training Step: 31 Training Loss: 0.6186567544937134 \n",
      "     Training Step: 32 Training Loss: 0.610439658164978 \n",
      "     Training Step: 33 Training Loss: 0.6178277730941772 \n",
      "     Training Step: 34 Training Loss: 0.617175817489624 \n",
      "     Training Step: 35 Training Loss: 0.6125676035881042 \n",
      "     Training Step: 36 Training Loss: 0.6129329800605774 \n",
      "     Training Step: 37 Training Loss: 0.6112387776374817 \n",
      "     Training Step: 38 Training Loss: 0.6114617586135864 \n",
      "     Training Step: 39 Training Loss: 0.6116659641265869 \n",
      "     Training Step: 40 Training Loss: 0.6121021509170532 \n",
      "     Training Step: 41 Training Loss: 0.6153257489204407 \n",
      "     Training Step: 42 Training Loss: 0.6152269840240479 \n",
      "     Training Step: 43 Training Loss: 0.617785632610321 \n",
      "     Training Step: 44 Training Loss: 0.6149441003799438 \n",
      "     Training Step: 45 Training Loss: 0.614812433719635 \n",
      "     Training Step: 46 Training Loss: 0.6156930327415466 \n",
      "     Training Step: 47 Training Loss: 0.6154286861419678 \n",
      "     Training Step: 48 Training Loss: 0.6133608222007751 \n",
      "     Training Step: 49 Training Loss: 0.6117508411407471 \n",
      "     Training Step: 50 Training Loss: 0.616564154624939 \n",
      "     Training Step: 51 Training Loss: 0.6151270866394043 \n",
      "     Training Step: 52 Training Loss: 0.6123101115226746 \n",
      "     Training Step: 53 Training Loss: 0.6161483526229858 \n",
      "     Training Step: 54 Training Loss: 0.6121887564659119 \n",
      "     Training Step: 55 Training Loss: 0.6168405413627625 \n",
      "     Training Step: 56 Training Loss: 0.6116427779197693 \n",
      "     Training Step: 57 Training Loss: 0.6160348057746887 \n",
      "     Training Step: 58 Training Loss: 0.6106778979301453 \n",
      "     Training Step: 59 Training Loss: 0.6122382879257202 \n",
      "     Training Step: 60 Training Loss: 0.6137614250183105 \n",
      "     Training Step: 61 Training Loss: 0.6160812377929688 \n",
      "     Training Step: 62 Training Loss: 0.6133311986923218 \n",
      "     Training Step: 63 Training Loss: 0.6147438287734985 \n",
      "     Training Step: 64 Training Loss: 0.6133056879043579 \n",
      "     Training Step: 65 Training Loss: 0.614723265171051 \n",
      "     Training Step: 66 Training Loss: 0.6119350790977478 \n",
      "     Training Step: 67 Training Loss: 0.6154430508613586 \n",
      "     Training Step: 68 Training Loss: 0.612286388874054 \n",
      "     Training Step: 69 Training Loss: 0.6122386455535889 \n",
      "     Training Step: 70 Training Loss: 0.6105836033821106 \n",
      "     Training Step: 71 Training Loss: 0.6157804727554321 \n",
      "     Training Step: 72 Training Loss: 0.6136842966079712 \n",
      "     Training Step: 73 Training Loss: 0.6116294264793396 \n",
      "     Training Step: 74 Training Loss: 0.6097186803817749 \n",
      "     Training Step: 75 Training Loss: 0.6147617101669312 \n",
      "     Training Step: 76 Training Loss: 0.6143520474433899 \n",
      "     Training Step: 77 Training Loss: 0.6125420928001404 \n",
      "     Training Step: 78 Training Loss: 0.6128469705581665 \n",
      "     Training Step: 79 Training Loss: 0.614658534526825 \n",
      "     Training Step: 80 Training Loss: 0.6116399168968201 \n",
      "     Training Step: 81 Training Loss: 0.614938497543335 \n",
      "     Training Step: 82 Training Loss: 0.6097525954246521 \n",
      "     Training Step: 83 Training Loss: 0.614234447479248 \n",
      "     Training Step: 84 Training Loss: 0.6147063374519348 \n",
      "     Training Step: 85 Training Loss: 0.6155484914779663 \n",
      "     Training Step: 86 Training Loss: 0.6199322938919067 \n",
      "     Training Step: 87 Training Loss: 0.6121688485145569 \n",
      "     Training Step: 88 Training Loss: 0.6108249425888062 \n",
      "     Training Step: 89 Training Loss: 0.6132169365882874 \n",
      "     Training Step: 90 Training Loss: 0.611563503742218 \n",
      "     Training Step: 91 Training Loss: 0.6118466854095459 \n",
      "     Training Step: 92 Training Loss: 0.6144477725028992 \n",
      "     Training Step: 93 Training Loss: 0.6197842955589294 \n",
      "     Training Step: 94 Training Loss: 0.6111608147621155 \n",
      "     Training Step: 95 Training Loss: 0.6154985427856445 \n",
      "     Training Step: 96 Training Loss: 0.6141787171363831 \n",
      "     Training Step: 97 Training Loss: 0.6132021546363831 \n",
      "     Training Step: 98 Training Loss: 0.6178175210952759 \n",
      "     Training Step: 99 Training Loss: 0.6133623123168945 \n",
      "     Training Step: 100 Training Loss: 0.6129736304283142 \n",
      "     Training Step: 101 Training Loss: 0.6139674186706543 \n",
      "     Training Step: 102 Training Loss: 0.6108957529067993 \n",
      "     Training Step: 103 Training Loss: 0.6169643998146057 \n",
      "     Training Step: 104 Training Loss: 0.6116400957107544 \n",
      "     Training Step: 105 Training Loss: 0.6153838038444519 \n",
      "     Training Step: 106 Training Loss: 0.6107693910598755 \n",
      "     Training Step: 107 Training Loss: 0.6095001101493835 \n",
      "     Training Step: 108 Training Loss: 0.6168413162231445 \n",
      "     Training Step: 109 Training Loss: 0.610492467880249 \n",
      "     Training Step: 110 Training Loss: 0.6153457760810852 \n",
      "     Training Step: 111 Training Loss: 0.6129521727561951 \n",
      "     Training Step: 112 Training Loss: 0.6148875951766968 \n",
      "     Training Step: 113 Training Loss: 0.6143001914024353 \n",
      "     Training Step: 114 Training Loss: 0.6116085648536682 \n",
      "     Training Step: 115 Training Loss: 0.6115363240242004 \n",
      "     Training Step: 116 Training Loss: 0.6114599108695984 \n",
      "     Training Step: 117 Training Loss: 0.6123910546302795 \n",
      "     Training Step: 118 Training Loss: 0.6185718774795532 \n",
      "     Training Step: 119 Training Loss: 0.6167543530464172 \n",
      "     Training Step: 120 Training Loss: 0.6158662438392639 \n",
      "     Training Step: 121 Training Loss: 0.6148274540901184 \n",
      "     Training Step: 122 Training Loss: 0.6138848662376404 \n",
      "     Training Step: 123 Training Loss: 0.6153323650360107 \n",
      "     Training Step: 124 Training Loss: 0.6126210689544678 \n",
      "     Training Step: 125 Training Loss: 0.6166588068008423 \n",
      "     Training Step: 126 Training Loss: 0.6143508553504944 \n",
      "     Training Step: 127 Training Loss: 0.6153704524040222 \n",
      "     Training Step: 128 Training Loss: 0.6186238527297974 \n",
      "     Training Step: 129 Training Loss: 0.6121829748153687 \n",
      "     Training Step: 130 Training Loss: 0.6133399605751038 \n",
      "     Training Step: 131 Training Loss: 0.6170927286148071 \n",
      "     Training Step: 132 Training Loss: 0.6114741563796997 \n",
      "     Training Step: 133 Training Loss: 0.6105251312255859 \n",
      "     Training Step: 134 Training Loss: 0.6172232627868652 \n",
      "     Training Step: 135 Training Loss: 0.6154857277870178 \n",
      "     Training Step: 136 Training Loss: 0.6123780012130737 \n",
      "     Training Step: 137 Training Loss: 0.61770099401474 \n",
      "     Training Step: 138 Training Loss: 0.6137678027153015 \n",
      "     Training Step: 139 Training Loss: 0.6146425604820251 \n",
      "     Training Step: 140 Training Loss: 0.6146897673606873 \n",
      "     Training Step: 141 Training Loss: 0.6142013072967529 \n",
      "     Training Step: 142 Training Loss: 0.6130967140197754 \n",
      "     Training Step: 143 Training Loss: 0.616641104221344 \n",
      "     Training Step: 144 Training Loss: 0.6114341616630554 \n",
      "     Training Step: 145 Training Loss: 0.6210428476333618 \n",
      "     Training Step: 146 Training Loss: 0.6134729385375977 \n",
      "     Training Step: 147 Training Loss: 0.6184559464454651 \n",
      "     Training Step: 148 Training Loss: 0.6123036742210388 \n",
      "     Training Step: 149 Training Loss: 0.6158009767532349 \n",
      "     Training Step: 150 Training Loss: 0.6171609163284302 \n",
      "     Training Step: 151 Training Loss: 0.6184295415878296 \n",
      "     Training Step: 152 Training Loss: 0.6137871742248535 \n",
      "     Training Step: 153 Training Loss: 0.6140785813331604 \n",
      "     Training Step: 154 Training Loss: 0.6101862788200378 \n",
      "     Training Step: 155 Training Loss: 0.6120632290840149 \n",
      "     Training Step: 156 Training Loss: 0.6131069660186768 \n",
      "     Training Step: 157 Training Loss: 0.6144700646400452 \n",
      "     Training Step: 158 Training Loss: 0.6183247566223145 \n",
      "     Training Step: 159 Training Loss: 0.6164073944091797 \n",
      "     Training Step: 160 Training Loss: 0.6102747917175293 \n",
      "     Training Step: 161 Training Loss: 0.6177200078964233 \n",
      "     Training Step: 162 Training Loss: 0.6131697297096252 \n",
      "     Training Step: 163 Training Loss: 0.614357054233551 \n",
      "     Training Step: 164 Training Loss: 0.6092297434806824 \n",
      "     Training Step: 165 Training Loss: 0.6115642189979553 \n",
      "     Training Step: 166 Training Loss: 0.611872136592865 \n",
      "     Training Step: 167 Training Loss: 0.6118767261505127 \n",
      "     Training Step: 168 Training Loss: 0.6147589683532715 \n",
      "     Training Step: 169 Training Loss: 0.6126528382301331 \n",
      "     Training Step: 170 Training Loss: 0.6109116077423096 \n",
      "     Training Step: 171 Training Loss: 0.6178053021430969 \n",
      "     Training Step: 172 Training Loss: 0.6189030408859253 \n",
      "     Training Step: 173 Training Loss: 0.6114779114723206 \n",
      "     Training Step: 174 Training Loss: 0.614709734916687 \n",
      "     Training Step: 175 Training Loss: 0.6173985004425049 \n",
      "     Training Step: 176 Training Loss: 0.610100269317627 \n",
      "     Training Step: 177 Training Loss: 0.618890106678009 \n",
      "     Training Step: 178 Training Loss: 0.6164434552192688 \n",
      "     Training Step: 179 Training Loss: 0.6202298998832703 \n",
      "     Training Step: 180 Training Loss: 0.6135731935501099 \n",
      "     Training Step: 181 Training Loss: 0.6119286417961121 \n",
      "     Training Step: 182 Training Loss: 0.6179963946342468 \n",
      "     Training Step: 183 Training Loss: 0.6127728223800659 \n",
      "     Training Step: 184 Training Loss: 0.6106090545654297 \n",
      "     Training Step: 185 Training Loss: 0.6168169975280762 \n",
      "     Training Step: 186 Training Loss: 0.6135351657867432 \n",
      "     Training Step: 187 Training Loss: 0.610694944858551 \n",
      "     Training Step: 188 Training Loss: 0.6172776222229004 \n",
      "     Training Step: 189 Training Loss: 0.6151154637336731 \n",
      "     Training Step: 190 Training Loss: 0.6133579611778259 \n",
      "     Training Step: 191 Training Loss: 0.6167450547218323 \n",
      "     Training Step: 192 Training Loss: 0.6145140528678894 \n",
      "     Training Step: 193 Training Loss: 0.612946629524231 \n",
      "     Training Step: 194 Training Loss: 0.6147048473358154 \n",
      "     Training Step: 195 Training Loss: 0.6194993257522583 \n",
      "     Training Step: 196 Training Loss: 0.6144399046897888 \n",
      "     Training Step: 197 Training Loss: 0.6142532229423523 \n",
      "     Training Step: 198 Training Loss: 0.6136125326156616 \n",
      "     Training Step: 199 Training Loss: 0.6180956959724426 \n",
      "     Training Step: 200 Training Loss: 0.6101453304290771 \n",
      "     Training Step: 201 Training Loss: 0.6156032085418701 \n",
      "     Training Step: 202 Training Loss: 0.6094387769699097 \n",
      "     Training Step: 203 Training Loss: 0.614496111869812 \n",
      "     Training Step: 204 Training Loss: 0.6146537065505981 \n",
      "     Training Step: 205 Training Loss: 0.6097161769866943 \n",
      "     Training Step: 206 Training Loss: 0.6153505444526672 \n",
      "     Training Step: 207 Training Loss: 0.6082603931427002 \n",
      "     Training Step: 208 Training Loss: 0.6118490695953369 \n",
      "     Training Step: 209 Training Loss: 0.6167899966239929 \n",
      "     Training Step: 210 Training Loss: 0.6103898286819458 \n",
      "     Training Step: 211 Training Loss: 0.6157791614532471 \n",
      "     Training Step: 212 Training Loss: 0.6182750463485718 \n",
      "     Training Step: 213 Training Loss: 0.6140244603157043 \n",
      "     Training Step: 214 Training Loss: 0.6140347123146057 \n",
      "     Training Step: 215 Training Loss: 0.6162549257278442 \n",
      "     Training Step: 216 Training Loss: 0.6140637397766113 \n",
      "     Training Step: 217 Training Loss: 0.6134592294692993 \n",
      "     Training Step: 218 Training Loss: 0.6105950474739075 \n",
      "     Training Step: 219 Training Loss: 0.6164429187774658 \n",
      "     Training Step: 220 Training Loss: 0.6175500154495239 \n",
      "     Training Step: 221 Training Loss: 0.6142207384109497 \n",
      "     Training Step: 222 Training Loss: 0.6115537881851196 \n",
      "     Training Step: 223 Training Loss: 0.6146669387817383 \n",
      "     Training Step: 224 Training Loss: 0.6128355264663696 \n",
      "     Training Step: 225 Training Loss: 0.6146388649940491 \n",
      "     Training Step: 226 Training Loss: 0.6135175824165344 \n",
      "     Training Step: 227 Training Loss: 0.6151973605155945 \n",
      "     Training Step: 228 Training Loss: 0.6168357729911804 \n",
      "     Training Step: 229 Training Loss: 0.6127687692642212 \n",
      "     Training Step: 230 Training Loss: 0.6136612892150879 \n",
      "     Training Step: 231 Training Loss: 0.6124007105827332 \n",
      "     Training Step: 232 Training Loss: 0.611171543598175 \n",
      "     Training Step: 233 Training Loss: 0.6159675121307373 \n",
      "     Training Step: 234 Training Loss: 0.6181106567382812 \n",
      "     Training Step: 235 Training Loss: 0.6139011979103088 \n",
      "     Training Step: 236 Training Loss: 0.6118654012680054 \n",
      "     Training Step: 237 Training Loss: 0.6167694330215454 \n",
      "     Training Step: 238 Training Loss: 0.6180436015129089 \n",
      "     Training Step: 239 Training Loss: 0.6149674654006958 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6182023882865906 \n",
      "     Validation Step: 1 Validation Loss: 0.6136890649795532 \n",
      "     Validation Step: 2 Validation Loss: 0.6130350232124329 \n",
      "     Validation Step: 3 Validation Loss: 0.6169977784156799 \n",
      "     Validation Step: 4 Validation Loss: 0.6162448525428772 \n",
      "     Validation Step: 5 Validation Loss: 0.6105671525001526 \n",
      "     Validation Step: 6 Validation Loss: 0.6175934076309204 \n",
      "     Validation Step: 7 Validation Loss: 0.6180385947227478 \n",
      "     Validation Step: 8 Validation Loss: 0.6156172156333923 \n",
      "     Validation Step: 9 Validation Loss: 0.610594630241394 \n",
      "     Validation Step: 10 Validation Loss: 0.6155751943588257 \n",
      "     Validation Step: 11 Validation Loss: 0.6142271757125854 \n",
      "     Validation Step: 12 Validation Loss: 0.6158040761947632 \n",
      "     Validation Step: 13 Validation Loss: 0.6142900586128235 \n",
      "     Validation Step: 14 Validation Loss: 0.6136652231216431 \n",
      "     Validation Step: 15 Validation Loss: 0.6183161735534668 \n",
      "     Validation Step: 16 Validation Loss: 0.6148629188537598 \n",
      "     Validation Step: 17 Validation Loss: 0.6116278767585754 \n",
      "     Validation Step: 18 Validation Loss: 0.6184489130973816 \n",
      "     Validation Step: 19 Validation Loss: 0.6102561950683594 \n",
      "     Validation Step: 20 Validation Loss: 0.6159865856170654 \n",
      "     Validation Step: 21 Validation Loss: 0.6128953099250793 \n",
      "     Validation Step: 22 Validation Loss: 0.6117183566093445 \n",
      "     Validation Step: 23 Validation Loss: 0.6145695447921753 \n",
      "     Validation Step: 24 Validation Loss: 0.6133514642715454 \n",
      "     Validation Step: 25 Validation Loss: 0.611247181892395 \n",
      "     Validation Step: 26 Validation Loss: 0.6141676902770996 \n",
      "     Validation Step: 27 Validation Loss: 0.6102699637413025 \n",
      "     Validation Step: 28 Validation Loss: 0.610727846622467 \n",
      "     Validation Step: 29 Validation Loss: 0.6149196028709412 \n",
      "     Validation Step: 30 Validation Loss: 0.6150460243225098 \n",
      "     Validation Step: 31 Validation Loss: 0.610219419002533 \n",
      "     Validation Step: 32 Validation Loss: 0.6076822280883789 \n",
      "     Validation Step: 33 Validation Loss: 0.6121972799301147 \n",
      "     Validation Step: 34 Validation Loss: 0.6153227090835571 \n",
      "     Validation Step: 35 Validation Loss: 0.6141605377197266 \n",
      "     Validation Step: 36 Validation Loss: 0.6119591593742371 \n",
      "     Validation Step: 37 Validation Loss: 0.6184661388397217 \n",
      "     Validation Step: 38 Validation Loss: 0.6172914505004883 \n",
      "     Validation Step: 39 Validation Loss: 0.6176827549934387 \n",
      "     Validation Step: 40 Validation Loss: 0.6112368106842041 \n",
      "     Validation Step: 41 Validation Loss: 0.614662766456604 \n",
      "     Validation Step: 42 Validation Loss: 0.6152427196502686 \n",
      "     Validation Step: 43 Validation Loss: 0.6145870089530945 \n",
      "     Validation Step: 44 Validation Loss: 0.6136987805366516 \n",
      "Epoch: 67\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6125959157943726 \n",
      "     Training Step: 1 Training Loss: 0.6152631640434265 \n",
      "     Training Step: 2 Training Loss: 0.6114731431007385 \n",
      "     Training Step: 3 Training Loss: 0.6147547960281372 \n",
      "     Training Step: 4 Training Loss: 0.6167130470275879 \n",
      "     Training Step: 5 Training Loss: 0.6167484521865845 \n",
      "     Training Step: 6 Training Loss: 0.6116995215415955 \n",
      "     Training Step: 7 Training Loss: 0.618226170539856 \n",
      "     Training Step: 8 Training Loss: 0.6155425906181335 \n",
      "     Training Step: 9 Training Loss: 0.6124464273452759 \n",
      "     Training Step: 10 Training Loss: 0.6153831481933594 \n",
      "     Training Step: 11 Training Loss: 0.6106805801391602 \n",
      "     Training Step: 12 Training Loss: 0.6104816198348999 \n",
      "     Training Step: 13 Training Loss: 0.6132540702819824 \n",
      "     Training Step: 14 Training Loss: 0.6175937652587891 \n",
      "     Training Step: 15 Training Loss: 0.6122487187385559 \n",
      "     Training Step: 16 Training Loss: 0.6144356727600098 \n",
      "     Training Step: 17 Training Loss: 0.6118234395980835 \n",
      "     Training Step: 18 Training Loss: 0.6160434484481812 \n",
      "     Training Step: 19 Training Loss: 0.6123672723770142 \n",
      "     Training Step: 20 Training Loss: 0.6183962821960449 \n",
      "     Training Step: 21 Training Loss: 0.6107224225997925 \n",
      "     Training Step: 22 Training Loss: 0.6135053038597107 \n",
      "     Training Step: 23 Training Loss: 0.6126594543457031 \n",
      "     Training Step: 24 Training Loss: 0.6118637919425964 \n",
      "     Training Step: 25 Training Loss: 0.6114670634269714 \n",
      "     Training Step: 26 Training Loss: 0.6157287359237671 \n",
      "     Training Step: 27 Training Loss: 0.6134151816368103 \n",
      "     Training Step: 28 Training Loss: 0.6143940091133118 \n",
      "     Training Step: 29 Training Loss: 0.6167243719100952 \n",
      "     Training Step: 30 Training Loss: 0.6112046837806702 \n",
      "     Training Step: 31 Training Loss: 0.6125566363334656 \n",
      "     Training Step: 32 Training Loss: 0.6158024072647095 \n",
      "     Training Step: 33 Training Loss: 0.6177483797073364 \n",
      "     Training Step: 34 Training Loss: 0.616161584854126 \n",
      "     Training Step: 35 Training Loss: 0.6132317781448364 \n",
      "     Training Step: 36 Training Loss: 0.6097842454910278 \n",
      "     Training Step: 37 Training Loss: 0.6133714914321899 \n",
      "     Training Step: 38 Training Loss: 0.6148912906646729 \n",
      "     Training Step: 39 Training Loss: 0.6118319034576416 \n",
      "     Training Step: 40 Training Loss: 0.6131107807159424 \n",
      "     Training Step: 41 Training Loss: 0.617156445980072 \n",
      "     Training Step: 42 Training Loss: 0.6167487502098083 \n",
      "     Training Step: 43 Training Loss: 0.6152577996253967 \n",
      "     Training Step: 44 Training Loss: 0.6121422648429871 \n",
      "     Training Step: 45 Training Loss: 0.614228367805481 \n",
      "     Training Step: 46 Training Loss: 0.6129001975059509 \n",
      "     Training Step: 47 Training Loss: 0.6118614077568054 \n",
      "     Training Step: 48 Training Loss: 0.6116199493408203 \n",
      "     Training Step: 49 Training Loss: 0.6128880381584167 \n",
      "     Training Step: 50 Training Loss: 0.6197920441627502 \n",
      "     Training Step: 51 Training Loss: 0.6154605150222778 \n",
      "     Training Step: 52 Training Loss: 0.6123248934745789 \n",
      "     Training Step: 53 Training Loss: 0.6155762672424316 \n",
      "     Training Step: 54 Training Loss: 0.6106942296028137 \n",
      "     Training Step: 55 Training Loss: 0.612942636013031 \n",
      "     Training Step: 56 Training Loss: 0.6169224381446838 \n",
      "     Training Step: 57 Training Loss: 0.6114336848258972 \n",
      "     Training Step: 58 Training Loss: 0.6141459941864014 \n",
      "     Training Step: 59 Training Loss: 0.6146523356437683 \n",
      "     Training Step: 60 Training Loss: 0.6143894195556641 \n",
      "     Training Step: 61 Training Loss: 0.6105871796607971 \n",
      "     Training Step: 62 Training Loss: 0.6141549944877625 \n",
      "     Training Step: 63 Training Loss: 0.6127944588661194 \n",
      "     Training Step: 64 Training Loss: 0.6147058606147766 \n",
      "     Training Step: 65 Training Loss: 0.615293562412262 \n",
      "     Training Step: 66 Training Loss: 0.6177827715873718 \n",
      "     Training Step: 67 Training Loss: 0.6116204857826233 \n",
      "     Training Step: 68 Training Loss: 0.614648163318634 \n",
      "     Training Step: 69 Training Loss: 0.6097370982170105 \n",
      "     Training Step: 70 Training Loss: 0.6131263375282288 \n",
      "     Training Step: 71 Training Loss: 0.614375650882721 \n",
      "     Training Step: 72 Training Loss: 0.6149521470069885 \n",
      "     Training Step: 73 Training Loss: 0.6133062243461609 \n",
      "     Training Step: 74 Training Loss: 0.6162359118461609 \n",
      "     Training Step: 75 Training Loss: 0.6149657964706421 \n",
      "     Training Step: 76 Training Loss: 0.6154884696006775 \n",
      "     Training Step: 77 Training Loss: 0.6104547381401062 \n",
      "     Training Step: 78 Training Loss: 0.6159632802009583 \n",
      "     Training Step: 79 Training Loss: 0.6162629127502441 \n",
      "     Training Step: 80 Training Loss: 0.615196943283081 \n",
      "     Training Step: 81 Training Loss: 0.6133506894111633 \n",
      "     Training Step: 82 Training Loss: 0.6168030500411987 \n",
      "     Training Step: 83 Training Loss: 0.6138243675231934 \n",
      "     Training Step: 84 Training Loss: 0.613912045955658 \n",
      "     Training Step: 85 Training Loss: 0.6106438040733337 \n",
      "     Training Step: 86 Training Loss: 0.6146643757820129 \n",
      "     Training Step: 87 Training Loss: 0.6142222285270691 \n",
      "     Training Step: 88 Training Loss: 0.6124881505966187 \n",
      "     Training Step: 89 Training Loss: 0.6142735481262207 \n",
      "     Training Step: 90 Training Loss: 0.6111944913864136 \n",
      "     Training Step: 91 Training Loss: 0.6082424521446228 \n",
      "     Training Step: 92 Training Loss: 0.6131417155265808 \n",
      "     Training Step: 93 Training Loss: 0.618983268737793 \n",
      "     Training Step: 94 Training Loss: 0.6167435050010681 \n",
      "     Training Step: 95 Training Loss: 0.6182425022125244 \n",
      "     Training Step: 96 Training Loss: 0.6177017092704773 \n",
      "     Training Step: 97 Training Loss: 0.6180950999259949 \n",
      "     Training Step: 98 Training Loss: 0.6152986288070679 \n",
      "     Training Step: 99 Training Loss: 0.6144028306007385 \n",
      "     Training Step: 100 Training Loss: 0.6136980056762695 \n",
      "     Training Step: 101 Training Loss: 0.618337869644165 \n",
      "     Training Step: 102 Training Loss: 0.6121907234191895 \n",
      "     Training Step: 103 Training Loss: 0.6178815364837646 \n",
      "     Training Step: 104 Training Loss: 0.6101265549659729 \n",
      "     Training Step: 105 Training Loss: 0.6156188249588013 \n",
      "     Training Step: 106 Training Loss: 0.6149432063102722 \n",
      "     Training Step: 107 Training Loss: 0.6100504398345947 \n",
      "     Training Step: 108 Training Loss: 0.6146063804626465 \n",
      "     Training Step: 109 Training Loss: 0.6157748103141785 \n",
      "     Training Step: 110 Training Loss: 0.6097163558006287 \n",
      "     Training Step: 111 Training Loss: 0.6116381287574768 \n",
      "     Training Step: 112 Training Loss: 0.6176440119743347 \n",
      "     Training Step: 113 Training Loss: 0.6168895959854126 \n",
      "     Training Step: 114 Training Loss: 0.615364670753479 \n",
      "     Training Step: 115 Training Loss: 0.6095172762870789 \n",
      "     Training Step: 116 Training Loss: 0.6155529618263245 \n",
      "     Training Step: 117 Training Loss: 0.6140975952148438 \n",
      "     Training Step: 118 Training Loss: 0.6118985414505005 \n",
      "     Training Step: 119 Training Loss: 0.616247296333313 \n",
      "     Training Step: 120 Training Loss: 0.6129810214042664 \n",
      "     Training Step: 121 Training Loss: 0.61468905210495 \n",
      "     Training Step: 122 Training Loss: 0.6154288053512573 \n",
      "     Training Step: 123 Training Loss: 0.610126793384552 \n",
      "     Training Step: 124 Training Loss: 0.6151010990142822 \n",
      "     Training Step: 125 Training Loss: 0.6127375960350037 \n",
      "     Training Step: 126 Training Loss: 0.6144496202468872 \n",
      "     Training Step: 127 Training Loss: 0.6180515885353088 \n",
      "     Training Step: 128 Training Loss: 0.6111759543418884 \n",
      "     Training Step: 129 Training Loss: 0.6166803240776062 \n",
      "     Training Step: 130 Training Loss: 0.6115649342536926 \n",
      "     Training Step: 131 Training Loss: 0.6113973259925842 \n",
      "     Training Step: 132 Training Loss: 0.6167592406272888 \n",
      "     Training Step: 133 Training Loss: 0.6168032884597778 \n",
      "     Training Step: 134 Training Loss: 0.6105137467384338 \n",
      "     Training Step: 135 Training Loss: 0.6123905777931213 \n",
      "     Training Step: 136 Training Loss: 0.6146255135536194 \n",
      "     Training Step: 137 Training Loss: 0.6140617728233337 \n",
      "     Training Step: 138 Training Loss: 0.6133071184158325 \n",
      "     Training Step: 139 Training Loss: 0.6131895184516907 \n",
      "     Training Step: 140 Training Loss: 0.6146572828292847 \n",
      "     Training Step: 141 Training Loss: 0.6171514987945557 \n",
      "     Training Step: 142 Training Loss: 0.6107825040817261 \n",
      "     Training Step: 143 Training Loss: 0.6180995106697083 \n",
      "     Training Step: 144 Training Loss: 0.6209295392036438 \n",
      "     Training Step: 145 Training Loss: 0.6156830787658691 \n",
      "     Training Step: 146 Training Loss: 0.6151393055915833 \n",
      "     Training Step: 147 Training Loss: 0.6123148798942566 \n",
      "     Training Step: 148 Training Loss: 0.6158257126808167 \n",
      "     Training Step: 149 Training Loss: 0.6110195517539978 \n",
      "     Training Step: 150 Training Loss: 0.6118402481079102 \n",
      "     Training Step: 151 Training Loss: 0.609232485294342 \n",
      "     Training Step: 152 Training Loss: 0.6140937805175781 \n",
      "     Training Step: 153 Training Loss: 0.6121846437454224 \n",
      "     Training Step: 154 Training Loss: 0.6181392073631287 \n",
      "     Training Step: 155 Training Loss: 0.6128922700881958 \n",
      "     Training Step: 156 Training Loss: 0.6108349561691284 \n",
      "     Training Step: 157 Training Loss: 0.6166660785675049 \n",
      "     Training Step: 158 Training Loss: 0.6114861965179443 \n",
      "     Training Step: 159 Training Loss: 0.6148369312286377 \n",
      "     Training Step: 160 Training Loss: 0.610599935054779 \n",
      "     Training Step: 161 Training Loss: 0.6133275628089905 \n",
      "     Training Step: 162 Training Loss: 0.6151496767997742 \n",
      "     Training Step: 163 Training Loss: 0.6121349930763245 \n",
      "     Training Step: 164 Training Loss: 0.6116635799407959 \n",
      "     Training Step: 165 Training Loss: 0.61250901222229 \n",
      "     Training Step: 166 Training Loss: 0.6163790225982666 \n",
      "     Training Step: 167 Training Loss: 0.6142960786819458 \n",
      "     Training Step: 168 Training Loss: 0.6153996586799622 \n",
      "     Training Step: 169 Training Loss: 0.6139311194419861 \n",
      "     Training Step: 170 Training Loss: 0.6122387051582336 \n",
      "     Training Step: 171 Training Loss: 0.6137723326683044 \n",
      "     Training Step: 172 Training Loss: 0.6164492964744568 \n",
      "     Training Step: 173 Training Loss: 0.6171656250953674 \n",
      "     Training Step: 174 Training Loss: 0.6120588779449463 \n",
      "     Training Step: 175 Training Loss: 0.6194259524345398 \n",
      "     Training Step: 176 Training Loss: 0.6124559044837952 \n",
      "     Training Step: 177 Training Loss: 0.6188480854034424 \n",
      "     Training Step: 178 Training Loss: 0.6176061034202576 \n",
      "     Training Step: 179 Training Loss: 0.616913914680481 \n",
      "     Training Step: 180 Training Loss: 0.6139670014381409 \n",
      "     Training Step: 181 Training Loss: 0.6154243350028992 \n",
      "     Training Step: 182 Training Loss: 0.615787923336029 \n",
      "     Training Step: 183 Training Loss: 0.614783763885498 \n",
      "     Training Step: 184 Training Loss: 0.6147537231445312 \n",
      "     Training Step: 185 Training Loss: 0.6136586666107178 \n",
      "     Training Step: 186 Training Loss: 0.6116543412208557 \n",
      "     Training Step: 187 Training Loss: 0.613739550113678 \n",
      "     Training Step: 188 Training Loss: 0.6114144325256348 \n",
      "     Training Step: 189 Training Loss: 0.6146882772445679 \n",
      "     Training Step: 190 Training Loss: 0.613457202911377 \n",
      "     Training Step: 191 Training Loss: 0.6128712892532349 \n",
      "     Training Step: 192 Training Loss: 0.6160361766815186 \n",
      "     Training Step: 193 Training Loss: 0.6127522587776184 \n",
      "     Training Step: 194 Training Loss: 0.6141712069511414 \n",
      "     Training Step: 195 Training Loss: 0.6102079153060913 \n",
      "     Training Step: 196 Training Loss: 0.6101312637329102 \n",
      "     Training Step: 197 Training Loss: 0.6185342073440552 \n",
      "     Training Step: 198 Training Loss: 0.615352988243103 \n",
      "     Training Step: 199 Training Loss: 0.6154043078422546 \n",
      "     Training Step: 200 Training Loss: 0.6130722165107727 \n",
      "     Training Step: 201 Training Loss: 0.6145241260528564 \n",
      "     Training Step: 202 Training Loss: 0.611595094203949 \n",
      "     Training Step: 203 Training Loss: 0.6198616623878479 \n",
      "     Training Step: 204 Training Loss: 0.6157577037811279 \n",
      "     Training Step: 205 Training Loss: 0.6196820139884949 \n",
      "     Training Step: 206 Training Loss: 0.6164112091064453 \n",
      "     Training Step: 207 Training Loss: 0.6133760213851929 \n",
      "     Training Step: 208 Training Loss: 0.6147278547286987 \n",
      "     Training Step: 209 Training Loss: 0.6172042489051819 \n",
      "     Training Step: 210 Training Loss: 0.6115731000900269 \n",
      "     Training Step: 211 Training Loss: 0.6202355027198792 \n",
      "     Training Step: 212 Training Loss: 0.6109131574630737 \n",
      "     Training Step: 213 Training Loss: 0.6177505850791931 \n",
      "     Training Step: 214 Training Loss: 0.6152885556221008 \n",
      "     Training Step: 215 Training Loss: 0.6140281558036804 \n",
      "     Training Step: 216 Training Loss: 0.6167589426040649 \n",
      "     Training Step: 217 Training Loss: 0.6186046004295349 \n",
      "     Training Step: 218 Training Loss: 0.6171592473983765 \n",
      "     Training Step: 219 Training Loss: 0.6184601783752441 \n",
      "     Training Step: 220 Training Loss: 0.610579788684845 \n",
      "     Training Step: 221 Training Loss: 0.6136109828948975 \n",
      "     Training Step: 222 Training Loss: 0.6165120005607605 \n",
      "     Training Step: 223 Training Loss: 0.6129628419876099 \n",
      "     Training Step: 224 Training Loss: 0.6115440726280212 \n",
      "     Training Step: 225 Training Loss: 0.6135104298591614 \n",
      "     Training Step: 226 Training Loss: 0.6136302947998047 \n",
      "     Training Step: 227 Training Loss: 0.6121764183044434 \n",
      "     Training Step: 228 Training Loss: 0.6100557446479797 \n",
      "     Training Step: 229 Training Loss: 0.6187124252319336 \n",
      "     Training Step: 230 Training Loss: 0.6168707013130188 \n",
      "     Training Step: 231 Training Loss: 0.614726185798645 \n",
      "     Training Step: 232 Training Loss: 0.6095420718193054 \n",
      "     Training Step: 233 Training Loss: 0.6178139448165894 \n",
      "     Training Step: 234 Training Loss: 0.6145262718200684 \n",
      "     Training Step: 235 Training Loss: 0.6137887239456177 \n",
      "     Training Step: 236 Training Loss: 0.6125723719596863 \n",
      "     Training Step: 237 Training Loss: 0.6118214130401611 \n",
      "     Training Step: 238 Training Loss: 0.6135189533233643 \n",
      "     Training Step: 239 Training Loss: 0.6145131587982178 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6106398105621338 \n",
      "     Validation Step: 1 Validation Loss: 0.6074954867362976 \n",
      "     Validation Step: 2 Validation Loss: 0.6105087399482727 \n",
      "     Validation Step: 3 Validation Loss: 0.6121519804000854 \n",
      "     Validation Step: 4 Validation Loss: 0.618497371673584 \n",
      "     Validation Step: 5 Validation Loss: 0.6153039932250977 \n",
      "     Validation Step: 6 Validation Loss: 0.6154196262359619 \n",
      "     Validation Step: 7 Validation Loss: 0.614940345287323 \n",
      "     Validation Step: 8 Validation Loss: 0.6112043857574463 \n",
      "     Validation Step: 9 Validation Loss: 0.6130295991897583 \n",
      "     Validation Step: 10 Validation Loss: 0.6159020662307739 \n",
      "     Validation Step: 11 Validation Loss: 0.6141405701637268 \n",
      "     Validation Step: 12 Validation Loss: 0.6116535663604736 \n",
      "     Validation Step: 13 Validation Loss: 0.617675244808197 \n",
      "     Validation Step: 14 Validation Loss: 0.6111993193626404 \n",
      "     Validation Step: 15 Validation Loss: 0.6128338575363159 \n",
      "     Validation Step: 16 Validation Loss: 0.617143452167511 \n",
      "     Validation Step: 17 Validation Loss: 0.6186270117759705 \n",
      "     Validation Step: 18 Validation Loss: 0.6185844540596008 \n",
      "     Validation Step: 19 Validation Loss: 0.6142640709877014 \n",
      "     Validation Step: 20 Validation Loss: 0.6145822405815125 \n",
      "     Validation Step: 21 Validation Loss: 0.6137557625770569 \n",
      "     Validation Step: 22 Validation Loss: 0.6118959784507751 \n",
      "     Validation Step: 23 Validation Loss: 0.6156973838806152 \n",
      "     Validation Step: 24 Validation Loss: 0.6147117018699646 \n",
      "     Validation Step: 25 Validation Loss: 0.6133304238319397 \n",
      "     Validation Step: 26 Validation Loss: 0.6178293824195862 \n",
      "     Validation Step: 27 Validation Loss: 0.6115843653678894 \n",
      "     Validation Step: 28 Validation Loss: 0.6181946992874146 \n",
      "     Validation Step: 29 Validation Loss: 0.6148983240127563 \n",
      "     Validation Step: 30 Validation Loss: 0.6157158017158508 \n",
      "     Validation Step: 31 Validation Loss: 0.6151627898216248 \n",
      "     Validation Step: 32 Validation Loss: 0.618400514125824 \n",
      "     Validation Step: 33 Validation Loss: 0.6146348714828491 \n",
      "     Validation Step: 34 Validation Loss: 0.6142939925193787 \n",
      "     Validation Step: 35 Validation Loss: 0.6105401515960693 \n",
      "     Validation Step: 36 Validation Loss: 0.6136423945426941 \n",
      "     Validation Step: 37 Validation Loss: 0.6173988580703735 \n",
      "     Validation Step: 38 Validation Loss: 0.6101460456848145 \n",
      "     Validation Step: 39 Validation Loss: 0.6161502599716187 \n",
      "     Validation Step: 40 Validation Loss: 0.6163586974143982 \n",
      "     Validation Step: 41 Validation Loss: 0.6141965985298157 \n",
      "     Validation Step: 42 Validation Loss: 0.6101508736610413 \n",
      "     Validation Step: 43 Validation Loss: 0.6101260185241699 \n",
      "     Validation Step: 44 Validation Loss: 0.6137636303901672 \n",
      "Epoch: 68\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6122683882713318 \n",
      "     Training Step: 1 Training Loss: 0.6167723536491394 \n",
      "     Training Step: 2 Training Loss: 0.6092455983161926 \n",
      "     Training Step: 3 Training Loss: 0.613323986530304 \n",
      "     Training Step: 4 Training Loss: 0.6153212785720825 \n",
      "     Training Step: 5 Training Loss: 0.6153697371482849 \n",
      "     Training Step: 6 Training Loss: 0.6185793280601501 \n",
      "     Training Step: 7 Training Loss: 0.6122937798500061 \n",
      "     Training Step: 8 Training Loss: 0.6125932335853577 \n",
      "     Training Step: 9 Training Loss: 0.6151915788650513 \n",
      "     Training Step: 10 Training Loss: 0.6177769303321838 \n",
      "     Training Step: 11 Training Loss: 0.6177399754524231 \n",
      "     Training Step: 12 Training Loss: 0.617143988609314 \n",
      "     Training Step: 13 Training Loss: 0.6168107390403748 \n",
      "     Training Step: 14 Training Loss: 0.6129587888717651 \n",
      "     Training Step: 15 Training Loss: 0.6146425604820251 \n",
      "     Training Step: 16 Training Loss: 0.6170921325683594 \n",
      "     Training Step: 17 Training Loss: 0.6127817034721375 \n",
      "     Training Step: 18 Training Loss: 0.6146885752677917 \n",
      "     Training Step: 19 Training Loss: 0.6104306578636169 \n",
      "     Training Step: 20 Training Loss: 0.6143691539764404 \n",
      "     Training Step: 21 Training Loss: 0.6189408302307129 \n",
      "     Training Step: 22 Training Loss: 0.6116524934768677 \n",
      "     Training Step: 23 Training Loss: 0.6140309572219849 \n",
      "     Training Step: 24 Training Loss: 0.611720860004425 \n",
      "     Training Step: 25 Training Loss: 0.6139445900917053 \n",
      "     Training Step: 26 Training Loss: 0.613214910030365 \n",
      "     Training Step: 27 Training Loss: 0.6168559193611145 \n",
      "     Training Step: 28 Training Loss: 0.6096840500831604 \n",
      "     Training Step: 29 Training Loss: 0.6181644797325134 \n",
      "     Training Step: 30 Training Loss: 0.6082610487937927 \n",
      "     Training Step: 31 Training Loss: 0.6140410900115967 \n",
      "     Training Step: 32 Training Loss: 0.6108729839324951 \n",
      "     Training Step: 33 Training Loss: 0.6131339073181152 \n",
      "     Training Step: 34 Training Loss: 0.6165382266044617 \n",
      "     Training Step: 35 Training Loss: 0.6142601370811462 \n",
      "     Training Step: 36 Training Loss: 0.6141631603240967 \n",
      "     Training Step: 37 Training Loss: 0.610601544380188 \n",
      "     Training Step: 38 Training Loss: 0.6157563328742981 \n",
      "     Training Step: 39 Training Loss: 0.6184852719306946 \n",
      "     Training Step: 40 Training Loss: 0.6143482327461243 \n",
      "     Training Step: 41 Training Loss: 0.6123929619789124 \n",
      "     Training Step: 42 Training Loss: 0.6176906228065491 \n",
      "     Training Step: 43 Training Loss: 0.6137552857398987 \n",
      "     Training Step: 44 Training Loss: 0.6173763871192932 \n",
      "     Training Step: 45 Training Loss: 0.6147541999816895 \n",
      "     Training Step: 46 Training Loss: 0.6100746989250183 \n",
      "     Training Step: 47 Training Loss: 0.6119101643562317 \n",
      "     Training Step: 48 Training Loss: 0.6141617298126221 \n",
      "     Training Step: 49 Training Loss: 0.6158269047737122 \n",
      "     Training Step: 50 Training Loss: 0.6167495250701904 \n",
      "     Training Step: 51 Training Loss: 0.6169098615646362 \n",
      "     Training Step: 52 Training Loss: 0.6144055128097534 \n",
      "     Training Step: 53 Training Loss: 0.6112419366836548 \n",
      "     Training Step: 54 Training Loss: 0.614971935749054 \n",
      "     Training Step: 55 Training Loss: 0.6114821434020996 \n",
      "     Training Step: 56 Training Loss: 0.6176882386207581 \n",
      "     Training Step: 57 Training Loss: 0.6138389706611633 \n",
      "     Training Step: 58 Training Loss: 0.6154019236564636 \n",
      "     Training Step: 59 Training Loss: 0.6122322678565979 \n",
      "     Training Step: 60 Training Loss: 0.616355299949646 \n",
      "     Training Step: 61 Training Loss: 0.6112354397773743 \n",
      "     Training Step: 62 Training Loss: 0.6158496737480164 \n",
      "     Training Step: 63 Training Loss: 0.6162058711051941 \n",
      "     Training Step: 64 Training Loss: 0.6125560402870178 \n",
      "     Training Step: 65 Training Loss: 0.6150894165039062 \n",
      "     Training Step: 66 Training Loss: 0.6154517531394958 \n",
      "     Training Step: 67 Training Loss: 0.6139286756515503 \n",
      "     Training Step: 68 Training Loss: 0.6167478561401367 \n",
      "     Training Step: 69 Training Loss: 0.6151540875434875 \n",
      "     Training Step: 70 Training Loss: 0.6146588921546936 \n",
      "     Training Step: 71 Training Loss: 0.618097186088562 \n",
      "     Training Step: 72 Training Loss: 0.6101592183113098 \n",
      "     Training Step: 73 Training Loss: 0.6171517968177795 \n",
      "     Training Step: 74 Training Loss: 0.6153050065040588 \n",
      "     Training Step: 75 Training Loss: 0.6121742725372314 \n",
      "     Training Step: 76 Training Loss: 0.6137451529502869 \n",
      "     Training Step: 77 Training Loss: 0.6199659109115601 \n",
      "     Training Step: 78 Training Loss: 0.6168133616447449 \n",
      "     Training Step: 79 Training Loss: 0.6097962856292725 \n",
      "     Training Step: 80 Training Loss: 0.6127926707267761 \n",
      "     Training Step: 81 Training Loss: 0.6121703386306763 \n",
      "     Training Step: 82 Training Loss: 0.6155487895011902 \n",
      "     Training Step: 83 Training Loss: 0.6114745736122131 \n",
      "     Training Step: 84 Training Loss: 0.6122933626174927 \n",
      "     Training Step: 85 Training Loss: 0.6103769540786743 \n",
      "     Training Step: 86 Training Loss: 0.6132121086120605 \n",
      "     Training Step: 87 Training Loss: 0.6163021326065063 \n",
      "     Training Step: 88 Training Loss: 0.618097186088562 \n",
      "     Training Step: 89 Training Loss: 0.6130174994468689 \n",
      "     Training Step: 90 Training Loss: 0.6127339005470276 \n",
      "     Training Step: 91 Training Loss: 0.610694169998169 \n",
      "     Training Step: 92 Training Loss: 0.6182558536529541 \n",
      "     Training Step: 93 Training Loss: 0.615123987197876 \n",
      "     Training Step: 94 Training Loss: 0.6147587895393372 \n",
      "     Training Step: 95 Training Loss: 0.6134657859802246 \n",
      "     Training Step: 96 Training Loss: 0.6133021116256714 \n",
      "     Training Step: 97 Training Loss: 0.613279402256012 \n",
      "     Training Step: 98 Training Loss: 0.6157411932945251 \n",
      "     Training Step: 99 Training Loss: 0.6124028563499451 \n",
      "     Training Step: 100 Training Loss: 0.6209667921066284 \n",
      "     Training Step: 101 Training Loss: 0.6114363670349121 \n",
      "     Training Step: 102 Training Loss: 0.6129347085952759 \n",
      "     Training Step: 103 Training Loss: 0.6127064228057861 \n",
      "     Training Step: 104 Training Loss: 0.6116068959236145 \n",
      "     Training Step: 105 Training Loss: 0.6155857443809509 \n",
      "     Training Step: 106 Training Loss: 0.6141155958175659 \n",
      "     Training Step: 107 Training Loss: 0.6121562719345093 \n",
      "     Training Step: 108 Training Loss: 0.6132047176361084 \n",
      "     Training Step: 109 Training Loss: 0.6167304515838623 \n",
      "     Training Step: 110 Training Loss: 0.6095334887504578 \n",
      "     Training Step: 111 Training Loss: 0.6178049445152283 \n",
      "     Training Step: 112 Training Loss: 0.6171693205833435 \n",
      "     Training Step: 113 Training Loss: 0.6166989803314209 \n",
      "     Training Step: 114 Training Loss: 0.6161234378814697 \n",
      "     Training Step: 115 Training Loss: 0.6147273778915405 \n",
      "     Training Step: 116 Training Loss: 0.615703821182251 \n",
      "     Training Step: 117 Training Loss: 0.6114842295646667 \n",
      "     Training Step: 118 Training Loss: 0.6160419583320618 \n",
      "     Training Step: 119 Training Loss: 0.6144577860832214 \n",
      "     Training Step: 120 Training Loss: 0.6154677271842957 \n",
      "     Training Step: 121 Training Loss: 0.612292468547821 \n",
      "     Training Step: 122 Training Loss: 0.6148017048835754 \n",
      "     Training Step: 123 Training Loss: 0.6145051121711731 \n",
      "     Training Step: 124 Training Loss: 0.6202446222305298 \n",
      "     Training Step: 125 Training Loss: 0.6157509684562683 \n",
      "     Training Step: 126 Training Loss: 0.6155040860176086 \n",
      "     Training Step: 127 Training Loss: 0.6144974827766418 \n",
      "     Training Step: 128 Training Loss: 0.6119116544723511 \n",
      "     Training Step: 129 Training Loss: 0.6115624308586121 \n",
      "     Training Step: 130 Training Loss: 0.6105896830558777 \n",
      "     Training Step: 131 Training Loss: 0.6137310266494751 \n",
      "     Training Step: 132 Training Loss: 0.6147300004959106 \n",
      "     Training Step: 133 Training Loss: 0.6146947741508484 \n",
      "     Training Step: 134 Training Loss: 0.6195099949836731 \n",
      "     Training Step: 135 Training Loss: 0.6186685562133789 \n",
      "     Training Step: 136 Training Loss: 0.6148071885108948 \n",
      "     Training Step: 137 Training Loss: 0.6154858469963074 \n",
      "     Training Step: 138 Training Loss: 0.61323082447052 \n",
      "     Training Step: 139 Training Loss: 0.6124333143234253 \n",
      "     Training Step: 140 Training Loss: 0.6150186061859131 \n",
      "     Training Step: 141 Training Loss: 0.6122256517410278 \n",
      "     Training Step: 142 Training Loss: 0.6161015033721924 \n",
      "     Training Step: 143 Training Loss: 0.6197152137756348 \n",
      "     Training Step: 144 Training Loss: 0.6167615652084351 \n",
      "     Training Step: 145 Training Loss: 0.6147713661193848 \n",
      "     Training Step: 146 Training Loss: 0.6154516339302063 \n",
      "     Training Step: 147 Training Loss: 0.6107925176620483 \n",
      "     Training Step: 148 Training Loss: 0.612521231174469 \n",
      "     Training Step: 149 Training Loss: 0.6161068677902222 \n",
      "     Training Step: 150 Training Loss: 0.61850506067276 \n",
      "     Training Step: 151 Training Loss: 0.6176842451095581 \n",
      "     Training Step: 152 Training Loss: 0.6118363738059998 \n",
      "     Training Step: 153 Training Loss: 0.6135241985321045 \n",
      "     Training Step: 154 Training Loss: 0.6153503060340881 \n",
      "     Training Step: 155 Training Loss: 0.614204466342926 \n",
      "     Training Step: 156 Training Loss: 0.6121416091918945 \n",
      "     Training Step: 157 Training Loss: 0.6106975078582764 \n",
      "     Training Step: 158 Training Loss: 0.612064003944397 \n",
      "     Training Step: 159 Training Loss: 0.6150970458984375 \n",
      "     Training Step: 160 Training Loss: 0.6155447363853455 \n",
      "     Training Step: 161 Training Loss: 0.6145246028900146 \n",
      "     Training Step: 162 Training Loss: 0.6118417382240295 \n",
      "     Training Step: 163 Training Loss: 0.6184478402137756 \n",
      "     Training Step: 164 Training Loss: 0.6139118671417236 \n",
      "     Training Step: 165 Training Loss: 0.6112932562828064 \n",
      "     Training Step: 166 Training Loss: 0.6128388047218323 \n",
      "     Training Step: 167 Training Loss: 0.6169864535331726 \n",
      "     Training Step: 168 Training Loss: 0.6136568784713745 \n",
      "     Training Step: 169 Training Loss: 0.6186310052871704 \n",
      "     Training Step: 170 Training Loss: 0.6133967638015747 \n",
      "     Training Step: 171 Training Loss: 0.611661434173584 \n",
      "     Training Step: 172 Training Loss: 0.6154463887214661 \n",
      "     Training Step: 173 Training Loss: 0.6153032183647156 \n",
      "     Training Step: 174 Training Loss: 0.6109795570373535 \n",
      "     Training Step: 175 Training Loss: 0.6135400533676147 \n",
      "     Training Step: 176 Training Loss: 0.6128984689712524 \n",
      "     Training Step: 177 Training Loss: 0.6107141971588135 \n",
      "     Training Step: 178 Training Loss: 0.6129113435745239 \n",
      "     Training Step: 179 Training Loss: 0.6093679666519165 \n",
      "     Training Step: 180 Training Loss: 0.6139609813690186 \n",
      "     Training Step: 181 Training Loss: 0.6101212501525879 \n",
      "     Training Step: 182 Training Loss: 0.6125261187553406 \n",
      "     Training Step: 183 Training Loss: 0.6143094301223755 \n",
      "     Training Step: 184 Training Loss: 0.61335688829422 \n",
      "     Training Step: 185 Training Loss: 0.6147358417510986 \n",
      "     Training Step: 186 Training Loss: 0.6132934093475342 \n",
      "     Training Step: 187 Training Loss: 0.6100817918777466 \n",
      "     Training Step: 188 Training Loss: 0.6116384267807007 \n",
      "     Training Step: 189 Training Loss: 0.6114129424095154 \n",
      "     Training Step: 190 Training Loss: 0.610489547252655 \n",
      "     Training Step: 191 Training Loss: 0.6147983074188232 \n",
      "     Training Step: 192 Training Loss: 0.6190061569213867 \n",
      "     Training Step: 193 Training Loss: 0.6142373085021973 \n",
      "     Training Step: 194 Training Loss: 0.6166506409645081 \n",
      "     Training Step: 195 Training Loss: 0.6148743629455566 \n",
      "     Training Step: 196 Training Loss: 0.6149716377258301 \n",
      "     Training Step: 197 Training Loss: 0.6146607398986816 \n",
      "     Training Step: 198 Training Loss: 0.6129269003868103 \n",
      "     Training Step: 199 Training Loss: 0.6116626262664795 \n",
      "     Training Step: 200 Training Loss: 0.6137250661849976 \n",
      "     Training Step: 201 Training Loss: 0.6168146133422852 \n",
      "     Training Step: 202 Training Loss: 0.616734504699707 \n",
      "     Training Step: 203 Training Loss: 0.6162723302841187 \n",
      "     Training Step: 204 Training Loss: 0.6156900525093079 \n",
      "     Training Step: 205 Training Loss: 0.6103113293647766 \n",
      "     Training Step: 206 Training Loss: 0.6176992058753967 \n",
      "     Training Step: 207 Training Loss: 0.6098730564117432 \n",
      "     Training Step: 208 Training Loss: 0.6105369329452515 \n",
      "     Training Step: 209 Training Loss: 0.6106969118118286 \n",
      "     Training Step: 210 Training Loss: 0.6137577891349792 \n",
      "     Training Step: 211 Training Loss: 0.615803599357605 \n",
      "     Training Step: 212 Training Loss: 0.6173771023750305 \n",
      "     Training Step: 213 Training Loss: 0.6144183874130249 \n",
      "     Training Step: 214 Training Loss: 0.6143509149551392 \n",
      "     Training Step: 215 Training Loss: 0.6120131611824036 \n",
      "     Training Step: 216 Training Loss: 0.610375165939331 \n",
      "     Training Step: 217 Training Loss: 0.6151103973388672 \n",
      "     Training Step: 218 Training Loss: 0.6144818067550659 \n",
      "     Training Step: 219 Training Loss: 0.617991030216217 \n",
      "     Training Step: 220 Training Loss: 0.6131240725517273 \n",
      "     Training Step: 221 Training Loss: 0.6115504503250122 \n",
      "     Training Step: 222 Training Loss: 0.6124740242958069 \n",
      "     Training Step: 223 Training Loss: 0.6197628378868103 \n",
      "     Training Step: 224 Training Loss: 0.6115359663963318 \n",
      "     Training Step: 225 Training Loss: 0.6135327219963074 \n",
      "     Training Step: 226 Training Loss: 0.6108009219169617 \n",
      "     Training Step: 227 Training Loss: 0.6130774617195129 \n",
      "     Training Step: 228 Training Loss: 0.6167306303977966 \n",
      "     Training Step: 229 Training Loss: 0.61179518699646 \n",
      "     Training Step: 230 Training Loss: 0.6181132197380066 \n",
      "     Training Step: 231 Training Loss: 0.6118685603141785 \n",
      "     Training Step: 232 Training Loss: 0.6116379499435425 \n",
      "     Training Step: 233 Training Loss: 0.6164160966873169 \n",
      "     Training Step: 234 Training Loss: 0.6115757822990417 \n",
      "     Training Step: 235 Training Loss: 0.6183688044548035 \n",
      "     Training Step: 236 Training Loss: 0.6156108379364014 \n",
      "     Training Step: 237 Training Loss: 0.6164312958717346 \n",
      "     Training Step: 238 Training Loss: 0.6134534478187561 \n",
      "     Training Step: 239 Training Loss: 0.6140233874320984 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6121547818183899 \n",
      "     Validation Step: 1 Validation Loss: 0.612997829914093 \n",
      "     Validation Step: 2 Validation Loss: 0.6142752170562744 \n",
      "     Validation Step: 3 Validation Loss: 0.6141369938850403 \n",
      "     Validation Step: 4 Validation Loss: 0.6156240105628967 \n",
      "     Validation Step: 5 Validation Loss: 0.6145843267440796 \n",
      "     Validation Step: 6 Validation Loss: 0.610542356967926 \n",
      "     Validation Step: 7 Validation Loss: 0.6176058650016785 \n",
      "     Validation Step: 8 Validation Loss: 0.6142168045043945 \n",
      "     Validation Step: 9 Validation Loss: 0.6184825897216797 \n",
      "     Validation Step: 10 Validation Loss: 0.612853467464447 \n",
      "     Validation Step: 11 Validation Loss: 0.6149094700813293 \n",
      "     Validation Step: 12 Validation Loss: 0.6182376742362976 \n",
      "     Validation Step: 13 Validation Loss: 0.6146485209465027 \n",
      "     Validation Step: 14 Validation Loss: 0.6105015277862549 \n",
      "     Validation Step: 15 Validation Loss: 0.6111839413642883 \n",
      "     Validation Step: 16 Validation Loss: 0.6155744791030884 \n",
      "     Validation Step: 17 Validation Loss: 0.6075778007507324 \n",
      "     Validation Step: 18 Validation Loss: 0.6136537194252014 \n",
      "     Validation Step: 19 Validation Loss: 0.610207200050354 \n",
      "     Validation Step: 20 Validation Loss: 0.6101781725883484 \n",
      "     Validation Step: 21 Validation Loss: 0.6153221726417542 \n",
      "     Validation Step: 22 Validation Loss: 0.6136614084243774 \n",
      "     Validation Step: 23 Validation Loss: 0.6176995038986206 \n",
      "     Validation Step: 24 Validation Loss: 0.6106729507446289 \n",
      "     Validation Step: 25 Validation Loss: 0.6111999750137329 \n",
      "     Validation Step: 26 Validation Loss: 0.6162486672401428 \n",
      "     Validation Step: 27 Validation Loss: 0.6150400042533875 \n",
      "     Validation Step: 28 Validation Loss: 0.6115772128105164 \n",
      "     Validation Step: 29 Validation Loss: 0.614136278629303 \n",
      "     Validation Step: 30 Validation Loss: 0.611912190914154 \n",
      "     Validation Step: 31 Validation Loss: 0.6184993386268616 \n",
      "     Validation Step: 32 Validation Loss: 0.6170216202735901 \n",
      "     Validation Step: 33 Validation Loss: 0.6180673241615295 \n",
      "     Validation Step: 34 Validation Loss: 0.6158062219619751 \n",
      "     Validation Step: 35 Validation Loss: 0.6133226752281189 \n",
      "     Validation Step: 36 Validation Loss: 0.6148521304130554 \n",
      "     Validation Step: 37 Validation Loss: 0.6145502328872681 \n",
      "     Validation Step: 38 Validation Loss: 0.6116692423820496 \n",
      "     Validation Step: 39 Validation Loss: 0.6152390241622925 \n",
      "     Validation Step: 40 Validation Loss: 0.6101447343826294 \n",
      "     Validation Step: 41 Validation Loss: 0.6159992218017578 \n",
      "     Validation Step: 42 Validation Loss: 0.6136763691902161 \n",
      "     Validation Step: 43 Validation Loss: 0.6173148155212402 \n",
      "     Validation Step: 44 Validation Loss: 0.6183444857597351 \n",
      "Epoch: 69\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6119014620780945 \n",
      "     Training Step: 1 Training Loss: 0.6146228313446045 \n",
      "     Training Step: 2 Training Loss: 0.6152892112731934 \n",
      "     Training Step: 3 Training Loss: 0.6188838481903076 \n",
      "     Training Step: 4 Training Loss: 0.6154319047927856 \n",
      "     Training Step: 5 Training Loss: 0.6180287599563599 \n",
      "     Training Step: 6 Training Loss: 0.617781400680542 \n",
      "     Training Step: 7 Training Loss: 0.6107369661331177 \n",
      "     Training Step: 8 Training Loss: 0.6162170767784119 \n",
      "     Training Step: 9 Training Loss: 0.6122192740440369 \n",
      "     Training Step: 10 Training Loss: 0.6146309971809387 \n",
      "     Training Step: 11 Training Loss: 0.6136822700500488 \n",
      "     Training Step: 12 Training Loss: 0.6178646683692932 \n",
      "     Training Step: 13 Training Loss: 0.6096888184547424 \n",
      "     Training Step: 14 Training Loss: 0.6147197484970093 \n",
      "     Training Step: 15 Training Loss: 0.6115102767944336 \n",
      "     Training Step: 16 Training Loss: 0.6160303354263306 \n",
      "     Training Step: 17 Training Loss: 0.6146766543388367 \n",
      "     Training Step: 18 Training Loss: 0.6101013422012329 \n",
      "     Training Step: 19 Training Loss: 0.6115257740020752 \n",
      "     Training Step: 20 Training Loss: 0.6133673191070557 \n",
      "     Training Step: 21 Training Loss: 0.6132145524024963 \n",
      "     Training Step: 22 Training Loss: 0.6129621863365173 \n",
      "     Training Step: 23 Training Loss: 0.6097140908241272 \n",
      "     Training Step: 24 Training Loss: 0.6147347688674927 \n",
      "     Training Step: 25 Training Loss: 0.6116440296173096 \n",
      "     Training Step: 26 Training Loss: 0.6163738369941711 \n",
      "     Training Step: 27 Training Loss: 0.6174938082695007 \n",
      "     Training Step: 28 Training Loss: 0.6182318329811096 \n",
      "     Training Step: 29 Training Loss: 0.6105269193649292 \n",
      "     Training Step: 30 Training Loss: 0.6131665110588074 \n",
      "     Training Step: 31 Training Loss: 0.61770099401474 \n",
      "     Training Step: 32 Training Loss: 0.6143908500671387 \n",
      "     Training Step: 33 Training Loss: 0.6154515147209167 \n",
      "     Training Step: 34 Training Loss: 0.6135479211807251 \n",
      "     Training Step: 35 Training Loss: 0.6106879711151123 \n",
      "     Training Step: 36 Training Loss: 0.614680290222168 \n",
      "     Training Step: 37 Training Loss: 0.6153011918067932 \n",
      "     Training Step: 38 Training Loss: 0.6114622354507446 \n",
      "     Training Step: 39 Training Loss: 0.6145012378692627 \n",
      "     Training Step: 40 Training Loss: 0.6186205744743347 \n",
      "     Training Step: 41 Training Loss: 0.6082978248596191 \n",
      "     Training Step: 42 Training Loss: 0.6132139563560486 \n",
      "     Training Step: 43 Training Loss: 0.6147487163543701 \n",
      "     Training Step: 44 Training Loss: 0.6149525046348572 \n",
      "     Training Step: 45 Training Loss: 0.6169867515563965 \n",
      "     Training Step: 46 Training Loss: 0.6105949282646179 \n",
      "     Training Step: 47 Training Loss: 0.6144281625747681 \n",
      "     Training Step: 48 Training Loss: 0.6146588325500488 \n",
      "     Training Step: 49 Training Loss: 0.6140776872634888 \n",
      "     Training Step: 50 Training Loss: 0.6140341758728027 \n",
      "     Training Step: 51 Training Loss: 0.6180164813995361 \n",
      "     Training Step: 52 Training Loss: 0.6092831492424011 \n",
      "     Training Step: 53 Training Loss: 0.6141217947006226 \n",
      "     Training Step: 54 Training Loss: 0.6127703189849854 \n",
      "     Training Step: 55 Training Loss: 0.6137756705284119 \n",
      "     Training Step: 56 Training Loss: 0.6195019483566284 \n",
      "     Training Step: 57 Training Loss: 0.6162493824958801 \n",
      "     Training Step: 58 Training Loss: 0.6133233308792114 \n",
      "     Training Step: 59 Training Loss: 0.6117519736289978 \n",
      "     Training Step: 60 Training Loss: 0.6125679016113281 \n",
      "     Training Step: 61 Training Loss: 0.6151067018508911 \n",
      "     Training Step: 62 Training Loss: 0.6152946352958679 \n",
      "     Training Step: 63 Training Loss: 0.6142507791519165 \n",
      "     Training Step: 64 Training Loss: 0.6136535406112671 \n",
      "     Training Step: 65 Training Loss: 0.6154054403305054 \n",
      "     Training Step: 66 Training Loss: 0.6152446866035461 \n",
      "     Training Step: 67 Training Loss: 0.6142967343330383 \n",
      "     Training Step: 68 Training Loss: 0.6164920330047607 \n",
      "     Training Step: 69 Training Loss: 0.6124311685562134 \n",
      "     Training Step: 70 Training Loss: 0.6122737526893616 \n",
      "     Training Step: 71 Training Loss: 0.6129652261734009 \n",
      "     Training Step: 72 Training Loss: 0.61362224817276 \n",
      "     Training Step: 73 Training Loss: 0.6139764189720154 \n",
      "     Training Step: 74 Training Loss: 0.6185873746871948 \n",
      "     Training Step: 75 Training Loss: 0.6100425124168396 \n",
      "     Training Step: 76 Training Loss: 0.6130645275115967 \n",
      "     Training Step: 77 Training Loss: 0.6123896241188049 \n",
      "     Training Step: 78 Training Loss: 0.6135010123252869 \n",
      "     Training Step: 79 Training Loss: 0.6121487021446228 \n",
      "     Training Step: 80 Training Loss: 0.6172005534172058 \n",
      "     Training Step: 81 Training Loss: 0.6125357151031494 \n",
      "     Training Step: 82 Training Loss: 0.6101892590522766 \n",
      "     Training Step: 83 Training Loss: 0.612070620059967 \n",
      "     Training Step: 84 Training Loss: 0.6107119917869568 \n",
      "     Training Step: 85 Training Loss: 0.6118499636650085 \n",
      "     Training Step: 86 Training Loss: 0.6149113178253174 \n",
      "     Training Step: 87 Training Loss: 0.6155727505683899 \n",
      "     Training Step: 88 Training Loss: 0.6137537956237793 \n",
      "     Training Step: 89 Training Loss: 0.6155387759208679 \n",
      "     Training Step: 90 Training Loss: 0.6172078847885132 \n",
      "     Training Step: 91 Training Loss: 0.6164368987083435 \n",
      "     Training Step: 92 Training Loss: 0.6155272126197815 \n",
      "     Training Step: 93 Training Loss: 0.6171379685401917 \n",
      "     Training Step: 94 Training Loss: 0.6095258593559265 \n",
      "     Training Step: 95 Training Loss: 0.611897885799408 \n",
      "     Training Step: 96 Training Loss: 0.6128276586532593 \n",
      "     Training Step: 97 Training Loss: 0.6142503619194031 \n",
      "     Training Step: 98 Training Loss: 0.6148235201835632 \n",
      "     Training Step: 99 Training Loss: 0.6168389320373535 \n",
      "     Training Step: 100 Training Loss: 0.6140766739845276 \n",
      "     Training Step: 101 Training Loss: 0.6150185465812683 \n",
      "     Training Step: 102 Training Loss: 0.6151636242866516 \n",
      "     Training Step: 103 Training Loss: 0.6124688386917114 \n",
      "     Training Step: 104 Training Loss: 0.6159668564796448 \n",
      "     Training Step: 105 Training Loss: 0.6169434785842896 \n",
      "     Training Step: 106 Training Loss: 0.613923192024231 \n",
      "     Training Step: 107 Training Loss: 0.6137927174568176 \n",
      "     Training Step: 108 Training Loss: 0.6197371482849121 \n",
      "     Training Step: 109 Training Loss: 0.6125574707984924 \n",
      "     Training Step: 110 Training Loss: 0.6119095087051392 \n",
      "     Training Step: 111 Training Loss: 0.6120787262916565 \n",
      "     Training Step: 112 Training Loss: 0.6182460188865662 \n",
      "     Training Step: 113 Training Loss: 0.6150930523872375 \n",
      "     Training Step: 114 Training Loss: 0.6115337014198303 \n",
      "     Training Step: 115 Training Loss: 0.6122952699661255 \n",
      "     Training Step: 116 Training Loss: 0.6115793585777283 \n",
      "     Training Step: 117 Training Loss: 0.6144840121269226 \n",
      "     Training Step: 118 Training Loss: 0.6117827296257019 \n",
      "     Training Step: 119 Training Loss: 0.612879753112793 \n",
      "     Training Step: 120 Training Loss: 0.6128484010696411 \n",
      "     Training Step: 121 Training Loss: 0.6133084297180176 \n",
      "     Training Step: 122 Training Loss: 0.6094748377799988 \n",
      "     Training Step: 123 Training Loss: 0.61224365234375 \n",
      "     Training Step: 124 Training Loss: 0.6154699325561523 \n",
      "     Training Step: 125 Training Loss: 0.6177797913551331 \n",
      "     Training Step: 126 Training Loss: 0.6108879446983337 \n",
      "     Training Step: 127 Training Loss: 0.6170983910560608 \n",
      "     Training Step: 128 Training Loss: 0.6168067455291748 \n",
      "     Training Step: 129 Training Loss: 0.6183831095695496 \n",
      "     Training Step: 130 Training Loss: 0.6115125417709351 \n",
      "     Training Step: 131 Training Loss: 0.6147999167442322 \n",
      "     Training Step: 132 Training Loss: 0.616650402545929 \n",
      "     Training Step: 133 Training Loss: 0.613946795463562 \n",
      "     Training Step: 134 Training Loss: 0.6111851930618286 \n",
      "     Training Step: 135 Training Loss: 0.6149528622627258 \n",
      "     Training Step: 136 Training Loss: 0.6184049844741821 \n",
      "     Training Step: 137 Training Loss: 0.6132088899612427 \n",
      "     Training Step: 138 Training Loss: 0.6156014800071716 \n",
      "     Training Step: 139 Training Loss: 0.6112532019615173 \n",
      "     Training Step: 140 Training Loss: 0.6158496737480164 \n",
      "     Training Step: 141 Training Loss: 0.6134787797927856 \n",
      "     Training Step: 142 Training Loss: 0.6103902459144592 \n",
      "     Training Step: 143 Training Loss: 0.6167306303977966 \n",
      "     Training Step: 144 Training Loss: 0.616869330406189 \n",
      "     Training Step: 145 Training Loss: 0.6199561953544617 \n",
      "     Training Step: 146 Training Loss: 0.6146173477172852 \n",
      "     Training Step: 147 Training Loss: 0.6108303070068359 \n",
      "     Training Step: 148 Training Loss: 0.6133713126182556 \n",
      "     Training Step: 149 Training Loss: 0.6185682415962219 \n",
      "     Training Step: 150 Training Loss: 0.6130768060684204 \n",
      "     Training Step: 151 Training Loss: 0.6166172027587891 \n",
      "     Training Step: 152 Training Loss: 0.6126635074615479 \n",
      "     Training Step: 153 Training Loss: 0.6122387051582336 \n",
      "     Training Step: 154 Training Loss: 0.6100219488143921 \n",
      "     Training Step: 155 Training Loss: 0.6167643070220947 \n",
      "     Training Step: 156 Training Loss: 0.6196831464767456 \n",
      "     Training Step: 157 Training Loss: 0.6129412055015564 \n",
      "     Training Step: 158 Training Loss: 0.6164273023605347 \n",
      "     Training Step: 159 Training Loss: 0.6153820157051086 \n",
      "     Training Step: 160 Training Loss: 0.6133868098258972 \n",
      "     Training Step: 161 Training Loss: 0.6116891503334045 \n",
      "     Training Step: 162 Training Loss: 0.614501953125 \n",
      "     Training Step: 163 Training Loss: 0.6167450547218323 \n",
      "     Training Step: 164 Training Loss: 0.6177768707275391 \n",
      "     Training Step: 165 Training Loss: 0.6141719818115234 \n",
      "     Training Step: 166 Training Loss: 0.6115682721138 \n",
      "     Training Step: 167 Training Loss: 0.6160262227058411 \n",
      "     Training Step: 168 Training Loss: 0.6168365478515625 \n",
      "     Training Step: 169 Training Loss: 0.6171484589576721 \n",
      "     Training Step: 170 Training Loss: 0.6117186546325684 \n",
      "     Training Step: 171 Training Loss: 0.6176093816757202 \n",
      "     Training Step: 172 Training Loss: 0.6143684387207031 \n",
      "     Training Step: 173 Training Loss: 0.6122000813484192 \n",
      "     Training Step: 174 Training Loss: 0.610576331615448 \n",
      "     Training Step: 175 Training Loss: 0.6167604923248291 \n",
      "     Training Step: 176 Training Loss: 0.6142765879631042 \n",
      "     Training Step: 177 Training Loss: 0.6166614294052124 \n",
      "     Training Step: 178 Training Loss: 0.614241898059845 \n",
      "     Training Step: 179 Training Loss: 0.6180543899536133 \n",
      "     Training Step: 180 Training Loss: 0.6125427484512329 \n",
      "     Training Step: 181 Training Loss: 0.6098427772521973 \n",
      "     Training Step: 182 Training Loss: 0.6106207370758057 \n",
      "     Training Step: 183 Training Loss: 0.6134483814239502 \n",
      "     Training Step: 184 Training Loss: 0.6175346374511719 \n",
      "     Training Step: 185 Training Loss: 0.6147316694259644 \n",
      "     Training Step: 186 Training Loss: 0.6101487278938293 \n",
      "     Training Step: 187 Training Loss: 0.6167623996734619 \n",
      "     Training Step: 188 Training Loss: 0.6118497252464294 \n",
      "     Training Step: 189 Training Loss: 0.6115116477012634 \n",
      "     Training Step: 190 Training Loss: 0.6127211451530457 \n",
      "     Training Step: 191 Training Loss: 0.6151886582374573 \n",
      "     Training Step: 192 Training Loss: 0.6178807020187378 \n",
      "     Training Step: 193 Training Loss: 0.6135185360908508 \n",
      "     Training Step: 194 Training Loss: 0.6133024096488953 \n",
      "     Training Step: 195 Training Loss: 0.6148014068603516 \n",
      "     Training Step: 196 Training Loss: 0.6158086657524109 \n",
      "     Training Step: 197 Training Loss: 0.6143718361854553 \n",
      "     Training Step: 198 Training Loss: 0.6116642355918884 \n",
      "     Training Step: 199 Training Loss: 0.6128922700881958 \n",
      "     Training Step: 200 Training Loss: 0.6181273460388184 \n",
      "     Training Step: 201 Training Loss: 0.6100812554359436 \n",
      "     Training Step: 202 Training Loss: 0.6155027747154236 \n",
      "     Training Step: 203 Training Loss: 0.616173267364502 \n",
      "     Training Step: 204 Training Loss: 0.6151646971702576 \n",
      "     Training Step: 205 Training Loss: 0.6111791133880615 \n",
      "     Training Step: 206 Training Loss: 0.6127784252166748 \n",
      "     Training Step: 207 Training Loss: 0.613830029964447 \n",
      "     Training Step: 208 Training Loss: 0.6153026223182678 \n",
      "     Training Step: 209 Training Loss: 0.6157670021057129 \n",
      "     Training Step: 210 Training Loss: 0.6140381097793579 \n",
      "     Training Step: 211 Training Loss: 0.6157501935958862 \n",
      "     Training Step: 212 Training Loss: 0.6106269359588623 \n",
      "     Training Step: 213 Training Loss: 0.611403226852417 \n",
      "     Training Step: 214 Training Loss: 0.6122963428497314 \n",
      "     Training Step: 215 Training Loss: 0.613157331943512 \n",
      "     Training Step: 216 Training Loss: 0.6157395243644714 \n",
      "     Training Step: 217 Training Loss: 0.6108863353729248 \n",
      "     Training Step: 218 Training Loss: 0.6124916672706604 \n",
      "     Training Step: 219 Training Loss: 0.611823558807373 \n",
      "     Training Step: 220 Training Loss: 0.6143748760223389 \n",
      "     Training Step: 221 Training Loss: 0.6114174723625183 \n",
      "     Training Step: 222 Training Loss: 0.6209931969642639 \n",
      "     Training Step: 223 Training Loss: 0.6162093877792358 \n",
      "     Training Step: 224 Training Loss: 0.6156670451164246 \n",
      "     Training Step: 225 Training Loss: 0.6184528470039368 \n",
      "     Training Step: 226 Training Loss: 0.6116751432418823 \n",
      "     Training Step: 227 Training Loss: 0.6154036521911621 \n",
      "     Training Step: 228 Training Loss: 0.6188644170761108 \n",
      "     Training Step: 229 Training Loss: 0.6157633662223816 \n",
      "     Training Step: 230 Training Loss: 0.6167295575141907 \n",
      "     Training Step: 231 Training Loss: 0.6105697751045227 \n",
      "     Training Step: 232 Training Loss: 0.6146603226661682 \n",
      "     Training Step: 233 Training Loss: 0.6121804714202881 \n",
      "     Training Step: 234 Training Loss: 0.6144689321517944 \n",
      "     Training Step: 235 Training Loss: 0.6203010082244873 \n",
      "     Training Step: 236 Training Loss: 0.6146945357322693 \n",
      "     Training Step: 237 Training Loss: 0.6154223680496216 \n",
      "     Training Step: 238 Training Loss: 0.6137133836746216 \n",
      "     Training Step: 239 Training Loss: 0.6108283400535583 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6133562326431274 \n",
      "     Validation Step: 1 Validation Loss: 0.6176308393478394 \n",
      "     Validation Step: 2 Validation Loss: 0.6152506470680237 \n",
      "     Validation Step: 3 Validation Loss: 0.6136999726295471 \n",
      "     Validation Step: 4 Validation Loss: 0.6177088022232056 \n",
      "     Validation Step: 5 Validation Loss: 0.6141607761383057 \n",
      "     Validation Step: 6 Validation Loss: 0.6116083264350891 \n",
      "     Validation Step: 7 Validation Loss: 0.6182429790496826 \n",
      "     Validation Step: 8 Validation Loss: 0.6136660575866699 \n",
      "     Validation Step: 9 Validation Loss: 0.616262674331665 \n",
      "     Validation Step: 10 Validation Loss: 0.6145773530006409 \n",
      "     Validation Step: 11 Validation Loss: 0.6121822595596313 \n",
      "     Validation Step: 12 Validation Loss: 0.6185019016265869 \n",
      "     Validation Step: 13 Validation Loss: 0.6136972904205322 \n",
      "     Validation Step: 14 Validation Loss: 0.6119464039802551 \n",
      "     Validation Step: 15 Validation Loss: 0.61057049036026 \n",
      "     Validation Step: 16 Validation Loss: 0.6101707220077515 \n",
      "     Validation Step: 17 Validation Loss: 0.6153345704078674 \n",
      "     Validation Step: 18 Validation Loss: 0.610244631767273 \n",
      "     Validation Step: 19 Validation Loss: 0.6173292994499207 \n",
      "     Validation Step: 20 Validation Loss: 0.6117001175880432 \n",
      "     Validation Step: 21 Validation Loss: 0.6158285737037659 \n",
      "     Validation Step: 22 Validation Loss: 0.6142369508743286 \n",
      "     Validation Step: 23 Validation Loss: 0.6150503754615784 \n",
      "     Validation Step: 24 Validation Loss: 0.6112089157104492 \n",
      "     Validation Step: 25 Validation Loss: 0.6076198220252991 \n",
      "     Validation Step: 26 Validation Loss: 0.6185118556022644 \n",
      "     Validation Step: 27 Validation Loss: 0.6149405837059021 \n",
      "     Validation Step: 28 Validation Loss: 0.6130319237709045 \n",
      "     Validation Step: 29 Validation Loss: 0.614669919013977 \n",
      "     Validation Step: 30 Validation Loss: 0.6106982231140137 \n",
      "     Validation Step: 31 Validation Loss: 0.617035984992981 \n",
      "     Validation Step: 32 Validation Loss: 0.610203206539154 \n",
      "     Validation Step: 33 Validation Loss: 0.614876925945282 \n",
      "     Validation Step: 34 Validation Loss: 0.6145981550216675 \n",
      "     Validation Step: 35 Validation Loss: 0.6141698360443115 \n",
      "     Validation Step: 36 Validation Loss: 0.615587055683136 \n",
      "     Validation Step: 37 Validation Loss: 0.6180744767189026 \n",
      "     Validation Step: 38 Validation Loss: 0.6112294793128967 \n",
      "     Validation Step: 39 Validation Loss: 0.6156313419342041 \n",
      "     Validation Step: 40 Validation Loss: 0.6143015623092651 \n",
      "     Validation Step: 41 Validation Loss: 0.6160117983818054 \n",
      "     Validation Step: 42 Validation Loss: 0.6105390787124634 \n",
      "     Validation Step: 43 Validation Loss: 0.6128968000411987 \n",
      "     Validation Step: 44 Validation Loss: 0.6183494329452515 \n",
      "Epoch: 70\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6167382597923279 \n",
      "     Training Step: 1 Training Loss: 0.6195061206817627 \n",
      "     Training Step: 2 Training Loss: 0.6139430999755859 \n",
      "     Training Step: 3 Training Loss: 0.616730272769928 \n",
      "     Training Step: 4 Training Loss: 0.6177193522453308 \n",
      "     Training Step: 5 Training Loss: 0.6112571358680725 \n",
      "     Training Step: 6 Training Loss: 0.6152546405792236 \n",
      "     Training Step: 7 Training Loss: 0.6115973591804504 \n",
      "     Training Step: 8 Training Loss: 0.6157737970352173 \n",
      "     Training Step: 9 Training Loss: 0.6147106289863586 \n",
      "     Training Step: 10 Training Loss: 0.6121636629104614 \n",
      "     Training Step: 11 Training Loss: 0.6172449588775635 \n",
      "     Training Step: 12 Training Loss: 0.6140390038490295 \n",
      "     Training Step: 13 Training Loss: 0.6123216152191162 \n",
      "     Training Step: 14 Training Loss: 0.6124091744422913 \n",
      "     Training Step: 15 Training Loss: 0.6167807579040527 \n",
      "     Training Step: 16 Training Loss: 0.6138968467712402 \n",
      "     Training Step: 17 Training Loss: 0.6146240830421448 \n",
      "     Training Step: 18 Training Loss: 0.6136070489883423 \n",
      "     Training Step: 19 Training Loss: 0.6128188371658325 \n",
      "     Training Step: 20 Training Loss: 0.6122915148735046 \n",
      "     Training Step: 21 Training Loss: 0.6116943359375 \n",
      "     Training Step: 22 Training Loss: 0.6165508031845093 \n",
      "     Training Step: 23 Training Loss: 0.6186729073524475 \n",
      "     Training Step: 24 Training Loss: 0.6154319643974304 \n",
      "     Training Step: 25 Training Loss: 0.6098340153694153 \n",
      "     Training Step: 26 Training Loss: 0.6114969253540039 \n",
      "     Training Step: 27 Training Loss: 0.613768994808197 \n",
      "     Training Step: 28 Training Loss: 0.6154636144638062 \n",
      "     Training Step: 29 Training Loss: 0.6160612106323242 \n",
      "     Training Step: 30 Training Loss: 0.6144554615020752 \n",
      "     Training Step: 31 Training Loss: 0.6157799959182739 \n",
      "     Training Step: 32 Training Loss: 0.6171390414237976 \n",
      "     Training Step: 33 Training Loss: 0.6129282712936401 \n",
      "     Training Step: 34 Training Loss: 0.6121982336044312 \n",
      "     Training Step: 35 Training Loss: 0.6178014278411865 \n",
      "     Training Step: 36 Training Loss: 0.61402827501297 \n",
      "     Training Step: 37 Training Loss: 0.6131502389907837 \n",
      "     Training Step: 38 Training Loss: 0.6120728850364685 \n",
      "     Training Step: 39 Training Loss: 0.6143752336502075 \n",
      "     Training Step: 40 Training Loss: 0.611443042755127 \n",
      "     Training Step: 41 Training Loss: 0.6177902221679688 \n",
      "     Training Step: 42 Training Loss: 0.6137499809265137 \n",
      "     Training Step: 43 Training Loss: 0.6143936514854431 \n",
      "     Training Step: 44 Training Loss: 0.6146352887153625 \n",
      "     Training Step: 45 Training Loss: 0.6133291721343994 \n",
      "     Training Step: 46 Training Loss: 0.6136618852615356 \n",
      "     Training Step: 47 Training Loss: 0.6109334826469421 \n",
      "     Training Step: 48 Training Loss: 0.6153232455253601 \n",
      "     Training Step: 49 Training Loss: 0.6197645664215088 \n",
      "     Training Step: 50 Training Loss: 0.6129316687583923 \n",
      "     Training Step: 51 Training Loss: 0.6104329824447632 \n",
      "     Training Step: 52 Training Loss: 0.6153731942176819 \n",
      "     Training Step: 53 Training Loss: 0.6150988340377808 \n",
      "     Training Step: 54 Training Loss: 0.6181042194366455 \n",
      "     Training Step: 55 Training Loss: 0.6167165040969849 \n",
      "     Training Step: 56 Training Loss: 0.613446831703186 \n",
      "     Training Step: 57 Training Loss: 0.6147101521492004 \n",
      "     Training Step: 58 Training Loss: 0.612856388092041 \n",
      "     Training Step: 59 Training Loss: 0.6106078624725342 \n",
      "     Training Step: 60 Training Loss: 0.6136670708656311 \n",
      "     Training Step: 61 Training Loss: 0.6099818348884583 \n",
      "     Training Step: 62 Training Loss: 0.6125533580780029 \n",
      "     Training Step: 63 Training Loss: 0.6156835556030273 \n",
      "     Training Step: 64 Training Loss: 0.6118441224098206 \n",
      "     Training Step: 65 Training Loss: 0.616740882396698 \n",
      "     Training Step: 66 Training Loss: 0.6199028491973877 \n",
      "     Training Step: 67 Training Loss: 0.6169256567955017 \n",
      "     Training Step: 68 Training Loss: 0.6171557903289795 \n",
      "     Training Step: 69 Training Loss: 0.6148658394813538 \n",
      "     Training Step: 70 Training Loss: 0.6182766556739807 \n",
      "     Training Step: 71 Training Loss: 0.6167072057723999 \n",
      "     Training Step: 72 Training Loss: 0.6118484139442444 \n",
      "     Training Step: 73 Training Loss: 0.6154972314834595 \n",
      "     Training Step: 74 Training Loss: 0.6139949560165405 \n",
      "     Training Step: 75 Training Loss: 0.6148248314857483 \n",
      "     Training Step: 76 Training Loss: 0.6181495189666748 \n",
      "     Training Step: 77 Training Loss: 0.6128502488136292 \n",
      "     Training Step: 78 Training Loss: 0.6119217872619629 \n",
      "     Training Step: 79 Training Loss: 0.615578830242157 \n",
      "     Training Step: 80 Training Loss: 0.6137633323669434 \n",
      "     Training Step: 81 Training Loss: 0.6174164414405823 \n",
      "     Training Step: 82 Training Loss: 0.6182645559310913 \n",
      "     Training Step: 83 Training Loss: 0.6155669093132019 \n",
      "     Training Step: 84 Training Loss: 0.6177042722702026 \n",
      "     Training Step: 85 Training Loss: 0.611551821231842 \n",
      "     Training Step: 86 Training Loss: 0.6124510765075684 \n",
      "     Training Step: 87 Training Loss: 0.6106170415878296 \n",
      "     Training Step: 88 Training Loss: 0.6142828464508057 \n",
      "     Training Step: 89 Training Loss: 0.6164536476135254 \n",
      "     Training Step: 90 Training Loss: 0.6148492097854614 \n",
      "     Training Step: 91 Training Loss: 0.6111583709716797 \n",
      "     Training Step: 92 Training Loss: 0.616296648979187 \n",
      "     Training Step: 93 Training Loss: 0.6144967675209045 \n",
      "     Training Step: 94 Training Loss: 0.6178171634674072 \n",
      "     Training Step: 95 Training Loss: 0.6084470152854919 \n",
      "     Training Step: 96 Training Loss: 0.6131638288497925 \n",
      "     Training Step: 97 Training Loss: 0.610733687877655 \n",
      "     Training Step: 98 Training Loss: 0.6154189109802246 \n",
      "     Training Step: 99 Training Loss: 0.6176444888114929 \n",
      "     Training Step: 100 Training Loss: 0.6097338795661926 \n",
      "     Training Step: 101 Training Loss: 0.6132937669754028 \n",
      "     Training Step: 102 Training Loss: 0.6125110983848572 \n",
      "     Training Step: 103 Training Loss: 0.6152157187461853 \n",
      "     Training Step: 104 Training Loss: 0.6114667654037476 \n",
      "     Training Step: 105 Training Loss: 0.6151347160339355 \n",
      "     Training Step: 106 Training Loss: 0.6106179356575012 \n",
      "     Training Step: 107 Training Loss: 0.6118261218070984 \n",
      "     Training Step: 108 Training Loss: 0.6185957193374634 \n",
      "     Training Step: 109 Training Loss: 0.6181563138961792 \n",
      "     Training Step: 110 Training Loss: 0.6154507398605347 \n",
      "     Training Step: 111 Training Loss: 0.6156984567642212 \n",
      "     Training Step: 112 Training Loss: 0.6153566837310791 \n",
      "     Training Step: 113 Training Loss: 0.6139531135559082 \n",
      "     Training Step: 114 Training Loss: 0.6109906435012817 \n",
      "     Training Step: 115 Training Loss: 0.6118374466896057 \n",
      "     Training Step: 116 Training Loss: 0.6135905981063843 \n",
      "     Training Step: 117 Training Loss: 0.6117273569107056 \n",
      "     Training Step: 118 Training Loss: 0.6173587441444397 \n",
      "     Training Step: 119 Training Loss: 0.6094037294387817 \n",
      "     Training Step: 120 Training Loss: 0.6153367757797241 \n",
      "     Training Step: 121 Training Loss: 0.6114440560340881 \n",
      "     Training Step: 122 Training Loss: 0.6167351007461548 \n",
      "     Training Step: 123 Training Loss: 0.6162409782409668 \n",
      "     Training Step: 124 Training Loss: 0.6160488724708557 \n",
      "     Training Step: 125 Training Loss: 0.6115801334381104 \n",
      "     Training Step: 126 Training Loss: 0.6133684515953064 \n",
      "     Training Step: 127 Training Loss: 0.6141989827156067 \n",
      "     Training Step: 128 Training Loss: 0.6142139434814453 \n",
      "     Training Step: 129 Training Loss: 0.615447461605072 \n",
      "     Training Step: 130 Training Loss: 0.6125364899635315 \n",
      "     Training Step: 131 Training Loss: 0.61507248878479 \n",
      "     Training Step: 132 Training Loss: 0.6161256432533264 \n",
      "     Training Step: 133 Training Loss: 0.6182363033294678 \n",
      "     Training Step: 134 Training Loss: 0.611957311630249 \n",
      "     Training Step: 135 Training Loss: 0.613038182258606 \n",
      "     Training Step: 136 Training Loss: 0.6171642541885376 \n",
      "     Training Step: 137 Training Loss: 0.6141111850738525 \n",
      "     Training Step: 138 Training Loss: 0.6155083775520325 \n",
      "     Training Step: 139 Training Loss: 0.6149727702140808 \n",
      "     Training Step: 140 Training Loss: 0.6100764870643616 \n",
      "     Training Step: 141 Training Loss: 0.6125025153160095 \n",
      "     Training Step: 142 Training Loss: 0.6121606826782227 \n",
      "     Training Step: 143 Training Loss: 0.6134992241859436 \n",
      "     Training Step: 144 Training Loss: 0.6129447221755981 \n",
      "     Training Step: 145 Training Loss: 0.6149364113807678 \n",
      "     Training Step: 146 Training Loss: 0.6157609224319458 \n",
      "     Training Step: 147 Training Loss: 0.6184351444244385 \n",
      "     Training Step: 148 Training Loss: 0.6153075098991394 \n",
      "     Training Step: 149 Training Loss: 0.6136809587478638 \n",
      "     Training Step: 150 Training Loss: 0.6159524917602539 \n",
      "     Training Step: 151 Training Loss: 0.6196812391281128 \n",
      "     Training Step: 152 Training Loss: 0.6102191209793091 \n",
      "     Training Step: 153 Training Loss: 0.6177574396133423 \n",
      "     Training Step: 154 Training Loss: 0.6130785942077637 \n",
      "     Training Step: 155 Training Loss: 0.613309919834137 \n",
      "     Training Step: 156 Training Loss: 0.61436927318573 \n",
      "     Training Step: 157 Training Loss: 0.6202768683433533 \n",
      "     Training Step: 158 Training Loss: 0.611925482749939 \n",
      "     Training Step: 159 Training Loss: 0.6104445457458496 \n",
      "     Training Step: 160 Training Loss: 0.6115732192993164 \n",
      "     Training Step: 161 Training Loss: 0.611178457736969 \n",
      "     Training Step: 162 Training Loss: 0.612704873085022 \n",
      "     Training Step: 163 Training Loss: 0.6114742755889893 \n",
      "     Training Step: 164 Training Loss: 0.6116610765457153 \n",
      "     Training Step: 165 Training Loss: 0.617843508720398 \n",
      "     Training Step: 166 Training Loss: 0.6167231202125549 \n",
      "     Training Step: 167 Training Loss: 0.6146957874298096 \n",
      "     Training Step: 168 Training Loss: 0.6126160025596619 \n",
      "     Training Step: 169 Training Loss: 0.6147732138633728 \n",
      "     Training Step: 170 Training Loss: 0.6157963871955872 \n",
      "     Training Step: 171 Training Loss: 0.6097700595855713 \n",
      "     Training Step: 172 Training Loss: 0.613479495048523 \n",
      "     Training Step: 173 Training Loss: 0.6148133873939514 \n",
      "     Training Step: 174 Training Loss: 0.6132762432098389 \n",
      "     Training Step: 175 Training Loss: 0.6145485043525696 \n",
      "     Training Step: 176 Training Loss: 0.6106692552566528 \n",
      "     Training Step: 177 Training Loss: 0.6188978552818298 \n",
      "     Training Step: 178 Training Loss: 0.6149057745933533 \n",
      "     Training Step: 179 Training Loss: 0.616237998008728 \n",
      "     Training Step: 180 Training Loss: 0.6168149709701538 \n",
      "     Training Step: 181 Training Loss: 0.6122806668281555 \n",
      "     Training Step: 182 Training Loss: 0.6116902232170105 \n",
      "     Training Step: 183 Training Loss: 0.618636429309845 \n",
      "     Training Step: 184 Training Loss: 0.6105373501777649 \n",
      "     Training Step: 185 Training Loss: 0.6120731830596924 \n",
      "     Training Step: 186 Training Loss: 0.6144639253616333 \n",
      "     Training Step: 187 Training Loss: 0.6149745583534241 \n",
      "     Training Step: 188 Training Loss: 0.6104974150657654 \n",
      "     Training Step: 189 Training Loss: 0.6122831106185913 \n",
      "     Training Step: 190 Training Loss: 0.6144207119941711 \n",
      "     Training Step: 191 Training Loss: 0.6168633103370667 \n",
      "     Training Step: 192 Training Loss: 0.6115520596504211 \n",
      "     Training Step: 193 Training Loss: 0.618506133556366 \n",
      "     Training Step: 194 Training Loss: 0.6130831241607666 \n",
      "     Training Step: 195 Training Loss: 0.6140915155410767 \n",
      "     Training Step: 196 Training Loss: 0.6126803755760193 \n",
      "     Training Step: 197 Training Loss: 0.6127877831459045 \n",
      "     Training Step: 198 Training Loss: 0.610076367855072 \n",
      "     Training Step: 199 Training Loss: 0.6164758801460266 \n",
      "     Training Step: 200 Training Loss: 0.6164547801017761 \n",
      "     Training Step: 201 Training Loss: 0.6132121682167053 \n",
      "     Training Step: 202 Training Loss: 0.6128875613212585 \n",
      "     Training Step: 203 Training Loss: 0.6107669472694397 \n",
      "     Training Step: 204 Training Loss: 0.6169100999832153 \n",
      "     Training Step: 205 Training Loss: 0.6142218708992004 \n",
      "     Training Step: 206 Training Loss: 0.6157006621360779 \n",
      "     Training Step: 207 Training Loss: 0.6188682317733765 \n",
      "     Training Step: 208 Training Loss: 0.6135736107826233 \n",
      "     Training Step: 209 Training Loss: 0.6146737337112427 \n",
      "     Training Step: 210 Training Loss: 0.614687442779541 \n",
      "     Training Step: 211 Training Loss: 0.6122599840164185 \n",
      "     Training Step: 212 Training Loss: 0.6094614863395691 \n",
      "     Training Step: 213 Training Loss: 0.6167328953742981 \n",
      "     Training Step: 214 Training Loss: 0.6092062592506409 \n",
      "     Training Step: 215 Training Loss: 0.6116528511047363 \n",
      "     Training Step: 216 Training Loss: 0.6133435964584351 \n",
      "     Training Step: 217 Training Loss: 0.6146477460861206 \n",
      "     Training Step: 218 Training Loss: 0.6121604442596436 \n",
      "     Training Step: 219 Training Loss: 0.6124011874198914 \n",
      "     Training Step: 220 Training Loss: 0.6142392158508301 \n",
      "     Training Step: 221 Training Loss: 0.6133222579956055 \n",
      "     Training Step: 222 Training Loss: 0.6141217350959778 \n",
      "     Training Step: 223 Training Loss: 0.6151713132858276 \n",
      "     Training Step: 224 Training Loss: 0.6146698594093323 \n",
      "     Training Step: 225 Training Loss: 0.6115854978561401 \n",
      "     Training Step: 226 Training Loss: 0.6144683957099915 \n",
      "     Training Step: 227 Training Loss: 0.6167019009590149 \n",
      "     Training Step: 228 Training Loss: 0.6106986999511719 \n",
      "     Training Step: 229 Training Loss: 0.6142832636833191 \n",
      "     Training Step: 230 Training Loss: 0.6105987429618835 \n",
      "     Training Step: 231 Training Loss: 0.6101953983306885 \n",
      "     Training Step: 232 Training Loss: 0.6131885051727295 \n",
      "     Training Step: 233 Training Loss: 0.6100478172302246 \n",
      "     Training Step: 234 Training Loss: 0.6156319975852966 \n",
      "     Training Step: 235 Training Loss: 0.6168861985206604 \n",
      "     Training Step: 236 Training Loss: 0.6180768013000488 \n",
      "     Training Step: 237 Training Loss: 0.615876317024231 \n",
      "     Training Step: 238 Training Loss: 0.6147480010986328 \n",
      "     Training Step: 239 Training Loss: 0.6209017038345337 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6173192858695984 \n",
      "     Validation Step: 1 Validation Loss: 0.6170274615287781 \n",
      "     Validation Step: 2 Validation Loss: 0.6149921417236328 \n",
      "     Validation Step: 3 Validation Loss: 0.6103885173797607 \n",
      "     Validation Step: 4 Validation Loss: 0.6162814497947693 \n",
      "     Validation Step: 5 Validation Loss: 0.6182183623313904 \n",
      "     Validation Step: 6 Validation Loss: 0.6146368980407715 \n",
      "     Validation Step: 7 Validation Loss: 0.6129871010780334 \n",
      "     Validation Step: 8 Validation Loss: 0.6103419065475464 \n",
      "     Validation Step: 9 Validation Loss: 0.6142393946647644 \n",
      "     Validation Step: 10 Validation Loss: 0.6113263964653015 \n",
      "     Validation Step: 11 Validation Loss: 0.6122925877571106 \n",
      "     Validation Step: 12 Validation Loss: 0.6137550473213196 \n",
      "     Validation Step: 13 Validation Loss: 0.6106643676757812 \n",
      "     Validation Step: 14 Validation Loss: 0.6134419441223145 \n",
      "     Validation Step: 15 Validation Loss: 0.6117219924926758 \n",
      "     Validation Step: 16 Validation Loss: 0.6113364100456238 \n",
      "     Validation Step: 17 Validation Loss: 0.618334949016571 \n",
      "     Validation Step: 18 Validation Loss: 0.61583012342453 \n",
      "     Validation Step: 19 Validation Loss: 0.6103816032409668 \n",
      "     Validation Step: 20 Validation Loss: 0.6153746247291565 \n",
      "     Validation Step: 21 Validation Loss: 0.6142429709434509 \n",
      "     Validation Step: 22 Validation Loss: 0.6160009503364563 \n",
      "     Validation Step: 23 Validation Loss: 0.6118097901344299 \n",
      "     Validation Step: 24 Validation Loss: 0.6147308945655823 \n",
      "     Validation Step: 25 Validation Loss: 0.610832154750824 \n",
      "     Validation Step: 26 Validation Loss: 0.6137658357620239 \n",
      "     Validation Step: 27 Validation Loss: 0.6156575083732605 \n",
      "     Validation Step: 28 Validation Loss: 0.6180646419525146 \n",
      "     Validation Step: 29 Validation Loss: 0.6131113767623901 \n",
      "     Validation Step: 30 Validation Loss: 0.6143055558204651 \n",
      "     Validation Step: 31 Validation Loss: 0.6150904893875122 \n",
      "     Validation Step: 32 Validation Loss: 0.6184812784194946 \n",
      "     Validation Step: 33 Validation Loss: 0.6137104034423828 \n",
      "     Validation Step: 34 Validation Loss: 0.6176358461380005 \n",
      "     Validation Step: 35 Validation Loss: 0.6156049370765686 \n",
      "     Validation Step: 36 Validation Loss: 0.6152896881103516 \n",
      "     Validation Step: 37 Validation Loss: 0.6143818497657776 \n",
      "     Validation Step: 38 Validation Loss: 0.618480384349823 \n",
      "     Validation Step: 39 Validation Loss: 0.6106916069984436 \n",
      "     Validation Step: 40 Validation Loss: 0.6078309416770935 \n",
      "     Validation Step: 41 Validation Loss: 0.6120491027832031 \n",
      "     Validation Step: 42 Validation Loss: 0.614922821521759 \n",
      "     Validation Step: 43 Validation Loss: 0.6176919937133789 \n",
      "     Validation Step: 44 Validation Loss: 0.6146493554115295 \n",
      "Epoch: 71\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6156447529792786 \n",
      "     Training Step: 1 Training Loss: 0.6132750511169434 \n",
      "     Training Step: 2 Training Loss: 0.6129428148269653 \n",
      "     Training Step: 3 Training Loss: 0.6177220940589905 \n",
      "     Training Step: 4 Training Loss: 0.6133406758308411 \n",
      "     Training Step: 5 Training Loss: 0.6149649620056152 \n",
      "     Training Step: 6 Training Loss: 0.6138020157814026 \n",
      "     Training Step: 7 Training Loss: 0.6147937178611755 \n",
      "     Training Step: 8 Training Loss: 0.616387665271759 \n",
      "     Training Step: 9 Training Loss: 0.6141877770423889 \n",
      "     Training Step: 10 Training Loss: 0.6098167300224304 \n",
      "     Training Step: 11 Training Loss: 0.6155307292938232 \n",
      "     Training Step: 12 Training Loss: 0.6105108261108398 \n",
      "     Training Step: 13 Training Loss: 0.6178934574127197 \n",
      "     Training Step: 14 Training Loss: 0.6158480048179626 \n",
      "     Training Step: 15 Training Loss: 0.6115351915359497 \n",
      "     Training Step: 16 Training Loss: 0.6154894232749939 \n",
      "     Training Step: 17 Training Loss: 0.6135278940200806 \n",
      "     Training Step: 18 Training Loss: 0.6167367696762085 \n",
      "     Training Step: 19 Training Loss: 0.6142926812171936 \n",
      "     Training Step: 20 Training Loss: 0.6134819388389587 \n",
      "     Training Step: 21 Training Loss: 0.6147179007530212 \n",
      "     Training Step: 22 Training Loss: 0.6109089255332947 \n",
      "     Training Step: 23 Training Loss: 0.6106546521186829 \n",
      "     Training Step: 24 Training Loss: 0.6145315170288086 \n",
      "     Training Step: 25 Training Loss: 0.6179139018058777 \n",
      "     Training Step: 26 Training Loss: 0.6123793721199036 \n",
      "     Training Step: 27 Training Loss: 0.6167200803756714 \n",
      "     Training Step: 28 Training Loss: 0.6101616621017456 \n",
      "     Training Step: 29 Training Loss: 0.6101284027099609 \n",
      "     Training Step: 30 Training Loss: 0.6171070337295532 \n",
      "     Training Step: 31 Training Loss: 0.6140424013137817 \n",
      "     Training Step: 32 Training Loss: 0.6155692934989929 \n",
      "     Training Step: 33 Training Loss: 0.6184179186820984 \n",
      "     Training Step: 34 Training Loss: 0.6148812770843506 \n",
      "     Training Step: 35 Training Loss: 0.6166849136352539 \n",
      "     Training Step: 36 Training Loss: 0.6167118549346924 \n",
      "     Training Step: 37 Training Loss: 0.6103503704071045 \n",
      "     Training Step: 38 Training Loss: 0.6153857111930847 \n",
      "     Training Step: 39 Training Loss: 0.6133829951286316 \n",
      "     Training Step: 40 Training Loss: 0.6147770285606384 \n",
      "     Training Step: 41 Training Loss: 0.6151183843612671 \n",
      "     Training Step: 42 Training Loss: 0.6114228367805481 \n",
      "     Training Step: 43 Training Loss: 0.6137620210647583 \n",
      "     Training Step: 44 Training Loss: 0.6118264198303223 \n",
      "     Training Step: 45 Training Loss: 0.611440122127533 \n",
      "     Training Step: 46 Training Loss: 0.6116371154785156 \n",
      "     Training Step: 47 Training Loss: 0.6142135858535767 \n",
      "     Training Step: 48 Training Loss: 0.6186915636062622 \n",
      "     Training Step: 49 Training Loss: 0.61383455991745 \n",
      "     Training Step: 50 Training Loss: 0.6209821105003357 \n",
      "     Training Step: 51 Training Loss: 0.617500901222229 \n",
      "     Training Step: 52 Training Loss: 0.6133888363838196 \n",
      "     Training Step: 53 Training Loss: 0.6169252991676331 \n",
      "     Training Step: 54 Training Loss: 0.6096535325050354 \n",
      "     Training Step: 55 Training Loss: 0.6119548082351685 \n",
      "     Training Step: 56 Training Loss: 0.6157467365264893 \n",
      "     Training Step: 57 Training Loss: 0.6153382658958435 \n",
      "     Training Step: 58 Training Loss: 0.6158345341682434 \n",
      "     Training Step: 59 Training Loss: 0.6123111844062805 \n",
      "     Training Step: 60 Training Loss: 0.6083073616027832 \n",
      "     Training Step: 61 Training Loss: 0.6160420179367065 \n",
      "     Training Step: 62 Training Loss: 0.6146855354309082 \n",
      "     Training Step: 63 Training Loss: 0.6162572503089905 \n",
      "     Training Step: 64 Training Loss: 0.6109080910682678 \n",
      "     Training Step: 65 Training Loss: 0.6165116429328918 \n",
      "     Training Step: 66 Training Loss: 0.6101543307304382 \n",
      "     Training Step: 67 Training Loss: 0.6186637878417969 \n",
      "     Training Step: 68 Training Loss: 0.6154071092605591 \n",
      "     Training Step: 69 Training Loss: 0.6154573559761047 \n",
      "     Training Step: 70 Training Loss: 0.6196920275688171 \n",
      "     Training Step: 71 Training Loss: 0.6135245561599731 \n",
      "     Training Step: 72 Training Loss: 0.6147885918617249 \n",
      "     Training Step: 73 Training Loss: 0.6128178834915161 \n",
      "     Training Step: 74 Training Loss: 0.6166444420814514 \n",
      "     Training Step: 75 Training Loss: 0.618256688117981 \n",
      "     Training Step: 76 Training Loss: 0.6125436425209045 \n",
      "     Training Step: 77 Training Loss: 0.6160917282104492 \n",
      "     Training Step: 78 Training Loss: 0.6100488305091858 \n",
      "     Training Step: 79 Training Loss: 0.61320960521698 \n",
      "     Training Step: 80 Training Loss: 0.6189144849777222 \n",
      "     Training Step: 81 Training Loss: 0.6168119311332703 \n",
      "     Training Step: 82 Training Loss: 0.6105574369430542 \n",
      "     Training Step: 83 Training Loss: 0.6116735935211182 \n",
      "     Training Step: 84 Training Loss: 0.614264190196991 \n",
      "     Training Step: 85 Training Loss: 0.6133372187614441 \n",
      "     Training Step: 86 Training Loss: 0.6142011284828186 \n",
      "     Training Step: 87 Training Loss: 0.6105716824531555 \n",
      "     Training Step: 88 Training Loss: 0.6122416257858276 \n",
      "     Training Step: 89 Training Loss: 0.6171924471855164 \n",
      "     Training Step: 90 Training Loss: 0.6161287426948547 \n",
      "     Training Step: 91 Training Loss: 0.6162638068199158 \n",
      "     Training Step: 92 Training Loss: 0.613340437412262 \n",
      "     Training Step: 93 Training Loss: 0.6131134033203125 \n",
      "     Training Step: 94 Training Loss: 0.6144364476203918 \n",
      "     Training Step: 95 Training Loss: 0.6141216158866882 \n",
      "     Training Step: 96 Training Loss: 0.6168335676193237 \n",
      "     Training Step: 97 Training Loss: 0.6128645539283752 \n",
      "     Training Step: 98 Training Loss: 0.6140952110290527 \n",
      "     Training Step: 99 Training Loss: 0.6128888130187988 \n",
      "     Training Step: 100 Training Loss: 0.6100937724113464 \n",
      "     Training Step: 101 Training Loss: 0.6133155226707458 \n",
      "     Training Step: 102 Training Loss: 0.6154475212097168 \n",
      "     Training Step: 103 Training Loss: 0.6144675016403198 \n",
      "     Training Step: 104 Training Loss: 0.6107087731361389 \n",
      "     Training Step: 105 Training Loss: 0.615981936454773 \n",
      "     Training Step: 106 Training Loss: 0.618067741394043 \n",
      "     Training Step: 107 Training Loss: 0.614458441734314 \n",
      "     Training Step: 108 Training Loss: 0.6148145794868469 \n",
      "     Training Step: 109 Training Loss: 0.6118509769439697 \n",
      "     Training Step: 110 Training Loss: 0.6140339970588684 \n",
      "     Training Step: 111 Training Loss: 0.6115639805793762 \n",
      "     Training Step: 112 Training Loss: 0.6149484515190125 \n",
      "     Training Step: 113 Training Loss: 0.6125612854957581 \n",
      "     Training Step: 114 Training Loss: 0.61341792345047 \n",
      "     Training Step: 115 Training Loss: 0.6146678924560547 \n",
      "     Training Step: 116 Training Loss: 0.6149801015853882 \n",
      "     Training Step: 117 Training Loss: 0.6094156503677368 \n",
      "     Training Step: 118 Training Loss: 0.6132129430770874 \n",
      "     Training Step: 119 Training Loss: 0.6120731234550476 \n",
      "     Training Step: 120 Training Loss: 0.6122390627861023 \n",
      "     Training Step: 121 Training Loss: 0.6153811812400818 \n",
      "     Training Step: 122 Training Loss: 0.6107324957847595 \n",
      "     Training Step: 123 Training Loss: 0.6122884750366211 \n",
      "     Training Step: 124 Training Loss: 0.6130661964416504 \n",
      "     Training Step: 125 Training Loss: 0.6136163473129272 \n",
      "     Training Step: 126 Training Loss: 0.609734833240509 \n",
      "     Training Step: 127 Training Loss: 0.6172255873680115 \n",
      "     Training Step: 128 Training Loss: 0.6125230193138123 \n",
      "     Training Step: 129 Training Loss: 0.6143736839294434 \n",
      "     Training Step: 130 Training Loss: 0.6167259812355042 \n",
      "     Training Step: 131 Training Loss: 0.6106519103050232 \n",
      "     Training Step: 132 Training Loss: 0.6136740446090698 \n",
      "     Training Step: 133 Training Loss: 0.618480920791626 \n",
      "     Training Step: 134 Training Loss: 0.6124067306518555 \n",
      "     Training Step: 135 Training Loss: 0.611853837966919 \n",
      "     Training Step: 136 Training Loss: 0.6157475113868713 \n",
      "     Training Step: 137 Training Loss: 0.6114954352378845 \n",
      "     Training Step: 138 Training Loss: 0.6171733736991882 \n",
      "     Training Step: 139 Training Loss: 0.6111568808555603 \n",
      "     Training Step: 140 Training Loss: 0.6172329187393188 \n",
      "     Training Step: 141 Training Loss: 0.6146950721740723 \n",
      "     Training Step: 142 Training Loss: 0.6145070195198059 \n",
      "     Training Step: 143 Training Loss: 0.6146492958068848 \n",
      "     Training Step: 144 Training Loss: 0.6161971092224121 \n",
      "     Training Step: 145 Training Loss: 0.6124996542930603 \n",
      "     Training Step: 146 Training Loss: 0.610640287399292 \n",
      "     Training Step: 147 Training Loss: 0.6142159700393677 \n",
      "     Training Step: 148 Training Loss: 0.6153270602226257 \n",
      "     Training Step: 149 Training Loss: 0.6116155982017517 \n",
      "     Training Step: 150 Training Loss: 0.6126457452774048 \n",
      "     Training Step: 151 Training Loss: 0.615094006061554 \n",
      "     Training Step: 152 Training Loss: 0.6129865646362305 \n",
      "     Training Step: 153 Training Loss: 0.6131341457366943 \n",
      "     Training Step: 154 Training Loss: 0.6137615442276001 \n",
      "     Training Step: 155 Training Loss: 0.617706298828125 \n",
      "     Training Step: 156 Training Loss: 0.6136301755905151 \n",
      "     Training Step: 157 Training Loss: 0.6168211102485657 \n",
      "     Training Step: 158 Training Loss: 0.609264612197876 \n",
      "     Training Step: 159 Training Loss: 0.6151042580604553 \n",
      "     Training Step: 160 Training Loss: 0.6116481423377991 \n",
      "     Training Step: 161 Training Loss: 0.6166844964027405 \n",
      "     Training Step: 162 Training Loss: 0.6121603846549988 \n",
      "     Training Step: 163 Training Loss: 0.6146813035011292 \n",
      "     Training Step: 164 Training Loss: 0.6143442392349243 \n",
      "     Training Step: 165 Training Loss: 0.6097565293312073 \n",
      "     Training Step: 166 Training Loss: 0.6114148497581482 \n",
      "     Training Step: 167 Training Loss: 0.61829674243927 \n",
      "     Training Step: 168 Training Loss: 0.6154842972755432 \n",
      "     Training Step: 169 Training Loss: 0.6105713248252869 \n",
      "     Training Step: 170 Training Loss: 0.613914430141449 \n",
      "     Training Step: 171 Training Loss: 0.6177778244018555 \n",
      "     Training Step: 172 Training Loss: 0.6196410059928894 \n",
      "     Training Step: 173 Training Loss: 0.6129732131958008 \n",
      "     Training Step: 174 Training Loss: 0.6180958151817322 \n",
      "     Training Step: 175 Training Loss: 0.6147242188453674 \n",
      "     Training Step: 176 Training Loss: 0.6146723031997681 \n",
      "     Training Step: 177 Training Loss: 0.6118913888931274 \n",
      "     Training Step: 178 Training Loss: 0.6184406280517578 \n",
      "     Training Step: 179 Training Loss: 0.614362359046936 \n",
      "     Training Step: 180 Training Loss: 0.6154130697250366 \n",
      "     Training Step: 181 Training Loss: 0.6151806712150574 \n",
      "     Training Step: 182 Training Loss: 0.6127183437347412 \n",
      "     Training Step: 183 Training Loss: 0.6153153777122498 \n",
      "     Training Step: 184 Training Loss: 0.6194428205490112 \n",
      "     Training Step: 185 Training Loss: 0.6135286688804626 \n",
      "     Training Step: 186 Training Loss: 0.6125487089157104 \n",
      "     Training Step: 187 Training Loss: 0.617631733417511 \n",
      "     Training Step: 188 Training Loss: 0.615287721157074 \n",
      "     Training Step: 189 Training Loss: 0.6122353076934814 \n",
      "     Training Step: 190 Training Loss: 0.6146324276924133 \n",
      "     Training Step: 191 Training Loss: 0.616624116897583 \n",
      "     Training Step: 192 Training Loss: 0.615833580493927 \n",
      "     Training Step: 193 Training Loss: 0.6184217929840088 \n",
      "     Training Step: 194 Training Loss: 0.6116101145744324 \n",
      "     Training Step: 195 Training Loss: 0.6115078330039978 \n",
      "     Training Step: 196 Training Loss: 0.6139421463012695 \n",
      "     Training Step: 197 Training Loss: 0.6107217073440552 \n",
      "     Training Step: 198 Training Loss: 0.6128270626068115 \n",
      "     Training Step: 199 Training Loss: 0.6140968203544617 \n",
      "     Training Step: 200 Training Loss: 0.6115431189537048 \n",
      "     Training Step: 201 Training Loss: 0.6111961603164673 \n",
      "     Training Step: 202 Training Loss: 0.6169487833976746 \n",
      "     Training Step: 203 Training Loss: 0.6180687546730042 \n",
      "     Training Step: 204 Training Loss: 0.61774742603302 \n",
      "     Training Step: 205 Training Loss: 0.6152603626251221 \n",
      "     Training Step: 206 Training Loss: 0.6137672066688538 \n",
      "     Training Step: 207 Training Loss: 0.6121953129768372 \n",
      "     Training Step: 208 Training Loss: 0.6118007302284241 \n",
      "     Training Step: 209 Training Loss: 0.614793598651886 \n",
      "     Training Step: 210 Training Loss: 0.6104421019554138 \n",
      "     Training Step: 211 Training Loss: 0.6117779612541199 \n",
      "     Training Step: 212 Training Loss: 0.6153206825256348 \n",
      "     Training Step: 213 Training Loss: 0.6199955940246582 \n",
      "     Training Step: 214 Training Loss: 0.6122370958328247 \n",
      "     Training Step: 215 Training Loss: 0.6121779680252075 \n",
      "     Training Step: 216 Training Loss: 0.6130021810531616 \n",
      "     Training Step: 217 Training Loss: 0.6112151741981506 \n",
      "     Training Step: 218 Training Loss: 0.6174293160438538 \n",
      "     Training Step: 219 Training Loss: 0.6121647357940674 \n",
      "     Training Step: 220 Training Loss: 0.6116377711296082 \n",
      "     Training Step: 221 Training Loss: 0.6165467500686646 \n",
      "     Training Step: 222 Training Loss: 0.6203144192695618 \n",
      "     Training Step: 223 Training Loss: 0.6115113496780396 \n",
      "     Training Step: 224 Training Loss: 0.6105484366416931 \n",
      "     Training Step: 225 Training Loss: 0.6157447695732117 \n",
      "     Training Step: 226 Training Loss: 0.6146453022956848 \n",
      "     Training Step: 227 Training Loss: 0.6144036650657654 \n",
      "     Training Step: 228 Training Loss: 0.6139148473739624 \n",
      "     Training Step: 229 Training Loss: 0.6123824119567871 \n",
      "     Training Step: 230 Training Loss: 0.6182018518447876 \n",
      "     Training Step: 231 Training Loss: 0.6167064905166626 \n",
      "     Training Step: 232 Training Loss: 0.6157678961753845 \n",
      "     Training Step: 233 Training Loss: 0.6168211102485657 \n",
      "     Training Step: 234 Training Loss: 0.6128957271575928 \n",
      "     Training Step: 235 Training Loss: 0.6155611872673035 \n",
      "     Training Step: 236 Training Loss: 0.6119292378425598 \n",
      "     Training Step: 237 Training Loss: 0.6132344603538513 \n",
      "     Training Step: 238 Training Loss: 0.616462767124176 \n",
      "     Training Step: 239 Training Loss: 0.6189947724342346 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6160789132118225 \n",
      "     Validation Step: 1 Validation Loss: 0.6148925423622131 \n",
      "     Validation Step: 2 Validation Loss: 0.6156944632530212 \n",
      "     Validation Step: 3 Validation Loss: 0.6184313893318176 \n",
      "     Validation Step: 4 Validation Loss: 0.6133517026901245 \n",
      "     Validation Step: 5 Validation Loss: 0.615094006061554 \n",
      "     Validation Step: 6 Validation Loss: 0.6111904382705688 \n",
      "     Validation Step: 7 Validation Loss: 0.618575930595398 \n",
      "     Validation Step: 8 Validation Loss: 0.6137123107910156 \n",
      "     Validation Step: 9 Validation Loss: 0.6143099665641785 \n",
      "     Validation Step: 10 Validation Loss: 0.6174128651618958 \n",
      "     Validation Step: 11 Validation Loss: 0.6153038144111633 \n",
      "     Validation Step: 12 Validation Loss: 0.6171247959136963 \n",
      "     Validation Step: 13 Validation Loss: 0.6176939606666565 \n",
      "     Validation Step: 14 Validation Loss: 0.6141646504402161 \n",
      "     Validation Step: 15 Validation Loss: 0.6112143397331238 \n",
      "     Validation Step: 16 Validation Loss: 0.6121673583984375 \n",
      "     Validation Step: 17 Validation Loss: 0.6142505407333374 \n",
      "     Validation Step: 18 Validation Loss: 0.6130151748657227 \n",
      "     Validation Step: 19 Validation Loss: 0.6146841049194336 \n",
      "     Validation Step: 20 Validation Loss: 0.6145884394645691 \n",
      "     Validation Step: 21 Validation Loss: 0.6128667593002319 \n",
      "     Validation Step: 22 Validation Loss: 0.6115919947624207 \n",
      "     Validation Step: 23 Validation Loss: 0.6141590476036072 \n",
      "     Validation Step: 24 Validation Loss: 0.6102132201194763 \n",
      "     Validation Step: 25 Validation Loss: 0.613689124584198 \n",
      "     Validation Step: 26 Validation Loss: 0.6181631088256836 \n",
      "     Validation Step: 27 Validation Loss: 0.615381121635437 \n",
      "     Validation Step: 28 Validation Loss: 0.6116907596588135 \n",
      "     Validation Step: 29 Validation Loss: 0.6105095744132996 \n",
      "     Validation Step: 30 Validation Loss: 0.6163363456726074 \n",
      "     Validation Step: 31 Validation Loss: 0.618333637714386 \n",
      "     Validation Step: 32 Validation Loss: 0.6186052560806274 \n",
      "     Validation Step: 33 Validation Loss: 0.6137151718139648 \n",
      "     Validation Step: 34 Validation Loss: 0.6106778383255005 \n",
      "     Validation Step: 35 Validation Loss: 0.6105403304100037 \n",
      "     Validation Step: 36 Validation Loss: 0.6075195670127869 \n",
      "     Validation Step: 37 Validation Loss: 0.6149630546569824 \n",
      "     Validation Step: 38 Validation Loss: 0.6119108200073242 \n",
      "     Validation Step: 39 Validation Loss: 0.6156623959541321 \n",
      "     Validation Step: 40 Validation Loss: 0.6101282835006714 \n",
      "     Validation Step: 41 Validation Loss: 0.6101780533790588 \n",
      "     Validation Step: 42 Validation Loss: 0.617790699005127 \n",
      "     Validation Step: 43 Validation Loss: 0.6158552169799805 \n",
      "     Validation Step: 44 Validation Loss: 0.6146559119224548 \n",
      "Epoch: 72\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6156007051467896 \n",
      "     Training Step: 1 Training Loss: 0.6134936213493347 \n",
      "     Training Step: 2 Training Loss: 0.6196884512901306 \n",
      "     Training Step: 3 Training Loss: 0.6156300902366638 \n",
      "     Training Step: 4 Training Loss: 0.6128068566322327 \n",
      "     Training Step: 5 Training Loss: 0.6128504872322083 \n",
      "     Training Step: 6 Training Loss: 0.6149798631668091 \n",
      "     Training Step: 7 Training Loss: 0.6106148958206177 \n",
      "     Training Step: 8 Training Loss: 0.6159344911575317 \n",
      "     Training Step: 9 Training Loss: 0.6109278202056885 \n",
      "     Training Step: 10 Training Loss: 0.6154038906097412 \n",
      "     Training Step: 11 Training Loss: 0.6152150630950928 \n",
      "     Training Step: 12 Training Loss: 0.6128128170967102 \n",
      "     Training Step: 13 Training Loss: 0.6141823530197144 \n",
      "     Training Step: 14 Training Loss: 0.6196733713150024 \n",
      "     Training Step: 15 Training Loss: 0.6161572933197021 \n",
      "     Training Step: 16 Training Loss: 0.6138211488723755 \n",
      "     Training Step: 17 Training Loss: 0.6147814393043518 \n",
      "     Training Step: 18 Training Loss: 0.613720715045929 \n",
      "     Training Step: 19 Training Loss: 0.6135249733924866 \n",
      "     Training Step: 20 Training Loss: 0.6154876947402954 \n",
      "     Training Step: 21 Training Loss: 0.6082936525344849 \n",
      "     Training Step: 22 Training Loss: 0.609228253364563 \n",
      "     Training Step: 23 Training Loss: 0.6115429401397705 \n",
      "     Training Step: 24 Training Loss: 0.6133989691734314 \n",
      "     Training Step: 25 Training Loss: 0.617569625377655 \n",
      "     Training Step: 26 Training Loss: 0.6114473938941956 \n",
      "     Training Step: 27 Training Loss: 0.6167994737625122 \n",
      "     Training Step: 28 Training Loss: 0.6142646074295044 \n",
      "     Training Step: 29 Training Loss: 0.6188506484031677 \n",
      "     Training Step: 30 Training Loss: 0.6153304576873779 \n",
      "     Training Step: 31 Training Loss: 0.6148236393928528 \n",
      "     Training Step: 32 Training Loss: 0.6118857860565186 \n",
      "     Training Step: 33 Training Loss: 0.614483654499054 \n",
      "     Training Step: 34 Training Loss: 0.6182594895362854 \n",
      "     Training Step: 35 Training Loss: 0.61217200756073 \n",
      "     Training Step: 36 Training Loss: 0.6097800731658936 \n",
      "     Training Step: 37 Training Loss: 0.6100676655769348 \n",
      "     Training Step: 38 Training Loss: 0.6149691343307495 \n",
      "     Training Step: 39 Training Loss: 0.6146363019943237 \n",
      "     Training Step: 40 Training Loss: 0.6140450239181519 \n",
      "     Training Step: 41 Training Loss: 0.6120221614837646 \n",
      "     Training Step: 42 Training Loss: 0.6151599287986755 \n",
      "     Training Step: 43 Training Loss: 0.6166771650314331 \n",
      "     Training Step: 44 Training Loss: 0.6116815209388733 \n",
      "     Training Step: 45 Training Loss: 0.6171726584434509 \n",
      "     Training Step: 46 Training Loss: 0.6180312633514404 \n",
      "     Training Step: 47 Training Loss: 0.6184051632881165 \n",
      "     Training Step: 48 Training Loss: 0.6137135624885559 \n",
      "     Training Step: 49 Training Loss: 0.6106953620910645 \n",
      "     Training Step: 50 Training Loss: 0.614700436592102 \n",
      "     Training Step: 51 Training Loss: 0.6177811026573181 \n",
      "     Training Step: 52 Training Loss: 0.6130095720291138 \n",
      "     Training Step: 53 Training Loss: 0.6115782856941223 \n",
      "     Training Step: 54 Training Loss: 0.6154106259346008 \n",
      "     Training Step: 55 Training Loss: 0.6128619909286499 \n",
      "     Training Step: 56 Training Loss: 0.6146838068962097 \n",
      "     Training Step: 57 Training Loss: 0.6131513118743896 \n",
      "     Training Step: 58 Training Loss: 0.6165090799331665 \n",
      "     Training Step: 59 Training Loss: 0.612147867679596 \n",
      "     Training Step: 60 Training Loss: 0.6124037504196167 \n",
      "     Training Step: 61 Training Loss: 0.6118389368057251 \n",
      "     Training Step: 62 Training Loss: 0.6094514727592468 \n",
      "     Training Step: 63 Training Loss: 0.6111668348312378 \n",
      "     Training Step: 64 Training Loss: 0.6116475462913513 \n",
      "     Training Step: 65 Training Loss: 0.6151785850524902 \n",
      "     Training Step: 66 Training Loss: 0.614775538444519 \n",
      "     Training Step: 67 Training Loss: 0.6131001710891724 \n",
      "     Training Step: 68 Training Loss: 0.6153159737586975 \n",
      "     Training Step: 69 Training Loss: 0.614204466342926 \n",
      "     Training Step: 70 Training Loss: 0.6129670143127441 \n",
      "     Training Step: 71 Training Loss: 0.618852972984314 \n",
      "     Training Step: 72 Training Loss: 0.6098355054855347 \n",
      "     Training Step: 73 Training Loss: 0.6168227195739746 \n",
      "     Training Step: 74 Training Loss: 0.6128972768783569 \n",
      "     Training Step: 75 Training Loss: 0.6168742775917053 \n",
      "     Training Step: 76 Training Loss: 0.6139290928840637 \n",
      "     Training Step: 77 Training Loss: 0.6139115691184998 \n",
      "     Training Step: 78 Training Loss: 0.6185988783836365 \n",
      "     Training Step: 79 Training Loss: 0.6107801198959351 \n",
      "     Training Step: 80 Training Loss: 0.6167268753051758 \n",
      "     Training Step: 81 Training Loss: 0.6140342950820923 \n",
      "     Training Step: 82 Training Loss: 0.6107511520385742 \n",
      "     Training Step: 83 Training Loss: 0.6122450828552246 \n",
      "     Training Step: 84 Training Loss: 0.6167941093444824 \n",
      "     Training Step: 85 Training Loss: 0.6145433187484741 \n",
      "     Training Step: 86 Training Loss: 0.6178316473960876 \n",
      "     Training Step: 87 Training Loss: 0.615534782409668 \n",
      "     Training Step: 88 Training Loss: 0.6117388606071472 \n",
      "     Training Step: 89 Training Loss: 0.614753782749176 \n",
      "     Training Step: 90 Training Loss: 0.6146308779716492 \n",
      "     Training Step: 91 Training Loss: 0.6094465851783752 \n",
      "     Training Step: 92 Training Loss: 0.611705482006073 \n",
      "     Training Step: 93 Training Loss: 0.6169585585594177 \n",
      "     Training Step: 94 Training Loss: 0.6125825643539429 \n",
      "     Training Step: 95 Training Loss: 0.6157744526863098 \n",
      "     Training Step: 96 Training Loss: 0.6176498532295227 \n",
      "     Training Step: 97 Training Loss: 0.6119951605796814 \n",
      "     Training Step: 98 Training Loss: 0.613625168800354 \n",
      "     Training Step: 99 Training Loss: 0.6122716069221497 \n",
      "     Training Step: 100 Training Loss: 0.615775465965271 \n",
      "     Training Step: 101 Training Loss: 0.6146972179412842 \n",
      "     Training Step: 102 Training Loss: 0.6133183836936951 \n",
      "     Training Step: 103 Training Loss: 0.6103910803794861 \n",
      "     Training Step: 104 Training Loss: 0.620415985584259 \n",
      "     Training Step: 105 Training Loss: 0.6114798784255981 \n",
      "     Training Step: 106 Training Loss: 0.6199337244033813 \n",
      "     Training Step: 107 Training Loss: 0.6142759323120117 \n",
      "     Training Step: 108 Training Loss: 0.6119346618652344 \n",
      "     Training Step: 109 Training Loss: 0.6143918633460999 \n",
      "     Training Step: 110 Training Loss: 0.6106436252593994 \n",
      "     Training Step: 111 Training Loss: 0.6143661141395569 \n",
      "     Training Step: 112 Training Loss: 0.6210910677909851 \n",
      "     Training Step: 113 Training Loss: 0.61225426197052 \n",
      "     Training Step: 114 Training Loss: 0.6167647242546082 \n",
      "     Training Step: 115 Training Loss: 0.6153802871704102 \n",
      "     Training Step: 116 Training Loss: 0.6154676079750061 \n",
      "     Training Step: 117 Training Loss: 0.6152336001396179 \n",
      "     Training Step: 118 Training Loss: 0.6177088022232056 \n",
      "     Training Step: 119 Training Loss: 0.611627459526062 \n",
      "     Training Step: 120 Training Loss: 0.6160099506378174 \n",
      "     Training Step: 121 Training Loss: 0.6146822571754456 \n",
      "     Training Step: 122 Training Loss: 0.6182927489280701 \n",
      "     Training Step: 123 Training Loss: 0.6106019020080566 \n",
      "     Training Step: 124 Training Loss: 0.6148121356964111 \n",
      "     Training Step: 125 Training Loss: 0.6184089183807373 \n",
      "     Training Step: 126 Training Loss: 0.6127796173095703 \n",
      "     Training Step: 127 Training Loss: 0.6132320165634155 \n",
      "     Training Step: 128 Training Loss: 0.6180696487426758 \n",
      "     Training Step: 129 Training Loss: 0.6177647709846497 \n",
      "     Training Step: 130 Training Loss: 0.6144579648971558 \n",
      "     Training Step: 131 Training Loss: 0.6143596172332764 \n",
      "     Training Step: 132 Training Loss: 0.614160418510437 \n",
      "     Training Step: 133 Training Loss: 0.614710807800293 \n",
      "     Training Step: 134 Training Loss: 0.6109042167663574 \n",
      "     Training Step: 135 Training Loss: 0.6125427484512329 \n",
      "     Training Step: 136 Training Loss: 0.61674565076828 \n",
      "     Training Step: 137 Training Loss: 0.6114497184753418 \n",
      "     Training Step: 138 Training Loss: 0.619509756565094 \n",
      "     Training Step: 139 Training Loss: 0.6149488687515259 \n",
      "     Training Step: 140 Training Loss: 0.6144205927848816 \n",
      "     Training Step: 141 Training Loss: 0.6101734638214111 \n",
      "     Training Step: 142 Training Loss: 0.6171367764472961 \n",
      "     Training Step: 143 Training Loss: 0.613826334476471 \n",
      "     Training Step: 144 Training Loss: 0.6139439344406128 \n",
      "     Training Step: 145 Training Loss: 0.6128088235855103 \n",
      "     Training Step: 146 Training Loss: 0.6164577007293701 \n",
      "     Training Step: 147 Training Loss: 0.6132141351699829 \n",
      "     Training Step: 148 Training Loss: 0.6111575961112976 \n",
      "     Training Step: 149 Training Loss: 0.6121721267700195 \n",
      "     Training Step: 150 Training Loss: 0.6133666038513184 \n",
      "     Training Step: 151 Training Loss: 0.6100643277168274 \n",
      "     Training Step: 152 Training Loss: 0.6186951398849487 \n",
      "     Training Step: 153 Training Loss: 0.6178507804870605 \n",
      "     Training Step: 154 Training Loss: 0.6107513308525085 \n",
      "     Training Step: 155 Training Loss: 0.6180474162101746 \n",
      "     Training Step: 156 Training Loss: 0.6171912550926208 \n",
      "     Training Step: 157 Training Loss: 0.6124348044395447 \n",
      "     Training Step: 158 Training Loss: 0.6166167259216309 \n",
      "     Training Step: 159 Training Loss: 0.6131030917167664 \n",
      "     Training Step: 160 Training Loss: 0.6101911067962646 \n",
      "     Training Step: 161 Training Loss: 0.6146873831748962 \n",
      "     Training Step: 162 Training Loss: 0.6115962862968445 \n",
      "     Training Step: 163 Training Loss: 0.6153635382652283 \n",
      "     Training Step: 164 Training Loss: 0.6153628826141357 \n",
      "     Training Step: 165 Training Loss: 0.6137750148773193 \n",
      "     Training Step: 166 Training Loss: 0.6155652403831482 \n",
      "     Training Step: 167 Training Loss: 0.6157633662223816 \n",
      "     Training Step: 168 Training Loss: 0.6167027950286865 \n",
      "     Training Step: 169 Training Loss: 0.6098129749298096 \n",
      "     Training Step: 170 Training Loss: 0.6148030757904053 \n",
      "     Training Step: 171 Training Loss: 0.6177462935447693 \n",
      "     Training Step: 172 Training Loss: 0.6154880523681641 \n",
      "     Training Step: 173 Training Loss: 0.6129637956619263 \n",
      "     Training Step: 174 Training Loss: 0.6105296611785889 \n",
      "     Training Step: 175 Training Loss: 0.616285502910614 \n",
      "     Training Step: 176 Training Loss: 0.6115483641624451 \n",
      "     Training Step: 177 Training Loss: 0.6144936084747314 \n",
      "     Training Step: 178 Training Loss: 0.6114013195037842 \n",
      "     Training Step: 179 Training Loss: 0.6124776005744934 \n",
      "     Training Step: 180 Training Loss: 0.6158301830291748 \n",
      "     Training Step: 181 Training Loss: 0.6132810115814209 \n",
      "     Training Step: 182 Training Loss: 0.615764856338501 \n",
      "     Training Step: 183 Training Loss: 0.6132110357284546 \n",
      "     Training Step: 184 Training Loss: 0.6144475340843201 \n",
      "     Training Step: 185 Training Loss: 0.610027015209198 \n",
      "     Training Step: 186 Training Loss: 0.618118941783905 \n",
      "     Training Step: 187 Training Loss: 0.6123024225234985 \n",
      "     Training Step: 188 Training Loss: 0.6154384016990662 \n",
      "     Training Step: 189 Training Loss: 0.6101967096328735 \n",
      "     Training Step: 190 Training Loss: 0.6133145093917847 \n",
      "     Training Step: 191 Training Loss: 0.6124991774559021 \n",
      "     Training Step: 192 Training Loss: 0.6117916703224182 \n",
      "     Training Step: 193 Training Loss: 0.6185319423675537 \n",
      "     Training Step: 194 Training Loss: 0.6162254214286804 \n",
      "     Training Step: 195 Training Loss: 0.6175019145011902 \n",
      "     Training Step: 196 Training Loss: 0.6107946038246155 \n",
      "     Training Step: 197 Training Loss: 0.6159632205963135 \n",
      "     Training Step: 198 Training Loss: 0.6125800013542175 \n",
      "     Training Step: 199 Training Loss: 0.6171090006828308 \n",
      "     Training Step: 200 Training Loss: 0.6135377883911133 \n",
      "     Training Step: 201 Training Loss: 0.6136023998260498 \n",
      "     Training Step: 202 Training Loss: 0.6140970587730408 \n",
      "     Training Step: 203 Training Loss: 0.6166772842407227 \n",
      "     Training Step: 204 Training Loss: 0.6137544512748718 \n",
      "     Training Step: 205 Training Loss: 0.6124070286750793 \n",
      "     Training Step: 206 Training Loss: 0.617144763469696 \n",
      "     Training Step: 207 Training Loss: 0.6146240830421448 \n",
      "     Training Step: 208 Training Loss: 0.6156914234161377 \n",
      "     Training Step: 209 Training Loss: 0.6163504719734192 \n",
      "     Training Step: 210 Training Loss: 0.6136581301689148 \n",
      "     Training Step: 211 Training Loss: 0.6168970465660095 \n",
      "     Training Step: 212 Training Loss: 0.6142177581787109 \n",
      "     Training Step: 213 Training Loss: 0.6114732027053833 \n",
      "     Training Step: 214 Training Loss: 0.6184805035591125 \n",
      "     Training Step: 215 Training Loss: 0.6160404682159424 \n",
      "     Training Step: 216 Training Loss: 0.6114609241485596 \n",
      "     Training Step: 217 Training Loss: 0.6118607521057129 \n",
      "     Training Step: 218 Training Loss: 0.6118559241294861 \n",
      "     Training Step: 219 Training Loss: 0.612883985042572 \n",
      "     Training Step: 220 Training Loss: 0.6170469522476196 \n",
      "     Training Step: 221 Training Loss: 0.6140398383140564 \n",
      "     Training Step: 222 Training Loss: 0.6152580976486206 \n",
      "     Training Step: 223 Training Loss: 0.6105172634124756 \n",
      "     Training Step: 224 Training Loss: 0.6134310960769653 \n",
      "     Training Step: 225 Training Loss: 0.6104092001914978 \n",
      "     Training Step: 226 Training Loss: 0.613283634185791 \n",
      "     Training Step: 227 Training Loss: 0.616503894329071 \n",
      "     Training Step: 228 Training Loss: 0.612277626991272 \n",
      "     Training Step: 229 Training Loss: 0.6122394800186157 \n",
      "     Training Step: 230 Training Loss: 0.6131249666213989 \n",
      "     Training Step: 231 Training Loss: 0.6167942881584167 \n",
      "     Training Step: 232 Training Loss: 0.6154842376708984 \n",
      "     Training Step: 233 Training Loss: 0.6150798797607422 \n",
      "     Training Step: 234 Training Loss: 0.6112543940544128 \n",
      "     Training Step: 235 Training Loss: 0.6116623878479004 \n",
      "     Training Step: 236 Training Loss: 0.6162137985229492 \n",
      "     Training Step: 237 Training Loss: 0.6120818853378296 \n",
      "     Training Step: 238 Training Loss: 0.6143114566802979 \n",
      "     Training Step: 239 Training Loss: 0.6148971915245056 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6180917024612427 \n",
      "     Validation Step: 1 Validation Loss: 0.6136805415153503 \n",
      "     Validation Step: 2 Validation Loss: 0.6106289029121399 \n",
      "     Validation Step: 3 Validation Loss: 0.6133083701133728 \n",
      "     Validation Step: 4 Validation Loss: 0.6141178607940674 \n",
      "     Validation Step: 5 Validation Loss: 0.6129876375198364 \n",
      "     Validation Step: 6 Validation Loss: 0.615238606929779 \n",
      "     Validation Step: 7 Validation Loss: 0.6176162958145142 \n",
      "     Validation Step: 8 Validation Loss: 0.6136401891708374 \n",
      "     Validation Step: 9 Validation Loss: 0.6145444512367249 \n",
      "     Validation Step: 10 Validation Loss: 0.6146472096443176 \n",
      "     Validation Step: 11 Validation Loss: 0.6149009466171265 \n",
      "     Validation Step: 12 Validation Loss: 0.6158167719841003 \n",
      "     Validation Step: 13 Validation Loss: 0.6075179576873779 \n",
      "     Validation Step: 14 Validation Loss: 0.6156304478645325 \n",
      "     Validation Step: 15 Validation Loss: 0.6105126738548279 \n",
      "     Validation Step: 16 Validation Loss: 0.6145793199539185 \n",
      "     Validation Step: 17 Validation Loss: 0.6150595545768738 \n",
      "     Validation Step: 18 Validation Loss: 0.6173268556594849 \n",
      "     Validation Step: 19 Validation Loss: 0.6111598014831543 \n",
      "     Validation Step: 20 Validation Loss: 0.618527889251709 \n",
      "     Validation Step: 21 Validation Loss: 0.6128283739089966 \n",
      "     Validation Step: 22 Validation Loss: 0.6182731986045837 \n",
      "     Validation Step: 23 Validation Loss: 0.6116424798965454 \n",
      "     Validation Step: 24 Validation Loss: 0.6183778643608093 \n",
      "     Validation Step: 25 Validation Loss: 0.610133945941925 \n",
      "     Validation Step: 26 Validation Loss: 0.6141355633735657 \n",
      "     Validation Step: 27 Validation Loss: 0.6115524172782898 \n",
      "     Validation Step: 28 Validation Loss: 0.6111776828765869 \n",
      "     Validation Step: 29 Validation Loss: 0.6177253723144531 \n",
      "     Validation Step: 30 Validation Loss: 0.6142619252204895 \n",
      "     Validation Step: 31 Validation Loss: 0.6170516610145569 \n",
      "     Validation Step: 32 Validation Loss: 0.6148467659950256 \n",
      "     Validation Step: 33 Validation Loss: 0.6153337359428406 \n",
      "     Validation Step: 34 Validation Loss: 0.618502676486969 \n",
      "     Validation Step: 35 Validation Loss: 0.61048424243927 \n",
      "     Validation Step: 36 Validation Loss: 0.6142075657844543 \n",
      "     Validation Step: 37 Validation Loss: 0.6160319447517395 \n",
      "     Validation Step: 38 Validation Loss: 0.6101036071777344 \n",
      "     Validation Step: 39 Validation Loss: 0.6121274828910828 \n",
      "     Validation Step: 40 Validation Loss: 0.6162644028663635 \n",
      "     Validation Step: 41 Validation Loss: 0.6156007647514343 \n",
      "     Validation Step: 42 Validation Loss: 0.6136679649353027 \n",
      "     Validation Step: 43 Validation Loss: 0.6101640462875366 \n",
      "     Validation Step: 44 Validation Loss: 0.6118790507316589 \n",
      "Epoch: 73\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6132800579071045 \n",
      "     Training Step: 1 Training Loss: 0.6133138537406921 \n",
      "     Training Step: 2 Training Loss: 0.6168199777603149 \n",
      "     Training Step: 3 Training Loss: 0.6107243895530701 \n",
      "     Training Step: 4 Training Loss: 0.6186338067054749 \n",
      "     Training Step: 5 Training Loss: 0.6169360876083374 \n",
      "     Training Step: 6 Training Loss: 0.6154496073722839 \n",
      "     Training Step: 7 Training Loss: 0.6164109706878662 \n",
      "     Training Step: 8 Training Loss: 0.6102457046508789 \n",
      "     Training Step: 9 Training Loss: 0.6104750037193298 \n",
      "     Training Step: 10 Training Loss: 0.6184747815132141 \n",
      "     Training Step: 11 Training Loss: 0.6147586703300476 \n",
      "     Training Step: 12 Training Loss: 0.6128771305084229 \n",
      "     Training Step: 13 Training Loss: 0.6158735156059265 \n",
      "     Training Step: 14 Training Loss: 0.6180919408798218 \n",
      "     Training Step: 15 Training Loss: 0.6137828230857849 \n",
      "     Training Step: 16 Training Loss: 0.6152963042259216 \n",
      "     Training Step: 17 Training Loss: 0.6131775975227356 \n",
      "     Training Step: 18 Training Loss: 0.6134699583053589 \n",
      "     Training Step: 19 Training Loss: 0.6153922080993652 \n",
      "     Training Step: 20 Training Loss: 0.612390398979187 \n",
      "     Training Step: 21 Training Loss: 0.611526608467102 \n",
      "     Training Step: 22 Training Loss: 0.613524854183197 \n",
      "     Training Step: 23 Training Loss: 0.613372802734375 \n",
      "     Training Step: 24 Training Loss: 0.6183050870895386 \n",
      "     Training Step: 25 Training Loss: 0.6184336543083191 \n",
      "     Training Step: 26 Training Loss: 0.6154274940490723 \n",
      "     Training Step: 27 Training Loss: 0.6145758628845215 \n",
      "     Training Step: 28 Training Loss: 0.6154431700706482 \n",
      "     Training Step: 29 Training Loss: 0.6122922301292419 \n",
      "     Training Step: 30 Training Loss: 0.6171615719795227 \n",
      "     Training Step: 31 Training Loss: 0.6124240159988403 \n",
      "     Training Step: 32 Training Loss: 0.6183620095252991 \n",
      "     Training Step: 33 Training Loss: 0.6157353520393372 \n",
      "     Training Step: 34 Training Loss: 0.6125173568725586 \n",
      "     Training Step: 35 Training Loss: 0.6181005239486694 \n",
      "     Training Step: 36 Training Loss: 0.6116220951080322 \n",
      "     Training Step: 37 Training Loss: 0.6146906018257141 \n",
      "     Training Step: 38 Training Loss: 0.6167793869972229 \n",
      "     Training Step: 39 Training Loss: 0.6083019375801086 \n",
      "     Training Step: 40 Training Loss: 0.6155514717102051 \n",
      "     Training Step: 41 Training Loss: 0.612666666507721 \n",
      "     Training Step: 42 Training Loss: 0.6121960282325745 \n",
      "     Training Step: 43 Training Loss: 0.615597665309906 \n",
      "     Training Step: 44 Training Loss: 0.6177554130554199 \n",
      "     Training Step: 45 Training Loss: 0.6177034974098206 \n",
      "     Training Step: 46 Training Loss: 0.61264967918396 \n",
      "     Training Step: 47 Training Loss: 0.6099534630775452 \n",
      "     Training Step: 48 Training Loss: 0.6123589277267456 \n",
      "     Training Step: 49 Training Loss: 0.6103834509849548 \n",
      "     Training Step: 50 Training Loss: 0.6160843372344971 \n",
      "     Training Step: 51 Training Loss: 0.611915111541748 \n",
      "     Training Step: 52 Training Loss: 0.614361584186554 \n",
      "     Training Step: 53 Training Loss: 0.6149807572364807 \n",
      "     Training Step: 54 Training Loss: 0.6118952035903931 \n",
      "     Training Step: 55 Training Loss: 0.6144271492958069 \n",
      "     Training Step: 56 Training Loss: 0.6122668385505676 \n",
      "     Training Step: 57 Training Loss: 0.613792359828949 \n",
      "     Training Step: 58 Training Loss: 0.6146083474159241 \n",
      "     Training Step: 59 Training Loss: 0.6139205098152161 \n",
      "     Training Step: 60 Training Loss: 0.6172031760215759 \n",
      "     Training Step: 61 Training Loss: 0.6105999946594238 \n",
      "     Training Step: 62 Training Loss: 0.6148003339767456 \n",
      "     Training Step: 63 Training Loss: 0.616823673248291 \n",
      "     Training Step: 64 Training Loss: 0.6154885292053223 \n",
      "     Training Step: 65 Training Loss: 0.6180260181427002 \n",
      "     Training Step: 66 Training Loss: 0.6139934062957764 \n",
      "     Training Step: 67 Training Loss: 0.6184602975845337 \n",
      "     Training Step: 68 Training Loss: 0.6136514544487 \n",
      "     Training Step: 69 Training Loss: 0.6114570498466492 \n",
      "     Training Step: 70 Training Loss: 0.6123175024986267 \n",
      "     Training Step: 71 Training Loss: 0.6167678833007812 \n",
      "     Training Step: 72 Training Loss: 0.6147056818008423 \n",
      "     Training Step: 73 Training Loss: 0.6127844452857971 \n",
      "     Training Step: 74 Training Loss: 0.6168402433395386 \n",
      "     Training Step: 75 Training Loss: 0.6168965101242065 \n",
      "     Training Step: 76 Training Loss: 0.6127753853797913 \n",
      "     Training Step: 77 Training Loss: 0.6161528825759888 \n",
      "     Training Step: 78 Training Loss: 0.614179253578186 \n",
      "     Training Step: 79 Training Loss: 0.6188606023788452 \n",
      "     Training Step: 80 Training Loss: 0.6157910227775574 \n",
      "     Training Step: 81 Training Loss: 0.6144668459892273 \n",
      "     Training Step: 82 Training Loss: 0.6118761301040649 \n",
      "     Training Step: 83 Training Loss: 0.6199386715888977 \n",
      "     Training Step: 84 Training Loss: 0.6147089004516602 \n",
      "     Training Step: 85 Training Loss: 0.6152691841125488 \n",
      "     Training Step: 86 Training Loss: 0.6116906404495239 \n",
      "     Training Step: 87 Training Loss: 0.6143460869789124 \n",
      "     Training Step: 88 Training Loss: 0.6117308735847473 \n",
      "     Training Step: 89 Training Loss: 0.6144279837608337 \n",
      "     Training Step: 90 Training Loss: 0.6153281331062317 \n",
      "     Training Step: 91 Training Loss: 0.6118183732032776 \n",
      "     Training Step: 92 Training Loss: 0.6129241585731506 \n",
      "     Training Step: 93 Training Loss: 0.6144880652427673 \n",
      "     Training Step: 94 Training Loss: 0.6105168461799622 \n",
      "     Training Step: 95 Training Loss: 0.6118007898330688 \n",
      "     Training Step: 96 Training Loss: 0.615170955657959 \n",
      "     Training Step: 97 Training Loss: 0.6114144325256348 \n",
      "     Training Step: 98 Training Loss: 0.6107358932495117 \n",
      "     Training Step: 99 Training Loss: 0.6132426857948303 \n",
      "     Training Step: 100 Training Loss: 0.6187311410903931 \n",
      "     Training Step: 101 Training Loss: 0.6172223687171936 \n",
      "     Training Step: 102 Training Loss: 0.6136752963066101 \n",
      "     Training Step: 103 Training Loss: 0.6168230175971985 \n",
      "     Training Step: 104 Training Loss: 0.6143279075622559 \n",
      "     Training Step: 105 Training Loss: 0.6137377023696899 \n",
      "     Training Step: 106 Training Loss: 0.6115410923957825 \n",
      "     Training Step: 107 Training Loss: 0.6121446490287781 \n",
      "     Training Step: 108 Training Loss: 0.6147467494010925 \n",
      "     Training Step: 109 Training Loss: 0.6125808358192444 \n",
      "     Training Step: 110 Training Loss: 0.6179612874984741 \n",
      "     Training Step: 111 Training Loss: 0.6108720302581787 \n",
      "     Training Step: 112 Training Loss: 0.6196863055229187 \n",
      "     Training Step: 113 Training Loss: 0.6178240180015564 \n",
      "     Training Step: 114 Training Loss: 0.6151993274688721 \n",
      "     Training Step: 115 Training Loss: 0.6153419613838196 \n",
      "     Training Step: 116 Training Loss: 0.6155464053153992 \n",
      "     Training Step: 117 Training Loss: 0.613491415977478 \n",
      "     Training Step: 118 Training Loss: 0.6147619485855103 \n",
      "     Training Step: 119 Training Loss: 0.6135640740394592 \n",
      "     Training Step: 120 Training Loss: 0.6184146404266357 \n",
      "     Training Step: 121 Training Loss: 0.6188732981681824 \n",
      "     Training Step: 122 Training Loss: 0.616697371006012 \n",
      "     Training Step: 123 Training Loss: 0.6137314438819885 \n",
      "     Training Step: 124 Training Loss: 0.6108453273773193 \n",
      "     Training Step: 125 Training Loss: 0.6144047379493713 \n",
      "     Training Step: 126 Training Loss: 0.6128603219985962 \n",
      "     Training Step: 127 Training Loss: 0.6160929203033447 \n",
      "     Training Step: 128 Training Loss: 0.6177793145179749 \n",
      "     Training Step: 129 Training Loss: 0.616783618927002 \n",
      "     Training Step: 130 Training Loss: 0.6105936765670776 \n",
      "     Training Step: 131 Training Loss: 0.6115443706512451 \n",
      "     Training Step: 132 Training Loss: 0.6111948490142822 \n",
      "     Training Step: 133 Training Loss: 0.616019070148468 \n",
      "     Training Step: 134 Training Loss: 0.6153420209884644 \n",
      "     Training Step: 135 Training Loss: 0.6101908683776855 \n",
      "     Training Step: 136 Training Loss: 0.6149335503578186 \n",
      "     Training Step: 137 Training Loss: 0.6149788498878479 \n",
      "     Training Step: 138 Training Loss: 0.6162515878677368 \n",
      "     Training Step: 139 Training Loss: 0.6157568693161011 \n",
      "     Training Step: 140 Training Loss: 0.6116571426391602 \n",
      "     Training Step: 141 Training Loss: 0.6148006319999695 \n",
      "     Training Step: 142 Training Loss: 0.6092807650566101 \n",
      "     Training Step: 143 Training Loss: 0.6144551634788513 \n",
      "     Training Step: 144 Training Loss: 0.6174284219741821 \n",
      "     Training Step: 145 Training Loss: 0.6096988916397095 \n",
      "     Training Step: 146 Training Loss: 0.6157893538475037 \n",
      "     Training Step: 147 Training Loss: 0.6131880879402161 \n",
      "     Training Step: 148 Training Loss: 0.613119900226593 \n",
      "     Training Step: 149 Training Loss: 0.6166942715644836 \n",
      "     Training Step: 150 Training Loss: 0.6153871417045593 \n",
      "     Training Step: 151 Training Loss: 0.6153069734573364 \n",
      "     Training Step: 152 Training Loss: 0.6101250052452087 \n",
      "     Training Step: 153 Training Loss: 0.6132962107658386 \n",
      "     Training Step: 154 Training Loss: 0.6167460083961487 \n",
      "     Training Step: 155 Training Loss: 0.6093885898590088 \n",
      "     Training Step: 156 Training Loss: 0.6147416830062866 \n",
      "     Training Step: 157 Training Loss: 0.6116347312927246 \n",
      "     Training Step: 158 Training Loss: 0.6100520491600037 \n",
      "     Training Step: 159 Training Loss: 0.6151599287986755 \n",
      "     Training Step: 160 Training Loss: 0.6122342944145203 \n",
      "     Training Step: 161 Training Loss: 0.6132732629776001 \n",
      "     Training Step: 162 Training Loss: 0.6143627762794495 \n",
      "     Training Step: 163 Training Loss: 0.6121695041656494 \n",
      "     Training Step: 164 Training Loss: 0.6137769818305969 \n",
      "     Training Step: 165 Training Loss: 0.6115933060646057 \n",
      "     Training Step: 166 Training Loss: 0.6118281483650208 \n",
      "     Training Step: 167 Training Loss: 0.6141372919082642 \n",
      "     Training Step: 168 Training Loss: 0.6107404232025146 \n",
      "     Training Step: 169 Training Loss: 0.6147894859313965 \n",
      "     Training Step: 170 Training Loss: 0.616432785987854 \n",
      "     Training Step: 171 Training Loss: 0.6105177998542786 \n",
      "     Training Step: 172 Training Loss: 0.6156051754951477 \n",
      "     Training Step: 173 Training Loss: 0.6133179068565369 \n",
      "     Training Step: 174 Training Loss: 0.6100878119468689 \n",
      "     Training Step: 175 Training Loss: 0.6148816347122192 \n",
      "     Training Step: 176 Training Loss: 0.6115214824676514 \n",
      "     Training Step: 177 Training Loss: 0.6139103770256042 \n",
      "     Training Step: 178 Training Loss: 0.6167801022529602 \n",
      "     Training Step: 179 Training Loss: 0.6154409646987915 \n",
      "     Training Step: 180 Training Loss: 0.6105866432189941 \n",
      "     Training Step: 181 Training Loss: 0.6174924373626709 \n",
      "     Training Step: 182 Training Loss: 0.61418616771698 \n",
      "     Training Step: 183 Training Loss: 0.6130142211914062 \n",
      "     Training Step: 184 Training Loss: 0.6114760041236877 \n",
      "     Training Step: 185 Training Loss: 0.6114141345024109 \n",
      "     Training Step: 186 Training Loss: 0.6164326071739197 \n",
      "     Training Step: 187 Training Loss: 0.6094234585762024 \n",
      "     Training Step: 188 Training Loss: 0.6196214556694031 \n",
      "     Training Step: 189 Training Loss: 0.6128813028335571 \n",
      "     Training Step: 190 Training Loss: 0.6132166981697083 \n",
      "     Training Step: 191 Training Loss: 0.6162122488021851 \n",
      "     Training Step: 192 Training Loss: 0.6166370511054993 \n",
      "     Training Step: 193 Training Loss: 0.6180113554000854 \n",
      "     Training Step: 194 Training Loss: 0.6156908273696899 \n",
      "     Training Step: 195 Training Loss: 0.6123104691505432 \n",
      "     Training Step: 196 Training Loss: 0.6140577793121338 \n",
      "     Training Step: 197 Training Loss: 0.6131219863891602 \n",
      "     Training Step: 198 Training Loss: 0.6172824501991272 \n",
      "     Training Step: 199 Training Loss: 0.6134942770004272 \n",
      "     Training Step: 200 Training Loss: 0.6109229326248169 \n",
      "     Training Step: 201 Training Loss: 0.6143182516098022 \n",
      "     Training Step: 202 Training Loss: 0.6147447228431702 \n",
      "     Training Step: 203 Training Loss: 0.612166702747345 \n",
      "     Training Step: 204 Training Loss: 0.6198058724403381 \n",
      "     Training Step: 205 Training Loss: 0.6177840828895569 \n",
      "     Training Step: 206 Training Loss: 0.6130990386009216 \n",
      "     Training Step: 207 Training Loss: 0.6165011525154114 \n",
      "     Training Step: 208 Training Loss: 0.6120918989181519 \n",
      "     Training Step: 209 Training Loss: 0.6112010478973389 \n",
      "     Training Step: 210 Training Loss: 0.6128241419792175 \n",
      "     Training Step: 211 Training Loss: 0.6203189492225647 \n",
      "     Training Step: 212 Training Loss: 0.6147028803825378 \n",
      "     Training Step: 213 Training Loss: 0.6124750375747681 \n",
      "     Training Step: 214 Training Loss: 0.6127668023109436 \n",
      "     Training Step: 215 Training Loss: 0.6171133518218994 \n",
      "     Training Step: 216 Training Loss: 0.6168107986450195 \n",
      "     Training Step: 217 Training Loss: 0.6115067005157471 \n",
      "     Training Step: 218 Training Loss: 0.6140323281288147 \n",
      "     Training Step: 219 Training Loss: 0.614076554775238 \n",
      "     Training Step: 220 Training Loss: 0.6147245764732361 \n",
      "     Training Step: 221 Training Loss: 0.6125730276107788 \n",
      "     Training Step: 222 Training Loss: 0.6176825165748596 \n",
      "     Training Step: 223 Training Loss: 0.6112220287322998 \n",
      "     Training Step: 224 Training Loss: 0.6138367056846619 \n",
      "     Training Step: 225 Training Loss: 0.6209251284599304 \n",
      "     Training Step: 226 Training Loss: 0.6140686869621277 \n",
      "     Training Step: 227 Training Loss: 0.6150889992713928 \n",
      "     Training Step: 228 Training Loss: 0.6151895523071289 \n",
      "     Training Step: 229 Training Loss: 0.6106103658676147 \n",
      "     Training Step: 230 Training Loss: 0.6097220778465271 \n",
      "     Training Step: 231 Training Loss: 0.6158294081687927 \n",
      "     Training Step: 232 Training Loss: 0.6163613200187683 \n",
      "     Training Step: 233 Training Loss: 0.6116142272949219 \n",
      "     Training Step: 234 Training Loss: 0.6117775440216064 \n",
      "     Training Step: 235 Training Loss: 0.6142211556434631 \n",
      "     Training Step: 236 Training Loss: 0.6100357174873352 \n",
      "     Training Step: 237 Training Loss: 0.6146259307861328 \n",
      "     Training Step: 238 Training Loss: 0.6123877167701721 \n",
      "     Training Step: 239 Training Loss: 0.6129258871078491 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.615344226360321 \n",
      "     Validation Step: 1 Validation Loss: 0.6148481369018555 \n",
      "     Validation Step: 2 Validation Loss: 0.6145440340042114 \n",
      "     Validation Step: 3 Validation Loss: 0.6116361618041992 \n",
      "     Validation Step: 4 Validation Loss: 0.6162800192832947 \n",
      "     Validation Step: 5 Validation Loss: 0.618118405342102 \n",
      "     Validation Step: 6 Validation Loss: 0.614128589630127 \n",
      "     Validation Step: 7 Validation Loss: 0.617743968963623 \n",
      "     Validation Step: 8 Validation Loss: 0.610086977481842 \n",
      "     Validation Step: 9 Validation Loss: 0.6133064031600952 \n",
      "     Validation Step: 10 Validation Loss: 0.6156439781188965 \n",
      "     Validation Step: 11 Validation Loss: 0.615610659122467 \n",
      "     Validation Step: 12 Validation Loss: 0.6150597929954529 \n",
      "     Validation Step: 13 Validation Loss: 0.6136394739151001 \n",
      "     Validation Step: 14 Validation Loss: 0.6146487593650818 \n",
      "     Validation Step: 15 Validation Loss: 0.617354154586792 \n",
      "     Validation Step: 16 Validation Loss: 0.6182920932769775 \n",
      "     Validation Step: 17 Validation Loss: 0.6104968190193176 \n",
      "     Validation Step: 18 Validation Loss: 0.6136721968650818 \n",
      "     Validation Step: 19 Validation Loss: 0.6141241788864136 \n",
      "     Validation Step: 20 Validation Loss: 0.6115474700927734 \n",
      "     Validation Step: 21 Validation Loss: 0.6121231913566589 \n",
      "     Validation Step: 22 Validation Loss: 0.6104655861854553 \n",
      "     Validation Step: 23 Validation Loss: 0.6152488589286804 \n",
      "     Validation Step: 24 Validation Loss: 0.6106268167495728 \n",
      "     Validation Step: 25 Validation Loss: 0.6118727922439575 \n",
      "     Validation Step: 26 Validation Loss: 0.6111495494842529 \n",
      "     Validation Step: 27 Validation Loss: 0.6145955920219421 \n",
      "     Validation Step: 28 Validation Loss: 0.6111716628074646 \n",
      "     Validation Step: 29 Validation Loss: 0.6136686205863953 \n",
      "     Validation Step: 30 Validation Loss: 0.618398129940033 \n",
      "     Validation Step: 31 Validation Loss: 0.6101529002189636 \n",
      "     Validation Step: 32 Validation Loss: 0.6101239323616028 \n",
      "     Validation Step: 33 Validation Loss: 0.6142662763595581 \n",
      "     Validation Step: 34 Validation Loss: 0.6158234477043152 \n",
      "     Validation Step: 35 Validation Loss: 0.6185550689697266 \n",
      "     Validation Step: 36 Validation Loss: 0.6160402894020081 \n",
      "     Validation Step: 37 Validation Loss: 0.6170732975006104 \n",
      "     Validation Step: 38 Validation Loss: 0.6149160861968994 \n",
      "     Validation Step: 39 Validation Loss: 0.6176407337188721 \n",
      "     Validation Step: 40 Validation Loss: 0.6074857711791992 \n",
      "     Validation Step: 41 Validation Loss: 0.6142129898071289 \n",
      "     Validation Step: 42 Validation Loss: 0.6185290217399597 \n",
      "     Validation Step: 43 Validation Loss: 0.6128224730491638 \n",
      "     Validation Step: 44 Validation Loss: 0.61298006772995 \n",
      "Epoch: 74\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6115773320198059 \n",
      "     Training Step: 1 Training Loss: 0.6149618625640869 \n",
      "     Training Step: 2 Training Loss: 0.6154118180274963 \n",
      "     Training Step: 3 Training Loss: 0.616801917552948 \n",
      "     Training Step: 4 Training Loss: 0.6147371530532837 \n",
      "     Training Step: 5 Training Loss: 0.6122635006904602 \n",
      "     Training Step: 6 Training Loss: 0.6164219379425049 \n",
      "     Training Step: 7 Training Loss: 0.6146246194839478 \n",
      "     Training Step: 8 Training Loss: 0.6146126985549927 \n",
      "     Training Step: 9 Training Loss: 0.6161249876022339 \n",
      "     Training Step: 10 Training Loss: 0.612428605556488 \n",
      "     Training Step: 11 Training Loss: 0.6150940656661987 \n",
      "     Training Step: 12 Training Loss: 0.6171151995658875 \n",
      "     Training Step: 13 Training Loss: 0.616823673248291 \n",
      "     Training Step: 14 Training Loss: 0.6097769141197205 \n",
      "     Training Step: 15 Training Loss: 0.6083052158355713 \n",
      "     Training Step: 16 Training Loss: 0.6114117503166199 \n",
      "     Training Step: 17 Training Loss: 0.6153497695922852 \n",
      "     Training Step: 18 Training Loss: 0.6182260513305664 \n",
      "     Training Step: 19 Training Loss: 0.6162777543067932 \n",
      "     Training Step: 20 Training Loss: 0.6165151000022888 \n",
      "     Training Step: 21 Training Loss: 0.6148396134376526 \n",
      "     Training Step: 22 Training Loss: 0.6119118928909302 \n",
      "     Training Step: 23 Training Loss: 0.6121042966842651 \n",
      "     Training Step: 24 Training Loss: 0.6169405579566956 \n",
      "     Training Step: 25 Training Loss: 0.6195001006126404 \n",
      "     Training Step: 26 Training Loss: 0.6106982827186584 \n",
      "     Training Step: 27 Training Loss: 0.6124803423881531 \n",
      "     Training Step: 28 Training Loss: 0.6116567254066467 \n",
      "     Training Step: 29 Training Loss: 0.6135129928588867 \n",
      "     Training Step: 30 Training Loss: 0.6142197847366333 \n",
      "     Training Step: 31 Training Loss: 0.6132770776748657 \n",
      "     Training Step: 32 Training Loss: 0.6184219717979431 \n",
      "     Training Step: 33 Training Loss: 0.6106155514717102 \n",
      "     Training Step: 34 Training Loss: 0.6134313344955444 \n",
      "     Training Step: 35 Training Loss: 0.6115643978118896 \n",
      "     Training Step: 36 Training Loss: 0.6168304681777954 \n",
      "     Training Step: 37 Training Loss: 0.6144976019859314 \n",
      "     Training Step: 38 Training Loss: 0.6122919917106628 \n",
      "     Training Step: 39 Training Loss: 0.6121749877929688 \n",
      "     Training Step: 40 Training Loss: 0.6159734725952148 \n",
      "     Training Step: 41 Training Loss: 0.6139224767684937 \n",
      "     Training Step: 42 Training Loss: 0.6209568977355957 \n",
      "     Training Step: 43 Training Loss: 0.6146354079246521 \n",
      "     Training Step: 44 Training Loss: 0.6173864603042603 \n",
      "     Training Step: 45 Training Loss: 0.6184380054473877 \n",
      "     Training Step: 46 Training Loss: 0.6129618883132935 \n",
      "     Training Step: 47 Training Loss: 0.6153749227523804 \n",
      "     Training Step: 48 Training Loss: 0.6185781955718994 \n",
      "     Training Step: 49 Training Loss: 0.6171621680259705 \n",
      "     Training Step: 50 Training Loss: 0.6132454872131348 \n",
      "     Training Step: 51 Training Loss: 0.6101506948471069 \n",
      "     Training Step: 52 Training Loss: 0.6107501983642578 \n",
      "     Training Step: 53 Training Loss: 0.6177342534065247 \n",
      "     Training Step: 54 Training Loss: 0.6186790466308594 \n",
      "     Training Step: 55 Training Loss: 0.6114770174026489 \n",
      "     Training Step: 56 Training Loss: 0.615868330001831 \n",
      "     Training Step: 57 Training Loss: 0.6177369356155396 \n",
      "     Training Step: 58 Training Loss: 0.6130056977272034 \n",
      "     Training Step: 59 Training Loss: 0.6168228387832642 \n",
      "     Training Step: 60 Training Loss: 0.615807056427002 \n",
      "     Training Step: 61 Training Loss: 0.6123576760292053 \n",
      "     Training Step: 62 Training Loss: 0.6166971325874329 \n",
      "     Training Step: 63 Training Loss: 0.6116634607315063 \n",
      "     Training Step: 64 Training Loss: 0.616662323474884 \n",
      "     Training Step: 65 Training Loss: 0.6150953769683838 \n",
      "     Training Step: 66 Training Loss: 0.6129334568977356 \n",
      "     Training Step: 67 Training Loss: 0.6139113903045654 \n",
      "     Training Step: 68 Training Loss: 0.6138410568237305 \n",
      "     Training Step: 69 Training Loss: 0.619646430015564 \n",
      "     Training Step: 70 Training Loss: 0.6114640235900879 \n",
      "     Training Step: 71 Training Loss: 0.6177965402603149 \n",
      "     Training Step: 72 Training Loss: 0.6125699281692505 \n",
      "     Training Step: 73 Training Loss: 0.6128591299057007 \n",
      "     Training Step: 74 Training Loss: 0.6144557595252991 \n",
      "     Training Step: 75 Training Loss: 0.615177571773529 \n",
      "     Training Step: 76 Training Loss: 0.6155527830123901 \n",
      "     Training Step: 77 Training Loss: 0.6101832389831543 \n",
      "     Training Step: 78 Training Loss: 0.6155391335487366 \n",
      "     Training Step: 79 Training Loss: 0.6128183007240295 \n",
      "     Training Step: 80 Training Loss: 0.6118391156196594 \n",
      "     Training Step: 81 Training Loss: 0.6103946566581726 \n",
      "     Training Step: 82 Training Loss: 0.6113921999931335 \n",
      "     Training Step: 83 Training Loss: 0.6169698238372803 \n",
      "     Training Step: 84 Training Loss: 0.6111345291137695 \n",
      "     Training Step: 85 Training Loss: 0.6146579384803772 \n",
      "     Training Step: 86 Training Loss: 0.6114341616630554 \n",
      "     Training Step: 87 Training Loss: 0.6146686673164368 \n",
      "     Training Step: 88 Training Loss: 0.6115288734436035 \n",
      "     Training Step: 89 Training Loss: 0.6171614527702332 \n",
      "     Training Step: 90 Training Loss: 0.6154289841651917 \n",
      "     Training Step: 91 Training Loss: 0.612713098526001 \n",
      "     Training Step: 92 Training Loss: 0.6141115427017212 \n",
      "     Training Step: 93 Training Loss: 0.6133107542991638 \n",
      "     Training Step: 94 Training Loss: 0.6131203174591064 \n",
      "     Training Step: 95 Training Loss: 0.6094509959220886 \n",
      "     Training Step: 96 Training Loss: 0.6118080019950867 \n",
      "     Training Step: 97 Training Loss: 0.6168720722198486 \n",
      "     Training Step: 98 Training Loss: 0.6130927205085754 \n",
      "     Training Step: 99 Training Loss: 0.6130583882331848 \n",
      "     Training Step: 100 Training Loss: 0.6132065057754517 \n",
      "     Training Step: 101 Training Loss: 0.6146957874298096 \n",
      "     Training Step: 102 Training Loss: 0.6142532229423523 \n",
      "     Training Step: 103 Training Loss: 0.6174978017807007 \n",
      "     Training Step: 104 Training Loss: 0.6140239834785461 \n",
      "     Training Step: 105 Training Loss: 0.612261950969696 \n",
      "     Training Step: 106 Training Loss: 0.6143395304679871 \n",
      "     Training Step: 107 Training Loss: 0.6116283535957336 \n",
      "     Training Step: 108 Training Loss: 0.6118306517601013 \n",
      "     Training Step: 109 Training Loss: 0.617217481136322 \n",
      "     Training Step: 110 Training Loss: 0.6136025190353394 \n",
      "     Training Step: 111 Training Loss: 0.6154239773750305 \n",
      "     Training Step: 112 Training Loss: 0.6188566088676453 \n",
      "     Training Step: 113 Training Loss: 0.610177755355835 \n",
      "     Training Step: 114 Training Loss: 0.6184194684028625 \n",
      "     Training Step: 115 Training Loss: 0.614439845085144 \n",
      "     Training Step: 116 Training Loss: 0.6115848422050476 \n",
      "     Training Step: 117 Training Loss: 0.6151126027107239 \n",
      "     Training Step: 118 Training Loss: 0.6136474609375 \n",
      "     Training Step: 119 Training Loss: 0.6133180856704712 \n",
      "     Training Step: 120 Training Loss: 0.6137431263923645 \n",
      "     Training Step: 121 Training Loss: 0.61187744140625 \n",
      "     Training Step: 122 Training Loss: 0.6104753017425537 \n",
      "     Training Step: 123 Training Loss: 0.6160606741905212 \n",
      "     Training Step: 124 Training Loss: 0.6182887554168701 \n",
      "     Training Step: 125 Training Loss: 0.6121646165847778 \n",
      "     Training Step: 126 Training Loss: 0.6144940257072449 \n",
      "     Training Step: 127 Training Loss: 0.6155484318733215 \n",
      "     Training Step: 128 Training Loss: 0.6178003549575806 \n",
      "     Training Step: 129 Training Loss: 0.6118307709693909 \n",
      "     Training Step: 130 Training Loss: 0.6106539964675903 \n",
      "     Training Step: 131 Training Loss: 0.6106261610984802 \n",
      "     Training Step: 132 Training Loss: 0.6167961955070496 \n",
      "     Training Step: 133 Training Loss: 0.6198073029518127 \n",
      "     Training Step: 134 Training Loss: 0.6126511096954346 \n",
      "     Training Step: 135 Training Loss: 0.6156960725784302 \n",
      "     Training Step: 136 Training Loss: 0.6135565638542175 \n",
      "     Training Step: 137 Training Loss: 0.614785373210907 \n",
      "     Training Step: 138 Training Loss: 0.6141117811203003 \n",
      "     Training Step: 139 Training Loss: 0.6147798895835876 \n",
      "     Training Step: 140 Training Loss: 0.6133624911308289 \n",
      "     Training Step: 141 Training Loss: 0.6116932034492493 \n",
      "     Training Step: 142 Training Loss: 0.6128872036933899 \n",
      "     Training Step: 143 Training Loss: 0.614393413066864 \n",
      "     Training Step: 144 Training Loss: 0.6154810786247253 \n",
      "     Training Step: 145 Training Loss: 0.6114581823348999 \n",
      "     Training Step: 146 Training Loss: 0.6184081435203552 \n",
      "     Training Step: 147 Training Loss: 0.6160129904747009 \n",
      "     Training Step: 148 Training Loss: 0.6153450012207031 \n",
      "     Training Step: 149 Training Loss: 0.6156987547874451 \n",
      "     Training Step: 150 Training Loss: 0.6097897291183472 \n",
      "     Training Step: 151 Training Loss: 0.613215982913971 \n",
      "     Training Step: 152 Training Loss: 0.6121902465820312 \n",
      "     Training Step: 153 Training Loss: 0.6200506687164307 \n",
      "     Training Step: 154 Training Loss: 0.6178354620933533 \n",
      "     Training Step: 155 Training Loss: 0.6092754602432251 \n",
      "     Training Step: 156 Training Loss: 0.618217945098877 \n",
      "     Training Step: 157 Training Loss: 0.6139695644378662 \n",
      "     Training Step: 158 Training Loss: 0.6124464273452759 \n",
      "     Training Step: 159 Training Loss: 0.613488495349884 \n",
      "     Training Step: 160 Training Loss: 0.6147245168685913 \n",
      "     Training Step: 161 Training Loss: 0.6162856817245483 \n",
      "     Training Step: 162 Training Loss: 0.6167352199554443 \n",
      "     Training Step: 163 Training Loss: 0.6157741546630859 \n",
      "     Training Step: 164 Training Loss: 0.6151941418647766 \n",
      "     Training Step: 165 Training Loss: 0.6125810146331787 \n",
      "     Training Step: 166 Training Loss: 0.6147302389144897 \n",
      "     Training Step: 167 Training Loss: 0.6157695651054382 \n",
      "     Training Step: 168 Training Loss: 0.6127828359603882 \n",
      "     Training Step: 169 Training Loss: 0.6101114749908447 \n",
      "     Training Step: 170 Training Loss: 0.6164737343788147 \n",
      "     Training Step: 171 Training Loss: 0.6096744537353516 \n",
      "     Training Step: 172 Training Loss: 0.6115537881851196 \n",
      "     Training Step: 173 Training Loss: 0.6133588552474976 \n",
      "     Training Step: 174 Training Loss: 0.6178330183029175 \n",
      "     Training Step: 175 Training Loss: 0.6180912852287292 \n",
      "     Training Step: 176 Training Loss: 0.6188623905181885 \n",
      "     Training Step: 177 Training Loss: 0.6097024083137512 \n",
      "     Training Step: 178 Training Loss: 0.6122419834136963 \n",
      "     Training Step: 179 Training Loss: 0.6202009916305542 \n",
      "     Training Step: 180 Training Loss: 0.618027925491333 \n",
      "     Training Step: 181 Training Loss: 0.6167241930961609 \n",
      "     Training Step: 182 Training Loss: 0.6125519275665283 \n",
      "     Training Step: 183 Training Loss: 0.6153849363327026 \n",
      "     Training Step: 184 Training Loss: 0.6121845841407776 \n",
      "     Training Step: 185 Training Loss: 0.6131818890571594 \n",
      "     Training Step: 186 Training Loss: 0.6137773990631104 \n",
      "     Training Step: 187 Training Loss: 0.6133363246917725 \n",
      "     Training Step: 188 Training Loss: 0.6140511631965637 \n",
      "     Training Step: 189 Training Loss: 0.6124946475028992 \n",
      "     Training Step: 190 Training Loss: 0.6144129037857056 \n",
      "     Training Step: 191 Training Loss: 0.6167781352996826 \n",
      "     Training Step: 192 Training Loss: 0.6154088377952576 \n",
      "     Training Step: 193 Training Loss: 0.6112194657325745 \n",
      "     Training Step: 194 Training Loss: 0.6127865314483643 \n",
      "     Training Step: 195 Training Loss: 0.6181387305259705 \n",
      "     Training Step: 196 Training Loss: 0.614215612411499 \n",
      "     Training Step: 197 Training Loss: 0.6149725317955017 \n",
      "     Training Step: 198 Training Loss: 0.61492919921875 \n",
      "     Training Step: 199 Training Loss: 0.6146790981292725 \n",
      "     Training Step: 200 Training Loss: 0.6140338182449341 \n",
      "     Training Step: 201 Training Loss: 0.61487877368927 \n",
      "     Training Step: 202 Training Loss: 0.6153086423873901 \n",
      "     Training Step: 203 Training Loss: 0.6141616106033325 \n",
      "     Training Step: 204 Training Loss: 0.6152492165565491 \n",
      "     Training Step: 205 Training Loss: 0.6152771711349487 \n",
      "     Training Step: 206 Training Loss: 0.6136610507965088 \n",
      "     Training Step: 207 Training Loss: 0.6146922707557678 \n",
      "     Training Step: 208 Training Loss: 0.6122359037399292 \n",
      "     Training Step: 209 Training Loss: 0.6116161346435547 \n",
      "     Training Step: 210 Training Loss: 0.6158037185668945 \n",
      "     Training Step: 211 Training Loss: 0.6141748428344727 \n",
      "     Training Step: 212 Training Loss: 0.6176678538322449 \n",
      "     Training Step: 213 Training Loss: 0.6128901243209839 \n",
      "     Training Step: 214 Training Loss: 0.6143060922622681 \n",
      "     Training Step: 215 Training Loss: 0.6124439239501953 \n",
      "     Training Step: 216 Training Loss: 0.6146814823150635 \n",
      "     Training Step: 217 Training Loss: 0.6154817342758179 \n",
      "     Training Step: 218 Training Loss: 0.6099798083305359 \n",
      "     Training Step: 219 Training Loss: 0.616411566734314 \n",
      "     Training Step: 220 Training Loss: 0.6109166741371155 \n",
      "     Training Step: 221 Training Loss: 0.6101242303848267 \n",
      "     Training Step: 222 Training Loss: 0.6156520843505859 \n",
      "     Training Step: 223 Training Loss: 0.6172474026679993 \n",
      "     Training Step: 224 Training Loss: 0.6167346835136414 \n",
      "     Training Step: 225 Training Loss: 0.6112511157989502 \n",
      "     Training Step: 226 Training Loss: 0.6144092082977295 \n",
      "     Training Step: 227 Training Loss: 0.6106359362602234 \n",
      "     Training Step: 228 Training Loss: 0.6136468648910522 \n",
      "     Training Step: 229 Training Loss: 0.6118562817573547 \n",
      "     Training Step: 230 Training Loss: 0.6138020753860474 \n",
      "     Training Step: 231 Training Loss: 0.6106955409049988 \n",
      "     Training Step: 232 Training Loss: 0.6108649969100952 \n",
      "     Training Step: 233 Training Loss: 0.6107368469238281 \n",
      "     Training Step: 234 Training Loss: 0.6167248487472534 \n",
      "     Training Step: 235 Training Loss: 0.6145177483558655 \n",
      "     Training Step: 236 Training Loss: 0.6163056492805481 \n",
      "     Training Step: 237 Training Loss: 0.6104809045791626 \n",
      "     Training Step: 238 Training Loss: 0.6105668544769287 \n",
      "     Training Step: 239 Training Loss: 0.6134819388389587 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6101449131965637 \n",
      "     Validation Step: 1 Validation Loss: 0.6137235760688782 \n",
      "     Validation Step: 2 Validation Loss: 0.6171245574951172 \n",
      "     Validation Step: 3 Validation Loss: 0.6163153648376465 \n",
      "     Validation Step: 4 Validation Loss: 0.611922025680542 \n",
      "     Validation Step: 5 Validation Loss: 0.6186127662658691 \n",
      "     Validation Step: 6 Validation Loss: 0.6177093386650085 \n",
      "     Validation Step: 7 Validation Loss: 0.6075195670127869 \n",
      "     Validation Step: 8 Validation Loss: 0.6184314489364624 \n",
      "     Validation Step: 9 Validation Loss: 0.6158719658851624 \n",
      "     Validation Step: 10 Validation Loss: 0.6106700897216797 \n",
      "     Validation Step: 11 Validation Loss: 0.6174238920211792 \n",
      "     Validation Step: 12 Validation Loss: 0.613705039024353 \n",
      "     Validation Step: 13 Validation Loss: 0.6153754591941833 \n",
      "     Validation Step: 14 Validation Loss: 0.6141843199729919 \n",
      "     Validation Step: 15 Validation Loss: 0.6142587661743164 \n",
      "     Validation Step: 16 Validation Loss: 0.6141633987426758 \n",
      "     Validation Step: 17 Validation Loss: 0.6149725914001465 \n",
      "     Validation Step: 18 Validation Loss: 0.6156472563743591 \n",
      "     Validation Step: 19 Validation Loss: 0.6133657693862915 \n",
      "     Validation Step: 20 Validation Loss: 0.6153005957603455 \n",
      "     Validation Step: 21 Validation Loss: 0.6128886342048645 \n",
      "     Validation Step: 22 Validation Loss: 0.6101030111312866 \n",
      "     Validation Step: 23 Validation Loss: 0.61460280418396 \n",
      "     Validation Step: 24 Validation Loss: 0.6121687889099121 \n",
      "     Validation Step: 25 Validation Loss: 0.6102100014686584 \n",
      "     Validation Step: 26 Validation Loss: 0.615683376789093 \n",
      "     Validation Step: 27 Validation Loss: 0.6183297038078308 \n",
      "     Validation Step: 28 Validation Loss: 0.6105363368988037 \n",
      "     Validation Step: 29 Validation Loss: 0.6150844693183899 \n",
      "     Validation Step: 30 Validation Loss: 0.6177895665168762 \n",
      "     Validation Step: 31 Validation Loss: 0.6146557927131653 \n",
      "     Validation Step: 32 Validation Loss: 0.6185990571975708 \n",
      "     Validation Step: 33 Validation Loss: 0.6160703301429749 \n",
      "     Validation Step: 34 Validation Loss: 0.614905834197998 \n",
      "     Validation Step: 35 Validation Loss: 0.6143255233764648 \n",
      "     Validation Step: 36 Validation Loss: 0.6146897077560425 \n",
      "     Validation Step: 37 Validation Loss: 0.6136877536773682 \n",
      "     Validation Step: 38 Validation Loss: 0.6130232214927673 \n",
      "     Validation Step: 39 Validation Loss: 0.6181660294532776 \n",
      "     Validation Step: 40 Validation Loss: 0.6115824580192566 \n",
      "     Validation Step: 41 Validation Loss: 0.6111815571784973 \n",
      "     Validation Step: 42 Validation Loss: 0.6105016469955444 \n",
      "     Validation Step: 43 Validation Loss: 0.6112107038497925 \n",
      "     Validation Step: 44 Validation Loss: 0.6116905212402344 \n"
     ]
    }
   ],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 1\n",
    "num_epochs = 75\n",
    "loss_weights = (1.0, 1.0, 1.0)\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF_chained(None,None, embedder_model,ACOPF_optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6210901737213135 \n",
      "     Training Step: 1 Training Loss: 0.6127399206161499 \n",
      "     Training Step: 2 Training Loss: 0.6145259141921997 \n",
      "     Training Step: 3 Training Loss: 0.619731068611145 \n",
      "     Training Step: 4 Training Loss: 0.6151012182235718 \n",
      "     Training Step: 5 Training Loss: 0.6166698932647705 \n",
      "     Training Step: 6 Training Loss: 0.6152848601341248 \n",
      "     Training Step: 7 Training Loss: 0.6185854077339172 \n",
      "     Training Step: 8 Training Loss: 0.6166726350784302 \n",
      "     Training Step: 9 Training Loss: 0.6107650399208069 \n",
      "     Training Step: 10 Training Loss: 0.6117033362388611 \n",
      "     Training Step: 11 Training Loss: 0.6114595532417297 \n",
      "     Training Step: 12 Training Loss: 0.6180527210235596 \n",
      "     Training Step: 13 Training Loss: 0.6164128184318542 \n",
      "     Training Step: 14 Training Loss: 0.6140622496604919 \n",
      "     Training Step: 15 Training Loss: 0.6106172800064087 \n",
      "     Training Step: 16 Training Loss: 0.6177979707717896 \n",
      "     Training Step: 17 Training Loss: 0.6135252714157104 \n",
      "     Training Step: 18 Training Loss: 0.6149168610572815 \n",
      "     Training Step: 19 Training Loss: 0.6143452525138855 \n",
      "     Training Step: 20 Training Loss: 0.6152998208999634 \n",
      "     Training Step: 21 Training Loss: 0.6105059385299683 \n",
      "     Training Step: 22 Training Loss: 0.6138883829116821 \n",
      "     Training Step: 23 Training Loss: 0.6105749607086182 \n",
      "     Training Step: 24 Training Loss: 0.6111608147621155 \n",
      "     Training Step: 25 Training Loss: 0.6130720376968384 \n",
      "     Training Step: 26 Training Loss: 0.6154041886329651 \n",
      "     Training Step: 27 Training Loss: 0.6141625642776489 \n",
      "     Training Step: 28 Training Loss: 0.6146173477172852 \n",
      "     Training Step: 29 Training Loss: 0.6128736734390259 \n",
      "     Training Step: 30 Training Loss: 0.6154570579528809 \n",
      "     Training Step: 31 Training Loss: 0.6154298186302185 \n",
      "     Training Step: 32 Training Loss: 0.6146760582923889 \n",
      "     Training Step: 33 Training Loss: 0.6152910590171814 \n",
      "     Training Step: 34 Training Loss: 0.6147283315658569 \n",
      "     Training Step: 35 Training Loss: 0.619446337223053 \n",
      "     Training Step: 36 Training Loss: 0.6176275610923767 \n",
      "     Training Step: 37 Training Loss: 0.617085337638855 \n",
      "     Training Step: 38 Training Loss: 0.6158370971679688 \n",
      "     Training Step: 39 Training Loss: 0.6121794581413269 \n",
      "     Training Step: 40 Training Loss: 0.6119289994239807 \n",
      "     Training Step: 41 Training Loss: 0.6129510998725891 \n",
      "     Training Step: 42 Training Loss: 0.6101418137550354 \n",
      "     Training Step: 43 Training Loss: 0.6176953911781311 \n",
      "     Training Step: 44 Training Loss: 0.6169126629829407 \n",
      "     Training Step: 45 Training Loss: 0.6155255436897278 \n",
      "     Training Step: 46 Training Loss: 0.6118658185005188 \n",
      "     Training Step: 47 Training Loss: 0.6146745085716248 \n",
      "     Training Step: 48 Training Loss: 0.6196160316467285 \n",
      "     Training Step: 49 Training Loss: 0.6155942678451538 \n",
      "     Training Step: 50 Training Loss: 0.6116272211074829 \n",
      "     Training Step: 51 Training Loss: 0.6153347492218018 \n",
      "     Training Step: 52 Training Loss: 0.6101511716842651 \n",
      "     Training Step: 53 Training Loss: 0.6114464998245239 \n",
      "     Training Step: 54 Training Loss: 0.6142116189002991 \n",
      "     Training Step: 55 Training Loss: 0.6162157654762268 \n",
      "     Training Step: 56 Training Loss: 0.6177717447280884 \n",
      "     Training Step: 57 Training Loss: 0.6137656569480896 \n",
      "     Training Step: 58 Training Loss: 0.6124876141548157 \n",
      "     Training Step: 59 Training Loss: 0.6115212440490723 \n",
      "     Training Step: 60 Training Loss: 0.6133121848106384 \n",
      "     Training Step: 61 Training Loss: 0.6092251539230347 \n",
      "     Training Step: 62 Training Loss: 0.6136161684989929 \n",
      "     Training Step: 63 Training Loss: 0.6182553172111511 \n",
      "     Training Step: 64 Training Loss: 0.6141133308410645 \n",
      "     Training Step: 65 Training Loss: 0.6100488305091858 \n",
      "     Training Step: 66 Training Loss: 0.612369179725647 \n",
      "     Training Step: 67 Training Loss: 0.6131128668785095 \n",
      "     Training Step: 68 Training Loss: 0.6151588559150696 \n",
      "     Training Step: 69 Training Loss: 0.6140892505645752 \n",
      "     Training Step: 70 Training Loss: 0.6144260764122009 \n",
      "     Training Step: 71 Training Loss: 0.6132028698921204 \n",
      "     Training Step: 72 Training Loss: 0.6105718612670898 \n",
      "     Training Step: 73 Training Loss: 0.6159603595733643 \n",
      "     Training Step: 74 Training Loss: 0.6145997643470764 \n",
      "     Training Step: 75 Training Loss: 0.6161191463470459 \n",
      "     Training Step: 76 Training Loss: 0.6147707104682922 \n",
      "     Training Step: 77 Training Loss: 0.6129683256149292 \n",
      "     Training Step: 78 Training Loss: 0.6146849393844604 \n",
      "     Training Step: 79 Training Loss: 0.6123796701431274 \n",
      "     Training Step: 80 Training Loss: 0.609411895275116 \n",
      "     Training Step: 81 Training Loss: 0.6157863140106201 \n",
      "     Training Step: 82 Training Loss: 0.6121221780776978 \n",
      "     Training Step: 83 Training Loss: 0.611412763595581 \n",
      "     Training Step: 84 Training Loss: 0.6144522428512573 \n",
      "     Training Step: 85 Training Loss: 0.6199272274971008 \n",
      "     Training Step: 86 Training Loss: 0.6139116883277893 \n",
      "     Training Step: 87 Training Loss: 0.6099873185157776 \n",
      "     Training Step: 88 Training Loss: 0.6168186664581299 \n",
      "     Training Step: 89 Training Loss: 0.6107357740402222 \n",
      "     Training Step: 90 Training Loss: 0.6146902441978455 \n",
      "     Training Step: 91 Training Loss: 0.6116849184036255 \n",
      "     Training Step: 92 Training Loss: 0.616254448890686 \n",
      "     Training Step: 93 Training Loss: 0.6134205460548401 \n",
      "     Training Step: 94 Training Loss: 0.6152716279029846 \n",
      "     Training Step: 95 Training Loss: 0.6120194792747498 \n",
      "     Training Step: 96 Training Loss: 0.6131830811500549 \n",
      "     Training Step: 97 Training Loss: 0.6125162839889526 \n",
      "     Training Step: 98 Training Loss: 0.6116407513618469 \n",
      "     Training Step: 99 Training Loss: 0.6122890710830688 \n",
      "     Training Step: 100 Training Loss: 0.6127516031265259 \n",
      "     Training Step: 101 Training Loss: 0.6107144355773926 \n",
      "     Training Step: 102 Training Loss: 0.6154319047927856 \n",
      "     Training Step: 103 Training Loss: 0.6155433654785156 \n",
      "     Training Step: 104 Training Loss: 0.6177080273628235 \n",
      "     Training Step: 105 Training Loss: 0.6124613285064697 \n",
      "     Training Step: 106 Training Loss: 0.6142428517341614 \n",
      "     Training Step: 107 Training Loss: 0.618607759475708 \n",
      "     Training Step: 108 Training Loss: 0.6097288131713867 \n",
      "     Training Step: 109 Training Loss: 0.616733193397522 \n",
      "     Training Step: 110 Training Loss: 0.6127662062644958 \n",
      "     Training Step: 111 Training Loss: 0.6151755452156067 \n",
      "     Training Step: 112 Training Loss: 0.6167603731155396 \n",
      "     Training Step: 113 Training Loss: 0.6173810362815857 \n",
      "     Training Step: 114 Training Loss: 0.6167116165161133 \n",
      "     Training Step: 115 Training Loss: 0.6144844889640808 \n",
      "     Training Step: 116 Training Loss: 0.6138291358947754 \n",
      "     Training Step: 117 Training Loss: 0.6147524118423462 \n",
      "     Training Step: 118 Training Loss: 0.6125505566596985 \n",
      "     Training Step: 119 Training Loss: 0.6143494248390198 \n",
      "     Training Step: 120 Training Loss: 0.6123140454292297 \n",
      "     Training Step: 121 Training Loss: 0.6116355061531067 \n",
      "     Training Step: 122 Training Loss: 0.6144473552703857 \n",
      "     Training Step: 123 Training Loss: 0.6167215704917908 \n",
      "     Training Step: 124 Training Loss: 0.6188439726829529 \n",
      "     Training Step: 125 Training Loss: 0.6147981286048889 \n",
      "     Training Step: 126 Training Loss: 0.6126485466957092 \n",
      "     Training Step: 127 Training Loss: 0.614867627620697 \n",
      "     Training Step: 128 Training Loss: 0.6146478652954102 \n",
      "     Training Step: 129 Training Loss: 0.6143361926078796 \n",
      "     Training Step: 130 Training Loss: 0.6150673031806946 \n",
      "     Training Step: 131 Training Loss: 0.6147049069404602 \n",
      "     Training Step: 132 Training Loss: 0.6097598671913147 \n",
      "     Training Step: 133 Training Loss: 0.610612154006958 \n",
      "     Training Step: 134 Training Loss: 0.6157441139221191 \n",
      "     Training Step: 135 Training Loss: 0.6114389896392822 \n",
      "     Training Step: 136 Training Loss: 0.6111410856246948 \n",
      "     Training Step: 137 Training Loss: 0.6106612086296082 \n",
      "     Training Step: 138 Training Loss: 0.6157667636871338 \n",
      "     Training Step: 139 Training Loss: 0.6136012077331543 \n",
      "     Training Step: 140 Training Loss: 0.6147177815437317 \n",
      "     Training Step: 141 Training Loss: 0.6178379654884338 \n",
      "     Training Step: 142 Training Loss: 0.6202617287635803 \n",
      "     Training Step: 143 Training Loss: 0.6136370897293091 \n",
      "     Training Step: 144 Training Loss: 0.6164957284927368 \n",
      "     Training Step: 145 Training Loss: 0.6094733476638794 \n",
      "     Training Step: 146 Training Loss: 0.6123881936073303 \n",
      "     Training Step: 147 Training Loss: 0.6174859404563904 \n",
      "     Training Step: 148 Training Loss: 0.6164136528968811 \n",
      "     Training Step: 149 Training Loss: 0.6125426292419434 \n",
      "     Training Step: 150 Training Loss: 0.6159968376159668 \n",
      "     Training Step: 151 Training Loss: 0.6115784049034119 \n",
      "     Training Step: 152 Training Loss: 0.6139284372329712 \n",
      "     Training Step: 153 Training Loss: 0.6134624481201172 \n",
      "     Training Step: 154 Training Loss: 0.6182304620742798 \n",
      "     Training Step: 155 Training Loss: 0.6136484742164612 \n",
      "     Training Step: 156 Training Loss: 0.6152387857437134 \n",
      "     Training Step: 157 Training Loss: 0.6133561134338379 \n",
      "     Training Step: 158 Training Loss: 0.6130592823028564 \n",
      "     Training Step: 159 Training Loss: 0.6114184856414795 \n",
      "     Training Step: 160 Training Loss: 0.6117820739746094 \n",
      "     Training Step: 161 Training Loss: 0.6101747751235962 \n",
      "     Training Step: 162 Training Loss: 0.6108944416046143 \n",
      "     Training Step: 163 Training Loss: 0.6118308305740356 \n",
      "     Training Step: 164 Training Loss: 0.6117837429046631 \n",
      "     Training Step: 165 Training Loss: 0.6141604781150818 \n",
      "     Training Step: 166 Training Loss: 0.6128485798835754 \n",
      "     Training Step: 167 Training Loss: 0.6137579083442688 \n",
      "     Training Step: 168 Training Loss: 0.6149808764457703 \n",
      "     Training Step: 169 Training Loss: 0.6155006289482117 \n",
      "     Training Step: 170 Training Loss: 0.6121513843536377 \n",
      "     Training Step: 171 Training Loss: 0.6135047674179077 \n",
      "     Training Step: 172 Training Loss: 0.6184573769569397 \n",
      "     Training Step: 173 Training Loss: 0.6131313443183899 \n",
      "     Training Step: 174 Training Loss: 0.6118064522743225 \n",
      "     Training Step: 175 Training Loss: 0.6169018149375916 \n",
      "     Training Step: 176 Training Loss: 0.6146370768547058 \n",
      "     Training Step: 177 Training Loss: 0.6166528463363647 \n",
      "     Training Step: 178 Training Loss: 0.612927258014679 \n",
      "     Training Step: 179 Training Loss: 0.6180318593978882 \n",
      "     Training Step: 180 Training Loss: 0.6143786907196045 \n",
      "     Training Step: 181 Training Loss: 0.6134923696517944 \n",
      "     Training Step: 182 Training Loss: 0.614940345287323 \n",
      "     Training Step: 183 Training Loss: 0.6184483170509338 \n",
      "     Training Step: 184 Training Loss: 0.6171280741691589 \n",
      "     Training Step: 185 Training Loss: 0.6142910122871399 \n",
      "     Training Step: 186 Training Loss: 0.6142359375953674 \n",
      "     Training Step: 187 Training Loss: 0.6109321713447571 \n",
      "     Training Step: 188 Training Loss: 0.6112293004989624 \n",
      "     Training Step: 189 Training Loss: 0.6116126179695129 \n",
      "     Training Step: 190 Training Loss: 0.6105198264122009 \n",
      "     Training Step: 191 Training Loss: 0.6140249967575073 \n",
      "     Training Step: 192 Training Loss: 0.615383505821228 \n",
      "     Training Step: 193 Training Loss: 0.618135392665863 \n",
      "     Training Step: 194 Training Loss: 0.6133067607879639 \n",
      "     Training Step: 195 Training Loss: 0.6146243810653687 \n",
      "     Training Step: 196 Training Loss: 0.613737940788269 \n",
      "     Training Step: 197 Training Loss: 0.6121450066566467 \n",
      "     Training Step: 198 Training Loss: 0.6122347116470337 \n",
      "     Training Step: 199 Training Loss: 0.6155388355255127 \n",
      "     Training Step: 200 Training Loss: 0.6140204668045044 \n",
      "     Training Step: 201 Training Loss: 0.616219162940979 \n",
      "     Training Step: 202 Training Loss: 0.6132053732872009 \n",
      "     Training Step: 203 Training Loss: 0.6122308969497681 \n",
      "     Training Step: 204 Training Loss: 0.612068235874176 \n",
      "     Training Step: 205 Training Loss: 0.618446946144104 \n",
      "     Training Step: 206 Training Loss: 0.6132945418357849 \n",
      "     Training Step: 207 Training Loss: 0.6183252930641174 \n",
      "     Training Step: 208 Training Loss: 0.6168031096458435 \n",
      "     Training Step: 209 Training Loss: 0.611503005027771 \n",
      "     Training Step: 210 Training Loss: 0.6083314418792725 \n",
      "     Training Step: 211 Training Loss: 0.6097521185874939 \n",
      "     Training Step: 212 Training Loss: 0.6132947206497192 \n",
      "     Training Step: 213 Training Loss: 0.6180212497711182 \n",
      "     Training Step: 214 Training Loss: 0.6167113780975342 \n",
      "     Training Step: 215 Training Loss: 0.6128634214401245 \n",
      "     Training Step: 216 Training Loss: 0.6177011728286743 \n",
      "     Training Step: 217 Training Loss: 0.6171708703041077 \n",
      "     Training Step: 218 Training Loss: 0.6104087233543396 \n",
      "     Training Step: 219 Training Loss: 0.6132884621620178 \n",
      "     Training Step: 220 Training Loss: 0.6103811264038086 \n",
      "     Training Step: 221 Training Loss: 0.617202877998352 \n",
      "     Training Step: 222 Training Loss: 0.6166285872459412 \n",
      "     Training Step: 223 Training Loss: 0.6115345358848572 \n",
      "     Training Step: 224 Training Loss: 0.6160404682159424 \n",
      "     Training Step: 225 Training Loss: 0.6156889796257019 \n",
      "     Training Step: 226 Training Loss: 0.6153940558433533 \n",
      "     Training Step: 227 Training Loss: 0.6171374917030334 \n",
      "     Training Step: 228 Training Loss: 0.6163514256477356 \n",
      "     Training Step: 229 Training Loss: 0.6188588738441467 \n",
      "     Training Step: 230 Training Loss: 0.6157440543174744 \n",
      "     Training Step: 231 Training Loss: 0.6150978803634644 \n",
      "     Training Step: 232 Training Loss: 0.6122509241104126 \n",
      "     Training Step: 233 Training Loss: 0.6101076006889343 \n",
      "     Training Step: 234 Training Loss: 0.6128260493278503 \n",
      "     Training Step: 235 Training Loss: 0.6118494272232056 \n",
      "     Training Step: 236 Training Loss: 0.6156826019287109 \n",
      "     Training Step: 237 Training Loss: 0.6166532635688782 \n",
      "     Training Step: 238 Training Loss: 0.6115348935127258 \n",
      "     Training Step: 239 Training Loss: 0.6168050169944763 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6142615675926208 \n",
      "     Validation Step: 1 Validation Loss: 0.6162497401237488 \n",
      "     Validation Step: 2 Validation Loss: 0.6129843592643738 \n",
      "     Validation Step: 3 Validation Loss: 0.6141191720962524 \n",
      "     Validation Step: 4 Validation Loss: 0.6105161309242249 \n",
      "     Validation Step: 5 Validation Loss: 0.615319013595581 \n",
      "     Validation Step: 6 Validation Loss: 0.6136686205863953 \n",
      "     Validation Step: 7 Validation Loss: 0.607529878616333 \n",
      "     Validation Step: 8 Validation Loss: 0.6145773530006409 \n",
      "     Validation Step: 9 Validation Loss: 0.6101412773132324 \n",
      "     Validation Step: 10 Validation Loss: 0.6185116171836853 \n",
      "     Validation Step: 11 Validation Loss: 0.6136458516120911 \n",
      "     Validation Step: 12 Validation Loss: 0.6104840636253357 \n",
      "     Validation Step: 13 Validation Loss: 0.6116467118263245 \n",
      "     Validation Step: 14 Validation Loss: 0.6148996353149414 \n",
      "     Validation Step: 15 Validation Loss: 0.6106367111206055 \n",
      "     Validation Step: 16 Validation Loss: 0.6182478666305542 \n",
      "     Validation Step: 17 Validation Loss: 0.6101746559143066 \n",
      "     Validation Step: 18 Validation Loss: 0.6121293306350708 \n",
      "     Validation Step: 19 Validation Loss: 0.617708146572113 \n",
      "     Validation Step: 20 Validation Loss: 0.6173185110092163 \n",
      "     Validation Step: 21 Validation Loss: 0.6133103370666504 \n",
      "     Validation Step: 22 Validation Loss: 0.6160074472427368 \n",
      "     Validation Step: 23 Validation Loss: 0.6128350496292114 \n",
      "     Validation Step: 24 Validation Loss: 0.6145408153533936 \n",
      "     Validation Step: 25 Validation Loss: 0.6155834794044495 \n",
      "     Validation Step: 26 Validation Loss: 0.6118830442428589 \n",
      "     Validation Step: 27 Validation Loss: 0.6142025589942932 \n",
      "     Validation Step: 28 Validation Loss: 0.6176092624664307 \n",
      "     Validation Step: 29 Validation Loss: 0.617034375667572 \n",
      "     Validation Step: 30 Validation Loss: 0.6146397590637207 \n",
      "     Validation Step: 31 Validation Loss: 0.615803062915802 \n",
      "     Validation Step: 32 Validation Loss: 0.6136513352394104 \n",
      "     Validation Step: 33 Validation Loss: 0.6184893250465393 \n",
      "     Validation Step: 34 Validation Loss: 0.6111605763435364 \n",
      "     Validation Step: 35 Validation Loss: 0.6183533072471619 \n",
      "     Validation Step: 36 Validation Loss: 0.6148415207862854 \n",
      "     Validation Step: 37 Validation Loss: 0.6152329444885254 \n",
      "     Validation Step: 38 Validation Loss: 0.6180747151374817 \n",
      "     Validation Step: 39 Validation Loss: 0.6141263842582703 \n",
      "     Validation Step: 40 Validation Loss: 0.6101077198982239 \n",
      "     Validation Step: 41 Validation Loss: 0.6111790537834167 \n",
      "     Validation Step: 42 Validation Loss: 0.6150412559509277 \n",
      "     Validation Step: 43 Validation Loss: 0.6115548610687256 \n",
      "     Validation Step: 44 Validation Loss: 0.6156207323074341 \n",
      "Epoch: 76\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6122254133224487 \n",
      "     Training Step: 1 Training Loss: 0.6122872829437256 \n",
      "     Training Step: 2 Training Loss: 0.6120630502700806 \n",
      "     Training Step: 3 Training Loss: 0.6184400320053101 \n",
      "     Training Step: 4 Training Loss: 0.6186316013336182 \n",
      "     Training Step: 5 Training Loss: 0.6116366386413574 \n",
      "     Training Step: 6 Training Loss: 0.6114060282707214 \n",
      "     Training Step: 7 Training Loss: 0.6144430041313171 \n",
      "     Training Step: 8 Training Loss: 0.6164136528968811 \n",
      "     Training Step: 9 Training Loss: 0.6123777627944946 \n",
      "     Training Step: 10 Training Loss: 0.6147943139076233 \n",
      "     Training Step: 11 Training Loss: 0.6146489977836609 \n",
      "     Training Step: 12 Training Loss: 0.6209456920623779 \n",
      "     Training Step: 13 Training Loss: 0.612877368927002 \n",
      "     Training Step: 14 Training Loss: 0.6118510365486145 \n",
      "     Training Step: 15 Training Loss: 0.6152923703193665 \n",
      "     Training Step: 16 Training Loss: 0.6182117462158203 \n",
      "     Training Step: 17 Training Loss: 0.6161964535713196 \n",
      "     Training Step: 18 Training Loss: 0.6177855730056763 \n",
      "     Training Step: 19 Training Loss: 0.6127273440361023 \n",
      "     Training Step: 20 Training Loss: 0.6169154644012451 \n",
      "     Training Step: 21 Training Loss: 0.6107359528541565 \n",
      "     Training Step: 22 Training Loss: 0.6167953610420227 \n",
      "     Training Step: 23 Training Loss: 0.614682137966156 \n",
      "     Training Step: 24 Training Loss: 0.6109434962272644 \n",
      "     Training Step: 25 Training Loss: 0.614369809627533 \n",
      "     Training Step: 26 Training Loss: 0.6120355725288391 \n",
      "     Training Step: 27 Training Loss: 0.6183198690414429 \n",
      "     Training Step: 28 Training Loss: 0.6178072690963745 \n",
      "     Training Step: 29 Training Loss: 0.6174902319908142 \n",
      "     Training Step: 30 Training Loss: 0.6125413775444031 \n",
      "     Training Step: 31 Training Loss: 0.614915668964386 \n",
      "     Training Step: 32 Training Loss: 0.6147040128707886 \n",
      "     Training Step: 33 Training Loss: 0.6153777837753296 \n",
      "     Training Step: 34 Training Loss: 0.6168907284736633 \n",
      "     Training Step: 35 Training Loss: 0.6101595163345337 \n",
      "     Training Step: 36 Training Loss: 0.6167083382606506 \n",
      "     Training Step: 37 Training Loss: 0.6118404269218445 \n",
      "     Training Step: 38 Training Loss: 0.6140456199645996 \n",
      "     Training Step: 39 Training Loss: 0.6129724979400635 \n",
      "     Training Step: 40 Training Loss: 0.6149313449859619 \n",
      "     Training Step: 41 Training Loss: 0.6105809807777405 \n",
      "     Training Step: 42 Training Loss: 0.6154752969741821 \n",
      "     Training Step: 43 Training Loss: 0.6144981980323792 \n",
      "     Training Step: 44 Training Loss: 0.6160047650337219 \n",
      "     Training Step: 45 Training Loss: 0.616768479347229 \n",
      "     Training Step: 46 Training Loss: 0.6094629168510437 \n",
      "     Training Step: 47 Training Loss: 0.6082789897918701 \n",
      "     Training Step: 48 Training Loss: 0.6114109754562378 \n",
      "     Training Step: 49 Training Loss: 0.6146934032440186 \n",
      "     Training Step: 50 Training Loss: 0.6165068745613098 \n",
      "     Training Step: 51 Training Loss: 0.6139078736305237 \n",
      "     Training Step: 52 Training Loss: 0.6111353635787964 \n",
      "     Training Step: 53 Training Loss: 0.614086925983429 \n",
      "     Training Step: 54 Training Loss: 0.6177077293395996 \n",
      "     Training Step: 55 Training Loss: 0.6096911430358887 \n",
      "     Training Step: 56 Training Loss: 0.6196723580360413 \n",
      "     Training Step: 57 Training Loss: 0.6152791976928711 \n",
      "     Training Step: 58 Training Loss: 0.6124864220619202 \n",
      "     Training Step: 59 Training Loss: 0.6188849806785583 \n",
      "     Training Step: 60 Training Loss: 0.6121443510055542 \n",
      "     Training Step: 61 Training Loss: 0.6103885769844055 \n",
      "     Training Step: 62 Training Loss: 0.6155995726585388 \n",
      "     Training Step: 63 Training Loss: 0.6100891828536987 \n",
      "     Training Step: 64 Training Loss: 0.613471269607544 \n",
      "     Training Step: 65 Training Loss: 0.6147708892822266 \n",
      "     Training Step: 66 Training Loss: 0.6141526699066162 \n",
      "     Training Step: 67 Training Loss: 0.6166566610336304 \n",
      "     Training Step: 68 Training Loss: 0.6151030659675598 \n",
      "     Training Step: 69 Training Loss: 0.6131439208984375 \n",
      "     Training Step: 70 Training Loss: 0.6146127581596375 \n",
      "     Training Step: 71 Training Loss: 0.6115309596061707 \n",
      "     Training Step: 72 Training Loss: 0.6134580373764038 \n",
      "     Training Step: 73 Training Loss: 0.6138851046562195 \n",
      "     Training Step: 74 Training Loss: 0.6156861186027527 \n",
      "     Training Step: 75 Training Loss: 0.6124619841575623 \n",
      "     Training Step: 76 Training Loss: 0.6176419854164124 \n",
      "     Training Step: 77 Training Loss: 0.6166536211967468 \n",
      "     Training Step: 78 Training Loss: 0.6132016181945801 \n",
      "     Training Step: 79 Training Loss: 0.6168025732040405 \n",
      "     Training Step: 80 Training Loss: 0.617158055305481 \n",
      "     Training Step: 81 Training Loss: 0.6157638430595398 \n",
      "     Training Step: 82 Training Loss: 0.6150903701782227 \n",
      "     Training Step: 83 Training Loss: 0.6185628175735474 \n",
      "     Training Step: 84 Training Loss: 0.6092889308929443 \n",
      "     Training Step: 85 Training Loss: 0.6130784153938293 \n",
      "     Training Step: 86 Training Loss: 0.6157785058021545 \n",
      "     Training Step: 87 Training Loss: 0.6107577681541443 \n",
      "     Training Step: 88 Training Loss: 0.6144840717315674 \n",
      "     Training Step: 89 Training Loss: 0.6105231642723083 \n",
      "     Training Step: 90 Training Loss: 0.6173839569091797 \n",
      "     Training Step: 91 Training Loss: 0.6123780012130737 \n",
      "     Training Step: 92 Training Loss: 0.6115917563438416 \n",
      "     Training Step: 93 Training Loss: 0.6129274964332581 \n",
      "     Training Step: 94 Training Loss: 0.6202555298805237 \n",
      "     Training Step: 95 Training Loss: 0.6140280961990356 \n",
      "     Training Step: 96 Training Loss: 0.6136510968208313 \n",
      "     Training Step: 97 Training Loss: 0.6111579537391663 \n",
      "     Training Step: 98 Training Loss: 0.6130735874176025 \n",
      "     Training Step: 99 Training Loss: 0.6167027950286865 \n",
      "     Training Step: 100 Training Loss: 0.615178108215332 \n",
      "     Training Step: 101 Training Loss: 0.6133111715316772 \n",
      "     Training Step: 102 Training Loss: 0.6107383370399475 \n",
      "     Training Step: 103 Training Loss: 0.6180481910705566 \n",
      "     Training Step: 104 Training Loss: 0.6106038093566895 \n",
      "     Training Step: 105 Training Loss: 0.6180271506309509 \n",
      "     Training Step: 106 Training Loss: 0.6182350516319275 \n",
      "     Training Step: 107 Training Loss: 0.6128082871437073 \n",
      "     Training Step: 108 Training Loss: 0.6146984100341797 \n",
      "     Training Step: 109 Training Loss: 0.6136412620544434 \n",
      "     Training Step: 110 Training Loss: 0.6157407164573669 \n",
      "     Training Step: 111 Training Loss: 0.6143529415130615 \n",
      "     Training Step: 112 Training Loss: 0.6114631295204163 \n",
      "     Training Step: 113 Training Loss: 0.6100773215293884 \n",
      "     Training Step: 114 Training Loss: 0.612843930721283 \n",
      "     Training Step: 115 Training Loss: 0.6114245057106018 \n",
      "     Training Step: 116 Training Loss: 0.6147490739822388 \n",
      "     Training Step: 117 Training Loss: 0.6147292852401733 \n",
      "     Training Step: 118 Training Loss: 0.6142458319664001 \n",
      "     Training Step: 119 Training Loss: 0.6142117977142334 \n",
      "     Training Step: 120 Training Loss: 0.6136161684989929 \n",
      "     Training Step: 121 Training Loss: 0.6197614669799805 \n",
      "     Training Step: 122 Training Loss: 0.6097168922424316 \n",
      "     Training Step: 123 Training Loss: 0.616270124912262 \n",
      "     Training Step: 124 Training Loss: 0.6137403845787048 \n",
      "     Training Step: 125 Training Loss: 0.6199324131011963 \n",
      "     Training Step: 126 Training Loss: 0.6135956645011902 \n",
      "     Training Step: 127 Training Loss: 0.6125125288963318 \n",
      "     Training Step: 128 Training Loss: 0.6158371567726135 \n",
      "     Training Step: 129 Training Loss: 0.6121562123298645 \n",
      "     Training Step: 130 Training Loss: 0.6116558909416199 \n",
      "     Training Step: 131 Training Loss: 0.6161119341850281 \n",
      "     Training Step: 132 Training Loss: 0.6152840852737427 \n",
      "     Training Step: 133 Training Loss: 0.6146034002304077 \n",
      "     Training Step: 134 Training Loss: 0.6109036207199097 \n",
      "     Training Step: 135 Training Loss: 0.6146410703659058 \n",
      "     Training Step: 136 Training Loss: 0.6166152358055115 \n",
      "     Training Step: 137 Training Loss: 0.6149597764015198 \n",
      "     Training Step: 138 Training Loss: 0.616719663143158 \n",
      "     Training Step: 139 Training Loss: 0.6104176044464111 \n",
      "     Training Step: 140 Training Loss: 0.6155294179916382 \n",
      "     Training Step: 141 Training Loss: 0.6144514679908752 \n",
      "     Training Step: 142 Training Loss: 0.6167181134223938 \n",
      "     Training Step: 143 Training Loss: 0.6116899251937866 \n",
      "     Training Step: 144 Training Loss: 0.6138172149658203 \n",
      "     Training Step: 145 Training Loss: 0.6162087917327881 \n",
      "     Training Step: 146 Training Loss: 0.6126434803009033 \n",
      "     Training Step: 147 Training Loss: 0.6105090379714966 \n",
      "     Training Step: 148 Training Loss: 0.6150689721107483 \n",
      "     Training Step: 149 Training Loss: 0.6105836033821106 \n",
      "     Training Step: 150 Training Loss: 0.6118257641792297 \n",
      "     Training Step: 151 Training Loss: 0.6152432560920715 \n",
      "     Training Step: 152 Training Loss: 0.6164385676383972 \n",
      "     Training Step: 153 Training Loss: 0.6184702515602112 \n",
      "     Training Step: 154 Training Loss: 0.615426242351532 \n",
      "     Training Step: 155 Training Loss: 0.6194623112678528 \n",
      "     Training Step: 156 Training Loss: 0.6154218316078186 \n",
      "     Training Step: 157 Training Loss: 0.6171349883079529 \n",
      "     Training Step: 158 Training Loss: 0.613364040851593 \n",
      "     Training Step: 159 Training Loss: 0.615683376789093 \n",
      "     Training Step: 160 Training Loss: 0.6137756705284119 \n",
      "     Training Step: 161 Training Loss: 0.6146145462989807 \n",
      "     Training Step: 162 Training Loss: 0.6159423589706421 \n",
      "     Training Step: 163 Training Loss: 0.6118305325508118 \n",
      "     Training Step: 164 Training Loss: 0.6127744317054749 \n",
      "     Training Step: 165 Training Loss: 0.6131412982940674 \n",
      "     Training Step: 166 Training Loss: 0.6133036017417908 \n",
      "     Training Step: 167 Training Loss: 0.6115825772285461 \n",
      "     Training Step: 168 Training Loss: 0.6094265580177307 \n",
      "     Training Step: 169 Training Loss: 0.6128665208816528 \n",
      "     Training Step: 170 Training Loss: 0.614346981048584 \n",
      "     Training Step: 171 Training Loss: 0.6134246587753296 \n",
      "     Training Step: 172 Training Loss: 0.615354597568512 \n",
      "     Training Step: 173 Training Loss: 0.6116171479225159 \n",
      "     Training Step: 174 Training Loss: 0.6153943538665771 \n",
      "     Training Step: 175 Training Loss: 0.61016845703125 \n",
      "     Training Step: 176 Training Loss: 0.6172410249710083 \n",
      "     Training Step: 177 Training Loss: 0.6135144829750061 \n",
      "     Training Step: 178 Training Loss: 0.617734432220459 \n",
      "     Training Step: 179 Training Loss: 0.6163740158081055 \n",
      "     Training Step: 180 Training Loss: 0.613497257232666 \n",
      "     Training Step: 181 Training Loss: 0.6115274429321289 \n",
      "     Training Step: 182 Training Loss: 0.6122373342514038 \n",
      "     Training Step: 183 Training Loss: 0.6155404448509216 \n",
      "     Training Step: 184 Training Loss: 0.6177589297294617 \n",
      "     Training Step: 185 Training Loss: 0.6148754358291626 \n",
      "     Training Step: 186 Training Loss: 0.6142183542251587 \n",
      "     Training Step: 187 Training Loss: 0.615521252155304 \n",
      "     Training Step: 188 Training Loss: 0.6121401786804199 \n",
      "     Training Step: 189 Training Loss: 0.6167073249816895 \n",
      "     Training Step: 190 Training Loss: 0.6176992654800415 \n",
      "     Training Step: 191 Training Loss: 0.6133111715316772 \n",
      "     Training Step: 192 Training Loss: 0.6125491261482239 \n",
      "     Training Step: 193 Training Loss: 0.6140220761299133 \n",
      "     Training Step: 194 Training Loss: 0.6127792000770569 \n",
      "     Training Step: 195 Training Loss: 0.6129114031791687 \n",
      "     Training Step: 196 Training Loss: 0.614295482635498 \n",
      "     Training Step: 197 Training Loss: 0.613196611404419 \n",
      "     Training Step: 198 Training Loss: 0.6117833852767944 \n",
      "     Training Step: 199 Training Loss: 0.610016942024231 \n",
      "     Training Step: 200 Training Loss: 0.613948404788971 \n",
      "     Training Step: 201 Training Loss: 0.6181052327156067 \n",
      "     Training Step: 202 Training Loss: 0.6122251152992249 \n",
      "     Training Step: 203 Training Loss: 0.6153071522712708 \n",
      "     Training Step: 204 Training Loss: 0.6166914105415344 \n",
      "     Training Step: 205 Training Loss: 0.6132799386978149 \n",
      "     Training Step: 206 Training Loss: 0.6171592473983765 \n",
      "     Training Step: 207 Training Loss: 0.6111972332000732 \n",
      "     Training Step: 208 Training Loss: 0.6100633144378662 \n",
      "     Training Step: 209 Training Loss: 0.6146638989448547 \n",
      "     Training Step: 210 Training Loss: 0.6121614575386047 \n",
      "     Training Step: 211 Training Loss: 0.6143480539321899 \n",
      "     Training Step: 212 Training Loss: 0.612295389175415 \n",
      "     Training Step: 213 Training Loss: 0.6132054328918457 \n",
      "     Training Step: 214 Training Loss: 0.615386962890625 \n",
      "     Training Step: 215 Training Loss: 0.6160337328910828 \n",
      "     Training Step: 216 Training Loss: 0.6168267130851746 \n",
      "     Training Step: 217 Training Loss: 0.6171107888221741 \n",
      "     Training Step: 218 Training Loss: 0.6105732321739197 \n",
      "     Training Step: 219 Training Loss: 0.6146586537361145 \n",
      "     Training Step: 220 Training Loss: 0.6141575574874878 \n",
      "     Training Step: 221 Training Loss: 0.6151530146598816 \n",
      "     Training Step: 222 Training Loss: 0.6114495992660522 \n",
      "     Training Step: 223 Training Loss: 0.6137475371360779 \n",
      "     Training Step: 224 Training Loss: 0.61152583360672 \n",
      "     Training Step: 225 Training Loss: 0.6188834309577942 \n",
      "     Training Step: 226 Training Loss: 0.6132754683494568 \n",
      "     Training Step: 227 Training Loss: 0.6097274422645569 \n",
      "     Training Step: 228 Training Loss: 0.6117956638336182 \n",
      "     Training Step: 229 Training Loss: 0.6184817552566528 \n",
      "     Training Step: 230 Training Loss: 0.6106549501419067 \n",
      "     Training Step: 231 Training Loss: 0.6181252598762512 \n",
      "     Training Step: 232 Training Loss: 0.6154487729072571 \n",
      "     Training Step: 233 Training Loss: 0.6144244074821472 \n",
      "     Training Step: 234 Training Loss: 0.6116114258766174 \n",
      "     Training Step: 235 Training Loss: 0.6141091585159302 \n",
      "     Training Step: 236 Training Loss: 0.6114774346351624 \n",
      "     Training Step: 237 Training Loss: 0.6118823885917664 \n",
      "     Training Step: 238 Training Loss: 0.6157457828521729 \n",
      "     Training Step: 239 Training Loss: 0.6123781800270081 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6132988333702087 \n",
      "     Validation Step: 1 Validation Loss: 0.6128219962120056 \n",
      "     Validation Step: 2 Validation Loss: 0.6146370768547058 \n",
      "     Validation Step: 3 Validation Loss: 0.6104773283004761 \n",
      "     Validation Step: 4 Validation Loss: 0.6158035397529602 \n",
      "     Validation Step: 5 Validation Loss: 0.613667905330658 \n",
      "     Validation Step: 6 Validation Loss: 0.6116366386413574 \n",
      "     Validation Step: 7 Validation Loss: 0.6176044940948486 \n",
      "     Validation Step: 8 Validation Loss: 0.6141987442970276 \n",
      "     Validation Step: 9 Validation Loss: 0.6105082631111145 \n",
      "     Validation Step: 10 Validation Loss: 0.6162534356117249 \n",
      "     Validation Step: 11 Validation Loss: 0.6136581897735596 \n",
      "     Validation Step: 12 Validation Loss: 0.6170358657836914 \n",
      "     Validation Step: 13 Validation Loss: 0.6101362109184265 \n",
      "     Validation Step: 14 Validation Loss: 0.6173153519630432 \n",
      "     Validation Step: 15 Validation Loss: 0.6184858679771423 \n",
      "     Validation Step: 16 Validation Loss: 0.6101586818695068 \n",
      "     Validation Step: 17 Validation Loss: 0.6141116619110107 \n",
      "     Validation Step: 18 Validation Loss: 0.6152303218841553 \n",
      "     Validation Step: 19 Validation Loss: 0.6101047992706299 \n",
      "     Validation Step: 20 Validation Loss: 0.6111581921577454 \n",
      "     Validation Step: 21 Validation Loss: 0.6141254901885986 \n",
      "     Validation Step: 22 Validation Loss: 0.6118761897087097 \n",
      "     Validation Step: 23 Validation Loss: 0.6115508079528809 \n",
      "     Validation Step: 24 Validation Loss: 0.6185117959976196 \n",
      "     Validation Step: 25 Validation Loss: 0.617712676525116 \n",
      "     Validation Step: 26 Validation Loss: 0.6106299161911011 \n",
      "     Validation Step: 27 Validation Loss: 0.6160147190093994 \n",
      "     Validation Step: 28 Validation Loss: 0.6182540059089661 \n",
      "     Validation Step: 29 Validation Loss: 0.6136296987533569 \n",
      "     Validation Step: 30 Validation Loss: 0.618360698223114 \n",
      "     Validation Step: 31 Validation Loss: 0.6145322322845459 \n",
      "     Validation Step: 32 Validation Loss: 0.611173152923584 \n",
      "     Validation Step: 33 Validation Loss: 0.6148360371589661 \n",
      "     Validation Step: 34 Validation Loss: 0.612980306148529 \n",
      "     Validation Step: 35 Validation Loss: 0.6142516732215881 \n",
      "     Validation Step: 36 Validation Loss: 0.6155852675437927 \n",
      "     Validation Step: 37 Validation Loss: 0.615046501159668 \n",
      "     Validation Step: 38 Validation Loss: 0.6121228933334351 \n",
      "     Validation Step: 39 Validation Loss: 0.6075172424316406 \n",
      "     Validation Step: 40 Validation Loss: 0.6145724654197693 \n",
      "     Validation Step: 41 Validation Loss: 0.6180779933929443 \n",
      "     Validation Step: 42 Validation Loss: 0.6153223514556885 \n",
      "     Validation Step: 43 Validation Loss: 0.6156215071678162 \n",
      "     Validation Step: 44 Validation Loss: 0.6148941516876221 \n",
      "Epoch: 77\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.616726279258728 \n",
      "     Training Step: 1 Training Loss: 0.6147040128707886 \n",
      "     Training Step: 2 Training Loss: 0.612764298915863 \n",
      "     Training Step: 3 Training Loss: 0.6122334003448486 \n",
      "     Training Step: 4 Training Loss: 0.6099892258644104 \n",
      "     Training Step: 5 Training Loss: 0.6104985475540161 \n",
      "     Training Step: 6 Training Loss: 0.6147289872169495 \n",
      "     Training Step: 7 Training Loss: 0.6101248860359192 \n",
      "     Training Step: 8 Training Loss: 0.6140257120132446 \n",
      "     Training Step: 9 Training Loss: 0.6114626526832581 \n",
      "     Training Step: 10 Training Loss: 0.6177665591239929 \n",
      "     Training Step: 11 Training Loss: 0.6189088225364685 \n",
      "     Training Step: 12 Training Loss: 0.6138904094696045 \n",
      "     Training Step: 13 Training Loss: 0.6154818534851074 \n",
      "     Training Step: 14 Training Loss: 0.6167296767234802 \n",
      "     Training Step: 15 Training Loss: 0.6128396987915039 \n",
      "     Training Step: 16 Training Loss: 0.6122444868087769 \n",
      "     Training Step: 17 Training Loss: 0.6197077035903931 \n",
      "     Training Step: 18 Training Loss: 0.6136309504508972 \n",
      "     Training Step: 19 Training Loss: 0.6131309866905212 \n",
      "     Training Step: 20 Training Loss: 0.6167513132095337 \n",
      "     Training Step: 21 Training Loss: 0.6188569664955139 \n",
      "     Training Step: 22 Training Loss: 0.6129193902015686 \n",
      "     Training Step: 23 Training Loss: 0.6121425032615662 \n",
      "     Training Step: 24 Training Loss: 0.6097673177719116 \n",
      "     Training Step: 25 Training Loss: 0.617129385471344 \n",
      "     Training Step: 26 Training Loss: 0.6170837879180908 \n",
      "     Training Step: 27 Training Loss: 0.6176789402961731 \n",
      "     Training Step: 28 Training Loss: 0.6164805293083191 \n",
      "     Training Step: 29 Training Loss: 0.616109311580658 \n",
      "     Training Step: 30 Training Loss: 0.6148687601089478 \n",
      "     Training Step: 31 Training Loss: 0.6146044731140137 \n",
      "     Training Step: 32 Training Loss: 0.6116163730621338 \n",
      "     Training Step: 33 Training Loss: 0.6146897673606873 \n",
      "     Training Step: 34 Training Loss: 0.6144578456878662 \n",
      "     Training Step: 35 Training Loss: 0.6140177249908447 \n",
      "     Training Step: 36 Training Loss: 0.6152898669242859 \n",
      "     Training Step: 37 Training Loss: 0.6114299893379211 \n",
      "     Training Step: 38 Training Loss: 0.6122269630432129 \n",
      "     Training Step: 39 Training Loss: 0.6094652414321899 \n",
      "     Training Step: 40 Training Loss: 0.6178122162818909 \n",
      "     Training Step: 41 Training Loss: 0.6121407747268677 \n",
      "     Training Step: 42 Training Loss: 0.6133116483688354 \n",
      "     Training Step: 43 Training Loss: 0.6155456900596619 \n",
      "     Training Step: 44 Training Loss: 0.6146461367607117 \n",
      "     Training Step: 45 Training Loss: 0.6143471002578735 \n",
      "     Training Step: 46 Training Loss: 0.6113957166671753 \n",
      "     Training Step: 47 Training Loss: 0.6184648275375366 \n",
      "     Training Step: 48 Training Loss: 0.6176553964614868 \n",
      "     Training Step: 49 Training Loss: 0.6182277798652649 \n",
      "     Training Step: 50 Training Loss: 0.610076904296875 \n",
      "     Training Step: 51 Training Loss: 0.6140384078025818 \n",
      "     Training Step: 52 Training Loss: 0.6147945523262024 \n",
      "     Training Step: 53 Training Loss: 0.615531325340271 \n",
      "     Training Step: 54 Training Loss: 0.6123924851417542 \n",
      "     Training Step: 55 Training Loss: 0.6142493486404419 \n",
      "     Training Step: 56 Training Loss: 0.6184492707252502 \n",
      "     Training Step: 57 Training Loss: 0.6137552261352539 \n",
      "     Training Step: 58 Training Loss: 0.6118209362030029 \n",
      "     Training Step: 59 Training Loss: 0.6131482124328613 \n",
      "     Training Step: 60 Training Loss: 0.6134644746780396 \n",
      "     Training Step: 61 Training Loss: 0.6105011701583862 \n",
      "     Training Step: 62 Training Loss: 0.6174963116645813 \n",
      "     Training Step: 63 Training Loss: 0.6125307083129883 \n",
      "     Training Step: 64 Training Loss: 0.6152433156967163 \n",
      "     Training Step: 65 Training Loss: 0.6101728081703186 \n",
      "     Training Step: 66 Training Loss: 0.6157678365707397 \n",
      "     Training Step: 67 Training Loss: 0.611623227596283 \n",
      "     Training Step: 68 Training Loss: 0.6127991676330566 \n",
      "     Training Step: 69 Training Loss: 0.6197000741958618 \n",
      "     Training Step: 70 Training Loss: 0.6130551099777222 \n",
      "     Training Step: 71 Training Loss: 0.6120612621307373 \n",
      "     Training Step: 72 Training Loss: 0.61448734998703 \n",
      "     Training Step: 73 Training Loss: 0.61275315284729 \n",
      "     Training Step: 74 Training Loss: 0.6114035248756409 \n",
      "     Training Step: 75 Training Loss: 0.6105663776397705 \n",
      "     Training Step: 76 Training Loss: 0.6209777593612671 \n",
      "     Training Step: 77 Training Loss: 0.6124876141548157 \n",
      "     Training Step: 78 Training Loss: 0.6153902411460876 \n",
      "     Training Step: 79 Training Loss: 0.6142054796218872 \n",
      "     Training Step: 80 Training Loss: 0.613511323928833 \n",
      "     Training Step: 81 Training Loss: 0.6123806834220886 \n",
      "     Training Step: 82 Training Loss: 0.6147090196609497 \n",
      "     Training Step: 83 Training Loss: 0.6142839193344116 \n",
      "     Training Step: 84 Training Loss: 0.6184157729148865 \n",
      "     Training Step: 85 Training Loss: 0.61411452293396 \n",
      "     Training Step: 86 Training Loss: 0.616000235080719 \n",
      "     Training Step: 87 Training Loss: 0.6180276870727539 \n",
      "     Training Step: 88 Training Loss: 0.6132122874259949 \n",
      "     Training Step: 89 Training Loss: 0.6152881383895874 \n",
      "     Training Step: 90 Training Loss: 0.6133648753166199 \n",
      "     Training Step: 91 Training Loss: 0.6151019930839539 \n",
      "     Training Step: 92 Training Loss: 0.6168022155761719 \n",
      "     Training Step: 93 Training Loss: 0.6119001507759094 \n",
      "     Training Step: 94 Training Loss: 0.6117891073226929 \n",
      "     Training Step: 95 Training Loss: 0.6126497983932495 \n",
      "     Training Step: 96 Training Loss: 0.6169220209121704 \n",
      "     Training Step: 97 Training Loss: 0.6182512044906616 \n",
      "     Training Step: 98 Training Loss: 0.615786075592041 \n",
      "     Training Step: 99 Training Loss: 0.6157463788986206 \n",
      "     Training Step: 100 Training Loss: 0.6158266663551331 \n",
      "     Training Step: 101 Training Loss: 0.6104246973991394 \n",
      "     Training Step: 102 Training Loss: 0.6146782636642456 \n",
      "     Training Step: 103 Training Loss: 0.6139352321624756 \n",
      "     Training Step: 104 Training Loss: 0.6122967600822449 \n",
      "     Training Step: 105 Training Loss: 0.614772379398346 \n",
      "     Training Step: 106 Training Loss: 0.6111507415771484 \n",
      "     Training Step: 107 Training Loss: 0.609240710735321 \n",
      "     Training Step: 108 Training Loss: 0.617766261100769 \n",
      "     Training Step: 109 Training Loss: 0.6093993782997131 \n",
      "     Training Step: 110 Training Loss: 0.6116083264350891 \n",
      "     Training Step: 111 Training Loss: 0.6172280311584473 \n",
      "     Training Step: 112 Training Loss: 0.6150850653648376 \n",
      "     Training Step: 113 Training Loss: 0.6116883754730225 \n",
      "     Training Step: 114 Training Loss: 0.6166927814483643 \n",
      "     Training Step: 115 Training Loss: 0.6153442859649658 \n",
      "     Training Step: 116 Training Loss: 0.6140914559364319 \n",
      "     Training Step: 117 Training Loss: 0.6111593246459961 \n",
      "     Training Step: 118 Training Loss: 0.6136500835418701 \n",
      "     Training Step: 119 Training Loss: 0.6132736206054688 \n",
      "     Training Step: 120 Training Loss: 0.6130696535110474 \n",
      "     Training Step: 121 Training Loss: 0.6156834363937378 \n",
      "     Training Step: 122 Training Loss: 0.6159529685974121 \n",
      "     Training Step: 123 Training Loss: 0.6118396520614624 \n",
      "     Training Step: 124 Training Loss: 0.6143503189086914 \n",
      "     Training Step: 125 Training Loss: 0.615443229675293 \n",
      "     Training Step: 126 Training Loss: 0.6154171824455261 \n",
      "     Training Step: 127 Training Loss: 0.6166216135025024 \n",
      "     Training Step: 128 Training Loss: 0.6114473938941956 \n",
      "     Training Step: 129 Training Loss: 0.6141535639762878 \n",
      "     Training Step: 130 Training Loss: 0.6127148866653442 \n",
      "     Training Step: 131 Training Loss: 0.6162368059158325 \n",
      "     Training Step: 132 Training Loss: 0.6136391162872314 \n",
      "     Training Step: 133 Training Loss: 0.6097303032875061 \n",
      "     Training Step: 134 Training Loss: 0.6106101274490356 \n",
      "     Training Step: 135 Training Loss: 0.6180463433265686 \n",
      "     Training Step: 136 Training Loss: 0.6149208545684814 \n",
      "     Training Step: 137 Training Loss: 0.6146793365478516 \n",
      "     Training Step: 138 Training Loss: 0.6168181300163269 \n",
      "     Training Step: 139 Training Loss: 0.6164115071296692 \n",
      "     Training Step: 140 Training Loss: 0.6151571273803711 \n",
      "     Training Step: 141 Training Loss: 0.6160442233085632 \n",
      "     Training Step: 142 Training Loss: 0.6167163848876953 \n",
      "     Training Step: 143 Training Loss: 0.6150850653648376 \n",
      "     Training Step: 144 Training Loss: 0.6131927967071533 \n",
      "     Training Step: 145 Training Loss: 0.6132957935333252 \n",
      "     Training Step: 146 Training Loss: 0.6138232946395874 \n",
      "     Training Step: 147 Training Loss: 0.6118467450141907 \n",
      "     Training Step: 148 Training Loss: 0.6202059388160706 \n",
      "     Training Step: 149 Training Loss: 0.6156744956970215 \n",
      "     Training Step: 150 Training Loss: 0.61329185962677 \n",
      "     Training Step: 151 Training Loss: 0.6144223213195801 \n",
      "     Training Step: 152 Training Loss: 0.6143684387207031 \n",
      "     Training Step: 153 Training Loss: 0.613594114780426 \n",
      "     Training Step: 154 Training Loss: 0.6120376586914062 \n",
      "     Training Step: 155 Training Loss: 0.6115530133247375 \n",
      "     Training Step: 156 Training Loss: 0.6114557385444641 \n",
      "     Training Step: 157 Training Loss: 0.6146030426025391 \n",
      "     Training Step: 158 Training Loss: 0.6116440892219543 \n",
      "     Training Step: 159 Training Loss: 0.6103730201721191 \n",
      "     Training Step: 160 Training Loss: 0.6143510341644287 \n",
      "     Training Step: 161 Training Loss: 0.615540087223053 \n",
      "     Training Step: 162 Training Loss: 0.6166939735412598 \n",
      "     Training Step: 163 Training Loss: 0.6144441366195679 \n",
      "     Training Step: 164 Training Loss: 0.6151835918426514 \n",
      "     Training Step: 165 Training Loss: 0.615281343460083 \n",
      "     Training Step: 166 Training Loss: 0.615611732006073 \n",
      "     Training Step: 167 Training Loss: 0.6106576919555664 \n",
      "     Training Step: 168 Training Loss: 0.6121547222137451 \n",
      "     Training Step: 169 Training Loss: 0.6147515773773193 \n",
      "     Training Step: 170 Training Loss: 0.6100775599479675 \n",
      "     Training Step: 171 Training Loss: 0.6185901761054993 \n",
      "     Training Step: 172 Training Loss: 0.6128725409507751 \n",
      "     Training Step: 173 Training Loss: 0.6082850694656372 \n",
      "     Training Step: 174 Training Loss: 0.6100518107414246 \n",
      "     Training Step: 175 Training Loss: 0.6115228533744812 \n",
      "     Training Step: 176 Training Loss: 0.6146693229675293 \n",
      "     Training Step: 177 Training Loss: 0.6154048442840576 \n",
      "     Training Step: 178 Training Loss: 0.6181037425994873 \n",
      "     Training Step: 179 Training Loss: 0.612155556678772 \n",
      "     Training Step: 180 Training Loss: 0.6149618029594421 \n",
      "     Training Step: 181 Training Loss: 0.6134766340255737 \n",
      "     Training Step: 182 Training Loss: 0.6146583557128906 \n",
      "     Training Step: 183 Training Loss: 0.6106870174407959 \n",
      "     Training Step: 184 Training Loss: 0.6167271733283997 \n",
      "     Training Step: 185 Training Loss: 0.6134958863258362 \n",
      "     Training Step: 186 Training Loss: 0.6129181385040283 \n",
      "     Training Step: 187 Training Loss: 0.617174506187439 \n",
      "     Training Step: 188 Training Loss: 0.6116272807121277 \n",
      "     Training Step: 189 Training Loss: 0.6142101883888245 \n",
      "     Training Step: 190 Training Loss: 0.6128644943237305 \n",
      "     Training Step: 191 Training Loss: 0.6178131699562073 \n",
      "     Training Step: 192 Training Loss: 0.6194459795951843 \n",
      "     Training Step: 193 Training Loss: 0.6118381023406982 \n",
      "     Training Step: 194 Training Loss: 0.6115421652793884 \n",
      "     Training Step: 195 Training Loss: 0.6180989146232605 \n",
      "     Training Step: 196 Training Loss: 0.6109415888786316 \n",
      "     Training Step: 197 Training Loss: 0.6171314120292664 \n",
      "     Training Step: 198 Training Loss: 0.6134340763092041 \n",
      "     Training Step: 199 Training Loss: 0.6141529083251953 \n",
      "     Training Step: 200 Training Loss: 0.6133078932762146 \n",
      "     Training Step: 201 Training Loss: 0.6146130561828613 \n",
      "     Training Step: 202 Training Loss: 0.6109008193016052 \n",
      "     Training Step: 203 Training Loss: 0.6107476949691772 \n",
      "     Training Step: 204 Training Loss: 0.6115590333938599 \n",
      "     Training Step: 205 Training Loss: 0.6186223030090332 \n",
      "     Training Step: 206 Training Loss: 0.6149301528930664 \n",
      "     Training Step: 207 Training Loss: 0.6167038679122925 \n",
      "     Training Step: 208 Training Loss: 0.6105749607086182 \n",
      "     Training Step: 209 Training Loss: 0.6124633550643921 \n",
      "     Training Step: 210 Training Loss: 0.6137421727180481 \n",
      "     Training Step: 211 Training Loss: 0.6162202954292297 \n",
      "     Training Step: 212 Training Loss: 0.6111934185028076 \n",
      "     Training Step: 213 Training Loss: 0.61574786901474 \n",
      "     Training Step: 214 Training Loss: 0.6199241876602173 \n",
      "     Training Step: 215 Training Loss: 0.6107187271118164 \n",
      "     Training Step: 216 Training Loss: 0.6163535118103027 \n",
      "     Training Step: 217 Training Loss: 0.6137649416923523 \n",
      "     Training Step: 218 Training Loss: 0.6139145493507385 \n",
      "     Training Step: 219 Training Loss: 0.618323028087616 \n",
      "     Training Step: 220 Training Loss: 0.617684543132782 \n",
      "     Training Step: 221 Training Loss: 0.6153687238693237 \n",
      "     Training Step: 222 Training Loss: 0.615418016910553 \n",
      "     Training Step: 223 Training Loss: 0.610602855682373 \n",
      "     Training Step: 224 Training Loss: 0.6129863262176514 \n",
      "     Training Step: 225 Training Loss: 0.6145027279853821 \n",
      "     Training Step: 226 Training Loss: 0.6125437021255493 \n",
      "     Training Step: 227 Training Loss: 0.6167919039726257 \n",
      "     Training Step: 228 Training Loss: 0.6168872714042664 \n",
      "     Training Step: 229 Training Loss: 0.6123064160346985 \n",
      "     Training Step: 230 Training Loss: 0.6164119243621826 \n",
      "     Training Step: 231 Training Loss: 0.6162031888961792 \n",
      "     Training Step: 232 Training Loss: 0.6125340461730957 \n",
      "     Training Step: 233 Training Loss: 0.6173790693283081 \n",
      "     Training Step: 234 Training Loss: 0.6132112741470337 \n",
      "     Training Step: 235 Training Loss: 0.6152961850166321 \n",
      "     Training Step: 236 Training Loss: 0.6166486740112305 \n",
      "     Training Step: 237 Training Loss: 0.6118252277374268 \n",
      "     Training Step: 238 Training Loss: 0.6123867630958557 \n",
      "     Training Step: 239 Training Loss: 0.6097579002380371 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6116481423377991 \n",
      "     Validation Step: 1 Validation Loss: 0.6170200705528259 \n",
      "     Validation Step: 2 Validation Loss: 0.6141155362129211 \n",
      "     Validation Step: 3 Validation Loss: 0.615612268447876 \n",
      "     Validation Step: 4 Validation Loss: 0.6104904413223267 \n",
      "     Validation Step: 5 Validation Loss: 0.6146366596221924 \n",
      "     Validation Step: 6 Validation Loss: 0.6159976124763489 \n",
      "     Validation Step: 7 Validation Loss: 0.6142576336860657 \n",
      "     Validation Step: 8 Validation Loss: 0.611168622970581 \n",
      "     Validation Step: 9 Validation Loss: 0.6153173446655273 \n",
      "     Validation Step: 10 Validation Loss: 0.6141292452812195 \n",
      "     Validation Step: 11 Validation Loss: 0.6176933646202087 \n",
      "     Validation Step: 12 Validation Loss: 0.6180598735809326 \n",
      "     Validation Step: 13 Validation Loss: 0.6173012256622314 \n",
      "     Validation Step: 14 Validation Loss: 0.6150385737419128 \n",
      "     Validation Step: 15 Validation Loss: 0.6075488328933716 \n",
      "     Validation Step: 16 Validation Loss: 0.6152244210243225 \n",
      "     Validation Step: 17 Validation Loss: 0.6136516332626343 \n",
      "     Validation Step: 18 Validation Loss: 0.6148949861526489 \n",
      "     Validation Step: 19 Validation Loss: 0.6142013669013977 \n",
      "     Validation Step: 20 Validation Loss: 0.6184912323951721 \n",
      "     Validation Step: 21 Validation Loss: 0.6115621328353882 \n",
      "     Validation Step: 22 Validation Loss: 0.6184698343276978 \n",
      "     Validation Step: 23 Validation Loss: 0.6133069396018982 \n",
      "     Validation Step: 24 Validation Loss: 0.6182332038879395 \n",
      "     Validation Step: 25 Validation Loss: 0.6175931692123413 \n",
      "     Validation Step: 26 Validation Loss: 0.6129859089851379 \n",
      "     Validation Step: 27 Validation Loss: 0.6136681437492371 \n",
      "     Validation Step: 28 Validation Loss: 0.6121352314949036 \n",
      "     Validation Step: 29 Validation Loss: 0.6157923340797424 \n",
      "     Validation Step: 30 Validation Loss: 0.6101241111755371 \n",
      "     Validation Step: 31 Validation Loss: 0.6148355603218079 \n",
      "     Validation Step: 32 Validation Loss: 0.6105238795280457 \n",
      "     Validation Step: 33 Validation Loss: 0.6118871569633484 \n",
      "     Validation Step: 34 Validation Loss: 0.618341326713562 \n",
      "     Validation Step: 35 Validation Loss: 0.6106430292129517 \n",
      "     Validation Step: 36 Validation Loss: 0.6128342747688293 \n",
      "     Validation Step: 37 Validation Loss: 0.6101783514022827 \n",
      "     Validation Step: 38 Validation Loss: 0.611182689666748 \n",
      "     Validation Step: 39 Validation Loss: 0.61363685131073 \n",
      "     Validation Step: 40 Validation Loss: 0.6162423491477966 \n",
      "     Validation Step: 41 Validation Loss: 0.6101561784744263 \n",
      "     Validation Step: 42 Validation Loss: 0.6155723333358765 \n",
      "     Validation Step: 43 Validation Loss: 0.6145681142807007 \n",
      "     Validation Step: 44 Validation Loss: 0.6145355701446533 \n",
      "Epoch: 78\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6159985661506653 \n",
      "     Training Step: 1 Training Loss: 0.6156864166259766 \n",
      "     Training Step: 2 Training Loss: 0.6164129376411438 \n",
      "     Training Step: 3 Training Loss: 0.6188738346099854 \n",
      "     Training Step: 4 Training Loss: 0.615283727645874 \n",
      "     Training Step: 5 Training Loss: 0.6115663647651672 \n",
      "     Training Step: 6 Training Loss: 0.6155203580856323 \n",
      "     Training Step: 7 Training Loss: 0.6146096587181091 \n",
      "     Training Step: 8 Training Loss: 0.6130635738372803 \n",
      "     Training Step: 9 Training Loss: 0.6176906824111938 \n",
      "     Training Step: 10 Training Loss: 0.6132859587669373 \n",
      "     Training Step: 11 Training Loss: 0.614213764667511 \n",
      "     Training Step: 12 Training Loss: 0.6171385049819946 \n",
      "     Training Step: 13 Training Loss: 0.612386167049408 \n",
      "     Training Step: 14 Training Loss: 0.6144519448280334 \n",
      "     Training Step: 15 Training Loss: 0.6177602410316467 \n",
      "     Training Step: 16 Training Loss: 0.6141512393951416 \n",
      "     Training Step: 17 Training Loss: 0.612526535987854 \n",
      "     Training Step: 18 Training Loss: 0.6152655482292175 \n",
      "     Training Step: 19 Training Loss: 0.6100895404815674 \n",
      "     Training Step: 20 Training Loss: 0.6166219711303711 \n",
      "     Training Step: 21 Training Loss: 0.6118855476379395 \n",
      "     Training Step: 22 Training Loss: 0.6140226721763611 \n",
      "     Training Step: 23 Training Loss: 0.6166591048240662 \n",
      "     Training Step: 24 Training Loss: 0.6144218444824219 \n",
      "     Training Step: 25 Training Loss: 0.6111618876457214 \n",
      "     Training Step: 26 Training Loss: 0.614341139793396 \n",
      "     Training Step: 27 Training Loss: 0.6094606518745422 \n",
      "     Training Step: 28 Training Loss: 0.6101365089416504 \n",
      "     Training Step: 29 Training Loss: 0.6129195690155029 \n",
      "     Training Step: 30 Training Loss: 0.6196606755256653 \n",
      "     Training Step: 31 Training Loss: 0.6121605038642883 \n",
      "     Training Step: 32 Training Loss: 0.6120139956474304 \n",
      "     Training Step: 33 Training Loss: 0.6181245446205139 \n",
      "     Training Step: 34 Training Loss: 0.6154294013977051 \n",
      "     Training Step: 35 Training Loss: 0.6131147146224976 \n",
      "     Training Step: 36 Training Loss: 0.6107202768325806 \n",
      "     Training Step: 37 Training Loss: 0.6146829128265381 \n",
      "     Training Step: 38 Training Loss: 0.616814911365509 \n",
      "     Training Step: 39 Training Loss: 0.6121222376823425 \n",
      "     Training Step: 40 Training Loss: 0.6147064566612244 \n",
      "     Training Step: 41 Training Loss: 0.6157593727111816 \n",
      "     Training Step: 42 Training Loss: 0.6170966029167175 \n",
      "     Training Step: 43 Training Loss: 0.6197090744972229 \n",
      "     Training Step: 44 Training Loss: 0.6132941246032715 \n",
      "     Training Step: 45 Training Loss: 0.6100162267684937 \n",
      "     Training Step: 46 Training Loss: 0.614771842956543 \n",
      "     Training Step: 47 Training Loss: 0.6107550263404846 \n",
      "     Training Step: 48 Training Loss: 0.6123883724212646 \n",
      "     Training Step: 49 Training Loss: 0.6144960522651672 \n",
      "     Training Step: 50 Training Loss: 0.6162481307983398 \n",
      "     Training Step: 51 Training Loss: 0.6118293404579163 \n",
      "     Training Step: 52 Training Loss: 0.6116447448730469 \n",
      "     Training Step: 53 Training Loss: 0.6129136085510254 \n",
      "     Training Step: 54 Training Loss: 0.6147288680076599 \n",
      "     Training Step: 55 Training Loss: 0.6114698052406311 \n",
      "     Training Step: 56 Training Loss: 0.6103658676147461 \n",
      "     Training Step: 57 Training Loss: 0.6209914088249207 \n",
      "     Training Step: 58 Training Loss: 0.6136202216148376 \n",
      "     Training Step: 59 Training Loss: 0.6140375137329102 \n",
      "     Training Step: 60 Training Loss: 0.6109002828598022 \n",
      "     Training Step: 61 Training Loss: 0.6111358404159546 \n",
      "     Training Step: 62 Training Loss: 0.6141588091850281 \n",
      "     Training Step: 63 Training Loss: 0.61673903465271 \n",
      "     Training Step: 64 Training Loss: 0.6184367537498474 \n",
      "     Training Step: 65 Training Loss: 0.6188868284225464 \n",
      "     Training Step: 66 Training Loss: 0.6146906614303589 \n",
      "     Training Step: 67 Training Loss: 0.6135969161987305 \n",
      "     Training Step: 68 Training Loss: 0.6115482449531555 \n",
      "     Training Step: 69 Training Loss: 0.6149561405181885 \n",
      "     Training Step: 70 Training Loss: 0.6151036620140076 \n",
      "     Training Step: 71 Training Loss: 0.612390398979187 \n",
      "     Training Step: 72 Training Loss: 0.6154749393463135 \n",
      "     Training Step: 73 Training Loss: 0.6123104691505432 \n",
      "     Training Step: 74 Training Loss: 0.612162172794342 \n",
      "     Training Step: 75 Training Loss: 0.6180155873298645 \n",
      "     Training Step: 76 Training Loss: 0.6117873787879944 \n",
      "     Training Step: 77 Training Loss: 0.6186014413833618 \n",
      "     Training Step: 78 Training Loss: 0.6122291684150696 \n",
      "     Training Step: 79 Training Loss: 0.6128790378570557 \n",
      "     Training Step: 80 Training Loss: 0.6144848465919495 \n",
      "     Training Step: 81 Training Loss: 0.6135224103927612 \n",
      "     Training Step: 82 Training Loss: 0.6154220104217529 \n",
      "     Training Step: 83 Training Loss: 0.6167658567428589 \n",
      "     Training Step: 84 Training Loss: 0.6115300059318542 \n",
      "     Training Step: 85 Training Loss: 0.6097152829170227 \n",
      "     Training Step: 86 Training Loss: 0.6146708130836487 \n",
      "     Training Step: 87 Training Loss: 0.6144395470619202 \n",
      "     Training Step: 88 Training Loss: 0.6133017539978027 \n",
      "     Training Step: 89 Training Loss: 0.6104978919029236 \n",
      "     Training Step: 90 Training Loss: 0.615178108215332 \n",
      "     Training Step: 91 Training Loss: 0.6177027225494385 \n",
      "     Training Step: 92 Training Loss: 0.6116847395896912 \n",
      "     Training Step: 93 Training Loss: 0.614243745803833 \n",
      "     Training Step: 94 Training Loss: 0.6118245124816895 \n",
      "     Training Step: 95 Training Loss: 0.6136491298675537 \n",
      "     Training Step: 96 Training Loss: 0.616735577583313 \n",
      "     Training Step: 97 Training Loss: 0.6120674014091492 \n",
      "     Training Step: 98 Training Loss: 0.6149294376373291 \n",
      "     Training Step: 99 Training Loss: 0.6125364303588867 \n",
      "     Training Step: 100 Training Loss: 0.6145964860916138 \n",
      "     Training Step: 101 Training Loss: 0.6147927045822144 \n",
      "     Training Step: 102 Training Loss: 0.6174962520599365 \n",
      "     Training Step: 103 Training Loss: 0.6114137172698975 \n",
      "     Training Step: 104 Training Loss: 0.6100520491600037 \n",
      "     Training Step: 105 Training Loss: 0.6128430962562561 \n",
      "     Training Step: 106 Training Loss: 0.611632227897644 \n",
      "     Training Step: 107 Training Loss: 0.6139076948165894 \n",
      "     Training Step: 108 Training Loss: 0.6117894649505615 \n",
      "     Training Step: 109 Training Loss: 0.6124835014343262 \n",
      "     Training Step: 110 Training Loss: 0.6153168082237244 \n",
      "     Training Step: 111 Training Loss: 0.6168320178985596 \n",
      "     Training Step: 112 Training Loss: 0.6139227151870728 \n",
      "     Training Step: 113 Training Loss: 0.6122311353683472 \n",
      "     Training Step: 114 Training Loss: 0.6155529022216797 \n",
      "     Training Step: 115 Training Loss: 0.6163654327392578 \n",
      "     Training Step: 116 Training Loss: 0.6128641366958618 \n",
      "     Training Step: 117 Training Loss: 0.6153714656829834 \n",
      "     Training Step: 118 Training Loss: 0.6127034425735474 \n",
      "     Training Step: 119 Training Loss: 0.6082893013954163 \n",
      "     Training Step: 120 Training Loss: 0.6155326962471008 \n",
      "     Training Step: 121 Training Loss: 0.6114428043365479 \n",
      "     Training Step: 122 Training Loss: 0.6143507361412048 \n",
      "     Training Step: 123 Training Loss: 0.6182315349578857 \n",
      "     Training Step: 124 Training Loss: 0.6156026721000671 \n",
      "     Training Step: 125 Training Loss: 0.6126415729522705 \n",
      "     Training Step: 126 Training Loss: 0.6130671501159668 \n",
      "     Training Step: 127 Training Loss: 0.6177167892456055 \n",
      "     Training Step: 128 Training Loss: 0.6105963587760925 \n",
      "     Training Step: 129 Training Loss: 0.6183213591575623 \n",
      "     Training Step: 130 Training Loss: 0.6097582578659058 \n",
      "     Training Step: 131 Training Loss: 0.6164164543151855 \n",
      "     Training Step: 132 Training Loss: 0.6133233308792114 \n",
      "     Training Step: 133 Training Loss: 0.6162038445472717 \n",
      "     Training Step: 134 Training Loss: 0.6143457889556885 \n",
      "     Training Step: 135 Training Loss: 0.6127995848655701 \n",
      "     Training Step: 136 Training Loss: 0.6132915616035461 \n",
      "     Training Step: 137 Training Loss: 0.6167965531349182 \n",
      "     Training Step: 138 Training Loss: 0.6185780763626099 \n",
      "     Training Step: 139 Training Loss: 0.6140722036361694 \n",
      "     Training Step: 140 Training Loss: 0.6114162802696228 \n",
      "     Training Step: 141 Training Loss: 0.6097468733787537 \n",
      "     Training Step: 142 Training Loss: 0.612467885017395 \n",
      "     Training Step: 143 Training Loss: 0.6146069169044495 \n",
      "     Training Step: 144 Training Loss: 0.6152421236038208 \n",
      "     Training Step: 145 Training Loss: 0.6106042265892029 \n",
      "     Training Step: 146 Training Loss: 0.6112008094787598 \n",
      "     Training Step: 147 Training Loss: 0.6134987473487854 \n",
      "     Training Step: 148 Training Loss: 0.615090012550354 \n",
      "     Training Step: 149 Training Loss: 0.6165078282356262 \n",
      "     Training Step: 150 Training Loss: 0.6194772720336914 \n",
      "     Training Step: 151 Training Loss: 0.611630380153656 \n",
      "     Training Step: 152 Training Loss: 0.614379346370697 \n",
      "     Training Step: 153 Training Loss: 0.6118030548095703 \n",
      "     Training Step: 154 Training Loss: 0.6122353076934814 \n",
      "     Training Step: 155 Training Loss: 0.6122830510139465 \n",
      "     Training Step: 156 Training Loss: 0.6134738326072693 \n",
      "     Training Step: 157 Training Loss: 0.6202371120452881 \n",
      "     Training Step: 158 Training Loss: 0.6133555173873901 \n",
      "     Training Step: 159 Training Loss: 0.6131415963172913 \n",
      "     Training Step: 160 Training Loss: 0.614655077457428 \n",
      "     Training Step: 161 Training Loss: 0.614699125289917 \n",
      "     Training Step: 162 Training Loss: 0.6184620261192322 \n",
      "     Training Step: 163 Training Loss: 0.6114486455917358 \n",
      "     Training Step: 164 Training Loss: 0.6136388778686523 \n",
      "     Training Step: 165 Training Loss: 0.6134621500968933 \n",
      "     Training Step: 166 Training Loss: 0.6121495962142944 \n",
      "     Training Step: 167 Training Loss: 0.6105836629867554 \n",
      "     Training Step: 168 Training Loss: 0.6151520013809204 \n",
      "     Training Step: 169 Training Loss: 0.6138165593147278 \n",
      "     Training Step: 170 Training Loss: 0.6180505156517029 \n",
      "     Training Step: 171 Training Loss: 0.6171532869338989 \n",
      "     Training Step: 172 Training Loss: 0.6166788339614868 \n",
      "     Training Step: 173 Training Loss: 0.6157867908477783 \n",
      "     Training Step: 174 Training Loss: 0.6157428622245789 \n",
      "     Training Step: 175 Training Loss: 0.6147482395172119 \n",
      "     Training Step: 176 Training Loss: 0.6167177557945251 \n",
      "     Training Step: 177 Training Loss: 0.6177991032600403 \n",
      "     Training Step: 178 Training Loss: 0.6154426336288452 \n",
      "     Training Step: 179 Training Loss: 0.6116430163383484 \n",
      "     Training Step: 180 Training Loss: 0.610544741153717 \n",
      "     Training Step: 181 Training Loss: 0.6101101636886597 \n",
      "     Training Step: 182 Training Loss: 0.6127748489379883 \n",
      "     Training Step: 183 Training Loss: 0.6171557307243347 \n",
      "     Training Step: 184 Training Loss: 0.6171939373016357 \n",
      "     Training Step: 185 Training Loss: 0.6129671335220337 \n",
      "     Training Step: 186 Training Loss: 0.6115291714668274 \n",
      "     Training Step: 187 Training Loss: 0.6182472705841064 \n",
      "     Training Step: 188 Training Loss: 0.609228253364563 \n",
      "     Training Step: 189 Training Loss: 0.6169466972351074 \n",
      "     Training Step: 190 Training Loss: 0.6103976964950562 \n",
      "     Training Step: 191 Training Loss: 0.6146441698074341 \n",
      "     Training Step: 192 Training Loss: 0.6093881130218506 \n",
      "     Training Step: 193 Training Loss: 0.6142114996910095 \n",
      "     Training Step: 194 Training Loss: 0.6106764674186707 \n",
      "     Training Step: 195 Training Loss: 0.6150820851325989 \n",
      "     Training Step: 196 Training Loss: 0.6132059693336487 \n",
      "     Training Step: 197 Training Loss: 0.6167406439781189 \n",
      "     Training Step: 198 Training Loss: 0.6167144179344177 \n",
      "     Training Step: 199 Training Loss: 0.6158488392829895 \n",
      "     Training Step: 200 Training Loss: 0.6156834959983826 \n",
      "     Training Step: 201 Training Loss: 0.613738477230072 \n",
      "     Training Step: 202 Training Loss: 0.6148685812950134 \n",
      "     Training Step: 203 Training Loss: 0.6118407845497131 \n",
      "     Training Step: 204 Training Loss: 0.6106794476509094 \n",
      "     Training Step: 205 Training Loss: 0.6140183806419373 \n",
      "     Training Step: 206 Training Loss: 0.6153966784477234 \n",
      "     Training Step: 207 Training Loss: 0.6137637495994568 \n",
      "     Training Step: 208 Training Loss: 0.6137444376945496 \n",
      "     Training Step: 209 Training Loss: 0.6177981495857239 \n",
      "     Training Step: 210 Training Loss: 0.6153815984725952 \n",
      "     Training Step: 211 Training Loss: 0.6127579808235168 \n",
      "     Training Step: 212 Training Loss: 0.6101977825164795 \n",
      "     Training Step: 213 Training Loss: 0.6115959882736206 \n",
      "     Training Step: 214 Training Loss: 0.6146711707115173 \n",
      "     Training Step: 215 Training Loss: 0.6108815670013428 \n",
      "     Training Step: 216 Training Loss: 0.6159524321556091 \n",
      "     Training Step: 217 Training Loss: 0.6134169697761536 \n",
      "     Training Step: 218 Training Loss: 0.6142890453338623 \n",
      "     Training Step: 219 Training Loss: 0.6138902902603149 \n",
      "     Training Step: 220 Training Loss: 0.6180967092514038 \n",
      "     Training Step: 221 Training Loss: 0.6174103021621704 \n",
      "     Training Step: 222 Training Loss: 0.6166728734970093 \n",
      "     Training Step: 223 Training Loss: 0.6199081540107727 \n",
      "     Training Step: 224 Training Loss: 0.6160268783569336 \n",
      "     Training Step: 225 Training Loss: 0.6149165034294128 \n",
      "     Training Step: 226 Training Loss: 0.618421733379364 \n",
      "     Training Step: 227 Training Loss: 0.6106221079826355 \n",
      "     Training Step: 228 Training Loss: 0.6168842315673828 \n",
      "     Training Step: 229 Training Loss: 0.6152932643890381 \n",
      "     Training Step: 230 Training Loss: 0.6132200360298157 \n",
      "     Training Step: 231 Training Loss: 0.6161909103393555 \n",
      "     Training Step: 232 Training Loss: 0.6153451204299927 \n",
      "     Training Step: 233 Training Loss: 0.61257004737854 \n",
      "     Training Step: 234 Training Loss: 0.6176077723503113 \n",
      "     Training Step: 235 Training Loss: 0.6114510893821716 \n",
      "     Training Step: 236 Training Loss: 0.6161113977432251 \n",
      "     Training Step: 237 Training Loss: 0.6157445907592773 \n",
      "     Training Step: 238 Training Loss: 0.6141096949577332 \n",
      "     Training Step: 239 Training Loss: 0.6132059693336487 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6116567850112915 \n",
      "     Validation Step: 1 Validation Loss: 0.6170215606689453 \n",
      "     Validation Step: 2 Validation Loss: 0.6111699938774109 \n",
      "     Validation Step: 3 Validation Loss: 0.6176016926765442 \n",
      "     Validation Step: 4 Validation Loss: 0.6136494874954224 \n",
      "     Validation Step: 5 Validation Loss: 0.6156154870986938 \n",
      "     Validation Step: 6 Validation Loss: 0.6180622577667236 \n",
      "     Validation Step: 7 Validation Loss: 0.6111862659454346 \n",
      "     Validation Step: 8 Validation Loss: 0.6104937791824341 \n",
      "     Validation Step: 9 Validation Loss: 0.615314781665802 \n",
      "     Validation Step: 10 Validation Loss: 0.6075515747070312 \n",
      "     Validation Step: 11 Validation Loss: 0.6142615675926208 \n",
      "     Validation Step: 12 Validation Loss: 0.6157892942428589 \n",
      "     Validation Step: 13 Validation Loss: 0.6141270399093628 \n",
      "     Validation Step: 14 Validation Loss: 0.614119827747345 \n",
      "     Validation Step: 15 Validation Loss: 0.617695152759552 \n",
      "     Validation Step: 16 Validation Loss: 0.6142022609710693 \n",
      "     Validation Step: 17 Validation Loss: 0.6162468791007996 \n",
      "     Validation Step: 18 Validation Loss: 0.610188364982605 \n",
      "     Validation Step: 19 Validation Loss: 0.61849445104599 \n",
      "     Validation Step: 20 Validation Loss: 0.6105256080627441 \n",
      "     Validation Step: 21 Validation Loss: 0.613645076751709 \n",
      "     Validation Step: 22 Validation Loss: 0.6128385066986084 \n",
      "     Validation Step: 23 Validation Loss: 0.6184722185134888 \n",
      "     Validation Step: 24 Validation Loss: 0.6159923076629639 \n",
      "     Validation Step: 25 Validation Loss: 0.614898145198822 \n",
      "     Validation Step: 26 Validation Loss: 0.6148372888565063 \n",
      "     Validation Step: 27 Validation Loss: 0.6182292699813843 \n",
      "     Validation Step: 28 Validation Loss: 0.6152305006980896 \n",
      "     Validation Step: 29 Validation Loss: 0.6121376156806946 \n",
      "     Validation Step: 30 Validation Loss: 0.6173079609870911 \n",
      "     Validation Step: 31 Validation Loss: 0.6115655303001404 \n",
      "     Validation Step: 32 Validation Loss: 0.6101273894309998 \n",
      "     Validation Step: 33 Validation Loss: 0.610165536403656 \n",
      "     Validation Step: 34 Validation Loss: 0.6136664152145386 \n",
      "     Validation Step: 35 Validation Loss: 0.6106514930725098 \n",
      "     Validation Step: 36 Validation Loss: 0.611889660358429 \n",
      "     Validation Step: 37 Validation Loss: 0.6145398616790771 \n",
      "     Validation Step: 38 Validation Loss: 0.6145774126052856 \n",
      "     Validation Step: 39 Validation Loss: 0.6183379292488098 \n",
      "     Validation Step: 40 Validation Loss: 0.6133127212524414 \n",
      "     Validation Step: 41 Validation Loss: 0.6129859685897827 \n",
      "     Validation Step: 42 Validation Loss: 0.6150345802307129 \n",
      "     Validation Step: 43 Validation Loss: 0.6155765056610107 \n",
      "     Validation Step: 44 Validation Loss: 0.6146379709243774 \n",
      "Epoch: 79\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.612073540687561 \n",
      "     Training Step: 1 Training Loss: 0.6094098687171936 \n",
      "     Training Step: 2 Training Loss: 0.6147820353507996 \n",
      "     Training Step: 3 Training Loss: 0.6183558106422424 \n",
      "     Training Step: 4 Training Loss: 0.6162269115447998 \n",
      "     Training Step: 5 Training Loss: 0.6124888062477112 \n",
      "     Training Step: 6 Training Loss: 0.6118300557136536 \n",
      "     Training Step: 7 Training Loss: 0.6142155528068542 \n",
      "     Training Step: 8 Training Loss: 0.6140807271003723 \n",
      "     Training Step: 9 Training Loss: 0.6144855618476868 \n",
      "     Training Step: 10 Training Loss: 0.6117984652519226 \n",
      "     Training Step: 11 Training Loss: 0.6126405000686646 \n",
      "     Training Step: 12 Training Loss: 0.6164182424545288 \n",
      "     Training Step: 13 Training Loss: 0.6141505241394043 \n",
      "     Training Step: 14 Training Loss: 0.6106914281845093 \n",
      "     Training Step: 15 Training Loss: 0.6150680780410767 \n",
      "     Training Step: 16 Training Loss: 0.6132034063339233 \n",
      "     Training Step: 17 Training Loss: 0.6132782101631165 \n",
      "     Training Step: 18 Training Loss: 0.6147279143333435 \n",
      "     Training Step: 19 Training Loss: 0.6100664138793945 \n",
      "     Training Step: 20 Training Loss: 0.6118033528327942 \n",
      "     Training Step: 21 Training Loss: 0.6116071939468384 \n",
      "     Training Step: 22 Training Loss: 0.6116256713867188 \n",
      "     Training Step: 23 Training Loss: 0.6137393116950989 \n",
      "     Training Step: 24 Training Loss: 0.6101173758506775 \n",
      "     Training Step: 25 Training Loss: 0.6128687262535095 \n",
      "     Training Step: 26 Training Loss: 0.616528332233429 \n",
      "     Training Step: 27 Training Loss: 0.6155589818954468 \n",
      "     Training Step: 28 Training Loss: 0.618123471736908 \n",
      "     Training Step: 29 Training Loss: 0.6172278523445129 \n",
      "     Training Step: 30 Training Loss: 0.6154327392578125 \n",
      "     Training Step: 31 Training Loss: 0.6168186664581299 \n",
      "     Training Step: 32 Training Loss: 0.6100580096244812 \n",
      "     Training Step: 33 Training Loss: 0.6121754050254822 \n",
      "     Training Step: 34 Training Loss: 0.6186037063598633 \n",
      "     Training Step: 35 Training Loss: 0.6139211058616638 \n",
      "     Training Step: 36 Training Loss: 0.6083142757415771 \n",
      "     Training Step: 37 Training Loss: 0.609760582447052 \n",
      "     Training Step: 38 Training Loss: 0.6180338859558105 \n",
      "     Training Step: 39 Training Loss: 0.611445426940918 \n",
      "     Training Step: 40 Training Loss: 0.6105738878250122 \n",
      "     Training Step: 41 Training Loss: 0.6171585917472839 \n",
      "     Training Step: 42 Training Loss: 0.6117846965789795 \n",
      "     Training Step: 43 Training Loss: 0.6171633005142212 \n",
      "     Training Step: 44 Training Loss: 0.6136152148246765 \n",
      "     Training Step: 45 Training Loss: 0.614018440246582 \n",
      "     Training Step: 46 Training Loss: 0.6114113330841064 \n",
      "     Training Step: 47 Training Loss: 0.6126948595046997 \n",
      "     Training Step: 48 Training Loss: 0.6121394038200378 \n",
      "     Training Step: 49 Training Loss: 0.6143589019775391 \n",
      "     Training Step: 50 Training Loss: 0.6163696646690369 \n",
      "     Training Step: 51 Training Loss: 0.617824375629425 \n",
      "     Training Step: 52 Training Loss: 0.6202497482299805 \n",
      "     Training Step: 53 Training Loss: 0.6142106056213379 \n",
      "     Training Step: 54 Training Loss: 0.6120259761810303 \n",
      "     Training Step: 55 Training Loss: 0.6146723031997681 \n",
      "     Training Step: 56 Training Loss: 0.6116068363189697 \n",
      "     Training Step: 57 Training Loss: 0.6117009520530701 \n",
      "     Training Step: 58 Training Loss: 0.6160324811935425 \n",
      "     Training Step: 59 Training Loss: 0.6116393804550171 \n",
      "     Training Step: 60 Training Loss: 0.6118492484092712 \n",
      "     Training Step: 61 Training Loss: 0.6094786524772644 \n",
      "     Training Step: 62 Training Loss: 0.6140238046646118 \n",
      "     Training Step: 63 Training Loss: 0.615604043006897 \n",
      "     Training Step: 64 Training Loss: 0.6125304102897644 \n",
      "     Training Step: 65 Training Loss: 0.6135091781616211 \n",
      "     Training Step: 66 Training Loss: 0.6158508062362671 \n",
      "     Training Step: 67 Training Loss: 0.6158003807067871 \n",
      "     Training Step: 68 Training Loss: 0.6099783778190613 \n",
      "     Training Step: 69 Training Loss: 0.6104758381843567 \n",
      "     Training Step: 70 Training Loss: 0.6137697696685791 \n",
      "     Training Step: 71 Training Loss: 0.6177151799201965 \n",
      "     Training Step: 72 Training Loss: 0.617843747138977 \n",
      "     Training Step: 73 Training Loss: 0.6142446398735046 \n",
      "     Training Step: 74 Training Loss: 0.6175052523612976 \n",
      "     Training Step: 75 Training Loss: 0.6155267953872681 \n",
      "     Training Step: 76 Training Loss: 0.6152382493019104 \n",
      "     Training Step: 77 Training Loss: 0.6125344038009644 \n",
      "     Training Step: 78 Training Loss: 0.6150901317596436 \n",
      "     Training Step: 79 Training Loss: 0.6118964552879333 \n",
      "     Training Step: 80 Training Loss: 0.6123912930488586 \n",
      "     Training Step: 81 Training Loss: 0.6111612319946289 \n",
      "     Training Step: 82 Training Loss: 0.6198862195014954 \n",
      "     Training Step: 83 Training Loss: 0.6167165637016296 \n",
      "     Training Step: 84 Training Loss: 0.6151495575904846 \n",
      "     Training Step: 85 Training Loss: 0.6146008372306824 \n",
      "     Training Step: 86 Training Loss: 0.6154746413230896 \n",
      "     Training Step: 87 Training Loss: 0.6144976615905762 \n",
      "     Training Step: 88 Training Loss: 0.6146066188812256 \n",
      "     Training Step: 89 Training Loss: 0.6100978255271912 \n",
      "     Training Step: 90 Training Loss: 0.6176305413246155 \n",
      "     Training Step: 91 Training Loss: 0.6116496920585632 \n",
      "     Training Step: 92 Training Loss: 0.6132077574729919 \n",
      "     Training Step: 93 Training Loss: 0.6144214868545532 \n",
      "     Training Step: 94 Training Loss: 0.6173866987228394 \n",
      "     Training Step: 95 Training Loss: 0.614668607711792 \n",
      "     Training Step: 96 Training Loss: 0.6123790144920349 \n",
      "     Training Step: 97 Training Loss: 0.6143435835838318 \n",
      "     Training Step: 98 Training Loss: 0.6129659414291382 \n",
      "     Training Step: 99 Training Loss: 0.6196337938308716 \n",
      "     Training Step: 100 Training Loss: 0.6148670315742493 \n",
      "     Training Step: 101 Training Loss: 0.6182209253311157 \n",
      "     Training Step: 102 Training Loss: 0.6146862506866455 \n",
      "     Training Step: 103 Training Loss: 0.6139283776283264 \n",
      "     Training Step: 104 Training Loss: 0.6159417033195496 \n",
      "     Training Step: 105 Training Loss: 0.6185596585273743 \n",
      "     Training Step: 106 Training Loss: 0.6130792498588562 \n",
      "     Training Step: 107 Training Loss: 0.6184230446815491 \n",
      "     Training Step: 108 Training Loss: 0.6209036111831665 \n",
      "     Training Step: 109 Training Loss: 0.610781192779541 \n",
      "     Training Step: 110 Training Loss: 0.6121832132339478 \n",
      "     Training Step: 111 Training Loss: 0.6161891222000122 \n",
      "     Training Step: 112 Training Loss: 0.610777735710144 \n",
      "     Training Step: 113 Training Loss: 0.6196757555007935 \n",
      "     Training Step: 114 Training Loss: 0.6157572865486145 \n",
      "     Training Step: 115 Training Loss: 0.612851619720459 \n",
      "     Training Step: 116 Training Loss: 0.6106202602386475 \n",
      "     Training Step: 117 Training Loss: 0.6166665554046631 \n",
      "     Training Step: 118 Training Loss: 0.6149349212646484 \n",
      "     Training Step: 119 Training Loss: 0.6155346035957336 \n",
      "     Training Step: 120 Training Loss: 0.6171630024909973 \n",
      "     Training Step: 121 Training Loss: 0.6135876178741455 \n",
      "     Training Step: 122 Training Loss: 0.6144424676895142 \n",
      "     Training Step: 123 Training Loss: 0.6114218235015869 \n",
      "     Training Step: 124 Training Loss: 0.6153783798217773 \n",
      "     Training Step: 125 Training Loss: 0.6092422008514404 \n",
      "     Training Step: 126 Training Loss: 0.6184676885604858 \n",
      "     Training Step: 127 Training Loss: 0.611440122127533 \n",
      "     Training Step: 128 Training Loss: 0.6131179928779602 \n",
      "     Training Step: 129 Training Loss: 0.6167672276496887 \n",
      "     Training Step: 130 Training Loss: 0.6152938008308411 \n",
      "     Training Step: 131 Training Loss: 0.614339292049408 \n",
      "     Training Step: 132 Training Loss: 0.610916793346405 \n",
      "     Training Step: 133 Training Loss: 0.6103853583335876 \n",
      "     Training Step: 134 Training Loss: 0.6156835556030273 \n",
      "     Training Step: 135 Training Loss: 0.6160030364990234 \n",
      "     Training Step: 136 Training Loss: 0.6115322709083557 \n",
      "     Training Step: 137 Training Loss: 0.6128686666488647 \n",
      "     Training Step: 138 Training Loss: 0.6134536266326904 \n",
      "     Training Step: 139 Training Loss: 0.6140322685241699 \n",
      "     Training Step: 140 Training Loss: 0.6133111119270325 \n",
      "     Training Step: 141 Training Loss: 0.6147100925445557 \n",
      "     Training Step: 142 Training Loss: 0.6113923788070679 \n",
      "     Training Step: 143 Training Loss: 0.612286388874054 \n",
      "     Training Step: 144 Training Loss: 0.616278886795044 \n",
      "     Training Step: 145 Training Loss: 0.6142883896827698 \n",
      "     Training Step: 146 Training Loss: 0.6106497049331665 \n",
      "     Training Step: 147 Training Loss: 0.6171170473098755 \n",
      "     Training Step: 148 Training Loss: 0.6143845915794373 \n",
      "     Training Step: 149 Training Loss: 0.6166828870773315 \n",
      "     Training Step: 150 Training Loss: 0.6103960275650024 \n",
      "     Training Step: 151 Training Loss: 0.6122819781303406 \n",
      "     Training Step: 152 Training Loss: 0.61681067943573 \n",
      "     Training Step: 153 Training Loss: 0.6152876019477844 \n",
      "     Training Step: 154 Training Loss: 0.613649308681488 \n",
      "     Training Step: 155 Training Loss: 0.6097174882888794 \n",
      "     Training Step: 156 Training Loss: 0.618108332157135 \n",
      "     Training Step: 157 Training Loss: 0.613292932510376 \n",
      "     Training Step: 158 Training Loss: 0.6128013134002686 \n",
      "     Training Step: 159 Training Loss: 0.6164228320121765 \n",
      "     Training Step: 160 Training Loss: 0.6131848692893982 \n",
      "     Training Step: 161 Training Loss: 0.6157459616661072 \n",
      "     Training Step: 162 Training Loss: 0.6134703159332275 \n",
      "     Training Step: 163 Training Loss: 0.6166528463363647 \n",
      "     Training Step: 164 Training Loss: 0.6101934909820557 \n",
      "     Training Step: 165 Training Loss: 0.6188502907752991 \n",
      "     Training Step: 166 Training Loss: 0.6177079677581787 \n",
      "     Training Step: 167 Training Loss: 0.6147077083587646 \n",
      "     Training Step: 168 Training Loss: 0.6168006658554077 \n",
      "     Training Step: 169 Training Loss: 0.6111806631088257 \n",
      "     Training Step: 170 Training Loss: 0.610607385635376 \n",
      "     Training Step: 171 Training Loss: 0.6129120588302612 \n",
      "     Training Step: 172 Training Loss: 0.6144549250602722 \n",
      "     Training Step: 173 Training Loss: 0.6154050230979919 \n",
      "     Training Step: 174 Training Loss: 0.6134344339370728 \n",
      "     Training Step: 175 Training Loss: 0.616887629032135 \n",
      "     Training Step: 176 Training Loss: 0.6112094521522522 \n",
      "     Training Step: 177 Training Loss: 0.6130698919296265 \n",
      "     Training Step: 178 Training Loss: 0.6131413578987122 \n",
      "     Training Step: 179 Training Loss: 0.6153017282485962 \n",
      "     Training Step: 180 Training Loss: 0.6127661466598511 \n",
      "     Training Step: 181 Training Loss: 0.6118271350860596 \n",
      "     Training Step: 182 Training Loss: 0.6177713871002197 \n",
      "     Training Step: 183 Training Loss: 0.6115507483482361 \n",
      "     Training Step: 184 Training Loss: 0.612750232219696 \n",
      "     Training Step: 185 Training Loss: 0.6166695356369019 \n",
      "     Training Step: 186 Training Loss: 0.6146373748779297 \n",
      "     Training Step: 187 Training Loss: 0.6124669313430786 \n",
      "     Training Step: 188 Training Loss: 0.6147922277450562 \n",
      "     Training Step: 189 Training Loss: 0.6122338175773621 \n",
      "     Training Step: 190 Training Loss: 0.610596776008606 \n",
      "     Training Step: 191 Training Loss: 0.6114763021469116 \n",
      "     Training Step: 192 Training Loss: 0.6104922890663147 \n",
      "     Training Step: 193 Training Loss: 0.6141142845153809 \n",
      "     Training Step: 194 Training Loss: 0.6125091314315796 \n",
      "     Training Step: 195 Training Loss: 0.6152911186218262 \n",
      "     Training Step: 196 Training Loss: 0.6147657036781311 \n",
      "     Training Step: 197 Training Loss: 0.6167506575584412 \n",
      "     Training Step: 198 Training Loss: 0.6153506636619568 \n",
      "     Training Step: 199 Training Loss: 0.6122321486473083 \n",
      "     Training Step: 200 Training Loss: 0.6157554984092712 \n",
      "     Training Step: 201 Training Loss: 0.609723687171936 \n",
      "     Training Step: 202 Training Loss: 0.6133538484573364 \n",
      "     Training Step: 203 Training Loss: 0.616628885269165 \n",
      "     Training Step: 204 Training Loss: 0.6134971380233765 \n",
      "     Training Step: 205 Training Loss: 0.615446150302887 \n",
      "     Training Step: 206 Training Loss: 0.614959180355072 \n",
      "     Training Step: 207 Training Loss: 0.6108942627906799 \n",
      "     Training Step: 208 Training Loss: 0.6151013970375061 \n",
      "     Training Step: 209 Training Loss: 0.612130880355835 \n",
      "     Training Step: 210 Training Loss: 0.6129286885261536 \n",
      "     Training Step: 211 Training Loss: 0.6149176359176636 \n",
      "     Training Step: 212 Training Loss: 0.6194456815719604 \n",
      "     Training Step: 213 Training Loss: 0.6115354299545288 \n",
      "     Training Step: 214 Training Loss: 0.6184046268463135 \n",
      "     Training Step: 215 Training Loss: 0.6188644170761108 \n",
      "     Training Step: 216 Training Loss: 0.6167118549346924 \n",
      "     Training Step: 217 Training Loss: 0.6136429309844971 \n",
      "     Training Step: 218 Training Loss: 0.6167107224464417 \n",
      "     Training Step: 219 Training Loss: 0.6146761178970337 \n",
      "     Training Step: 220 Training Loss: 0.6133109331130981 \n",
      "     Training Step: 221 Training Loss: 0.6154032349586487 \n",
      "     Training Step: 222 Training Loss: 0.6133098006248474 \n",
      "     Training Step: 223 Training Loss: 0.6182262301445007 \n",
      "     Training Step: 224 Training Loss: 0.6122413277626038 \n",
      "     Training Step: 225 Training Loss: 0.6151748299598694 \n",
      "     Training Step: 226 Training Loss: 0.6146063804626465 \n",
      "     Training Step: 227 Training Loss: 0.6156718730926514 \n",
      "     Training Step: 228 Training Loss: 0.6180083155632019 \n",
      "     Training Step: 229 Training Loss: 0.6141576766967773 \n",
      "     Training Step: 230 Training Loss: 0.6137476563453674 \n",
      "     Training Step: 231 Training Loss: 0.6138233542442322 \n",
      "     Training Step: 232 Training Loss: 0.6153714060783386 \n",
      "     Training Step: 233 Training Loss: 0.6176819801330566 \n",
      "     Training Step: 234 Training Loss: 0.6115427017211914 \n",
      "     Training Step: 235 Training Loss: 0.6161103248596191 \n",
      "     Training Step: 236 Training Loss: 0.6146466732025146 \n",
      "     Training Step: 237 Training Loss: 0.6138882637023926 \n",
      "     Training Step: 238 Training Loss: 0.6123827695846558 \n",
      "     Training Step: 239 Training Loss: 0.6169241666793823 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6121364235877991 \n",
      "     Validation Step: 1 Validation Loss: 0.6111714243888855 \n",
      "     Validation Step: 2 Validation Loss: 0.6155733466148376 \n",
      "     Validation Step: 3 Validation Loss: 0.6118887662887573 \n",
      "     Validation Step: 4 Validation Loss: 0.617298424243927 \n",
      "     Validation Step: 5 Validation Loss: 0.6141160130500793 \n",
      "     Validation Step: 6 Validation Loss: 0.611186683177948 \n",
      "     Validation Step: 7 Validation Loss: 0.6136520504951477 \n",
      "     Validation Step: 8 Validation Loss: 0.6133098006248474 \n",
      "     Validation Step: 9 Validation Loss: 0.6115660071372986 \n",
      "     Validation Step: 10 Validation Loss: 0.6116529107093811 \n",
      "     Validation Step: 11 Validation Loss: 0.6176899671554565 \n",
      "     Validation Step: 12 Validation Loss: 0.6146369576454163 \n",
      "     Validation Step: 13 Validation Loss: 0.6101853251457214 \n",
      "     Validation Step: 14 Validation Loss: 0.6148946285247803 \n",
      "     Validation Step: 15 Validation Loss: 0.6129872798919678 \n",
      "     Validation Step: 16 Validation Loss: 0.6136689782142639 \n",
      "     Validation Step: 17 Validation Loss: 0.6142582893371582 \n",
      "     Validation Step: 18 Validation Loss: 0.6159940958023071 \n",
      "     Validation Step: 19 Validation Loss: 0.6184645295143127 \n",
      "     Validation Step: 20 Validation Loss: 0.6180561780929565 \n",
      "     Validation Step: 21 Validation Loss: 0.6162424683570862 \n",
      "     Validation Step: 22 Validation Loss: 0.6106480360031128 \n",
      "     Validation Step: 23 Validation Loss: 0.6145368218421936 \n",
      "     Validation Step: 24 Validation Loss: 0.6145692467689514 \n",
      "     Validation Step: 25 Validation Loss: 0.6104965209960938 \n",
      "     Validation Step: 26 Validation Loss: 0.6153147220611572 \n",
      "     Validation Step: 27 Validation Loss: 0.6136382222175598 \n",
      "     Validation Step: 28 Validation Loss: 0.6175914406776428 \n",
      "     Validation Step: 29 Validation Loss: 0.6142007112503052 \n",
      "     Validation Step: 30 Validation Loss: 0.618487536907196 \n",
      "     Validation Step: 31 Validation Loss: 0.6182270646095276 \n",
      "     Validation Step: 32 Validation Loss: 0.6101641058921814 \n",
      "     Validation Step: 33 Validation Loss: 0.6148346066474915 \n",
      "     Validation Step: 34 Validation Loss: 0.6075548529624939 \n",
      "     Validation Step: 35 Validation Loss: 0.6183357238769531 \n",
      "     Validation Step: 36 Validation Loss: 0.6150370836257935 \n",
      "     Validation Step: 37 Validation Loss: 0.6141283512115479 \n",
      "     Validation Step: 38 Validation Loss: 0.6156125664710999 \n",
      "     Validation Step: 39 Validation Loss: 0.6101307272911072 \n",
      "     Validation Step: 40 Validation Loss: 0.615787923336029 \n",
      "     Validation Step: 41 Validation Loss: 0.617017388343811 \n",
      "     Validation Step: 42 Validation Loss: 0.610527515411377 \n",
      "     Validation Step: 43 Validation Loss: 0.6128357648849487 \n",
      "     Validation Step: 44 Validation Loss: 0.6152256727218628 \n",
      "Epoch: 80\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.609745442867279 \n",
      "     Training Step: 1 Training Loss: 0.6121467351913452 \n",
      "     Training Step: 2 Training Loss: 0.6128008961677551 \n",
      "     Training Step: 3 Training Loss: 0.6174927949905396 \n",
      "     Training Step: 4 Training Loss: 0.6147050857543945 \n",
      "     Training Step: 5 Training Loss: 0.6101891398429871 \n",
      "     Training Step: 6 Training Loss: 0.6134241819381714 \n",
      "     Training Step: 7 Training Loss: 0.6149340867996216 \n",
      "     Training Step: 8 Training Loss: 0.6172059178352356 \n",
      "     Training Step: 9 Training Loss: 0.6167727708816528 \n",
      "     Training Step: 10 Training Loss: 0.6097362637519836 \n",
      "     Training Step: 11 Training Loss: 0.6117992401123047 \n",
      "     Training Step: 12 Training Loss: 0.6123784184455872 \n",
      "     Training Step: 13 Training Loss: 0.618455171585083 \n",
      "     Training Step: 14 Training Loss: 0.6118782162666321 \n",
      "     Training Step: 15 Training Loss: 0.6094029545783997 \n",
      "     Training Step: 16 Training Loss: 0.6130756139755249 \n",
      "     Training Step: 17 Training Loss: 0.6160116195678711 \n",
      "     Training Step: 18 Training Loss: 0.6167317628860474 \n",
      "     Training Step: 19 Training Loss: 0.6142417788505554 \n",
      "     Training Step: 20 Training Loss: 0.61626797914505 \n",
      "     Training Step: 21 Training Loss: 0.6125074028968811 \n",
      "     Training Step: 22 Training Loss: 0.6157893538475037 \n",
      "     Training Step: 23 Training Loss: 0.6132785677909851 \n",
      "     Training Step: 24 Training Loss: 0.6118353605270386 \n",
      "     Training Step: 25 Training Loss: 0.6144566535949707 \n",
      "     Training Step: 26 Training Loss: 0.6116924285888672 \n",
      "     Training Step: 27 Training Loss: 0.6161166429519653 \n",
      "     Training Step: 28 Training Loss: 0.6170995831489563 \n",
      "     Training Step: 29 Training Loss: 0.6159512996673584 \n",
      "     Training Step: 30 Training Loss: 0.6167169809341431 \n",
      "     Training Step: 31 Training Loss: 0.6151517629623413 \n",
      "     Training Step: 32 Training Loss: 0.6196226477622986 \n",
      "     Training Step: 33 Training Loss: 0.6138871908187866 \n",
      "     Training Step: 34 Training Loss: 0.6128852963447571 \n",
      "     Training Step: 35 Training Loss: 0.6157582402229309 \n",
      "     Training Step: 36 Training Loss: 0.6166678667068481 \n",
      "     Training Step: 37 Training Loss: 0.6140767335891724 \n",
      "     Training Step: 38 Training Loss: 0.6126623749732971 \n",
      "     Training Step: 39 Training Loss: 0.6161916851997375 \n",
      "     Training Step: 40 Training Loss: 0.6118050813674927 \n",
      "     Training Step: 41 Training Loss: 0.6101135611534119 \n",
      "     Training Step: 42 Training Loss: 0.6133584380149841 \n",
      "     Training Step: 43 Training Loss: 0.610062837600708 \n",
      "     Training Step: 44 Training Loss: 0.614730179309845 \n",
      "     Training Step: 45 Training Loss: 0.6164163947105408 \n",
      "     Training Step: 46 Training Loss: 0.6177091002464294 \n",
      "     Training Step: 47 Training Loss: 0.6143513917922974 \n",
      "     Training Step: 48 Training Loss: 0.6180524826049805 \n",
      "     Training Step: 49 Training Loss: 0.614039957523346 \n",
      "     Training Step: 50 Training Loss: 0.6148746609687805 \n",
      "     Training Step: 51 Training Loss: 0.6181154251098633 \n",
      "     Training Step: 52 Training Loss: 0.6115649342536926 \n",
      "     Training Step: 53 Training Loss: 0.6130625605583191 \n",
      "     Training Step: 54 Training Loss: 0.6146695613861084 \n",
      "     Training Step: 55 Training Loss: 0.6155306696891785 \n",
      "     Training Step: 56 Training Loss: 0.6142165660858154 \n",
      "     Training Step: 57 Training Loss: 0.6111773252487183 \n",
      "     Training Step: 58 Training Loss: 0.614281415939331 \n",
      "     Training Step: 59 Training Loss: 0.6127744913101196 \n",
      "     Training Step: 60 Training Loss: 0.6188669800758362 \n",
      "     Training Step: 61 Training Loss: 0.6129725575447083 \n",
      "     Training Step: 62 Training Loss: 0.6106739640235901 \n",
      "     Training Step: 63 Training Loss: 0.6151727437973022 \n",
      "     Training Step: 64 Training Loss: 0.6105908155441284 \n",
      "     Training Step: 65 Training Loss: 0.6166621446609497 \n",
      "     Training Step: 66 Training Loss: 0.6141070127487183 \n",
      "     Training Step: 67 Training Loss: 0.6122338175773621 \n",
      "     Training Step: 68 Training Loss: 0.6183489561080933 \n",
      "     Training Step: 69 Training Loss: 0.6107129454612732 \n",
      "     Training Step: 70 Training Loss: 0.6121657490730286 \n",
      "     Training Step: 71 Training Loss: 0.6157562136650085 \n",
      "     Training Step: 72 Training Loss: 0.6146096587181091 \n",
      "     Training Step: 73 Training Loss: 0.6171798706054688 \n",
      "     Training Step: 74 Training Loss: 0.6125282645225525 \n",
      "     Training Step: 75 Training Loss: 0.6146776080131531 \n",
      "     Training Step: 76 Training Loss: 0.6136489510536194 \n",
      "     Training Step: 77 Training Loss: 0.611846923828125 \n",
      "     Training Step: 78 Training Loss: 0.6194502711296082 \n",
      "     Training Step: 79 Training Loss: 0.6178069114685059 \n",
      "     Training Step: 80 Training Loss: 0.6115398406982422 \n",
      "     Training Step: 81 Training Loss: 0.6127604842185974 \n",
      "     Training Step: 82 Training Loss: 0.6135225892066956 \n",
      "     Training Step: 83 Training Loss: 0.6146352887153625 \n",
      "     Training Step: 84 Training Loss: 0.6131317019462585 \n",
      "     Training Step: 85 Training Loss: 0.616647481918335 \n",
      "     Training Step: 86 Training Loss: 0.6104223728179932 \n",
      "     Training Step: 87 Training Loss: 0.613818883895874 \n",
      "     Training Step: 88 Training Loss: 0.6121587753295898 \n",
      "     Training Step: 89 Training Loss: 0.6107409596443176 \n",
      "     Training Step: 90 Training Loss: 0.610572099685669 \n",
      "     Training Step: 91 Training Loss: 0.6186107397079468 \n",
      "     Training Step: 92 Training Loss: 0.6115149259567261 \n",
      "     Training Step: 93 Training Loss: 0.6120622754096985 \n",
      "     Training Step: 94 Training Loss: 0.6155412197113037 \n",
      "     Training Step: 95 Training Loss: 0.6184515953063965 \n",
      "     Training Step: 96 Training Loss: 0.616958498954773 \n",
      "     Training Step: 97 Training Loss: 0.6144256591796875 \n",
      "     Training Step: 98 Training Loss: 0.6122249364852905 \n",
      "     Training Step: 99 Training Loss: 0.6176987290382385 \n",
      "     Training Step: 100 Training Loss: 0.6177652478218079 \n",
      "     Training Step: 101 Training Loss: 0.6133238077163696 \n",
      "     Training Step: 102 Training Loss: 0.6184503436088562 \n",
      "     Training Step: 103 Training Loss: 0.6147028207778931 \n",
      "     Training Step: 104 Training Loss: 0.6115233898162842 \n",
      "     Training Step: 105 Training Loss: 0.6092972159385681 \n",
      "     Training Step: 106 Training Loss: 0.612314760684967 \n",
      "     Training Step: 107 Training Loss: 0.6152858734130859 \n",
      "     Training Step: 108 Training Loss: 0.6124765872955322 \n",
      "     Training Step: 109 Training Loss: 0.6162043809890747 \n",
      "     Training Step: 110 Training Loss: 0.610920786857605 \n",
      "     Training Step: 111 Training Loss: 0.6147934794425964 \n",
      "     Training Step: 112 Training Loss: 0.6154288649559021 \n",
      "     Training Step: 113 Training Loss: 0.6111959218978882 \n",
      "     Training Step: 114 Training Loss: 0.6171579957008362 \n",
      "     Training Step: 115 Training Loss: 0.6123723387718201 \n",
      "     Training Step: 116 Training Loss: 0.6168243885040283 \n",
      "     Training Step: 117 Training Loss: 0.6132069826126099 \n",
      "     Training Step: 118 Training Loss: 0.61542809009552 \n",
      "     Training Step: 119 Training Loss: 0.6178142428398132 \n",
      "     Training Step: 120 Training Loss: 0.6100648045539856 \n",
      "     Training Step: 121 Training Loss: 0.6143524050712585 \n",
      "     Training Step: 122 Training Loss: 0.6129136085510254 \n",
      "     Training Step: 123 Training Loss: 0.6129217147827148 \n",
      "     Training Step: 124 Training Loss: 0.6128446459770203 \n",
      "     Training Step: 125 Training Loss: 0.6142162680625916 \n",
      "     Training Step: 126 Training Loss: 0.6168922185897827 \n",
      "     Training Step: 127 Training Loss: 0.6182201504707336 \n",
      "     Training Step: 128 Training Loss: 0.6135022044181824 \n",
      "     Training Step: 129 Training Loss: 0.6149551272392273 \n",
      "     Training Step: 130 Training Loss: 0.6150875091552734 \n",
      "     Training Step: 131 Training Loss: 0.6152917146682739 \n",
      "     Training Step: 132 Training Loss: 0.6132122278213501 \n",
      "     Training Step: 133 Training Loss: 0.6139285564422607 \n",
      "     Training Step: 134 Training Loss: 0.6121334433555603 \n",
      "     Training Step: 135 Training Loss: 0.6157388687133789 \n",
      "     Training Step: 136 Training Loss: 0.6171355843544006 \n",
      "     Training Step: 137 Training Loss: 0.6146500706672668 \n",
      "     Training Step: 138 Training Loss: 0.612028956413269 \n",
      "     Training Step: 139 Training Loss: 0.6116437911987305 \n",
      "     Training Step: 140 Training Loss: 0.613765299320221 \n",
      "     Training Step: 141 Training Loss: 0.6128657460212708 \n",
      "     Training Step: 142 Training Loss: 0.6136342287063599 \n",
      "     Training Step: 143 Training Loss: 0.6143450140953064 \n",
      "     Training Step: 144 Training Loss: 0.6118347644805908 \n",
      "     Training Step: 145 Training Loss: 0.612228274345398 \n",
      "     Training Step: 146 Training Loss: 0.6099830865859985 \n",
      "     Training Step: 147 Training Loss: 0.61048823595047 \n",
      "     Training Step: 148 Training Loss: 0.6135959625244141 \n",
      "     Training Step: 149 Training Loss: 0.610359787940979 \n",
      "     Training Step: 150 Training Loss: 0.6186596155166626 \n",
      "     Training Step: 151 Training Loss: 0.6118040084838867 \n",
      "     Training Step: 152 Training Loss: 0.6147626042366028 \n",
      "     Training Step: 153 Training Loss: 0.6164491772651672 \n",
      "     Training Step: 154 Training Loss: 0.6137334704399109 \n",
      "     Training Step: 155 Training Loss: 0.6149214506149292 \n",
      "     Training Step: 156 Training Loss: 0.6144840121269226 \n",
      "     Training Step: 157 Training Loss: 0.6131863594055176 \n",
      "     Training Step: 158 Training Loss: 0.6127074956893921 \n",
      "     Training Step: 159 Training Loss: 0.6188509464263916 \n",
      "     Training Step: 160 Training Loss: 0.6134645342826843 \n",
      "     Training Step: 161 Training Loss: 0.6146090626716614 \n",
      "     Training Step: 162 Training Loss: 0.6114330887794495 \n",
      "     Training Step: 163 Training Loss: 0.6167129874229431 \n",
      "     Training Step: 164 Training Loss: 0.6105981469154358 \n",
      "     Training Step: 165 Training Loss: 0.6123911738395691 \n",
      "     Training Step: 166 Training Loss: 0.6116513013839722 \n",
      "     Training Step: 167 Training Loss: 0.6124952435493469 \n",
      "     Training Step: 168 Training Loss: 0.615834653377533 \n",
      "     Training Step: 169 Training Loss: 0.6114022135734558 \n",
      "     Training Step: 170 Training Loss: 0.6147756576538086 \n",
      "     Training Step: 171 Training Loss: 0.6209827661514282 \n",
      "     Training Step: 172 Training Loss: 0.6152723431587219 \n",
      "     Training Step: 173 Training Loss: 0.6156958341598511 \n",
      "     Training Step: 174 Training Loss: 0.6101366877555847 \n",
      "     Training Step: 175 Training Loss: 0.6114358901977539 \n",
      "     Training Step: 176 Training Loss: 0.6146708726882935 \n",
      "     Training Step: 177 Training Loss: 0.6106879711151123 \n",
      "     Training Step: 178 Training Loss: 0.6151021122932434 \n",
      "     Training Step: 179 Training Loss: 0.6104931235313416 \n",
      "     Training Step: 180 Training Loss: 0.6153731346130371 \n",
      "     Training Step: 181 Training Loss: 0.6116099953651428 \n",
      "     Training Step: 182 Training Loss: 0.6111370325088501 \n",
      "     Training Step: 183 Training Loss: 0.6144998073577881 \n",
      "     Training Step: 184 Training Loss: 0.6115795969963074 \n",
      "     Training Step: 185 Training Loss: 0.613134503364563 \n",
      "     Training Step: 186 Training Loss: 0.6134743094444275 \n",
      "     Training Step: 187 Training Loss: 0.6154844760894775 \n",
      "     Training Step: 188 Training Loss: 0.6153441667556763 \n",
      "     Training Step: 189 Training Loss: 0.6125218272209167 \n",
      "     Training Step: 190 Training Loss: 0.617675244808197 \n",
      "     Training Step: 191 Training Loss: 0.6199348568916321 \n",
      "     Training Step: 192 Training Loss: 0.6156073808670044 \n",
      "     Training Step: 193 Training Loss: 0.614020824432373 \n",
      "     Training Step: 194 Training Loss: 0.6182239055633545 \n",
      "     Training Step: 195 Training Loss: 0.6132951974868774 \n",
      "     Training Step: 196 Training Loss: 0.6141602396965027 \n",
      "     Training Step: 197 Training Loss: 0.6106486916542053 \n",
      "     Training Step: 198 Training Loss: 0.6173743605613708 \n",
      "     Training Step: 199 Training Loss: 0.6167953014373779 \n",
      "     Training Step: 200 Training Loss: 0.6166701912879944 \n",
      "     Training Step: 201 Training Loss: 0.6180545091629028 \n",
      "     Training Step: 202 Training Loss: 0.6141534447669983 \n",
      "     Training Step: 203 Training Loss: 0.6146923899650574 \n",
      "     Training Step: 204 Training Loss: 0.6097618937492371 \n",
      "     Training Step: 205 Training Loss: 0.6137513518333435 \n",
      "     Training Step: 206 Training Loss: 0.6132987141609192 \n",
      "     Training Step: 207 Training Loss: 0.6146118640899658 \n",
      "     Training Step: 208 Training Loss: 0.6152953505516052 \n",
      "     Training Step: 209 Training Loss: 0.6168099641799927 \n",
      "     Training Step: 210 Training Loss: 0.6122946739196777 \n",
      "     Training Step: 211 Training Loss: 0.6082845330238342 \n",
      "     Training Step: 212 Training Loss: 0.6140180230140686 \n",
      "     Training Step: 213 Training Loss: 0.615454912185669 \n",
      "     Training Step: 214 Training Loss: 0.6163647174835205 \n",
      "     Training Step: 215 Training Loss: 0.6133054494857788 \n",
      "     Training Step: 216 Training Loss: 0.6160474419593811 \n",
      "     Training Step: 217 Training Loss: 0.6167325377464294 \n",
      "     Training Step: 218 Training Loss: 0.6150721311569214 \n",
      "     Training Step: 219 Training Loss: 0.615240216255188 \n",
      "     Training Step: 220 Training Loss: 0.6180394291877747 \n",
      "     Training Step: 221 Training Loss: 0.6153883934020996 \n",
      "     Training Step: 222 Training Loss: 0.615683913230896 \n",
      "     Training Step: 223 Training Loss: 0.6116596460342407 \n",
      "     Training Step: 224 Training Loss: 0.6196901798248291 \n",
      "     Training Step: 225 Training Loss: 0.6176983714103699 \n",
      "     Training Step: 226 Training Loss: 0.611575186252594 \n",
      "     Training Step: 227 Training Loss: 0.6114599704742432 \n",
      "     Training Step: 228 Training Loss: 0.6201937198638916 \n",
      "     Training Step: 229 Training Loss: 0.6136419177055359 \n",
      "     Training Step: 230 Training Loss: 0.6164796948432922 \n",
      "     Training Step: 231 Training Loss: 0.6166076064109802 \n",
      "     Training Step: 232 Training Loss: 0.615373969078064 \n",
      "     Training Step: 233 Training Loss: 0.6114707589149475 \n",
      "     Training Step: 234 Training Loss: 0.6144468784332275 \n",
      "     Training Step: 235 Training Loss: 0.6139266490936279 \n",
      "     Training Step: 236 Training Loss: 0.6109000444412231 \n",
      "     Training Step: 237 Training Loss: 0.6094693541526794 \n",
      "     Training Step: 238 Training Loss: 0.6143735647201538 \n",
      "     Training Step: 239 Training Loss: 0.615549623966217 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.617331326007843 \n",
      "     Validation Step: 1 Validation Loss: 0.6158000230789185 \n",
      "     Validation Step: 2 Validation Loss: 0.614117443561554 \n",
      "     Validation Step: 3 Validation Loss: 0.6142057776451111 \n",
      "     Validation Step: 4 Validation Loss: 0.615595281124115 \n",
      "     Validation Step: 5 Validation Loss: 0.6150454878807068 \n",
      "     Validation Step: 6 Validation Loss: 0.6101325750350952 \n",
      "     Validation Step: 7 Validation Loss: 0.6149064898490906 \n",
      "     Validation Step: 8 Validation Loss: 0.607505202293396 \n",
      "     Validation Step: 9 Validation Loss: 0.6177159547805786 \n",
      "     Validation Step: 10 Validation Loss: 0.6136537790298462 \n",
      "     Validation Step: 11 Validation Loss: 0.6142666339874268 \n",
      "     Validation Step: 12 Validation Loss: 0.6141260266304016 \n",
      "     Validation Step: 13 Validation Loss: 0.6148398518562317 \n",
      "     Validation Step: 14 Validation Loss: 0.6133109927177429 \n",
      "     Validation Step: 15 Validation Loss: 0.6100946664810181 \n",
      "     Validation Step: 16 Validation Loss: 0.6111718416213989 \n",
      "     Validation Step: 17 Validation Loss: 0.6136415004730225 \n",
      "     Validation Step: 18 Validation Loss: 0.61601322889328 \n",
      "     Validation Step: 19 Validation Loss: 0.6106278300285339 \n",
      "     Validation Step: 20 Validation Loss: 0.6115490794181824 \n",
      "     Validation Step: 21 Validation Loss: 0.6170516610145569 \n",
      "     Validation Step: 22 Validation Loss: 0.6182600259780884 \n",
      "     Validation Step: 23 Validation Loss: 0.6185050010681152 \n",
      "     Validation Step: 24 Validation Loss: 0.6183680295944214 \n",
      "     Validation Step: 25 Validation Loss: 0.6116412878036499 \n",
      "     Validation Step: 26 Validation Loss: 0.6104761958122253 \n",
      "     Validation Step: 27 Validation Loss: 0.6128275990486145 \n",
      "     Validation Step: 28 Validation Loss: 0.6185274124145508 \n",
      "     Validation Step: 29 Validation Loss: 0.6145846843719482 \n",
      "     Validation Step: 30 Validation Loss: 0.6136647462844849 \n",
      "     Validation Step: 31 Validation Loss: 0.6145443320274353 \n",
      "     Validation Step: 32 Validation Loss: 0.6176233291625977 \n",
      "     Validation Step: 33 Validation Loss: 0.6146416664123535 \n",
      "     Validation Step: 34 Validation Loss: 0.6156253218650818 \n",
      "     Validation Step: 35 Validation Loss: 0.6105034947395325 \n",
      "     Validation Step: 36 Validation Loss: 0.6153263449668884 \n",
      "     Validation Step: 37 Validation Loss: 0.6129770874977112 \n",
      "     Validation Step: 38 Validation Loss: 0.6162601709365845 \n",
      "     Validation Step: 39 Validation Loss: 0.6101647615432739 \n",
      "     Validation Step: 40 Validation Loss: 0.6121227145195007 \n",
      "     Validation Step: 41 Validation Loss: 0.6118710041046143 \n",
      "     Validation Step: 42 Validation Loss: 0.6180904507637024 \n",
      "     Validation Step: 43 Validation Loss: 0.6111502647399902 \n",
      "     Validation Step: 44 Validation Loss: 0.6152378916740417 \n",
      "Epoch: 81\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6182557344436646 \n",
      "     Training Step: 1 Training Loss: 0.6138212084770203 \n",
      "     Training Step: 2 Training Loss: 0.6129128932952881 \n",
      "     Training Step: 3 Training Loss: 0.6109047532081604 \n",
      "     Training Step: 4 Training Loss: 0.615180253982544 \n",
      "     Training Step: 5 Training Loss: 0.6125099062919617 \n",
      "     Training Step: 6 Training Loss: 0.6104950904846191 \n",
      "     Training Step: 7 Training Loss: 0.614206075668335 \n",
      "     Training Step: 8 Training Loss: 0.6129177808761597 \n",
      "     Training Step: 9 Training Loss: 0.6121516227722168 \n",
      "     Training Step: 10 Training Loss: 0.6132055521011353 \n",
      "     Training Step: 11 Training Loss: 0.6141543388366699 \n",
      "     Training Step: 12 Training Loss: 0.6122397184371948 \n",
      "     Training Step: 13 Training Loss: 0.6143876314163208 \n",
      "     Training Step: 14 Training Loss: 0.6143496632575989 \n",
      "     Training Step: 15 Training Loss: 0.6118739247322083 \n",
      "     Training Step: 16 Training Loss: 0.6163657307624817 \n",
      "     Training Step: 17 Training Loss: 0.6106879115104675 \n",
      "     Training Step: 18 Training Loss: 0.6111971139907837 \n",
      "     Training Step: 19 Training Loss: 0.6133018136024475 \n",
      "     Training Step: 20 Training Loss: 0.614960253238678 \n",
      "     Training Step: 21 Training Loss: 0.6123700141906738 \n",
      "     Training Step: 22 Training Loss: 0.6114065647125244 \n",
      "     Training Step: 23 Training Loss: 0.6121362447738647 \n",
      "     Training Step: 24 Training Loss: 0.6162592172622681 \n",
      "     Training Step: 25 Training Loss: 0.6094431281089783 \n",
      "     Training Step: 26 Training Loss: 0.6111518144607544 \n",
      "     Training Step: 27 Training Loss: 0.6142935752868652 \n",
      "     Training Step: 28 Training Loss: 0.6130695343017578 \n",
      "     Training Step: 29 Training Loss: 0.6123777627944946 \n",
      "     Training Step: 30 Training Loss: 0.6168322563171387 \n",
      "     Training Step: 31 Training Loss: 0.6148770451545715 \n",
      "     Training Step: 32 Training Loss: 0.6157954931259155 \n",
      "     Training Step: 33 Training Loss: 0.6103694438934326 \n",
      "     Training Step: 34 Training Loss: 0.6105808019638062 \n",
      "     Training Step: 35 Training Loss: 0.6115533113479614 \n",
      "     Training Step: 36 Training Loss: 0.6105663776397705 \n",
      "     Training Step: 37 Training Loss: 0.6100383996963501 \n",
      "     Training Step: 38 Training Loss: 0.6184390783309937 \n",
      "     Training Step: 39 Training Loss: 0.6152415871620178 \n",
      "     Training Step: 40 Training Loss: 0.6167354583740234 \n",
      "     Training Step: 41 Training Loss: 0.6153135895729065 \n",
      "     Training Step: 42 Training Loss: 0.6169516444206238 \n",
      "     Training Step: 43 Training Loss: 0.6168127655982971 \n",
      "     Training Step: 44 Training Loss: 0.6144956946372986 \n",
      "     Training Step: 45 Training Loss: 0.6097468733787537 \n",
      "     Training Step: 46 Training Loss: 0.6151092648506165 \n",
      "     Training Step: 47 Training Loss: 0.6116572022438049 \n",
      "     Training Step: 48 Training Loss: 0.6167580485343933 \n",
      "     Training Step: 49 Training Loss: 0.6138864159584045 \n",
      "     Training Step: 50 Training Loss: 0.6106191277503967 \n",
      "     Training Step: 51 Training Loss: 0.6161133050918579 \n",
      "     Training Step: 52 Training Loss: 0.6135193705558777 \n",
      "     Training Step: 53 Training Loss: 0.6140740513801575 \n",
      "     Training Step: 54 Training Loss: 0.615530252456665 \n",
      "     Training Step: 55 Training Loss: 0.6174944043159485 \n",
      "     Training Step: 56 Training Loss: 0.6104110479354858 \n",
      "     Training Step: 57 Training Loss: 0.6171416640281677 \n",
      "     Training Step: 58 Training Loss: 0.6124948263168335 \n",
      "     Training Step: 59 Training Loss: 0.6149317622184753 \n",
      "     Training Step: 60 Training Loss: 0.612294614315033 \n",
      "     Training Step: 61 Training Loss: 0.619712233543396 \n",
      "     Training Step: 62 Training Loss: 0.6082895994186401 \n",
      "     Training Step: 63 Training Loss: 0.6202282309532166 \n",
      "     Training Step: 64 Training Loss: 0.6136385798454285 \n",
      "     Training Step: 65 Training Loss: 0.6115930080413818 \n",
      "     Training Step: 66 Training Loss: 0.6155192255973816 \n",
      "     Training Step: 67 Training Loss: 0.6133557558059692 \n",
      "     Training Step: 68 Training Loss: 0.6127052903175354 \n",
      "     Training Step: 69 Training Loss: 0.6146117448806763 \n",
      "     Training Step: 70 Training Loss: 0.613277792930603 \n",
      "     Training Step: 71 Training Loss: 0.6137412190437317 \n",
      "     Training Step: 72 Training Loss: 0.6146625876426697 \n",
      "     Training Step: 73 Training Loss: 0.6185985803604126 \n",
      "     Training Step: 74 Training Loss: 0.616723358631134 \n",
      "     Training Step: 75 Training Loss: 0.6157395243644714 \n",
      "     Training Step: 76 Training Loss: 0.6154159903526306 \n",
      "     Training Step: 77 Training Loss: 0.6156781911849976 \n",
      "     Training Step: 78 Training Loss: 0.6198761463165283 \n",
      "     Training Step: 79 Training Loss: 0.61240154504776 \n",
      "     Training Step: 80 Training Loss: 0.6149190664291382 \n",
      "     Training Step: 81 Training Loss: 0.6166369318962097 \n",
      "     Training Step: 82 Training Loss: 0.6118887066841125 \n",
      "     Training Step: 83 Training Loss: 0.6125651001930237 \n",
      "     Training Step: 84 Training Loss: 0.6137547492980957 \n",
      "     Training Step: 85 Training Loss: 0.6131644248962402 \n",
      "     Training Step: 86 Training Loss: 0.6135054230690002 \n",
      "     Training Step: 87 Training Loss: 0.6132120490074158 \n",
      "     Training Step: 88 Training Loss: 0.6159462332725525 \n",
      "     Training Step: 89 Training Loss: 0.6156023144721985 \n",
      "     Training Step: 90 Training Loss: 0.6140422821044922 \n",
      "     Training Step: 91 Training Loss: 0.6183345317840576 \n",
      "     Training Step: 92 Training Loss: 0.617200493812561 \n",
      "     Training Step: 93 Training Loss: 0.6100746989250183 \n",
      "     Training Step: 94 Training Loss: 0.6154219508171082 \n",
      "     Training Step: 95 Training Loss: 0.6152855157852173 \n",
      "     Training Step: 96 Training Loss: 0.6131216883659363 \n",
      "     Training Step: 97 Training Loss: 0.616489827632904 \n",
      "     Training Step: 98 Training Loss: 0.620936930179596 \n",
      "     Training Step: 99 Training Loss: 0.6134620308876038 \n",
      "     Training Step: 100 Training Loss: 0.6167066097259521 \n",
      "     Training Step: 101 Training Loss: 0.6152843236923218 \n",
      "     Training Step: 102 Training Loss: 0.6139309406280518 \n",
      "     Training Step: 103 Training Loss: 0.6120387315750122 \n",
      "     Training Step: 104 Training Loss: 0.6156840324401855 \n",
      "     Training Step: 105 Training Loss: 0.6126564741134644 \n",
      "     Training Step: 106 Training Loss: 0.6107552647590637 \n",
      "     Training Step: 107 Training Loss: 0.6100083589553833 \n",
      "     Training Step: 108 Training Loss: 0.6125347018241882 \n",
      "     Training Step: 109 Training Loss: 0.6181121468544006 \n",
      "     Training Step: 110 Training Loss: 0.6135925650596619 \n",
      "     Training Step: 111 Training Loss: 0.6139124631881714 \n",
      "     Training Step: 112 Training Loss: 0.6111340522766113 \n",
      "     Training Step: 113 Training Loss: 0.6146124601364136 \n",
      "     Training Step: 114 Training Loss: 0.6153500080108643 \n",
      "     Training Step: 115 Training Loss: 0.6141533255577087 \n",
      "     Training Step: 116 Training Loss: 0.6117766499519348 \n",
      "     Training Step: 117 Training Loss: 0.6113918423652649 \n",
      "     Training Step: 118 Training Loss: 0.6153815388679504 \n",
      "     Training Step: 119 Training Loss: 0.6131821870803833 \n",
      "     Training Step: 120 Training Loss: 0.6147757172584534 \n",
      "     Training Step: 121 Training Loss: 0.6171630620956421 \n",
      "     Training Step: 122 Training Loss: 0.617700457572937 \n",
      "     Training Step: 123 Training Loss: 0.6146374344825745 \n",
      "     Training Step: 124 Training Loss: 0.6194486021995544 \n",
      "     Training Step: 125 Training Loss: 0.6115425229072571 \n",
      "     Training Step: 126 Training Loss: 0.6122525930404663 \n",
      "     Training Step: 127 Training Loss: 0.6196178793907166 \n",
      "     Training Step: 128 Training Loss: 0.6144266724586487 \n",
      "     Training Step: 129 Training Loss: 0.6147128939628601 \n",
      "     Training Step: 130 Training Loss: 0.6105318069458008 \n",
      "     Training Step: 131 Training Loss: 0.6128212213516235 \n",
      "     Training Step: 132 Training Loss: 0.615382194519043 \n",
      "     Training Step: 133 Training Loss: 0.6124718189239502 \n",
      "     Training Step: 134 Training Loss: 0.61402428150177 \n",
      "     Training Step: 135 Training Loss: 0.6166179180145264 \n",
      "     Training Step: 136 Training Loss: 0.6128638386726379 \n",
      "     Training Step: 137 Training Loss: 0.6160070300102234 \n",
      "     Training Step: 138 Training Loss: 0.6143390536308289 \n",
      "     Training Step: 139 Training Loss: 0.616711437702179 \n",
      "     Training Step: 140 Training Loss: 0.6188816428184509 \n",
      "     Training Step: 141 Training Loss: 0.616665244102478 \n",
      "     Training Step: 142 Training Loss: 0.6155350208282471 \n",
      "     Training Step: 143 Training Loss: 0.6147174835205078 \n",
      "     Training Step: 144 Training Loss: 0.6142027974128723 \n",
      "     Training Step: 145 Training Loss: 0.6154822707176208 \n",
      "     Training Step: 146 Training Loss: 0.617791473865509 \n",
      "     Training Step: 147 Training Loss: 0.6184384226799011 \n",
      "     Training Step: 148 Training Loss: 0.6127910017967224 \n",
      "     Training Step: 149 Training Loss: 0.6129146218299866 \n",
      "     Training Step: 150 Training Loss: 0.6148160099983215 \n",
      "     Training Step: 151 Training Loss: 0.6136747598648071 \n",
      "     Training Step: 152 Training Loss: 0.6146199703216553 \n",
      "     Training Step: 153 Training Loss: 0.610190749168396 \n",
      "     Training Step: 154 Training Loss: 0.6182238459587097 \n",
      "     Training Step: 155 Training Loss: 0.6164132356643677 \n",
      "     Training Step: 156 Training Loss: 0.6144433617591858 \n",
      "     Training Step: 157 Training Loss: 0.6122350096702576 \n",
      "     Training Step: 158 Training Loss: 0.6097402572631836 \n",
      "     Training Step: 159 Training Loss: 0.6107230186462402 \n",
      "     Training Step: 160 Training Loss: 0.6141089797019958 \n",
      "     Training Step: 161 Training Loss: 0.6116211414337158 \n",
      "     Training Step: 162 Training Loss: 0.6171283721923828 \n",
      "     Training Step: 163 Training Loss: 0.616704523563385 \n",
      "     Training Step: 164 Training Loss: 0.618086576461792 \n",
      "     Training Step: 165 Training Loss: 0.6162291765213013 \n",
      "     Training Step: 166 Training Loss: 0.6133131384849548 \n",
      "     Training Step: 167 Training Loss: 0.6132895946502686 \n",
      "     Training Step: 168 Training Loss: 0.6114370226860046 \n",
      "     Training Step: 169 Training Loss: 0.6162130832672119 \n",
      "     Training Step: 170 Training Loss: 0.6115323305130005 \n",
      "     Training Step: 171 Training Loss: 0.6134752035140991 \n",
      "     Training Step: 172 Training Loss: 0.6106724143028259 \n",
      "     Training Step: 173 Training Loss: 0.6092461347579956 \n",
      "     Training Step: 174 Training Loss: 0.6147525310516357 \n",
      "     Training Step: 175 Training Loss: 0.6127660870552063 \n",
      "     Training Step: 176 Training Loss: 0.6146913766860962 \n",
      "     Training Step: 177 Training Loss: 0.6116091012954712 \n",
      "     Training Step: 178 Training Loss: 0.6143440008163452 \n",
      "     Training Step: 179 Training Loss: 0.6132750511169434 \n",
      "     Training Step: 180 Training Loss: 0.6186348795890808 \n",
      "     Training Step: 181 Training Loss: 0.617186427116394 \n",
      "     Training Step: 182 Training Loss: 0.6118039488792419 \n",
      "     Training Step: 183 Training Loss: 0.6146730184555054 \n",
      "     Training Step: 184 Training Loss: 0.6094003319740295 \n",
      "     Training Step: 185 Training Loss: 0.6097058653831482 \n",
      "     Training Step: 186 Training Loss: 0.6180404424667358 \n",
      "     Training Step: 187 Training Loss: 0.6118316054344177 \n",
      "     Training Step: 188 Training Loss: 0.6101765632629395 \n",
      "     Training Step: 189 Training Loss: 0.6160460114479065 \n",
      "     Training Step: 190 Training Loss: 0.6114104986190796 \n",
      "     Training Step: 191 Training Loss: 0.6122776865959167 \n",
      "     Training Step: 192 Training Loss: 0.6120626926422119 \n",
      "     Training Step: 193 Training Loss: 0.6116239428520203 \n",
      "     Training Step: 194 Training Loss: 0.6168361902236938 \n",
      "     Training Step: 195 Training Loss: 0.6152862310409546 \n",
      "     Training Step: 196 Training Loss: 0.6154519319534302 \n",
      "     Training Step: 197 Training Loss: 0.6150773763656616 \n",
      "     Training Step: 198 Training Loss: 0.6117880344390869 \n",
      "     Training Step: 199 Training Loss: 0.6164208054542542 \n",
      "     Training Step: 200 Training Loss: 0.612123966217041 \n",
      "     Training Step: 201 Training Loss: 0.6115249395370483 \n",
      "     Training Step: 202 Training Loss: 0.6173971891403198 \n",
      "     Training Step: 203 Training Loss: 0.6147261261940002 \n",
      "     Training Step: 204 Training Loss: 0.614016592502594 \n",
      "     Training Step: 205 Training Loss: 0.6150867342948914 \n",
      "     Training Step: 206 Training Loss: 0.615834653377533 \n",
      "     Training Step: 207 Training Loss: 0.6177552938461304 \n",
      "     Training Step: 208 Training Loss: 0.6168878674507141 \n",
      "     Training Step: 209 Training Loss: 0.6180630326271057 \n",
      "     Training Step: 210 Training Loss: 0.6128501296043396 \n",
      "     Training Step: 211 Training Loss: 0.6176937222480774 \n",
      "     Training Step: 212 Training Loss: 0.6176062822341919 \n",
      "     Training Step: 213 Training Loss: 0.6142582297325134 \n",
      "     Training Step: 214 Training Loss: 0.6146812438964844 \n",
      "     Training Step: 215 Training Loss: 0.6146597862243652 \n",
      "     Training Step: 216 Training Loss: 0.6101575493812561 \n",
      "     Training Step: 217 Training Loss: 0.6157670617103577 \n",
      "     Training Step: 218 Training Loss: 0.6151605844497681 \n",
      "     Training Step: 219 Training Loss: 0.615740954875946 \n",
      "     Training Step: 220 Training Loss: 0.6184247136116028 \n",
      "     Training Step: 221 Training Loss: 0.6137680411338806 \n",
      "     Training Step: 222 Training Loss: 0.6144827604293823 \n",
      "     Training Step: 223 Training Loss: 0.6177884340286255 \n",
      "     Training Step: 224 Training Loss: 0.6144565343856812 \n",
      "     Training Step: 225 Training Loss: 0.6108970642089844 \n",
      "     Training Step: 226 Training Loss: 0.6188486814498901 \n",
      "     Training Step: 227 Training Loss: 0.6116943955421448 \n",
      "     Training Step: 228 Training Loss: 0.6176905035972595 \n",
      "     Training Step: 229 Training Loss: 0.6114871501922607 \n",
      "     Training Step: 230 Training Loss: 0.6136252880096436 \n",
      "     Training Step: 231 Training Loss: 0.6114458441734314 \n",
      "     Training Step: 232 Training Loss: 0.6154013276100159 \n",
      "     Training Step: 233 Training Loss: 0.6118329763412476 \n",
      "     Training Step: 234 Training Loss: 0.6134209036827087 \n",
      "     Training Step: 235 Training Loss: 0.6129648685455322 \n",
      "     Training Step: 236 Training Loss: 0.6130566596984863 \n",
      "     Training Step: 237 Training Loss: 0.6121569871902466 \n",
      "     Training Step: 238 Training Loss: 0.6105659604072571 \n",
      "     Training Step: 239 Training Loss: 0.6166840195655823 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6145378947257996 \n",
      "     Validation Step: 1 Validation Loss: 0.6181029677391052 \n",
      "     Validation Step: 2 Validation Loss: 0.6111699342727661 \n",
      "     Validation Step: 3 Validation Loss: 0.6074855923652649 \n",
      "     Validation Step: 4 Validation Loss: 0.6104682683944702 \n",
      "     Validation Step: 5 Validation Loss: 0.6141141057014465 \n",
      "     Validation Step: 6 Validation Loss: 0.6156032085418701 \n",
      "     Validation Step: 7 Validation Loss: 0.6129798889160156 \n",
      "     Validation Step: 8 Validation Loss: 0.6128167510032654 \n",
      "     Validation Step: 9 Validation Loss: 0.6183881759643555 \n",
      "     Validation Step: 10 Validation Loss: 0.6170608997344971 \n",
      "     Validation Step: 11 Validation Loss: 0.6142049431800842 \n",
      "     Validation Step: 12 Validation Loss: 0.6136302351951599 \n",
      "     Validation Step: 13 Validation Loss: 0.614902138710022 \n",
      "     Validation Step: 14 Validation Loss: 0.6142554879188538 \n",
      "     Validation Step: 15 Validation Loss: 0.6146435141563416 \n",
      "     Validation Step: 16 Validation Loss: 0.6185401082038879 \n",
      "     Validation Step: 17 Validation Loss: 0.6104996204376221 \n",
      "     Validation Step: 18 Validation Loss: 0.6145834922790527 \n",
      "     Validation Step: 19 Validation Loss: 0.6111522912979126 \n",
      "     Validation Step: 20 Validation Loss: 0.6136754751205444 \n",
      "     Validation Step: 21 Validation Loss: 0.6106168627738953 \n",
      "     Validation Step: 22 Validation Loss: 0.6160381436347961 \n",
      "     Validation Step: 23 Validation Loss: 0.6153386831283569 \n",
      "     Validation Step: 24 Validation Loss: 0.61162930727005 \n",
      "     Validation Step: 25 Validation Loss: 0.6132984757423401 \n",
      "     Validation Step: 26 Validation Loss: 0.6100868582725525 \n",
      "     Validation Step: 27 Validation Loss: 0.6148436069488525 \n",
      "     Validation Step: 28 Validation Loss: 0.6136714220046997 \n",
      "     Validation Step: 29 Validation Loss: 0.6152433156967163 \n",
      "     Validation Step: 30 Validation Loss: 0.617620587348938 \n",
      "     Validation Step: 31 Validation Loss: 0.6158196926116943 \n",
      "     Validation Step: 32 Validation Loss: 0.6150627732276917 \n",
      "     Validation Step: 33 Validation Loss: 0.6141297817230225 \n",
      "     Validation Step: 34 Validation Loss: 0.6115434169769287 \n",
      "     Validation Step: 35 Validation Loss: 0.617337703704834 \n",
      "     Validation Step: 36 Validation Loss: 0.6101144552230835 \n",
      "     Validation Step: 37 Validation Loss: 0.6177365183830261 \n",
      "     Validation Step: 38 Validation Loss: 0.6118689179420471 \n",
      "     Validation Step: 39 Validation Loss: 0.6185095906257629 \n",
      "     Validation Step: 40 Validation Loss: 0.6101434230804443 \n",
      "     Validation Step: 41 Validation Loss: 0.6182818412780762 \n",
      "     Validation Step: 42 Validation Loss: 0.6156372427940369 \n",
      "     Validation Step: 43 Validation Loss: 0.6121175289154053 \n",
      "     Validation Step: 44 Validation Loss: 0.6162676811218262 \n",
      "Epoch: 82\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.612275242805481 \n",
      "     Training Step: 1 Training Loss: 0.617174506187439 \n",
      "     Training Step: 2 Training Loss: 0.6105640530586243 \n",
      "     Training Step: 3 Training Loss: 0.6163758039474487 \n",
      "     Training Step: 4 Training Loss: 0.6150955557823181 \n",
      "     Training Step: 5 Training Loss: 0.6167373061180115 \n",
      "     Training Step: 6 Training Loss: 0.6150745153427124 \n",
      "     Training Step: 7 Training Loss: 0.6153746247291565 \n",
      "     Training Step: 8 Training Loss: 0.6167700290679932 \n",
      "     Training Step: 9 Training Loss: 0.6133667230606079 \n",
      "     Training Step: 10 Training Loss: 0.6123841404914856 \n",
      "     Training Step: 11 Training Loss: 0.6114580035209656 \n",
      "     Training Step: 12 Training Loss: 0.6106237769126892 \n",
      "     Training Step: 13 Training Loss: 0.6092531085014343 \n",
      "     Training Step: 14 Training Loss: 0.6152418851852417 \n",
      "     Training Step: 15 Training Loss: 0.6158412098884583 \n",
      "     Training Step: 16 Training Loss: 0.6157504320144653 \n",
      "     Training Step: 17 Training Loss: 0.6155418753623962 \n",
      "     Training Step: 18 Training Loss: 0.6140285730361938 \n",
      "     Training Step: 19 Training Loss: 0.6180628538131714 \n",
      "     Training Step: 20 Training Loss: 0.6121370196342468 \n",
      "     Training Step: 21 Training Loss: 0.6137329339981079 \n",
      "     Training Step: 22 Training Loss: 0.6155434846878052 \n",
      "     Training Step: 23 Training Loss: 0.6127110123634338 \n",
      "     Training Step: 24 Training Loss: 0.6146459579467773 \n",
      "     Training Step: 25 Training Loss: 0.6101945042610168 \n",
      "     Training Step: 26 Training Loss: 0.6109147667884827 \n",
      "     Training Step: 27 Training Loss: 0.6126413345336914 \n",
      "     Training Step: 28 Training Loss: 0.6202334761619568 \n",
      "     Training Step: 29 Training Loss: 0.6177287101745605 \n",
      "     Training Step: 30 Training Loss: 0.6125134825706482 \n",
      "     Training Step: 31 Training Loss: 0.6146554946899414 \n",
      "     Training Step: 32 Training Loss: 0.615425169467926 \n",
      "     Training Step: 33 Training Loss: 0.6120229959487915 \n",
      "     Training Step: 34 Training Loss: 0.6180972456932068 \n",
      "     Training Step: 35 Training Loss: 0.6131926774978638 \n",
      "     Training Step: 36 Training Loss: 0.6197049617767334 \n",
      "     Training Step: 37 Training Loss: 0.6148713231086731 \n",
      "     Training Step: 38 Training Loss: 0.6164841055870056 \n",
      "     Training Step: 39 Training Loss: 0.6156756281852722 \n",
      "     Training Step: 40 Training Loss: 0.6130779981613159 \n",
      "     Training Step: 41 Training Loss: 0.6144818067550659 \n",
      "     Training Step: 42 Training Loss: 0.6155169606208801 \n",
      "     Training Step: 43 Training Loss: 0.6154756546020508 \n",
      "     Training Step: 44 Training Loss: 0.6180033087730408 \n",
      "     Training Step: 45 Training Loss: 0.6194214820861816 \n",
      "     Training Step: 46 Training Loss: 0.6135363578796387 \n",
      "     Training Step: 47 Training Loss: 0.6140227913856506 \n",
      "     Training Step: 48 Training Loss: 0.6124037504196167 \n",
      "     Training Step: 49 Training Loss: 0.6159957051277161 \n",
      "     Training Step: 50 Training Loss: 0.61344313621521 \n",
      "     Training Step: 51 Training Loss: 0.6198561191558838 \n",
      "     Training Step: 52 Training Loss: 0.6132141351699829 \n",
      "     Training Step: 53 Training Loss: 0.6140490770339966 \n",
      "     Training Step: 54 Training Loss: 0.6136521697044373 \n",
      "     Training Step: 55 Training Loss: 0.6116281747817993 \n",
      "     Training Step: 56 Training Loss: 0.6111714243888855 \n",
      "     Training Step: 57 Training Loss: 0.6116265058517456 \n",
      "     Training Step: 58 Training Loss: 0.6094035506248474 \n",
      "     Training Step: 59 Training Loss: 0.6180921792984009 \n",
      "     Training Step: 60 Training Loss: 0.6142153739929199 \n",
      "     Training Step: 61 Training Loss: 0.6103677153587341 \n",
      "     Training Step: 62 Training Loss: 0.6124629378318787 \n",
      "     Training Step: 63 Training Loss: 0.618506908416748 \n",
      "     Training Step: 64 Training Loss: 0.6154443025588989 \n",
      "     Training Step: 65 Training Loss: 0.611685037612915 \n",
      "     Training Step: 66 Training Loss: 0.6183683276176453 \n",
      "     Training Step: 67 Training Loss: 0.6120633482933044 \n",
      "     Training Step: 68 Training Loss: 0.6115270256996155 \n",
      "     Training Step: 69 Training Loss: 0.6082726716995239 \n",
      "     Training Step: 70 Training Loss: 0.6166570782661438 \n",
      "     Training Step: 71 Training Loss: 0.6143410801887512 \n",
      "     Training Step: 72 Training Loss: 0.6114140152931213 \n",
      "     Training Step: 73 Training Loss: 0.6105061173439026 \n",
      "     Training Step: 74 Training Loss: 0.6139241456985474 \n",
      "     Training Step: 75 Training Loss: 0.6118798851966858 \n",
      "     Training Step: 76 Training Loss: 0.6175016164779663 \n",
      "     Training Step: 77 Training Loss: 0.6166337728500366 \n",
      "     Training Step: 78 Training Loss: 0.6107192635536194 \n",
      "     Training Step: 79 Training Loss: 0.6157588362693787 \n",
      "     Training Step: 80 Training Loss: 0.6138869524002075 \n",
      "     Training Step: 81 Training Loss: 0.6147928237915039 \n",
      "     Training Step: 82 Training Loss: 0.6172037720680237 \n",
      "     Training Step: 83 Training Loss: 0.6104012727737427 \n",
      "     Training Step: 84 Training Loss: 0.6111419796943665 \n",
      "     Training Step: 85 Training Loss: 0.6178082823753357 \n",
      "     Training Step: 86 Training Loss: 0.6167197823524475 \n",
      "     Training Step: 87 Training Loss: 0.6146101951599121 \n",
      "     Training Step: 88 Training Loss: 0.6137639284133911 \n",
      "     Training Step: 89 Training Loss: 0.6188718676567078 \n",
      "     Training Step: 90 Training Loss: 0.6161128282546997 \n",
      "     Training Step: 91 Training Loss: 0.6123928427696228 \n",
      "     Training Step: 92 Training Loss: 0.6147292256355286 \n",
      "     Training Step: 93 Training Loss: 0.614345908164978 \n",
      "     Training Step: 94 Training Loss: 0.6115002632141113 \n",
      "     Training Step: 95 Training Loss: 0.6115421652793884 \n",
      "     Training Step: 96 Training Loss: 0.6116516590118408 \n",
      "     Training Step: 97 Training Loss: 0.6146095991134644 \n",
      "     Training Step: 98 Training Loss: 0.6157458424568176 \n",
      "     Training Step: 99 Training Loss: 0.6151735782623291 \n",
      "     Training Step: 100 Training Loss: 0.6132791638374329 \n",
      "     Training Step: 101 Training Loss: 0.6128636002540588 \n",
      "     Training Step: 102 Training Loss: 0.6153414845466614 \n",
      "     Training Step: 103 Training Loss: 0.614439845085144 \n",
      "     Training Step: 104 Training Loss: 0.6146837472915649 \n",
      "     Training Step: 105 Training Loss: 0.6182451248168945 \n",
      "     Training Step: 106 Training Loss: 0.6196509599685669 \n",
      "     Training Step: 107 Training Loss: 0.61347496509552 \n",
      "     Training Step: 108 Training Loss: 0.6114081740379333 \n",
      "     Training Step: 109 Training Loss: 0.614376962184906 \n",
      "     Training Step: 110 Training Loss: 0.6149155497550964 \n",
      "     Training Step: 111 Training Loss: 0.6167988181114197 \n",
      "     Training Step: 112 Training Loss: 0.6129767298698425 \n",
      "     Training Step: 113 Training Loss: 0.617620050907135 \n",
      "     Training Step: 114 Training Loss: 0.6100974678993225 \n",
      "     Training Step: 115 Training Loss: 0.6183894872665405 \n",
      "     Training Step: 116 Training Loss: 0.6152827739715576 \n",
      "     Training Step: 117 Training Loss: 0.6131360530853271 \n",
      "     Training Step: 118 Training Loss: 0.6141592860221863 \n",
      "     Training Step: 119 Training Loss: 0.612182080745697 \n",
      "     Training Step: 120 Training Loss: 0.6166630983352661 \n",
      "     Training Step: 121 Training Loss: 0.6135879755020142 \n",
      "     Training Step: 122 Training Loss: 0.6147491335868835 \n",
      "     Training Step: 123 Training Loss: 0.6114546656608582 \n",
      "     Training Step: 124 Training Loss: 0.6185785531997681 \n",
      "     Training Step: 125 Training Loss: 0.6186026930809021 \n",
      "     Training Step: 126 Training Loss: 0.6107473373413086 \n",
      "     Training Step: 127 Training Loss: 0.616801917552948 \n",
      "     Training Step: 128 Training Loss: 0.6147052049636841 \n",
      "     Training Step: 129 Training Loss: 0.6097543835639954 \n",
      "     Training Step: 130 Training Loss: 0.6114259362220764 \n",
      "     Training Step: 131 Training Loss: 0.6118480563163757 \n",
      "     Training Step: 132 Training Loss: 0.6131430864334106 \n",
      "     Training Step: 133 Training Loss: 0.614500105381012 \n",
      "     Training Step: 134 Training Loss: 0.6152759790420532 \n",
      "     Training Step: 135 Training Loss: 0.612840473651886 \n",
      "     Training Step: 136 Training Loss: 0.6171581745147705 \n",
      "     Training Step: 137 Training Loss: 0.6151133179664612 \n",
      "     Training Step: 138 Training Loss: 0.6130571961402893 \n",
      "     Training Step: 139 Training Loss: 0.6121225953102112 \n",
      "     Training Step: 140 Training Loss: 0.616253674030304 \n",
      "     Training Step: 141 Training Loss: 0.6149269938468933 \n",
      "     Training Step: 142 Training Loss: 0.6132904291152954 \n",
      "     Training Step: 143 Training Loss: 0.6164249181747437 \n",
      "     Training Step: 144 Training Loss: 0.6152851581573486 \n",
      "     Training Step: 145 Training Loss: 0.6115734577178955 \n",
      "     Training Step: 146 Training Loss: 0.6146690249443054 \n",
      "     Training Step: 147 Training Loss: 0.6182176470756531 \n",
      "     Training Step: 148 Training Loss: 0.6137433052062988 \n",
      "     Training Step: 149 Training Loss: 0.6147710084915161 \n",
      "     Training Step: 150 Training Loss: 0.6138202548027039 \n",
      "     Training Step: 151 Training Loss: 0.6173833608627319 \n",
      "     Training Step: 152 Training Loss: 0.6176916360855103 \n",
      "     Training Step: 153 Training Loss: 0.6125448942184448 \n",
      "     Training Step: 154 Training Loss: 0.6146714091300964 \n",
      "     Training Step: 155 Training Loss: 0.6107071042060852 \n",
      "     Training Step: 156 Training Loss: 0.6169186234474182 \n",
      "     Training Step: 157 Training Loss: 0.6159422397613525 \n",
      "     Training Step: 158 Training Loss: 0.6143430471420288 \n",
      "     Training Step: 159 Training Loss: 0.6115475296974182 \n",
      "     Training Step: 160 Training Loss: 0.6127692461013794 \n",
      "     Training Step: 161 Training Loss: 0.6162015795707703 \n",
      "     Training Step: 162 Training Loss: 0.6115953922271729 \n",
      "     Training Step: 163 Training Loss: 0.6100561022758484 \n",
      "     Training Step: 164 Training Loss: 0.6124919056892395 \n",
      "     Training Step: 165 Training Loss: 0.6142932176589966 \n",
      "     Training Step: 166 Training Loss: 0.6132861375808716 \n",
      "     Training Step: 167 Training Loss: 0.6188837289810181 \n",
      "     Training Step: 168 Training Loss: 0.6136394143104553 \n",
      "     Training Step: 169 Training Loss: 0.61442631483078 \n",
      "     Training Step: 170 Training Loss: 0.6171080470085144 \n",
      "     Training Step: 171 Training Loss: 0.6118264198303223 \n",
      "     Training Step: 172 Training Loss: 0.6178174018859863 \n",
      "     Training Step: 173 Training Loss: 0.6177572011947632 \n",
      "     Training Step: 174 Training Loss: 0.6101526021957397 \n",
      "     Training Step: 175 Training Loss: 0.6157820224761963 \n",
      "     Training Step: 176 Training Loss: 0.6141508221626282 \n",
      "     Training Step: 177 Training Loss: 0.6209201812744141 \n",
      "     Training Step: 178 Training Loss: 0.611662745475769 \n",
      "     Training Step: 179 Training Loss: 0.6125516295433044 \n",
      "     Training Step: 180 Training Loss: 0.6139253973960876 \n",
      "     Training Step: 181 Training Loss: 0.6106017231941223 \n",
      "     Training Step: 182 Training Loss: 0.6106842160224915 \n",
      "     Training Step: 183 Training Loss: 0.6156002283096313 \n",
      "     Training Step: 184 Training Loss: 0.6105754971504211 \n",
      "     Training Step: 185 Training Loss: 0.6162164807319641 \n",
      "     Training Step: 186 Training Loss: 0.6154481172561646 \n",
      "     Training Step: 187 Training Loss: 0.616421103477478 \n",
      "     Training Step: 188 Training Loss: 0.6147119402885437 \n",
      "     Training Step: 189 Training Loss: 0.6167371273040771 \n",
      "     Training Step: 190 Training Loss: 0.6132001876831055 \n",
      "     Training Step: 191 Training Loss: 0.6111969351768494 \n",
      "     Training Step: 192 Training Loss: 0.6118311285972595 \n",
      "     Training Step: 193 Training Loss: 0.6108810901641846 \n",
      "     Training Step: 194 Training Loss: 0.6122275590896606 \n",
      "     Training Step: 195 Training Loss: 0.6097014546394348 \n",
      "     Training Step: 196 Training Loss: 0.6129105091094971 \n",
      "     Training Step: 197 Training Loss: 0.6142135858535767 \n",
      "     Training Step: 198 Training Loss: 0.6134555339813232 \n",
      "     Training Step: 199 Training Loss: 0.6160613894462585 \n",
      "     Training Step: 200 Training Loss: 0.6117718815803528 \n",
      "     Training Step: 201 Training Loss: 0.6169141530990601 \n",
      "     Training Step: 202 Training Loss: 0.6151610612869263 \n",
      "     Training Step: 203 Training Loss: 0.6153925061225891 \n",
      "     Training Step: 204 Training Loss: 0.6133115291595459 \n",
      "     Training Step: 205 Training Loss: 0.6129201054573059 \n",
      "     Training Step: 206 Training Loss: 0.6128026247024536 \n",
      "     Training Step: 207 Training Loss: 0.6142407059669495 \n",
      "     Training Step: 208 Training Loss: 0.6128731369972229 \n",
      "     Training Step: 209 Training Loss: 0.6168076992034912 \n",
      "     Training Step: 210 Training Loss: 0.6127526164054871 \n",
      "     Training Step: 211 Training Loss: 0.6166707277297974 \n",
      "     Training Step: 212 Training Loss: 0.6118029952049255 \n",
      "     Training Step: 213 Training Loss: 0.6153894066810608 \n",
      "     Training Step: 214 Training Loss: 0.6140822172164917 \n",
      "     Training Step: 215 Training Loss: 0.6121575236320496 \n",
      "     Training Step: 216 Training Loss: 0.61445552110672 \n",
      "     Training Step: 217 Training Loss: 0.6171725988388062 \n",
      "     Training Step: 218 Training Loss: 0.609990656375885 \n",
      "     Training Step: 219 Training Loss: 0.6122910380363464 \n",
      "     Training Step: 220 Training Loss: 0.6122339963912964 \n",
      "     Training Step: 221 Training Loss: 0.616672158241272 \n",
      "     Training Step: 222 Training Loss: 0.6122311353683472 \n",
      "     Training Step: 223 Training Loss: 0.6141062378883362 \n",
      "     Training Step: 224 Training Loss: 0.6167370080947876 \n",
      "     Training Step: 225 Training Loss: 0.6118025779724121 \n",
      "     Training Step: 226 Training Loss: 0.6136216521263123 \n",
      "     Training Step: 227 Training Loss: 0.6104845404624939 \n",
      "     Training Step: 228 Training Loss: 0.6156883239746094 \n",
      "     Training Step: 229 Training Loss: 0.6146089434623718 \n",
      "     Training Step: 230 Training Loss: 0.6149622201919556 \n",
      "     Training Step: 231 Training Loss: 0.6153083443641663 \n",
      "     Training Step: 232 Training Loss: 0.6100654602050781 \n",
      "     Training Step: 233 Training Loss: 0.6176956295967102 \n",
      "     Training Step: 234 Training Loss: 0.6134998202323914 \n",
      "     Training Step: 235 Training Loss: 0.6146934628486633 \n",
      "     Training Step: 236 Training Loss: 0.6094692945480347 \n",
      "     Training Step: 237 Training Loss: 0.6133079528808594 \n",
      "     Training Step: 238 Training Loss: 0.6184468269348145 \n",
      "     Training Step: 239 Training Loss: 0.6097350120544434 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.618501603603363 \n",
      "     Validation Step: 1 Validation Loss: 0.6155818700790405 \n",
      "     Validation Step: 2 Validation Loss: 0.6162503957748413 \n",
      "     Validation Step: 3 Validation Loss: 0.6145315170288086 \n",
      "     Validation Step: 4 Validation Loss: 0.6115560531616211 \n",
      "     Validation Step: 5 Validation Loss: 0.6141106486320496 \n",
      "     Validation Step: 6 Validation Loss: 0.6101597547531128 \n",
      "     Validation Step: 7 Validation Loss: 0.6180707216262817 \n",
      "     Validation Step: 8 Validation Loss: 0.610632061958313 \n",
      "     Validation Step: 9 Validation Loss: 0.6129854917526245 \n",
      "     Validation Step: 10 Validation Loss: 0.6148360371589661 \n",
      "     Validation Step: 11 Validation Loss: 0.6136277914047241 \n",
      "     Validation Step: 12 Validation Loss: 0.6182471513748169 \n",
      "     Validation Step: 13 Validation Loss: 0.6105157732963562 \n",
      "     Validation Step: 14 Validation Loss: 0.6132993102073669 \n",
      "     Validation Step: 15 Validation Loss: 0.6148910522460938 \n",
      "     Validation Step: 16 Validation Loss: 0.6175959706306458 \n",
      "     Validation Step: 17 Validation Loss: 0.6158028841018677 \n",
      "     Validation Step: 18 Validation Loss: 0.6150493621826172 \n",
      "     Validation Step: 19 Validation Loss: 0.617306649684906 \n",
      "     Validation Step: 20 Validation Loss: 0.6101136207580566 \n",
      "     Validation Step: 21 Validation Loss: 0.6141998767852783 \n",
      "     Validation Step: 22 Validation Loss: 0.6121271252632141 \n",
      "     Validation Step: 23 Validation Loss: 0.6111661791801453 \n",
      "     Validation Step: 24 Validation Loss: 0.6177082657814026 \n",
      "     Validation Step: 25 Validation Loss: 0.61366206407547 \n",
      "     Validation Step: 26 Validation Loss: 0.6101418137550354 \n",
      "     Validation Step: 27 Validation Loss: 0.6141308546066284 \n",
      "     Validation Step: 28 Validation Loss: 0.6160125732421875 \n",
      "     Validation Step: 29 Validation Loss: 0.6145689487457275 \n",
      "     Validation Step: 30 Validation Loss: 0.6118802428245544 \n",
      "     Validation Step: 31 Validation Loss: 0.614639163017273 \n",
      "     Validation Step: 32 Validation Loss: 0.6170289516448975 \n",
      "     Validation Step: 33 Validation Loss: 0.6128239035606384 \n",
      "     Validation Step: 34 Validation Loss: 0.614250123500824 \n",
      "     Validation Step: 35 Validation Loss: 0.6075272560119629 \n",
      "     Validation Step: 36 Validation Loss: 0.6153229475021362 \n",
      "     Validation Step: 37 Validation Loss: 0.6116388440132141 \n",
      "     Validation Step: 38 Validation Loss: 0.6136723756790161 \n",
      "     Validation Step: 39 Validation Loss: 0.6183550953865051 \n",
      "     Validation Step: 40 Validation Loss: 0.6111775636672974 \n",
      "     Validation Step: 41 Validation Loss: 0.6152283549308777 \n",
      "     Validation Step: 42 Validation Loss: 0.6156198978424072 \n",
      "     Validation Step: 43 Validation Loss: 0.6104840040206909 \n",
      "     Validation Step: 44 Validation Loss: 0.6184755563735962 \n",
      "Epoch: 83\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6188704371452332 \n",
      "     Training Step: 1 Training Loss: 0.6115314960479736 \n",
      "     Training Step: 2 Training Loss: 0.6139197945594788 \n",
      "     Training Step: 3 Training Loss: 0.6167187690734863 \n",
      "     Training Step: 4 Training Loss: 0.6133554577827454 \n",
      "     Training Step: 5 Training Loss: 0.6146379113197327 \n",
      "     Training Step: 6 Training Loss: 0.6134166121482849 \n",
      "     Training Step: 7 Training Loss: 0.6167210340499878 \n",
      "     Training Step: 8 Training Loss: 0.6118358373641968 \n",
      "     Training Step: 9 Training Loss: 0.6146663427352905 \n",
      "     Training Step: 10 Training Loss: 0.6111429333686829 \n",
      "     Training Step: 11 Training Loss: 0.6146198511123657 \n",
      "     Training Step: 12 Training Loss: 0.6132737398147583 \n",
      "     Training Step: 13 Training Loss: 0.6138871908187866 \n",
      "     Training Step: 14 Training Loss: 0.6157025098800659 \n",
      "     Training Step: 15 Training Loss: 0.6149190664291382 \n",
      "     Training Step: 16 Training Loss: 0.6166332960128784 \n",
      "     Training Step: 17 Training Loss: 0.6132031083106995 \n",
      "     Training Step: 18 Training Loss: 0.6167152523994446 \n",
      "     Training Step: 19 Training Loss: 0.6166502833366394 \n",
      "     Training Step: 20 Training Loss: 0.6154789328575134 \n",
      "     Training Step: 21 Training Loss: 0.6100975871086121 \n",
      "     Training Step: 22 Training Loss: 0.6120856404304504 \n",
      "     Training Step: 23 Training Loss: 0.613641619682312 \n",
      "     Training Step: 24 Training Loss: 0.6209291815757751 \n",
      "     Training Step: 25 Training Loss: 0.6150866150856018 \n",
      "     Training Step: 26 Training Loss: 0.6144816875457764 \n",
      "     Training Step: 27 Training Loss: 0.6185736060142517 \n",
      "     Training Step: 28 Training Loss: 0.6153420209884644 \n",
      "     Training Step: 29 Training Loss: 0.618230938911438 \n",
      "     Training Step: 30 Training Loss: 0.6169124841690063 \n",
      "     Training Step: 31 Training Loss: 0.6188495755195618 \n",
      "     Training Step: 32 Training Loss: 0.6118609309196472 \n",
      "     Training Step: 33 Training Loss: 0.6093001365661621 \n",
      "     Training Step: 34 Training Loss: 0.6183812618255615 \n",
      "     Training Step: 35 Training Loss: 0.6142547130584717 \n",
      "     Training Step: 36 Training Loss: 0.6142117977142334 \n",
      "     Training Step: 37 Training Loss: 0.6196761131286621 \n",
      "     Training Step: 38 Training Loss: 0.6171263456344604 \n",
      "     Training Step: 39 Training Loss: 0.6121404767036438 \n",
      "     Training Step: 40 Training Loss: 0.6130752563476562 \n",
      "     Training Step: 41 Training Loss: 0.6184247136116028 \n",
      "     Training Step: 42 Training Loss: 0.6116604208946228 \n",
      "     Training Step: 43 Training Loss: 0.6115849614143372 \n",
      "     Training Step: 44 Training Loss: 0.6100727915763855 \n",
      "     Training Step: 45 Training Loss: 0.6105139255523682 \n",
      "     Training Step: 46 Training Loss: 0.6143735647201538 \n",
      "     Training Step: 47 Training Loss: 0.6160153150558472 \n",
      "     Training Step: 48 Training Loss: 0.617669939994812 \n",
      "     Training Step: 49 Training Loss: 0.6183560490608215 \n",
      "     Training Step: 50 Training Loss: 0.613183856010437 \n",
      "     Training Step: 51 Training Loss: 0.6146083474159241 \n",
      "     Training Step: 52 Training Loss: 0.6167685389518738 \n",
      "     Training Step: 53 Training Loss: 0.6140241026878357 \n",
      "     Training Step: 54 Training Loss: 0.6155211329460144 \n",
      "     Training Step: 55 Training Loss: 0.6142883896827698 \n",
      "     Training Step: 56 Training Loss: 0.6153994798660278 \n",
      "     Training Step: 57 Training Loss: 0.6104359030723572 \n",
      "     Training Step: 58 Training Loss: 0.6177791953086853 \n",
      "     Training Step: 59 Training Loss: 0.6146799921989441 \n",
      "     Training Step: 60 Training Loss: 0.6121881008148193 \n",
      "     Training Step: 61 Training Loss: 0.6144993305206299 \n",
      "     Training Step: 62 Training Loss: 0.6168829202651978 \n",
      "     Training Step: 63 Training Loss: 0.6149353981018066 \n",
      "     Training Step: 64 Training Loss: 0.6162004470825195 \n",
      "     Training Step: 65 Training Loss: 0.616347074508667 \n",
      "     Training Step: 66 Training Loss: 0.6149551272392273 \n",
      "     Training Step: 67 Training Loss: 0.6143390536308289 \n",
      "     Training Step: 68 Training Loss: 0.6115418672561646 \n",
      "     Training Step: 69 Training Loss: 0.615753710269928 \n",
      "     Training Step: 70 Training Loss: 0.6112084984779358 \n",
      "     Training Step: 71 Training Loss: 0.6143448948860168 \n",
      "     Training Step: 72 Training Loss: 0.612379252910614 \n",
      "     Training Step: 73 Training Loss: 0.6137639284133911 \n",
      "     Training Step: 74 Training Loss: 0.610183596611023 \n",
      "     Training Step: 75 Training Loss: 0.6118019819259644 \n",
      "     Training Step: 76 Training Loss: 0.6122819185256958 \n",
      "     Training Step: 77 Training Loss: 0.6121365427970886 \n",
      "     Training Step: 78 Training Loss: 0.614436149597168 \n",
      "     Training Step: 79 Training Loss: 0.6172073483467102 \n",
      "     Training Step: 80 Training Loss: 0.6167053580284119 \n",
      "     Training Step: 81 Training Loss: 0.6133073568344116 \n",
      "     Training Step: 82 Training Loss: 0.6168304681777954 \n",
      "     Training Step: 83 Training Loss: 0.6093887686729431 \n",
      "     Training Step: 84 Training Loss: 0.6114097833633423 \n",
      "     Training Step: 85 Training Loss: 0.6196458339691162 \n",
      "     Training Step: 86 Training Loss: 0.6152708530426025 \n",
      "     Training Step: 87 Training Loss: 0.6168079972267151 \n",
      "     Training Step: 88 Training Loss: 0.6118148565292358 \n",
      "     Training Step: 89 Training Loss: 0.6140792369842529 \n",
      "     Training Step: 90 Training Loss: 0.6155965328216553 \n",
      "     Training Step: 91 Training Loss: 0.6177511215209961 \n",
      "     Training Step: 92 Training Loss: 0.6100311875343323 \n",
      "     Training Step: 93 Training Loss: 0.6152451038360596 \n",
      "     Training Step: 94 Training Loss: 0.6135091185569763 \n",
      "     Training Step: 95 Training Loss: 0.6125084757804871 \n",
      "     Training Step: 96 Training Loss: 0.614443838596344 \n",
      "     Training Step: 97 Training Loss: 0.6157824993133545 \n",
      "     Training Step: 98 Training Loss: 0.6137397885322571 \n",
      "     Training Step: 99 Training Loss: 0.6164886355400085 \n",
      "     Training Step: 100 Training Loss: 0.6128719449043274 \n",
      "     Training Step: 101 Training Loss: 0.6186113953590393 \n",
      "     Training Step: 102 Training Loss: 0.6135909557342529 \n",
      "     Training Step: 103 Training Loss: 0.6106665730476379 \n",
      "     Training Step: 104 Training Loss: 0.6125358939170837 \n",
      "     Training Step: 105 Training Loss: 0.6125178933143616 \n",
      "     Training Step: 106 Training Loss: 0.6141523718833923 \n",
      "     Training Step: 107 Training Loss: 0.6147310733795166 \n",
      "     Training Step: 108 Training Loss: 0.6126991510391235 \n",
      "     Training Step: 109 Training Loss: 0.6160433292388916 \n",
      "     Training Step: 110 Training Loss: 0.6150738000869751 \n",
      "     Training Step: 111 Training Loss: 0.6122314929962158 \n",
      "     Training Step: 112 Training Loss: 0.6116324663162231 \n",
      "     Training Step: 113 Training Loss: 0.616706132888794 \n",
      "     Training Step: 114 Training Loss: 0.610688328742981 \n",
      "     Training Step: 115 Training Loss: 0.6162086129188538 \n",
      "     Training Step: 116 Training Loss: 0.6157457828521729 \n",
      "     Training Step: 117 Training Loss: 0.6132020354270935 \n",
      "     Training Step: 118 Training Loss: 0.6126412153244019 \n",
      "     Training Step: 119 Training Loss: 0.6133127212524414 \n",
      "     Training Step: 120 Training Loss: 0.6176958084106445 \n",
      "     Training Step: 121 Training Loss: 0.6122276782989502 \n",
      "     Training Step: 122 Training Loss: 0.6114052534103394 \n",
      "     Training Step: 123 Training Loss: 0.6135168671607971 \n",
      "     Training Step: 124 Training Loss: 0.6130620241165161 \n",
      "     Training Step: 125 Training Loss: 0.6147713661193848 \n",
      "     Training Step: 126 Training Loss: 0.6155411005020142 \n",
      "     Training Step: 127 Training Loss: 0.615424394607544 \n",
      "     Training Step: 128 Training Loss: 0.613914966583252 \n",
      "     Training Step: 129 Training Loss: 0.6180285811424255 \n",
      "     Training Step: 130 Training Loss: 0.6159470081329346 \n",
      "     Training Step: 131 Training Loss: 0.6129132509231567 \n",
      "     Training Step: 132 Training Loss: 0.6111690998077393 \n",
      "     Training Step: 133 Training Loss: 0.6115967035293579 \n",
      "     Training Step: 134 Training Loss: 0.6151724457740784 \n",
      "     Training Step: 135 Training Loss: 0.6122987270355225 \n",
      "     Training Step: 136 Training Loss: 0.6141490340232849 \n",
      "     Training Step: 137 Training Loss: 0.6131172180175781 \n",
      "     Training Step: 138 Training Loss: 0.6168066263198853 \n",
      "     Training Step: 139 Training Loss: 0.612152099609375 \n",
      "     Training Step: 140 Training Loss: 0.6146696209907532 \n",
      "     Training Step: 141 Training Loss: 0.6147944927215576 \n",
      "     Training Step: 142 Training Loss: 0.6127609610557556 \n",
      "     Training Step: 143 Training Loss: 0.6132888793945312 \n",
      "     Training Step: 144 Training Loss: 0.6131380796432495 \n",
      "     Training Step: 145 Training Loss: 0.6182405948638916 \n",
      "     Training Step: 146 Training Loss: 0.6167312264442444 \n",
      "     Training Step: 147 Training Loss: 0.6151516437530518 \n",
      "     Training Step: 148 Training Loss: 0.614037036895752 \n",
      "     Training Step: 149 Training Loss: 0.6114438772201538 \n",
      "     Training Step: 150 Training Loss: 0.6138209104537964 \n",
      "     Training Step: 151 Training Loss: 0.6180669069290161 \n",
      "     Training Step: 152 Training Loss: 0.6147465705871582 \n",
      "     Training Step: 153 Training Loss: 0.6173822283744812 \n",
      "     Training Step: 154 Training Loss: 0.6176828742027283 \n",
      "     Training Step: 155 Training Loss: 0.6176989674568176 \n",
      "     Training Step: 156 Training Loss: 0.6164069175720215 \n",
      "     Training Step: 157 Training Loss: 0.6129391193389893 \n",
      "     Training Step: 158 Training Loss: 0.614651620388031 \n",
      "     Training Step: 159 Training Loss: 0.6184395551681519 \n",
      "     Training Step: 160 Training Loss: 0.6164053082466125 \n",
      "     Training Step: 161 Training Loss: 0.6129941344261169 \n",
      "     Training Step: 162 Training Loss: 0.6174716949462891 \n",
      "     Training Step: 163 Training Loss: 0.615093469619751 \n",
      "     Training Step: 164 Training Loss: 0.6162217855453491 \n",
      "     Training Step: 165 Training Loss: 0.611457109451294 \n",
      "     Training Step: 166 Training Loss: 0.6105490326881409 \n",
      "     Training Step: 167 Training Loss: 0.6116601824760437 \n",
      "     Training Step: 168 Training Loss: 0.6202129125595093 \n",
      "     Training Step: 169 Training Loss: 0.615687370300293 \n",
      "     Training Step: 170 Training Loss: 0.610155463218689 \n",
      "     Training Step: 171 Training Loss: 0.6140162944793701 \n",
      "     Training Step: 172 Training Loss: 0.6128668189048767 \n",
      "     Training Step: 173 Training Loss: 0.614693820476532 \n",
      "     Training Step: 174 Training Loss: 0.6153009533882141 \n",
      "     Training Step: 175 Training Loss: 0.6137461066246033 \n",
      "     Training Step: 176 Training Loss: 0.6153773665428162 \n",
      "     Training Step: 177 Training Loss: 0.6142107248306274 \n",
      "     Training Step: 178 Training Loss: 0.611472487449646 \n",
      "     Training Step: 179 Training Loss: 0.6134733557701111 \n",
      "     Training Step: 180 Training Loss: 0.6143494844436646 \n",
      "     Training Step: 181 Training Loss: 0.6154305934906006 \n",
      "     Training Step: 182 Training Loss: 0.6158431172370911 \n",
      "     Training Step: 183 Training Loss: 0.615289568901062 \n",
      "     Training Step: 184 Training Loss: 0.6147026419639587 \n",
      "     Training Step: 185 Training Loss: 0.6109063029289246 \n",
      "     Training Step: 186 Training Loss: 0.6118850111961365 \n",
      "     Training Step: 187 Training Loss: 0.6114434599876404 \n",
      "     Training Step: 188 Training Loss: 0.6166669726371765 \n",
      "     Training Step: 189 Training Loss: 0.6157425045967102 \n",
      "     Training Step: 190 Training Loss: 0.6118288040161133 \n",
      "     Training Step: 191 Training Loss: 0.6141082048416138 \n",
      "     Training Step: 192 Training Loss: 0.6161176562309265 \n",
      "     Training Step: 193 Training Loss: 0.6123734712600708 \n",
      "     Training Step: 194 Training Loss: 0.6153876781463623 \n",
      "     Training Step: 195 Training Loss: 0.6108809113502502 \n",
      "     Training Step: 196 Training Loss: 0.6155361533164978 \n",
      "     Training Step: 197 Training Loss: 0.6147065758705139 \n",
      "     Training Step: 198 Training Loss: 0.6105695366859436 \n",
      "     Training Step: 199 Training Loss: 0.612382173538208 \n",
      "     Training Step: 200 Training Loss: 0.613455057144165 \n",
      "     Training Step: 201 Training Loss: 0.6171560883522034 \n",
      "     Training Step: 202 Training Loss: 0.6154496073722839 \n",
      "     Training Step: 203 Training Loss: 0.6107158064842224 \n",
      "     Training Step: 204 Training Loss: 0.6125303506851196 \n",
      "     Training Step: 205 Training Loss: 0.6132742762565613 \n",
      "     Training Step: 206 Training Loss: 0.6094459295272827 \n",
      "     Training Step: 207 Training Loss: 0.6105839610099792 \n",
      "     Training Step: 208 Training Loss: 0.6100527048110962 \n",
      "     Training Step: 209 Training Loss: 0.6124629974365234 \n",
      "     Training Step: 210 Training Loss: 0.6180926561355591 \n",
      "     Training Step: 211 Training Loss: 0.6122369766235352 \n",
      "     Training Step: 212 Training Loss: 0.6120085120201111 \n",
      "     Training Step: 213 Training Loss: 0.6171354055404663 \n",
      "     Training Step: 214 Training Loss: 0.6146110892295837 \n",
      "     Training Step: 215 Training Loss: 0.6128001809120178 \n",
      "     Training Step: 216 Training Loss: 0.6103643178939819 \n",
      "     Training Step: 217 Training Loss: 0.6181247234344482 \n",
      "     Training Step: 218 Training Loss: 0.6144552230834961 \n",
      "     Training Step: 219 Training Loss: 0.6178231835365295 \n",
      "     Training Step: 220 Training Loss: 0.6115263104438782 \n",
      "     Training Step: 221 Training Loss: 0.6198973059654236 \n",
      "     Training Step: 222 Training Loss: 0.6097474098205566 \n",
      "     Training Step: 223 Training Loss: 0.6105943322181702 \n",
      "     Training Step: 224 Training Loss: 0.6117023825645447 \n",
      "     Training Step: 225 Training Loss: 0.6097570657730103 \n",
      "     Training Step: 226 Training Loss: 0.6117902398109436 \n",
      "     Training Step: 227 Training Loss: 0.6148692965507507 \n",
      "     Training Step: 228 Training Loss: 0.6153028011322021 \n",
      "     Training Step: 229 Training Loss: 0.6136221885681152 \n",
      "     Training Step: 230 Training Loss: 0.6194707155227661 \n",
      "     Training Step: 231 Training Loss: 0.6128380298614502 \n",
      "     Training Step: 232 Training Loss: 0.6105773448944092 \n",
      "     Training Step: 233 Training Loss: 0.6107334494590759 \n",
      "     Training Step: 234 Training Loss: 0.6082581877708435 \n",
      "     Training Step: 235 Training Loss: 0.61275714635849 \n",
      "     Training Step: 236 Training Loss: 0.6096794605255127 \n",
      "     Training Step: 237 Training Loss: 0.6136547923088074 \n",
      "     Training Step: 238 Training Loss: 0.6115999817848206 \n",
      "     Training Step: 239 Training Loss: 0.6172463297843933 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6177533864974976 \n",
      "     Validation Step: 1 Validation Loss: 0.6141175031661987 \n",
      "     Validation Step: 2 Validation Loss: 0.614262044429779 \n",
      "     Validation Step: 3 Validation Loss: 0.6101369261741638 \n",
      "     Validation Step: 4 Validation Loss: 0.6141325235366821 \n",
      "     Validation Step: 5 Validation Loss: 0.6149095296859741 \n",
      "     Validation Step: 6 Validation Loss: 0.6100764274597168 \n",
      "     Validation Step: 7 Validation Loss: 0.6156191825866699 \n",
      "     Validation Step: 8 Validation Loss: 0.6185622215270996 \n",
      "     Validation Step: 9 Validation Loss: 0.6106113195419312 \n",
      "     Validation Step: 10 Validation Loss: 0.6104638576507568 \n",
      "     Validation Step: 11 Validation Loss: 0.6104922890663147 \n",
      "     Validation Step: 12 Validation Loss: 0.607465922832489 \n",
      "     Validation Step: 13 Validation Loss: 0.6115400195121765 \n",
      "     Validation Step: 14 Validation Loss: 0.6181231141090393 \n",
      "     Validation Step: 15 Validation Loss: 0.6152534484863281 \n",
      "     Validation Step: 16 Validation Loss: 0.6153479218482971 \n",
      "     Validation Step: 17 Validation Loss: 0.6185303926467896 \n",
      "     Validation Step: 18 Validation Loss: 0.6176384091377258 \n",
      "     Validation Step: 19 Validation Loss: 0.6101062297821045 \n",
      "     Validation Step: 20 Validation Loss: 0.6183016896247864 \n",
      "     Validation Step: 21 Validation Loss: 0.6156494617462158 \n",
      "     Validation Step: 22 Validation Loss: 0.6142107248306274 \n",
      "     Validation Step: 23 Validation Loss: 0.617355465888977 \n",
      "     Validation Step: 24 Validation Loss: 0.6118634343147278 \n",
      "     Validation Step: 25 Validation Loss: 0.6145942807197571 \n",
      "     Validation Step: 26 Validation Loss: 0.6111472845077515 \n",
      "     Validation Step: 27 Validation Loss: 0.6136330366134644 \n",
      "     Validation Step: 28 Validation Loss: 0.6116279363632202 \n",
      "     Validation Step: 29 Validation Loss: 0.6111675500869751 \n",
      "     Validation Step: 30 Validation Loss: 0.6129799485206604 \n",
      "     Validation Step: 31 Validation Loss: 0.6136792898178101 \n",
      "     Validation Step: 32 Validation Loss: 0.6133025288581848 \n",
      "     Validation Step: 33 Validation Loss: 0.618407130241394 \n",
      "     Validation Step: 34 Validation Loss: 0.6121150255203247 \n",
      "     Validation Step: 35 Validation Loss: 0.6158289909362793 \n",
      "     Validation Step: 36 Validation Loss: 0.6160518527030945 \n",
      "     Validation Step: 37 Validation Loss: 0.6136788725852966 \n",
      "     Validation Step: 38 Validation Loss: 0.6145439743995667 \n",
      "     Validation Step: 39 Validation Loss: 0.6150721907615662 \n",
      "     Validation Step: 40 Validation Loss: 0.614650547504425 \n",
      "     Validation Step: 41 Validation Loss: 0.614849328994751 \n",
      "     Validation Step: 42 Validation Loss: 0.6162816882133484 \n",
      "     Validation Step: 43 Validation Loss: 0.6170804500579834 \n",
      "     Validation Step: 44 Validation Loss: 0.6128147840499878 \n",
      "Epoch: 84\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6127581596374512 \n",
      "     Training Step: 1 Training Loss: 0.6092016100883484 \n",
      "     Training Step: 2 Training Loss: 0.6140344142913818 \n",
      "     Training Step: 3 Training Loss: 0.6128467321395874 \n",
      "     Training Step: 4 Training Loss: 0.6156316995620728 \n",
      "     Training Step: 5 Training Loss: 0.6178514957427979 \n",
      "     Training Step: 6 Training Loss: 0.6132063865661621 \n",
      "     Training Step: 7 Training Loss: 0.6169549822807312 \n",
      "     Training Step: 8 Training Loss: 0.6121576428413391 \n",
      "     Training Step: 9 Training Loss: 0.6133653521537781 \n",
      "     Training Step: 10 Training Loss: 0.6146994233131409 \n",
      "     Training Step: 11 Training Loss: 0.6141138672828674 \n",
      "     Training Step: 12 Training Loss: 0.6144460439682007 \n",
      "     Training Step: 13 Training Loss: 0.6132977604866028 \n",
      "     Training Step: 14 Training Loss: 0.6155292391777039 \n",
      "     Training Step: 15 Training Loss: 0.6125051379203796 \n",
      "     Training Step: 16 Training Loss: 0.6104221940040588 \n",
      "     Training Step: 17 Training Loss: 0.615781843662262 \n",
      "     Training Step: 18 Training Loss: 0.618074357509613 \n",
      "     Training Step: 19 Training Loss: 0.6128730177879333 \n",
      "     Training Step: 20 Training Loss: 0.6136336326599121 \n",
      "     Training Step: 21 Training Loss: 0.6151750087738037 \n",
      "     Training Step: 22 Training Loss: 0.6202290058135986 \n",
      "     Training Step: 23 Training Loss: 0.6180407404899597 \n",
      "     Training Step: 24 Training Loss: 0.6156833171844482 \n",
      "     Training Step: 25 Training Loss: 0.6182307004928589 \n",
      "     Training Step: 26 Training Loss: 0.6140230894088745 \n",
      "     Training Step: 27 Training Loss: 0.6209149360656738 \n",
      "     Training Step: 28 Training Loss: 0.6154783368110657 \n",
      "     Training Step: 29 Training Loss: 0.6171844005584717 \n",
      "     Training Step: 30 Training Loss: 0.6137658357620239 \n",
      "     Training Step: 31 Training Loss: 0.6134937405586243 \n",
      "     Training Step: 32 Training Loss: 0.6133148670196533 \n",
      "     Training Step: 33 Training Loss: 0.6104376912117004 \n",
      "     Training Step: 34 Training Loss: 0.6177749037742615 \n",
      "     Training Step: 35 Training Loss: 0.6115890741348267 \n",
      "     Training Step: 36 Training Loss: 0.6166983842849731 \n",
      "     Training Step: 37 Training Loss: 0.6150661706924438 \n",
      "     Training Step: 38 Training Loss: 0.6123833060264587 \n",
      "     Training Step: 39 Training Loss: 0.6140726208686829 \n",
      "     Training Step: 40 Training Loss: 0.6129303574562073 \n",
      "     Training Step: 41 Training Loss: 0.6143731474876404 \n",
      "     Training Step: 42 Training Loss: 0.6194643974304199 \n",
      "     Training Step: 43 Training Loss: 0.6162127256393433 \n",
      "     Training Step: 44 Training Loss: 0.6132915019989014 \n",
      "     Training Step: 45 Training Loss: 0.61988365650177 \n",
      "     Training Step: 46 Training Loss: 0.6105806827545166 \n",
      "     Training Step: 47 Training Loss: 0.6118220090866089 \n",
      "     Training Step: 48 Training Loss: 0.6100172996520996 \n",
      "     Training Step: 49 Training Loss: 0.6116533279418945 \n",
      "     Training Step: 50 Training Loss: 0.6147052049636841 \n",
      "     Training Step: 51 Training Loss: 0.6177125573158264 \n",
      "     Training Step: 52 Training Loss: 0.6186044216156006 \n",
      "     Training Step: 53 Training Loss: 0.612384557723999 \n",
      "     Training Step: 54 Training Loss: 0.6151008605957031 \n",
      "     Training Step: 55 Training Loss: 0.6166158318519592 \n",
      "     Training Step: 56 Training Loss: 0.6163486838340759 \n",
      "     Training Step: 57 Training Loss: 0.6168014407157898 \n",
      "     Training Step: 58 Training Loss: 0.6105313301086426 \n",
      "     Training Step: 59 Training Loss: 0.6159984469413757 \n",
      "     Training Step: 60 Training Loss: 0.6142248511314392 \n",
      "     Training Step: 61 Training Loss: 0.6180105209350586 \n",
      "     Training Step: 62 Training Loss: 0.6146886944770813 \n",
      "     Training Step: 63 Training Loss: 0.6111763119697571 \n",
      "     Training Step: 64 Training Loss: 0.6155279278755188 \n",
      "     Training Step: 65 Training Loss: 0.6107417345046997 \n",
      "     Training Step: 66 Training Loss: 0.6152439117431641 \n",
      "     Training Step: 67 Training Loss: 0.6154119968414307 \n",
      "     Training Step: 68 Training Loss: 0.612910270690918 \n",
      "     Training Step: 69 Training Loss: 0.613059401512146 \n",
      "     Training Step: 70 Training Loss: 0.6122931241989136 \n",
      "     Training Step: 71 Training Loss: 0.6114752292633057 \n",
      "     Training Step: 72 Training Loss: 0.6121382713317871 \n",
      "     Training Step: 73 Training Loss: 0.6143442988395691 \n",
      "     Training Step: 74 Training Loss: 0.6114148497581482 \n",
      "     Training Step: 75 Training Loss: 0.6147559285163879 \n",
      "     Training Step: 76 Training Loss: 0.6113983988761902 \n",
      "     Training Step: 77 Training Loss: 0.6141577363014221 \n",
      "     Training Step: 78 Training Loss: 0.6188912391662598 \n",
      "     Training Step: 79 Training Loss: 0.6100599765777588 \n",
      "     Training Step: 80 Training Loss: 0.6118234395980835 \n",
      "     Training Step: 81 Training Loss: 0.6154534220695496 \n",
      "     Training Step: 82 Training Loss: 0.6171834468841553 \n",
      "     Training Step: 83 Training Loss: 0.611431360244751 \n",
      "     Training Step: 84 Training Loss: 0.6118783354759216 \n",
      "     Training Step: 85 Training Loss: 0.6157457232475281 \n",
      "     Training Step: 86 Training Loss: 0.6146368980407715 \n",
      "     Training Step: 87 Training Loss: 0.6107412576675415 \n",
      "     Training Step: 88 Training Loss: 0.6115905046463013 \n",
      "     Training Step: 89 Training Loss: 0.619642436504364 \n",
      "     Training Step: 90 Training Loss: 0.6118402481079102 \n",
      "     Training Step: 91 Training Loss: 0.6133058071136475 \n",
      "     Training Step: 92 Training Loss: 0.6184133291244507 \n",
      "     Training Step: 93 Training Loss: 0.6155253648757935 \n",
      "     Training Step: 94 Training Loss: 0.6168020963668823 \n",
      "     Training Step: 95 Training Loss: 0.6101570725440979 \n",
      "     Training Step: 96 Training Loss: 0.6127108931541443 \n",
      "     Training Step: 97 Training Loss: 0.6127686500549316 \n",
      "     Training Step: 98 Training Loss: 0.6097485423088074 \n",
      "     Training Step: 99 Training Loss: 0.610666036605835 \n",
      "     Training Step: 100 Training Loss: 0.6131854057312012 \n",
      "     Training Step: 101 Training Loss: 0.6116089820861816 \n",
      "     Training Step: 102 Training Loss: 0.6169149279594421 \n",
      "     Training Step: 103 Training Loss: 0.6183711886405945 \n",
      "     Training Step: 104 Training Loss: 0.6096839308738708 \n",
      "     Training Step: 105 Training Loss: 0.6189122796058655 \n",
      "     Training Step: 106 Training Loss: 0.6143500804901123 \n",
      "     Training Step: 107 Training Loss: 0.6167836785316467 \n",
      "     Training Step: 108 Training Loss: 0.6123787760734558 \n",
      "     Training Step: 109 Training Loss: 0.6146537065505981 \n",
      "     Training Step: 110 Training Loss: 0.6171453595161438 \n",
      "     Training Step: 111 Training Loss: 0.616714596748352 \n",
      "     Training Step: 112 Training Loss: 0.612244188785553 \n",
      "     Training Step: 113 Training Loss: 0.6159404516220093 \n",
      "     Training Step: 114 Training Loss: 0.6121848821640015 \n",
      "     Training Step: 115 Training Loss: 0.6153348684310913 \n",
      "     Training Step: 116 Training Loss: 0.612820565700531 \n",
      "     Training Step: 117 Training Loss: 0.6116482019424438 \n",
      "     Training Step: 118 Training Loss: 0.6152870655059814 \n",
      "     Training Step: 119 Training Loss: 0.6125442385673523 \n",
      "     Training Step: 120 Training Loss: 0.6120266914367676 \n",
      "     Training Step: 121 Training Loss: 0.6109148263931274 \n",
      "     Training Step: 122 Training Loss: 0.6174972057342529 \n",
      "     Training Step: 123 Training Loss: 0.6157588362693787 \n",
      "     Training Step: 124 Training Loss: 0.616211473941803 \n",
      "     Training Step: 125 Training Loss: 0.6166974306106567 \n",
      "     Training Step: 126 Training Loss: 0.6144988536834717 \n",
      "     Training Step: 127 Training Loss: 0.6164966821670532 \n",
      "     Training Step: 128 Training Loss: 0.6125137805938721 \n",
      "     Training Step: 129 Training Loss: 0.6140174865722656 \n",
      "     Training Step: 130 Training Loss: 0.6158374547958374 \n",
      "     Training Step: 131 Training Loss: 0.615284264087677 \n",
      "     Training Step: 132 Training Loss: 0.6180947422981262 \n",
      "     Training Step: 133 Training Loss: 0.6126489043235779 \n",
      "     Training Step: 134 Training Loss: 0.6115399599075317 \n",
      "     Training Step: 135 Training Loss: 0.6100735068321228 \n",
      "     Training Step: 136 Training Loss: 0.6161103248596191 \n",
      "     Training Step: 137 Training Loss: 0.6157393455505371 \n",
      "     Training Step: 138 Training Loss: 0.6173821687698364 \n",
      "     Training Step: 139 Training Loss: 0.6105829477310181 \n",
      "     Training Step: 140 Training Loss: 0.6146675944328308 \n",
      "     Training Step: 141 Training Loss: 0.6122379302978516 \n",
      "     Training Step: 142 Training Loss: 0.6167251467704773 \n",
      "     Training Step: 143 Training Loss: 0.6139116287231445 \n",
      "     Training Step: 144 Training Loss: 0.6101802587509155 \n",
      "     Training Step: 145 Training Loss: 0.6131142973899841 \n",
      "     Training Step: 146 Training Loss: 0.6137426495552063 \n",
      "     Training Step: 147 Training Loss: 0.6100478172302246 \n",
      "     Training Step: 148 Training Loss: 0.6121185421943665 \n",
      "     Training Step: 149 Training Loss: 0.6164525151252747 \n",
      "     Training Step: 150 Training Loss: 0.6149399280548096 \n",
      "     Training Step: 151 Training Loss: 0.6118224263191223 \n",
      "     Training Step: 152 Training Loss: 0.6097173690795898 \n",
      "     Training Step: 153 Training Loss: 0.6134105920791626 \n",
      "     Training Step: 154 Training Loss: 0.6185019016265869 \n",
      "     Training Step: 155 Training Loss: 0.6093796491622925 \n",
      "     Training Step: 156 Training Loss: 0.6138186454772949 \n",
      "     Training Step: 157 Training Loss: 0.6154094934463501 \n",
      "     Training Step: 158 Training Loss: 0.6166843175888062 \n",
      "     Training Step: 159 Training Loss: 0.6164239645004272 \n",
      "     Training Step: 160 Training Loss: 0.6142846941947937 \n",
      "     Training Step: 161 Training Loss: 0.6176405549049377 \n",
      "     Training Step: 162 Training Loss: 0.6125401258468628 \n",
      "     Training Step: 163 Training Loss: 0.6149556040763855 \n",
      "     Training Step: 164 Training Loss: 0.6115539073944092 \n",
      "     Training Step: 165 Training Loss: 0.6146785020828247 \n",
      "     Training Step: 166 Training Loss: 0.6117998957633972 \n",
      "     Training Step: 167 Training Loss: 0.6152927875518799 \n",
      "     Training Step: 168 Training Loss: 0.6166443228721619 \n",
      "     Training Step: 169 Training Loss: 0.6147980093955994 \n",
      "     Training Step: 170 Training Loss: 0.6094919443130493 \n",
      "     Training Step: 171 Training Loss: 0.6146761178970337 \n",
      "     Training Step: 172 Training Loss: 0.6151590347290039 \n",
      "     Training Step: 173 Training Loss: 0.6122285723686218 \n",
      "     Training Step: 174 Training Loss: 0.6116408109664917 \n",
      "     Training Step: 175 Training Loss: 0.6167232394218445 \n",
      "     Training Step: 176 Training Loss: 0.6185933351516724 \n",
      "     Training Step: 177 Training Loss: 0.611527144908905 \n",
      "     Training Step: 178 Training Loss: 0.6108779907226562 \n",
      "     Training Step: 179 Training Loss: 0.6153764128684998 \n",
      "     Training Step: 180 Training Loss: 0.6130702495574951 \n",
      "     Training Step: 181 Training Loss: 0.6146106719970703 \n",
      "     Training Step: 182 Training Loss: 0.6106855273246765 \n",
      "     Training Step: 183 Training Loss: 0.6147295236587524 \n",
      "     Training Step: 184 Training Loss: 0.6133134961128235 \n",
      "     Training Step: 185 Training Loss: 0.6105900406837463 \n",
      "     Training Step: 186 Training Loss: 0.6132040619850159 \n",
      "     Training Step: 187 Training Loss: 0.6129626035690308 \n",
      "     Training Step: 188 Training Loss: 0.61350017786026 \n",
      "     Training Step: 189 Training Loss: 0.6149258017539978 \n",
      "     Training Step: 190 Training Loss: 0.61415696144104 \n",
      "     Training Step: 191 Training Loss: 0.6157055497169495 \n",
      "     Training Step: 192 Training Loss: 0.6146020293235779 \n",
      "     Training Step: 193 Training Loss: 0.6176972985267639 \n",
      "     Training Step: 194 Training Loss: 0.6116906404495239 \n",
      "     Training Step: 195 Training Loss: 0.6142125129699707 \n",
      "     Training Step: 196 Training Loss: 0.6177579164505005 \n",
      "     Training Step: 197 Training Loss: 0.6182140111923218 \n",
      "     Training Step: 198 Training Loss: 0.6106163859367371 \n",
      "     Training Step: 199 Training Loss: 0.6154205799102783 \n",
      "     Training Step: 200 Training Loss: 0.6114617586135864 \n",
      "     Training Step: 201 Training Loss: 0.6144590973854065 \n",
      "     Training Step: 202 Training Loss: 0.6144222021102905 \n",
      "     Training Step: 203 Training Loss: 0.6124715209007263 \n",
      "     Training Step: 204 Training Loss: 0.6134645938873291 \n",
      "     Training Step: 205 Training Loss: 0.61114501953125 \n",
      "     Training Step: 206 Training Loss: 0.6137598156929016 \n",
      "     Training Step: 207 Training Loss: 0.6143438816070557 \n",
      "     Training Step: 208 Training Loss: 0.6144874095916748 \n",
      "     Training Step: 209 Training Loss: 0.6184725165367126 \n",
      "     Training Step: 210 Training Loss: 0.6150921583175659 \n",
      "     Training Step: 211 Training Loss: 0.6135186553001404 \n",
      "     Training Step: 212 Training Loss: 0.612285852432251 \n",
      "     Training Step: 213 Training Loss: 0.6139308214187622 \n",
      "     Training Step: 214 Training Loss: 0.613621711730957 \n",
      "     Training Step: 215 Training Loss: 0.611801266670227 \n",
      "     Training Step: 216 Training Loss: 0.6111968755722046 \n",
      "     Training Step: 217 Training Loss: 0.6104816794395447 \n",
      "     Training Step: 218 Training Loss: 0.6160395741462708 \n",
      "     Training Step: 219 Training Loss: 0.6147735118865967 \n",
      "     Training Step: 220 Training Loss: 0.6166837811470032 \n",
      "     Training Step: 221 Training Loss: 0.6171079874038696 \n",
      "     Training Step: 222 Training Loss: 0.6148794293403625 \n",
      "     Training Step: 223 Training Loss: 0.613895058631897 \n",
      "     Training Step: 224 Training Loss: 0.6168136596679688 \n",
      "     Training Step: 225 Training Loss: 0.6128746867179871 \n",
      "     Training Step: 226 Training Loss: 0.6146180033683777 \n",
      "     Training Step: 227 Training Loss: 0.615276575088501 \n",
      "     Training Step: 228 Training Loss: 0.6171332001686096 \n",
      "     Training Step: 229 Training Loss: 0.6135979890823364 \n",
      "     Training Step: 230 Training Loss: 0.6153976321220398 \n",
      "     Training Step: 231 Training Loss: 0.6083195805549622 \n",
      "     Training Step: 232 Training Loss: 0.6196892261505127 \n",
      "     Training Step: 233 Training Loss: 0.6176817417144775 \n",
      "     Training Step: 234 Training Loss: 0.611431360244751 \n",
      "     Training Step: 235 Training Loss: 0.614247739315033 \n",
      "     Training Step: 236 Training Loss: 0.6131511926651001 \n",
      "     Training Step: 237 Training Loss: 0.6162352561950684 \n",
      "     Training Step: 238 Training Loss: 0.6136499047279358 \n",
      "     Training Step: 239 Training Loss: 0.6120701432228088 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.615313708782196 \n",
      "     Validation Step: 1 Validation Loss: 0.6159933805465698 \n",
      "     Validation Step: 2 Validation Loss: 0.6106418967247009 \n",
      "     Validation Step: 3 Validation Loss: 0.6142011880874634 \n",
      "     Validation Step: 4 Validation Loss: 0.6101498007774353 \n",
      "     Validation Step: 5 Validation Loss: 0.6180626749992371 \n",
      "     Validation Step: 6 Validation Loss: 0.6173091530799866 \n",
      "     Validation Step: 7 Validation Loss: 0.6111654043197632 \n",
      "     Validation Step: 8 Validation Loss: 0.6156139373779297 \n",
      "     Validation Step: 9 Validation Loss: 0.6105195879936218 \n",
      "     Validation Step: 10 Validation Loss: 0.6101800203323364 \n",
      "     Validation Step: 11 Validation Loss: 0.611650288105011 \n",
      "     Validation Step: 12 Validation Loss: 0.6182309985160828 \n",
      "     Validation Step: 13 Validation Loss: 0.6157931685447693 \n",
      "     Validation Step: 14 Validation Loss: 0.6184976100921631 \n",
      "     Validation Step: 15 Validation Loss: 0.6141200661659241 \n",
      "     Validation Step: 16 Validation Loss: 0.6176026463508606 \n",
      "     Validation Step: 17 Validation Loss: 0.6155743598937988 \n",
      "     Validation Step: 18 Validation Loss: 0.6115596890449524 \n",
      "     Validation Step: 19 Validation Loss: 0.615034282207489 \n",
      "     Validation Step: 20 Validation Loss: 0.6104881167411804 \n",
      "     Validation Step: 21 Validation Loss: 0.615229070186615 \n",
      "     Validation Step: 22 Validation Loss: 0.6128379106521606 \n",
      "     Validation Step: 23 Validation Loss: 0.6145398020744324 \n",
      "     Validation Step: 24 Validation Loss: 0.6162428855895996 \n",
      "     Validation Step: 25 Validation Loss: 0.6148987412452698 \n",
      "     Validation Step: 26 Validation Loss: 0.6136458516120911 \n",
      "     Validation Step: 27 Validation Loss: 0.6184768676757812 \n",
      "     Validation Step: 28 Validation Loss: 0.613311767578125 \n",
      "     Validation Step: 29 Validation Loss: 0.6118863224983215 \n",
      "     Validation Step: 30 Validation Loss: 0.6176960468292236 \n",
      "     Validation Step: 31 Validation Loss: 0.6183395981788635 \n",
      "     Validation Step: 32 Validation Loss: 0.6141261458396912 \n",
      "     Validation Step: 33 Validation Loss: 0.611182689666748 \n",
      "     Validation Step: 34 Validation Loss: 0.612132728099823 \n",
      "     Validation Step: 35 Validation Loss: 0.6129850149154663 \n",
      "     Validation Step: 36 Validation Loss: 0.617023229598999 \n",
      "     Validation Step: 37 Validation Loss: 0.6101159453392029 \n",
      "     Validation Step: 38 Validation Loss: 0.6148382425308228 \n",
      "     Validation Step: 39 Validation Loss: 0.6145740747451782 \n",
      "     Validation Step: 40 Validation Loss: 0.607539176940918 \n",
      "     Validation Step: 41 Validation Loss: 0.6146374344825745 \n",
      "     Validation Step: 42 Validation Loss: 0.6142613291740417 \n",
      "     Validation Step: 43 Validation Loss: 0.6136647462844849 \n",
      "     Validation Step: 44 Validation Loss: 0.6136466264724731 \n",
      "Epoch: 85\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6133524179458618 \n",
      "     Training Step: 1 Training Loss: 0.6155214905738831 \n",
      "     Training Step: 2 Training Loss: 0.6132035255432129 \n",
      "     Training Step: 3 Training Loss: 0.6144431233406067 \n",
      "     Training Step: 4 Training Loss: 0.6151768565177917 \n",
      "     Training Step: 5 Training Loss: 0.6160075068473816 \n",
      "     Training Step: 6 Training Loss: 0.6122913360595703 \n",
      "     Training Step: 7 Training Loss: 0.615744948387146 \n",
      "     Training Step: 8 Training Loss: 0.613886833190918 \n",
      "     Training Step: 9 Training Loss: 0.6159476041793823 \n",
      "     Training Step: 10 Training Loss: 0.6166805624961853 \n",
      "     Training Step: 11 Training Loss: 0.6196210980415344 \n",
      "     Training Step: 12 Training Loss: 0.6134648323059082 \n",
      "     Training Step: 13 Training Loss: 0.612242579460144 \n",
      "     Training Step: 14 Training Loss: 0.6169111132621765 \n",
      "     Training Step: 15 Training Loss: 0.6141499280929565 \n",
      "     Training Step: 16 Training Loss: 0.613641083240509 \n",
      "     Training Step: 17 Training Loss: 0.6101775169372559 \n",
      "     Training Step: 18 Training Loss: 0.6109417676925659 \n",
      "     Training Step: 19 Training Loss: 0.6168841123580933 \n",
      "     Training Step: 20 Training Loss: 0.6135022640228271 \n",
      "     Training Step: 21 Training Loss: 0.6166170239448547 \n",
      "     Training Step: 22 Training Loss: 0.6129254102706909 \n",
      "     Training Step: 23 Training Loss: 0.6116328239440918 \n",
      "     Training Step: 24 Training Loss: 0.6132784485816956 \n",
      "     Training Step: 25 Training Loss: 0.6153079271316528 \n",
      "     Training Step: 26 Training Loss: 0.6114320755004883 \n",
      "     Training Step: 27 Training Loss: 0.6115175485610962 \n",
      "     Training Step: 28 Training Loss: 0.6128391623497009 \n",
      "     Training Step: 29 Training Loss: 0.6116382479667664 \n",
      "     Training Step: 30 Training Loss: 0.6130751371383667 \n",
      "     Training Step: 31 Training Loss: 0.6125320196151733 \n",
      "     Training Step: 32 Training Loss: 0.6153023838996887 \n",
      "     Training Step: 33 Training Loss: 0.6164553761482239 \n",
      "     Training Step: 34 Training Loss: 0.6141548156738281 \n",
      "     Training Step: 35 Training Loss: 0.6122308373451233 \n",
      "     Training Step: 36 Training Loss: 0.6162246465682983 \n",
      "     Training Step: 37 Training Loss: 0.6184611916542053 \n",
      "     Training Step: 38 Training Loss: 0.6137431859970093 \n",
      "     Training Step: 39 Training Loss: 0.615534782409668 \n",
      "     Training Step: 40 Training Loss: 0.6146993041038513 \n",
      "     Training Step: 41 Training Loss: 0.6171532273292542 \n",
      "     Training Step: 42 Training Loss: 0.6158345341682434 \n",
      "     Training Step: 43 Training Loss: 0.6157822012901306 \n",
      "     Training Step: 44 Training Loss: 0.6143448948860168 \n",
      "     Training Step: 45 Training Loss: 0.6114670634269714 \n",
      "     Training Step: 46 Training Loss: 0.6140406131744385 \n",
      "     Training Step: 47 Training Loss: 0.6109403967857361 \n",
      "     Training Step: 48 Training Loss: 0.6198453307151794 \n",
      "     Training Step: 49 Training Loss: 0.6167955994606018 \n",
      "     Training Step: 50 Training Loss: 0.616226077079773 \n",
      "     Training Step: 51 Training Loss: 0.6167964339256287 \n",
      "     Training Step: 52 Training Loss: 0.6150635480880737 \n",
      "     Training Step: 53 Training Loss: 0.6148033738136292 \n",
      "     Training Step: 54 Training Loss: 0.6143333315849304 \n",
      "     Training Step: 55 Training Loss: 0.6176181435585022 \n",
      "     Training Step: 56 Training Loss: 0.6155270338058472 \n",
      "     Training Step: 57 Training Loss: 0.6127755641937256 \n",
      "     Training Step: 58 Training Loss: 0.6101058721542358 \n",
      "     Training Step: 59 Training Loss: 0.6140698194503784 \n",
      "     Training Step: 60 Training Loss: 0.6180084943771362 \n",
      "     Training Step: 61 Training Loss: 0.611492395401001 \n",
      "     Training Step: 62 Training Loss: 0.6142889261245728 \n",
      "     Training Step: 63 Training Loss: 0.611627459526062 \n",
      "     Training Step: 64 Training Loss: 0.6142175197601318 \n",
      "     Training Step: 65 Training Loss: 0.6139301657676697 \n",
      "     Training Step: 66 Training Loss: 0.613294243812561 \n",
      "     Training Step: 67 Training Loss: 0.6167726516723633 \n",
      "     Training Step: 68 Training Loss: 0.6132056713104248 \n",
      "     Training Step: 69 Training Loss: 0.6123778223991394 \n",
      "     Training Step: 70 Training Loss: 0.6153921484947205 \n",
      "     Training Step: 71 Training Loss: 0.6092315316200256 \n",
      "     Training Step: 72 Training Loss: 0.6130573153495789 \n",
      "     Training Step: 73 Training Loss: 0.6154809594154358 \n",
      "     Training Step: 74 Training Loss: 0.61280357837677 \n",
      "     Training Step: 75 Training Loss: 0.6153408885002136 \n",
      "     Training Step: 76 Training Loss: 0.6123718023300171 \n",
      "     Training Step: 77 Training Loss: 0.6156179904937744 \n",
      "     Training Step: 78 Training Loss: 0.6115762591362 \n",
      "     Training Step: 79 Training Loss: 0.6118192076683044 \n",
      "     Training Step: 80 Training Loss: 0.6103618741035461 \n",
      "     Training Step: 81 Training Loss: 0.6146682500839233 \n",
      "     Training Step: 82 Training Loss: 0.617519736289978 \n",
      "     Training Step: 83 Training Loss: 0.615409255027771 \n",
      "     Training Step: 84 Training Loss: 0.6133065223693848 \n",
      "     Training Step: 85 Training Loss: 0.6166789531707764 \n",
      "     Training Step: 86 Training Loss: 0.6101795434951782 \n",
      "     Training Step: 87 Training Loss: 0.6118833422660828 \n",
      "     Training Step: 88 Training Loss: 0.6147236824035645 \n",
      "     Training Step: 89 Training Loss: 0.6166664958000183 \n",
      "     Training Step: 90 Training Loss: 0.6148701906204224 \n",
      "     Training Step: 91 Training Loss: 0.6181043982505798 \n",
      "     Training Step: 92 Training Loss: 0.611561119556427 \n",
      "     Training Step: 93 Training Loss: 0.6120748519897461 \n",
      "     Training Step: 94 Training Loss: 0.6111670136451721 \n",
      "     Training Step: 95 Training Loss: 0.6137402057647705 \n",
      "     Training Step: 96 Training Loss: 0.6135917901992798 \n",
      "     Training Step: 97 Training Loss: 0.6146364212036133 \n",
      "     Training Step: 98 Training Loss: 0.6168065071105957 \n",
      "     Training Step: 99 Training Loss: 0.6209571361541748 \n",
      "     Training Step: 100 Training Loss: 0.6147706508636475 \n",
      "     Training Step: 101 Training Loss: 0.6147518754005432 \n",
      "     Training Step: 102 Training Loss: 0.6105865240097046 \n",
      "     Training Step: 103 Training Loss: 0.6105139255523682 \n",
      "     Training Step: 104 Training Loss: 0.6133026480674744 \n",
      "     Training Step: 105 Training Loss: 0.6149142384529114 \n",
      "     Training Step: 106 Training Loss: 0.6171951293945312 \n",
      "     Training Step: 107 Training Loss: 0.6176896095275879 \n",
      "     Training Step: 108 Training Loss: 0.618320107460022 \n",
      "     Training Step: 109 Training Loss: 0.6128678321838379 \n",
      "     Training Step: 110 Training Loss: 0.6153714656829834 \n",
      "     Training Step: 111 Training Loss: 0.615088701248169 \n",
      "     Training Step: 112 Training Loss: 0.6154162287712097 \n",
      "     Training Step: 113 Training Loss: 0.6145017743110657 \n",
      "     Training Step: 114 Training Loss: 0.6157404780387878 \n",
      "     Training Step: 115 Training Loss: 0.6167055368423462 \n",
      "     Training Step: 116 Training Loss: 0.6152872443199158 \n",
      "     Training Step: 117 Training Loss: 0.6144245266914368 \n",
      "     Training Step: 118 Training Loss: 0.6100893020629883 \n",
      "     Training Step: 119 Training Loss: 0.6118156909942627 \n",
      "     Training Step: 120 Training Loss: 0.6140170097351074 \n",
      "     Training Step: 121 Training Loss: 0.6167116165161133 \n",
      "     Training Step: 122 Training Loss: 0.6121270656585693 \n",
      "     Training Step: 123 Training Loss: 0.6170961856842041 \n",
      "     Training Step: 124 Training Loss: 0.6149343252182007 \n",
      "     Training Step: 125 Training Loss: 0.6180769205093384 \n",
      "     Training Step: 126 Training Loss: 0.6164926290512085 \n",
      "     Training Step: 127 Training Loss: 0.61665278673172 \n",
      "     Training Step: 128 Training Loss: 0.6171351671218872 \n",
      "     Training Step: 129 Training Loss: 0.6177554130554199 \n",
      "     Training Step: 130 Training Loss: 0.6121659278869629 \n",
      "     Training Step: 131 Training Loss: 0.6134392619132996 \n",
      "     Training Step: 132 Training Loss: 0.6173664927482605 \n",
      "     Training Step: 133 Training Loss: 0.6152470707893372 \n",
      "     Training Step: 134 Training Loss: 0.61217200756073 \n",
      "     Training Step: 135 Training Loss: 0.6106460690498352 \n",
      "     Training Step: 136 Training Loss: 0.6097654104232788 \n",
      "     Training Step: 137 Training Loss: 0.6167152523994446 \n",
      "     Training Step: 138 Training Loss: 0.614342987537384 \n",
      "     Training Step: 139 Training Loss: 0.6116290092468262 \n",
      "     Training Step: 140 Training Loss: 0.610586404800415 \n",
      "     Training Step: 141 Training Loss: 0.6146849393844604 \n",
      "     Training Step: 142 Training Loss: 0.6126427054405212 \n",
      "     Training Step: 143 Training Loss: 0.6111347675323486 \n",
      "     Training Step: 144 Training Loss: 0.6127038598060608 \n",
      "     Training Step: 145 Training Loss: 0.6122310161590576 \n",
      "     Training Step: 146 Training Loss: 0.6106728315353394 \n",
      "     Training Step: 147 Training Loss: 0.6142665147781372 \n",
      "     Training Step: 148 Training Loss: 0.6120164394378662 \n",
      "     Training Step: 149 Training Loss: 0.6157905459403992 \n",
      "     Training Step: 150 Training Loss: 0.6147269606590271 \n",
      "     Training Step: 151 Training Loss: 0.617849588394165 \n",
      "     Training Step: 152 Training Loss: 0.6105585694313049 \n",
      "     Training Step: 153 Training Loss: 0.6117782592773438 \n",
      "     Training Step: 154 Training Loss: 0.6136415004730225 \n",
      "     Training Step: 155 Training Loss: 0.614621639251709 \n",
      "     Training Step: 156 Training Loss: 0.6197279095649719 \n",
      "     Training Step: 157 Training Loss: 0.6182265281677246 \n",
      "     Training Step: 158 Training Loss: 0.6146671772003174 \n",
      "     Training Step: 159 Training Loss: 0.6144688725471497 \n",
      "     Training Step: 160 Training Loss: 0.6135291457176208 \n",
      "     Training Step: 161 Training Loss: 0.6105315685272217 \n",
      "     Training Step: 162 Training Loss: 0.6188595294952393 \n",
      "     Training Step: 163 Training Loss: 0.6194285750389099 \n",
      "     Training Step: 164 Training Loss: 0.6146214604377747 \n",
      "     Training Step: 165 Training Loss: 0.6146938800811768 \n",
      "     Training Step: 166 Training Loss: 0.6151053309440613 \n",
      "     Training Step: 167 Training Loss: 0.6132863759994507 \n",
      "     Training Step: 168 Training Loss: 0.6188392639160156 \n",
      "     Training Step: 169 Training Loss: 0.6136504411697388 \n",
      "     Training Step: 170 Training Loss: 0.6143679022789001 \n",
      "     Training Step: 171 Training Loss: 0.6202123761177063 \n",
      "     Training Step: 172 Training Loss: 0.6176857948303223 \n",
      "     Training Step: 173 Training Loss: 0.6183935403823853 \n",
      "     Training Step: 174 Training Loss: 0.613135039806366 \n",
      "     Training Step: 175 Training Loss: 0.609780490398407 \n",
      "     Training Step: 176 Training Loss: 0.6139344573020935 \n",
      "     Training Step: 177 Training Loss: 0.613469660282135 \n",
      "     Training Step: 178 Training Loss: 0.6163442134857178 \n",
      "     Training Step: 179 Training Loss: 0.6129770278930664 \n",
      "     Training Step: 180 Training Loss: 0.6083030700683594 \n",
      "     Training Step: 181 Training Loss: 0.6182251572608948 \n",
      "     Training Step: 182 Training Loss: 0.6178138256072998 \n",
      "     Training Step: 183 Training Loss: 0.6123795509338379 \n",
      "     Training Step: 184 Training Loss: 0.6177194714546204 \n",
      "     Training Step: 185 Training Loss: 0.610747218132019 \n",
      "     Training Step: 186 Training Loss: 0.6137652397155762 \n",
      "     Training Step: 187 Training Loss: 0.614212155342102 \n",
      "     Training Step: 188 Training Loss: 0.6160383224487305 \n",
      "     Training Step: 189 Training Loss: 0.6124935150146484 \n",
      "     Training Step: 190 Training Loss: 0.6131864786148071 \n",
      "     Training Step: 191 Training Loss: 0.6161125302314758 \n",
      "     Training Step: 192 Training Loss: 0.614036500453949 \n",
      "     Training Step: 193 Training Loss: 0.6107302904129028 \n",
      "     Training Step: 194 Training Loss: 0.609994649887085 \n",
      "     Training Step: 195 Training Loss: 0.6164162755012512 \n",
      "     Training Step: 196 Training Loss: 0.6127656102180481 \n",
      "     Training Step: 197 Training Loss: 0.6124625205993652 \n",
      "     Training Step: 198 Training Loss: 0.6125270128250122 \n",
      "     Training Step: 199 Training Loss: 0.6186112761497498 \n",
      "     Training Step: 200 Training Loss: 0.6116828322410583 \n",
      "     Training Step: 201 Training Loss: 0.616217315196991 \n",
      "     Training Step: 202 Training Loss: 0.6154266595840454 \n",
      "     Training Step: 203 Training Loss: 0.6149592995643616 \n",
      "     Training Step: 204 Training Loss: 0.61527019739151 \n",
      "     Training Step: 205 Training Loss: 0.6114189028739929 \n",
      "     Training Step: 206 Training Loss: 0.6115338206291199 \n",
      "     Training Step: 207 Training Loss: 0.612520158290863 \n",
      "     Training Step: 208 Training Loss: 0.6144789457321167 \n",
      "     Training Step: 209 Training Loss: 0.6115302443504333 \n",
      "     Training Step: 210 Training Loss: 0.6104050874710083 \n",
      "     Training Step: 211 Training Loss: 0.6106594204902649 \n",
      "     Training Step: 212 Training Loss: 0.6114304065704346 \n",
      "     Training Step: 213 Training Loss: 0.6122775077819824 \n",
      "     Training Step: 214 Training Loss: 0.60943603515625 \n",
      "     Training Step: 215 Training Loss: 0.6157292127609253 \n",
      "     Training Step: 216 Training Loss: 0.6167596578598022 \n",
      "     Training Step: 217 Training Loss: 0.6151698231697083 \n",
      "     Training Step: 218 Training Loss: 0.6186631917953491 \n",
      "     Training Step: 219 Training Loss: 0.6093739867210388 \n",
      "     Training Step: 220 Training Loss: 0.6118296384811401 \n",
      "     Training Step: 221 Training Loss: 0.6154582500457764 \n",
      "     Training Step: 222 Training Loss: 0.609693169593811 \n",
      "     Training Step: 223 Training Loss: 0.6111932396888733 \n",
      "     Training Step: 224 Training Loss: 0.6128690242767334 \n",
      "     Training Step: 225 Training Loss: 0.6100565195083618 \n",
      "     Training Step: 226 Training Loss: 0.6171659827232361 \n",
      "     Training Step: 227 Training Loss: 0.6113895773887634 \n",
      "     Training Step: 228 Training Loss: 0.6117814779281616 \n",
      "     Training Step: 229 Training Loss: 0.6131335496902466 \n",
      "     Training Step: 230 Training Loss: 0.6121541857719421 \n",
      "     Training Step: 231 Training Loss: 0.6129220724105835 \n",
      "     Training Step: 232 Training Loss: 0.6184980273246765 \n",
      "     Training Step: 233 Training Loss: 0.6146019697189331 \n",
      "     Training Step: 234 Training Loss: 0.611821711063385 \n",
      "     Training Step: 235 Training Loss: 0.6180571913719177 \n",
      "     Training Step: 236 Training Loss: 0.6156845688819885 \n",
      "     Training Step: 237 Training Loss: 0.6141093373298645 \n",
      "     Training Step: 238 Training Loss: 0.6138237714767456 \n",
      "     Training Step: 239 Training Loss: 0.6146580576896667 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6142033338546753 \n",
      "     Validation Step: 1 Validation Loss: 0.6146422028541565 \n",
      "     Validation Step: 2 Validation Loss: 0.6101740002632141 \n",
      "     Validation Step: 3 Validation Loss: 0.6075502038002014 \n",
      "     Validation Step: 4 Validation Loss: 0.6141162514686584 \n",
      "     Validation Step: 5 Validation Loss: 0.6101317405700684 \n",
      "     Validation Step: 6 Validation Loss: 0.6129922866821289 \n",
      "     Validation Step: 7 Validation Loss: 0.6148936152458191 \n",
      "     Validation Step: 8 Validation Loss: 0.6115683317184448 \n",
      "     Validation Step: 9 Validation Loss: 0.6184907555580139 \n",
      "     Validation Step: 10 Validation Loss: 0.6175913214683533 \n",
      "     Validation Step: 11 Validation Loss: 0.6133051514625549 \n",
      "     Validation Step: 12 Validation Loss: 0.6106470227241516 \n",
      "     Validation Step: 13 Validation Loss: 0.6101608872413635 \n",
      "     Validation Step: 14 Validation Loss: 0.6111791133880615 \n",
      "     Validation Step: 15 Validation Loss: 0.6136763095855713 \n",
      "     Validation Step: 16 Validation Loss: 0.615229070186615 \n",
      "     Validation Step: 17 Validation Loss: 0.6155766844749451 \n",
      "     Validation Step: 18 Validation Loss: 0.6145337224006653 \n",
      "     Validation Step: 19 Validation Loss: 0.6128336787223816 \n",
      "     Validation Step: 20 Validation Loss: 0.6104943156242371 \n",
      "     Validation Step: 21 Validation Loss: 0.6145700216293335 \n",
      "     Validation Step: 22 Validation Loss: 0.6158009171485901 \n",
      "     Validation Step: 23 Validation Loss: 0.6121379137039185 \n",
      "     Validation Step: 24 Validation Loss: 0.6150466799736023 \n",
      "     Validation Step: 25 Validation Loss: 0.6142528057098389 \n",
      "     Validation Step: 26 Validation Loss: 0.618344783782959 \n",
      "     Validation Step: 27 Validation Loss: 0.6118927001953125 \n",
      "     Validation Step: 28 Validation Loss: 0.6153219938278198 \n",
      "     Validation Step: 29 Validation Loss: 0.6136632561683655 \n",
      "     Validation Step: 30 Validation Loss: 0.6141354441642761 \n",
      "     Validation Step: 31 Validation Loss: 0.6111879348754883 \n",
      "     Validation Step: 32 Validation Loss: 0.6173008680343628 \n",
      "     Validation Step: 33 Validation Loss: 0.617019534111023 \n",
      "     Validation Step: 34 Validation Loss: 0.6180616021156311 \n",
      "     Validation Step: 35 Validation Loss: 0.6160047054290771 \n",
      "     Validation Step: 36 Validation Loss: 0.6184658408164978 \n",
      "     Validation Step: 37 Validation Loss: 0.6177015900611877 \n",
      "     Validation Step: 38 Validation Loss: 0.6182355880737305 \n",
      "     Validation Step: 39 Validation Loss: 0.616249144077301 \n",
      "     Validation Step: 40 Validation Loss: 0.6148391366004944 \n",
      "     Validation Step: 41 Validation Loss: 0.6136336326599121 \n",
      "     Validation Step: 42 Validation Loss: 0.6116498708724976 \n",
      "     Validation Step: 43 Validation Loss: 0.6156190037727356 \n",
      "     Validation Step: 44 Validation Loss: 0.610528826713562 \n",
      "Epoch: 86\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6188657879829407 \n",
      "     Training Step: 1 Training Loss: 0.6116442084312439 \n",
      "     Training Step: 2 Training Loss: 0.6180270910263062 \n",
      "     Training Step: 3 Training Loss: 0.6114306449890137 \n",
      "     Training Step: 4 Training Loss: 0.6115474700927734 \n",
      "     Training Step: 5 Training Loss: 0.6106826663017273 \n",
      "     Training Step: 6 Training Loss: 0.6152419447898865 \n",
      "     Training Step: 7 Training Loss: 0.6100744009017944 \n",
      "     Training Step: 8 Training Loss: 0.60999596118927 \n",
      "     Training Step: 9 Training Loss: 0.6167760491371155 \n",
      "     Training Step: 10 Training Loss: 0.6171832084655762 \n",
      "     Training Step: 11 Training Loss: 0.6118009090423584 \n",
      "     Training Step: 12 Training Loss: 0.6122279763221741 \n",
      "     Training Step: 13 Training Loss: 0.61187744140625 \n",
      "     Training Step: 14 Training Loss: 0.6122281551361084 \n",
      "     Training Step: 15 Training Loss: 0.6117938756942749 \n",
      "     Training Step: 16 Training Loss: 0.6118296384811401 \n",
      "     Training Step: 17 Training Loss: 0.6166800856590271 \n",
      "     Training Step: 18 Training Loss: 0.6096817255020142 \n",
      "     Training Step: 19 Training Loss: 0.6209941506385803 \n",
      "     Training Step: 20 Training Loss: 0.6181002855300903 \n",
      "     Training Step: 21 Training Loss: 0.6149202585220337 \n",
      "     Training Step: 22 Training Loss: 0.6129182577133179 \n",
      "     Training Step: 23 Training Loss: 0.6168949604034424 \n",
      "     Training Step: 24 Training Loss: 0.6141509413719177 \n",
      "     Training Step: 25 Training Loss: 0.6153326630592346 \n",
      "     Training Step: 26 Training Loss: 0.6142851114273071 \n",
      "     Training Step: 27 Training Loss: 0.6148005127906799 \n",
      "     Training Step: 28 Training Loss: 0.6152779459953308 \n",
      "     Training Step: 29 Training Loss: 0.6157439351081848 \n",
      "     Training Step: 30 Training Loss: 0.6125150322914124 \n",
      "     Training Step: 31 Training Loss: 0.6083254218101501 \n",
      "     Training Step: 32 Training Loss: 0.6097618341445923 \n",
      "     Training Step: 33 Training Loss: 0.6116944551467896 \n",
      "     Training Step: 34 Training Loss: 0.6127530336380005 \n",
      "     Training Step: 35 Training Loss: 0.6100592613220215 \n",
      "     Training Step: 36 Training Loss: 0.6100258231163025 \n",
      "     Training Step: 37 Training Loss: 0.6122791171073914 \n",
      "     Training Step: 38 Training Loss: 0.6125146150588989 \n",
      "     Training Step: 39 Training Loss: 0.6115750670433044 \n",
      "     Training Step: 40 Training Loss: 0.6182034611701965 \n",
      "     Training Step: 41 Training Loss: 0.6184571385383606 \n",
      "     Training Step: 42 Training Loss: 0.6153280138969421 \n",
      "     Training Step: 43 Training Loss: 0.6153233051300049 \n",
      "     Training Step: 44 Training Loss: 0.6164739727973938 \n",
      "     Training Step: 45 Training Loss: 0.6123729944229126 \n",
      "     Training Step: 46 Training Loss: 0.6116474270820618 \n",
      "     Training Step: 47 Training Loss: 0.6148761510848999 \n",
      "     Training Step: 48 Training Loss: 0.6132079362869263 \n",
      "     Training Step: 49 Training Loss: 0.6177955269813538 \n",
      "     Training Step: 50 Training Loss: 0.6176222562789917 \n",
      "     Training Step: 51 Training Loss: 0.615425169467926 \n",
      "     Training Step: 52 Training Loss: 0.6143597960472107 \n",
      "     Training Step: 53 Training Loss: 0.6176865696907043 \n",
      "     Training Step: 54 Training Loss: 0.6122186779975891 \n",
      "     Training Step: 55 Training Loss: 0.616041898727417 \n",
      "     Training Step: 56 Training Loss: 0.6115838885307312 \n",
      "     Training Step: 57 Training Loss: 0.6149675846099854 \n",
      "     Training Step: 58 Training Loss: 0.6115782856941223 \n",
      "     Training Step: 59 Training Loss: 0.6125475168228149 \n",
      "     Training Step: 60 Training Loss: 0.6168009638786316 \n",
      "     Training Step: 61 Training Loss: 0.6177583932876587 \n",
      "     Training Step: 62 Training Loss: 0.6146067380905151 \n",
      "     Training Step: 63 Training Loss: 0.6184112429618835 \n",
      "     Training Step: 64 Training Loss: 0.6146782636642456 \n",
      "     Training Step: 65 Training Loss: 0.6171990036964417 \n",
      "     Training Step: 66 Training Loss: 0.6138178706169128 \n",
      "     Training Step: 67 Training Loss: 0.6147741675376892 \n",
      "     Training Step: 68 Training Loss: 0.6118330359458923 \n",
      "     Training Step: 69 Training Loss: 0.6157853007316589 \n",
      "     Training Step: 70 Training Loss: 0.6164108514785767 \n",
      "     Training Step: 71 Training Loss: 0.6150840520858765 \n",
      "     Training Step: 72 Training Loss: 0.6162424683570862 \n",
      "     Training Step: 73 Training Loss: 0.6159414052963257 \n",
      "     Training Step: 74 Training Loss: 0.6136249303817749 \n",
      "     Training Step: 75 Training Loss: 0.6104029417037964 \n",
      "     Training Step: 76 Training Loss: 0.6106057167053223 \n",
      "     Training Step: 77 Training Loss: 0.614688515663147 \n",
      "     Training Step: 78 Training Loss: 0.6198840141296387 \n",
      "     Training Step: 79 Training Loss: 0.6109145879745483 \n",
      "     Training Step: 80 Training Loss: 0.6114434599876404 \n",
      "     Training Step: 81 Training Loss: 0.6146088242530823 \n",
      "     Training Step: 82 Training Loss: 0.6151039600372314 \n",
      "     Training Step: 83 Training Loss: 0.6177024841308594 \n",
      "     Training Step: 84 Training Loss: 0.6141504049301147 \n",
      "     Training Step: 85 Training Loss: 0.6196377873420715 \n",
      "     Training Step: 86 Training Loss: 0.6155374646186829 \n",
      "     Training Step: 87 Training Loss: 0.6144804358482361 \n",
      "     Training Step: 88 Training Loss: 0.6123911738395691 \n",
      "     Training Step: 89 Training Loss: 0.6129293441772461 \n",
      "     Training Step: 90 Training Loss: 0.6132893562316895 \n",
      "     Training Step: 91 Training Loss: 0.6156748533248901 \n",
      "     Training Step: 92 Training Loss: 0.6169150471687317 \n",
      "     Training Step: 93 Training Loss: 0.6176800727844238 \n",
      "     Training Step: 94 Training Loss: 0.6143391728401184 \n",
      "     Training Step: 95 Training Loss: 0.6151761412620544 \n",
      "     Training Step: 96 Training Loss: 0.6146805286407471 \n",
      "     Training Step: 97 Training Loss: 0.6111698150634766 \n",
      "     Training Step: 98 Training Loss: 0.618004322052002 \n",
      "     Training Step: 99 Training Loss: 0.6134369969367981 \n",
      "     Training Step: 100 Training Loss: 0.6167977452278137 \n",
      "     Training Step: 101 Training Loss: 0.618428111076355 \n",
      "     Training Step: 102 Training Loss: 0.6135911345481873 \n",
      "     Training Step: 103 Training Loss: 0.6154014468193054 \n",
      "     Training Step: 104 Training Loss: 0.6196861267089844 \n",
      "     Training Step: 105 Training Loss: 0.6149317622184753 \n",
      "     Training Step: 106 Training Loss: 0.6135029196739197 \n",
      "     Training Step: 107 Training Loss: 0.6140250563621521 \n",
      "     Training Step: 108 Training Loss: 0.6120389103889465 \n",
      "     Training Step: 109 Training Loss: 0.6094342470169067 \n",
      "     Training Step: 110 Training Loss: 0.615368127822876 \n",
      "     Training Step: 111 Training Loss: 0.6153828501701355 \n",
      "     Training Step: 112 Training Loss: 0.6130695939064026 \n",
      "     Training Step: 113 Training Loss: 0.6117828488349915 \n",
      "     Training Step: 114 Training Loss: 0.6156080961227417 \n",
      "     Training Step: 115 Training Loss: 0.6107155680656433 \n",
      "     Training Step: 116 Training Loss: 0.610680103302002 \n",
      "     Training Step: 117 Training Loss: 0.6140403151512146 \n",
      "     Training Step: 118 Training Loss: 0.6202686429023743 \n",
      "     Training Step: 119 Training Loss: 0.6147536039352417 \n",
      "     Training Step: 120 Training Loss: 0.6134544014930725 \n",
      "     Training Step: 121 Training Loss: 0.6168182492256165 \n",
      "     Training Step: 122 Training Loss: 0.6127005219459534 \n",
      "     Training Step: 123 Training Loss: 0.613055944442749 \n",
      "     Training Step: 124 Training Loss: 0.6128631234169006 \n",
      "     Training Step: 125 Training Loss: 0.6155219078063965 \n",
      "     Training Step: 126 Training Loss: 0.6128007173538208 \n",
      "     Training Step: 127 Training Loss: 0.6156805753707886 \n",
      "     Training Step: 128 Training Loss: 0.6166719198226929 \n",
      "     Training Step: 129 Training Loss: 0.6133172512054443 \n",
      "     Training Step: 130 Training Loss: 0.6128430962562561 \n",
      "     Training Step: 131 Training Loss: 0.6129698753356934 \n",
      "     Training Step: 132 Training Loss: 0.6137427687644958 \n",
      "     Training Step: 133 Training Loss: 0.6154137253761292 \n",
      "     Training Step: 134 Training Loss: 0.612381100654602 \n",
      "     Training Step: 135 Training Loss: 0.6166210174560547 \n",
      "     Training Step: 136 Training Loss: 0.6131426692008972 \n",
      "     Training Step: 137 Training Loss: 0.6151545643806458 \n",
      "     Training Step: 138 Training Loss: 0.6166536211967468 \n",
      "     Training Step: 139 Training Loss: 0.613735020160675 \n",
      "     Training Step: 140 Training Loss: 0.6114804148674011 \n",
      "     Training Step: 141 Training Loss: 0.6121222972869873 \n",
      "     Training Step: 142 Training Loss: 0.614441454410553 \n",
      "     Training Step: 143 Training Loss: 0.6131178736686707 \n",
      "     Training Step: 144 Training Loss: 0.6132805943489075 \n",
      "     Training Step: 145 Training Loss: 0.6139138340950012 \n",
      "     Training Step: 146 Training Loss: 0.6144253611564636 \n",
      "     Training Step: 147 Training Loss: 0.6103925108909607 \n",
      "     Training Step: 148 Training Loss: 0.6143501400947571 \n",
      "     Training Step: 149 Training Loss: 0.6146731972694397 \n",
      "     Training Step: 150 Training Loss: 0.6133064031600952 \n",
      "     Training Step: 151 Training Loss: 0.6105833053588867 \n",
      "     Training Step: 152 Training Loss: 0.612064778804779 \n",
      "     Training Step: 153 Training Loss: 0.6097204089164734 \n",
      "     Training Step: 154 Training Loss: 0.6171728372573853 \n",
      "     Training Step: 155 Training Loss: 0.614224910736084 \n",
      "     Training Step: 156 Training Loss: 0.613891065120697 \n",
      "     Training Step: 157 Training Loss: 0.6104868650436401 \n",
      "     Training Step: 158 Training Loss: 0.6182494759559631 \n",
      "     Training Step: 159 Training Loss: 0.6113937497138977 \n",
      "     Training Step: 160 Training Loss: 0.6144530177116394 \n",
      "     Training Step: 161 Training Loss: 0.6121548414230347 \n",
      "     Training Step: 162 Training Loss: 0.6147076487541199 \n",
      "     Training Step: 163 Training Loss: 0.6166819930076599 \n",
      "     Training Step: 164 Training Loss: 0.6101880073547363 \n",
      "     Training Step: 165 Training Loss: 0.6136495471000671 \n",
      "     Training Step: 166 Training Loss: 0.6162099838256836 \n",
      "     Training Step: 167 Training Loss: 0.6170963048934937 \n",
      "     Training Step: 168 Training Loss: 0.61944180727005 \n",
      "     Training Step: 169 Training Loss: 0.6157608032226562 \n",
      "     Training Step: 170 Training Loss: 0.6147311329841614 \n",
      "     Training Step: 171 Training Loss: 0.6132004261016846 \n",
      "     Training Step: 172 Training Loss: 0.6133031845092773 \n",
      "     Training Step: 173 Training Loss: 0.6167064905166626 \n",
      "     Training Step: 174 Training Loss: 0.6128883957862854 \n",
      "     Training Step: 175 Training Loss: 0.6184440851211548 \n",
      "     Training Step: 176 Training Loss: 0.6106036305427551 \n",
      "     Training Step: 177 Training Loss: 0.6166998744010925 \n",
      "     Training Step: 178 Training Loss: 0.6173704862594604 \n",
      "     Training Step: 179 Training Loss: 0.6111761927604675 \n",
      "     Training Step: 180 Training Loss: 0.6133577823638916 \n",
      "     Training Step: 181 Training Loss: 0.6167153716087341 \n",
      "     Training Step: 182 Training Loss: 0.6157435774803162 \n",
      "     Training Step: 183 Training Loss: 0.6140143871307373 \n",
      "     Training Step: 184 Training Loss: 0.6127651929855347 \n",
      "     Training Step: 185 Training Loss: 0.6134681701660156 \n",
      "     Training Step: 186 Training Loss: 0.6146075129508972 \n",
      "     Training Step: 187 Training Loss: 0.6105747222900391 \n",
      "     Training Step: 188 Training Loss: 0.6115650534629822 \n",
      "     Training Step: 189 Training Loss: 0.6178226470947266 \n",
      "     Training Step: 190 Training Loss: 0.6092274785041809 \n",
      "     Training Step: 191 Training Loss: 0.6118327975273132 \n",
      "     Training Step: 192 Training Loss: 0.6188744306564331 \n",
      "     Training Step: 193 Training Loss: 0.6135114431381226 \n",
      "     Training Step: 194 Training Loss: 0.6154510378837585 \n",
      "     Training Step: 195 Training Loss: 0.6125280261039734 \n",
      "     Training Step: 196 Training Loss: 0.612235426902771 \n",
      "     Training Step: 197 Training Loss: 0.6126394271850586 \n",
      "     Training Step: 198 Training Loss: 0.6155382394790649 \n",
      "     Training Step: 199 Training Loss: 0.6150704026222229 \n",
      "     Training Step: 200 Training Loss: 0.6142047047615051 \n",
      "     Training Step: 201 Training Loss: 0.6162073016166687 \n",
      "     Training Step: 202 Training Loss: 0.6153019666671753 \n",
      "     Training Step: 203 Training Loss: 0.6159992814064026 \n",
      "     Training Step: 204 Training Loss: 0.6116401553153992 \n",
      "     Training Step: 205 Training Loss: 0.6137695908546448 \n",
      "     Training Step: 206 Training Loss: 0.6174842715263367 \n",
      "     Training Step: 207 Training Loss: 0.6141120195388794 \n",
      "     Training Step: 208 Training Loss: 0.6164849996566772 \n",
      "     Training Step: 209 Training Loss: 0.6142448782920837 \n",
      "     Training Step: 210 Training Loss: 0.6167117357254028 \n",
      "     Training Step: 211 Training Loss: 0.6114577651023865 \n",
      "     Training Step: 212 Training Loss: 0.6154758334159851 \n",
      "     Training Step: 213 Training Loss: 0.6171334385871887 \n",
      "     Training Step: 214 Training Loss: 0.6182276606559753 \n",
      "     Training Step: 215 Training Loss: 0.6109034419059753 \n",
      "     Training Step: 216 Training Loss: 0.6163458228111267 \n",
      "     Training Step: 217 Training Loss: 0.6107509732246399 \n",
      "     Training Step: 218 Training Loss: 0.6116213798522949 \n",
      "     Training Step: 219 Training Loss: 0.615831732749939 \n",
      "     Training Step: 220 Training Loss: 0.6112016439437866 \n",
      "     Training Step: 221 Training Loss: 0.6101366281509399 \n",
      "     Training Step: 222 Training Loss: 0.6122878789901733 \n",
      "     Training Step: 223 Training Loss: 0.6147140264511108 \n",
      "     Training Step: 224 Training Loss: 0.6146482825279236 \n",
      "     Training Step: 225 Training Loss: 0.618645429611206 \n",
      "     Training Step: 226 Training Loss: 0.6114109754562378 \n",
      "     Training Step: 227 Training Loss: 0.6140874624252319 \n",
      "     Training Step: 228 Training Loss: 0.6104733347892761 \n",
      "     Training Step: 229 Training Loss: 0.6186094284057617 \n",
      "     Training Step: 230 Training Loss: 0.6124610304832458 \n",
      "     Training Step: 231 Training Loss: 0.6094500422477722 \n",
      "     Training Step: 232 Training Loss: 0.6132011413574219 \n",
      "     Training Step: 233 Training Loss: 0.6139203310012817 \n",
      "     Training Step: 234 Training Loss: 0.6144994497299194 \n",
      "     Training Step: 235 Training Loss: 0.6121385097503662 \n",
      "     Training Step: 236 Training Loss: 0.6161178946495056 \n",
      "     Training Step: 237 Training Loss: 0.6143760681152344 \n",
      "     Training Step: 238 Training Loss: 0.6146536469459534 \n",
      "     Training Step: 239 Training Loss: 0.6136378645896912 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6148359775543213 \n",
      "     Validation Step: 1 Validation Loss: 0.6105142831802368 \n",
      "     Validation Step: 2 Validation Loss: 0.610141932964325 \n",
      "     Validation Step: 3 Validation Loss: 0.6184766888618469 \n",
      "     Validation Step: 4 Validation Loss: 0.6115553379058838 \n",
      "     Validation Step: 5 Validation Loss: 0.6155754923820496 \n",
      "     Validation Step: 6 Validation Loss: 0.6177016496658325 \n",
      "     Validation Step: 7 Validation Loss: 0.618066668510437 \n",
      "     Validation Step: 8 Validation Loss: 0.6133049130439758 \n",
      "     Validation Step: 9 Validation Loss: 0.6128307580947876 \n",
      "     Validation Step: 10 Validation Loss: 0.610166609287262 \n",
      "     Validation Step: 11 Validation Loss: 0.6136351227760315 \n",
      "     Validation Step: 12 Validation Loss: 0.6156157851219177 \n",
      "     Validation Step: 13 Validation Loss: 0.6182400584220886 \n",
      "     Validation Step: 14 Validation Loss: 0.6141158938407898 \n",
      "     Validation Step: 15 Validation Loss: 0.6075301766395569 \n",
      "     Validation Step: 16 Validation Loss: 0.6183480620384216 \n",
      "     Validation Step: 17 Validation Loss: 0.616244912147522 \n",
      "     Validation Step: 18 Validation Loss: 0.6121286749839783 \n",
      "     Validation Step: 19 Validation Loss: 0.6170242428779602 \n",
      "     Validation Step: 20 Validation Loss: 0.6176002025604248 \n",
      "     Validation Step: 21 Validation Loss: 0.6152275204658508 \n",
      "     Validation Step: 22 Validation Loss: 0.6142551302909851 \n",
      "     Validation Step: 23 Validation Loss: 0.614895761013031 \n",
      "     Validation Step: 24 Validation Loss: 0.6141284704208374 \n",
      "     Validation Step: 25 Validation Loss: 0.6142005324363708 \n",
      "     Validation Step: 26 Validation Loss: 0.6185011863708496 \n",
      "     Validation Step: 27 Validation Loss: 0.6129845380783081 \n",
      "     Validation Step: 28 Validation Loss: 0.615318775177002 \n",
      "     Validation Step: 29 Validation Loss: 0.616000771522522 \n",
      "     Validation Step: 30 Validation Loss: 0.6116408109664917 \n",
      "     Validation Step: 31 Validation Loss: 0.6118820905685425 \n",
      "     Validation Step: 32 Validation Loss: 0.6136671304702759 \n",
      "     Validation Step: 33 Validation Loss: 0.6111646294593811 \n",
      "     Validation Step: 34 Validation Loss: 0.6146381497383118 \n",
      "     Validation Step: 35 Validation Loss: 0.6150407791137695 \n",
      "     Validation Step: 36 Validation Loss: 0.6173072457313538 \n",
      "     Validation Step: 37 Validation Loss: 0.6136521697044373 \n",
      "     Validation Step: 38 Validation Loss: 0.6104825735092163 \n",
      "     Validation Step: 39 Validation Loss: 0.611179769039154 \n",
      "     Validation Step: 40 Validation Loss: 0.6101127862930298 \n",
      "     Validation Step: 41 Validation Loss: 0.614534854888916 \n",
      "     Validation Step: 42 Validation Loss: 0.6106340885162354 \n",
      "     Validation Step: 43 Validation Loss: 0.6157990097999573 \n",
      "     Validation Step: 44 Validation Loss: 0.6145687699317932 \n",
      "Epoch: 87\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6146000027656555 \n",
      "     Training Step: 1 Training Loss: 0.614107608795166 \n",
      "     Training Step: 2 Training Loss: 0.6137427091598511 \n",
      "     Training Step: 3 Training Loss: 0.6155346632003784 \n",
      "     Training Step: 4 Training Loss: 0.6164079308509827 \n",
      "     Training Step: 5 Training Loss: 0.6129247546195984 \n",
      "     Training Step: 6 Training Loss: 0.6129735708236694 \n",
      "     Training Step: 7 Training Loss: 0.6176879405975342 \n",
      "     Training Step: 8 Training Loss: 0.6112110614776611 \n",
      "     Training Step: 9 Training Loss: 0.6139294505119324 \n",
      "     Training Step: 10 Training Loss: 0.6154744625091553 \n",
      "     Training Step: 11 Training Loss: 0.6142081022262573 \n",
      "     Training Step: 12 Training Loss: 0.6107448935508728 \n",
      "     Training Step: 13 Training Loss: 0.6161153316497803 \n",
      "     Training Step: 14 Training Loss: 0.614479124546051 \n",
      "     Training Step: 15 Training Loss: 0.6123762130737305 \n",
      "     Training Step: 16 Training Loss: 0.6174980401992798 \n",
      "     Training Step: 17 Training Loss: 0.6093994975090027 \n",
      "     Training Step: 18 Training Loss: 0.6181126236915588 \n",
      "     Training Step: 19 Training Loss: 0.6166773438453674 \n",
      "     Training Step: 20 Training Loss: 0.6126422882080078 \n",
      "     Training Step: 21 Training Loss: 0.6144238114356995 \n",
      "     Training Step: 22 Training Loss: 0.6101850867271423 \n",
      "     Training Step: 23 Training Loss: 0.6151083111763 \n",
      "     Training Step: 24 Training Loss: 0.6082704663276672 \n",
      "     Training Step: 25 Training Loss: 0.6105926632881165 \n",
      "     Training Step: 26 Training Loss: 0.615540623664856 \n",
      "     Training Step: 27 Training Loss: 0.6134989857673645 \n",
      "     Training Step: 28 Training Loss: 0.6120085716247559 \n",
      "     Training Step: 29 Training Loss: 0.6133094429969788 \n",
      "     Training Step: 30 Training Loss: 0.6116027235984802 \n",
      "     Training Step: 31 Training Loss: 0.6174267530441284 \n",
      "     Training Step: 32 Training Loss: 0.6168338656425476 \n",
      "     Training Step: 33 Training Loss: 0.618442714214325 \n",
      "     Training Step: 34 Training Loss: 0.6153101921081543 \n",
      "     Training Step: 35 Training Loss: 0.6149288415908813 \n",
      "     Training Step: 36 Training Loss: 0.616719126701355 \n",
      "     Training Step: 37 Training Loss: 0.6135210990905762 \n",
      "     Training Step: 38 Training Loss: 0.6157568693161011 \n",
      "     Training Step: 39 Training Loss: 0.614730954170227 \n",
      "     Training Step: 40 Training Loss: 0.6109365820884705 \n",
      "     Training Step: 41 Training Loss: 0.610544741153717 \n",
      "     Training Step: 42 Training Loss: 0.6171319484710693 \n",
      "     Training Step: 43 Training Loss: 0.615242063999176 \n",
      "     Training Step: 44 Training Loss: 0.6111766695976257 \n",
      "     Training Step: 45 Training Loss: 0.6162095665931702 \n",
      "     Training Step: 46 Training Loss: 0.6134135127067566 \n",
      "     Training Step: 47 Training Loss: 0.6162015199661255 \n",
      "     Training Step: 48 Training Loss: 0.6168105602264404 \n",
      "     Training Step: 49 Training Loss: 0.6146658062934875 \n",
      "     Training Step: 50 Training Loss: 0.6104846000671387 \n",
      "     Training Step: 51 Training Loss: 0.6106578707695007 \n",
      "     Training Step: 52 Training Loss: 0.6209688186645508 \n",
      "     Training Step: 53 Training Loss: 0.6188888549804688 \n",
      "     Training Step: 54 Training Loss: 0.6146162152290344 \n",
      "     Training Step: 55 Training Loss: 0.6167689561843872 \n",
      "     Training Step: 56 Training Loss: 0.6142148971557617 \n",
      "     Training Step: 57 Training Loss: 0.6136186718940735 \n",
      "     Training Step: 58 Training Loss: 0.6182146072387695 \n",
      "     Training Step: 59 Training Loss: 0.6138243675231934 \n",
      "     Training Step: 60 Training Loss: 0.6159924864768982 \n",
      "     Training Step: 61 Training Loss: 0.6122578978538513 \n",
      "     Training Step: 62 Training Loss: 0.6167171001434326 \n",
      "     Training Step: 63 Training Loss: 0.6115575432777405 \n",
      "     Training Step: 64 Training Loss: 0.6149556040763855 \n",
      "     Training Step: 65 Training Loss: 0.6144580841064453 \n",
      "     Training Step: 66 Training Loss: 0.6131498217582703 \n",
      "     Training Step: 67 Training Loss: 0.6125359535217285 \n",
      "     Training Step: 68 Training Loss: 0.6123794317245483 \n",
      "     Training Step: 69 Training Loss: 0.6137625575065613 \n",
      "     Training Step: 70 Training Loss: 0.6140151023864746 \n",
      "     Training Step: 71 Training Loss: 0.6111382842063904 \n",
      "     Training Step: 72 Training Loss: 0.6128396391868591 \n",
      "     Training Step: 73 Training Loss: 0.6156930923461914 \n",
      "     Training Step: 74 Training Loss: 0.6158470511436462 \n",
      "     Training Step: 75 Training Loss: 0.6140443086624146 \n",
      "     Training Step: 76 Training Loss: 0.6183603405952454 \n",
      "     Training Step: 77 Training Loss: 0.6164344549179077 \n",
      "     Training Step: 78 Training Loss: 0.6146743297576904 \n",
      "     Training Step: 79 Training Loss: 0.6101422309875488 \n",
      "     Training Step: 80 Training Loss: 0.6128643155097961 \n",
      "     Training Step: 81 Training Loss: 0.6153383255004883 \n",
      "     Training Step: 82 Training Loss: 0.6117907166481018 \n",
      "     Training Step: 83 Training Loss: 0.6171584725379944 \n",
      "     Training Step: 84 Training Loss: 0.6146507263183594 \n",
      "     Training Step: 85 Training Loss: 0.6136508584022522 \n",
      "     Training Step: 86 Training Loss: 0.6106060147285461 \n",
      "     Training Step: 87 Training Loss: 0.6105870604515076 \n",
      "     Training Step: 88 Training Loss: 0.6118048429489136 \n",
      "     Training Step: 89 Training Loss: 0.6153721213340759 \n",
      "     Training Step: 90 Training Loss: 0.6139082908630371 \n",
      "     Training Step: 91 Training Loss: 0.6132044196128845 \n",
      "     Training Step: 92 Training Loss: 0.6122295260429382 \n",
      "     Training Step: 93 Training Loss: 0.6132751703262329 \n",
      "     Training Step: 94 Training Loss: 0.6100249290466309 \n",
      "     Training Step: 95 Training Loss: 0.6131104826927185 \n",
      "     Training Step: 96 Training Loss: 0.6178018450737 \n",
      "     Training Step: 97 Training Loss: 0.6159774661064148 \n",
      "     Training Step: 98 Training Loss: 0.614719808101654 \n",
      "     Training Step: 99 Training Loss: 0.6114083528518677 \n",
      "     Training Step: 100 Training Loss: 0.6165122389793396 \n",
      "     Training Step: 101 Training Loss: 0.6132944822311401 \n",
      "     Training Step: 102 Training Loss: 0.6157025694847107 \n",
      "     Training Step: 103 Training Loss: 0.6122426390647888 \n",
      "     Training Step: 104 Training Loss: 0.609738826751709 \n",
      "     Training Step: 105 Training Loss: 0.6133602261543274 \n",
      "     Training Step: 106 Training Loss: 0.6133041977882385 \n",
      "     Training Step: 107 Training Loss: 0.6130676865577698 \n",
      "     Training Step: 108 Training Loss: 0.6132051348686218 \n",
      "     Training Step: 109 Training Loss: 0.6135908961296082 \n",
      "     Training Step: 110 Training Loss: 0.6176398992538452 \n",
      "     Training Step: 111 Training Loss: 0.6125239729881287 \n",
      "     Training Step: 112 Training Loss: 0.6134625673294067 \n",
      "     Training Step: 113 Training Loss: 0.6114486455917358 \n",
      "     Training Step: 114 Training Loss: 0.6166513562202454 \n",
      "     Training Step: 115 Training Loss: 0.6124618649482727 \n",
      "     Training Step: 116 Training Loss: 0.6157888770103455 \n",
      "     Training Step: 117 Training Loss: 0.6118471622467041 \n",
      "     Training Step: 118 Training Loss: 0.6180455684661865 \n",
      "     Training Step: 119 Training Loss: 0.6129093766212463 \n",
      "     Training Step: 120 Training Loss: 0.6146764159202576 \n",
      "     Training Step: 121 Training Loss: 0.6121528744697571 \n",
      "     Training Step: 122 Training Loss: 0.6167078614234924 \n",
      "     Training Step: 123 Training Loss: 0.6153785586357117 \n",
      "     Training Step: 124 Training Loss: 0.612805187702179 \n",
      "     Training Step: 125 Training Loss: 0.611575722694397 \n",
      "     Training Step: 126 Training Loss: 0.6184544563293457 \n",
      "     Training Step: 127 Training Loss: 0.6114916801452637 \n",
      "     Training Step: 128 Training Loss: 0.6176996827125549 \n",
      "     Training Step: 129 Training Loss: 0.6104002594947815 \n",
      "     Training Step: 130 Training Loss: 0.6104174256324768 \n",
      "     Training Step: 131 Training Loss: 0.6154033541679382 \n",
      "     Training Step: 132 Training Loss: 0.6156013607978821 \n",
      "     Training Step: 133 Training Loss: 0.6136345267295837 \n",
      "     Training Step: 134 Training Loss: 0.6169312596321106 \n",
      "     Training Step: 135 Training Loss: 0.6141499280929565 \n",
      "     Training Step: 136 Training Loss: 0.61140376329422 \n",
      "     Training Step: 137 Training Loss: 0.6115905046463013 \n",
      "     Training Step: 138 Training Loss: 0.6171990633010864 \n",
      "     Training Step: 139 Training Loss: 0.6166224479675293 \n",
      "     Training Step: 140 Training Loss: 0.6143427491188049 \n",
      "     Training Step: 141 Training Loss: 0.6194432973861694 \n",
      "     Training Step: 142 Training Loss: 0.6154412627220154 \n",
      "     Training Step: 143 Training Loss: 0.6150873899459839 \n",
      "     Training Step: 144 Training Loss: 0.61165851354599 \n",
      "     Training Step: 145 Training Loss: 0.6142826676368713 \n",
      "     Training Step: 146 Training Loss: 0.6101005673408508 \n",
      "     Training Step: 147 Training Loss: 0.6166425347328186 \n",
      "     Training Step: 148 Training Loss: 0.6148697137832642 \n",
      "     Training Step: 149 Training Loss: 0.617087185382843 \n",
      "     Training Step: 150 Training Loss: 0.6120771765708923 \n",
      "     Training Step: 151 Training Loss: 0.6182248592376709 \n",
      "     Training Step: 152 Training Loss: 0.6150673031806946 \n",
      "     Training Step: 153 Training Loss: 0.6118344664573669 \n",
      "     Training Step: 154 Training Loss: 0.6100715398788452 \n",
      "     Training Step: 155 Training Loss: 0.6162471771240234 \n",
      "     Training Step: 156 Training Loss: 0.6154140830039978 \n",
      "     Training Step: 157 Training Loss: 0.6144987344741821 \n",
      "     Training Step: 158 Training Loss: 0.6160359382629395 \n",
      "     Training Step: 159 Training Loss: 0.6143486499786377 \n",
      "     Training Step: 160 Training Loss: 0.6132802367210388 \n",
      "     Training Step: 161 Training Loss: 0.6105737090110779 \n",
      "     Training Step: 162 Training Loss: 0.611636757850647 \n",
      "     Training Step: 163 Training Loss: 0.6180816888809204 \n",
      "     Training Step: 164 Training Loss: 0.6143748760223389 \n",
      "     Training Step: 165 Training Loss: 0.6199005246162415 \n",
      "     Training Step: 166 Training Loss: 0.6127038598060608 \n",
      "     Training Step: 167 Training Loss: 0.6099981069564819 \n",
      "     Training Step: 168 Training Loss: 0.609735906124115 \n",
      "     Training Step: 169 Training Loss: 0.6138845086097717 \n",
      "     Training Step: 170 Training Loss: 0.6168031692504883 \n",
      "     Training Step: 171 Training Loss: 0.6092326641082764 \n",
      "     Training Step: 172 Training Loss: 0.612291693687439 \n",
      "     Training Step: 173 Training Loss: 0.6124886870384216 \n",
      "     Training Step: 174 Training Loss: 0.6118760704994202 \n",
      "     Training Step: 175 Training Loss: 0.6118327379226685 \n",
      "     Training Step: 176 Training Loss: 0.6197505593299866 \n",
      "     Training Step: 177 Training Loss: 0.6144413352012634 \n",
      "     Training Step: 178 Training Loss: 0.6152782440185547 \n",
      "     Training Step: 179 Training Loss: 0.6152905821800232 \n",
      "     Training Step: 180 Training Loss: 0.6116182208061218 \n",
      "     Training Step: 181 Training Loss: 0.6142460703849792 \n",
      "     Training Step: 182 Training Loss: 0.6196410655975342 \n",
      "     Training Step: 183 Training Loss: 0.6176929473876953 \n",
      "     Training Step: 184 Training Loss: 0.6128767132759094 \n",
      "     Training Step: 185 Training Loss: 0.6146062016487122 \n",
      "     Training Step: 186 Training Loss: 0.6127719879150391 \n",
      "     Training Step: 187 Training Loss: 0.6134735345840454 \n",
      "     Training Step: 188 Training Loss: 0.6157429814338684 \n",
      "     Training Step: 189 Training Loss: 0.6115477681159973 \n",
      "     Training Step: 190 Training Loss: 0.6166591048240662 \n",
      "     Training Step: 191 Training Loss: 0.6180049180984497 \n",
      "     Training Step: 192 Training Loss: 0.616343080997467 \n",
      "     Training Step: 193 Training Loss: 0.6167165040969849 \n",
      "     Training Step: 194 Training Loss: 0.6124054789543152 \n",
      "     Training Step: 195 Training Loss: 0.6114617586135864 \n",
      "     Training Step: 196 Training Loss: 0.6121524572372437 \n",
      "     Training Step: 197 Training Loss: 0.6141573786735535 \n",
      "     Training Step: 198 Training Loss: 0.6146421432495117 \n",
      "     Training Step: 199 Training Loss: 0.615743100643158 \n",
      "     Training Step: 200 Training Loss: 0.6094677448272705 \n",
      "     Training Step: 201 Training Loss: 0.6155198812484741 \n",
      "     Training Step: 202 Training Loss: 0.6202380061149597 \n",
      "     Training Step: 203 Training Loss: 0.6188593506813049 \n",
      "     Training Step: 204 Training Loss: 0.6152905821800232 \n",
      "     Training Step: 205 Training Loss: 0.6140217781066895 \n",
      "     Training Step: 206 Training Loss: 0.6118072271347046 \n",
      "     Training Step: 207 Training Loss: 0.6151520013809204 \n",
      "     Training Step: 208 Training Loss: 0.6131923198699951 \n",
      "     Training Step: 209 Training Loss: 0.6147729158401489 \n",
      "     Training Step: 210 Training Loss: 0.6147454977035522 \n",
      "     Training Step: 211 Training Loss: 0.6108959913253784 \n",
      "     Training Step: 212 Training Loss: 0.6147032976150513 \n",
      "     Training Step: 213 Training Loss: 0.6125326752662659 \n",
      "     Training Step: 214 Training Loss: 0.6154223680496216 \n",
      "     Training Step: 215 Training Loss: 0.6121262311935425 \n",
      "     Training Step: 216 Training Loss: 0.6185957789421082 \n",
      "     Training Step: 217 Training Loss: 0.6137370467185974 \n",
      "     Training Step: 218 Training Loss: 0.6130550503730774 \n",
      "     Training Step: 219 Training Loss: 0.6116868257522583 \n",
      "     Training Step: 220 Training Loss: 0.6147981882095337 \n",
      "     Training Step: 221 Training Loss: 0.6115236878395081 \n",
      "     Training Step: 222 Training Loss: 0.6184634566307068 \n",
      "     Training Step: 223 Training Loss: 0.6178248524665833 \n",
      "     Training Step: 224 Training Loss: 0.610683262348175 \n",
      "     Training Step: 225 Training Loss: 0.6097052693367004 \n",
      "     Training Step: 226 Training Loss: 0.6168996691703796 \n",
      "     Training Step: 227 Training Loss: 0.6122826933860779 \n",
      "     Training Step: 228 Training Loss: 0.6107180714607239 \n",
      "     Training Step: 229 Training Loss: 0.6140779256820679 \n",
      "     Training Step: 230 Training Loss: 0.6151776909828186 \n",
      "     Training Step: 231 Training Loss: 0.6127519607543945 \n",
      "     Training Step: 232 Training Loss: 0.6114062666893005 \n",
      "     Training Step: 233 Training Loss: 0.6143444776535034 \n",
      "     Training Step: 234 Training Loss: 0.614922046661377 \n",
      "     Training Step: 235 Training Loss: 0.6146900057792664 \n",
      "     Training Step: 236 Training Loss: 0.6186209321022034 \n",
      "     Training Step: 237 Training Loss: 0.6178063154220581 \n",
      "     Training Step: 238 Training Loss: 0.6121676564216614 \n",
      "     Training Step: 239 Training Loss: 0.6171389222145081 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6129929423332214 \n",
      "     Validation Step: 1 Validation Loss: 0.6111909747123718 \n",
      "     Validation Step: 2 Validation Loss: 0.6145703196525574 \n",
      "     Validation Step: 3 Validation Loss: 0.6118951439857483 \n",
      "     Validation Step: 4 Validation Loss: 0.6142573952674866 \n",
      "     Validation Step: 5 Validation Loss: 0.6106518507003784 \n",
      "     Validation Step: 6 Validation Loss: 0.6141191720962524 \n",
      "     Validation Step: 7 Validation Loss: 0.6101675629615784 \n",
      "     Validation Step: 8 Validation Loss: 0.6150385737419128 \n",
      "     Validation Step: 9 Validation Loss: 0.6133109927177429 \n",
      "     Validation Step: 10 Validation Loss: 0.6111790537834167 \n",
      "     Validation Step: 11 Validation Loss: 0.6136545538902283 \n",
      "     Validation Step: 12 Validation Loss: 0.6148379445075989 \n",
      "     Validation Step: 13 Validation Loss: 0.6141334772109985 \n",
      "     Validation Step: 14 Validation Loss: 0.614535927772522 \n",
      "     Validation Step: 15 Validation Loss: 0.6121423840522766 \n",
      "     Validation Step: 16 Validation Loss: 0.6170129179954529 \n",
      "     Validation Step: 17 Validation Loss: 0.6136724352836609 \n",
      "     Validation Step: 18 Validation Loss: 0.6148961186408997 \n",
      "     Validation Step: 19 Validation Loss: 0.6180552244186401 \n",
      "     Validation Step: 20 Validation Loss: 0.6128398180007935 \n",
      "     Validation Step: 21 Validation Loss: 0.6115709543228149 \n",
      "     Validation Step: 22 Validation Loss: 0.618334949016571 \n",
      "     Validation Step: 23 Validation Loss: 0.6162437796592712 \n",
      "     Validation Step: 24 Validation Loss: 0.6153176426887512 \n",
      "     Validation Step: 25 Validation Loss: 0.6156145930290222 \n",
      "     Validation Step: 26 Validation Loss: 0.6075596809387207 \n",
      "     Validation Step: 27 Validation Loss: 0.6184623837471008 \n",
      "     Validation Step: 28 Validation Loss: 0.6105320453643799 \n",
      "     Validation Step: 29 Validation Loss: 0.6176921129226685 \n",
      "     Validation Step: 30 Validation Loss: 0.6172981858253479 \n",
      "     Validation Step: 31 Validation Loss: 0.615793764591217 \n",
      "     Validation Step: 32 Validation Loss: 0.6116548180580139 \n",
      "     Validation Step: 33 Validation Loss: 0.6159924268722534 \n",
      "     Validation Step: 34 Validation Loss: 0.6142047047615051 \n",
      "     Validation Step: 35 Validation Loss: 0.6175916790962219 \n",
      "     Validation Step: 36 Validation Loss: 0.6155704855918884 \n",
      "     Validation Step: 37 Validation Loss: 0.6104968190193176 \n",
      "     Validation Step: 38 Validation Loss: 0.6184854507446289 \n",
      "     Validation Step: 39 Validation Loss: 0.6146411299705505 \n",
      "     Validation Step: 40 Validation Loss: 0.6136400103569031 \n",
      "     Validation Step: 41 Validation Loss: 0.610136091709137 \n",
      "     Validation Step: 42 Validation Loss: 0.6101834774017334 \n",
      "     Validation Step: 43 Validation Loss: 0.615227222442627 \n",
      "     Validation Step: 44 Validation Loss: 0.6182251572608948 \n",
      "Epoch: 88\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6169200539588928 \n",
      "     Training Step: 1 Training Loss: 0.6125330328941345 \n",
      "     Training Step: 2 Training Loss: 0.6132926940917969 \n",
      "     Training Step: 3 Training Loss: 0.6201947331428528 \n",
      "     Training Step: 4 Training Loss: 0.6156716346740723 \n",
      "     Training Step: 5 Training Loss: 0.6107068657875061 \n",
      "     Training Step: 6 Training Loss: 0.6114794015884399 \n",
      "     Training Step: 7 Training Loss: 0.6140304207801819 \n",
      "     Training Step: 8 Training Loss: 0.6159393787384033 \n",
      "     Training Step: 9 Training Loss: 0.6171545386314392 \n",
      "     Training Step: 10 Training Loss: 0.619429886341095 \n",
      "     Training Step: 11 Training Loss: 0.6143702268600464 \n",
      "     Training Step: 12 Training Loss: 0.6153762936592102 \n",
      "     Training Step: 13 Training Loss: 0.6128824353218079 \n",
      "     Training Step: 14 Training Loss: 0.6094326376914978 \n",
      "     Training Step: 15 Training Loss: 0.6137658953666687 \n",
      "     Training Step: 16 Training Loss: 0.6181023716926575 \n",
      "     Training Step: 17 Training Loss: 0.6185981035232544 \n",
      "     Training Step: 18 Training Loss: 0.6147983074188232 \n",
      "     Training Step: 19 Training Loss: 0.6145013570785522 \n",
      "     Training Step: 20 Training Loss: 0.6177905797958374 \n",
      "     Training Step: 21 Training Loss: 0.6107499003410339 \n",
      "     Training Step: 22 Training Loss: 0.6147289276123047 \n",
      "     Training Step: 23 Training Loss: 0.6138191223144531 \n",
      "     Training Step: 24 Training Loss: 0.6142830848693848 \n",
      "     Training Step: 25 Training Loss: 0.6143403053283691 \n",
      "     Training Step: 26 Training Loss: 0.6152933239936829 \n",
      "     Training Step: 27 Training Loss: 0.6154232025146484 \n",
      "     Training Step: 28 Training Loss: 0.6114326119422913 \n",
      "     Training Step: 29 Training Loss: 0.6177988052368164 \n",
      "     Training Step: 30 Training Loss: 0.6121625304222107 \n",
      "     Training Step: 31 Training Loss: 0.610011637210846 \n",
      "     Training Step: 32 Training Loss: 0.6182259917259216 \n",
      "     Training Step: 33 Training Loss: 0.6120254397392273 \n",
      "     Training Step: 34 Training Loss: 0.6132835149765015 \n",
      "     Training Step: 35 Training Loss: 0.6177141070365906 \n",
      "     Training Step: 36 Training Loss: 0.6105751991271973 \n",
      "     Training Step: 37 Training Loss: 0.6131368279457092 \n",
      "     Training Step: 38 Training Loss: 0.6138867139816284 \n",
      "     Training Step: 39 Training Loss: 0.6157456636428833 \n",
      "     Training Step: 40 Training Loss: 0.6104751229286194 \n",
      "     Training Step: 41 Training Loss: 0.6117807030677795 \n",
      "     Training Step: 42 Training Loss: 0.6144556403160095 \n",
      "     Training Step: 43 Training Loss: 0.6120614409446716 \n",
      "     Training Step: 44 Training Loss: 0.6135994791984558 \n",
      "     Training Step: 45 Training Loss: 0.6180785894393921 \n",
      "     Training Step: 46 Training Loss: 0.6132040023803711 \n",
      "     Training Step: 47 Training Loss: 0.6168278455734253 \n",
      "     Training Step: 48 Training Loss: 0.6103642582893372 \n",
      "     Training Step: 49 Training Loss: 0.614872395992279 \n",
      "     Training Step: 50 Training Loss: 0.6154199242591858 \n",
      "     Training Step: 51 Training Loss: 0.6118270754814148 \n",
      "     Training Step: 52 Training Loss: 0.6105871200561523 \n",
      "     Training Step: 53 Training Loss: 0.6158345937728882 \n",
      "     Training Step: 54 Training Loss: 0.6141528487205505 \n",
      "     Training Step: 55 Training Loss: 0.6097381114959717 \n",
      "     Training Step: 56 Training Loss: 0.6105015873908997 \n",
      "     Training Step: 57 Training Loss: 0.6144221425056458 \n",
      "     Training Step: 58 Training Loss: 0.618091344833374 \n",
      "     Training Step: 59 Training Loss: 0.6160088777542114 \n",
      "     Training Step: 60 Training Loss: 0.6209606528282166 \n",
      "     Training Step: 61 Training Loss: 0.6167659759521484 \n",
      "     Training Step: 62 Training Loss: 0.614347517490387 \n",
      "     Training Step: 63 Training Loss: 0.615265965461731 \n",
      "     Training Step: 64 Training Loss: 0.6146029233932495 \n",
      "     Training Step: 65 Training Loss: 0.6166650056838989 \n",
      "     Training Step: 66 Training Loss: 0.6134433150291443 \n",
      "     Training Step: 67 Training Loss: 0.6146964430809021 \n",
      "     Training Step: 68 Training Loss: 0.6166374087333679 \n",
      "     Training Step: 69 Training Loss: 0.6179972290992737 \n",
      "     Training Step: 70 Training Loss: 0.6119207143783569 \n",
      "     Training Step: 71 Training Loss: 0.6114493608474731 \n",
      "     Training Step: 72 Training Loss: 0.6107208132743835 \n",
      "     Training Step: 73 Training Loss: 0.6164824366569519 \n",
      "     Training Step: 74 Training Loss: 0.6135046482086182 \n",
      "     Training Step: 75 Training Loss: 0.6100931763648987 \n",
      "     Training Step: 76 Training Loss: 0.6167209148406982 \n",
      "     Training Step: 77 Training Loss: 0.6123768091201782 \n",
      "     Training Step: 78 Training Loss: 0.61229407787323 \n",
      "     Training Step: 79 Training Loss: 0.6153848171234131 \n",
      "     Training Step: 80 Training Loss: 0.6127966642379761 \n",
      "     Training Step: 81 Training Loss: 0.6144940257072449 \n",
      "     Training Step: 82 Training Loss: 0.6109017729759216 \n",
      "     Training Step: 83 Training Loss: 0.6107050776481628 \n",
      "     Training Step: 84 Training Loss: 0.6139300465583801 \n",
      "     Training Step: 85 Training Loss: 0.6136568188667297 \n",
      "     Training Step: 86 Training Loss: 0.6176897883415222 \n",
      "     Training Step: 87 Training Loss: 0.6147164106369019 \n",
      "     Training Step: 88 Training Loss: 0.61212158203125 \n",
      "     Training Step: 89 Training Loss: 0.6188938617706299 \n",
      "     Training Step: 90 Training Loss: 0.6111450791358948 \n",
      "     Training Step: 91 Training Loss: 0.6117900013923645 \n",
      "     Training Step: 92 Training Loss: 0.6094713807106018 \n",
      "     Training Step: 93 Training Loss: 0.616204559803009 \n",
      "     Training Step: 94 Training Loss: 0.6116519570350647 \n",
      "     Training Step: 95 Training Loss: 0.6112054586410522 \n",
      "     Training Step: 96 Training Loss: 0.6140340566635132 \n",
      "     Training Step: 97 Training Loss: 0.6161186099052429 \n",
      "     Training Step: 98 Training Loss: 0.6126411557197571 \n",
      "     Training Step: 99 Training Loss: 0.6175040006637573 \n",
      "     Training Step: 100 Training Loss: 0.6137460470199585 \n",
      "     Training Step: 101 Training Loss: 0.6157631278038025 \n",
      "     Training Step: 102 Training Loss: 0.6116337776184082 \n",
      "     Training Step: 103 Training Loss: 0.6105936765670776 \n",
      "     Training Step: 104 Training Loss: 0.6169049739837646 \n",
      "     Training Step: 105 Training Loss: 0.6182424426078796 \n",
      "     Training Step: 106 Training Loss: 0.6167380809783936 \n",
      "     Training Step: 107 Training Loss: 0.6171501278877258 \n",
      "     Training Step: 108 Training Loss: 0.6150684356689453 \n",
      "     Training Step: 109 Training Loss: 0.6198801398277283 \n",
      "     Training Step: 110 Training Loss: 0.6115781664848328 \n",
      "     Training Step: 111 Training Loss: 0.6141146421432495 \n",
      "     Training Step: 112 Training Loss: 0.6129191517829895 \n",
      "     Training Step: 113 Training Loss: 0.6125583648681641 \n",
      "     Training Step: 114 Training Loss: 0.6140223741531372 \n",
      "     Training Step: 115 Training Loss: 0.617750346660614 \n",
      "     Training Step: 116 Training Loss: 0.61467444896698 \n",
      "     Training Step: 117 Training Loss: 0.6122633218765259 \n",
      "     Training Step: 118 Training Loss: 0.6142253875732422 \n",
      "     Training Step: 119 Training Loss: 0.6183956861495972 \n",
      "     Training Step: 120 Training Loss: 0.6125363111495972 \n",
      "     Training Step: 121 Training Loss: 0.6152442097663879 \n",
      "     Training Step: 122 Training Loss: 0.6135194301605225 \n",
      "     Training Step: 123 Training Loss: 0.6166752576828003 \n",
      "     Training Step: 124 Training Loss: 0.6154425144195557 \n",
      "     Training Step: 125 Training Loss: 0.6173868775367737 \n",
      "     Training Step: 126 Training Loss: 0.6156001687049866 \n",
      "     Training Step: 127 Training Loss: 0.6184388995170593 \n",
      "     Training Step: 128 Training Loss: 0.6128459572792053 \n",
      "     Training Step: 129 Training Loss: 0.6097525358200073 \n",
      "     Training Step: 130 Training Loss: 0.6176808476448059 \n",
      "     Training Step: 131 Training Loss: 0.6139217019081116 \n",
      "     Training Step: 132 Training Loss: 0.6109054684638977 \n",
      "     Training Step: 133 Training Loss: 0.6127570271492004 \n",
      "     Training Step: 134 Training Loss: 0.61238694190979 \n",
      "     Training Step: 135 Training Loss: 0.6166528463363647 \n",
      "     Training Step: 136 Training Loss: 0.6136159896850586 \n",
      "     Training Step: 137 Training Loss: 0.6101364493370056 \n",
      "     Training Step: 138 Training Loss: 0.6118363738059998 \n",
      "     Training Step: 139 Training Loss: 0.614440381526947 \n",
      "     Training Step: 140 Training Loss: 0.6162285804748535 \n",
      "     Training Step: 141 Training Loss: 0.612141489982605 \n",
      "     Training Step: 142 Training Loss: 0.6134517192840576 \n",
      "     Training Step: 143 Training Loss: 0.6157992482185364 \n",
      "     Training Step: 144 Training Loss: 0.6122308969497681 \n",
      "     Training Step: 145 Training Loss: 0.6115158796310425 \n",
      "     Training Step: 146 Training Loss: 0.6147776246070862 \n",
      "     Training Step: 147 Training Loss: 0.6118009686470032 \n",
      "     Training Step: 148 Training Loss: 0.615110456943512 \n",
      "     Training Step: 149 Training Loss: 0.6154009103775024 \n",
      "     Training Step: 150 Training Loss: 0.6133008599281311 \n",
      "     Training Step: 151 Training Loss: 0.6129186749458313 \n",
      "     Training Step: 152 Training Loss: 0.6141510605812073 \n",
      "     Training Step: 153 Training Loss: 0.615683913230896 \n",
      "     Training Step: 154 Training Loss: 0.6185889840126038 \n",
      "     Training Step: 155 Training Loss: 0.6124659180641174 \n",
      "     Training Step: 156 Training Loss: 0.6167068481445312 \n",
      "     Training Step: 157 Training Loss: 0.6136398315429688 \n",
      "     Training Step: 158 Training Loss: 0.6132968664169312 \n",
      "     Training Step: 159 Training Loss: 0.614347517490387 \n",
      "     Training Step: 160 Training Loss: 0.6128714680671692 \n",
      "     Training Step: 161 Training Loss: 0.6133236289024353 \n",
      "     Training Step: 162 Training Loss: 0.6146491169929504 \n",
      "     Training Step: 163 Training Loss: 0.6130675673484802 \n",
      "     Training Step: 164 Training Loss: 0.6170938014984131 \n",
      "     Training Step: 165 Training Loss: 0.6155326962471008 \n",
      "     Training Step: 166 Training Loss: 0.6155294179916382 \n",
      "     Training Step: 167 Training Loss: 0.6116231679916382 \n",
      "     Training Step: 168 Training Loss: 0.6127140522003174 \n",
      "     Training Step: 169 Training Loss: 0.6151564717292786 \n",
      "     Training Step: 170 Training Loss: 0.6122283935546875 \n",
      "     Training Step: 171 Training Loss: 0.6134666800498962 \n",
      "     Training Step: 172 Training Loss: 0.6133522987365723 \n",
      "     Training Step: 173 Training Loss: 0.6163515448570251 \n",
      "     Training Step: 174 Training Loss: 0.6171502470970154 \n",
      "     Training Step: 175 Training Loss: 0.6151754856109619 \n",
      "     Training Step: 176 Training Loss: 0.6114052534103394 \n",
      "     Training Step: 177 Training Loss: 0.6171959042549133 \n",
      "     Training Step: 178 Training Loss: 0.615286111831665 \n",
      "     Training Step: 179 Training Loss: 0.6092456579208374 \n",
      "     Training Step: 180 Training Loss: 0.6183202266693115 \n",
      "     Training Step: 181 Training Loss: 0.6162396669387817 \n",
      "     Training Step: 182 Training Loss: 0.6147048473358154 \n",
      "     Training Step: 183 Training Loss: 0.6166107654571533 \n",
      "     Training Step: 184 Training Loss: 0.618447482585907 \n",
      "     Training Step: 185 Training Loss: 0.6167923808097839 \n",
      "     Training Step: 186 Training Loss: 0.6157417297363281 \n",
      "     Training Step: 187 Training Loss: 0.6129900217056274 \n",
      "     Training Step: 188 Training Loss: 0.6101037859916687 \n",
      "     Training Step: 189 Training Loss: 0.6115146279335022 \n",
      "     Training Step: 190 Training Loss: 0.6196792125701904 \n",
      "     Training Step: 191 Training Loss: 0.6125062108039856 \n",
      "     Training Step: 192 Training Loss: 0.6115401983261108 \n",
      "     Training Step: 193 Training Loss: 0.6111688017845154 \n",
      "     Training Step: 194 Training Loss: 0.613063633441925 \n",
      "     Training Step: 195 Training Loss: 0.6137353181838989 \n",
      "     Training Step: 196 Training Loss: 0.6140788793563843 \n",
      "     Training Step: 197 Training Loss: 0.614928126335144 \n",
      "     Training Step: 198 Training Loss: 0.6131867170333862 \n",
      "     Training Step: 199 Training Loss: 0.614943265914917 \n",
      "     Training Step: 200 Training Loss: 0.6147580146789551 \n",
      "     Training Step: 201 Training Loss: 0.6146729588508606 \n",
      "     Training Step: 202 Training Loss: 0.6168243885040283 \n",
      "     Training Step: 203 Training Loss: 0.6146692037582397 \n",
      "     Training Step: 204 Training Loss: 0.6167256236076355 \n",
      "     Training Step: 205 Training Loss: 0.6153342723846436 \n",
      "     Training Step: 206 Training Loss: 0.6176972389221191 \n",
      "     Training Step: 207 Training Loss: 0.6142438054084778 \n",
      "     Training Step: 208 Training Loss: 0.616417646408081 \n",
      "     Training Step: 209 Training Loss: 0.6127846240997314 \n",
      "     Training Step: 210 Training Loss: 0.6123074293136597 \n",
      "     Training Step: 211 Training Loss: 0.6149611473083496 \n",
      "     Training Step: 212 Training Loss: 0.6196088790893555 \n",
      "     Training Step: 213 Training Loss: 0.6155194640159607 \n",
      "     Training Step: 214 Training Loss: 0.6152904033660889 \n",
      "     Training Step: 215 Training Loss: 0.6146104335784912 \n",
      "     Training Step: 216 Training Loss: 0.6150898337364197 \n",
      "     Training Step: 217 Training Loss: 0.6117045283317566 \n",
      "     Training Step: 218 Training Loss: 0.6118616461753845 \n",
      "     Training Step: 219 Training Loss: 0.6114484071731567 \n",
      "     Training Step: 220 Training Loss: 0.6146430373191833 \n",
      "     Training Step: 221 Training Loss: 0.610071063041687 \n",
      "     Training Step: 222 Training Loss: 0.6097023487091064 \n",
      "     Training Step: 223 Training Loss: 0.6082572340965271 \n",
      "     Training Step: 224 Training Loss: 0.6131197214126587 \n",
      "     Training Step: 225 Training Loss: 0.6103888154029846 \n",
      "     Training Step: 226 Training Loss: 0.6105706691741943 \n",
      "     Training Step: 227 Training Loss: 0.6146575808525085 \n",
      "     Training Step: 228 Training Loss: 0.6101595163345337 \n",
      "     Training Step: 229 Training Loss: 0.616515576839447 \n",
      "     Training Step: 230 Training Loss: 0.6115185618400574 \n",
      "     Training Step: 231 Training Loss: 0.6189651489257812 \n",
      "     Training Step: 232 Training Loss: 0.614247739315033 \n",
      "     Training Step: 233 Training Loss: 0.6154993176460266 \n",
      "     Training Step: 234 Training Loss: 0.6115737557411194 \n",
      "     Training Step: 235 Training Loss: 0.6123700737953186 \n",
      "     Training Step: 236 Training Loss: 0.612162172794342 \n",
      "     Training Step: 237 Training Loss: 0.6160342693328857 \n",
      "     Training Step: 238 Training Loss: 0.6132100820541382 \n",
      "     Training Step: 239 Training Loss: 0.6116188764572144 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6101329922676086 \n",
      "     Validation Step: 1 Validation Loss: 0.6170233488082886 \n",
      "     Validation Step: 2 Validation Loss: 0.6075507998466492 \n",
      "     Validation Step: 3 Validation Loss: 0.6104939579963684 \n",
      "     Validation Step: 4 Validation Loss: 0.6180664896965027 \n",
      "     Validation Step: 5 Validation Loss: 0.6177064776420593 \n",
      "     Validation Step: 6 Validation Loss: 0.6160107254981995 \n",
      "     Validation Step: 7 Validation Loss: 0.6142073273658752 \n",
      "     Validation Step: 8 Validation Loss: 0.618494987487793 \n",
      "     Validation Step: 9 Validation Loss: 0.6148960590362549 \n",
      "     Validation Step: 10 Validation Loss: 0.6141417026519775 \n",
      "     Validation Step: 11 Validation Loss: 0.6116479635238647 \n",
      "     Validation Step: 12 Validation Loss: 0.6129960417747498 \n",
      "     Validation Step: 13 Validation Loss: 0.6155775189399719 \n",
      "     Validation Step: 14 Validation Loss: 0.615054190158844 \n",
      "     Validation Step: 15 Validation Loss: 0.6133057475090027 \n",
      "     Validation Step: 16 Validation Loss: 0.6115684509277344 \n",
      "     Validation Step: 17 Validation Loss: 0.6182425022125244 \n",
      "     Validation Step: 18 Validation Loss: 0.6152300834655762 \n",
      "     Validation Step: 19 Validation Loss: 0.6111816167831421 \n",
      "     Validation Step: 20 Validation Loss: 0.6118952035903931 \n",
      "     Validation Step: 21 Validation Loss: 0.6173022389411926 \n",
      "     Validation Step: 22 Validation Loss: 0.6175921559333801 \n",
      "     Validation Step: 23 Validation Loss: 0.6158077120780945 \n",
      "     Validation Step: 24 Validation Loss: 0.6156212687492371 \n",
      "     Validation Step: 25 Validation Loss: 0.6153301000595093 \n",
      "     Validation Step: 26 Validation Loss: 0.6136335134506226 \n",
      "     Validation Step: 27 Validation Loss: 0.6101570725440979 \n",
      "     Validation Step: 28 Validation Loss: 0.6145691871643066 \n",
      "     Validation Step: 29 Validation Loss: 0.6106451749801636 \n",
      "     Validation Step: 30 Validation Loss: 0.6145359873771667 \n",
      "     Validation Step: 31 Validation Loss: 0.6128338575363159 \n",
      "     Validation Step: 32 Validation Loss: 0.6111904382705688 \n",
      "     Validation Step: 33 Validation Loss: 0.6105292439460754 \n",
      "     Validation Step: 34 Validation Loss: 0.614119827747345 \n",
      "     Validation Step: 35 Validation Loss: 0.6101691126823425 \n",
      "     Validation Step: 36 Validation Loss: 0.6162490844726562 \n",
      "     Validation Step: 37 Validation Loss: 0.6136803030967712 \n",
      "     Validation Step: 38 Validation Loss: 0.6136676073074341 \n",
      "     Validation Step: 39 Validation Loss: 0.6146464347839355 \n",
      "     Validation Step: 40 Validation Loss: 0.6121400594711304 \n",
      "     Validation Step: 41 Validation Loss: 0.6184695959091187 \n",
      "     Validation Step: 42 Validation Loss: 0.6142547130584717 \n",
      "     Validation Step: 43 Validation Loss: 0.6183537840843201 \n",
      "     Validation Step: 44 Validation Loss: 0.6148415803909302 \n",
      "Epoch: 89\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6131289005279541 \n",
      "     Training Step: 1 Training Loss: 0.6139177680015564 \n",
      "     Training Step: 2 Training Loss: 0.6149207353591919 \n",
      "     Training Step: 3 Training Loss: 0.614931046962738 \n",
      "     Training Step: 4 Training Loss: 0.6196337938308716 \n",
      "     Training Step: 5 Training Loss: 0.6107450127601624 \n",
      "     Training Step: 6 Training Loss: 0.612918496131897 \n",
      "     Training Step: 7 Training Loss: 0.6158369779586792 \n",
      "     Training Step: 8 Training Loss: 0.6145985722541809 \n",
      "     Training Step: 9 Training Loss: 0.6141499876976013 \n",
      "     Training Step: 10 Training Loss: 0.6155328154563904 \n",
      "     Training Step: 11 Training Loss: 0.6177966594696045 \n",
      "     Training Step: 12 Training Loss: 0.6154192090034485 \n",
      "     Training Step: 13 Training Loss: 0.6092478632926941 \n",
      "     Training Step: 14 Training Loss: 0.6108919978141785 \n",
      "     Training Step: 15 Training Loss: 0.6124629378318787 \n",
      "     Training Step: 16 Training Loss: 0.6168957352638245 \n",
      "     Training Step: 17 Training Loss: 0.6143425703048706 \n",
      "     Training Step: 18 Training Loss: 0.6129674911499023 \n",
      "     Training Step: 19 Training Loss: 0.6141085028648376 \n",
      "     Training Step: 20 Training Loss: 0.6135188341140747 \n",
      "     Training Step: 21 Training Loss: 0.6116134524345398 \n",
      "     Training Step: 22 Training Loss: 0.613620936870575 \n",
      "     Training Step: 23 Training Loss: 0.6137648224830627 \n",
      "     Training Step: 24 Training Loss: 0.6177070140838623 \n",
      "     Training Step: 25 Training Loss: 0.6177780032157898 \n",
      "     Training Step: 26 Training Loss: 0.6122837066650391 \n",
      "     Training Step: 27 Training Loss: 0.6151753664016724 \n",
      "     Training Step: 28 Training Loss: 0.6133555173873901 \n",
      "     Training Step: 29 Training Loss: 0.61665940284729 \n",
      "     Training Step: 30 Training Loss: 0.6174867153167725 \n",
      "     Training Step: 31 Training Loss: 0.619699239730835 \n",
      "     Training Step: 32 Training Loss: 0.609741747379303 \n",
      "     Training Step: 33 Training Loss: 0.6144455075263977 \n",
      "     Training Step: 34 Training Loss: 0.6106892824172974 \n",
      "     Training Step: 35 Training Loss: 0.6101661324501038 \n",
      "     Training Step: 36 Training Loss: 0.6116347908973694 \n",
      "     Training Step: 37 Training Loss: 0.6135889887809753 \n",
      "     Training Step: 38 Training Loss: 0.6132054328918457 \n",
      "     Training Step: 39 Training Loss: 0.6145006418228149 \n",
      "     Training Step: 40 Training Loss: 0.6155485510826111 \n",
      "     Training Step: 41 Training Loss: 0.6114135980606079 \n",
      "     Training Step: 42 Training Loss: 0.6161296367645264 \n",
      "     Training Step: 43 Training Loss: 0.6115816235542297 \n",
      "     Training Step: 44 Training Loss: 0.6182498931884766 \n",
      "     Training Step: 45 Training Loss: 0.6164331436157227 \n",
      "     Training Step: 46 Training Loss: 0.6146702766418457 \n",
      "     Training Step: 47 Training Loss: 0.6097316741943359 \n",
      "     Training Step: 48 Training Loss: 0.6121532917022705 \n",
      "     Training Step: 49 Training Loss: 0.6142836809158325 \n",
      "     Training Step: 50 Training Loss: 0.616033136844635 \n",
      "     Training Step: 51 Training Loss: 0.6097346544265747 \n",
      "     Training Step: 52 Training Loss: 0.6140204071998596 \n",
      "     Training Step: 53 Training Loss: 0.6142416596412659 \n",
      "     Training Step: 54 Training Loss: 0.6152762770652771 \n",
      "     Training Step: 55 Training Loss: 0.6199012994766235 \n",
      "     Training Step: 56 Training Loss: 0.6139195561408997 \n",
      "     Training Step: 57 Training Loss: 0.6171401143074036 \n",
      "     Training Step: 58 Training Loss: 0.6140204668045044 \n",
      "     Training Step: 59 Training Loss: 0.6162407994270325 \n",
      "     Training Step: 60 Training Loss: 0.6147284507751465 \n",
      "     Training Step: 61 Training Loss: 0.6153982281684875 \n",
      "     Training Step: 62 Training Loss: 0.615757405757904 \n",
      "     Training Step: 63 Training Loss: 0.6083185076713562 \n",
      "     Training Step: 64 Training Loss: 0.6114156246185303 \n",
      "     Training Step: 65 Training Loss: 0.6100870370864868 \n",
      "     Training Step: 66 Training Loss: 0.6180235147476196 \n",
      "     Training Step: 67 Training Loss: 0.6188838481903076 \n",
      "     Training Step: 68 Training Loss: 0.6117862462997437 \n",
      "     Training Step: 69 Training Loss: 0.6153799295425415 \n",
      "     Training Step: 70 Training Loss: 0.6130577921867371 \n",
      "     Training Step: 71 Training Loss: 0.6152889132499695 \n",
      "     Training Step: 72 Training Loss: 0.616727352142334 \n",
      "     Training Step: 73 Training Loss: 0.6144834756851196 \n",
      "     Training Step: 74 Training Loss: 0.6138909459114075 \n",
      "     Training Step: 75 Training Loss: 0.6171517372131348 \n",
      "     Training Step: 76 Training Loss: 0.6136494874954224 \n",
      "     Training Step: 77 Training Loss: 0.6159983277320862 \n",
      "     Training Step: 78 Training Loss: 0.620211660861969 \n",
      "     Training Step: 79 Training Loss: 0.615407407283783 \n",
      "     Training Step: 80 Training Loss: 0.6134732365608215 \n",
      "     Training Step: 81 Training Loss: 0.6147759556770325 \n",
      "     Training Step: 82 Training Loss: 0.6094629764556885 \n",
      "     Training Step: 83 Training Loss: 0.612821102142334 \n",
      "     Training Step: 84 Training Loss: 0.6133020520210266 \n",
      "     Training Step: 85 Training Loss: 0.6170874834060669 \n",
      "     Training Step: 86 Training Loss: 0.6127583384513855 \n",
      "     Training Step: 87 Training Loss: 0.6164087653160095 \n",
      "     Training Step: 88 Training Loss: 0.6121662855148315 \n",
      "     Training Step: 89 Training Loss: 0.6099917888641357 \n",
      "     Training Step: 90 Training Loss: 0.6177252531051636 \n",
      "     Training Step: 91 Training Loss: 0.611431896686554 \n",
      "     Training Step: 92 Training Loss: 0.6127637028694153 \n",
      "     Training Step: 93 Training Loss: 0.6162277460098267 \n",
      "     Training Step: 94 Training Loss: 0.6152957677841187 \n",
      "     Training Step: 95 Training Loss: 0.6186317801475525 \n",
      "     Training Step: 96 Training Loss: 0.6166335344314575 \n",
      "     Training Step: 97 Training Loss: 0.6111970543861389 \n",
      "     Training Step: 98 Training Loss: 0.6180418729782104 \n",
      "     Training Step: 99 Training Loss: 0.611535906791687 \n",
      "     Training Step: 100 Training Loss: 0.6151520013809204 \n",
      "     Training Step: 101 Training Loss: 0.6111749410629272 \n",
      "     Training Step: 102 Training Loss: 0.6167117357254028 \n",
      "     Training Step: 103 Training Loss: 0.6125401258468628 \n",
      "     Training Step: 104 Training Loss: 0.611496090888977 \n",
      "     Training Step: 105 Training Loss: 0.6151021122932434 \n",
      "     Training Step: 106 Training Loss: 0.6153815388679504 \n",
      "     Training Step: 107 Training Loss: 0.612712562084198 \n",
      "     Training Step: 108 Training Loss: 0.6122298240661621 \n",
      "     Training Step: 109 Training Loss: 0.6137365698814392 \n",
      "     Training Step: 110 Training Loss: 0.6142067909240723 \n",
      "     Training Step: 111 Training Loss: 0.6100673675537109 \n",
      "     Training Step: 112 Training Loss: 0.6125183701515198 \n",
      "     Training Step: 113 Training Loss: 0.6169466376304626 \n",
      "     Training Step: 114 Training Loss: 0.6147099137306213 \n",
      "     Training Step: 115 Training Loss: 0.6184720396995544 \n",
      "     Training Step: 116 Training Loss: 0.6171777248382568 \n",
      "     Training Step: 117 Training Loss: 0.6116312742233276 \n",
      "     Training Step: 118 Training Loss: 0.6143361330032349 \n",
      "     Training Step: 119 Training Loss: 0.6122315526008606 \n",
      "     Training Step: 120 Training Loss: 0.6181027293205261 \n",
      "     Training Step: 121 Training Loss: 0.6094783544540405 \n",
      "     Training Step: 122 Training Loss: 0.6123821139335632 \n",
      "     Training Step: 123 Training Loss: 0.6114460229873657 \n",
      "     Training Step: 124 Training Loss: 0.6118046641349792 \n",
      "     Training Step: 125 Training Loss: 0.6155189871788025 \n",
      "     Training Step: 126 Training Loss: 0.6118282079696655 \n",
      "     Training Step: 127 Training Loss: 0.6121214032173157 \n",
      "     Training Step: 128 Training Loss: 0.6147934198379517 \n",
      "     Training Step: 129 Training Loss: 0.614693820476532 \n",
      "     Training Step: 130 Training Loss: 0.6153435111045837 \n",
      "     Training Step: 131 Training Loss: 0.6134761571884155 \n",
      "     Training Step: 132 Training Loss: 0.6115440130233765 \n",
      "     Training Step: 133 Training Loss: 0.6166725158691406 \n",
      "     Training Step: 134 Training Loss: 0.6144534349441528 \n",
      "     Training Step: 135 Training Loss: 0.6103911399841309 \n",
      "     Training Step: 136 Training Loss: 0.6154784560203552 \n",
      "     Training Step: 137 Training Loss: 0.6115204095840454 \n",
      "     Training Step: 138 Training Loss: 0.6165041923522949 \n",
      "     Training Step: 139 Training Loss: 0.6142070293426514 \n",
      "     Training Step: 140 Training Loss: 0.611515998840332 \n",
      "     Training Step: 141 Training Loss: 0.617655873298645 \n",
      "     Training Step: 142 Training Loss: 0.6134994029998779 \n",
      "     Training Step: 143 Training Loss: 0.6129168272018433 \n",
      "     Training Step: 144 Training Loss: 0.6166725754737854 \n",
      "     Training Step: 145 Training Loss: 0.615441620349884 \n",
      "     Training Step: 146 Training Loss: 0.618329644203186 \n",
      "     Training Step: 147 Training Loss: 0.6140815615653992 \n",
      "     Training Step: 148 Training Loss: 0.6176833510398865 \n",
      "     Training Step: 149 Training Loss: 0.6122584342956543 \n",
      "     Training Step: 150 Training Loss: 0.6171850562095642 \n",
      "     Training Step: 151 Training Loss: 0.6100865006446838 \n",
      "     Training Step: 152 Training Loss: 0.6130743026733398 \n",
      "     Training Step: 153 Training Loss: 0.6138232350349426 \n",
      "     Training Step: 154 Training Loss: 0.6131500601768494 \n",
      "     Training Step: 155 Training Loss: 0.6105132699012756 \n",
      "     Training Step: 156 Training Loss: 0.6109146475791931 \n",
      "     Training Step: 157 Training Loss: 0.6147094368934631 \n",
      "     Training Step: 158 Training Loss: 0.6118022799491882 \n",
      "     Training Step: 159 Training Loss: 0.6126416921615601 \n",
      "     Training Step: 160 Training Loss: 0.610673189163208 \n",
      "     Training Step: 161 Training Loss: 0.6132972836494446 \n",
      "     Training Step: 162 Training Loss: 0.6118338704109192 \n",
      "     Training Step: 163 Training Loss: 0.6118234395980835 \n",
      "     Training Step: 164 Training Loss: 0.6132097840309143 \n",
      "     Training Step: 165 Training Loss: 0.6210304498672485 \n",
      "     Training Step: 166 Training Loss: 0.6133175492286682 \n",
      "     Training Step: 167 Training Loss: 0.6146963834762573 \n",
      "     Training Step: 168 Training Loss: 0.612842857837677 \n",
      "     Training Step: 169 Training Loss: 0.6140308976173401 \n",
      "     Training Step: 170 Training Loss: 0.6144253015518188 \n",
      "     Training Step: 171 Training Loss: 0.6185961365699768 \n",
      "     Training Step: 172 Training Loss: 0.6133070588111877 \n",
      "     Training Step: 173 Training Loss: 0.6118904948234558 \n",
      "     Training Step: 174 Training Loss: 0.6156845092773438 \n",
      "     Training Step: 175 Training Loss: 0.6155959367752075 \n",
      "     Training Step: 176 Training Loss: 0.6152439117431641 \n",
      "     Training Step: 177 Training Loss: 0.6150928735733032 \n",
      "     Training Step: 178 Training Loss: 0.6131986975669861 \n",
      "     Training Step: 179 Training Loss: 0.6121529936790466 \n",
      "     Training Step: 180 Training Loss: 0.6167017221450806 \n",
      "     Training Step: 181 Training Loss: 0.6183958053588867 \n",
      "     Training Step: 182 Training Loss: 0.6159428358078003 \n",
      "     Training Step: 183 Training Loss: 0.6106127500534058 \n",
      "     Training Step: 184 Training Loss: 0.617368757724762 \n",
      "     Training Step: 185 Training Loss: 0.6107393503189087 \n",
      "     Training Step: 186 Training Loss: 0.6152974963188171 \n",
      "     Training Step: 187 Training Loss: 0.6111500263214111 \n",
      "     Training Step: 188 Training Loss: 0.6167997717857361 \n",
      "     Training Step: 189 Training Loss: 0.6101928949356079 \n",
      "     Training Step: 190 Training Loss: 0.6194568872451782 \n",
      "     Training Step: 191 Training Loss: 0.6105963587760925 \n",
      "     Training Step: 192 Training Loss: 0.614608883857727 \n",
      "     Training Step: 193 Training Loss: 0.612383246421814 \n",
      "     Training Step: 194 Training Loss: 0.6120632886886597 \n",
      "     Training Step: 195 Training Loss: 0.6147522330284119 \n",
      "     Training Step: 196 Training Loss: 0.6132822632789612 \n",
      "     Training Step: 197 Training Loss: 0.6146727800369263 \n",
      "     Training Step: 198 Training Loss: 0.6136369109153748 \n",
      "     Training Step: 199 Training Loss: 0.6143796443939209 \n",
      "     Training Step: 200 Training Loss: 0.6182412505149841 \n",
      "     Training Step: 201 Training Loss: 0.6149573922157288 \n",
      "     Training Step: 202 Training Loss: 0.6125385165214539 \n",
      "     Training Step: 203 Training Loss: 0.6162028312683105 \n",
      "     Training Step: 204 Training Loss: 0.6134129762649536 \n",
      "     Training Step: 205 Training Loss: 0.6148728728294373 \n",
      "     Training Step: 206 Training Loss: 0.61039799451828 \n",
      "     Training Step: 207 Training Loss: 0.6128761768341064 \n",
      "     Training Step: 208 Training Loss: 0.6105815768241882 \n",
      "     Training Step: 209 Training Loss: 0.6156909465789795 \n",
      "     Training Step: 210 Training Loss: 0.6146141886711121 \n",
      "     Training Step: 211 Training Loss: 0.6120123863220215 \n",
      "     Training Step: 212 Training Loss: 0.6105678081512451 \n",
      "     Training Step: 213 Training Loss: 0.617824137210846 \n",
      "     Training Step: 214 Training Loss: 0.6124855875968933 \n",
      "     Training Step: 215 Training Loss: 0.6128625869750977 \n",
      "     Training Step: 216 Training Loss: 0.6150751113891602 \n",
      "     Training Step: 217 Training Loss: 0.6184836626052856 \n",
      "     Training Step: 218 Training Loss: 0.612288773059845 \n",
      "     Training Step: 219 Training Loss: 0.6141531467437744 \n",
      "     Training Step: 220 Training Loss: 0.6157485246658325 \n",
      "     Training Step: 221 Training Loss: 0.6163561940193176 \n",
      "     Training Step: 222 Training Loss: 0.6166771054267883 \n",
      "     Training Step: 223 Training Loss: 0.6146492958068848 \n",
      "     Training Step: 224 Training Loss: 0.6146389245986938 \n",
      "     Training Step: 225 Training Loss: 0.6123895049095154 \n",
      "     Training Step: 226 Training Loss: 0.616703450679779 \n",
      "     Training Step: 227 Training Loss: 0.6157792210578918 \n",
      "     Training Step: 228 Training Loss: 0.6167968511581421 \n",
      "     Training Step: 229 Training Loss: 0.6137505769729614 \n",
      "     Training Step: 230 Training Loss: 0.611707329750061 \n",
      "     Training Step: 231 Training Loss: 0.6180532574653625 \n",
      "     Training Step: 232 Training Loss: 0.6116649508476257 \n",
      "     Training Step: 233 Training Loss: 0.6157428622245789 \n",
      "     Training Step: 234 Training Loss: 0.6114367842674255 \n",
      "     Training Step: 235 Training Loss: 0.6167465448379517 \n",
      "     Training Step: 236 Training Loss: 0.6105165481567383 \n",
      "     Training Step: 237 Training Loss: 0.6188338398933411 \n",
      "     Training Step: 238 Training Loss: 0.6168006658554077 \n",
      "     Training Step: 239 Training Loss: 0.6143467426300049 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6111840009689331 \n",
      "     Validation Step: 1 Validation Loss: 0.6115622520446777 \n",
      "     Validation Step: 2 Validation Loss: 0.6145393252372742 \n",
      "     Validation Step: 3 Validation Loss: 0.6128340363502502 \n",
      "     Validation Step: 4 Validation Loss: 0.614637553691864 \n",
      "     Validation Step: 5 Validation Loss: 0.6152253150939941 \n",
      "     Validation Step: 6 Validation Loss: 0.6136487126350403 \n",
      "     Validation Step: 7 Validation Loss: 0.6121335029602051 \n",
      "     Validation Step: 8 Validation Loss: 0.6106426119804382 \n",
      "     Validation Step: 9 Validation Loss: 0.6176872253417969 \n",
      "     Validation Step: 10 Validation Loss: 0.611882746219635 \n",
      "     Validation Step: 11 Validation Loss: 0.6142617464065552 \n",
      "     Validation Step: 12 Validation Loss: 0.6129838228225708 \n",
      "     Validation Step: 13 Validation Loss: 0.6133123636245728 \n",
      "     Validation Step: 14 Validation Loss: 0.6145699620246887 \n",
      "     Validation Step: 15 Validation Loss: 0.6142022013664246 \n",
      "     Validation Step: 16 Validation Loss: 0.6156110167503357 \n",
      "     Validation Step: 17 Validation Loss: 0.6157817840576172 \n",
      "     Validation Step: 18 Validation Loss: 0.6104927659034729 \n",
      "     Validation Step: 19 Validation Loss: 0.6155741810798645 \n",
      "     Validation Step: 20 Validation Loss: 0.6141275763511658 \n",
      "     Validation Step: 21 Validation Loss: 0.6150348782539368 \n",
      "     Validation Step: 22 Validation Loss: 0.6105207800865173 \n",
      "     Validation Step: 23 Validation Loss: 0.6162436008453369 \n",
      "     Validation Step: 24 Validation Loss: 0.6116504669189453 \n",
      "     Validation Step: 25 Validation Loss: 0.6170203685760498 \n",
      "     Validation Step: 26 Validation Loss: 0.6101248264312744 \n",
      "     Validation Step: 27 Validation Loss: 0.6173012256622314 \n",
      "     Validation Step: 28 Validation Loss: 0.6101813316345215 \n",
      "     Validation Step: 29 Validation Loss: 0.6111657023429871 \n",
      "     Validation Step: 30 Validation Loss: 0.6159883737564087 \n",
      "     Validation Step: 31 Validation Loss: 0.6153157949447632 \n",
      "     Validation Step: 32 Validation Loss: 0.6180588006973267 \n",
      "     Validation Step: 33 Validation Loss: 0.614116370677948 \n",
      "     Validation Step: 34 Validation Loss: 0.6101597547531128 \n",
      "     Validation Step: 35 Validation Loss: 0.6183370351791382 \n",
      "     Validation Step: 36 Validation Loss: 0.614896833896637 \n",
      "     Validation Step: 37 Validation Loss: 0.6184907555580139 \n",
      "     Validation Step: 38 Validation Loss: 0.6136380434036255 \n",
      "     Validation Step: 39 Validation Loss: 0.6075443029403687 \n",
      "     Validation Step: 40 Validation Loss: 0.6136652827262878 \n",
      "     Validation Step: 41 Validation Loss: 0.6182263493537903 \n",
      "     Validation Step: 42 Validation Loss: 0.6175974011421204 \n",
      "     Validation Step: 43 Validation Loss: 0.6148324012756348 \n",
      "     Validation Step: 44 Validation Loss: 0.6184683442115784 \n",
      "Epoch: 90\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6146047115325928 \n",
      "     Training Step: 1 Training Loss: 0.6125224828720093 \n",
      "     Training Step: 2 Training Loss: 0.6171448826789856 \n",
      "     Training Step: 3 Training Loss: 0.6166464686393738 \n",
      "     Training Step: 4 Training Loss: 0.6138218641281128 \n",
      "     Training Step: 5 Training Loss: 0.6177873015403748 \n",
      "     Training Step: 6 Training Loss: 0.615096390247345 \n",
      "     Training Step: 7 Training Loss: 0.6120794415473938 \n",
      "     Training Step: 8 Training Loss: 0.6128066182136536 \n",
      "     Training Step: 9 Training Loss: 0.6127700805664062 \n",
      "     Training Step: 10 Training Loss: 0.6152434945106506 \n",
      "     Training Step: 11 Training Loss: 0.6154183745384216 \n",
      "     Training Step: 12 Training Loss: 0.6135246753692627 \n",
      "     Training Step: 13 Training Loss: 0.6146393418312073 \n",
      "     Training Step: 14 Training Loss: 0.6120266318321228 \n",
      "     Training Step: 15 Training Loss: 0.6101937294006348 \n",
      "     Training Step: 16 Training Loss: 0.6171443462371826 \n",
      "     Training Step: 17 Training Loss: 0.6104881167411804 \n",
      "     Training Step: 18 Training Loss: 0.6100648641586304 \n",
      "     Training Step: 19 Training Loss: 0.6136401891708374 \n",
      "     Training Step: 20 Training Loss: 0.6209832429885864 \n",
      "     Training Step: 21 Training Loss: 0.6167368292808533 \n",
      "     Training Step: 22 Training Loss: 0.6161244511604309 \n",
      "     Training Step: 23 Training Loss: 0.6155349016189575 \n",
      "     Training Step: 24 Training Loss: 0.614707887172699 \n",
      "     Training Step: 25 Training Loss: 0.613648533821106 \n",
      "     Training Step: 26 Training Loss: 0.6131401658058167 \n",
      "     Training Step: 27 Training Loss: 0.6167594194412231 \n",
      "     Training Step: 28 Training Loss: 0.6132081747055054 \n",
      "     Training Step: 29 Training Loss: 0.6106827855110168 \n",
      "     Training Step: 30 Training Loss: 0.6194302439689636 \n",
      "     Training Step: 31 Training Loss: 0.6185934543609619 \n",
      "     Training Step: 32 Training Loss: 0.6094404458999634 \n",
      "     Training Step: 33 Training Loss: 0.614218533039093 \n",
      "     Training Step: 34 Training Loss: 0.6152850389480591 \n",
      "     Training Step: 35 Training Loss: 0.6153773069381714 \n",
      "     Training Step: 36 Training Loss: 0.6132836937904358 \n",
      "     Training Step: 37 Training Loss: 0.6104146242141724 \n",
      "     Training Step: 38 Training Loss: 0.6130598187446594 \n",
      "     Training Step: 39 Training Loss: 0.6147716045379639 \n",
      "     Training Step: 40 Training Loss: 0.6176964044570923 \n",
      "     Training Step: 41 Training Loss: 0.6147088408470154 \n",
      "     Training Step: 42 Training Loss: 0.6172047853469849 \n",
      "     Training Step: 43 Training Loss: 0.6097013354301453 \n",
      "     Training Step: 44 Training Loss: 0.6097231507301331 \n",
      "     Training Step: 45 Training Loss: 0.6150727272033691 \n",
      "     Training Step: 46 Training Loss: 0.6140170097351074 \n",
      "     Training Step: 47 Training Loss: 0.6153978705406189 \n",
      "     Training Step: 48 Training Loss: 0.6141084432601929 \n",
      "     Training Step: 49 Training Loss: 0.6106810569763184 \n",
      "     Training Step: 50 Training Loss: 0.6101254820823669 \n",
      "     Training Step: 51 Training Loss: 0.6137669086456299 \n",
      "     Training Step: 52 Training Loss: 0.6160150766372681 \n",
      "     Training Step: 53 Training Loss: 0.6115173697471619 \n",
      "     Training Step: 54 Training Loss: 0.6133103966712952 \n",
      "     Training Step: 55 Training Loss: 0.6149256229400635 \n",
      "     Training Step: 56 Training Loss: 0.6154334545135498 \n",
      "     Training Step: 57 Training Loss: 0.6146613955497742 \n",
      "     Training Step: 58 Training Loss: 0.6129611730575562 \n",
      "     Training Step: 59 Training Loss: 0.6167343258857727 \n",
      "     Training Step: 60 Training Loss: 0.6129167675971985 \n",
      "     Training Step: 61 Training Loss: 0.6143748760223389 \n",
      "     Training Step: 62 Training Loss: 0.6111639142036438 \n",
      "     Training Step: 63 Training Loss: 0.6144986748695374 \n",
      "     Training Step: 64 Training Loss: 0.6123775243759155 \n",
      "     Training Step: 65 Training Loss: 0.6184437870979309 \n",
      "     Training Step: 66 Training Loss: 0.6105749011039734 \n",
      "     Training Step: 67 Training Loss: 0.6168075799942017 \n",
      "     Training Step: 68 Training Loss: 0.6154435276985168 \n",
      "     Training Step: 69 Training Loss: 0.6121429800987244 \n",
      "     Training Step: 70 Training Loss: 0.6105856895446777 \n",
      "     Training Step: 71 Training Loss: 0.6100583672523499 \n",
      "     Training Step: 72 Training Loss: 0.6124927401542664 \n",
      "     Training Step: 73 Training Loss: 0.6121202111244202 \n",
      "     Training Step: 74 Training Loss: 0.6174004077911377 \n",
      "     Training Step: 75 Training Loss: 0.6167327165603638 \n",
      "     Training Step: 76 Training Loss: 0.6118760704994202 \n",
      "     Training Step: 77 Training Loss: 0.6166688203811646 \n",
      "     Training Step: 78 Training Loss: 0.6134539246559143 \n",
      "     Training Step: 79 Training Loss: 0.6177761554718018 \n",
      "     Training Step: 80 Training Loss: 0.6108965873718262 \n",
      "     Training Step: 81 Training Loss: 0.6128649711608887 \n",
      "     Training Step: 82 Training Loss: 0.6082649230957031 \n",
      "     Training Step: 83 Training Loss: 0.6105794906616211 \n",
      "     Training Step: 84 Training Loss: 0.616441011428833 \n",
      "     Training Step: 85 Training Loss: 0.6128431558609009 \n",
      "     Training Step: 86 Training Loss: 0.6103663444519043 \n",
      "     Training Step: 87 Training Loss: 0.6156163215637207 \n",
      "     Training Step: 88 Training Loss: 0.6104829907417297 \n",
      "     Training Step: 89 Training Loss: 0.6199426054954529 \n",
      "     Training Step: 90 Training Loss: 0.6142189502716064 \n",
      "     Training Step: 91 Training Loss: 0.6162611842155457 \n",
      "     Training Step: 92 Training Loss: 0.614689290523529 \n",
      "     Training Step: 93 Training Loss: 0.6188811659812927 \n",
      "     Training Step: 94 Training Loss: 0.6174907684326172 \n",
      "     Training Step: 95 Training Loss: 0.6133068799972534 \n",
      "     Training Step: 96 Training Loss: 0.6180272102355957 \n",
      "     Training Step: 97 Training Loss: 0.6148722767829895 \n",
      "     Training Step: 98 Training Loss: 0.6167083978652954 \n",
      "     Training Step: 99 Training Loss: 0.6155356764793396 \n",
      "     Training Step: 100 Training Loss: 0.6117329001426697 \n",
      "     Training Step: 101 Training Loss: 0.6160424947738647 \n",
      "     Training Step: 102 Training Loss: 0.6143587231636047 \n",
      "     Training Step: 103 Training Loss: 0.6176738739013672 \n",
      "     Training Step: 104 Training Loss: 0.6182848215103149 \n",
      "     Training Step: 105 Training Loss: 0.6153467893600464 \n",
      "     Training Step: 106 Training Loss: 0.6180492639541626 \n",
      "     Training Step: 107 Training Loss: 0.6136494874954224 \n",
      "     Training Step: 108 Training Loss: 0.6159355640411377 \n",
      "     Training Step: 109 Training Loss: 0.6128950119018555 \n",
      "     Training Step: 110 Training Loss: 0.6195902824401855 \n",
      "     Training Step: 111 Training Loss: 0.6118572354316711 \n",
      "     Training Step: 112 Training Loss: 0.6161960959434509 \n",
      "     Training Step: 113 Training Loss: 0.6182237863540649 \n",
      "     Training Step: 114 Training Loss: 0.6152845621109009 \n",
      "     Training Step: 115 Training Loss: 0.618445873260498 \n",
      "     Training Step: 116 Training Loss: 0.6166676878929138 \n",
      "     Training Step: 117 Training Loss: 0.6143416166305542 \n",
      "     Training Step: 118 Training Loss: 0.6116467714309692 \n",
      "     Training Step: 119 Training Loss: 0.6180849671363831 \n",
      "     Training Step: 120 Training Loss: 0.6100954413414001 \n",
      "     Training Step: 121 Training Loss: 0.6127629280090332 \n",
      "     Training Step: 122 Training Loss: 0.6147259473800659 \n",
      "     Training Step: 123 Training Loss: 0.6166722178459167 \n",
      "     Training Step: 124 Training Loss: 0.6147506237030029 \n",
      "     Training Step: 125 Training Loss: 0.6122302412986755 \n",
      "     Training Step: 126 Training Loss: 0.6185764670372009 \n",
      "     Training Step: 127 Training Loss: 0.6146092414855957 \n",
      "     Training Step: 128 Training Loss: 0.6114500164985657 \n",
      "     Training Step: 129 Training Loss: 0.6094719767570496 \n",
      "     Training Step: 130 Training Loss: 0.6122967600822449 \n",
      "     Training Step: 131 Training Loss: 0.614961564540863 \n",
      "     Training Step: 132 Training Loss: 0.6154795289039612 \n",
      "     Training Step: 133 Training Loss: 0.6157935857772827 \n",
      "     Training Step: 134 Training Loss: 0.616817831993103 \n",
      "     Training Step: 135 Training Loss: 0.6116077899932861 \n",
      "     Training Step: 136 Training Loss: 0.6177218556404114 \n",
      "     Training Step: 137 Training Loss: 0.6142860054969788 \n",
      "     Training Step: 138 Training Loss: 0.6108824014663696 \n",
      "     Training Step: 139 Training Loss: 0.6158348321914673 \n",
      "     Training Step: 140 Training Loss: 0.6111468076705933 \n",
      "     Training Step: 141 Training Loss: 0.6176372766494751 \n",
      "     Training Step: 142 Training Loss: 0.612240731716156 \n",
      "     Training Step: 143 Training Loss: 0.6117985248565674 \n",
      "     Training Step: 144 Training Loss: 0.6188559532165527 \n",
      "     Training Step: 145 Training Loss: 0.6166219711303711 \n",
      "     Training Step: 146 Training Loss: 0.6122384667396545 \n",
      "     Training Step: 147 Training Loss: 0.6143461465835571 \n",
      "     Training Step: 148 Training Loss: 0.6141519546508789 \n",
      "     Training Step: 149 Training Loss: 0.6146761178970337 \n",
      "     Training Step: 150 Training Loss: 0.614421546459198 \n",
      "     Training Step: 151 Training Loss: 0.6180165410041809 \n",
      "     Training Step: 152 Training Loss: 0.6122983694076538 \n",
      "     Training Step: 153 Training Loss: 0.6157644987106323 \n",
      "     Training Step: 154 Training Loss: 0.6151566505432129 \n",
      "     Training Step: 155 Training Loss: 0.6137391924858093 \n",
      "     Training Step: 156 Training Loss: 0.613294780254364 \n",
      "     Training Step: 157 Training Loss: 0.6156764626502991 \n",
      "     Training Step: 158 Training Loss: 0.6164875030517578 \n",
      "     Training Step: 159 Training Loss: 0.6131912469863892 \n",
      "     Training Step: 160 Training Loss: 0.6140403151512146 \n",
      "     Training Step: 161 Training Loss: 0.6142441034317017 \n",
      "     Training Step: 162 Training Loss: 0.6141471266746521 \n",
      "     Training Step: 163 Training Loss: 0.6145988702774048 \n",
      "     Training Step: 164 Training Loss: 0.613359808921814 \n",
      "     Training Step: 165 Training Loss: 0.6125319004058838 \n",
      "     Training Step: 166 Training Loss: 0.6132037043571472 \n",
      "     Training Step: 167 Training Loss: 0.6116300821304321 \n",
      "     Training Step: 168 Training Loss: 0.612464427947998 \n",
      "     Training Step: 169 Training Loss: 0.6126959323883057 \n",
      "     Training Step: 170 Training Loss: 0.6140857934951782 \n",
      "     Training Step: 171 Training Loss: 0.6202603578567505 \n",
      "     Training Step: 172 Training Loss: 0.6115225553512573 \n",
      "     Training Step: 173 Training Loss: 0.6144552230834961 \n",
      "     Training Step: 174 Training Loss: 0.6117796897888184 \n",
      "     Training Step: 175 Training Loss: 0.6155505180358887 \n",
      "     Training Step: 176 Training Loss: 0.6146547794342041 \n",
      "     Training Step: 177 Training Loss: 0.6107365489006042 \n",
      "     Training Step: 178 Training Loss: 0.6121520400047302 \n",
      "     Training Step: 179 Training Loss: 0.6126389503479004 \n",
      "     Training Step: 180 Training Loss: 0.613883912563324 \n",
      "     Training Step: 181 Training Loss: 0.615088164806366 \n",
      "     Training Step: 182 Training Loss: 0.6130645871162415 \n",
      "     Training Step: 183 Training Loss: 0.6157466173171997 \n",
      "     Training Step: 184 Training Loss: 0.615263819694519 \n",
      "     Training Step: 185 Training Loss: 0.6115301847457886 \n",
      "     Training Step: 186 Training Loss: 0.6168012619018555 \n",
      "     Training Step: 187 Training Loss: 0.6144825220108032 \n",
      "     Training Step: 188 Training Loss: 0.6118398904800415 \n",
      "     Training Step: 189 Training Loss: 0.6170920729637146 \n",
      "     Training Step: 190 Training Loss: 0.6157429814338684 \n",
      "     Training Step: 191 Training Loss: 0.6164036393165588 \n",
      "     Training Step: 192 Training Loss: 0.6129131317138672 \n",
      "     Training Step: 193 Training Loss: 0.6152931451797485 \n",
      "     Training Step: 194 Training Loss: 0.6139213442802429 \n",
      "     Training Step: 195 Training Loss: 0.6134732961654663 \n",
      "     Training Step: 196 Training Loss: 0.6168842911720276 \n",
      "     Training Step: 197 Training Loss: 0.6140217781066895 \n",
      "     Training Step: 198 Training Loss: 0.6114322543144226 \n",
      "     Training Step: 199 Training Loss: 0.6135003566741943 \n",
      "     Training Step: 200 Training Loss: 0.6151750087738037 \n",
      "     Training Step: 201 Training Loss: 0.611405074596405 \n",
      "     Training Step: 202 Training Loss: 0.6118372678756714 \n",
      "     Training Step: 203 Training Loss: 0.6123776435852051 \n",
      "     Training Step: 204 Training Loss: 0.611469566822052 \n",
      "     Training Step: 205 Training Loss: 0.6146754026412964 \n",
      "     Training Step: 206 Training Loss: 0.6123756766319275 \n",
      "     Training Step: 207 Training Loss: 0.6144446134567261 \n",
      "     Training Step: 208 Training Loss: 0.6136037707328796 \n",
      "     Training Step: 209 Training Loss: 0.6097164154052734 \n",
      "     Training Step: 210 Training Loss: 0.6114001274108887 \n",
      "     Training Step: 211 Training Loss: 0.6139249205589294 \n",
      "     Training Step: 212 Training Loss: 0.6163860559463501 \n",
      "     Training Step: 213 Training Loss: 0.6099634766578674 \n",
      "     Training Step: 214 Training Loss: 0.6172036528587341 \n",
      "     Training Step: 215 Training Loss: 0.6137474775314331 \n",
      "     Training Step: 216 Training Loss: 0.6118042469024658 \n",
      "     Training Step: 217 Training Loss: 0.615687906742096 \n",
      "     Training Step: 218 Training Loss: 0.6092228889465332 \n",
      "     Training Step: 219 Training Loss: 0.6105920076370239 \n",
      "     Training Step: 220 Training Loss: 0.6115816831588745 \n",
      "     Training Step: 221 Training Loss: 0.6132918000221252 \n",
      "     Training Step: 222 Training Loss: 0.6125332713127136 \n",
      "     Training Step: 223 Training Loss: 0.6114256381988525 \n",
      "     Training Step: 224 Training Loss: 0.6184411644935608 \n",
      "     Training Step: 225 Training Loss: 0.6115447282791138 \n",
      "     Training Step: 226 Training Loss: 0.6147959232330322 \n",
      "     Training Step: 227 Training Loss: 0.6149393916130066 \n",
      "     Training Step: 228 Training Loss: 0.6121551394462585 \n",
      "     Training Step: 229 Training Loss: 0.6162142157554626 \n",
      "     Training Step: 230 Training Loss: 0.6153802871704102 \n",
      "     Training Step: 231 Training Loss: 0.6107163429260254 \n",
      "     Training Step: 232 Training Loss: 0.6197242140769958 \n",
      "     Training Step: 233 Training Loss: 0.6111975312232971 \n",
      "     Training Step: 234 Training Loss: 0.6182272434234619 \n",
      "     Training Step: 235 Training Loss: 0.61780846118927 \n",
      "     Training Step: 236 Training Loss: 0.6134200692176819 \n",
      "     Training Step: 237 Training Loss: 0.613131046295166 \n",
      "     Training Step: 238 Training Loss: 0.6116610169410706 \n",
      "     Training Step: 239 Training Loss: 0.616919755935669 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6183339357376099 \n",
      "     Validation Step: 1 Validation Loss: 0.6172924637794495 \n",
      "     Validation Step: 2 Validation Loss: 0.6159964799880981 \n",
      "     Validation Step: 3 Validation Loss: 0.6136596202850342 \n",
      "     Validation Step: 4 Validation Loss: 0.6111827492713928 \n",
      "     Validation Step: 5 Validation Loss: 0.6129977107048035 \n",
      "     Validation Step: 6 Validation Loss: 0.6153180599212646 \n",
      "     Validation Step: 7 Validation Loss: 0.6180508136749268 \n",
      "     Validation Step: 8 Validation Loss: 0.6105039715766907 \n",
      "     Validation Step: 9 Validation Loss: 0.6146425008773804 \n",
      "     Validation Step: 10 Validation Loss: 0.6106520891189575 \n",
      "     Validation Step: 11 Validation Loss: 0.6105362176895142 \n",
      "     Validation Step: 12 Validation Loss: 0.618457019329071 \n",
      "     Validation Step: 13 Validation Loss: 0.610167384147644 \n",
      "     Validation Step: 14 Validation Loss: 0.6111934781074524 \n",
      "     Validation Step: 15 Validation Loss: 0.6101829409599304 \n",
      "     Validation Step: 16 Validation Loss: 0.6184796094894409 \n",
      "     Validation Step: 17 Validation Loss: 0.6141378879547119 \n",
      "     Validation Step: 18 Validation Loss: 0.6121425032615662 \n",
      "     Validation Step: 19 Validation Loss: 0.6148939728736877 \n",
      "     Validation Step: 20 Validation Loss: 0.6145370602607727 \n",
      "     Validation Step: 21 Validation Loss: 0.6128414273262024 \n",
      "     Validation Step: 22 Validation Loss: 0.614255964756012 \n",
      "     Validation Step: 23 Validation Loss: 0.6162423491477966 \n",
      "     Validation Step: 24 Validation Loss: 0.6155705451965332 \n",
      "     Validation Step: 25 Validation Loss: 0.6148393154144287 \n",
      "     Validation Step: 26 Validation Loss: 0.6116559505462646 \n",
      "     Validation Step: 27 Validation Loss: 0.6145659685134888 \n",
      "     Validation Step: 28 Validation Loss: 0.6142042875289917 \n",
      "     Validation Step: 29 Validation Loss: 0.6157956123352051 \n",
      "     Validation Step: 30 Validation Loss: 0.6182224154472351 \n",
      "     Validation Step: 31 Validation Loss: 0.6176902651786804 \n",
      "     Validation Step: 32 Validation Loss: 0.6118990778923035 \n",
      "     Validation Step: 33 Validation Loss: 0.6150423288345337 \n",
      "     Validation Step: 34 Validation Loss: 0.6133109331130981 \n",
      "     Validation Step: 35 Validation Loss: 0.6115736961364746 \n",
      "     Validation Step: 36 Validation Loss: 0.6170111894607544 \n",
      "     Validation Step: 37 Validation Loss: 0.6152253150939941 \n",
      "     Validation Step: 38 Validation Loss: 0.6175855994224548 \n",
      "     Validation Step: 39 Validation Loss: 0.610139787197113 \n",
      "     Validation Step: 40 Validation Loss: 0.6075650453567505 \n",
      "     Validation Step: 41 Validation Loss: 0.613639771938324 \n",
      "     Validation Step: 42 Validation Loss: 0.6156127452850342 \n",
      "     Validation Step: 43 Validation Loss: 0.6136770248413086 \n",
      "     Validation Step: 44 Validation Loss: 0.6141197681427002 \n",
      "Epoch: 91\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6159930229187012 \n",
      "     Training Step: 1 Training Loss: 0.6144969463348389 \n",
      "     Training Step: 2 Training Loss: 0.6139233112335205 \n",
      "     Training Step: 3 Training Loss: 0.6176196336746216 \n",
      "     Training Step: 4 Training Loss: 0.6164072751998901 \n",
      "     Training Step: 5 Training Loss: 0.6155263781547546 \n",
      "     Training Step: 6 Training Loss: 0.6150639653205872 \n",
      "     Training Step: 7 Training Loss: 0.6167060732841492 \n",
      "     Training Step: 8 Training Loss: 0.6107189059257507 \n",
      "     Training Step: 9 Training Loss: 0.615664541721344 \n",
      "     Training Step: 10 Training Loss: 0.6133052706718445 \n",
      "     Training Step: 11 Training Loss: 0.6127282977104187 \n",
      "     Training Step: 12 Training Loss: 0.6146757006645203 \n",
      "     Training Step: 13 Training Loss: 0.6155965924263 \n",
      "     Training Step: 14 Training Loss: 0.6161987781524658 \n",
      "     Training Step: 15 Training Loss: 0.6169177889823914 \n",
      "     Training Step: 16 Training Loss: 0.6150854229927063 \n",
      "     Training Step: 17 Training Loss: 0.6138840913772583 \n",
      "     Training Step: 18 Training Loss: 0.6133565902709961 \n",
      "     Training Step: 19 Training Loss: 0.6152891516685486 \n",
      "     Training Step: 20 Training Loss: 0.6123920679092407 \n",
      "     Training Step: 21 Training Loss: 0.6180654764175415 \n",
      "     Training Step: 22 Training Loss: 0.6121728420257568 \n",
      "     Training Step: 23 Training Loss: 0.61331707239151 \n",
      "     Training Step: 24 Training Loss: 0.6143480539321899 \n",
      "     Training Step: 25 Training Loss: 0.6153334975242615 \n",
      "     Training Step: 26 Training Loss: 0.611405074596405 \n",
      "     Training Step: 27 Training Loss: 0.6157534122467041 \n",
      "     Training Step: 28 Training Loss: 0.6115571856498718 \n",
      "     Training Step: 29 Training Loss: 0.6141086220741272 \n",
      "     Training Step: 30 Training Loss: 0.6118259429931641 \n",
      "     Training Step: 31 Training Loss: 0.6135954856872559 \n",
      "     Training Step: 32 Training Loss: 0.6126397252082825 \n",
      "     Training Step: 33 Training Loss: 0.6146162748336792 \n",
      "     Training Step: 34 Training Loss: 0.6149341464042664 \n",
      "     Training Step: 35 Training Loss: 0.6174113750457764 \n",
      "     Training Step: 36 Training Loss: 0.6143473386764526 \n",
      "     Training Step: 37 Training Loss: 0.6197301745414734 \n",
      "     Training Step: 38 Training Loss: 0.613118588924408 \n",
      "     Training Step: 39 Training Loss: 0.616423487663269 \n",
      "     Training Step: 40 Training Loss: 0.6180196404457092 \n",
      "     Training Step: 41 Training Loss: 0.6140430569648743 \n",
      "     Training Step: 42 Training Loss: 0.617779552936554 \n",
      "     Training Step: 43 Training Loss: 0.6130790114402771 \n",
      "     Training Step: 44 Training Loss: 0.6170820593833923 \n",
      "     Training Step: 45 Training Loss: 0.6153799295425415 \n",
      "     Training Step: 46 Training Loss: 0.6176809668540955 \n",
      "     Training Step: 47 Training Loss: 0.6144346594810486 \n",
      "     Training Step: 48 Training Loss: 0.6129058599472046 \n",
      "     Training Step: 49 Training Loss: 0.6124909520149231 \n",
      "     Training Step: 50 Training Loss: 0.6164767742156982 \n",
      "     Training Step: 51 Training Loss: 0.6114491820335388 \n",
      "     Training Step: 52 Training Loss: 0.6129816174507141 \n",
      "     Training Step: 53 Training Loss: 0.611173152923584 \n",
      "     Training Step: 54 Training Loss: 0.6202184557914734 \n",
      "     Training Step: 55 Training Loss: 0.6125315427780151 \n",
      "     Training Step: 56 Training Loss: 0.6166253685951233 \n",
      "     Training Step: 57 Training Loss: 0.6198994517326355 \n",
      "     Training Step: 58 Training Loss: 0.6114411354064941 \n",
      "     Training Step: 59 Training Loss: 0.6100673675537109 \n",
      "     Training Step: 60 Training Loss: 0.6154259443283081 \n",
      "     Training Step: 61 Training Loss: 0.6123742461204529 \n",
      "     Training Step: 62 Training Loss: 0.6108794212341309 \n",
      "     Training Step: 63 Training Loss: 0.6097277998924255 \n",
      "     Training Step: 64 Training Loss: 0.6104789972305298 \n",
      "     Training Step: 65 Training Loss: 0.6122276186943054 \n",
      "     Training Step: 66 Training Loss: 0.6147018671035767 \n",
      "     Training Step: 67 Training Loss: 0.6210059523582458 \n",
      "     Training Step: 68 Training Loss: 0.6144918203353882 \n",
      "     Training Step: 69 Training Loss: 0.6157570481300354 \n",
      "     Training Step: 70 Training Loss: 0.611133337020874 \n",
      "     Training Step: 71 Training Loss: 0.6129160523414612 \n",
      "     Training Step: 72 Training Loss: 0.6146389245986938 \n",
      "     Training Step: 73 Training Loss: 0.6137664914131165 \n",
      "     Training Step: 74 Training Loss: 0.6105060577392578 \n",
      "     Training Step: 75 Training Loss: 0.610381007194519 \n",
      "     Training Step: 76 Training Loss: 0.6153905391693115 \n",
      "     Training Step: 77 Training Loss: 0.6157442927360535 \n",
      "     Training Step: 78 Training Loss: 0.6105796098709106 \n",
      "     Training Step: 79 Training Loss: 0.6182376742362976 \n",
      "     Training Step: 80 Training Loss: 0.6167421340942383 \n",
      "     Training Step: 81 Training Loss: 0.6121203899383545 \n",
      "     Training Step: 82 Training Loss: 0.6153087019920349 \n",
      "     Training Step: 83 Training Loss: 0.6136165857315063 \n",
      "     Training Step: 84 Training Loss: 0.6151760816574097 \n",
      "     Training Step: 85 Training Loss: 0.6125147938728333 \n",
      "     Training Step: 86 Training Loss: 0.6168978214263916 \n",
      "     Training Step: 87 Training Loss: 0.6107376217842102 \n",
      "     Training Step: 88 Training Loss: 0.609233021736145 \n",
      "     Training Step: 89 Training Loss: 0.6111969351768494 \n",
      "     Training Step: 90 Training Loss: 0.614709734916687 \n",
      "     Training Step: 91 Training Loss: 0.6172105073928833 \n",
      "     Training Step: 92 Training Loss: 0.6177366971969604 \n",
      "     Training Step: 93 Training Loss: 0.6134721040725708 \n",
      "     Training Step: 94 Training Loss: 0.6131835579872131 \n",
      "     Training Step: 95 Training Loss: 0.6129103899002075 \n",
      "     Training Step: 96 Training Loss: 0.6166519522666931 \n",
      "     Training Step: 97 Training Loss: 0.6115915179252625 \n",
      "     Training Step: 98 Training Loss: 0.6114855408668518 \n",
      "     Training Step: 99 Training Loss: 0.6157864928245544 \n",
      "     Training Step: 100 Training Loss: 0.6149620413780212 \n",
      "     Training Step: 101 Training Loss: 0.6094676852226257 \n",
      "     Training Step: 102 Training Loss: 0.6115363836288452 \n",
      "     Training Step: 103 Training Loss: 0.6125343441963196 \n",
      "     Training Step: 104 Training Loss: 0.6116117835044861 \n",
      "     Training Step: 105 Training Loss: 0.6182470321655273 \n",
      "     Training Step: 106 Training Loss: 0.6146883368492126 \n",
      "     Training Step: 107 Training Loss: 0.6082510352134705 \n",
      "     Training Step: 108 Training Loss: 0.6124861240386963 \n",
      "     Training Step: 109 Training Loss: 0.6162652373313904 \n",
      "     Training Step: 110 Training Loss: 0.6196655035018921 \n",
      "     Training Step: 111 Training Loss: 0.6156858205795288 \n",
      "     Training Step: 112 Training Loss: 0.6146539449691772 \n",
      "     Training Step: 113 Training Loss: 0.6115257143974304 \n",
      "     Training Step: 114 Training Loss: 0.614870548248291 \n",
      "     Training Step: 115 Training Loss: 0.612234354019165 \n",
      "     Training Step: 116 Training Loss: 0.6146016120910645 \n",
      "     Training Step: 117 Training Loss: 0.6152849793434143 \n",
      "     Training Step: 118 Training Loss: 0.6106125712394714 \n",
      "     Training Step: 119 Training Loss: 0.6128427982330322 \n",
      "     Training Step: 120 Training Loss: 0.6141522526741028 \n",
      "     Training Step: 121 Training Loss: 0.6132059693336487 \n",
      "     Training Step: 122 Training Loss: 0.6167140007019043 \n",
      "     Training Step: 123 Training Loss: 0.6106632351875305 \n",
      "     Training Step: 124 Training Loss: 0.613737940788269 \n",
      "     Training Step: 125 Training Loss: 0.6171701550483704 \n",
      "     Training Step: 126 Training Loss: 0.6155357956886292 \n",
      "     Training Step: 127 Training Loss: 0.6160440444946289 \n",
      "     Training Step: 128 Training Loss: 0.6116883158683777 \n",
      "     Training Step: 129 Training Loss: 0.6159490346908569 \n",
      "     Training Step: 130 Training Loss: 0.6132866144180298 \n",
      "     Training Step: 131 Training Loss: 0.611784815788269 \n",
      "     Training Step: 132 Training Loss: 0.618866503238678 \n",
      "     Training Step: 133 Training Loss: 0.6135159730911255 \n",
      "     Training Step: 134 Training Loss: 0.6167541146278381 \n",
      "     Training Step: 135 Training Loss: 0.6180920600891113 \n",
      "     Training Step: 136 Training Loss: 0.6139283180236816 \n",
      "     Training Step: 137 Training Loss: 0.6183899641036987 \n",
      "     Training Step: 138 Training Loss: 0.6123025417327881 \n",
      "     Training Step: 139 Training Loss: 0.6134271621704102 \n",
      "     Training Step: 140 Training Loss: 0.6144644618034363 \n",
      "     Training Step: 141 Training Loss: 0.6180224418640137 \n",
      "     Training Step: 142 Training Loss: 0.6151095628738403 \n",
      "     Training Step: 143 Training Loss: 0.6149183511734009 \n",
      "     Training Step: 144 Training Loss: 0.6143426895141602 \n",
      "     Training Step: 145 Training Loss: 0.6138245463371277 \n",
      "     Training Step: 146 Training Loss: 0.6166670322418213 \n",
      "     Training Step: 147 Training Loss: 0.6152403354644775 \n",
      "     Training Step: 148 Training Loss: 0.6185688972473145 \n",
      "     Training Step: 149 Training Loss: 0.6167981624603271 \n",
      "     Training Step: 150 Training Loss: 0.6122499704360962 \n",
      "     Training Step: 151 Training Loss: 0.6147273778915405 \n",
      "     Training Step: 152 Training Loss: 0.6142461895942688 \n",
      "     Training Step: 153 Training Loss: 0.6154702305793762 \n",
      "     Training Step: 154 Training Loss: 0.6142042875289917 \n",
      "     Training Step: 155 Training Loss: 0.6131477355957031 \n",
      "     Training Step: 156 Training Loss: 0.6154412627220154 \n",
      "     Training Step: 157 Training Loss: 0.613066554069519 \n",
      "     Training Step: 158 Training Loss: 0.614215612411499 \n",
      "     Training Step: 159 Training Loss: 0.6162101626396179 \n",
      "     Training Step: 160 Training Loss: 0.6107280254364014 \n",
      "     Training Step: 161 Training Loss: 0.6171436905860901 \n",
      "     Training Step: 162 Training Loss: 0.609401285648346 \n",
      "     Training Step: 163 Training Loss: 0.6134543418884277 \n",
      "     Training Step: 164 Training Loss: 0.6118054986000061 \n",
      "     Training Step: 165 Training Loss: 0.6118292808532715 \n",
      "     Training Step: 166 Training Loss: 0.6194730401039124 \n",
      "     Training Step: 167 Training Loss: 0.6184712648391724 \n",
      "     Training Step: 168 Training Loss: 0.6152755618095398 \n",
      "     Training Step: 169 Training Loss: 0.6128643155097961 \n",
      "     Training Step: 170 Training Loss: 0.6153896450996399 \n",
      "     Training Step: 171 Training Loss: 0.6101393699645996 \n",
      "     Training Step: 172 Training Loss: 0.6140203475952148 \n",
      "     Training Step: 173 Training Loss: 0.6177593469619751 \n",
      "     Training Step: 174 Training Loss: 0.6114453077316284 \n",
      "     Training Step: 175 Training Loss: 0.6154143810272217 \n",
      "     Training Step: 176 Training Loss: 0.612302303314209 \n",
      "     Training Step: 177 Training Loss: 0.6168037056922913 \n",
      "     Training Step: 178 Training Loss: 0.6114205718040466 \n",
      "     Training Step: 179 Training Loss: 0.6155131459236145 \n",
      "     Training Step: 180 Training Loss: 0.6140713691711426 \n",
      "     Training Step: 181 Training Loss: 0.6132028102874756 \n",
      "     Training Step: 182 Training Loss: 0.6121524572372437 \n",
      "     Training Step: 183 Training Loss: 0.609736442565918 \n",
      "     Training Step: 184 Training Loss: 0.617151141166687 \n",
      "     Training Step: 185 Training Loss: 0.6183258891105652 \n",
      "     Training Step: 186 Training Loss: 0.614777147769928 \n",
      "     Training Step: 187 Training Loss: 0.611637532711029 \n",
      "     Training Step: 188 Training Loss: 0.6127957701683044 \n",
      "     Training Step: 189 Training Loss: 0.6143666505813599 \n",
      "     Training Step: 190 Training Loss: 0.6166676878929138 \n",
      "     Training Step: 191 Training Loss: 0.6120687127113342 \n",
      "     Training Step: 192 Training Loss: 0.6116187572479248 \n",
      "     Training Step: 193 Training Loss: 0.6151596307754517 \n",
      "     Training Step: 194 Training Loss: 0.6188403964042664 \n",
      "     Training Step: 195 Training Loss: 0.610197126865387 \n",
      "     Training Step: 196 Training Loss: 0.6174919009208679 \n",
      "     Training Step: 197 Training Loss: 0.6146048903465271 \n",
      "     Training Step: 198 Training Loss: 0.6144437789916992 \n",
      "     Training Step: 199 Training Loss: 0.6135004162788391 \n",
      "     Training Step: 200 Training Loss: 0.6142903566360474 \n",
      "     Training Step: 201 Training Loss: 0.6146750450134277 \n",
      "     Training Step: 202 Training Loss: 0.6118175983428955 \n",
      "     Training Step: 203 Training Loss: 0.616716206073761 \n",
      "     Training Step: 204 Training Loss: 0.618459165096283 \n",
      "     Training Step: 205 Training Loss: 0.6120246648788452 \n",
      "     Training Step: 206 Training Loss: 0.6116554141044617 \n",
      "     Training Step: 207 Training Loss: 0.6123778820037842 \n",
      "     Training Step: 208 Training Loss: 0.6147016882896423 \n",
      "     Training Step: 209 Training Loss: 0.6105756759643555 \n",
      "     Training Step: 210 Training Loss: 0.6132728457450867 \n",
      "     Training Step: 211 Training Loss: 0.613294780254364 \n",
      "     Training Step: 212 Training Loss: 0.6136436462402344 \n",
      "     Training Step: 213 Training Loss: 0.6096883416175842 \n",
      "     Training Step: 214 Training Loss: 0.6100438833236694 \n",
      "     Training Step: 215 Training Loss: 0.6103827357292175 \n",
      "     Training Step: 216 Training Loss: 0.6147709488868713 \n",
      "     Training Step: 217 Training Loss: 0.6178780794143677 \n",
      "     Training Step: 218 Training Loss: 0.6136617064476013 \n",
      "     Training Step: 219 Training Loss: 0.6186758875846863 \n",
      "     Training Step: 220 Training Loss: 0.6105577945709229 \n",
      "     Training Step: 221 Training Loss: 0.6166826486587524 \n",
      "     Training Step: 222 Training Loss: 0.6147971153259277 \n",
      "     Training Step: 223 Training Loss: 0.6141541004180908 \n",
      "     Training Step: 224 Training Loss: 0.6140164136886597 \n",
      "     Training Step: 225 Training Loss: 0.6121436357498169 \n",
      "     Training Step: 226 Training Loss: 0.6127561926841736 \n",
      "     Training Step: 227 Training Loss: 0.6158343553543091 \n",
      "     Training Step: 228 Training Loss: 0.6100151538848877 \n",
      "     Training Step: 229 Training Loss: 0.6118507981300354 \n",
      "     Training Step: 230 Training Loss: 0.6109221577644348 \n",
      "     Training Step: 231 Training Loss: 0.6115355491638184 \n",
      "     Training Step: 232 Training Loss: 0.6176957488059998 \n",
      "     Training Step: 233 Training Loss: 0.6118807196617126 \n",
      "     Training Step: 234 Training Loss: 0.6168082356452942 \n",
      "     Training Step: 235 Training Loss: 0.616361677646637 \n",
      "     Training Step: 236 Training Loss: 0.616122305393219 \n",
      "     Training Step: 237 Training Loss: 0.6127635836601257 \n",
      "     Training Step: 238 Training Loss: 0.610038697719574 \n",
      "     Training Step: 239 Training Loss: 0.6137415766716003 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6129776239395142 \n",
      "     Validation Step: 1 Validation Loss: 0.6157978177070618 \n",
      "     Validation Step: 2 Validation Loss: 0.6180793046951294 \n",
      "     Validation Step: 3 Validation Loss: 0.6136305928230286 \n",
      "     Validation Step: 4 Validation Loss: 0.6152319312095642 \n",
      "     Validation Step: 5 Validation Loss: 0.6148949265480042 \n",
      "     Validation Step: 6 Validation Loss: 0.6136561632156372 \n",
      "     Validation Step: 7 Validation Loss: 0.6115469932556152 \n",
      "     Validation Step: 8 Validation Loss: 0.6105039119720459 \n",
      "     Validation Step: 9 Validation Loss: 0.6111536622047424 \n",
      "     Validation Step: 10 Validation Loss: 0.6118709444999695 \n",
      "     Validation Step: 11 Validation Loss: 0.6185128688812256 \n",
      "     Validation Step: 12 Validation Loss: 0.6183604001998901 \n",
      "     Validation Step: 13 Validation Loss: 0.6100986003875732 \n",
      "     Validation Step: 14 Validation Loss: 0.6153238415718079 \n",
      "     Validation Step: 15 Validation Loss: 0.6128196120262146 \n",
      "     Validation Step: 16 Validation Loss: 0.6145341396331787 \n",
      "     Validation Step: 17 Validation Loss: 0.6176077127456665 \n",
      "     Validation Step: 18 Validation Loss: 0.6136656403541565 \n",
      "     Validation Step: 19 Validation Loss: 0.6173182129859924 \n",
      "     Validation Step: 20 Validation Loss: 0.6177108287811279 \n",
      "     Validation Step: 21 Validation Loss: 0.6111712455749512 \n",
      "     Validation Step: 22 Validation Loss: 0.6116353273391724 \n",
      "     Validation Step: 23 Validation Loss: 0.6150447726249695 \n",
      "     Validation Step: 24 Validation Loss: 0.6141114830970764 \n",
      "     Validation Step: 25 Validation Loss: 0.6101300120353699 \n",
      "     Validation Step: 26 Validation Loss: 0.6104730367660522 \n",
      "     Validation Step: 27 Validation Loss: 0.6106240153312683 \n",
      "     Validation Step: 28 Validation Loss: 0.6160085797309875 \n",
      "     Validation Step: 29 Validation Loss: 0.6156222820281982 \n",
      "     Validation Step: 30 Validation Loss: 0.6184873580932617 \n",
      "     Validation Step: 31 Validation Loss: 0.6075051426887512 \n",
      "     Validation Step: 32 Validation Loss: 0.615585207939148 \n",
      "     Validation Step: 33 Validation Loss: 0.6142539978027344 \n",
      "     Validation Step: 34 Validation Loss: 0.6162527203559875 \n",
      "     Validation Step: 35 Validation Loss: 0.6145751476287842 \n",
      "     Validation Step: 36 Validation Loss: 0.6170380711555481 \n",
      "     Validation Step: 37 Validation Loss: 0.6146379113197327 \n",
      "     Validation Step: 38 Validation Loss: 0.6148352026939392 \n",
      "     Validation Step: 39 Validation Loss: 0.6182504892349243 \n",
      "     Validation Step: 40 Validation Loss: 0.6133012175559998 \n",
      "     Validation Step: 41 Validation Loss: 0.6101529598236084 \n",
      "     Validation Step: 42 Validation Loss: 0.6141998767852783 \n",
      "     Validation Step: 43 Validation Loss: 0.6121214032173157 \n",
      "     Validation Step: 44 Validation Loss: 0.614124596118927 \n",
      "Epoch: 92\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.613817036151886 \n",
      "     Training Step: 1 Training Loss: 0.614692211151123 \n",
      "     Training Step: 2 Training Loss: 0.6174033880233765 \n",
      "     Training Step: 3 Training Loss: 0.6177278757095337 \n",
      "     Training Step: 4 Training Loss: 0.6117826700210571 \n",
      "     Training Step: 5 Training Loss: 0.6130583882331848 \n",
      "     Training Step: 6 Training Loss: 0.618600070476532 \n",
      "     Training Step: 7 Training Loss: 0.6141536235809326 \n",
      "     Training Step: 8 Training Loss: 0.6109263896942139 \n",
      "     Training Step: 9 Training Loss: 0.6147971749305725 \n",
      "     Training Step: 10 Training Loss: 0.6123940348625183 \n",
      "     Training Step: 11 Training Loss: 0.6134740710258484 \n",
      "     Training Step: 12 Training Loss: 0.6107469201087952 \n",
      "     Training Step: 13 Training Loss: 0.61611407995224 \n",
      "     Training Step: 14 Training Loss: 0.6166588664054871 \n",
      "     Training Step: 15 Training Loss: 0.616411566734314 \n",
      "     Training Step: 16 Training Loss: 0.6152884364128113 \n",
      "     Training Step: 17 Training Loss: 0.6132049560546875 \n",
      "     Training Step: 18 Training Loss: 0.6129231452941895 \n",
      "     Training Step: 19 Training Loss: 0.6132842302322388 \n",
      "     Training Step: 20 Training Loss: 0.6149281859397888 \n",
      "     Training Step: 21 Training Loss: 0.6134573817253113 \n",
      "     Training Step: 22 Training Loss: 0.616807758808136 \n",
      "     Training Step: 23 Training Loss: 0.612535297870636 \n",
      "     Training Step: 24 Training Loss: 0.6180279850959778 \n",
      "     Training Step: 25 Training Loss: 0.6122340559959412 \n",
      "     Training Step: 26 Training Loss: 0.6122834086418152 \n",
      "     Training Step: 27 Training Loss: 0.6177606582641602 \n",
      "     Training Step: 28 Training Loss: 0.6114410161972046 \n",
      "     Training Step: 29 Training Loss: 0.611534059047699 \n",
      "     Training Step: 30 Training Loss: 0.6202273368835449 \n",
      "     Training Step: 31 Training Loss: 0.6171990633010864 \n",
      "     Training Step: 32 Training Loss: 0.6157434582710266 \n",
      "     Training Step: 33 Training Loss: 0.6104126572608948 \n",
      "     Training Step: 34 Training Loss: 0.6103919148445129 \n",
      "     Training Step: 35 Training Loss: 0.6153697967529297 \n",
      "     Training Step: 36 Training Loss: 0.6140360236167908 \n",
      "     Training Step: 37 Training Loss: 0.6129134893417358 \n",
      "     Training Step: 38 Training Loss: 0.6151016354560852 \n",
      "     Training Step: 39 Training Loss: 0.6153376698493958 \n",
      "     Training Step: 40 Training Loss: 0.6167122721672058 \n",
      "     Training Step: 41 Training Loss: 0.6163519620895386 \n",
      "     Training Step: 42 Training Loss: 0.6120232939720154 \n",
      "     Training Step: 43 Training Loss: 0.6162381172180176 \n",
      "     Training Step: 44 Training Loss: 0.6197004914283752 \n",
      "     Training Step: 45 Training Loss: 0.6100635528564453 \n",
      "     Training Step: 46 Training Loss: 0.6155961751937866 \n",
      "     Training Step: 47 Training Loss: 0.6125414371490479 \n",
      "     Training Step: 48 Training Loss: 0.6100951433181763 \n",
      "     Training Step: 49 Training Loss: 0.6107381582260132 \n",
      "     Training Step: 50 Training Loss: 0.6112062931060791 \n",
      "     Training Step: 51 Training Loss: 0.6121211051940918 \n",
      "     Training Step: 52 Training Loss: 0.6180526614189148 \n",
      "     Training Step: 53 Training Loss: 0.6154489517211914 \n",
      "     Training Step: 54 Training Loss: 0.6166831254959106 \n",
      "     Training Step: 55 Training Loss: 0.6143471002578735 \n",
      "     Training Step: 56 Training Loss: 0.6199133992195129 \n",
      "     Training Step: 57 Training Loss: 0.6123738288879395 \n",
      "     Training Step: 58 Training Loss: 0.6150679588317871 \n",
      "     Training Step: 59 Training Loss: 0.6118083000183105 \n",
      "     Training Step: 60 Training Loss: 0.6188445687294006 \n",
      "     Training Step: 61 Training Loss: 0.6177860498428345 \n",
      "     Training Step: 62 Training Loss: 0.6144235730171204 \n",
      "     Training Step: 63 Training Loss: 0.6169096827507019 \n",
      "     Training Step: 64 Training Loss: 0.6146758794784546 \n",
      "     Training Step: 65 Training Loss: 0.6161943674087524 \n",
      "     Training Step: 66 Training Loss: 0.6171243786811829 \n",
      "     Training Step: 67 Training Loss: 0.6127500534057617 \n",
      "     Training Step: 68 Training Loss: 0.6143419742584229 \n",
      "     Training Step: 69 Training Loss: 0.6112042665481567 \n",
      "     Training Step: 70 Training Loss: 0.6141206622123718 \n",
      "     Training Step: 71 Training Loss: 0.6131644248962402 \n",
      "     Training Step: 72 Training Loss: 0.6149588227272034 \n",
      "     Training Step: 73 Training Loss: 0.6149156093597412 \n",
      "     Training Step: 74 Training Loss: 0.6126459240913391 \n",
      "     Training Step: 75 Training Loss: 0.6188751459121704 \n",
      "     Training Step: 76 Training Loss: 0.6178174614906311 \n",
      "     Training Step: 77 Training Loss: 0.6115596294403076 \n",
      "     Training Step: 78 Training Loss: 0.6181102395057678 \n",
      "     Training Step: 79 Training Loss: 0.6184173822402954 \n",
      "     Training Step: 80 Training Loss: 0.6156868934631348 \n",
      "     Training Step: 81 Training Loss: 0.6157835721969604 \n",
      "     Training Step: 82 Training Loss: 0.6121623516082764 \n",
      "     Training Step: 83 Training Loss: 0.6133658289909363 \n",
      "     Training Step: 84 Training Loss: 0.6106146574020386 \n",
      "     Training Step: 85 Training Loss: 0.611850380897522 \n",
      "     Training Step: 86 Training Loss: 0.609751284122467 \n",
      "     Training Step: 87 Training Loss: 0.6144953966140747 \n",
      "     Training Step: 88 Training Loss: 0.6160346269607544 \n",
      "     Training Step: 89 Training Loss: 0.6114323735237122 \n",
      "     Training Step: 90 Training Loss: 0.6144837141036987 \n",
      "     Training Step: 91 Training Loss: 0.6156922578811646 \n",
      "     Training Step: 92 Training Loss: 0.6115217804908752 \n",
      "     Training Step: 93 Training Loss: 0.6128692030906677 \n",
      "     Training Step: 94 Training Loss: 0.6082350015640259 \n",
      "     Training Step: 95 Training Loss: 0.6113909482955933 \n",
      "     Training Step: 96 Training Loss: 0.6137439608573914 \n",
      "     Training Step: 97 Training Loss: 0.6151710748672485 \n",
      "     Training Step: 98 Training Loss: 0.6140338778495789 \n",
      "     Training Step: 99 Training Loss: 0.6117743849754333 \n",
      "     Training Step: 100 Training Loss: 0.6101124286651611 \n",
      "     Training Step: 101 Training Loss: 0.6168134212493896 \n",
      "     Training Step: 102 Training Loss: 0.6181219816207886 \n",
      "     Training Step: 103 Training Loss: 0.61113041639328 \n",
      "     Training Step: 104 Training Loss: 0.6105586290359497 \n",
      "     Training Step: 105 Training Loss: 0.6139056086540222 \n",
      "     Training Step: 106 Training Loss: 0.615402102470398 \n",
      "     Training Step: 107 Training Loss: 0.6165019273757935 \n",
      "     Training Step: 108 Training Loss: 0.6154235601425171 \n",
      "     Training Step: 109 Training Loss: 0.6152726411819458 \n",
      "     Training Step: 110 Training Loss: 0.6132107973098755 \n",
      "     Training Step: 111 Training Loss: 0.6133304834365845 \n",
      "     Training Step: 112 Training Loss: 0.6147490739822388 \n",
      "     Training Step: 113 Training Loss: 0.6106883883476257 \n",
      "     Training Step: 114 Training Loss: 0.6142075657844543 \n",
      "     Training Step: 115 Training Loss: 0.6143669486045837 \n",
      "     Training Step: 116 Training Loss: 0.617135226726532 \n",
      "     Training Step: 117 Training Loss: 0.6131272912025452 \n",
      "     Training Step: 118 Training Loss: 0.6142227649688721 \n",
      "     Training Step: 119 Training Loss: 0.6118911504745483 \n",
      "     Training Step: 120 Training Loss: 0.6121372580528259 \n",
      "     Training Step: 121 Training Loss: 0.6154267191886902 \n",
      "     Training Step: 122 Training Loss: 0.6184763312339783 \n",
      "     Training Step: 123 Training Loss: 0.6136375665664673 \n",
      "     Training Step: 124 Training Loss: 0.613624095916748 \n",
      "     Training Step: 125 Training Loss: 0.6168990135192871 \n",
      "     Training Step: 126 Training Loss: 0.6182397603988647 \n",
      "     Training Step: 127 Training Loss: 0.6115273237228394 \n",
      "     Training Step: 128 Training Loss: 0.6116148829460144 \n",
      "     Training Step: 129 Training Loss: 0.6124670505523682 \n",
      "     Training Step: 130 Training Loss: 0.6137419939041138 \n",
      "     Training Step: 131 Training Loss: 0.6146365404129028 \n",
      "     Training Step: 132 Training Loss: 0.6128721833229065 \n",
      "     Training Step: 133 Training Loss: 0.6177008748054504 \n",
      "     Training Step: 134 Training Loss: 0.6129721999168396 \n",
      "     Training Step: 135 Training Loss: 0.6142410039901733 \n",
      "     Training Step: 136 Training Loss: 0.611481785774231 \n",
      "     Training Step: 137 Training Loss: 0.6127662658691406 \n",
      "     Training Step: 138 Training Loss: 0.6128044724464417 \n",
      "     Training Step: 139 Training Loss: 0.6093915700912476 \n",
      "     Training Step: 140 Training Loss: 0.6146146655082703 \n",
      "     Training Step: 141 Training Loss: 0.6122298240661621 \n",
      "     Training Step: 142 Training Loss: 0.6122266054153442 \n",
      "     Training Step: 143 Training Loss: 0.6116864681243896 \n",
      "     Training Step: 144 Training Loss: 0.6136568784713745 \n",
      "     Training Step: 145 Training Loss: 0.6155495047569275 \n",
      "     Training Step: 146 Training Loss: 0.6099726557731628 \n",
      "     Training Step: 147 Training Loss: 0.6154168844223022 \n",
      "     Training Step: 148 Training Loss: 0.6171289682388306 \n",
      "     Training Step: 149 Training Loss: 0.6155460476875305 \n",
      "     Training Step: 150 Training Loss: 0.6115772724151611 \n",
      "     Training Step: 151 Training Loss: 0.6128390431404114 \n",
      "     Training Step: 152 Training Loss: 0.6162058711051941 \n",
      "     Training Step: 153 Training Loss: 0.6116353273391724 \n",
      "     Training Step: 154 Training Loss: 0.6114214658737183 \n",
      "     Training Step: 155 Training Loss: 0.6157516241073608 \n",
      "     Training Step: 156 Training Loss: 0.6092427968978882 \n",
      "     Training Step: 157 Training Loss: 0.6132969260215759 \n",
      "     Training Step: 158 Training Loss: 0.6164317727088928 \n",
      "     Training Step: 159 Training Loss: 0.6139187216758728 \n",
      "     Training Step: 160 Training Loss: 0.6167343854904175 \n",
      "     Training Step: 161 Training Loss: 0.6157439947128296 \n",
      "     Training Step: 162 Training Loss: 0.6154754161834717 \n",
      "     Training Step: 163 Training Loss: 0.6135920882225037 \n",
      "     Training Step: 164 Training Loss: 0.6183253526687622 \n",
      "     Training Step: 165 Training Loss: 0.6127590537071228 \n",
      "     Training Step: 166 Training Loss: 0.611649215221405 \n",
      "     Training Step: 167 Training Loss: 0.6121757626533508 \n",
      "     Training Step: 168 Training Loss: 0.6116251349449158 \n",
      "     Training Step: 169 Training Loss: 0.6196160316467285 \n",
      "     Training Step: 170 Training Loss: 0.614649772644043 \n",
      "     Training Step: 171 Training Loss: 0.6158275008201599 \n",
      "     Training Step: 172 Training Loss: 0.6167211532592773 \n",
      "     Training Step: 173 Training Loss: 0.6182116866111755 \n",
      "     Training Step: 174 Training Loss: 0.6130704879760742 \n",
      "     Training Step: 175 Training Loss: 0.6176095604896545 \n",
      "     Training Step: 176 Training Loss: 0.6095089316368103 \n",
      "     Training Step: 177 Training Loss: 0.6105431318283081 \n",
      "     Training Step: 178 Training Loss: 0.6143365502357483 \n",
      "     Training Step: 179 Training Loss: 0.611846923828125 \n",
      "     Training Step: 180 Training Loss: 0.6134347915649414 \n",
      "     Training Step: 181 Training Loss: 0.6159440279006958 \n",
      "     Training Step: 182 Training Loss: 0.6122980117797852 \n",
      "     Training Step: 183 Training Loss: 0.6160046458244324 \n",
      "     Training Step: 184 Training Loss: 0.6168131232261658 \n",
      "     Training Step: 185 Training Loss: 0.6176947951316833 \n",
      "     Training Step: 186 Training Loss: 0.6155427694320679 \n",
      "     Training Step: 187 Training Loss: 0.6106922626495361 \n",
      "     Training Step: 188 Training Loss: 0.6146106719970703 \n",
      "     Training Step: 189 Training Loss: 0.6147714257240295 \n",
      "     Training Step: 190 Training Loss: 0.6144397854804993 \n",
      "     Training Step: 191 Training Loss: 0.6106055378913879 \n",
      "     Training Step: 192 Training Loss: 0.6171642541885376 \n",
      "     Training Step: 193 Training Loss: 0.6184409856796265 \n",
      "     Training Step: 194 Training Loss: 0.6123828291893005 \n",
      "     Training Step: 195 Training Loss: 0.6105785965919495 \n",
      "     Training Step: 196 Training Loss: 0.6148683428764343 \n",
      "     Training Step: 197 Training Loss: 0.6138831973075867 \n",
      "     Training Step: 198 Training Loss: 0.614727258682251 \n",
      "     Training Step: 199 Training Loss: 0.6100665926933289 \n",
      "     Training Step: 200 Training Loss: 0.6140133738517761 \n",
      "     Training Step: 201 Training Loss: 0.6146687865257263 \n",
      "     Training Step: 202 Training Loss: 0.6150875091552734 \n",
      "     Training Step: 203 Training Loss: 0.6152858734130859 \n",
      "     Training Step: 204 Training Loss: 0.6168164610862732 \n",
      "     Training Step: 205 Training Loss: 0.6118344068527222 \n",
      "     Training Step: 206 Training Loss: 0.6174992322921753 \n",
      "     Training Step: 207 Training Loss: 0.6114122271537781 \n",
      "     Training Step: 208 Training Loss: 0.6125179529190063 \n",
      "     Training Step: 209 Training Loss: 0.6146032810211182 \n",
      "     Training Step: 210 Training Loss: 0.6166554689407349 \n",
      "     Training Step: 211 Training Loss: 0.6166831254959106 \n",
      "     Training Step: 212 Training Loss: 0.6104936599731445 \n",
      "     Training Step: 213 Training Loss: 0.6167166829109192 \n",
      "     Training Step: 214 Training Loss: 0.6140784621238708 \n",
      "     Training Step: 215 Training Loss: 0.6146823167800903 \n",
      "     Training Step: 216 Training Loss: 0.6185786128044128 \n",
      "     Training Step: 217 Training Loss: 0.6152941584587097 \n",
      "     Training Step: 218 Training Loss: 0.6102080941200256 \n",
      "     Training Step: 219 Training Loss: 0.6135241985321045 \n",
      "     Training Step: 220 Training Loss: 0.6166086792945862 \n",
      "     Training Step: 221 Training Loss: 0.6147002577781677 \n",
      "     Training Step: 222 Training Loss: 0.6133041977882385 \n",
      "     Training Step: 223 Training Loss: 0.6097329258918762 \n",
      "     Training Step: 224 Training Loss: 0.6124956607818604 \n",
      "     Training Step: 225 Training Loss: 0.6151729226112366 \n",
      "     Training Step: 226 Training Loss: 0.6132845282554626 \n",
      "     Training Step: 227 Training Loss: 0.6152442693710327 \n",
      "     Training Step: 228 Training Loss: 0.6194677352905273 \n",
      "     Training Step: 229 Training Loss: 0.6147096157073975 \n",
      "     Training Step: 230 Training Loss: 0.6108741760253906 \n",
      "     Training Step: 231 Training Loss: 0.6144527792930603 \n",
      "     Training Step: 232 Training Loss: 0.6141546964645386 \n",
      "     Training Step: 233 Training Loss: 0.6134971976280212 \n",
      "     Training Step: 234 Training Loss: 0.6120653748512268 \n",
      "     Training Step: 235 Training Loss: 0.6142874360084534 \n",
      "     Training Step: 236 Training Loss: 0.6131834983825684 \n",
      "     Training Step: 237 Training Loss: 0.6209616661071777 \n",
      "     Training Step: 238 Training Loss: 0.6137667298316956 \n",
      "     Training Step: 239 Training Loss: 0.6097352504730225 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6183481812477112 \n",
      "     Validation Step: 1 Validation Loss: 0.6111795902252197 \n",
      "     Validation Step: 2 Validation Loss: 0.6175928115844727 \n",
      "     Validation Step: 3 Validation Loss: 0.6180651783943176 \n",
      "     Validation Step: 4 Validation Loss: 0.6116402745246887 \n",
      "     Validation Step: 5 Validation Loss: 0.6146376132965088 \n",
      "     Validation Step: 6 Validation Loss: 0.6184706091880798 \n",
      "     Validation Step: 7 Validation Loss: 0.6101158857345581 \n",
      "     Validation Step: 8 Validation Loss: 0.6157947778701782 \n",
      "     Validation Step: 9 Validation Loss: 0.6141297817230225 \n",
      "     Validation Step: 10 Validation Loss: 0.6121291518211365 \n",
      "     Validation Step: 11 Validation Loss: 0.6111661195755005 \n",
      "     Validation Step: 12 Validation Loss: 0.6150445938110352 \n",
      "     Validation Step: 13 Validation Loss: 0.6136566996574402 \n",
      "     Validation Step: 14 Validation Loss: 0.61420077085495 \n",
      "     Validation Step: 15 Validation Loss: 0.6136296987533569 \n",
      "     Validation Step: 16 Validation Loss: 0.6152266263961792 \n",
      "     Validation Step: 17 Validation Loss: 0.6156162023544312 \n",
      "     Validation Step: 18 Validation Loss: 0.6106336712837219 \n",
      "     Validation Step: 19 Validation Loss: 0.617699384689331 \n",
      "     Validation Step: 20 Validation Loss: 0.611880362033844 \n",
      "     Validation Step: 21 Validation Loss: 0.6136687397956848 \n",
      "     Validation Step: 22 Validation Loss: 0.6182365417480469 \n",
      "     Validation Step: 23 Validation Loss: 0.6148337125778198 \n",
      "     Validation Step: 24 Validation Loss: 0.6104844212532043 \n",
      "     Validation Step: 25 Validation Loss: 0.616243839263916 \n",
      "     Validation Step: 26 Validation Loss: 0.6148921251296997 \n",
      "     Validation Step: 27 Validation Loss: 0.6170240640640259 \n",
      "     Validation Step: 28 Validation Loss: 0.6129844188690186 \n",
      "     Validation Step: 29 Validation Loss: 0.6145327091217041 \n",
      "     Validation Step: 30 Validation Loss: 0.6153214573860168 \n",
      "     Validation Step: 31 Validation Loss: 0.6101437211036682 \n",
      "     Validation Step: 32 Validation Loss: 0.6155763864517212 \n",
      "     Validation Step: 33 Validation Loss: 0.6142529845237732 \n",
      "     Validation Step: 34 Validation Loss: 0.6075296998023987 \n",
      "     Validation Step: 35 Validation Loss: 0.614567756652832 \n",
      "     Validation Step: 36 Validation Loss: 0.6115567684173584 \n",
      "     Validation Step: 37 Validation Loss: 0.6184958815574646 \n",
      "     Validation Step: 38 Validation Loss: 0.6141127347946167 \n",
      "     Validation Step: 39 Validation Loss: 0.6173034906387329 \n",
      "     Validation Step: 40 Validation Loss: 0.6128250360488892 \n",
      "     Validation Step: 41 Validation Loss: 0.6105163097381592 \n",
      "     Validation Step: 42 Validation Loss: 0.6133018732070923 \n",
      "     Validation Step: 43 Validation Loss: 0.616002082824707 \n",
      "     Validation Step: 44 Validation Loss: 0.6101620197296143 \n",
      "Epoch: 93\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6184647083282471 \n",
      "     Training Step: 1 Training Loss: 0.6166699528694153 \n",
      "     Training Step: 2 Training Loss: 0.6196140646934509 \n",
      "     Training Step: 3 Training Loss: 0.613210141658783 \n",
      "     Training Step: 4 Training Loss: 0.6127212047576904 \n",
      "     Training Step: 5 Training Loss: 0.6148707270622253 \n",
      "     Training Step: 6 Training Loss: 0.6100291609764099 \n",
      "     Training Step: 7 Training Loss: 0.6164077520370483 \n",
      "     Training Step: 8 Training Loss: 0.6130738854408264 \n",
      "     Training Step: 9 Training Loss: 0.6140264868736267 \n",
      "     Training Step: 10 Training Loss: 0.6146693229675293 \n",
      "     Training Step: 11 Training Loss: 0.6154075264930725 \n",
      "     Training Step: 12 Training Loss: 0.6157589554786682 \n",
      "     Training Step: 13 Training Loss: 0.6121394038200378 \n",
      "     Training Step: 14 Training Loss: 0.6157866716384888 \n",
      "     Training Step: 15 Training Loss: 0.6146382689476013 \n",
      "     Training Step: 16 Training Loss: 0.6107227802276611 \n",
      "     Training Step: 17 Training Loss: 0.6167206764221191 \n",
      "     Training Step: 18 Training Loss: 0.6177175641059875 \n",
      "     Training Step: 19 Training Loss: 0.615100622177124 \n",
      "     Training Step: 20 Training Loss: 0.6162071228027344 \n",
      "     Training Step: 21 Training Loss: 0.6115367412567139 \n",
      "     Training Step: 22 Training Loss: 0.6173747181892395 \n",
      "     Training Step: 23 Training Loss: 0.6134300827980042 \n",
      "     Training Step: 24 Training Loss: 0.6135004758834839 \n",
      "     Training Step: 25 Training Loss: 0.6176815629005432 \n",
      "     Training Step: 26 Training Loss: 0.6130698323249817 \n",
      "     Training Step: 27 Training Loss: 0.6131529808044434 \n",
      "     Training Step: 28 Training Loss: 0.6155124306678772 \n",
      "     Training Step: 29 Training Loss: 0.6158285140991211 \n",
      "     Training Step: 30 Training Loss: 0.6116023659706116 \n",
      "     Training Step: 31 Training Loss: 0.6183125972747803 \n",
      "     Training Step: 32 Training Loss: 0.610393226146698 \n",
      "     Training Step: 33 Training Loss: 0.6149160265922546 \n",
      "     Training Step: 34 Training Loss: 0.611844539642334 \n",
      "     Training Step: 35 Training Loss: 0.6153991222381592 \n",
      "     Training Step: 36 Training Loss: 0.6153027415275574 \n",
      "     Training Step: 37 Training Loss: 0.6123727560043335 \n",
      "     Training Step: 38 Training Loss: 0.6131141781806946 \n",
      "     Training Step: 39 Training Loss: 0.6117900013923645 \n",
      "     Training Step: 40 Training Loss: 0.6137416362762451 \n",
      "     Training Step: 41 Training Loss: 0.6161240339279175 \n",
      "     Training Step: 42 Training Loss: 0.6167842149734497 \n",
      "     Training Step: 43 Training Loss: 0.6159578561782837 \n",
      "     Training Step: 44 Training Loss: 0.6177083253860474 \n",
      "     Training Step: 45 Training Loss: 0.6122933626174927 \n",
      "     Training Step: 46 Training Loss: 0.6160279512405396 \n",
      "     Training Step: 47 Training Loss: 0.6116434335708618 \n",
      "     Training Step: 48 Training Loss: 0.6118133068084717 \n",
      "     Training Step: 49 Training Loss: 0.6108918786048889 \n",
      "     Training Step: 50 Training Loss: 0.6114263534545898 \n",
      "     Training Step: 51 Training Loss: 0.6157398223876953 \n",
      "     Training Step: 52 Training Loss: 0.6147254109382629 \n",
      "     Training Step: 53 Training Loss: 0.6147705912590027 \n",
      "     Training Step: 54 Training Loss: 0.6116288900375366 \n",
      "     Training Step: 55 Training Loss: 0.6152898073196411 \n",
      "     Training Step: 56 Training Loss: 0.6146032810211182 \n",
      "     Training Step: 57 Training Loss: 0.6133027672767639 \n",
      "     Training Step: 58 Training Loss: 0.6122267842292786 \n",
      "     Training Step: 59 Training Loss: 0.6096935272216797 \n",
      "     Training Step: 60 Training Loss: 0.6153426766395569 \n",
      "     Training Step: 61 Training Loss: 0.614693284034729 \n",
      "     Training Step: 62 Training Loss: 0.6126371026039124 \n",
      "     Training Step: 63 Training Loss: 0.6106773018836975 \n",
      "     Training Step: 64 Training Loss: 0.6140337586402893 \n",
      "     Training Step: 65 Training Loss: 0.6100437641143799 \n",
      "     Training Step: 66 Training Loss: 0.6128634214401245 \n",
      "     Training Step: 67 Training Loss: 0.6143596172332764 \n",
      "     Training Step: 68 Training Loss: 0.6135083436965942 \n",
      "     Training Step: 69 Training Loss: 0.6101619601249695 \n",
      "     Training Step: 70 Training Loss: 0.6138193011283875 \n",
      "     Training Step: 71 Training Loss: 0.6120044589042664 \n",
      "     Training Step: 72 Training Loss: 0.6124840974807739 \n",
      "     Training Step: 73 Training Loss: 0.6155661940574646 \n",
      "     Training Step: 74 Training Loss: 0.6129159927368164 \n",
      "     Training Step: 75 Training Loss: 0.6167526841163635 \n",
      "     Training Step: 76 Training Loss: 0.613649845123291 \n",
      "     Training Step: 77 Training Loss: 0.6140825152397156 \n",
      "     Training Step: 78 Training Loss: 0.6143772006034851 \n",
      "     Training Step: 79 Training Loss: 0.6123812198638916 \n",
      "     Training Step: 80 Training Loss: 0.6147944927215576 \n",
      "     Training Step: 81 Training Loss: 0.6120739579200745 \n",
      "     Training Step: 82 Training Loss: 0.6159983277320862 \n",
      "     Training Step: 83 Training Loss: 0.6170915961265564 \n",
      "     Training Step: 84 Training Loss: 0.615263044834137 \n",
      "     Training Step: 85 Training Loss: 0.6146061420440674 \n",
      "     Training Step: 86 Training Loss: 0.6167164444923401 \n",
      "     Training Step: 87 Training Loss: 0.6132106184959412 \n",
      "     Training Step: 88 Training Loss: 0.6177813410758972 \n",
      "     Training Step: 89 Training Loss: 0.6152477860450745 \n",
      "     Training Step: 90 Training Loss: 0.6134745478630066 \n",
      "     Training Step: 91 Training Loss: 0.6114657521247864 \n",
      "     Training Step: 92 Training Loss: 0.6115433573722839 \n",
      "     Training Step: 93 Training Loss: 0.6105067729949951 \n",
      "     Training Step: 94 Training Loss: 0.6182219982147217 \n",
      "     Training Step: 95 Training Loss: 0.6136360168457031 \n",
      "     Training Step: 96 Training Loss: 0.6144406199455261 \n",
      "     Training Step: 97 Training Loss: 0.6146859526634216 \n",
      "     Training Step: 98 Training Loss: 0.6178292632102966 \n",
      "     Training Step: 99 Training Loss: 0.6186245083808899 \n",
      "     Training Step: 100 Training Loss: 0.6093982458114624 \n",
      "     Training Step: 101 Training Loss: 0.6125137209892273 \n",
      "     Training Step: 102 Training Loss: 0.6132894158363342 \n",
      "     Training Step: 103 Training Loss: 0.6125311255455017 \n",
      "     Training Step: 104 Training Loss: 0.6094581484794617 \n",
      "     Training Step: 105 Training Loss: 0.6147481799125671 \n",
      "     Training Step: 106 Training Loss: 0.6124624013900757 \n",
      "     Training Step: 107 Training Loss: 0.6166703701019287 \n",
      "     Training Step: 108 Training Loss: 0.6113952994346619 \n",
      "     Training Step: 109 Training Loss: 0.6101256608963013 \n",
      "     Training Step: 110 Training Loss: 0.6180630326271057 \n",
      "     Training Step: 111 Training Loss: 0.6146147847175598 \n",
      "     Training Step: 112 Training Loss: 0.6167252063751221 \n",
      "     Training Step: 113 Training Loss: 0.6139235496520996 \n",
      "     Training Step: 114 Training Loss: 0.6164958477020264 \n",
      "     Training Step: 115 Training Loss: 0.61212557554245 \n",
      "     Training Step: 116 Training Loss: 0.6151501536369324 \n",
      "     Training Step: 117 Training Loss: 0.613473117351532 \n",
      "     Training Step: 118 Training Loss: 0.6147046685218811 \n",
      "     Training Step: 119 Training Loss: 0.6168861985206604 \n",
      "     Training Step: 120 Training Loss: 0.6142466068267822 \n",
      "     Training Step: 121 Training Loss: 0.6133254766464233 \n",
      "     Training Step: 122 Training Loss: 0.616795539855957 \n",
      "     Training Step: 123 Training Loss: 0.6105888485908508 \n",
      "     Training Step: 124 Training Loss: 0.6166138648986816 \n",
      "     Training Step: 125 Training Loss: 0.6194375157356262 \n",
      "     Training Step: 126 Training Loss: 0.6149346828460693 \n",
      "     Training Step: 127 Training Loss: 0.6183957457542419 \n",
      "     Training Step: 128 Training Loss: 0.6117945313453674 \n",
      "     Training Step: 129 Training Loss: 0.6164078712463379 \n",
      "     Training Step: 130 Training Loss: 0.6137421727180481 \n",
      "     Training Step: 131 Training Loss: 0.6116334199905396 \n",
      "     Training Step: 132 Training Loss: 0.6188216209411621 \n",
      "     Training Step: 133 Training Loss: 0.6171370148658752 \n",
      "     Training Step: 134 Training Loss: 0.6127733588218689 \n",
      "     Training Step: 135 Training Loss: 0.6107622981071472 \n",
      "     Training Step: 136 Training Loss: 0.6139275431632996 \n",
      "     Training Step: 137 Training Loss: 0.6163434982299805 \n",
      "     Training Step: 138 Training Loss: 0.6121749877929688 \n",
      "     Training Step: 139 Training Loss: 0.6105851531028748 \n",
      "     Training Step: 140 Training Loss: 0.6128742694854736 \n",
      "     Training Step: 141 Training Loss: 0.609727680683136 \n",
      "     Training Step: 142 Training Loss: 0.6138895750045776 \n",
      "     Training Step: 143 Training Loss: 0.6132749915122986 \n",
      "     Training Step: 144 Training Loss: 0.6143592000007629 \n",
      "     Training Step: 145 Training Loss: 0.6153016090393066 \n",
      "     Training Step: 146 Training Loss: 0.6100240349769592 \n",
      "     Training Step: 147 Training Loss: 0.6166940331459045 \n",
      "     Training Step: 148 Training Loss: 0.611688494682312 \n",
      "     Training Step: 149 Training Loss: 0.6153827905654907 \n",
      "     Training Step: 150 Training Loss: 0.610883891582489 \n",
      "     Training Step: 151 Training Loss: 0.6150968074798584 \n",
      "     Training Step: 152 Training Loss: 0.6169585585594177 \n",
      "     Training Step: 153 Training Loss: 0.6129657626152039 \n",
      "     Training Step: 154 Training Loss: 0.6082565188407898 \n",
      "     Training Step: 155 Training Loss: 0.6144814491271973 \n",
      "     Training Step: 156 Training Loss: 0.6156097650527954 \n",
      "     Training Step: 157 Training Loss: 0.616208553314209 \n",
      "     Training Step: 158 Training Loss: 0.6128045916557312 \n",
      "     Training Step: 159 Training Loss: 0.6154773831367493 \n",
      "     Training Step: 160 Training Loss: 0.6105929017066956 \n",
      "     Training Step: 161 Training Loss: 0.6100741028785706 \n",
      "     Training Step: 162 Training Loss: 0.6128408908843994 \n",
      "     Training Step: 163 Training Loss: 0.6144219636917114 \n",
      "     Training Step: 164 Training Loss: 0.614154040813446 \n",
      "     Training Step: 165 Training Loss: 0.614285409450531 \n",
      "     Training Step: 166 Training Loss: 0.6168190836906433 \n",
      "     Training Step: 167 Training Loss: 0.6155386567115784 \n",
      "     Training Step: 168 Training Loss: 0.616249144077301 \n",
      "     Training Step: 169 Training Loss: 0.612380862236023 \n",
      "     Training Step: 170 Training Loss: 0.6141510605812073 \n",
      "     Training Step: 171 Training Loss: 0.6149556040763855 \n",
      "     Training Step: 172 Training Loss: 0.614342987537384 \n",
      "     Training Step: 173 Training Loss: 0.6197004318237305 \n",
      "     Training Step: 174 Training Loss: 0.6118424534797668 \n",
      "     Training Step: 175 Training Loss: 0.6132842898368835 \n",
      "     Training Step: 176 Training Loss: 0.6171310544013977 \n",
      "     Training Step: 177 Training Loss: 0.6177467107772827 \n",
      "     Training Step: 178 Training Loss: 0.6146551370620728 \n",
      "     Training Step: 179 Training Loss: 0.6115083694458008 \n",
      "     Training Step: 180 Training Loss: 0.6144989132881165 \n",
      "     Training Step: 181 Training Loss: 0.6185622811317444 \n",
      "     Training Step: 182 Training Loss: 0.6146736145019531 \n",
      "     Training Step: 183 Training Loss: 0.6115776896476746 \n",
      "     Training Step: 184 Training Loss: 0.6156828999519348 \n",
      "     Training Step: 185 Training Loss: 0.6092507243156433 \n",
      "     Training Step: 186 Training Loss: 0.6135905981063843 \n",
      "     Training Step: 187 Training Loss: 0.615443229675293 \n",
      "     Training Step: 188 Training Loss: 0.6171708703041077 \n",
      "     Training Step: 189 Training Loss: 0.6122319102287292 \n",
      "     Training Step: 190 Training Loss: 0.618462860584259 \n",
      "     Training Step: 191 Training Loss: 0.6111376881599426 \n",
      "     Training Step: 192 Training Loss: 0.6172060966491699 \n",
      "     Training Step: 193 Training Loss: 0.6188842058181763 \n",
      "     Training Step: 194 Training Loss: 0.6118257641792297 \n",
      "     Training Step: 195 Training Loss: 0.6105033755302429 \n",
      "     Training Step: 196 Training Loss: 0.6166728138923645 \n",
      "     Training Step: 197 Training Loss: 0.6180993914604187 \n",
      "     Training Step: 198 Training Loss: 0.6114458441734314 \n",
      "     Training Step: 199 Training Loss: 0.6106747388839722 \n",
      "     Training Step: 200 Training Loss: 0.6151713132858276 \n",
      "     Training Step: 201 Training Loss: 0.6180145144462585 \n",
      "     Training Step: 202 Training Loss: 0.611649751663208 \n",
      "     Training Step: 203 Training Loss: 0.6115404367446899 \n",
      "     Training Step: 204 Training Loss: 0.6154196262359619 \n",
      "     Training Step: 205 Training Loss: 0.6147010326385498 \n",
      "     Training Step: 206 Training Loss: 0.6121581196784973 \n",
      "     Training Step: 207 Training Loss: 0.6198815107345581 \n",
      "     Training Step: 208 Training Loss: 0.6209354996681213 \n",
      "     Training Step: 209 Training Loss: 0.6142175197601318 \n",
      "     Training Step: 210 Training Loss: 0.6122965216636658 \n",
      "     Training Step: 211 Training Loss: 0.611896812915802 \n",
      "     Training Step: 212 Training Loss: 0.6157405376434326 \n",
      "     Training Step: 213 Training Loss: 0.6141111254692078 \n",
      "     Training Step: 214 Training Loss: 0.6131919622421265 \n",
      "     Training Step: 215 Training Loss: 0.610414981842041 \n",
      "     Training Step: 216 Training Loss: 0.6129142045974731 \n",
      "     Training Step: 217 Training Loss: 0.6150658130645752 \n",
      "     Training Step: 218 Training Loss: 0.6137637495994568 \n",
      "     Training Step: 219 Training Loss: 0.6127517819404602 \n",
      "     Training Step: 220 Training Loss: 0.6097263693809509 \n",
      "     Training Step: 221 Training Loss: 0.613615870475769 \n",
      "     Training Step: 222 Training Loss: 0.6105816960334778 \n",
      "     Training Step: 223 Training Loss: 0.6111510396003723 \n",
      "     Training Step: 224 Training Loss: 0.6168394088745117 \n",
      "     Training Step: 225 Training Loss: 0.6175286173820496 \n",
      "     Training Step: 226 Training Loss: 0.6182671785354614 \n",
      "     Training Step: 227 Training Loss: 0.6125302314758301 \n",
      "     Training Step: 228 Training Loss: 0.61766517162323 \n",
      "     Training Step: 229 Training Loss: 0.6153914928436279 \n",
      "     Training Step: 230 Training Loss: 0.6133597493171692 \n",
      "     Training Step: 231 Training Loss: 0.6202158331871033 \n",
      "     Training Step: 232 Training Loss: 0.6156741380691528 \n",
      "     Training Step: 233 Training Loss: 0.6122560501098633 \n",
      "     Training Step: 234 Training Loss: 0.6112396121025085 \n",
      "     Training Step: 235 Training Loss: 0.6180477738380432 \n",
      "     Training Step: 236 Training Loss: 0.6144716739654541 \n",
      "     Training Step: 237 Training Loss: 0.6114572286605835 \n",
      "     Training Step: 238 Training Loss: 0.6140261292457581 \n",
      "     Training Step: 239 Training Loss: 0.6142074465751648 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6136470437049866 \n",
      "     Validation Step: 1 Validation Loss: 0.6182090044021606 \n",
      "     Validation Step: 2 Validation Loss: 0.611907958984375 \n",
      "     Validation Step: 3 Validation Loss: 0.6133278012275696 \n",
      "     Validation Step: 4 Validation Loss: 0.6112011075019836 \n",
      "     Validation Step: 5 Validation Loss: 0.6116692423820496 \n",
      "     Validation Step: 6 Validation Loss: 0.614140510559082 \n",
      "     Validation Step: 7 Validation Loss: 0.6146465539932251 \n",
      "     Validation Step: 8 Validation Loss: 0.6153126358985901 \n",
      "     Validation Step: 9 Validation Loss: 0.610148549079895 \n",
      "     Validation Step: 10 Validation Loss: 0.6105456948280334 \n",
      "     Validation Step: 11 Validation Loss: 0.6159793138504028 \n",
      "     Validation Step: 12 Validation Loss: 0.6170060038566589 \n",
      "     Validation Step: 13 Validation Loss: 0.6184594035148621 \n",
      "     Validation Step: 14 Validation Loss: 0.6102045774459839 \n",
      "     Validation Step: 15 Validation Loss: 0.6106643676757812 \n",
      "     Validation Step: 16 Validation Loss: 0.6172930002212524 \n",
      "     Validation Step: 17 Validation Loss: 0.6105145812034607 \n",
      "     Validation Step: 18 Validation Loss: 0.6155654191970825 \n",
      "     Validation Step: 19 Validation Loss: 0.6150335073471069 \n",
      "     Validation Step: 20 Validation Loss: 0.6142722964286804 \n",
      "     Validation Step: 21 Validation Loss: 0.6183212995529175 \n",
      "     Validation Step: 22 Validation Loss: 0.6180448532104492 \n",
      "     Validation Step: 23 Validation Loss: 0.6136730313301086 \n",
      "     Validation Step: 24 Validation Loss: 0.612153172492981 \n",
      "     Validation Step: 25 Validation Loss: 0.61285799741745 \n",
      "     Validation Step: 26 Validation Loss: 0.6157869100570679 \n",
      "     Validation Step: 27 Validation Loss: 0.607586145401001 \n",
      "     Validation Step: 28 Validation Loss: 0.6176766753196716 \n",
      "     Validation Step: 29 Validation Loss: 0.6145703792572021 \n",
      "     Validation Step: 30 Validation Loss: 0.6162365078926086 \n",
      "     Validation Step: 31 Validation Loss: 0.6156068444252014 \n",
      "     Validation Step: 32 Validation Loss: 0.6141310930252075 \n",
      "     Validation Step: 33 Validation Loss: 0.614843487739563 \n",
      "     Validation Step: 34 Validation Loss: 0.6152256727218628 \n",
      "     Validation Step: 35 Validation Loss: 0.6184741258621216 \n",
      "     Validation Step: 36 Validation Loss: 0.611187219619751 \n",
      "     Validation Step: 37 Validation Loss: 0.6136582493782043 \n",
      "     Validation Step: 38 Validation Loss: 0.6175922751426697 \n",
      "     Validation Step: 39 Validation Loss: 0.6145498156547546 \n",
      "     Validation Step: 40 Validation Loss: 0.6142125725746155 \n",
      "     Validation Step: 41 Validation Loss: 0.6149025559425354 \n",
      "     Validation Step: 42 Validation Loss: 0.6130030751228333 \n",
      "     Validation Step: 43 Validation Loss: 0.6115818619728088 \n",
      "     Validation Step: 44 Validation Loss: 0.6101805567741394 \n",
      "Epoch: 94\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.615748405456543 \n",
      "     Training Step: 1 Training Loss: 0.612772524356842 \n",
      "     Training Step: 2 Training Loss: 0.616640567779541 \n",
      "     Training Step: 3 Training Loss: 0.6139253973960876 \n",
      "     Training Step: 4 Training Loss: 0.6132045388221741 \n",
      "     Training Step: 5 Training Loss: 0.6133529543876648 \n",
      "     Training Step: 6 Training Loss: 0.6130689978599548 \n",
      "     Training Step: 7 Training Loss: 0.6135234832763672 \n",
      "     Training Step: 8 Training Loss: 0.6162478923797607 \n",
      "     Training Step: 9 Training Loss: 0.6100412011146545 \n",
      "     Training Step: 10 Training Loss: 0.6149385571479797 \n",
      "     Training Step: 11 Training Loss: 0.6197366118431091 \n",
      "     Training Step: 12 Training Loss: 0.6141061782836914 \n",
      "     Training Step: 13 Training Loss: 0.6184768080711365 \n",
      "     Training Step: 14 Training Loss: 0.6115327477455139 \n",
      "     Training Step: 15 Training Loss: 0.6125160455703735 \n",
      "     Training Step: 16 Training Loss: 0.6157426834106445 \n",
      "     Training Step: 17 Training Loss: 0.6140366792678833 \n",
      "     Training Step: 18 Training Loss: 0.6168867945671082 \n",
      "     Training Step: 19 Training Loss: 0.6134170293807983 \n",
      "     Training Step: 20 Training Loss: 0.6152740120887756 \n",
      "     Training Step: 21 Training Loss: 0.6100804805755615 \n",
      "     Training Step: 22 Training Loss: 0.6170891523361206 \n",
      "     Training Step: 23 Training Loss: 0.6106770038604736 \n",
      "     Training Step: 24 Training Loss: 0.6123896837234497 \n",
      "     Training Step: 25 Training Loss: 0.6117890477180481 \n",
      "     Training Step: 26 Training Loss: 0.6183463931083679 \n",
      "     Training Step: 27 Training Loss: 0.6149560213088989 \n",
      "     Training Step: 28 Training Loss: 0.6182390451431274 \n",
      "     Training Step: 29 Training Loss: 0.6144232153892517 \n",
      "     Training Step: 30 Training Loss: 0.6171725988388062 \n",
      "     Training Step: 31 Training Loss: 0.6134979724884033 \n",
      "     Training Step: 32 Training Loss: 0.6180745959281921 \n",
      "     Training Step: 33 Training Loss: 0.6118889451026917 \n",
      "     Training Step: 34 Training Loss: 0.6114479303359985 \n",
      "     Training Step: 35 Training Loss: 0.6136496067047119 \n",
      "     Training Step: 36 Training Loss: 0.6104981303215027 \n",
      "     Training Step: 37 Training Loss: 0.6137402057647705 \n",
      "     Training Step: 38 Training Loss: 0.6142117381095886 \n",
      "     Training Step: 39 Training Loss: 0.6141505837440491 \n",
      "     Training Step: 40 Training Loss: 0.611405611038208 \n",
      "     Training Step: 41 Training Loss: 0.6151139140129089 \n",
      "     Training Step: 42 Training Loss: 0.6143829822540283 \n",
      "     Training Step: 43 Training Loss: 0.6150768995285034 \n",
      "     Training Step: 44 Training Loss: 0.6135957837104797 \n",
      "     Training Step: 45 Training Loss: 0.615958571434021 \n",
      "     Training Step: 46 Training Loss: 0.6103935241699219 \n",
      "     Training Step: 47 Training Loss: 0.6121604442596436 \n",
      "     Training Step: 48 Training Loss: 0.6147737503051758 \n",
      "     Training Step: 49 Training Loss: 0.618035614490509 \n",
      "     Training Step: 50 Training Loss: 0.6143449544906616 \n",
      "     Training Step: 51 Training Loss: 0.6127021312713623 \n",
      "     Training Step: 52 Training Loss: 0.6176956295967102 \n",
      "     Training Step: 53 Training Loss: 0.6147251725196838 \n",
      "     Training Step: 54 Training Loss: 0.6141489148139954 \n",
      "     Training Step: 55 Training Loss: 0.6130654811859131 \n",
      "     Training Step: 56 Training Loss: 0.6144962906837463 \n",
      "     Training Step: 57 Training Loss: 0.6115404367446899 \n",
      "     Training Step: 58 Training Loss: 0.6152395009994507 \n",
      "     Training Step: 59 Training Loss: 0.6168040037155151 \n",
      "     Training Step: 60 Training Loss: 0.6122328639030457 \n",
      "     Training Step: 61 Training Loss: 0.6082839369773865 \n",
      "     Training Step: 62 Training Loss: 0.6114393472671509 \n",
      "     Training Step: 63 Training Loss: 0.6166589856147766 \n",
      "     Training Step: 64 Training Loss: 0.6115796566009521 \n",
      "     Training Step: 65 Training Loss: 0.6142436861991882 \n",
      "     Training Step: 66 Training Loss: 0.6153168082237244 \n",
      "     Training Step: 67 Training Loss: 0.6164237260818481 \n",
      "     Training Step: 68 Training Loss: 0.6123682260513306 \n",
      "     Training Step: 69 Training Loss: 0.6154322028160095 \n",
      "     Training Step: 70 Training Loss: 0.6120104789733887 \n",
      "     Training Step: 71 Training Loss: 0.6194677948951721 \n",
      "     Training Step: 72 Training Loss: 0.6160058975219727 \n",
      "     Training Step: 73 Training Loss: 0.6104993224143982 \n",
      "     Training Step: 74 Training Loss: 0.610600471496582 \n",
      "     Training Step: 75 Training Loss: 0.616200864315033 \n",
      "     Training Step: 76 Training Loss: 0.6132758855819702 \n",
      "     Training Step: 77 Training Loss: 0.6198877692222595 \n",
      "     Training Step: 78 Training Loss: 0.6146834492683411 \n",
      "     Training Step: 79 Training Loss: 0.6209251284599304 \n",
      "     Training Step: 80 Training Loss: 0.6146740317344666 \n",
      "     Training Step: 81 Training Loss: 0.61313796043396 \n",
      "     Training Step: 82 Training Loss: 0.6124807596206665 \n",
      "     Training Step: 83 Training Loss: 0.6154754161834717 \n",
      "     Training Step: 84 Training Loss: 0.6114429235458374 \n",
      "     Training Step: 85 Training Loss: 0.6195996999740601 \n",
      "     Training Step: 86 Training Loss: 0.611163318157196 \n",
      "     Training Step: 87 Training Loss: 0.6180911064147949 \n",
      "     Training Step: 88 Training Loss: 0.6128689646720886 \n",
      "     Training Step: 89 Training Loss: 0.6111699938774109 \n",
      "     Training Step: 90 Training Loss: 0.6121571063995361 \n",
      "     Training Step: 91 Training Loss: 0.6178097724914551 \n",
      "     Training Step: 92 Training Loss: 0.6142873167991638 \n",
      "     Training Step: 93 Training Loss: 0.6167619228363037 \n",
      "     Training Step: 94 Training Loss: 0.6140222549438477 \n",
      "     Training Step: 95 Training Loss: 0.6185988187789917 \n",
      "     Training Step: 96 Training Loss: 0.6188640594482422 \n",
      "     Training Step: 97 Training Loss: 0.6146697998046875 \n",
      "     Training Step: 98 Training Loss: 0.6171832084655762 \n",
      "     Training Step: 99 Training Loss: 0.6157561540603638 \n",
      "     Training Step: 100 Training Loss: 0.6137527823448181 \n",
      "     Training Step: 101 Training Loss: 0.616666853427887 \n",
      "     Training Step: 102 Training Loss: 0.6158334016799927 \n",
      "     Training Step: 103 Training Loss: 0.6118751764297485 \n",
      "     Training Step: 104 Training Loss: 0.6177365183830261 \n",
      "     Training Step: 105 Training Loss: 0.6106137633323669 \n",
      "     Training Step: 106 Training Loss: 0.6139340400695801 \n",
      "     Training Step: 107 Training Loss: 0.6182069778442383 \n",
      "     Training Step: 108 Training Loss: 0.6115012168884277 \n",
      "     Training Step: 109 Training Loss: 0.6177130937576294 \n",
      "     Training Step: 110 Training Loss: 0.612541675567627 \n",
      "     Training Step: 111 Training Loss: 0.6143420338630676 \n",
      "     Training Step: 112 Training Loss: 0.6155325770378113 \n",
      "     Training Step: 113 Training Loss: 0.612840473651886 \n",
      "     Training Step: 114 Training Loss: 0.617142379283905 \n",
      "     Training Step: 115 Training Loss: 0.6122312545776367 \n",
      "     Training Step: 116 Training Loss: 0.6127647161483765 \n",
      "     Training Step: 117 Training Loss: 0.6123901009559631 \n",
      "     Training Step: 118 Training Loss: 0.6167103052139282 \n",
      "     Training Step: 119 Training Loss: 0.6147066354751587 \n",
      "     Training Step: 120 Training Loss: 0.6154112219810486 \n",
      "     Training Step: 121 Training Loss: 0.6184106469154358 \n",
      "     Training Step: 122 Training Loss: 0.611840546131134 \n",
      "     Training Step: 123 Training Loss: 0.6142020225524902 \n",
      "     Training Step: 124 Training Loss: 0.6092531085014343 \n",
      "     Training Step: 125 Training Loss: 0.6140151023864746 \n",
      "     Training Step: 126 Training Loss: 0.612075686454773 \n",
      "     Training Step: 127 Training Loss: 0.6116225719451904 \n",
      "     Training Step: 128 Training Loss: 0.6116429567337036 \n",
      "     Training Step: 129 Training Loss: 0.6099977493286133 \n",
      "     Training Step: 130 Training Loss: 0.6101347804069519 \n",
      "     Training Step: 131 Training Loss: 0.6143477559089661 \n",
      "     Training Step: 132 Training Loss: 0.6097165942192078 \n",
      "     Training Step: 133 Training Loss: 0.6180862188339233 \n",
      "     Training Step: 134 Training Loss: 0.6146596074104309 \n",
      "     Training Step: 135 Training Loss: 0.6116363406181335 \n",
      "     Training Step: 136 Training Loss: 0.6094293594360352 \n",
      "     Training Step: 137 Training Loss: 0.6136168241500854 \n",
      "     Training Step: 138 Training Loss: 0.6146228909492493 \n",
      "     Training Step: 139 Training Loss: 0.6202787756919861 \n",
      "     Training Step: 140 Training Loss: 0.6122757196426392 \n",
      "     Training Step: 141 Training Loss: 0.6146952509880066 \n",
      "     Training Step: 142 Training Loss: 0.6107322573661804 \n",
      "     Training Step: 143 Training Loss: 0.6164989471435547 \n",
      "     Training Step: 144 Training Loss: 0.6153717637062073 \n",
      "     Training Step: 145 Training Loss: 0.6155388355255127 \n",
      "     Training Step: 146 Training Loss: 0.6112106442451477 \n",
      "     Training Step: 147 Training Loss: 0.6094290018081665 \n",
      "     Training Step: 148 Training Loss: 0.6105865240097046 \n",
      "     Training Step: 149 Training Loss: 0.617384672164917 \n",
      "     Training Step: 150 Training Loss: 0.6176880598068237 \n",
      "     Training Step: 151 Training Loss: 0.6134600043296814 \n",
      "     Training Step: 152 Training Loss: 0.6151527762413025 \n",
      "     Training Step: 153 Training Loss: 0.6122962236404419 \n",
      "     Training Step: 154 Training Loss: 0.6147008538246155 \n",
      "     Training Step: 155 Training Loss: 0.6131834983825684 \n",
      "     Training Step: 156 Training Loss: 0.6146124005317688 \n",
      "     Training Step: 157 Training Loss: 0.615289032459259 \n",
      "     Training Step: 158 Training Loss: 0.6129112839698792 \n",
      "     Training Step: 159 Training Loss: 0.6162124276161194 \n",
      "     Training Step: 160 Training Loss: 0.6133130788803101 \n",
      "     Training Step: 161 Training Loss: 0.6103776097297668 \n",
      "     Training Step: 162 Training Loss: 0.6126402616500854 \n",
      "     Training Step: 163 Training Loss: 0.6151752471923828 \n",
      "     Training Step: 164 Training Loss: 0.612869918346405 \n",
      "     Training Step: 165 Training Loss: 0.6167240738868713 \n",
      "     Training Step: 166 Training Loss: 0.6176483035087585 \n",
      "     Training Step: 167 Training Loss: 0.6157851815223694 \n",
      "     Training Step: 168 Training Loss: 0.6153811812400818 \n",
      "     Training Step: 169 Training Loss: 0.6128038763999939 \n",
      "     Training Step: 170 Training Loss: 0.6137660145759583 \n",
      "     Training Step: 171 Training Loss: 0.6148684620857239 \n",
      "     Training Step: 172 Training Loss: 0.6166055202484131 \n",
      "     Training Step: 173 Training Loss: 0.6102151870727539 \n",
      "     Training Step: 174 Training Loss: 0.6138292551040649 \n",
      "     Training Step: 175 Training Loss: 0.6156676411628723 \n",
      "     Training Step: 176 Training Loss: 0.6160395741462708 \n",
      "     Training Step: 177 Training Loss: 0.6166952252388 \n",
      "     Training Step: 178 Training Loss: 0.6144543886184692 \n",
      "     Training Step: 179 Training Loss: 0.6136373281478882 \n",
      "     Training Step: 180 Training Loss: 0.6129341721534729 \n",
      "     Training Step: 181 Training Loss: 0.616658091545105 \n",
      "     Training Step: 182 Training Loss: 0.6188265085220337 \n",
      "     Training Step: 183 Training Loss: 0.6155954003334045 \n",
      "     Training Step: 184 Training Loss: 0.6133020520210266 \n",
      "     Training Step: 185 Training Loss: 0.6140689253807068 \n",
      "     Training Step: 186 Training Loss: 0.6148006916046143 \n",
      "     Training Step: 187 Training Loss: 0.6133042573928833 \n",
      "     Training Step: 188 Training Loss: 0.614917516708374 \n",
      "     Training Step: 189 Training Loss: 0.6154047250747681 \n",
      "     Training Step: 190 Training Loss: 0.6122519373893738 \n",
      "     Training Step: 191 Training Loss: 0.6132924556732178 \n",
      "     Training Step: 192 Training Loss: 0.6164169907569885 \n",
      "     Training Step: 193 Training Loss: 0.6097373366355896 \n",
      "     Training Step: 194 Training Loss: 0.6144798994064331 \n",
      "     Training Step: 195 Training Loss: 0.6124885082244873 \n",
      "     Training Step: 196 Training Loss: 0.6156855225563049 \n",
      "     Training Step: 197 Training Loss: 0.6129631400108337 \n",
      "     Training Step: 198 Training Loss: 0.6121193170547485 \n",
      "     Training Step: 199 Training Loss: 0.618610680103302 \n",
      "     Training Step: 200 Training Loss: 0.6168211698532104 \n",
      "     Training Step: 201 Training Loss: 0.614439845085144 \n",
      "     Training Step: 202 Training Loss: 0.6125298738479614 \n",
      "     Training Step: 203 Training Loss: 0.61714768409729 \n",
      "     Training Step: 204 Training Loss: 0.6106911301612854 \n",
      "     Training Step: 205 Training Loss: 0.6114027500152588 \n",
      "     Training Step: 206 Training Loss: 0.612142026424408 \n",
      "     Training Step: 207 Training Loss: 0.6150879859924316 \n",
      "     Training Step: 208 Training Loss: 0.614656388759613 \n",
      "     Training Step: 209 Training Loss: 0.6174963712692261 \n",
      "     Training Step: 210 Training Loss: 0.6155233383178711 \n",
      "     Training Step: 211 Training Loss: 0.6147496700286865 \n",
      "     Training Step: 212 Training Loss: 0.6153382658958435 \n",
      "     Training Step: 213 Training Loss: 0.6107345223426819 \n",
      "     Training Step: 214 Training Loss: 0.6146016716957092 \n",
      "     Training Step: 215 Training Loss: 0.6116940379142761 \n",
      "     Training Step: 216 Training Loss: 0.6134700179100037 \n",
      "     Training Step: 217 Training Loss: 0.6161146759986877 \n",
      "     Training Step: 218 Training Loss: 0.6167194247245789 \n",
      "     Training Step: 219 Training Loss: 0.6169260144233704 \n",
      "     Training Step: 220 Training Loss: 0.6105924844741821 \n",
      "     Training Step: 221 Training Loss: 0.6154430508613586 \n",
      "     Training Step: 222 Training Loss: 0.616801917552948 \n",
      "     Training Step: 223 Training Loss: 0.6132084727287292 \n",
      "     Training Step: 224 Training Loss: 0.61088627576828 \n",
      "     Training Step: 225 Training Loss: 0.6116178035736084 \n",
      "     Training Step: 226 Training Loss: 0.6109023690223694 \n",
      "     Training Step: 227 Training Loss: 0.6131384372711182 \n",
      "     Training Step: 228 Training Loss: 0.6096992492675781 \n",
      "     Training Step: 229 Training Loss: 0.618471086025238 \n",
      "     Training Step: 230 Training Loss: 0.6117821931838989 \n",
      "     Training Step: 231 Training Loss: 0.6115411520004272 \n",
      "     Training Step: 232 Training Loss: 0.6163792610168457 \n",
      "     Training Step: 233 Training Loss: 0.6138990521430969 \n",
      "     Training Step: 234 Training Loss: 0.6115128993988037 \n",
      "     Training Step: 235 Training Loss: 0.6118053793907166 \n",
      "     Training Step: 236 Training Loss: 0.6100434064865112 \n",
      "     Training Step: 237 Training Loss: 0.617837131023407 \n",
      "     Training Step: 238 Training Loss: 0.6152989864349365 \n",
      "     Training Step: 239 Training Loss: 0.6118180155754089 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6129807829856873 \n",
      "     Validation Step: 1 Validation Loss: 0.6104686856269836 \n",
      "     Validation Step: 2 Validation Loss: 0.6153297424316406 \n",
      "     Validation Step: 3 Validation Loss: 0.6106186509132385 \n",
      "     Validation Step: 4 Validation Loss: 0.614896833896637 \n",
      "     Validation Step: 5 Validation Loss: 0.6173245906829834 \n",
      "     Validation Step: 6 Validation Loss: 0.6145764589309692 \n",
      "     Validation Step: 7 Validation Loss: 0.6152345538139343 \n",
      "     Validation Step: 8 Validation Loss: 0.6136623024940491 \n",
      "     Validation Step: 9 Validation Loss: 0.6142011880874634 \n",
      "     Validation Step: 10 Validation Loss: 0.6141123175621033 \n",
      "     Validation Step: 11 Validation Loss: 0.6150491237640381 \n",
      "     Validation Step: 12 Validation Loss: 0.6141265034675598 \n",
      "     Validation Step: 13 Validation Loss: 0.6156274676322937 \n",
      "     Validation Step: 14 Validation Loss: 0.6185200214385986 \n",
      "     Validation Step: 15 Validation Loss: 0.6160190105438232 \n",
      "     Validation Step: 16 Validation Loss: 0.6132991313934326 \n",
      "     Validation Step: 17 Validation Loss: 0.6155880093574524 \n",
      "     Validation Step: 18 Validation Loss: 0.6162578463554382 \n",
      "     Validation Step: 19 Validation Loss: 0.6180847883224487 \n",
      "     Validation Step: 20 Validation Loss: 0.6115450263023376 \n",
      "     Validation Step: 21 Validation Loss: 0.6145323514938354 \n",
      "     Validation Step: 22 Validation Loss: 0.618259608745575 \n",
      "     Validation Step: 23 Validation Loss: 0.6105028390884399 \n",
      "     Validation Step: 24 Validation Loss: 0.6158088445663452 \n",
      "     Validation Step: 25 Validation Loss: 0.6111705899238586 \n",
      "     Validation Step: 26 Validation Loss: 0.6170438528060913 \n",
      "     Validation Step: 27 Validation Loss: 0.6148399710655212 \n",
      "     Validation Step: 28 Validation Loss: 0.6142510771751404 \n",
      "     Validation Step: 29 Validation Loss: 0.6146396994590759 \n",
      "     Validation Step: 30 Validation Loss: 0.6100913286209106 \n",
      "     Validation Step: 31 Validation Loss: 0.6128204464912415 \n",
      "     Validation Step: 32 Validation Loss: 0.6183675527572632 \n",
      "     Validation Step: 33 Validation Loss: 0.6074938774108887 \n",
      "     Validation Step: 34 Validation Loss: 0.6116323471069336 \n",
      "     Validation Step: 35 Validation Loss: 0.6136316061019897 \n",
      "     Validation Step: 36 Validation Loss: 0.6176102757453918 \n",
      "     Validation Step: 37 Validation Loss: 0.6136716604232788 \n",
      "     Validation Step: 38 Validation Loss: 0.6111529469490051 \n",
      "     Validation Step: 39 Validation Loss: 0.612120509147644 \n",
      "     Validation Step: 40 Validation Loss: 0.6101457476615906 \n",
      "     Validation Step: 41 Validation Loss: 0.6184931993484497 \n",
      "     Validation Step: 42 Validation Loss: 0.6118717193603516 \n",
      "     Validation Step: 43 Validation Loss: 0.6101190447807312 \n",
      "     Validation Step: 44 Validation Loss: 0.6177197694778442 \n",
      "Epoch: 95\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6194719672203064 \n",
      "     Training Step: 1 Training Loss: 0.6146366596221924 \n",
      "     Training Step: 2 Training Loss: 0.6171689629554749 \n",
      "     Training Step: 3 Training Loss: 0.6168081164360046 \n",
      "     Training Step: 4 Training Loss: 0.6180927753448486 \n",
      "     Training Step: 5 Training Loss: 0.615092396736145 \n",
      "     Training Step: 6 Training Loss: 0.6137604117393494 \n",
      "     Training Step: 7 Training Loss: 0.6180006861686707 \n",
      "     Training Step: 8 Training Loss: 0.6154049038887024 \n",
      "     Training Step: 9 Training Loss: 0.6136132478713989 \n",
      "     Training Step: 10 Training Loss: 0.6126840114593506 \n",
      "     Training Step: 11 Training Loss: 0.615696370601654 \n",
      "     Training Step: 12 Training Loss: 0.6146988272666931 \n",
      "     Training Step: 13 Training Loss: 0.6127862930297852 \n",
      "     Training Step: 14 Training Loss: 0.610600471496582 \n",
      "     Training Step: 15 Training Loss: 0.6137661337852478 \n",
      "     Training Step: 16 Training Loss: 0.6129715442657471 \n",
      "     Training Step: 17 Training Loss: 0.6202324032783508 \n",
      "     Training Step: 18 Training Loss: 0.6183286309242249 \n",
      "     Training Step: 19 Training Loss: 0.6177187561988831 \n",
      "     Training Step: 20 Training Loss: 0.6138874292373657 \n",
      "     Training Step: 21 Training Loss: 0.6109282374382019 \n",
      "     Training Step: 22 Training Loss: 0.6162046790122986 \n",
      "     Training Step: 23 Training Loss: 0.6182208061218262 \n",
      "     Training Step: 24 Training Loss: 0.612159013748169 \n",
      "     Training Step: 25 Training Loss: 0.6169154047966003 \n",
      "     Training Step: 26 Training Loss: 0.6184245347976685 \n",
      "     Training Step: 27 Training Loss: 0.6182212233543396 \n",
      "     Training Step: 28 Training Loss: 0.6143398284912109 \n",
      "     Training Step: 29 Training Loss: 0.6102452278137207 \n",
      "     Training Step: 30 Training Loss: 0.6208894848823547 \n",
      "     Training Step: 31 Training Loss: 0.6112496852874756 \n",
      "     Training Step: 32 Training Loss: 0.6152727007865906 \n",
      "     Training Step: 33 Training Loss: 0.6109324097633362 \n",
      "     Training Step: 34 Training Loss: 0.6164097785949707 \n",
      "     Training Step: 35 Training Loss: 0.6136550903320312 \n",
      "     Training Step: 36 Training Loss: 0.6177484393119812 \n",
      "     Training Step: 37 Training Loss: 0.6178008317947388 \n",
      "     Training Step: 38 Training Loss: 0.6154179573059082 \n",
      "     Training Step: 39 Training Loss: 0.6176837086677551 \n",
      "     Training Step: 40 Training Loss: 0.6132866740226746 \n",
      "     Training Step: 41 Training Loss: 0.6134624481201172 \n",
      "     Training Step: 42 Training Loss: 0.6100019216537476 \n",
      "     Training Step: 43 Training Loss: 0.6130598187446594 \n",
      "     Training Step: 44 Training Loss: 0.6140748858451843 \n",
      "     Training Step: 45 Training Loss: 0.6166778802871704 \n",
      "     Training Step: 46 Training Loss: 0.6167256236076355 \n",
      "     Training Step: 47 Training Loss: 0.6188595294952393 \n",
      "     Training Step: 48 Training Loss: 0.61460280418396 \n",
      "     Training Step: 49 Training Loss: 0.6146774291992188 \n",
      "     Training Step: 50 Training Loss: 0.6115291714668274 \n",
      "     Training Step: 51 Training Loss: 0.61434406042099 \n",
      "     Training Step: 52 Training Loss: 0.6184554696083069 \n",
      "     Training Step: 53 Training Loss: 0.6127131581306458 \n",
      "     Training Step: 54 Training Loss: 0.6097469925880432 \n",
      "     Training Step: 55 Training Loss: 0.6147032976150513 \n",
      "     Training Step: 56 Training Loss: 0.6163464188575745 \n",
      "     Training Step: 57 Training Loss: 0.6118398308753967 \n",
      "     Training Step: 58 Training Loss: 0.6115952134132385 \n",
      "     Training Step: 59 Training Loss: 0.6159994006156921 \n",
      "     Training Step: 60 Training Loss: 0.6176917552947998 \n",
      "     Training Step: 61 Training Loss: 0.6092334985733032 \n",
      "     Training Step: 62 Training Loss: 0.6129103302955627 \n",
      "     Training Step: 63 Training Loss: 0.6142135858535767 \n",
      "     Training Step: 64 Training Loss: 0.6114724278450012 \n",
      "     Training Step: 65 Training Loss: 0.6143461465835571 \n",
      "     Training Step: 66 Training Loss: 0.6104860901832581 \n",
      "     Training Step: 67 Training Loss: 0.6144877076148987 \n",
      "     Training Step: 68 Training Loss: 0.6166774034500122 \n",
      "     Training Step: 69 Training Loss: 0.6134746074676514 \n",
      "     Training Step: 70 Training Loss: 0.6134987473487854 \n",
      "     Training Step: 71 Training Loss: 0.610389769077301 \n",
      "     Training Step: 72 Training Loss: 0.6161249279975891 \n",
      "     Training Step: 73 Training Loss: 0.6137363910675049 \n",
      "     Training Step: 74 Training Loss: 0.6144245266914368 \n",
      "     Training Step: 75 Training Loss: 0.6168124675750732 \n",
      "     Training Step: 76 Training Loss: 0.6120672225952148 \n",
      "     Training Step: 77 Training Loss: 0.6196320652961731 \n",
      "     Training Step: 78 Training Loss: 0.6146677732467651 \n",
      "     Training Step: 79 Training Loss: 0.6196953654289246 \n",
      "     Training Step: 80 Training Loss: 0.61007159948349 \n",
      "     Training Step: 81 Training Loss: 0.6166641116142273 \n",
      "     Training Step: 82 Training Loss: 0.6147293448448181 \n",
      "     Training Step: 83 Training Loss: 0.6150657534599304 \n",
      "     Training Step: 84 Training Loss: 0.6106271147727966 \n",
      "     Training Step: 85 Training Loss: 0.6146081686019897 \n",
      "     Training Step: 86 Training Loss: 0.6147478818893433 \n",
      "     Training Step: 87 Training Loss: 0.6162296533584595 \n",
      "     Training Step: 88 Training Loss: 0.6114495992660522 \n",
      "     Training Step: 89 Training Loss: 0.6138213872909546 \n",
      "     Training Step: 90 Training Loss: 0.6136280298233032 \n",
      "     Training Step: 91 Training Loss: 0.6114152073860168 \n",
      "     Training Step: 92 Training Loss: 0.6154792904853821 \n",
      "     Training Step: 93 Training Loss: 0.6136355400085449 \n",
      "     Training Step: 94 Training Loss: 0.615162193775177 \n",
      "     Training Step: 95 Training Loss: 0.6142135262489319 \n",
      "     Training Step: 96 Training Loss: 0.6184365153312683 \n",
      "     Training Step: 97 Training Loss: 0.6082532405853271 \n",
      "     Training Step: 98 Training Loss: 0.6157500147819519 \n",
      "     Training Step: 99 Training Loss: 0.6153982877731323 \n",
      "     Training Step: 100 Training Loss: 0.6118260025978088 \n",
      "     Training Step: 101 Training Loss: 0.6122354865074158 \n",
      "     Training Step: 102 Training Loss: 0.6135070323944092 \n",
      "     Training Step: 103 Training Loss: 0.6122353076934814 \n",
      "     Training Step: 104 Training Loss: 0.6116355061531067 \n",
      "     Training Step: 105 Training Loss: 0.6097018122673035 \n",
      "     Training Step: 106 Training Loss: 0.6093869805335999 \n",
      "     Training Step: 107 Training Loss: 0.6121541261672974 \n",
      "     Training Step: 108 Training Loss: 0.6114068031311035 \n",
      "     Training Step: 109 Training Loss: 0.6144464015960693 \n",
      "     Training Step: 110 Training Loss: 0.611775279045105 \n",
      "     Training Step: 111 Training Loss: 0.6111502647399902 \n",
      "     Training Step: 112 Training Loss: 0.6128767728805542 \n",
      "     Training Step: 113 Training Loss: 0.6155710816383362 \n",
      "     Training Step: 114 Training Loss: 0.6172016859054565 \n",
      "     Training Step: 115 Training Loss: 0.6149762868881226 \n",
      "     Training Step: 116 Training Loss: 0.6116212606430054 \n",
      "     Training Step: 117 Training Loss: 0.6105592846870422 \n",
      "     Training Step: 118 Training Loss: 0.6141583919525146 \n",
      "     Training Step: 119 Training Loss: 0.6123700737953186 \n",
      "     Training Step: 120 Training Loss: 0.6114288568496704 \n",
      "     Training Step: 121 Training Loss: 0.6124874949455261 \n",
      "     Training Step: 122 Training Loss: 0.614700973033905 \n",
      "     Training Step: 123 Training Loss: 0.6156058311462402 \n",
      "     Training Step: 124 Training Loss: 0.6121472120285034 \n",
      "     Training Step: 125 Training Loss: 0.6166313886642456 \n",
      "     Training Step: 126 Training Loss: 0.6153920888900757 \n",
      "     Training Step: 127 Training Loss: 0.6144965291023254 \n",
      "     Training Step: 128 Training Loss: 0.6152406930923462 \n",
      "     Training Step: 129 Training Loss: 0.6128779649734497 \n",
      "     Training Step: 130 Training Loss: 0.6139259338378906 \n",
      "     Training Step: 131 Training Loss: 0.6146513223648071 \n",
      "     Training Step: 132 Training Loss: 0.616202712059021 \n",
      "     Training Step: 133 Training Loss: 0.6133577227592468 \n",
      "     Training Step: 134 Training Loss: 0.6155291795730591 \n",
      "     Training Step: 135 Training Loss: 0.6149152517318726 \n",
      "     Training Step: 136 Training Loss: 0.6132073998451233 \n",
      "     Training Step: 137 Training Loss: 0.6131218075752258 \n",
      "     Training Step: 138 Training Loss: 0.612243115901947 \n",
      "     Training Step: 139 Training Loss: 0.6167169213294983 \n",
      "     Training Step: 140 Training Loss: 0.6141537427902222 \n",
      "     Training Step: 141 Training Loss: 0.6127564311027527 \n",
      "     Training Step: 142 Training Loss: 0.6101366877555847 \n",
      "     Training Step: 143 Training Loss: 0.6107183694839478 \n",
      "     Training Step: 144 Training Loss: 0.6167238354682922 \n",
      "     Training Step: 145 Training Loss: 0.6124613881111145 \n",
      "     Training Step: 146 Training Loss: 0.6180617213249207 \n",
      "     Training Step: 147 Training Loss: 0.6140242218971252 \n",
      "     Training Step: 148 Training Loss: 0.6144523024559021 \n",
      "     Training Step: 149 Training Loss: 0.6178085803985596 \n",
      "     Training Step: 150 Training Loss: 0.6140353679656982 \n",
      "     Training Step: 151 Training Loss: 0.6153008937835693 \n",
      "     Training Step: 152 Training Loss: 0.6106089353561401 \n",
      "     Training Step: 153 Training Loss: 0.6185767650604248 \n",
      "     Training Step: 154 Training Loss: 0.6117953062057495 \n",
      "     Training Step: 155 Training Loss: 0.6155272126197815 \n",
      "     Training Step: 156 Training Loss: 0.6114150881767273 \n",
      "     Training Step: 157 Training Loss: 0.6174799799919128 \n",
      "     Training Step: 158 Training Loss: 0.6129311323165894 \n",
      "     Training Step: 159 Training Loss: 0.6107541918754578 \n",
      "     Training Step: 160 Training Loss: 0.6125380396842957 \n",
      "     Training Step: 161 Training Loss: 0.615831196308136 \n",
      "     Training Step: 162 Training Loss: 0.6123871207237244 \n",
      "     Training Step: 163 Training Loss: 0.6103793978691101 \n",
      "     Training Step: 164 Training Loss: 0.6132034063339233 \n",
      "     Training Step: 165 Training Loss: 0.6115531921386719 \n",
      "     Training Step: 166 Training Loss: 0.6153831481933594 \n",
      "     Training Step: 167 Training Loss: 0.6142938137054443 \n",
      "     Training Step: 168 Training Loss: 0.6153509616851807 \n",
      "     Training Step: 169 Training Loss: 0.6157588362693787 \n",
      "     Training Step: 170 Training Loss: 0.6152961850166321 \n",
      "     Training Step: 171 Training Loss: 0.6166771054267883 \n",
      "     Training Step: 172 Training Loss: 0.6100546717643738 \n",
      "     Training Step: 173 Training Loss: 0.6121205687522888 \n",
      "     Training Step: 174 Training Loss: 0.6115235090255737 \n",
      "     Training Step: 175 Training Loss: 0.6106870770454407 \n",
      "     Training Step: 176 Training Loss: 0.6159546375274658 \n",
      "     Training Step: 177 Training Loss: 0.6116863489151001 \n",
      "     Training Step: 178 Training Loss: 0.6123756766319275 \n",
      "     Training Step: 179 Training Loss: 0.615176260471344 \n",
      "     Training Step: 180 Training Loss: 0.6167346239089966 \n",
      "     Training Step: 181 Training Loss: 0.613136351108551 \n",
      "     Training Step: 182 Training Loss: 0.6146687865257263 \n",
      "     Training Step: 183 Training Loss: 0.6097239851951599 \n",
      "     Training Step: 184 Training Loss: 0.6115238070487976 \n",
      "     Training Step: 185 Training Loss: 0.6147722601890564 \n",
      "     Training Step: 186 Training Loss: 0.6134145259857178 \n",
      "     Training Step: 187 Training Loss: 0.617398738861084 \n",
      "     Training Step: 188 Training Loss: 0.6186198592185974 \n",
      "     Training Step: 189 Training Loss: 0.6139107942581177 \n",
      "     Training Step: 190 Training Loss: 0.613068163394928 \n",
      "     Training Step: 191 Training Loss: 0.6180728077888489 \n",
      "     Training Step: 192 Training Loss: 0.6152826547622681 \n",
      "     Training Step: 193 Training Loss: 0.6143739819526672 \n",
      "     Training Step: 194 Training Loss: 0.6146106719970703 \n",
      "     Training Step: 195 Training Loss: 0.611629843711853 \n",
      "     Training Step: 196 Training Loss: 0.6128154397010803 \n",
      "     Training Step: 197 Training Loss: 0.6170826554298401 \n",
      "     Training Step: 198 Training Loss: 0.6164053678512573 \n",
      "     Training Step: 199 Training Loss: 0.6148699522018433 \n",
      "     Training Step: 200 Training Loss: 0.619856595993042 \n",
      "     Training Step: 201 Training Loss: 0.6125454306602478 \n",
      "     Training Step: 202 Training Loss: 0.6157587170600891 \n",
      "     Training Step: 203 Training Loss: 0.6164816617965698 \n",
      "     Training Step: 204 Training Loss: 0.6133281588554382 \n",
      "     Training Step: 205 Training Loss: 0.6160350441932678 \n",
      "     Training Step: 206 Training Loss: 0.6150943636894226 \n",
      "     Training Step: 207 Training Loss: 0.6141140460968018 \n",
      "     Training Step: 208 Training Loss: 0.614248514175415 \n",
      "     Training Step: 209 Training Loss: 0.6128441095352173 \n",
      "     Training Step: 210 Training Loss: 0.6118943691253662 \n",
      "     Training Step: 211 Training Loss: 0.6168015003204346 \n",
      "     Training Step: 212 Training Loss: 0.615678608417511 \n",
      "     Training Step: 213 Training Loss: 0.6147950291633606 \n",
      "     Training Step: 214 Training Loss: 0.6167567372322083 \n",
      "     Training Step: 215 Training Loss: 0.6149287819862366 \n",
      "     Training Step: 216 Training Loss: 0.6111531257629395 \n",
      "     Training Step: 217 Training Loss: 0.617625892162323 \n",
      "     Training Step: 218 Training Loss: 0.6116513013839722 \n",
      "     Training Step: 219 Training Loss: 0.61577969789505 \n",
      "     Training Step: 220 Training Loss: 0.6168886423110962 \n",
      "     Training Step: 221 Training Loss: 0.614017128944397 \n",
      "     Training Step: 222 Training Loss: 0.6122949123382568 \n",
      "     Training Step: 223 Training Loss: 0.6105060577392578 \n",
      "     Training Step: 224 Training Loss: 0.6118353009223938 \n",
      "     Training Step: 225 Training Loss: 0.6100661158561707 \n",
      "     Training Step: 226 Training Loss: 0.6132787466049194 \n",
      "     Training Step: 227 Training Loss: 0.6131842732429504 \n",
      "     Training Step: 228 Training Loss: 0.6106494665145874 \n",
      "     Training Step: 229 Training Loss: 0.6120067834854126 \n",
      "     Training Step: 230 Training Loss: 0.6125109195709229 \n",
      "     Training Step: 231 Training Loss: 0.6172468662261963 \n",
      "     Training Step: 232 Training Loss: 0.6189433932304382 \n",
      "     Training Step: 233 Training Loss: 0.6094237565994263 \n",
      "     Training Step: 234 Training Loss: 0.6118095517158508 \n",
      "     Training Step: 235 Training Loss: 0.6133157014846802 \n",
      "     Training Step: 236 Training Loss: 0.6171782612800598 \n",
      "     Training Step: 237 Training Loss: 0.612284243106842 \n",
      "     Training Step: 238 Training Loss: 0.6132912039756775 \n",
      "     Training Step: 239 Training Loss: 0.6154516339302063 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6129825711250305 \n",
      "     Validation Step: 1 Validation Loss: 0.6136285662651062 \n",
      "     Validation Step: 2 Validation Loss: 0.607510507106781 \n",
      "     Validation Step: 3 Validation Loss: 0.6176033020019531 \n",
      "     Validation Step: 4 Validation Loss: 0.6111754775047302 \n",
      "     Validation Step: 5 Validation Loss: 0.6136612892150879 \n",
      "     Validation Step: 6 Validation Loss: 0.6150488257408142 \n",
      "     Validation Step: 7 Validation Loss: 0.6118770241737366 \n",
      "     Validation Step: 8 Validation Loss: 0.6158056855201721 \n",
      "     Validation Step: 9 Validation Loss: 0.6155816316604614 \n",
      "     Validation Step: 10 Validation Loss: 0.6111615300178528 \n",
      "     Validation Step: 11 Validation Loss: 0.6152322292327881 \n",
      "     Validation Step: 12 Validation Loss: 0.6104735732078552 \n",
      "     Validation Step: 13 Validation Loss: 0.6142502427101135 \n",
      "     Validation Step: 14 Validation Loss: 0.6162526607513428 \n",
      "     Validation Step: 15 Validation Loss: 0.613670825958252 \n",
      "     Validation Step: 16 Validation Loss: 0.6156241297721863 \n",
      "     Validation Step: 17 Validation Loss: 0.6170338988304138 \n",
      "     Validation Step: 18 Validation Loss: 0.610150158405304 \n",
      "     Validation Step: 19 Validation Loss: 0.6141292452812195 \n",
      "     Validation Step: 20 Validation Loss: 0.6106262803077698 \n",
      "     Validation Step: 21 Validation Loss: 0.6121246814727783 \n",
      "     Validation Step: 22 Validation Loss: 0.6177137494087219 \n",
      "     Validation Step: 23 Validation Loss: 0.6148370504379272 \n",
      "     Validation Step: 24 Validation Loss: 0.6116345524787903 \n",
      "     Validation Step: 25 Validation Loss: 0.6173149943351746 \n",
      "     Validation Step: 26 Validation Loss: 0.6153280735015869 \n",
      "     Validation Step: 27 Validation Loss: 0.614640474319458 \n",
      "     Validation Step: 28 Validation Loss: 0.6185105443000793 \n",
      "     Validation Step: 29 Validation Loss: 0.6115506887435913 \n",
      "     Validation Step: 30 Validation Loss: 0.6101309061050415 \n",
      "     Validation Step: 31 Validation Loss: 0.6182514429092407 \n",
      "     Validation Step: 32 Validation Loss: 0.6180775165557861 \n",
      "     Validation Step: 33 Validation Loss: 0.6148949265480042 \n",
      "     Validation Step: 34 Validation Loss: 0.6145728826522827 \n",
      "     Validation Step: 35 Validation Loss: 0.6101046800613403 \n",
      "     Validation Step: 36 Validation Loss: 0.6160115599632263 \n",
      "     Validation Step: 37 Validation Loss: 0.6141127347946167 \n",
      "     Validation Step: 38 Validation Loss: 0.6184831261634827 \n",
      "     Validation Step: 39 Validation Loss: 0.6128212213516235 \n",
      "     Validation Step: 40 Validation Loss: 0.6145312786102295 \n",
      "     Validation Step: 41 Validation Loss: 0.610508143901825 \n",
      "     Validation Step: 42 Validation Loss: 0.618361234664917 \n",
      "     Validation Step: 43 Validation Loss: 0.6142016053199768 \n",
      "     Validation Step: 44 Validation Loss: 0.6132988333702087 \n",
      "Epoch: 96\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6111392974853516 \n",
      "     Training Step: 1 Training Loss: 0.61143559217453 \n",
      "     Training Step: 2 Training Loss: 0.6151764988899231 \n",
      "     Training Step: 3 Training Loss: 0.6147092580795288 \n",
      "     Training Step: 4 Training Loss: 0.6134747862815857 \n",
      "     Training Step: 5 Training Loss: 0.618110716342926 \n",
      "     Training Step: 6 Training Loss: 0.6128694415092468 \n",
      "     Training Step: 7 Training Loss: 0.6125227808952332 \n",
      "     Training Step: 8 Training Loss: 0.6167111992835999 \n",
      "     Training Step: 9 Training Loss: 0.6142452955245972 \n",
      "     Training Step: 10 Training Loss: 0.6118084192276001 \n",
      "     Training Step: 11 Training Loss: 0.6163498163223267 \n",
      "     Training Step: 12 Training Loss: 0.6142050623893738 \n",
      "     Training Step: 13 Training Loss: 0.6103925108909607 \n",
      "     Training Step: 14 Training Loss: 0.6129088997840881 \n",
      "     Training Step: 15 Training Loss: 0.6146741509437561 \n",
      "     Training Step: 16 Training Loss: 0.6156032085418701 \n",
      "     Training Step: 17 Training Loss: 0.6171464920043945 \n",
      "     Training Step: 18 Training Loss: 0.6092329025268555 \n",
      "     Training Step: 19 Training Loss: 0.6176958084106445 \n",
      "     Training Step: 20 Training Loss: 0.6152639985084534 \n",
      "     Training Step: 21 Training Loss: 0.6105890870094299 \n",
      "     Training Step: 22 Training Loss: 0.6111610531806946 \n",
      "     Training Step: 23 Training Loss: 0.6133513450622559 \n",
      "     Training Step: 24 Training Loss: 0.6143443584442139 \n",
      "     Training Step: 25 Training Loss: 0.6149603724479675 \n",
      "     Training Step: 26 Training Loss: 0.6082629561424255 \n",
      "     Training Step: 27 Training Loss: 0.6124853491783142 \n",
      "     Training Step: 28 Training Loss: 0.6153968572616577 \n",
      "     Training Step: 29 Training Loss: 0.6171621084213257 \n",
      "     Training Step: 30 Training Loss: 0.6126388907432556 \n",
      "     Training Step: 31 Training Loss: 0.6162114143371582 \n",
      "     Training Step: 32 Training Loss: 0.6160047650337219 \n",
      "     Training Step: 33 Training Loss: 0.6101803183555603 \n",
      "     Training Step: 34 Training Loss: 0.6118345260620117 \n",
      "     Training Step: 35 Training Loss: 0.6118806600570679 \n",
      "     Training Step: 36 Training Loss: 0.6121452450752258 \n",
      "     Training Step: 37 Training Loss: 0.6130576729774475 \n",
      "     Training Step: 38 Training Loss: 0.6162533164024353 \n",
      "     Training Step: 39 Training Loss: 0.6199021935462952 \n",
      "     Training Step: 40 Training Loss: 0.6159499287605286 \n",
      "     Training Step: 41 Training Loss: 0.6152850985527039 \n",
      "     Training Step: 42 Training Loss: 0.613821268081665 \n",
      "     Training Step: 43 Training Loss: 0.6168911457061768 \n",
      "     Training Step: 44 Training Loss: 0.6136393547058105 \n",
      "     Training Step: 45 Training Loss: 0.6155116558074951 \n",
      "     Training Step: 46 Training Loss: 0.6138876676559448 \n",
      "     Training Step: 47 Training Loss: 0.616480827331543 \n",
      "     Training Step: 48 Training Loss: 0.616798460483551 \n",
      "     Training Step: 49 Training Loss: 0.6115968227386475 \n",
      "     Training Step: 50 Training Loss: 0.6101186275482178 \n",
      "     Training Step: 51 Training Loss: 0.6134399771690369 \n",
      "     Training Step: 52 Training Loss: 0.6114943623542786 \n",
      "     Training Step: 53 Training Loss: 0.611598014831543 \n",
      "     Training Step: 54 Training Loss: 0.6105936765670776 \n",
      "     Training Step: 55 Training Loss: 0.6147536635398865 \n",
      "     Training Step: 56 Training Loss: 0.6105645298957825 \n",
      "     Training Step: 57 Training Loss: 0.610039234161377 \n",
      "     Training Step: 58 Training Loss: 0.6131163835525513 \n",
      "     Training Step: 59 Training Loss: 0.6122424602508545 \n",
      "     Training Step: 60 Training Loss: 0.6143853664398193 \n",
      "     Training Step: 61 Training Loss: 0.6181127429008484 \n",
      "     Training Step: 62 Training Loss: 0.6154695153236389 \n",
      "     Training Step: 63 Training Loss: 0.614620566368103 \n",
      "     Training Step: 64 Training Loss: 0.610727846622467 \n",
      "     Training Step: 65 Training Loss: 0.6127992272377014 \n",
      "     Training Step: 66 Training Loss: 0.6120647192001343 \n",
      "     Training Step: 67 Training Loss: 0.6139104962348938 \n",
      "     Training Step: 68 Training Loss: 0.613498330116272 \n",
      "     Training Step: 69 Training Loss: 0.6154780387878418 \n",
      "     Training Step: 70 Training Loss: 0.6133153438568115 \n",
      "     Training Step: 71 Training Loss: 0.6140369772911072 \n",
      "     Training Step: 72 Training Loss: 0.6144967675209045 \n",
      "     Training Step: 73 Training Loss: 0.6156848669052124 \n",
      "     Training Step: 74 Training Loss: 0.614696741104126 \n",
      "     Training Step: 75 Training Loss: 0.6144223809242249 \n",
      "     Training Step: 76 Training Loss: 0.6157413125038147 \n",
      "     Training Step: 77 Training Loss: 0.6183130741119385 \n",
      "     Training Step: 78 Training Loss: 0.615741491317749 \n",
      "     Training Step: 79 Training Loss: 0.6121805906295776 \n",
      "     Training Step: 80 Training Loss: 0.6167003512382507 \n",
      "     Training Step: 81 Training Loss: 0.6127626895904541 \n",
      "     Training Step: 82 Training Loss: 0.6135256290435791 \n",
      "     Training Step: 83 Training Loss: 0.6157530546188354 \n",
      "     Training Step: 84 Training Loss: 0.6150992512702942 \n",
      "     Training Step: 85 Training Loss: 0.6144784688949585 \n",
      "     Training Step: 86 Training Loss: 0.6097467541694641 \n",
      "     Training Step: 87 Training Loss: 0.6168034672737122 \n",
      "     Training Step: 88 Training Loss: 0.6132819652557373 \n",
      "     Training Step: 89 Training Loss: 0.6116105914115906 \n",
      "     Training Step: 90 Training Loss: 0.609393835067749 \n",
      "     Training Step: 91 Training Loss: 0.6178311705589294 \n",
      "     Training Step: 92 Training Loss: 0.6100282073020935 \n",
      "     Training Step: 93 Training Loss: 0.6152976155281067 \n",
      "     Training Step: 94 Training Loss: 0.6141135692596436 \n",
      "     Training Step: 95 Training Loss: 0.617428719997406 \n",
      "     Training Step: 96 Training Loss: 0.6101240515708923 \n",
      "     Training Step: 97 Training Loss: 0.6140908002853394 \n",
      "     Training Step: 98 Training Loss: 0.6146903038024902 \n",
      "     Training Step: 99 Training Loss: 0.6120126247406006 \n",
      "     Training Step: 100 Training Loss: 0.6186262965202332 \n",
      "     Training Step: 101 Training Loss: 0.6169403195381165 \n",
      "     Training Step: 102 Training Loss: 0.6094602346420288 \n",
      "     Training Step: 103 Training Loss: 0.6176890730857849 \n",
      "     Training Step: 104 Training Loss: 0.6117897629737854 \n",
      "     Training Step: 105 Training Loss: 0.6107397675514221 \n",
      "     Training Step: 106 Training Loss: 0.6147724986076355 \n",
      "     Training Step: 107 Training Loss: 0.6114277243614197 \n",
      "     Training Step: 108 Training Loss: 0.618868350982666 \n",
      "     Training Step: 109 Training Loss: 0.609721302986145 \n",
      "     Training Step: 110 Training Loss: 0.6105078458786011 \n",
      "     Training Step: 111 Training Loss: 0.6174965500831604 \n",
      "     Training Step: 112 Training Loss: 0.6140247583389282 \n",
      "     Training Step: 113 Training Loss: 0.6154507398605347 \n",
      "     Training Step: 114 Training Loss: 0.6132896542549133 \n",
      "     Training Step: 115 Training Loss: 0.6118292212486267 \n",
      "     Training Step: 116 Training Loss: 0.6176583766937256 \n",
      "     Training Step: 117 Training Loss: 0.6137425303459167 \n",
      "     Training Step: 118 Training Loss: 0.6184226870536804 \n",
      "     Training Step: 119 Training Loss: 0.6142842173576355 \n",
      "     Training Step: 120 Training Loss: 0.6115252375602722 \n",
      "     Training Step: 121 Training Loss: 0.616802990436554 \n",
      "     Training Step: 122 Training Loss: 0.6153692007064819 \n",
      "     Training Step: 123 Training Loss: 0.6155321598052979 \n",
      "     Training Step: 124 Training Loss: 0.6123113632202148 \n",
      "     Training Step: 125 Training Loss: 0.6118108034133911 \n",
      "     Training Step: 126 Training Loss: 0.617786169052124 \n",
      "     Training Step: 127 Training Loss: 0.6157790422439575 \n",
      "     Training Step: 128 Training Loss: 0.6196886301040649 \n",
      "     Training Step: 129 Training Loss: 0.6161091923713684 \n",
      "     Training Step: 130 Training Loss: 0.6136531829833984 \n",
      "     Training Step: 131 Training Loss: 0.6185624599456787 \n",
      "     Training Step: 132 Training Loss: 0.6166396141052246 \n",
      "     Training Step: 133 Training Loss: 0.6181995272636414 \n",
      "     Training Step: 134 Training Loss: 0.6171794533729553 \n",
      "     Training Step: 135 Training Loss: 0.6137775182723999 \n",
      "     Training Step: 136 Training Loss: 0.6150707006454468 \n",
      "     Training Step: 137 Training Loss: 0.6166297793388367 \n",
      "     Training Step: 138 Training Loss: 0.6122814416885376 \n",
      "     Training Step: 139 Training Loss: 0.6137551069259644 \n",
      "     Training Step: 140 Training Loss: 0.6114364862442017 \n",
      "     Training Step: 141 Training Loss: 0.6148008108139038 \n",
      "     Training Step: 142 Training Loss: 0.6123926043510437 \n",
      "     Training Step: 143 Training Loss: 0.6131919622421265 \n",
      "     Training Step: 144 Training Loss: 0.6127681732177734 \n",
      "     Training Step: 145 Training Loss: 0.6188579201698303 \n",
      "     Training Step: 146 Training Loss: 0.6151646971702576 \n",
      "     Training Step: 147 Training Loss: 0.6154093146324158 \n",
      "     Training Step: 148 Training Loss: 0.6154303550720215 \n",
      "     Training Step: 149 Training Loss: 0.6166871190071106 \n",
      "     Training Step: 150 Training Loss: 0.6194555163383484 \n",
      "     Training Step: 151 Training Loss: 0.6158322095870972 \n",
      "     Training Step: 152 Training Loss: 0.6121581792831421 \n",
      "     Training Step: 153 Training Loss: 0.6105926036834717 \n",
      "     Training Step: 154 Training Loss: 0.6146906018257141 \n",
      "     Training Step: 155 Training Loss: 0.6139317154884338 \n",
      "     Training Step: 156 Training Loss: 0.6106860637664795 \n",
      "     Training Step: 157 Training Loss: 0.6146380305290222 \n",
      "     Training Step: 158 Training Loss: 0.6116983890533447 \n",
      "     Training Step: 159 Training Loss: 0.6116374731063843 \n",
      "     Training Step: 160 Training Loss: 0.6152419447898865 \n",
      "     Training Step: 161 Training Loss: 0.610893964767456 \n",
      "     Training Step: 162 Training Loss: 0.6122356653213501 \n",
      "     Training Step: 163 Training Loss: 0.6180655360221863 \n",
      "     Training Step: 164 Training Loss: 0.6144564151763916 \n",
      "     Training Step: 165 Training Loss: 0.6132056713104248 \n",
      "     Training Step: 166 Training Loss: 0.6134518980979919 \n",
      "     Training Step: 167 Training Loss: 0.6182542443275452 \n",
      "     Training Step: 168 Training Loss: 0.6143871545791626 \n",
      "     Training Step: 169 Training Loss: 0.614730715751648 \n",
      "     Training Step: 170 Training Loss: 0.6133053302764893 \n",
      "     Training Step: 171 Training Loss: 0.6143501996994019 \n",
      "     Training Step: 172 Training Loss: 0.6104015707969666 \n",
      "     Training Step: 173 Training Loss: 0.615680456161499 \n",
      "     Training Step: 174 Training Loss: 0.6144394278526306 \n",
      "     Training Step: 175 Training Loss: 0.6160348653793335 \n",
      "     Training Step: 176 Training Loss: 0.6148704886436462 \n",
      "     Training Step: 177 Training Loss: 0.6125320792198181 \n",
      "     Training Step: 178 Training Loss: 0.6142115592956543 \n",
      "     Training Step: 179 Training Loss: 0.6106942892074585 \n",
      "     Training Step: 180 Training Loss: 0.613068163394928 \n",
      "     Training Step: 181 Training Loss: 0.6121224164962769 \n",
      "     Training Step: 182 Training Loss: 0.6132757663726807 \n",
      "     Training Step: 183 Training Loss: 0.6149325370788574 \n",
      "     Training Step: 184 Training Loss: 0.6114054918289185 \n",
      "     Training Step: 185 Training Loss: 0.6184859871864319 \n",
      "     Training Step: 186 Training Loss: 0.614019513130188 \n",
      "     Training Step: 187 Training Loss: 0.6128678321838379 \n",
      "     Training Step: 188 Training Loss: 0.6155492663383484 \n",
      "     Training Step: 189 Training Loss: 0.6171050667762756 \n",
      "     Training Step: 190 Training Loss: 0.6146644353866577 \n",
      "     Training Step: 191 Training Loss: 0.6153019070625305 \n",
      "     Training Step: 192 Training Loss: 0.6141510605812073 \n",
      "     Training Step: 193 Training Loss: 0.611638069152832 \n",
      "     Training Step: 194 Training Loss: 0.61961430311203 \n",
      "     Training Step: 195 Training Loss: 0.6141500473022461 \n",
      "     Training Step: 196 Training Loss: 0.6105153560638428 \n",
      "     Training Step: 197 Training Loss: 0.6109052896499634 \n",
      "     Training Step: 198 Training Loss: 0.6132097840309143 \n",
      "     Training Step: 199 Training Loss: 0.6171560883522034 \n",
      "     Training Step: 200 Training Loss: 0.6129254102706909 \n",
      "     Training Step: 201 Training Loss: 0.6118340492248535 \n",
      "     Training Step: 202 Training Loss: 0.6136199831962585 \n",
      "     Training Step: 203 Training Loss: 0.6167660355567932 \n",
      "     Training Step: 204 Training Loss: 0.6116397380828857 \n",
      "     Training Step: 205 Training Loss: 0.6202415823936462 \n",
      "     Training Step: 206 Training Loss: 0.6111956834793091 \n",
      "     Training Step: 207 Training Loss: 0.6115254163742065 \n",
      "     Training Step: 208 Training Loss: 0.6164278984069824 \n",
      "     Training Step: 209 Training Loss: 0.6097248792648315 \n",
      "     Training Step: 210 Training Loss: 0.6150861978530884 \n",
      "     Training Step: 211 Training Loss: 0.6177708506584167 \n",
      "     Training Step: 212 Training Loss: 0.6209473609924316 \n",
      "     Training Step: 213 Training Loss: 0.6164090633392334 \n",
      "     Training Step: 214 Training Loss: 0.6180649995803833 \n",
      "     Training Step: 215 Training Loss: 0.6166102290153503 \n",
      "     Training Step: 216 Training Loss: 0.612403392791748 \n",
      "     Training Step: 217 Training Loss: 0.6124844551086426 \n",
      "     Training Step: 218 Training Loss: 0.6176813840866089 \n",
      "     Training Step: 219 Training Loss: 0.6131719946861267 \n",
      "     Training Step: 220 Training Loss: 0.6129942536354065 \n",
      "     Training Step: 221 Training Loss: 0.6146547794342041 \n",
      "     Training Step: 222 Training Loss: 0.6149186491966248 \n",
      "     Training Step: 223 Training Loss: 0.6125473380088806 \n",
      "     Training Step: 224 Training Loss: 0.6135911345481873 \n",
      "     Training Step: 225 Training Loss: 0.6167156100273132 \n",
      "     Training Step: 226 Training Loss: 0.6100081205368042 \n",
      "     Training Step: 227 Training Loss: 0.6114464998245239 \n",
      "     Training Step: 228 Training Loss: 0.6122863292694092 \n",
      "     Training Step: 229 Training Loss: 0.6115261912345886 \n",
      "     Training Step: 230 Training Loss: 0.6162313222885132 \n",
      "     Training Step: 231 Training Loss: 0.614621102809906 \n",
      "     Training Step: 232 Training Loss: 0.6184929609298706 \n",
      "     Training Step: 233 Training Loss: 0.6153519749641418 \n",
      "     Training Step: 234 Training Loss: 0.6166911721229553 \n",
      "     Training Step: 235 Training Loss: 0.6128404140472412 \n",
      "     Training Step: 236 Training Loss: 0.6126950979232788 \n",
      "     Training Step: 237 Training Loss: 0.6123787760734558 \n",
      "     Training Step: 238 Training Loss: 0.6167237758636475 \n",
      "     Training Step: 239 Training Loss: 0.6146121025085449 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6170217394828796 \n",
      "     Validation Step: 1 Validation Loss: 0.6101648211479187 \n",
      "     Validation Step: 2 Validation Loss: 0.6157961487770081 \n",
      "     Validation Step: 3 Validation Loss: 0.6101511120796204 \n",
      "     Validation Step: 4 Validation Loss: 0.6172999143600464 \n",
      "     Validation Step: 5 Validation Loss: 0.6184653639793396 \n",
      "     Validation Step: 6 Validation Loss: 0.6104906797409058 \n",
      "     Validation Step: 7 Validation Loss: 0.6111721396446228 \n",
      "     Validation Step: 8 Validation Loss: 0.6142517328262329 \n",
      "     Validation Step: 9 Validation Loss: 0.6180618405342102 \n",
      "     Validation Step: 10 Validation Loss: 0.614641010761261 \n",
      "     Validation Step: 11 Validation Loss: 0.6152270436286926 \n",
      "     Validation Step: 12 Validation Loss: 0.6153227090835571 \n",
      "     Validation Step: 13 Validation Loss: 0.6129893064498901 \n",
      "     Validation Step: 14 Validation Loss: 0.6111835837364197 \n",
      "     Validation Step: 15 Validation Loss: 0.6106374859809875 \n",
      "     Validation Step: 16 Validation Loss: 0.6155778765678406 \n",
      "     Validation Step: 17 Validation Loss: 0.613673985004425 \n",
      "     Validation Step: 18 Validation Loss: 0.6156175136566162 \n",
      "     Validation Step: 19 Validation Loss: 0.6145326495170593 \n",
      "     Validation Step: 20 Validation Loss: 0.6133037209510803 \n",
      "     Validation Step: 21 Validation Loss: 0.6116446256637573 \n",
      "     Validation Step: 22 Validation Loss: 0.6101229190826416 \n",
      "     Validation Step: 23 Validation Loss: 0.6118846535682678 \n",
      "     Validation Step: 24 Validation Loss: 0.6142022013664246 \n",
      "     Validation Step: 25 Validation Loss: 0.6150467395782471 \n",
      "     Validation Step: 26 Validation Loss: 0.6176995038986206 \n",
      "     Validation Step: 27 Validation Loss: 0.6136298179626465 \n",
      "     Validation Step: 28 Validation Loss: 0.6145682334899902 \n",
      "     Validation Step: 29 Validation Loss: 0.614890992641449 \n",
      "     Validation Step: 30 Validation Loss: 0.6160030961036682 \n",
      "     Validation Step: 31 Validation Loss: 0.6182341575622559 \n",
      "     Validation Step: 32 Validation Loss: 0.6162474751472473 \n",
      "     Validation Step: 33 Validation Loss: 0.6175910234451294 \n",
      "     Validation Step: 34 Validation Loss: 0.6121329069137573 \n",
      "     Validation Step: 35 Validation Loss: 0.6184908151626587 \n",
      "     Validation Step: 36 Validation Loss: 0.6128280758857727 \n",
      "     Validation Step: 37 Validation Loss: 0.6075370907783508 \n",
      "     Validation Step: 38 Validation Loss: 0.6105232238769531 \n",
      "     Validation Step: 39 Validation Loss: 0.6141121983528137 \n",
      "     Validation Step: 40 Validation Loss: 0.6115624308586121 \n",
      "     Validation Step: 41 Validation Loss: 0.6141345500946045 \n",
      "     Validation Step: 42 Validation Loss: 0.6183457970619202 \n",
      "     Validation Step: 43 Validation Loss: 0.6148359179496765 \n",
      "     Validation Step: 44 Validation Loss: 0.61366206407547 \n",
      "Epoch: 97\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6141098141670227 \n",
      "     Training Step: 1 Training Loss: 0.6173824071884155 \n",
      "     Training Step: 2 Training Loss: 0.6188402771949768 \n",
      "     Training Step: 3 Training Loss: 0.6154424548149109 \n",
      "     Training Step: 4 Training Loss: 0.615293562412262 \n",
      "     Training Step: 5 Training Loss: 0.6116431355476379 \n",
      "     Training Step: 6 Training Loss: 0.6161959767341614 \n",
      "     Training Step: 7 Training Loss: 0.6134829521179199 \n",
      "     Training Step: 8 Training Loss: 0.6166509985923767 \n",
      "     Training Step: 9 Training Loss: 0.6145048141479492 \n",
      "     Training Step: 10 Training Loss: 0.6171277165412903 \n",
      "     Training Step: 11 Training Loss: 0.6149443984031677 \n",
      "     Training Step: 12 Training Loss: 0.6106279492378235 \n",
      "     Training Step: 13 Training Loss: 0.6146772503852844 \n",
      "     Training Step: 14 Training Loss: 0.6100978851318359 \n",
      "     Training Step: 15 Training Loss: 0.6162341237068176 \n",
      "     Training Step: 16 Training Loss: 0.6168071031570435 \n",
      "     Training Step: 17 Training Loss: 0.6157602667808533 \n",
      "     Training Step: 18 Training Loss: 0.6106626391410828 \n",
      "     Training Step: 19 Training Loss: 0.6124614477157593 \n",
      "     Training Step: 20 Training Loss: 0.6111572980880737 \n",
      "     Training Step: 21 Training Loss: 0.6167356967926025 \n",
      "     Training Step: 22 Training Loss: 0.6094440221786499 \n",
      "     Training Step: 23 Training Loss: 0.6154325604438782 \n",
      "     Training Step: 24 Training Loss: 0.6137696504592896 \n",
      "     Training Step: 25 Training Loss: 0.6118263006210327 \n",
      "     Training Step: 26 Training Loss: 0.6151132583618164 \n",
      "     Training Step: 27 Training Loss: 0.614752471446991 \n",
      "     Training Step: 28 Training Loss: 0.6153786778450012 \n",
      "     Training Step: 29 Training Loss: 0.6115186810493469 \n",
      "     Training Step: 30 Training Loss: 0.6202377080917358 \n",
      "     Training Step: 31 Training Loss: 0.616722047328949 \n",
      "     Training Step: 32 Training Loss: 0.6142077445983887 \n",
      "     Training Step: 33 Training Loss: 0.6139190793037415 \n",
      "     Training Step: 34 Training Loss: 0.6143468022346497 \n",
      "     Training Step: 35 Training Loss: 0.6105324029922485 \n",
      "     Training Step: 36 Training Loss: 0.6180562973022461 \n",
      "     Training Step: 37 Training Loss: 0.6128138899803162 \n",
      "     Training Step: 38 Training Loss: 0.613426685333252 \n",
      "     Training Step: 39 Training Loss: 0.6155298352241516 \n",
      "     Training Step: 40 Training Loss: 0.6131911277770996 \n",
      "     Training Step: 41 Training Loss: 0.6125215291976929 \n",
      "     Training Step: 42 Training Loss: 0.613060712814331 \n",
      "     Training Step: 43 Training Loss: 0.611586332321167 \n",
      "     Training Step: 44 Training Loss: 0.6099814176559448 \n",
      "     Training Step: 45 Training Loss: 0.6177054047584534 \n",
      "     Training Step: 46 Training Loss: 0.611517608165741 \n",
      "     Training Step: 47 Training Loss: 0.613645076751709 \n",
      "     Training Step: 48 Training Loss: 0.6152875423431396 \n",
      "     Training Step: 49 Training Loss: 0.6143596172332764 \n",
      "     Training Step: 50 Training Loss: 0.6129200458526611 \n",
      "     Training Step: 51 Training Loss: 0.6168335676193237 \n",
      "     Training Step: 52 Training Loss: 0.61347496509552 \n",
      "     Training Step: 53 Training Loss: 0.6156153082847595 \n",
      "     Training Step: 54 Training Loss: 0.6118013262748718 \n",
      "     Training Step: 55 Training Loss: 0.6121207475662231 \n",
      "     Training Step: 56 Training Loss: 0.610596776008606 \n",
      "     Training Step: 57 Training Loss: 0.6114181876182556 \n",
      "     Training Step: 58 Training Loss: 0.6082720160484314 \n",
      "     Training Step: 59 Training Loss: 0.6139259934425354 \n",
      "     Training Step: 60 Training Loss: 0.6186210513114929 \n",
      "     Training Step: 61 Training Loss: 0.6097186207771301 \n",
      "     Training Step: 62 Training Loss: 0.6124876141548157 \n",
      "     Training Step: 63 Training Loss: 0.6128383874893188 \n",
      "     Training Step: 64 Training Loss: 0.6108864545822144 \n",
      "     Training Step: 65 Training Loss: 0.6125238537788391 \n",
      "     Training Step: 66 Training Loss: 0.6160509586334229 \n",
      "     Training Step: 67 Training Loss: 0.6123707294464111 \n",
      "     Training Step: 68 Training Loss: 0.6153966784477234 \n",
      "     Training Step: 69 Training Loss: 0.6171748042106628 \n",
      "     Training Step: 70 Training Loss: 0.6142464876174927 \n",
      "     Training Step: 71 Training Loss: 0.6177804470062256 \n",
      "     Training Step: 72 Training Loss: 0.6133143305778503 \n",
      "     Training Step: 73 Training Loss: 0.6149234175682068 \n",
      "     Training Step: 74 Training Loss: 0.6147133708000183 \n",
      "     Training Step: 75 Training Loss: 0.6117039322853088 \n",
      "     Training Step: 76 Training Loss: 0.6138951778411865 \n",
      "     Training Step: 77 Training Loss: 0.6116500496864319 \n",
      "     Training Step: 78 Training Loss: 0.6152389645576477 \n",
      "     Training Step: 79 Training Loss: 0.6132060885429382 \n",
      "     Training Step: 80 Training Loss: 0.6140235066413879 \n",
      "     Training Step: 81 Training Loss: 0.6150875687599182 \n",
      "     Training Step: 82 Training Loss: 0.6127534508705139 \n",
      "     Training Step: 83 Training Loss: 0.6127643585205078 \n",
      "     Training Step: 84 Training Loss: 0.6157460808753967 \n",
      "     Training Step: 85 Training Loss: 0.6155394315719604 \n",
      "     Training Step: 86 Training Loss: 0.6112004518508911 \n",
      "     Training Step: 87 Training Loss: 0.6171666383743286 \n",
      "     Training Step: 88 Training Loss: 0.6158328056335449 \n",
      "     Training Step: 89 Training Loss: 0.6123767495155334 \n",
      "     Training Step: 90 Training Loss: 0.6132887005805969 \n",
      "     Training Step: 91 Training Loss: 0.6100690960884094 \n",
      "     Training Step: 92 Training Loss: 0.6117782592773438 \n",
      "     Training Step: 93 Training Loss: 0.6167683005332947 \n",
      "     Training Step: 94 Training Loss: 0.6105679273605347 \n",
      "     Training Step: 95 Training Loss: 0.6118246912956238 \n",
      "     Training Step: 96 Training Loss: 0.611792266368866 \n",
      "     Training Step: 97 Training Loss: 0.6147757172584534 \n",
      "     Training Step: 98 Training Loss: 0.6184730529785156 \n",
      "     Training Step: 99 Training Loss: 0.6121550798416138 \n",
      "     Training Step: 100 Training Loss: 0.6122867465019226 \n",
      "     Training Step: 101 Training Loss: 0.6144536137580872 \n",
      "     Training Step: 102 Training Loss: 0.6147271990776062 \n",
      "     Training Step: 103 Training Loss: 0.6103696823120117 \n",
      "     Training Step: 104 Training Loss: 0.61139315366745 \n",
      "     Training Step: 105 Training Loss: 0.6184824109077454 \n",
      "     Training Step: 106 Training Loss: 0.6159557104110718 \n",
      "     Training Step: 107 Training Loss: 0.6101320385932922 \n",
      "     Training Step: 108 Training Loss: 0.6126928925514221 \n",
      "     Training Step: 109 Training Loss: 0.6115472912788391 \n",
      "     Training Step: 110 Training Loss: 0.6116273999214172 \n",
      "     Training Step: 111 Training Loss: 0.6155329942703247 \n",
      "     Training Step: 112 Training Loss: 0.6138154864311218 \n",
      "     Training Step: 113 Training Loss: 0.6135007739067078 \n",
      "     Training Step: 114 Training Loss: 0.6160121560096741 \n",
      "     Training Step: 115 Training Loss: 0.6161239147186279 \n",
      "     Training Step: 116 Training Loss: 0.6151773929595947 \n",
      "     Training Step: 117 Training Loss: 0.6105655431747437 \n",
      "     Training Step: 118 Training Loss: 0.6100388169288635 \n",
      "     Training Step: 119 Training Loss: 0.6182355880737305 \n",
      "     Training Step: 120 Training Loss: 0.6106856465339661 \n",
      "     Training Step: 121 Training Loss: 0.6167238354682922 \n",
      "     Training Step: 122 Training Loss: 0.6107203364372253 \n",
      "     Training Step: 123 Training Loss: 0.6156876087188721 \n",
      "     Training Step: 124 Training Loss: 0.6101800203323364 \n",
      "     Training Step: 125 Training Loss: 0.6118793487548828 \n",
      "     Training Step: 126 Training Loss: 0.6128715872764587 \n",
      "     Training Step: 127 Training Loss: 0.6164292097091675 \n",
      "     Training Step: 128 Training Loss: 0.620963454246521 \n",
      "     Training Step: 129 Training Loss: 0.6104804277420044 \n",
      "     Training Step: 130 Training Loss: 0.6131159067153931 \n",
      "     Training Step: 131 Training Loss: 0.6180447340011597 \n",
      "     Training Step: 132 Training Loss: 0.6140143275260925 \n",
      "     Training Step: 133 Training Loss: 0.610740602016449 \n",
      "     Training Step: 134 Training Loss: 0.6171936392784119 \n",
      "     Training Step: 135 Training Loss: 0.6167060136795044 \n",
      "     Training Step: 136 Training Loss: 0.6114445328712463 \n",
      "     Training Step: 137 Training Loss: 0.6166126728057861 \n",
      "     Training Step: 138 Training Loss: 0.61708664894104 \n",
      "     Training Step: 139 Training Loss: 0.6092580556869507 \n",
      "     Training Step: 140 Training Loss: 0.6132845878601074 \n",
      "     Training Step: 141 Training Loss: 0.6135175824165344 \n",
      "     Training Step: 142 Training Loss: 0.6133183240890503 \n",
      "     Training Step: 143 Training Loss: 0.6141482591629028 \n",
      "     Training Step: 144 Training Loss: 0.6164924502372742 \n",
      "     Training Step: 145 Training Loss: 0.6148732900619507 \n",
      "     Training Step: 146 Training Loss: 0.6108783483505249 \n",
      "     Training Step: 147 Training Loss: 0.6146111488342285 \n",
      "     Training Step: 148 Training Loss: 0.6150685548782349 \n",
      "     Training Step: 149 Training Loss: 0.6122270226478577 \n",
      "     Training Step: 150 Training Loss: 0.6097256541252136 \n",
      "     Training Step: 151 Training Loss: 0.6137344837188721 \n",
      "     Training Step: 152 Training Loss: 0.6162137389183044 \n",
      "     Training Step: 153 Training Loss: 0.6166658997535706 \n",
      "     Training Step: 154 Training Loss: 0.616941511631012 \n",
      "     Training Step: 155 Training Loss: 0.6183292865753174 \n",
      "     Training Step: 156 Training Loss: 0.6114842891693115 \n",
      "     Training Step: 157 Training Loss: 0.6180153489112854 \n",
      "     Training Step: 158 Training Loss: 0.6146066784858704 \n",
      "     Training Step: 159 Training Loss: 0.6177788376808167 \n",
      "     Training Step: 160 Training Loss: 0.6195886731147766 \n",
      "     Training Step: 161 Training Loss: 0.6115676760673523 \n",
      "     Training Step: 162 Training Loss: 0.6125621795654297 \n",
      "     Training Step: 163 Training Loss: 0.6183752417564392 \n",
      "     Training Step: 164 Training Loss: 0.614295244216919 \n",
      "     Training Step: 165 Training Loss: 0.6153510808944702 \n",
      "     Training Step: 166 Training Loss: 0.6144928336143494 \n",
      "     Training Step: 167 Training Loss: 0.6146577000617981 \n",
      "     Training Step: 168 Training Loss: 0.619828462600708 \n",
      "     Training Step: 169 Training Loss: 0.6111831665039062 \n",
      "     Training Step: 170 Training Loss: 0.6121614575386047 \n",
      "     Training Step: 171 Training Loss: 0.6142297387123108 \n",
      "     Training Step: 172 Training Loss: 0.6180943250656128 \n",
      "     Training Step: 173 Training Loss: 0.6137458086013794 \n",
      "     Training Step: 174 Training Loss: 0.6128681898117065 \n",
      "     Training Step: 175 Training Loss: 0.6120256781578064 \n",
      "     Training Step: 176 Training Loss: 0.6114383339881897 \n",
      "     Training Step: 177 Training Loss: 0.6154286861419678 \n",
      "     Training Step: 178 Training Loss: 0.6136184334754944 \n",
      "     Training Step: 179 Training Loss: 0.6136511564254761 \n",
      "     Training Step: 180 Training Loss: 0.612278401851654 \n",
      "     Training Step: 181 Training Loss: 0.6103880405426025 \n",
      "     Training Step: 182 Training Loss: 0.6140363216400146 \n",
      "     Training Step: 183 Training Loss: 0.6141547560691833 \n",
      "     Training Step: 184 Training Loss: 0.6129254102706909 \n",
      "     Training Step: 185 Training Loss: 0.6118324398994446 \n",
      "     Training Step: 186 Training Loss: 0.6146646738052368 \n",
      "     Training Step: 187 Training Loss: 0.615803062915802 \n",
      "     Training Step: 188 Training Loss: 0.6096846461296082 \n",
      "     Training Step: 189 Training Loss: 0.6122393608093262 \n",
      "     Training Step: 190 Training Loss: 0.6178398728370667 \n",
      "     Training Step: 191 Training Loss: 0.6130702495574951 \n",
      "     Training Step: 192 Training Loss: 0.6140826344490051 \n",
      "     Training Step: 193 Training Loss: 0.6154796481132507 \n",
      "     Training Step: 194 Training Loss: 0.6166777610778809 \n",
      "     Training Step: 195 Training Loss: 0.614956259727478 \n",
      "     Training Step: 196 Training Loss: 0.6126457452774048 \n",
      "     Training Step: 197 Training Loss: 0.6114211082458496 \n",
      "     Training Step: 198 Training Loss: 0.6143686175346375 \n",
      "     Training Step: 199 Training Loss: 0.6174861788749695 \n",
      "     Training Step: 200 Training Loss: 0.6132987141609192 \n",
      "     Training Step: 201 Training Loss: 0.6120774745941162 \n",
      "     Training Step: 202 Training Loss: 0.6146758198738098 \n",
      "     Training Step: 203 Training Loss: 0.6146884560585022 \n",
      "     Training Step: 204 Training Loss: 0.6133571267127991 \n",
      "     Training Step: 205 Training Loss: 0.6152887940406799 \n",
      "     Training Step: 206 Training Loss: 0.6152790784835815 \n",
      "     Training Step: 207 Training Loss: 0.6163492202758789 \n",
      "     Training Step: 208 Training Loss: 0.6151575446128845 \n",
      "     Training Step: 209 Training Loss: 0.6146407723426819 \n",
      "     Training Step: 210 Training Loss: 0.6194411516189575 \n",
      "     Training Step: 211 Training Loss: 0.6129701137542725 \n",
      "     Training Step: 212 Training Loss: 0.6168861985206604 \n",
      "     Training Step: 213 Training Loss: 0.6176983118057251 \n",
      "     Training Step: 214 Training Loss: 0.61679607629776 \n",
      "     Training Step: 215 Training Loss: 0.6123923063278198 \n",
      "     Training Step: 216 Training Loss: 0.6143468618392944 \n",
      "     Training Step: 217 Training Loss: 0.6156851053237915 \n",
      "     Training Step: 218 Training Loss: 0.6182170510292053 \n",
      "     Training Step: 219 Training Loss: 0.6153727769851685 \n",
      "     Training Step: 220 Training Loss: 0.6166343092918396 \n",
      "     Training Step: 221 Training Loss: 0.618840754032135 \n",
      "     Training Step: 222 Training Loss: 0.6148076057434082 \n",
      "     Training Step: 223 Training Loss: 0.6176693439483643 \n",
      "     Training Step: 224 Training Loss: 0.6157450675964355 \n",
      "     Training Step: 225 Training Loss: 0.6164025068283081 \n",
      "     Training Step: 226 Training Loss: 0.613220751285553 \n",
      "     Training Step: 227 Training Loss: 0.609462559223175 \n",
      "     Training Step: 228 Training Loss: 0.6122649312019348 \n",
      "     Training Step: 229 Training Loss: 0.6147079467773438 \n",
      "     Training Step: 230 Training Loss: 0.6185728907585144 \n",
      "     Training Step: 231 Training Loss: 0.6197038292884827 \n",
      "     Training Step: 232 Training Loss: 0.6116470098495483 \n",
      "     Training Step: 233 Training Loss: 0.6144426465034485 \n",
      "     Training Step: 234 Training Loss: 0.6131460666656494 \n",
      "     Training Step: 235 Training Loss: 0.6144250631332397 \n",
      "     Training Step: 236 Training Loss: 0.6176395416259766 \n",
      "     Training Step: 237 Training Loss: 0.6135881543159485 \n",
      "     Training Step: 238 Training Loss: 0.614605724811554 \n",
      "     Training Step: 239 Training Loss: 0.612167477607727 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6136531829833984 \n",
      "     Validation Step: 1 Validation Loss: 0.6146366596221924 \n",
      "     Validation Step: 2 Validation Loss: 0.6150356531143188 \n",
      "     Validation Step: 3 Validation Loss: 0.6101506948471069 \n",
      "     Validation Step: 4 Validation Loss: 0.6145334839820862 \n",
      "     Validation Step: 5 Validation Loss: 0.6141158938407898 \n",
      "     Validation Step: 6 Validation Loss: 0.6105246543884277 \n",
      "     Validation Step: 7 Validation Loss: 0.610489010810852 \n",
      "     Validation Step: 8 Validation Loss: 0.614894449710846 \n",
      "     Validation Step: 9 Validation Loss: 0.6111693978309631 \n",
      "     Validation Step: 10 Validation Loss: 0.6121348738670349 \n",
      "     Validation Step: 11 Validation Loss: 0.6142007112503052 \n",
      "     Validation Step: 12 Validation Loss: 0.613307535648346 \n",
      "     Validation Step: 13 Validation Loss: 0.6101729273796082 \n",
      "     Validation Step: 14 Validation Loss: 0.6136702299118042 \n",
      "     Validation Step: 15 Validation Loss: 0.6136388182640076 \n",
      "     Validation Step: 16 Validation Loss: 0.6176921129226685 \n",
      "     Validation Step: 17 Validation Loss: 0.611648440361023 \n",
      "     Validation Step: 18 Validation Loss: 0.6129880547523499 \n",
      "     Validation Step: 19 Validation Loss: 0.6156145930290222 \n",
      "     Validation Step: 20 Validation Loss: 0.617302656173706 \n",
      "     Validation Step: 21 Validation Loss: 0.6142534017562866 \n",
      "     Validation Step: 22 Validation Loss: 0.615993082523346 \n",
      "     Validation Step: 23 Validation Loss: 0.6175928115844727 \n",
      "     Validation Step: 24 Validation Loss: 0.6157914400100708 \n",
      "     Validation Step: 25 Validation Loss: 0.6141273975372314 \n",
      "     Validation Step: 26 Validation Loss: 0.6155704259872437 \n",
      "     Validation Step: 27 Validation Loss: 0.6075384616851807 \n",
      "     Validation Step: 28 Validation Loss: 0.6118879318237305 \n",
      "     Validation Step: 29 Validation Loss: 0.6153169870376587 \n",
      "     Validation Step: 30 Validation Loss: 0.6148364543914795 \n",
      "     Validation Step: 31 Validation Loss: 0.6128351092338562 \n",
      "     Validation Step: 32 Validation Loss: 0.6183354258537292 \n",
      "     Validation Step: 33 Validation Loss: 0.6101212501525879 \n",
      "     Validation Step: 34 Validation Loss: 0.6106409430503845 \n",
      "     Validation Step: 35 Validation Loss: 0.6184887290000916 \n",
      "     Validation Step: 36 Validation Loss: 0.6170163750648499 \n",
      "     Validation Step: 37 Validation Loss: 0.6184645891189575 \n",
      "     Validation Step: 38 Validation Loss: 0.6162438988685608 \n",
      "     Validation Step: 39 Validation Loss: 0.6111846566200256 \n",
      "     Validation Step: 40 Validation Loss: 0.6115621328353882 \n",
      "     Validation Step: 41 Validation Loss: 0.6182258129119873 \n",
      "     Validation Step: 42 Validation Loss: 0.6180562376976013 \n",
      "     Validation Step: 43 Validation Loss: 0.615227222442627 \n",
      "     Validation Step: 44 Validation Loss: 0.6145709753036499 \n",
      "Epoch: 98\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6168010234832764 \n",
      "     Training Step: 1 Training Loss: 0.616705060005188 \n",
      "     Training Step: 2 Training Loss: 0.6128786206245422 \n",
      "     Training Step: 3 Training Loss: 0.6194286942481995 \n",
      "     Training Step: 4 Training Loss: 0.6166685819625854 \n",
      "     Training Step: 5 Training Loss: 0.6146910190582275 \n",
      "     Training Step: 6 Training Loss: 0.612392783164978 \n",
      "     Training Step: 7 Training Loss: 0.6170814037322998 \n",
      "     Training Step: 8 Training Loss: 0.6114381551742554 \n",
      "     Training Step: 9 Training Loss: 0.6101709604263306 \n",
      "     Training Step: 10 Training Loss: 0.6169138550758362 \n",
      "     Training Step: 11 Training Loss: 0.6129148602485657 \n",
      "     Training Step: 12 Training Loss: 0.6125374436378479 \n",
      "     Training Step: 13 Training Loss: 0.6147276163101196 \n",
      "     Training Step: 14 Training Loss: 0.6118291616439819 \n",
      "     Training Step: 15 Training Loss: 0.6182447671890259 \n",
      "     Training Step: 16 Training Loss: 0.6177700757980347 \n",
      "     Training Step: 17 Training Loss: 0.6144812703132629 \n",
      "     Training Step: 18 Training Loss: 0.612699031829834 \n",
      "     Training Step: 19 Training Loss: 0.6160045266151428 \n",
      "     Training Step: 20 Training Loss: 0.6100630164146423 \n",
      "     Training Step: 21 Training Loss: 0.6137427687644958 \n",
      "     Training Step: 22 Training Loss: 0.6158428192138672 \n",
      "     Training Step: 23 Training Loss: 0.6105728149414062 \n",
      "     Training Step: 24 Training Loss: 0.609453558921814 \n",
      "     Training Step: 25 Training Loss: 0.6177343726158142 \n",
      "     Training Step: 26 Training Loss: 0.6113904118537903 \n",
      "     Training Step: 27 Training Loss: 0.6141090393066406 \n",
      "     Training Step: 28 Training Loss: 0.6140193343162537 \n",
      "     Training Step: 29 Training Loss: 0.6177060604095459 \n",
      "     Training Step: 30 Training Loss: 0.6114688515663147 \n",
      "     Training Step: 31 Training Loss: 0.6138184666633606 \n",
      "     Training Step: 32 Training Loss: 0.6178149580955505 \n",
      "     Training Step: 33 Training Loss: 0.6166275143623352 \n",
      "     Training Step: 34 Training Loss: 0.6144506335258484 \n",
      "     Training Step: 35 Training Loss: 0.6143362522125244 \n",
      "     Training Step: 36 Training Loss: 0.6147722005844116 \n",
      "     Training Step: 37 Training Loss: 0.6118069291114807 \n",
      "     Training Step: 38 Training Loss: 0.6127613186836243 \n",
      "     Training Step: 39 Training Loss: 0.6140454411506653 \n",
      "     Training Step: 40 Training Loss: 0.6125310659408569 \n",
      "     Training Step: 41 Training Loss: 0.6139299273490906 \n",
      "     Training Step: 42 Training Loss: 0.6156786680221558 \n",
      "     Training Step: 43 Training Loss: 0.6106955409049988 \n",
      "     Training Step: 44 Training Loss: 0.6145987510681152 \n",
      "     Training Step: 45 Training Loss: 0.610403835773468 \n",
      "     Training Step: 46 Training Loss: 0.6146627068519592 \n",
      "     Training Step: 47 Training Loss: 0.6166622042655945 \n",
      "     Training Step: 48 Training Loss: 0.6146132946014404 \n",
      "     Training Step: 49 Training Loss: 0.6115188598632812 \n",
      "     Training Step: 50 Training Loss: 0.6140211224555969 \n",
      "     Training Step: 51 Training Loss: 0.6146416068077087 \n",
      "     Training Step: 52 Training Loss: 0.6114331483840942 \n",
      "     Training Step: 53 Training Loss: 0.6154308915138245 \n",
      "     Training Step: 54 Training Loss: 0.6188744902610779 \n",
      "     Training Step: 55 Training Loss: 0.6144383549690247 \n",
      "     Training Step: 56 Training Loss: 0.6152712106704712 \n",
      "     Training Step: 57 Training Loss: 0.6163531541824341 \n",
      "     Training Step: 58 Training Loss: 0.6150983572006226 \n",
      "     Training Step: 59 Training Loss: 0.6155990958213806 \n",
      "     Training Step: 60 Training Loss: 0.6142213940620422 \n",
      "     Training Step: 61 Training Loss: 0.6176806688308716 \n",
      "     Training Step: 62 Training Loss: 0.6123149991035461 \n",
      "     Training Step: 63 Training Loss: 0.6136420369148254 \n",
      "     Training Step: 64 Training Loss: 0.6157429218292236 \n",
      "     Training Step: 65 Training Loss: 0.6148684024810791 \n",
      "     Training Step: 66 Training Loss: 0.6159400343894958 \n",
      "     Training Step: 67 Training Loss: 0.6177952289581299 \n",
      "     Training Step: 68 Training Loss: 0.6201963424682617 \n",
      "     Training Step: 69 Training Loss: 0.614345371723175 \n",
      "     Training Step: 70 Training Loss: 0.6164059042930603 \n",
      "     Training Step: 71 Training Loss: 0.6114491820335388 \n",
      "     Training Step: 72 Training Loss: 0.6146760582923889 \n",
      "     Training Step: 73 Training Loss: 0.6208992600440979 \n",
      "     Training Step: 74 Training Loss: 0.6146100759506226 \n",
      "     Training Step: 75 Training Loss: 0.6140789985656738 \n",
      "     Training Step: 76 Training Loss: 0.6184194087982178 \n",
      "     Training Step: 77 Training Loss: 0.6198357939720154 \n",
      "     Training Step: 78 Training Loss: 0.6111827492713928 \n",
      "     Training Step: 79 Training Loss: 0.6133081316947937 \n",
      "     Training Step: 80 Training Loss: 0.6155267357826233 \n",
      "     Training Step: 81 Training Loss: 0.6147052049636841 \n",
      "     Training Step: 82 Training Loss: 0.6097778081893921 \n",
      "     Training Step: 83 Training Loss: 0.6185619831085205 \n",
      "     Training Step: 84 Training Loss: 0.6144250631332397 \n",
      "     Training Step: 85 Training Loss: 0.6130706667900085 \n",
      "     Training Step: 86 Training Loss: 0.6115440726280212 \n",
      "     Training Step: 87 Training Loss: 0.6152915954589844 \n",
      "     Training Step: 88 Training Loss: 0.6157581806182861 \n",
      "     Training Step: 89 Training Loss: 0.6134965419769287 \n",
      "     Training Step: 90 Training Loss: 0.6171472072601318 \n",
      "     Training Step: 91 Training Loss: 0.6132058501243591 \n",
      "     Training Step: 92 Training Loss: 0.6160370707511902 \n",
      "     Training Step: 93 Training Loss: 0.6155308485031128 \n",
      "     Training Step: 94 Training Loss: 0.6099897623062134 \n",
      "     Training Step: 95 Training Loss: 0.6136496067047119 \n",
      "     Training Step: 96 Training Loss: 0.6183400750160217 \n",
      "     Training Step: 97 Training Loss: 0.6123786568641663 \n",
      "     Training Step: 98 Training Loss: 0.6107302308082581 \n",
      "     Training Step: 99 Training Loss: 0.6161138415336609 \n",
      "     Training Step: 100 Training Loss: 0.6100558638572693 \n",
      "     Training Step: 101 Training Loss: 0.6116440892219543 \n",
      "     Training Step: 102 Training Loss: 0.6149563789367676 \n",
      "     Training Step: 103 Training Loss: 0.6153953671455383 \n",
      "     Training Step: 104 Training Loss: 0.6167311072349548 \n",
      "     Training Step: 105 Training Loss: 0.6162106990814209 \n",
      "     Training Step: 106 Training Loss: 0.6082548499107361 \n",
      "     Training Step: 107 Training Loss: 0.6149325370788574 \n",
      "     Training Step: 108 Training Loss: 0.6115446090698242 \n",
      "     Training Step: 109 Training Loss: 0.6137385964393616 \n",
      "     Training Step: 110 Training Loss: 0.6133080124855042 \n",
      "     Training Step: 111 Training Loss: 0.6157969832420349 \n",
      "     Training Step: 112 Training Loss: 0.6122255325317383 \n",
      "     Training Step: 113 Training Loss: 0.6115732789039612 \n",
      "     Training Step: 114 Training Loss: 0.6171881556510925 \n",
      "     Training Step: 115 Training Loss: 0.6135060787200928 \n",
      "     Training Step: 116 Training Loss: 0.6118171811103821 \n",
      "     Training Step: 117 Training Loss: 0.613643229007721 \n",
      "     Training Step: 118 Training Loss: 0.6186352372169495 \n",
      "     Training Step: 119 Training Loss: 0.6180446147918701 \n",
      "     Training Step: 120 Training Loss: 0.6105681657791138 \n",
      "     Training Step: 121 Training Loss: 0.6197118163108826 \n",
      "     Training Step: 122 Training Loss: 0.613768458366394 \n",
      "     Training Step: 123 Training Loss: 0.6124714612960815 \n",
      "     Training Step: 124 Training Loss: 0.6131299138069153 \n",
      "     Training Step: 125 Training Loss: 0.6196027994155884 \n",
      "     Training Step: 126 Training Loss: 0.6102133989334106 \n",
      "     Training Step: 127 Training Loss: 0.6134750247001648 \n",
      "     Training Step: 128 Training Loss: 0.6122360229492188 \n",
      "     Training Step: 129 Training Loss: 0.6184507012367249 \n",
      "     Training Step: 130 Training Loss: 0.6121445298194885 \n",
      "     Training Step: 131 Training Loss: 0.6128436326980591 \n",
      "     Training Step: 132 Training Loss: 0.6123831272125244 \n",
      "     Training Step: 133 Training Loss: 0.6116249561309814 \n",
      "     Training Step: 134 Training Loss: 0.6147485971450806 \n",
      "     Training Step: 135 Training Loss: 0.6111605167388916 \n",
      "     Training Step: 136 Training Loss: 0.6138878464698792 \n",
      "     Training Step: 137 Training Loss: 0.6165109872817993 \n",
      "     Training Step: 138 Training Loss: 0.6131442785263062 \n",
      "     Training Step: 139 Training Loss: 0.6166628003120422 \n",
      "     Training Step: 140 Training Loss: 0.6154804825782776 \n",
      "     Training Step: 141 Training Loss: 0.6180844902992249 \n",
      "     Training Step: 142 Training Loss: 0.6129662990570068 \n",
      "     Training Step: 143 Training Loss: 0.6139135360717773 \n",
      "     Training Step: 144 Training Loss: 0.6162391304969788 \n",
      "     Training Step: 145 Training Loss: 0.6133608222007751 \n",
      "     Training Step: 146 Training Loss: 0.6125417947769165 \n",
      "     Training Step: 147 Training Loss: 0.6130715012550354 \n",
      "     Training Step: 148 Training Loss: 0.6155291795730591 \n",
      "     Training Step: 149 Training Loss: 0.6173713207244873 \n",
      "     Training Step: 150 Training Loss: 0.6147043108940125 \n",
      "     Training Step: 151 Training Loss: 0.6183890700340271 \n",
      "     Training Step: 152 Training Loss: 0.6116561889648438 \n",
      "     Training Step: 153 Training Loss: 0.6112245321273804 \n",
      "     Training Step: 154 Training Loss: 0.6153380870819092 \n",
      "     Training Step: 155 Training Loss: 0.6152447462081909 \n",
      "     Training Step: 156 Training Loss: 0.6092547178268433 \n",
      "     Training Step: 157 Training Loss: 0.6152805089950562 \n",
      "     Training Step: 158 Training Loss: 0.6134225130081177 \n",
      "     Training Step: 159 Training Loss: 0.613588273525238 \n",
      "     Training Step: 160 Training Loss: 0.6121608018875122 \n",
      "     Training Step: 161 Training Loss: 0.6151587963104248 \n",
      "     Training Step: 162 Training Loss: 0.6132904291152954 \n",
      "     Training Step: 163 Training Loss: 0.6120104193687439 \n",
      "     Training Step: 164 Training Loss: 0.6103615760803223 \n",
      "     Training Step: 165 Training Loss: 0.6142246723175049 \n",
      "     Training Step: 166 Training Loss: 0.6114186644554138 \n",
      "     Training Step: 167 Training Loss: 0.6157603859901428 \n",
      "     Training Step: 168 Training Loss: 0.6181401610374451 \n",
      "     Training Step: 169 Training Loss: 0.6117808818817139 \n",
      "     Training Step: 170 Training Loss: 0.6128661036491394 \n",
      "     Training Step: 171 Training Loss: 0.613267183303833 \n",
      "     Training Step: 172 Training Loss: 0.6128026247024536 \n",
      "     Training Step: 173 Training Loss: 0.6129155158996582 \n",
      "     Training Step: 174 Training Loss: 0.6182423830032349 \n",
      "     Training Step: 175 Training Loss: 0.6146610379219055 \n",
      "     Training Step: 176 Training Loss: 0.6142831444740295 \n",
      "     Training Step: 177 Training Loss: 0.6153028607368469 \n",
      "     Training Step: 178 Training Loss: 0.6156835556030273 \n",
      "     Training Step: 179 Training Loss: 0.6142433881759644 \n",
      "     Training Step: 180 Training Loss: 0.6118492484092712 \n",
      "     Training Step: 181 Training Loss: 0.6188613772392273 \n",
      "     Training Step: 182 Training Loss: 0.6143531203269958 \n",
      "     Training Step: 183 Training Loss: 0.6105152368545532 \n",
      "     Training Step: 184 Training Loss: 0.6154401898384094 \n",
      "     Training Step: 185 Training Loss: 0.6118955016136169 \n",
      "     Training Step: 186 Training Loss: 0.6168873906135559 \n",
      "     Training Step: 187 Training Loss: 0.6141496896743774 \n",
      "     Training Step: 188 Training Loss: 0.6115330457687378 \n",
      "     Training Step: 189 Training Loss: 0.6116259694099426 \n",
      "     Training Step: 190 Training Loss: 0.6107354760169983 \n",
      "     Training Step: 191 Training Loss: 0.6104920506477356 \n",
      "     Training Step: 192 Training Loss: 0.6133045554161072 \n",
      "     Training Step: 193 Training Loss: 0.6151857376098633 \n",
      "     Training Step: 194 Training Loss: 0.6149333715438843 \n",
      "     Training Step: 195 Training Loss: 0.6108920574188232 \n",
      "     Training Step: 196 Training Loss: 0.6153911352157593 \n",
      "     Training Step: 197 Training Loss: 0.6120598912239075 \n",
      "     Training Step: 198 Training Loss: 0.6126381754875183 \n",
      "     Training Step: 199 Training Loss: 0.6171766519546509 \n",
      "     Training Step: 200 Training Loss: 0.6131832003593445 \n",
      "     Training Step: 201 Training Loss: 0.6132006645202637 \n",
      "     Training Step: 202 Training Loss: 0.6180544495582581 \n",
      "     Training Step: 203 Training Loss: 0.6168116331100464 \n",
      "     Training Step: 204 Training Loss: 0.6167228817939758 \n",
      "     Training Step: 205 Training Loss: 0.6143762469291687 \n",
      "     Training Step: 206 Training Loss: 0.6164087653160095 \n",
      "     Training Step: 207 Training Loss: 0.6122501492500305 \n",
      "     Training Step: 208 Training Loss: 0.6171817183494568 \n",
      "     Training Step: 209 Training Loss: 0.6109292507171631 \n",
      "     Training Step: 210 Training Loss: 0.6150669455528259 \n",
      "     Training Step: 211 Training Loss: 0.6118418574333191 \n",
      "     Training Step: 212 Training Loss: 0.6176117062568665 \n",
      "     Training Step: 213 Training Loss: 0.6121639609336853 \n",
      "     Training Step: 214 Training Loss: 0.6167932152748108 \n",
      "     Training Step: 215 Training Loss: 0.6106110215187073 \n",
      "     Training Step: 216 Training Loss: 0.6122944951057434 \n",
      "     Training Step: 217 Training Loss: 0.6166766285896301 \n",
      "     Training Step: 218 Training Loss: 0.6153789162635803 \n",
      "     Training Step: 219 Training Loss: 0.6097058057785034 \n",
      "     Training Step: 220 Training Loss: 0.6167733669281006 \n",
      "     Training Step: 221 Training Loss: 0.6141539216041565 \n",
      "     Training Step: 222 Training Loss: 0.6106582283973694 \n",
      "     Training Step: 223 Training Loss: 0.6134552955627441 \n",
      "     Training Step: 224 Training Loss: 0.612119734287262 \n",
      "     Training Step: 225 Training Loss: 0.6167387366294861 \n",
      "     Training Step: 226 Training Loss: 0.6093790531158447 \n",
      "     Training Step: 227 Training Loss: 0.6100260615348816 \n",
      "     Training Step: 228 Training Loss: 0.6097173690795898 \n",
      "     Training Step: 229 Training Loss: 0.6105756163597107 \n",
      "     Training Step: 230 Training Loss: 0.6151024103164673 \n",
      "     Training Step: 231 Training Loss: 0.6146745085716248 \n",
      "     Training Step: 232 Training Loss: 0.6127680540084839 \n",
      "     Training Step: 233 Training Loss: 0.6124838590621948 \n",
      "     Training Step: 234 Training Loss: 0.6145115494728088 \n",
      "     Training Step: 235 Training Loss: 0.6154415011405945 \n",
      "     Training Step: 236 Training Loss: 0.614797055721283 \n",
      "     Training Step: 237 Training Loss: 0.6162257790565491 \n",
      "     Training Step: 238 Training Loss: 0.6116885542869568 \n",
      "     Training Step: 239 Training Loss: 0.6174930930137634 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6152275204658508 \n",
      "     Validation Step: 1 Validation Loss: 0.6182298064231873 \n",
      "     Validation Step: 2 Validation Loss: 0.6142534017562866 \n",
      "     Validation Step: 3 Validation Loss: 0.6101492047309875 \n",
      "     Validation Step: 4 Validation Loss: 0.6150406002998352 \n",
      "     Validation Step: 5 Validation Loss: 0.6121341586112976 \n",
      "     Validation Step: 6 Validation Loss: 0.614639937877655 \n",
      "     Validation Step: 7 Validation Loss: 0.6101199984550476 \n",
      "     Validation Step: 8 Validation Loss: 0.6184911727905273 \n",
      "     Validation Step: 9 Validation Loss: 0.6118866205215454 \n",
      "     Validation Step: 10 Validation Loss: 0.614837110042572 \n",
      "     Validation Step: 11 Validation Loss: 0.6111710667610168 \n",
      "     Validation Step: 12 Validation Loss: 0.6183406114578247 \n",
      "     Validation Step: 13 Validation Loss: 0.6129900217056274 \n",
      "     Validation Step: 14 Validation Loss: 0.6145697832107544 \n",
      "     Validation Step: 15 Validation Loss: 0.614203155040741 \n",
      "     Validation Step: 16 Validation Loss: 0.618060827255249 \n",
      "     Validation Step: 17 Validation Loss: 0.6175941824913025 \n",
      "     Validation Step: 18 Validation Loss: 0.6116461157798767 \n",
      "     Validation Step: 19 Validation Loss: 0.6105222105979919 \n",
      "     Validation Step: 20 Validation Loss: 0.6141312718391418 \n",
      "     Validation Step: 21 Validation Loss: 0.6145339012145996 \n",
      "     Validation Step: 22 Validation Loss: 0.6148928999900818 \n",
      "     Validation Step: 23 Validation Loss: 0.6159970164299011 \n",
      "     Validation Step: 24 Validation Loss: 0.6101675033569336 \n",
      "     Validation Step: 25 Validation Loss: 0.6170187592506409 \n",
      "     Validation Step: 26 Validation Loss: 0.6162452697753906 \n",
      "     Validation Step: 27 Validation Loss: 0.6157960295677185 \n",
      "     Validation Step: 28 Validation Loss: 0.6155737638473511 \n",
      "     Validation Step: 29 Validation Loss: 0.6136363744735718 \n",
      "     Validation Step: 30 Validation Loss: 0.6141154766082764 \n",
      "     Validation Step: 31 Validation Loss: 0.6153185367584229 \n",
      "     Validation Step: 32 Validation Loss: 0.6111829280853271 \n",
      "     Validation Step: 33 Validation Loss: 0.618468165397644 \n",
      "     Validation Step: 34 Validation Loss: 0.6136547923088074 \n",
      "     Validation Step: 35 Validation Loss: 0.607536792755127 \n",
      "     Validation Step: 36 Validation Loss: 0.6156154870986938 \n",
      "     Validation Step: 37 Validation Loss: 0.6176974773406982 \n",
      "     Validation Step: 38 Validation Loss: 0.6115621328353882 \n",
      "     Validation Step: 39 Validation Loss: 0.6133062243461609 \n",
      "     Validation Step: 40 Validation Loss: 0.6173030734062195 \n",
      "     Validation Step: 41 Validation Loss: 0.6104883551597595 \n",
      "     Validation Step: 42 Validation Loss: 0.6128321886062622 \n",
      "     Validation Step: 43 Validation Loss: 0.6136703491210938 \n",
      "     Validation Step: 44 Validation Loss: 0.6106393337249756 \n",
      "Epoch: 99\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6117897629737854 \n",
      "     Training Step: 1 Training Loss: 0.6134207248687744 \n",
      "     Training Step: 2 Training Loss: 0.6141561269760132 \n",
      "     Training Step: 3 Training Loss: 0.612774133682251 \n",
      "     Training Step: 4 Training Loss: 0.614346981048584 \n",
      "     Training Step: 5 Training Loss: 0.6144979000091553 \n",
      "     Training Step: 6 Training Loss: 0.6140749454498291 \n",
      "     Training Step: 7 Training Loss: 0.6177515983581543 \n",
      "     Training Step: 8 Training Loss: 0.6125392913818359 \n",
      "     Training Step: 9 Training Loss: 0.6194362044334412 \n",
      "     Training Step: 10 Training Loss: 0.6114087104797363 \n",
      "     Training Step: 11 Training Loss: 0.6116943359375 \n",
      "     Training Step: 12 Training Loss: 0.6112099885940552 \n",
      "     Training Step: 13 Training Loss: 0.6097366809844971 \n",
      "     Training Step: 14 Training Loss: 0.6156832575798035 \n",
      "     Training Step: 15 Training Loss: 0.616807758808136 \n",
      "     Training Step: 16 Training Loss: 0.6152856349945068 \n",
      "     Training Step: 17 Training Loss: 0.6144824028015137 \n",
      "     Training Step: 18 Training Loss: 0.6115537881851196 \n",
      "     Training Step: 19 Training Loss: 0.6127971410751343 \n",
      "     Training Step: 20 Training Loss: 0.6082533001899719 \n",
      "     Training Step: 21 Training Loss: 0.6144413948059082 \n",
      "     Training Step: 22 Training Loss: 0.6162630915641785 \n",
      "     Training Step: 23 Training Loss: 0.6107059717178345 \n",
      "     Training Step: 24 Training Loss: 0.6100237965583801 \n",
      "     Training Step: 25 Training Loss: 0.6155509948730469 \n",
      "     Training Step: 26 Training Loss: 0.6155634522438049 \n",
      "     Training Step: 27 Training Loss: 0.6140202879905701 \n",
      "     Training Step: 28 Training Loss: 0.6142470836639404 \n",
      "     Training Step: 29 Training Loss: 0.6143438816070557 \n",
      "     Training Step: 30 Training Loss: 0.6132044196128845 \n",
      "     Training Step: 31 Training Loss: 0.6157611608505249 \n",
      "     Training Step: 32 Training Loss: 0.6097082495689392 \n",
      "     Training Step: 33 Training Loss: 0.6135122179985046 \n",
      "     Training Step: 34 Training Loss: 0.6171655654907227 \n",
      "     Training Step: 35 Training Loss: 0.6167183518409729 \n",
      "     Training Step: 36 Training Loss: 0.6157817244529724 \n",
      "     Training Step: 37 Training Loss: 0.610582172870636 \n",
      "     Training Step: 38 Training Loss: 0.6146807074546814 \n",
      "     Training Step: 39 Training Loss: 0.6123847365379333 \n",
      "     Training Step: 40 Training Loss: 0.613460898399353 \n",
      "     Training Step: 41 Training Loss: 0.6183205842971802 \n",
      "     Training Step: 42 Training Loss: 0.6122371554374695 \n",
      "     Training Step: 43 Training Loss: 0.6188662052154541 \n",
      "     Training Step: 44 Training Loss: 0.6177012920379639 \n",
      "     Training Step: 45 Training Loss: 0.6094790101051331 \n",
      "     Training Step: 46 Training Loss: 0.6167001128196716 \n",
      "     Training Step: 47 Training Loss: 0.6125302314758301 \n",
      "     Training Step: 48 Training Loss: 0.6136307716369629 \n",
      "     Training Step: 49 Training Loss: 0.6130660772323608 \n",
      "     Training Step: 50 Training Loss: 0.616607666015625 \n",
      "     Training Step: 51 Training Loss: 0.6171441078186035 \n",
      "     Training Step: 52 Training Loss: 0.6116475462913513 \n",
      "     Training Step: 53 Training Loss: 0.614732027053833 \n",
      "     Training Step: 54 Training Loss: 0.6120703220367432 \n",
      "     Training Step: 55 Training Loss: 0.6182186007499695 \n",
      "     Training Step: 56 Training Loss: 0.615291178226471 \n",
      "     Training Step: 57 Training Loss: 0.6134690046310425 \n",
      "     Training Step: 58 Training Loss: 0.6164132356643677 \n",
      "     Training Step: 59 Training Loss: 0.617799699306488 \n",
      "     Training Step: 60 Training Loss: 0.6118113398551941 \n",
      "     Training Step: 61 Training Loss: 0.6149569153785706 \n",
      "     Training Step: 62 Training Loss: 0.6185608506202698 \n",
      "     Training Step: 63 Training Loss: 0.615827739238739 \n",
      "     Training Step: 64 Training Loss: 0.6131609678268433 \n",
      "     Training Step: 65 Training Loss: 0.62090003490448 \n",
      "     Training Step: 66 Training Loss: 0.6146925687789917 \n",
      "     Training Step: 67 Training Loss: 0.6135113835334778 \n",
      "     Training Step: 68 Training Loss: 0.6154004335403442 \n",
      "     Training Step: 69 Training Loss: 0.6101186275482178 \n",
      "     Training Step: 70 Training Loss: 0.6157418489456177 \n",
      "     Training Step: 71 Training Loss: 0.6182234287261963 \n",
      "     Training Step: 72 Training Loss: 0.6176809072494507 \n",
      "     Training Step: 73 Training Loss: 0.6143700480461121 \n",
      "     Training Step: 74 Training Loss: 0.6183942556381226 \n",
      "     Training Step: 75 Training Loss: 0.6147927045822144 \n",
      "     Training Step: 76 Training Loss: 0.6185879111289978 \n",
      "     Training Step: 77 Training Loss: 0.6124729514122009 \n",
      "     Training Step: 78 Training Loss: 0.6156728267669678 \n",
      "     Training Step: 79 Training Loss: 0.6123010516166687 \n",
      "     Training Step: 80 Training Loss: 0.6126534938812256 \n",
      "     Training Step: 81 Training Loss: 0.6114563345909119 \n",
      "     Training Step: 82 Training Loss: 0.6132878661155701 \n",
      "     Training Step: 83 Training Loss: 0.6161973476409912 \n",
      "     Training Step: 84 Training Loss: 0.6146019697189331 \n",
      "     Training Step: 85 Training Loss: 0.6121406555175781 \n",
      "     Training Step: 86 Training Loss: 0.6144217848777771 \n",
      "     Training Step: 87 Training Loss: 0.611406683921814 \n",
      "     Training Step: 88 Training Loss: 0.6118242740631104 \n",
      "     Training Step: 89 Training Loss: 0.6114122867584229 \n",
      "     Training Step: 90 Training Loss: 0.6177154183387756 \n",
      "     Training Step: 91 Training Loss: 0.617115318775177 \n",
      "     Training Step: 92 Training Loss: 0.6115214824676514 \n",
      "     Training Step: 93 Training Loss: 0.6169072985649109 \n",
      "     Training Step: 94 Training Loss: 0.6147502660751343 \n",
      "     Training Step: 95 Training Loss: 0.6093927025794983 \n",
      "     Training Step: 96 Training Loss: 0.6107350587844849 \n",
      "     Training Step: 97 Training Loss: 0.6139198541641235 \n",
      "     Training Step: 98 Training Loss: 0.6188643574714661 \n",
      "     Training Step: 99 Training Loss: 0.6171478033065796 \n",
      "     Training Step: 100 Training Loss: 0.6101800799369812 \n",
      "     Training Step: 101 Training Loss: 0.6162110567092896 \n",
      "     Training Step: 102 Training Loss: 0.6137428283691406 \n",
      "     Training Step: 103 Training Loss: 0.6171920299530029 \n",
      "     Training Step: 104 Training Loss: 0.615172803401947 \n",
      "     Training Step: 105 Training Loss: 0.6128445267677307 \n",
      "     Training Step: 106 Training Loss: 0.611889660358429 \n",
      "     Training Step: 107 Training Loss: 0.6106739044189453 \n",
      "     Training Step: 108 Training Loss: 0.6133561134338379 \n",
      "     Training Step: 109 Training Loss: 0.6137630939483643 \n",
      "     Training Step: 110 Training Loss: 0.6136354207992554 \n",
      "     Training Step: 111 Training Loss: 0.6123748421669006 \n",
      "     Training Step: 112 Training Loss: 0.613292396068573 \n",
      "     Training Step: 113 Training Loss: 0.6167186498641968 \n",
      "     Training Step: 114 Training Loss: 0.6131150722503662 \n",
      "     Training Step: 115 Training Loss: 0.6151044964790344 \n",
      "     Training Step: 116 Training Loss: 0.6127020120620728 \n",
      "     Training Step: 117 Training Loss: 0.6166854500770569 \n",
      "     Training Step: 118 Training Loss: 0.6160101890563965 \n",
      "     Training Step: 119 Training Loss: 0.6163557767868042 \n",
      "     Training Step: 120 Training Loss: 0.6131846308708191 \n",
      "     Training Step: 121 Training Loss: 0.6150866746902466 \n",
      "     Training Step: 122 Training Loss: 0.6180321574211121 \n",
      "     Training Step: 123 Training Loss: 0.6146998405456543 \n",
      "     Training Step: 124 Training Loss: 0.6167111396789551 \n",
      "     Training Step: 125 Training Loss: 0.6107165813446045 \n",
      "     Training Step: 126 Training Loss: 0.6201941967010498 \n",
      "     Training Step: 127 Training Loss: 0.6109166145324707 \n",
      "     Training Step: 128 Training Loss: 0.6141203045845032 \n",
      "     Training Step: 129 Training Loss: 0.6184437870979309 \n",
      "     Training Step: 130 Training Loss: 0.6133118271827698 \n",
      "     Training Step: 131 Training Loss: 0.6128842830657959 \n",
      "     Training Step: 132 Training Loss: 0.6146072149276733 \n",
      "     Training Step: 133 Training Loss: 0.6153342127799988 \n",
      "     Training Step: 134 Training Loss: 0.6139175295829773 \n",
      "     Training Step: 135 Training Loss: 0.6152409315109253 \n",
      "     Training Step: 136 Training Loss: 0.6146055459976196 \n",
      "     Training Step: 137 Training Loss: 0.6167603731155396 \n",
      "     Training Step: 138 Training Loss: 0.6099948883056641 \n",
      "     Training Step: 139 Training Loss: 0.6141512393951416 \n",
      "     Training Step: 140 Training Loss: 0.6197153925895691 \n",
      "     Training Step: 141 Training Loss: 0.6105946898460388 \n",
      "     Training Step: 142 Training Loss: 0.6118337512016296 \n",
      "     Training Step: 143 Training Loss: 0.6153876185417175 \n",
      "     Training Step: 144 Training Loss: 0.6164112091064453 \n",
      "     Training Step: 145 Training Loss: 0.6153717041015625 \n",
      "     Training Step: 146 Training Loss: 0.6140204071998596 \n",
      "     Training Step: 147 Training Loss: 0.615516722202301 \n",
      "     Training Step: 148 Training Loss: 0.6198770403862 \n",
      "     Training Step: 149 Training Loss: 0.619610071182251 \n",
      "     Training Step: 150 Training Loss: 0.6146426200866699 \n",
      "     Training Step: 151 Training Loss: 0.6121950745582581 \n",
      "     Training Step: 152 Training Loss: 0.6121686697006226 \n",
      "     Training Step: 153 Training Loss: 0.6111949682235718 \n",
      "     Training Step: 154 Training Loss: 0.6127699613571167 \n",
      "     Training Step: 155 Training Loss: 0.6157436966896057 \n",
      "     Training Step: 156 Training Loss: 0.6166438460350037 \n",
      "     Training Step: 157 Training Loss: 0.6123145818710327 \n",
      "     Training Step: 158 Training Loss: 0.6125032901763916 \n",
      "     Training Step: 159 Training Loss: 0.611152172088623 \n",
      "     Training Step: 160 Training Loss: 0.6115918755531311 \n",
      "     Training Step: 161 Training Loss: 0.6146664023399353 \n",
      "     Training Step: 162 Training Loss: 0.6149346232414246 \n",
      "     Training Step: 163 Training Loss: 0.6116012334823608 \n",
      "     Training Step: 164 Training Loss: 0.6101201176643372 \n",
      "     Training Step: 165 Training Loss: 0.6120128035545349 \n",
      "     Training Step: 166 Training Loss: 0.6181305050849915 \n",
      "     Training Step: 167 Training Loss: 0.6092004776000977 \n",
      "     Training Step: 168 Training Loss: 0.6161490082740784 \n",
      "     Training Step: 169 Training Loss: 0.6154721975326538 \n",
      "     Training Step: 170 Training Loss: 0.6146750450134277 \n",
      "     Training Step: 171 Training Loss: 0.6176896691322327 \n",
      "     Training Step: 172 Training Loss: 0.6180548667907715 \n",
      "     Training Step: 173 Training Loss: 0.6160322427749634 \n",
      "     Training Step: 174 Training Loss: 0.6130648851394653 \n",
      "     Training Step: 175 Training Loss: 0.6115463972091675 \n",
      "     Training Step: 176 Training Loss: 0.6121432781219482 \n",
      "     Training Step: 177 Training Loss: 0.6152956485748291 \n",
      "     Training Step: 178 Training Loss: 0.6129879355430603 \n",
      "     Training Step: 179 Training Loss: 0.6148741841316223 \n",
      "     Training Step: 180 Training Loss: 0.6125507354736328 \n",
      "     Training Step: 181 Training Loss: 0.6154752969741821 \n",
      "     Training Step: 182 Training Loss: 0.6146706938743591 \n",
      "     Training Step: 183 Training Loss: 0.614043116569519 \n",
      "     Training Step: 184 Training Loss: 0.6116262078285217 \n",
      "     Training Step: 185 Training Loss: 0.6132076978683472 \n",
      "     Training Step: 186 Training Loss: 0.612863302230835 \n",
      "     Training Step: 187 Training Loss: 0.6138860583305359 \n",
      "     Training Step: 188 Training Loss: 0.6173960566520691 \n",
      "     Training Step: 189 Training Loss: 0.6100547313690186 \n",
      "     Training Step: 190 Training Loss: 0.6122259497642517 \n",
      "     Training Step: 191 Training Loss: 0.6149270534515381 \n",
      "     Training Step: 192 Training Loss: 0.6103614568710327 \n",
      "     Training Step: 193 Training Loss: 0.6168263554573059 \n",
      "     Training Step: 194 Training Loss: 0.6108969449996948 \n",
      "     Training Step: 195 Training Loss: 0.6147775650024414 \n",
      "     Training Step: 196 Training Loss: 0.6175129413604736 \n",
      "     Training Step: 197 Training Loss: 0.6133067011833191 \n",
      "     Training Step: 198 Training Loss: 0.6132718920707703 \n",
      "     Training Step: 199 Training Loss: 0.6114282608032227 \n",
      "     Training Step: 200 Training Loss: 0.6138166189193726 \n",
      "     Training Step: 201 Training Loss: 0.6142827868461609 \n",
      "     Training Step: 202 Training Loss: 0.6105809807777405 \n",
      "     Training Step: 203 Training Loss: 0.6159566640853882 \n",
      "     Training Step: 204 Training Loss: 0.6153849363327026 \n",
      "     Training Step: 205 Training Loss: 0.6150751113891602 \n",
      "     Training Step: 206 Training Loss: 0.6115217804908752 \n",
      "     Training Step: 207 Training Loss: 0.6129282116889954 \n",
      "     Training Step: 208 Training Loss: 0.6178132891654968 \n",
      "     Training Step: 209 Training Loss: 0.6103989481925964 \n",
      "     Training Step: 210 Training Loss: 0.6104785203933716 \n",
      "     Training Step: 211 Training Loss: 0.6169438362121582 \n",
      "     Training Step: 212 Training Loss: 0.6151536107063293 \n",
      "     Training Step: 213 Training Loss: 0.6143440008163452 \n",
      "     Training Step: 214 Training Loss: 0.6152709126472473 \n",
      "     Training Step: 215 Training Loss: 0.6122355461120605 \n",
      "     Training Step: 216 Training Loss: 0.614207923412323 \n",
      "     Training Step: 217 Training Loss: 0.6136479377746582 \n",
      "     Training Step: 218 Training Loss: 0.6114749312400818 \n",
      "     Training Step: 219 Training Loss: 0.6168131232261658 \n",
      "     Training Step: 220 Training Loss: 0.6181139945983887 \n",
      "     Training Step: 221 Training Loss: 0.611636757850647 \n",
      "     Training Step: 222 Training Loss: 0.6166489720344543 \n",
      "     Training Step: 223 Training Loss: 0.6135874390602112 \n",
      "     Training Step: 224 Training Loss: 0.6123824119567871 \n",
      "     Training Step: 225 Training Loss: 0.6155968904495239 \n",
      "     Training Step: 226 Training Loss: 0.6129301190376282 \n",
      "     Training Step: 227 Training Loss: 0.6097488403320312 \n",
      "     Training Step: 228 Training Loss: 0.6105083227157593 \n",
      "     Training Step: 229 Training Loss: 0.618445873260498 \n",
      "     Training Step: 230 Training Loss: 0.6118344664573669 \n",
      "     Training Step: 231 Training Loss: 0.6164970993995667 \n",
      "     Training Step: 232 Training Loss: 0.6154274940490723 \n",
      "     Training Step: 233 Training Loss: 0.614452600479126 \n",
      "     Training Step: 234 Training Loss: 0.6166799664497375 \n",
      "     Training Step: 235 Training Loss: 0.6117968559265137 \n",
      "     Training Step: 236 Training Loss: 0.6105731129646301 \n",
      "     Training Step: 237 Training Loss: 0.6137364506721497 \n",
      "     Training Step: 238 Training Loss: 0.6147045493125916 \n",
      "     Training Step: 239 Training Loss: 0.6142112016677856 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6111752986907959 \n",
      "     Validation Step: 1 Validation Loss: 0.6136502623558044 \n",
      "     Validation Step: 2 Validation Loss: 0.618500828742981 \n",
      "     Validation Step: 3 Validation Loss: 0.6159971356391907 \n",
      "     Validation Step: 4 Validation Loss: 0.6183469891548157 \n",
      "     Validation Step: 5 Validation Loss: 0.6156158447265625 \n",
      "     Validation Step: 6 Validation Loss: 0.6141999363899231 \n",
      "     Validation Step: 7 Validation Loss: 0.6157934069633484 \n",
      "     Validation Step: 8 Validation Loss: 0.6148340106010437 \n",
      "     Validation Step: 9 Validation Loss: 0.6133037805557251 \n",
      "     Validation Step: 10 Validation Loss: 0.6152285933494568 \n",
      "     Validation Step: 11 Validation Loss: 0.6148938536643982 \n",
      "     Validation Step: 12 Validation Loss: 0.6118764877319336 \n",
      "     Validation Step: 13 Validation Loss: 0.6182355880737305 \n",
      "     Validation Step: 14 Validation Loss: 0.6104785799980164 \n",
      "     Validation Step: 15 Validation Loss: 0.6105102300643921 \n",
      "     Validation Step: 16 Validation Loss: 0.6075173616409302 \n",
      "     Validation Step: 17 Validation Loss: 0.6136341094970703 \n",
      "     Validation Step: 18 Validation Loss: 0.6173095107078552 \n",
      "     Validation Step: 19 Validation Loss: 0.6176008582115173 \n",
      "     Validation Step: 20 Validation Loss: 0.6106287837028503 \n",
      "     Validation Step: 21 Validation Loss: 0.6128256320953369 \n",
      "     Validation Step: 22 Validation Loss: 0.6101350784301758 \n",
      "     Validation Step: 23 Validation Loss: 0.6121251583099365 \n",
      "     Validation Step: 24 Validation Loss: 0.6142540574073792 \n",
      "     Validation Step: 25 Validation Loss: 0.6141257286071777 \n",
      "     Validation Step: 26 Validation Loss: 0.6129807829856873 \n",
      "     Validation Step: 27 Validation Loss: 0.615318238735199 \n",
      "     Validation Step: 28 Validation Loss: 0.617700457572937 \n",
      "     Validation Step: 29 Validation Loss: 0.6170262694358826 \n",
      "     Validation Step: 30 Validation Loss: 0.6155765056610107 \n",
      "     Validation Step: 31 Validation Loss: 0.615039050579071 \n",
      "     Validation Step: 32 Validation Loss: 0.6162445545196533 \n",
      "     Validation Step: 33 Validation Loss: 0.6136637926101685 \n",
      "     Validation Step: 34 Validation Loss: 0.6180678606033325 \n",
      "     Validation Step: 35 Validation Loss: 0.6116389632225037 \n",
      "     Validation Step: 36 Validation Loss: 0.6101589798927307 \n",
      "     Validation Step: 37 Validation Loss: 0.6115518808364868 \n",
      "     Validation Step: 38 Validation Loss: 0.614534318447113 \n",
      "     Validation Step: 39 Validation Loss: 0.6141124963760376 \n",
      "     Validation Step: 40 Validation Loss: 0.6145719289779663 \n",
      "     Validation Step: 41 Validation Loss: 0.618476927280426 \n",
      "     Validation Step: 42 Validation Loss: 0.6146369576454163 \n",
      "     Validation Step: 43 Validation Loss: 0.6101052761077881 \n",
      "     Validation Step: 44 Validation Loss: 0.6111598014831543 \n",
      "Epoch: 100\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6113981604576111 \n",
      "     Training Step: 1 Training Loss: 0.6118305325508118 \n",
      "     Training Step: 2 Training Loss: 0.6122880578041077 \n",
      "     Training Step: 3 Training Loss: 0.6171571016311646 \n",
      "     Training Step: 4 Training Loss: 0.6167347431182861 \n",
      "     Training Step: 5 Training Loss: 0.6182425618171692 \n",
      "     Training Step: 6 Training Loss: 0.6146117448806763 \n",
      "     Training Step: 7 Training Loss: 0.613203227519989 \n",
      "     Training Step: 8 Training Loss: 0.6171623468399048 \n",
      "     Training Step: 9 Training Loss: 0.613640546798706 \n",
      "     Training Step: 10 Training Loss: 0.6114501357078552 \n",
      "     Training Step: 11 Training Loss: 0.6114305853843689 \n",
      "     Training Step: 12 Training Loss: 0.611569344997406 \n",
      "     Training Step: 13 Training Loss: 0.6125224828720093 \n",
      "     Training Step: 14 Training Loss: 0.6140214800834656 \n",
      "     Training Step: 15 Training Loss: 0.6117923855781555 \n",
      "     Training Step: 16 Training Loss: 0.6123723387718201 \n",
      "     Training Step: 17 Training Loss: 0.6167278289794922 \n",
      "     Training Step: 18 Training Loss: 0.6131847500801086 \n",
      "     Training Step: 19 Training Loss: 0.6167868971824646 \n",
      "     Training Step: 20 Training Loss: 0.6153407692909241 \n",
      "     Training Step: 21 Training Loss: 0.6140317916870117 \n",
      "     Training Step: 22 Training Loss: 0.6138193011283875 \n",
      "     Training Step: 23 Training Loss: 0.6108686923980713 \n",
      "     Training Step: 24 Training Loss: 0.6138917207717896 \n",
      "     Training Step: 25 Training Loss: 0.6153948903083801 \n",
      "     Training Step: 26 Training Loss: 0.6142421960830688 \n",
      "     Training Step: 27 Training Loss: 0.6172019839286804 \n",
      "     Training Step: 28 Training Loss: 0.6105701327323914 \n",
      "     Training Step: 29 Training Loss: 0.6118043065071106 \n",
      "     Training Step: 30 Training Loss: 0.6151513457298279 \n",
      "     Training Step: 31 Training Loss: 0.615754246711731 \n",
      "     Training Step: 32 Training Loss: 0.6135121583938599 \n",
      "     Training Step: 33 Training Loss: 0.6121543049812317 \n",
      "     Training Step: 34 Training Loss: 0.6147007346153259 \n",
      "     Training Step: 35 Training Loss: 0.6144782900810242 \n",
      "     Training Step: 36 Training Loss: 0.6143483519554138 \n",
      "     Training Step: 37 Training Loss: 0.6145975589752197 \n",
      "     Training Step: 38 Training Loss: 0.6104907989501953 \n",
      "     Training Step: 39 Training Loss: 0.6168904900550842 \n",
      "     Training Step: 40 Training Loss: 0.6101403832435608 \n",
      "     Training Step: 41 Training Loss: 0.6126404404640198 \n",
      "     Training Step: 42 Training Loss: 0.6181074976921082 \n",
      "     Training Step: 43 Training Loss: 0.6160346269607544 \n",
      "     Training Step: 44 Training Loss: 0.6142847537994385 \n",
      "     Training Step: 45 Training Loss: 0.6149272918701172 \n",
      "     Training Step: 46 Training Loss: 0.6148720383644104 \n",
      "     Training Step: 47 Training Loss: 0.6100656986236572 \n",
      "     Training Step: 48 Training Loss: 0.61018306016922 \n",
      "     Training Step: 49 Training Loss: 0.6135970950126648 \n",
      "     Training Step: 50 Training Loss: 0.6130715012550354 \n",
      "     Training Step: 51 Training Loss: 0.6147510409355164 \n",
      "     Training Step: 52 Training Loss: 0.6116395592689514 \n",
      "     Training Step: 53 Training Loss: 0.6121183633804321 \n",
      "     Training Step: 54 Training Loss: 0.616224467754364 \n",
      "     Training Step: 55 Training Loss: 0.619657039642334 \n",
      "     Training Step: 56 Training Loss: 0.6155282258987427 \n",
      "     Training Step: 57 Training Loss: 0.615415632724762 \n",
      "     Training Step: 58 Training Loss: 0.6147049069404602 \n",
      "     Training Step: 59 Training Loss: 0.6141560077667236 \n",
      "     Training Step: 60 Training Loss: 0.6143478751182556 \n",
      "     Training Step: 61 Training Loss: 0.6185820698738098 \n",
      "     Training Step: 62 Training Loss: 0.6201974153518677 \n",
      "     Training Step: 63 Training Loss: 0.6136493682861328 \n",
      "     Training Step: 64 Training Loss: 0.612824559211731 \n",
      "     Training Step: 65 Training Loss: 0.6162235140800476 \n",
      "     Training Step: 66 Training Loss: 0.6124887466430664 \n",
      "     Training Step: 67 Training Loss: 0.6180473566055298 \n",
      "     Training Step: 68 Training Loss: 0.6120537519454956 \n",
      "     Training Step: 69 Training Loss: 0.6155270934104919 \n",
      "     Training Step: 70 Training Loss: 0.6097643971443176 \n",
      "     Training Step: 71 Training Loss: 0.6134738922119141 \n",
      "     Training Step: 72 Training Loss: 0.6182335019111633 \n",
      "     Training Step: 73 Training Loss: 0.61058109998703 \n",
      "     Training Step: 74 Training Loss: 0.6144242286682129 \n",
      "     Training Step: 75 Training Loss: 0.6100422739982605 \n",
      "     Training Step: 76 Training Loss: 0.6100518703460693 \n",
      "     Training Step: 77 Training Loss: 0.6118335127830505 \n",
      "     Training Step: 78 Training Loss: 0.6153040528297424 \n",
      "     Training Step: 79 Training Loss: 0.6116877794265747 \n",
      "     Training Step: 80 Training Loss: 0.6210200190544128 \n",
      "     Training Step: 81 Training Loss: 0.6154686212539673 \n",
      "     Training Step: 82 Training Loss: 0.6132938861846924 \n",
      "     Training Step: 83 Training Loss: 0.6184462904930115 \n",
      "     Training Step: 84 Training Loss: 0.6115198135375977 \n",
      "     Training Step: 85 Training Loss: 0.6154770255088806 \n",
      "     Training Step: 86 Training Loss: 0.6152718663215637 \n",
      "     Training Step: 87 Training Loss: 0.6116361021995544 \n",
      "     Training Step: 88 Training Loss: 0.6092527508735657 \n",
      "     Training Step: 89 Training Loss: 0.6137658357620239 \n",
      "     Training Step: 90 Training Loss: 0.6147708296775818 \n",
      "     Training Step: 91 Training Loss: 0.614205539226532 \n",
      "     Training Step: 92 Training Loss: 0.6157844662666321 \n",
      "     Training Step: 93 Training Loss: 0.6161168813705444 \n",
      "     Training Step: 94 Training Loss: 0.6104143857955933 \n",
      "     Training Step: 95 Training Loss: 0.6180188655853271 \n",
      "     Training Step: 96 Training Loss: 0.612172544002533 \n",
      "     Training Step: 97 Training Loss: 0.6144513487815857 \n",
      "     Training Step: 98 Training Loss: 0.6176328063011169 \n",
      "     Training Step: 99 Training Loss: 0.6123861074447632 \n",
      "     Training Step: 100 Training Loss: 0.6150959134101868 \n",
      "     Training Step: 101 Training Loss: 0.6194403767585754 \n",
      "     Training Step: 102 Training Loss: 0.6156811714172363 \n",
      "     Training Step: 103 Training Loss: 0.6128444671630859 \n",
      "     Training Step: 104 Training Loss: 0.614334225654602 \n",
      "     Training Step: 105 Training Loss: 0.6170868873596191 \n",
      "     Training Step: 106 Training Loss: 0.6177959442138672 \n",
      "     Training Step: 107 Training Loss: 0.616667628288269 \n",
      "     Training Step: 108 Training Loss: 0.6166044473648071 \n",
      "     Training Step: 109 Training Loss: 0.6134800910949707 \n",
      "     Training Step: 110 Training Loss: 0.6188434958457947 \n",
      "     Training Step: 111 Training Loss: 0.6169041395187378 \n",
      "     Training Step: 112 Training Loss: 0.6114790439605713 \n",
      "     Training Step: 113 Training Loss: 0.6166384220123291 \n",
      "     Training Step: 114 Training Loss: 0.615827739238739 \n",
      "     Training Step: 115 Training Loss: 0.6155918836593628 \n",
      "     Training Step: 116 Training Loss: 0.6151787042617798 \n",
      "     Training Step: 117 Training Loss: 0.6106424331665039 \n",
      "     Training Step: 118 Training Loss: 0.6111688613891602 \n",
      "     Training Step: 119 Training Loss: 0.6159411072731018 \n",
      "     Training Step: 120 Training Loss: 0.6161983609199524 \n",
      "     Training Step: 121 Training Loss: 0.6103892922401428 \n",
      "     Training Step: 122 Training Loss: 0.6132908463478088 \n",
      "     Training Step: 123 Training Loss: 0.6157510876655579 \n",
      "     Training Step: 124 Training Loss: 0.619897723197937 \n",
      "     Training Step: 125 Training Loss: 0.6168082356452942 \n",
      "     Training Step: 126 Training Loss: 0.6185869574546814 \n",
      "     Training Step: 127 Training Loss: 0.615374743938446 \n",
      "     Training Step: 128 Training Loss: 0.6184364557266235 \n",
      "     Training Step: 129 Training Loss: 0.6150868535041809 \n",
      "     Training Step: 130 Training Loss: 0.6139336228370667 \n",
      "     Training Step: 131 Training Loss: 0.6127840280532837 \n",
      "     Training Step: 132 Training Loss: 0.61640864610672 \n",
      "     Training Step: 133 Training Loss: 0.6127294898033142 \n",
      "     Training Step: 134 Training Loss: 0.611857533454895 \n",
      "     Training Step: 135 Training Loss: 0.6154205799102783 \n",
      "     Training Step: 136 Training Loss: 0.6146917939186096 \n",
      "     Training Step: 137 Training Loss: 0.6140197515487671 \n",
      "     Training Step: 138 Training Loss: 0.6129264235496521 \n",
      "     Training Step: 139 Training Loss: 0.6153976321220398 \n",
      "     Training Step: 140 Training Loss: 0.6164102554321289 \n",
      "     Training Step: 141 Training Loss: 0.6146799325942993 \n",
      "     Training Step: 142 Training Loss: 0.611473798751831 \n",
      "     Training Step: 143 Training Loss: 0.6137434244155884 \n",
      "     Training Step: 144 Training Loss: 0.6175023317337036 \n",
      "     Training Step: 145 Training Loss: 0.6133524179458618 \n",
      "     Training Step: 146 Training Loss: 0.6111546754837036 \n",
      "     Training Step: 147 Training Loss: 0.6093828082084656 \n",
      "     Training Step: 148 Training Loss: 0.6130539774894714 \n",
      "     Training Step: 149 Training Loss: 0.6165151596069336 \n",
      "     Training Step: 150 Training Loss: 0.6121366024017334 \n",
      "     Training Step: 151 Training Loss: 0.6129615306854248 \n",
      "     Training Step: 152 Training Loss: 0.6166774034500122 \n",
      "     Training Step: 153 Training Loss: 0.6152452230453491 \n",
      "     Training Step: 154 Training Loss: 0.6141058802604675 \n",
      "     Training Step: 155 Training Loss: 0.6156923770904541 \n",
      "     Training Step: 156 Training Loss: 0.6107376217842102 \n",
      "     Training Step: 157 Training Loss: 0.6114138960838318 \n",
      "     Training Step: 158 Training Loss: 0.6115264892578125 \n",
      "     Training Step: 159 Training Loss: 0.6122278571128845 \n",
      "     Training Step: 160 Training Loss: 0.6123785972595215 \n",
      "     Training Step: 161 Training Loss: 0.6099883913993835 \n",
      "     Training Step: 162 Training Loss: 0.616724967956543 \n",
      "     Training Step: 163 Training Loss: 0.6094484329223633 \n",
      "     Training Step: 164 Training Loss: 0.6188694834709167 \n",
      "     Training Step: 165 Training Loss: 0.6142091751098633 \n",
      "     Training Step: 166 Training Loss: 0.6177331805229187 \n",
      "     Training Step: 167 Training Loss: 0.6132736802101135 \n",
      "     Training Step: 168 Training Loss: 0.6106811761856079 \n",
      "     Training Step: 169 Training Loss: 0.6160068511962891 \n",
      "     Training Step: 170 Training Loss: 0.6125295162200928 \n",
      "     Training Step: 171 Training Loss: 0.6176999807357788 \n",
      "     Training Step: 172 Training Loss: 0.6143770813941956 \n",
      "     Training Step: 173 Training Loss: 0.61472487449646 \n",
      "     Training Step: 174 Training Loss: 0.6147918105125427 \n",
      "     Training Step: 175 Training Loss: 0.6173799633979797 \n",
      "     Training Step: 176 Training Loss: 0.6131435632705688 \n",
      "     Training Step: 177 Training Loss: 0.6171312928199768 \n",
      "     Training Step: 178 Training Loss: 0.6140784025192261 \n",
      "     Training Step: 179 Training Loss: 0.6144992113113403 \n",
      "     Training Step: 180 Training Loss: 0.6127610206604004 \n",
      "     Training Step: 181 Training Loss: 0.6116116642951965 \n",
      "     Training Step: 182 Training Loss: 0.613505482673645 \n",
      "     Training Step: 183 Training Loss: 0.6177830696105957 \n",
      "     Training Step: 184 Training Loss: 0.61968594789505 \n",
      "     Training Step: 185 Training Loss: 0.6097525954246521 \n",
      "     Training Step: 186 Training Loss: 0.6107388138771057 \n",
      "     Training Step: 187 Training Loss: 0.6106695532798767 \n",
      "     Training Step: 188 Training Loss: 0.6146654486656189 \n",
      "     Training Step: 189 Training Loss: 0.6150701642036438 \n",
      "     Training Step: 190 Training Loss: 0.616815984249115 \n",
      "     Training Step: 191 Training Loss: 0.613414466381073 \n",
      "     Training Step: 192 Training Loss: 0.6104878783226013 \n",
      "     Training Step: 193 Training Loss: 0.6129165291786194 \n",
      "     Training Step: 194 Training Loss: 0.6146671175956726 \n",
      "     Training Step: 195 Training Loss: 0.6157517433166504 \n",
      "     Training Step: 196 Training Loss: 0.6177850961685181 \n",
      "     Training Step: 197 Training Loss: 0.6128667593002319 \n",
      "     Training Step: 198 Training Loss: 0.6149601936340332 \n",
      "     Training Step: 199 Training Loss: 0.6166978478431702 \n",
      "     Training Step: 200 Training Loss: 0.6176920533180237 \n",
      "     Training Step: 201 Training Loss: 0.6146348118782043 \n",
      "     Training Step: 202 Training Loss: 0.6146568655967712 \n",
      "     Training Step: 203 Training Loss: 0.6180235147476196 \n",
      "     Training Step: 204 Training Loss: 0.616799533367157 \n",
      "     Training Step: 205 Training Loss: 0.6125595569610596 \n",
      "     Training Step: 206 Training Loss: 0.6155252456665039 \n",
      "     Training Step: 207 Training Loss: 0.6182887554168701 \n",
      "     Training Step: 208 Training Loss: 0.616706907749176 \n",
      "     Training Step: 209 Training Loss: 0.6144649982452393 \n",
      "     Training Step: 210 Training Loss: 0.6121086478233337 \n",
      "     Training Step: 211 Training Loss: 0.6137577295303345 \n",
      "     Training Step: 212 Training Loss: 0.6152995228767395 \n",
      "     Training Step: 213 Training Loss: 0.6106330156326294 \n",
      "     Training Step: 214 Training Loss: 0.6139331459999084 \n",
      "     Training Step: 215 Training Loss: 0.6115382313728333 \n",
      "     Training Step: 216 Training Loss: 0.6122238039970398 \n",
      "     Training Step: 217 Training Loss: 0.61311936378479 \n",
      "     Training Step: 218 Training Loss: 0.61249178647995 \n",
      "     Training Step: 219 Training Loss: 0.6133225560188293 \n",
      "     Training Step: 220 Training Loss: 0.6108958721160889 \n",
      "     Training Step: 221 Training Loss: 0.6153478622436523 \n",
      "     Training Step: 222 Training Loss: 0.616397500038147 \n",
      "     Training Step: 223 Training Loss: 0.6133190393447876 \n",
      "     Training Step: 224 Training Loss: 0.6149401664733887 \n",
      "     Training Step: 225 Training Loss: 0.6116195917129517 \n",
      "     Training Step: 226 Training Loss: 0.60968416929245 \n",
      "     Training Step: 227 Training Loss: 0.6184970736503601 \n",
      "     Training Step: 228 Training Loss: 0.6117854118347168 \n",
      "     Training Step: 229 Training Loss: 0.6141527891159058 \n",
      "     Training Step: 230 Training Loss: 0.6118775010108948 \n",
      "     Training Step: 231 Training Loss: 0.6136487722396851 \n",
      "     Training Step: 232 Training Loss: 0.6146126985549927 \n",
      "     Training Step: 233 Training Loss: 0.6116095781326294 \n",
      "     Training Step: 234 Training Loss: 0.613203763961792 \n",
      "     Training Step: 235 Training Loss: 0.6122839450836182 \n",
      "     Training Step: 236 Training Loss: 0.6082644462585449 \n",
      "     Training Step: 237 Training Loss: 0.6111969947814941 \n",
      "     Training Step: 238 Training Loss: 0.6128652691841125 \n",
      "     Training Step: 239 Training Loss: 0.612228274345398 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6121156215667725 \n",
      "     Validation Step: 1 Validation Loss: 0.613667368888855 \n",
      "     Validation Step: 2 Validation Loss: 0.6074771881103516 \n",
      "     Validation Step: 3 Validation Loss: 0.6142042875289917 \n",
      "     Validation Step: 4 Validation Loss: 0.6101104617118835 \n",
      "     Validation Step: 5 Validation Loss: 0.6150537133216858 \n",
      "     Validation Step: 6 Validation Loss: 0.6185363531112671 \n",
      "     Validation Step: 7 Validation Loss: 0.6155978441238403 \n",
      "     Validation Step: 8 Validation Loss: 0.6129775047302246 \n",
      "     Validation Step: 9 Validation Loss: 0.6156316995620728 \n",
      "     Validation Step: 10 Validation Loss: 0.6145391464233398 \n",
      "     Validation Step: 11 Validation Loss: 0.6177289485931396 \n",
      "     Validation Step: 12 Validation Loss: 0.6160233020782471 \n",
      "     Validation Step: 13 Validation Loss: 0.6101408004760742 \n",
      "     Validation Step: 14 Validation Loss: 0.6104649901390076 \n",
      "     Validation Step: 15 Validation Loss: 0.6146419048309326 \n",
      "     Validation Step: 16 Validation Loss: 0.611628532409668 \n",
      "     Validation Step: 17 Validation Loss: 0.618269681930542 \n",
      "     Validation Step: 18 Validation Loss: 0.6142577528953552 \n",
      "     Validation Step: 19 Validation Loss: 0.6162631511688232 \n",
      "     Validation Step: 20 Validation Loss: 0.6149035692214966 \n",
      "     Validation Step: 21 Validation Loss: 0.6133012175559998 \n",
      "     Validation Step: 22 Validation Loss: 0.6128180623054504 \n",
      "     Validation Step: 23 Validation Loss: 0.61363285779953 \n",
      "     Validation Step: 24 Validation Loss: 0.6141154170036316 \n",
      "     Validation Step: 25 Validation Loss: 0.6180988550186157 \n",
      "     Validation Step: 26 Validation Loss: 0.6170572638511658 \n",
      "     Validation Step: 27 Validation Loss: 0.6111481189727783 \n",
      "     Validation Step: 28 Validation Loss: 0.6152418851852417 \n",
      "     Validation Step: 29 Validation Loss: 0.6148412227630615 \n",
      "     Validation Step: 30 Validation Loss: 0.6118651032447815 \n",
      "     Validation Step: 31 Validation Loss: 0.610081136226654 \n",
      "     Validation Step: 32 Validation Loss: 0.6115407943725586 \n",
      "     Validation Step: 33 Validation Loss: 0.6183791160583496 \n",
      "     Validation Step: 34 Validation Loss: 0.6104933023452759 \n",
      "     Validation Step: 35 Validation Loss: 0.6176241636276245 \n",
      "     Validation Step: 36 Validation Loss: 0.6136618256568909 \n",
      "     Validation Step: 37 Validation Loss: 0.6185085773468018 \n",
      "     Validation Step: 38 Validation Loss: 0.6145833730697632 \n",
      "     Validation Step: 39 Validation Loss: 0.6141272783279419 \n",
      "     Validation Step: 40 Validation Loss: 0.6111669540405273 \n",
      "     Validation Step: 41 Validation Loss: 0.6173374056816101 \n",
      "     Validation Step: 42 Validation Loss: 0.6153331398963928 \n",
      "     Validation Step: 43 Validation Loss: 0.6158109307289124 \n",
      "     Validation Step: 44 Validation Loss: 0.6106137037277222 \n",
      "Epoch: 101\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6154049038887024 \n",
      "     Training Step: 1 Training Loss: 0.6134109497070312 \n",
      "     Training Step: 2 Training Loss: 0.6135036945343018 \n",
      "     Training Step: 3 Training Loss: 0.6149338483810425 \n",
      "     Training Step: 4 Training Loss: 0.6167508959770203 \n",
      "     Training Step: 5 Training Loss: 0.6118717789649963 \n",
      "     Training Step: 6 Training Loss: 0.6130759716033936 \n",
      "     Training Step: 7 Training Loss: 0.6149609088897705 \n",
      "     Training Step: 8 Training Loss: 0.6154792904853821 \n",
      "     Training Step: 9 Training Loss: 0.6128031611442566 \n",
      "     Training Step: 10 Training Loss: 0.613117516040802 \n",
      "     Training Step: 11 Training Loss: 0.6157549619674683 \n",
      "     Training Step: 12 Training Loss: 0.6146634817123413 \n",
      "     Training Step: 13 Training Loss: 0.6155356168746948 \n",
      "     Training Step: 14 Training Loss: 0.6164138913154602 \n",
      "     Training Step: 15 Training Loss: 0.6163507699966431 \n",
      "     Training Step: 16 Training Loss: 0.610585629940033 \n",
      "     Training Step: 17 Training Loss: 0.6155164241790771 \n",
      "     Training Step: 18 Training Loss: 0.6162002086639404 \n",
      "     Training Step: 19 Training Loss: 0.616662859916687 \n",
      "     Training Step: 20 Training Loss: 0.6146752238273621 \n",
      "     Training Step: 21 Training Loss: 0.6105925440788269 \n",
      "     Training Step: 22 Training Loss: 0.6148682236671448 \n",
      "     Training Step: 23 Training Loss: 0.6171854734420776 \n",
      "     Training Step: 24 Training Loss: 0.6092612743377686 \n",
      "     Training Step: 25 Training Loss: 0.6118444204330444 \n",
      "     Training Step: 26 Training Loss: 0.6128426790237427 \n",
      "     Training Step: 27 Training Loss: 0.6157491207122803 \n",
      "     Training Step: 28 Training Loss: 0.6114111542701721 \n",
      "     Training Step: 29 Training Loss: 0.6166194677352905 \n",
      "     Training Step: 30 Training Loss: 0.6152656674385071 \n",
      "     Training Step: 31 Training Loss: 0.615250289440155 \n",
      "     Training Step: 32 Training Loss: 0.6186103820800781 \n",
      "     Training Step: 33 Training Loss: 0.6097321510314941 \n",
      "     Training Step: 34 Training Loss: 0.6125204563140869 \n",
      "     Training Step: 35 Training Loss: 0.6153393387794495 \n",
      "     Training Step: 36 Training Loss: 0.6109038591384888 \n",
      "     Training Step: 37 Training Loss: 0.6146094799041748 \n",
      "     Training Step: 38 Training Loss: 0.612125813961029 \n",
      "     Training Step: 39 Training Loss: 0.6194524168968201 \n",
      "     Training Step: 40 Training Loss: 0.6134781241416931 \n",
      "     Training Step: 41 Training Loss: 0.6147056818008423 \n",
      "     Training Step: 42 Training Loss: 0.6146105527877808 \n",
      "     Training Step: 43 Training Loss: 0.6147461533546448 \n",
      "     Training Step: 44 Training Loss: 0.6101454496383667 \n",
      "     Training Step: 45 Training Loss: 0.6122376322746277 \n",
      "     Training Step: 46 Training Loss: 0.6125381588935852 \n",
      "     Training Step: 47 Training Loss: 0.612068772315979 \n",
      "     Training Step: 48 Training Loss: 0.6159525513648987 \n",
      "     Training Step: 49 Training Loss: 0.6178242564201355 \n",
      "     Training Step: 50 Training Loss: 0.6133158206939697 \n",
      "     Training Step: 51 Training Loss: 0.6122389435768127 \n",
      "     Training Step: 52 Training Loss: 0.6121357679367065 \n",
      "     Training Step: 53 Training Loss: 0.6104949116706848 \n",
      "     Training Step: 54 Training Loss: 0.6176563501358032 \n",
      "     Training Step: 55 Training Loss: 0.6180559396743774 \n",
      "     Training Step: 56 Training Loss: 0.6171472668647766 \n",
      "     Training Step: 57 Training Loss: 0.6124628782272339 \n",
      "     Training Step: 58 Training Loss: 0.614369809627533 \n",
      "     Training Step: 59 Training Loss: 0.6139324903488159 \n",
      "     Training Step: 60 Training Loss: 0.617756187915802 \n",
      "     Training Step: 61 Training Loss: 0.6135924458503723 \n",
      "     Training Step: 62 Training Loss: 0.6154409646987915 \n",
      "     Training Step: 63 Training Loss: 0.613365888595581 \n",
      "     Training Step: 64 Training Loss: 0.6106256246566772 \n",
      "     Training Step: 65 Training Loss: 0.6141192317008972 \n",
      "     Training Step: 66 Training Loss: 0.6123858690261841 \n",
      "     Training Step: 67 Training Loss: 0.6173756122589111 \n",
      "     Training Step: 68 Training Loss: 0.6101932525634766 \n",
      "     Training Step: 69 Training Loss: 0.6140773296356201 \n",
      "     Training Step: 70 Training Loss: 0.6143416166305542 \n",
      "     Training Step: 71 Training Loss: 0.6137649416923523 \n",
      "     Training Step: 72 Training Loss: 0.6099812984466553 \n",
      "     Training Step: 73 Training Loss: 0.6094433069229126 \n",
      "     Training Step: 74 Training Loss: 0.6156190633773804 \n",
      "     Training Step: 75 Training Loss: 0.6131370067596436 \n",
      "     Training Step: 76 Training Loss: 0.6138226389884949 \n",
      "     Training Step: 77 Training Loss: 0.6123763918876648 \n",
      "     Training Step: 78 Training Loss: 0.6114597320556641 \n",
      "     Training Step: 79 Training Loss: 0.618073046207428 \n",
      "     Training Step: 80 Training Loss: 0.6147010922431946 \n",
      "     Training Step: 81 Training Loss: 0.6166806817054749 \n",
      "     Training Step: 82 Training Loss: 0.6167360544204712 \n",
      "     Training Step: 83 Training Loss: 0.6177256107330322 \n",
      "     Training Step: 84 Training Loss: 0.6144212484359741 \n",
      "     Training Step: 85 Training Loss: 0.6118431687355042 \n",
      "     Training Step: 86 Training Loss: 0.6171539425849915 \n",
      "     Training Step: 87 Training Loss: 0.614162027835846 \n",
      "     Training Step: 88 Training Loss: 0.6127278804779053 \n",
      "     Training Step: 89 Training Loss: 0.6118192076683044 \n",
      "     Training Step: 90 Training Loss: 0.6141564249992371 \n",
      "     Training Step: 91 Training Loss: 0.6131994724273682 \n",
      "     Training Step: 92 Training Loss: 0.6153789162635803 \n",
      "     Training Step: 93 Training Loss: 0.6106782555580139 \n",
      "     Training Step: 94 Training Loss: 0.6116400361061096 \n",
      "     Training Step: 95 Training Loss: 0.6180773973464966 \n",
      "     Training Step: 96 Training Loss: 0.6132785081863403 \n",
      "     Training Step: 97 Training Loss: 0.6121524572372437 \n",
      "     Training Step: 98 Training Loss: 0.6132803559303284 \n",
      "     Training Step: 99 Training Loss: 0.6151115298271179 \n",
      "     Training Step: 100 Training Loss: 0.616222620010376 \n",
      "     Training Step: 101 Training Loss: 0.6168221235275269 \n",
      "     Training Step: 102 Training Loss: 0.6150901317596436 \n",
      "     Training Step: 103 Training Loss: 0.6183422803878784 \n",
      "     Training Step: 104 Training Loss: 0.6108767986297607 \n",
      "     Training Step: 105 Training Loss: 0.6202216148376465 \n",
      "     Training Step: 106 Training Loss: 0.6156754493713379 \n",
      "     Training Step: 107 Training Loss: 0.6116062998771667 \n",
      "     Training Step: 108 Training Loss: 0.6166678667068481 \n",
      "     Training Step: 109 Training Loss: 0.6174710392951965 \n",
      "     Training Step: 110 Training Loss: 0.614338219165802 \n",
      "     Training Step: 111 Training Loss: 0.6188112497329712 \n",
      "     Training Step: 112 Training Loss: 0.616112232208252 \n",
      "     Training Step: 113 Training Loss: 0.614788830280304 \n",
      "     Training Step: 114 Training Loss: 0.609792172908783 \n",
      "     Training Step: 115 Training Loss: 0.6123349070549011 \n",
      "     Training Step: 116 Training Loss: 0.615416407585144 \n",
      "     Training Step: 117 Training Loss: 0.6107524633407593 \n",
      "     Training Step: 118 Training Loss: 0.6121759414672852 \n",
      "     Training Step: 119 Training Loss: 0.6122287511825562 \n",
      "     Training Step: 120 Training Loss: 0.6146131157875061 \n",
      "     Training Step: 121 Training Loss: 0.6136220693588257 \n",
      "     Training Step: 122 Training Loss: 0.6182684898376465 \n",
      "     Training Step: 123 Training Loss: 0.6115167737007141 \n",
      "     Training Step: 124 Training Loss: 0.6146793365478516 \n",
      "     Training Step: 125 Training Loss: 0.611639678478241 \n",
      "     Training Step: 126 Training Loss: 0.6114187240600586 \n",
      "     Training Step: 127 Training Loss: 0.6139028668403625 \n",
      "     Training Step: 128 Training Loss: 0.6122778654098511 \n",
      "     Training Step: 129 Training Loss: 0.6142087578773499 \n",
      "     Training Step: 130 Training Loss: 0.6167680025100708 \n",
      "     Training Step: 131 Training Loss: 0.6142927408218384 \n",
      "     Training Step: 132 Training Loss: 0.6140264272689819 \n",
      "     Training Step: 133 Training Loss: 0.6104701161384583 \n",
      "     Training Step: 134 Training Loss: 0.616948664188385 \n",
      "     Training Step: 135 Training Loss: 0.6160038113594055 \n",
      "     Training Step: 136 Training Loss: 0.6168078184127808 \n",
      "     Training Step: 137 Training Loss: 0.617091715335846 \n",
      "     Training Step: 138 Training Loss: 0.6151769757270813 \n",
      "     Training Step: 139 Training Loss: 0.6157433986663818 \n",
      "     Training Step: 140 Training Loss: 0.6129385232925415 \n",
      "     Training Step: 141 Training Loss: 0.6160315275192261 \n",
      "     Training Step: 142 Training Loss: 0.6127697229385376 \n",
      "     Training Step: 143 Training Loss: 0.6198484301567078 \n",
      "     Training Step: 144 Training Loss: 0.6097707152366638 \n",
      "     Training Step: 145 Training Loss: 0.6083261966705322 \n",
      "     Training Step: 146 Training Loss: 0.6171278953552246 \n",
      "     Training Step: 147 Training Loss: 0.616800844669342 \n",
      "     Training Step: 148 Training Loss: 0.6114192605018616 \n",
      "     Training Step: 149 Training Loss: 0.6115341782569885 \n",
      "     Training Step: 150 Training Loss: 0.6164934039115906 \n",
      "     Training Step: 151 Training Loss: 0.61665940284729 \n",
      "     Training Step: 152 Training Loss: 0.6129120588302612 \n",
      "     Training Step: 153 Training Loss: 0.6120139956474304 \n",
      "     Training Step: 154 Training Loss: 0.6115257740020752 \n",
      "     Training Step: 155 Training Loss: 0.6184312701225281 \n",
      "     Training Step: 156 Training Loss: 0.611553430557251 \n",
      "     Training Step: 157 Training Loss: 0.6111543774604797 \n",
      "     Training Step: 158 Training Loss: 0.6130579113960266 \n",
      "     Training Step: 159 Training Loss: 0.6114107370376587 \n",
      "     Training Step: 160 Training Loss: 0.615161120891571 \n",
      "     Training Step: 161 Training Loss: 0.6167336106300354 \n",
      "     Training Step: 162 Training Loss: 0.6127614378929138 \n",
      "     Training Step: 163 Training Loss: 0.6144529581069946 \n",
      "     Training Step: 164 Training Loss: 0.6147050261497498 \n",
      "     Training Step: 165 Training Loss: 0.6176940202713013 \n",
      "     Training Step: 166 Training Loss: 0.6153004765510559 \n",
      "     Training Step: 167 Training Loss: 0.6143484115600586 \n",
      "     Training Step: 168 Training Loss: 0.6114479303359985 \n",
      "     Training Step: 169 Training Loss: 0.6167527437210083 \n",
      "     Training Step: 170 Training Loss: 0.6144976615905762 \n",
      "     Training Step: 171 Training Loss: 0.6104050278663635 \n",
      "     Training Step: 172 Training Loss: 0.6152843236923218 \n",
      "     Training Step: 173 Training Loss: 0.6100919246673584 \n",
      "     Training Step: 174 Training Loss: 0.6146368384361267 \n",
      "     Training Step: 175 Training Loss: 0.6139149069786072 \n",
      "     Training Step: 176 Training Loss: 0.6144383549690247 \n",
      "     Training Step: 177 Training Loss: 0.6128653883934021 \n",
      "     Training Step: 178 Training Loss: 0.61424320936203 \n",
      "     Training Step: 179 Training Loss: 0.615289568901062 \n",
      "     Training Step: 180 Training Loss: 0.6117833852767944 \n",
      "     Training Step: 181 Training Loss: 0.6164398789405823 \n",
      "     Training Step: 182 Training Loss: 0.6118199825286865 \n",
      "     Training Step: 183 Training Loss: 0.6124826669692993 \n",
      "     Training Step: 184 Training Loss: 0.6137441396713257 \n",
      "     Training Step: 185 Training Loss: 0.611683189868927 \n",
      "     Training Step: 186 Training Loss: 0.612371027469635 \n",
      "     Training Step: 187 Training Loss: 0.6189026236534119 \n",
      "     Training Step: 188 Training Loss: 0.614482581615448 \n",
      "     Training Step: 189 Training Loss: 0.6146560907363892 \n",
      "     Training Step: 190 Training Loss: 0.6182355880737305 \n",
      "     Training Step: 191 Training Loss: 0.6136476397514343 \n",
      "     Training Step: 192 Training Loss: 0.6157827377319336 \n",
      "     Training Step: 193 Training Loss: 0.6156809329986572 \n",
      "     Training Step: 194 Training Loss: 0.6107557415962219 \n",
      "     Training Step: 195 Training Loss: 0.6133081316947937 \n",
      "     Training Step: 196 Training Loss: 0.6106247305870056 \n",
      "     Training Step: 197 Training Loss: 0.6184267401695251 \n",
      "     Training Step: 198 Training Loss: 0.6111574769020081 \n",
      "     Training Step: 199 Training Loss: 0.6158298254013062 \n",
      "     Training Step: 200 Training Loss: 0.6150663495063782 \n",
      "     Training Step: 201 Training Loss: 0.6142040491104126 \n",
      "     Training Step: 202 Training Loss: 0.6196141839027405 \n",
      "     Training Step: 203 Training Loss: 0.6140427589416504 \n",
      "     Training Step: 204 Training Loss: 0.6125402450561523 \n",
      "     Training Step: 205 Training Loss: 0.6149338483810425 \n",
      "     Training Step: 206 Training Loss: 0.615406334400177 \n",
      "     Training Step: 207 Training Loss: 0.6196902394294739 \n",
      "     Training Step: 208 Training Loss: 0.6112121343612671 \n",
      "     Training Step: 209 Training Loss: 0.6209185123443604 \n",
      "     Training Step: 210 Training Loss: 0.6147320866584778 \n",
      "     Training Step: 211 Training Loss: 0.6128847002983093 \n",
      "     Training Step: 212 Training Loss: 0.6153685450553894 \n",
      "     Training Step: 213 Training Loss: 0.6126552224159241 \n",
      "     Training Step: 214 Training Loss: 0.6135222315788269 \n",
      "     Training Step: 215 Training Loss: 0.61402428150177 \n",
      "     Training Step: 216 Training Loss: 0.6118122339248657 \n",
      "     Training Step: 217 Training Loss: 0.6136375665664673 \n",
      "     Training Step: 218 Training Loss: 0.6185846924781799 \n",
      "     Training Step: 219 Training Loss: 0.610051691532135 \n",
      "     Training Step: 220 Training Loss: 0.6155382394790649 \n",
      "     Training Step: 221 Training Loss: 0.613201379776001 \n",
      "     Training Step: 222 Training Loss: 0.6100585460662842 \n",
      "     Training Step: 223 Training Loss: 0.6116187572479248 \n",
      "     Training Step: 224 Training Loss: 0.6181215047836304 \n",
      "     Training Step: 225 Training Loss: 0.618483304977417 \n",
      "     Training Step: 226 Training Loss: 0.616908073425293 \n",
      "     Training Step: 227 Training Loss: 0.6106842160224915 \n",
      "     Training Step: 228 Training Loss: 0.6162415742874146 \n",
      "     Training Step: 229 Training Loss: 0.6134587526321411 \n",
      "     Training Step: 230 Training Loss: 0.6177957057952881 \n",
      "     Training Step: 231 Training Loss: 0.6137361526489258 \n",
      "     Training Step: 232 Training Loss: 0.6176835298538208 \n",
      "     Training Step: 233 Training Loss: 0.6129730939865112 \n",
      "     Training Step: 234 Training Loss: 0.6104252934455872 \n",
      "     Training Step: 235 Training Loss: 0.6132106781005859 \n",
      "     Training Step: 236 Training Loss: 0.6147955656051636 \n",
      "     Training Step: 237 Training Loss: 0.6132968068122864 \n",
      "     Training Step: 238 Training Loss: 0.6094188690185547 \n",
      "     Training Step: 239 Training Loss: 0.6116155385971069 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6118782162666321 \n",
      "     Validation Step: 1 Validation Loss: 0.6136646866798401 \n",
      "     Validation Step: 2 Validation Loss: 0.6105106472969055 \n",
      "     Validation Step: 3 Validation Loss: 0.6101313829421997 \n",
      "     Validation Step: 4 Validation Loss: 0.6162444353103638 \n",
      "     Validation Step: 5 Validation Loss: 0.6182355880737305 \n",
      "     Validation Step: 6 Validation Loss: 0.6111756563186646 \n",
      "     Validation Step: 7 Validation Loss: 0.6148951053619385 \n",
      "     Validation Step: 8 Validation Loss: 0.6141149997711182 \n",
      "     Validation Step: 9 Validation Loss: 0.6145725846290588 \n",
      "     Validation Step: 10 Validation Loss: 0.6129816174507141 \n",
      "     Validation Step: 11 Validation Loss: 0.6142001152038574 \n",
      "     Validation Step: 12 Validation Loss: 0.6170276999473572 \n",
      "     Validation Step: 13 Validation Loss: 0.6146365404129028 \n",
      "     Validation Step: 14 Validation Loss: 0.6121256947517395 \n",
      "     Validation Step: 15 Validation Loss: 0.6106281280517578 \n",
      "     Validation Step: 16 Validation Loss: 0.6136507391929626 \n",
      "     Validation Step: 17 Validation Loss: 0.6101588606834412 \n",
      "     Validation Step: 18 Validation Loss: 0.6156166195869446 \n",
      "     Validation Step: 19 Validation Loss: 0.6183474063873291 \n",
      "     Validation Step: 20 Validation Loss: 0.615795373916626 \n",
      "     Validation Step: 21 Validation Loss: 0.611159086227417 \n",
      "     Validation Step: 22 Validation Loss: 0.6173123121261597 \n",
      "     Validation Step: 23 Validation Loss: 0.6155760288238525 \n",
      "     Validation Step: 24 Validation Loss: 0.6159982085227966 \n",
      "     Validation Step: 25 Validation Loss: 0.6133038997650146 \n",
      "     Validation Step: 26 Validation Loss: 0.6184784173965454 \n",
      "     Validation Step: 27 Validation Loss: 0.6180680394172668 \n",
      "     Validation Step: 28 Validation Loss: 0.611639678478241 \n",
      "     Validation Step: 29 Validation Loss: 0.6136376857757568 \n",
      "     Validation Step: 30 Validation Loss: 0.6148364543914795 \n",
      "     Validation Step: 31 Validation Loss: 0.6153189539909363 \n",
      "     Validation Step: 32 Validation Loss: 0.6150383949279785 \n",
      "     Validation Step: 33 Validation Loss: 0.6145349144935608 \n",
      "     Validation Step: 34 Validation Loss: 0.610102117061615 \n",
      "     Validation Step: 35 Validation Loss: 0.6185015439987183 \n",
      "     Validation Step: 36 Validation Loss: 0.6142544150352478 \n",
      "     Validation Step: 37 Validation Loss: 0.6104778051376343 \n",
      "     Validation Step: 38 Validation Loss: 0.61155104637146 \n",
      "     Validation Step: 39 Validation Loss: 0.6128275394439697 \n",
      "     Validation Step: 40 Validation Loss: 0.6176019310951233 \n",
      "     Validation Step: 41 Validation Loss: 0.6177011728286743 \n",
      "     Validation Step: 42 Validation Loss: 0.6152288913726807 \n",
      "     Validation Step: 43 Validation Loss: 0.6075149178504944 \n",
      "     Validation Step: 44 Validation Loss: 0.614125669002533 \n",
      "Epoch: 102\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6126998066902161 \n",
      "     Training Step: 1 Training Loss: 0.6161211729049683 \n",
      "     Training Step: 2 Training Loss: 0.6166948676109314 \n",
      "     Training Step: 3 Training Loss: 0.6115995645523071 \n",
      "     Training Step: 4 Training Loss: 0.6199285387992859 \n",
      "     Training Step: 5 Training Loss: 0.61004239320755 \n",
      "     Training Step: 6 Training Loss: 0.6167362928390503 \n",
      "     Training Step: 7 Training Loss: 0.61248379945755 \n",
      "     Training Step: 8 Training Loss: 0.6121551990509033 \n",
      "     Training Step: 9 Training Loss: 0.6123769283294678 \n",
      "     Training Step: 10 Training Loss: 0.6140236854553223 \n",
      "     Training Step: 11 Training Loss: 0.6178183555603027 \n",
      "     Training Step: 12 Training Loss: 0.6122921109199524 \n",
      "     Training Step: 13 Training Loss: 0.6151736378669739 \n",
      "     Training Step: 14 Training Loss: 0.6154125332832336 \n",
      "     Training Step: 15 Training Loss: 0.6133134961128235 \n",
      "     Training Step: 16 Training Loss: 0.616803765296936 \n",
      "     Training Step: 17 Training Loss: 0.6166685223579407 \n",
      "     Training Step: 18 Training Loss: 0.6108983159065247 \n",
      "     Training Step: 19 Training Loss: 0.6100872755050659 \n",
      "     Training Step: 20 Training Loss: 0.6180136203765869 \n",
      "     Training Step: 21 Training Loss: 0.616800844669342 \n",
      "     Training Step: 22 Training Loss: 0.6121277213096619 \n",
      "     Training Step: 23 Training Loss: 0.6134986877441406 \n",
      "     Training Step: 24 Training Loss: 0.6131411790847778 \n",
      "     Training Step: 25 Training Loss: 0.6130593419075012 \n",
      "     Training Step: 26 Training Loss: 0.6146490573883057 \n",
      "     Training Step: 27 Training Loss: 0.6149566173553467 \n",
      "     Training Step: 28 Training Loss: 0.6132042407989502 \n",
      "     Training Step: 29 Training Loss: 0.6146082282066345 \n",
      "     Training Step: 30 Training Loss: 0.6171718835830688 \n",
      "     Training Step: 31 Training Loss: 0.6181082725524902 \n",
      "     Training Step: 32 Training Loss: 0.6139111518859863 \n",
      "     Training Step: 33 Training Loss: 0.6209373474121094 \n",
      "     Training Step: 34 Training Loss: 0.6157809495925903 \n",
      "     Training Step: 35 Training Loss: 0.6149284839630127 \n",
      "     Training Step: 36 Training Loss: 0.6137720346450806 \n",
      "     Training Step: 37 Training Loss: 0.6119052767753601 \n",
      "     Training Step: 38 Training Loss: 0.6146745085716248 \n",
      "     Training Step: 39 Training Loss: 0.610534131526947 \n",
      "     Training Step: 40 Training Loss: 0.6162367463111877 \n",
      "     Training Step: 41 Training Loss: 0.6144969463348389 \n",
      "     Training Step: 42 Training Loss: 0.6194339394569397 \n",
      "     Training Step: 43 Training Loss: 0.61329585313797 \n",
      "     Training Step: 44 Training Loss: 0.615942656993866 \n",
      "     Training Step: 45 Training Loss: 0.6115390062332153 \n",
      "     Training Step: 46 Training Loss: 0.6106707453727722 \n",
      "     Training Step: 47 Training Loss: 0.6112022399902344 \n",
      "     Training Step: 48 Training Loss: 0.616352915763855 \n",
      "     Training Step: 49 Training Loss: 0.6178054809570312 \n",
      "     Training Step: 50 Training Loss: 0.6160428524017334 \n",
      "     Training Step: 51 Training Loss: 0.6114091277122498 \n",
      "     Training Step: 52 Training Loss: 0.6140153408050537 \n",
      "     Training Step: 53 Training Loss: 0.6162069439888 \n",
      "     Training Step: 54 Training Loss: 0.6153857111930847 \n",
      "     Training Step: 55 Training Loss: 0.6132804751396179 \n",
      "     Training Step: 56 Training Loss: 0.6152844429016113 \n",
      "     Training Step: 57 Training Loss: 0.6147918105125427 \n",
      "     Training Step: 58 Training Loss: 0.6173796653747559 \n",
      "     Training Step: 59 Training Loss: 0.614280641078949 \n",
      "     Training Step: 60 Training Loss: 0.6120771169662476 \n",
      "     Training Step: 61 Training Loss: 0.616200864315033 \n",
      "     Training Step: 62 Training Loss: 0.6097459197044373 \n",
      "     Training Step: 63 Training Loss: 0.6107043623924255 \n",
      "     Training Step: 64 Training Loss: 0.615831196308136 \n",
      "     Training Step: 65 Training Loss: 0.6132798790931702 \n",
      "     Training Step: 66 Training Loss: 0.6147012710571289 \n",
      "     Training Step: 67 Training Loss: 0.6184641122817993 \n",
      "     Training Step: 68 Training Loss: 0.6133025288581848 \n",
      "     Training Step: 69 Training Loss: 0.6136205792427063 \n",
      "     Training Step: 70 Training Loss: 0.615536093711853 \n",
      "     Training Step: 71 Training Loss: 0.6150866150856018 \n",
      "     Training Step: 72 Training Loss: 0.614601731300354 \n",
      "     Training Step: 73 Training Loss: 0.6176880598068237 \n",
      "     Training Step: 74 Training Loss: 0.6196955442428589 \n",
      "     Training Step: 75 Training Loss: 0.614456832408905 \n",
      "     Training Step: 76 Training Loss: 0.61670982837677 \n",
      "     Training Step: 77 Training Loss: 0.6129196882247925 \n",
      "     Training Step: 78 Training Loss: 0.6143732666969299 \n",
      "     Training Step: 79 Training Loss: 0.6182969212532043 \n",
      "     Training Step: 80 Training Loss: 0.6148719191551208 \n",
      "     Training Step: 81 Training Loss: 0.6128917336463928 \n",
      "     Training Step: 82 Training Loss: 0.6154060363769531 \n",
      "     Training Step: 83 Training Loss: 0.610762894153595 \n",
      "     Training Step: 84 Training Loss: 0.615475058555603 \n",
      "     Training Step: 85 Training Loss: 0.6115435361862183 \n",
      "     Training Step: 86 Training Loss: 0.6111683249473572 \n",
      "     Training Step: 87 Training Loss: 0.6121395230293274 \n",
      "     Training Step: 88 Training Loss: 0.6164237260818481 \n",
      "     Training Step: 89 Training Loss: 0.6153807640075684 \n",
      "     Training Step: 90 Training Loss: 0.616908848285675 \n",
      "     Training Step: 91 Training Loss: 0.6143428683280945 \n",
      "     Training Step: 92 Training Loss: 0.616662323474884 \n",
      "     Training Step: 93 Training Loss: 0.6149218678474426 \n",
      "     Training Step: 94 Training Loss: 0.6171465516090393 \n",
      "     Training Step: 95 Training Loss: 0.6153038740158081 \n",
      "     Training Step: 96 Training Loss: 0.613886833190918 \n",
      "     Training Step: 97 Training Loss: 0.6144822835922241 \n",
      "     Training Step: 98 Training Loss: 0.6166427135467529 \n",
      "     Training Step: 99 Training Loss: 0.6154433488845825 \n",
      "     Training Step: 100 Training Loss: 0.6181960701942444 \n",
      "     Training Step: 101 Training Loss: 0.6137592792510986 \n",
      "     Training Step: 102 Training Loss: 0.614084780216217 \n",
      "     Training Step: 103 Training Loss: 0.6133837699890137 \n",
      "     Training Step: 104 Training Loss: 0.6114875674247742 \n",
      "     Training Step: 105 Training Loss: 0.6182225942611694 \n",
      "     Training Step: 106 Training Loss: 0.610609233379364 \n",
      "     Training Step: 107 Training Loss: 0.6140450835227966 \n",
      "     Training Step: 108 Training Loss: 0.6156768202781677 \n",
      "     Training Step: 109 Training Loss: 0.6157407760620117 \n",
      "     Training Step: 110 Training Loss: 0.6124660968780518 \n",
      "     Training Step: 111 Training Loss: 0.6180734634399414 \n",
      "     Training Step: 112 Training Loss: 0.6144260168075562 \n",
      "     Training Step: 113 Training Loss: 0.6172012090682983 \n",
      "     Training Step: 114 Training Loss: 0.6120224595069885 \n",
      "     Training Step: 115 Training Loss: 0.6093984842300415 \n",
      "     Training Step: 116 Training Loss: 0.6100384593009949 \n",
      "     Training Step: 117 Training Loss: 0.6129183173179626 \n",
      "     Training Step: 118 Training Loss: 0.6154337525367737 \n",
      "     Training Step: 119 Training Loss: 0.617673933506012 \n",
      "     Training Step: 120 Training Loss: 0.611132025718689 \n",
      "     Training Step: 121 Training Loss: 0.6101222634315491 \n",
      "     Training Step: 122 Training Loss: 0.6126382350921631 \n",
      "     Training Step: 123 Training Loss: 0.6104657649993896 \n",
      "     Training Step: 124 Training Loss: 0.6123711466789246 \n",
      "     Training Step: 125 Training Loss: 0.6153481602668762 \n",
      "     Training Step: 126 Training Loss: 0.6103567481040955 \n",
      "     Training Step: 127 Training Loss: 0.6156924366950989 \n",
      "     Training Step: 128 Training Loss: 0.6117749810218811 \n",
      "     Training Step: 129 Training Loss: 0.6136450171470642 \n",
      "     Training Step: 130 Training Loss: 0.6115396618843079 \n",
      "     Training Step: 131 Training Loss: 0.6142076253890991 \n",
      "     Training Step: 132 Training Loss: 0.6157556176185608 \n",
      "     Training Step: 133 Training Loss: 0.6092089414596558 \n",
      "     Training Step: 134 Training Loss: 0.6188883185386658 \n",
      "     Training Step: 135 Training Loss: 0.6139177083969116 \n",
      "     Training Step: 136 Training Loss: 0.6116827130317688 \n",
      "     Training Step: 137 Training Loss: 0.6094443798065186 \n",
      "     Training Step: 138 Training Loss: 0.6134530305862427 \n",
      "     Training Step: 139 Training Loss: 0.6157601475715637 \n",
      "     Training Step: 140 Training Loss: 0.613408088684082 \n",
      "     Training Step: 141 Training Loss: 0.6122756004333496 \n",
      "     Training Step: 142 Training Loss: 0.612505316734314 \n",
      "     Training Step: 143 Training Loss: 0.610730767250061 \n",
      "     Training Step: 144 Training Loss: 0.610875129699707 \n",
      "     Training Step: 145 Training Loss: 0.6131839156150818 \n",
      "     Training Step: 146 Training Loss: 0.6130764484405518 \n",
      "     Training Step: 147 Training Loss: 0.6105585098266602 \n",
      "     Training Step: 148 Training Loss: 0.6114071011543274 \n",
      "     Training Step: 149 Training Loss: 0.6127517819404602 \n",
      "     Training Step: 150 Training Loss: 0.611616313457489 \n",
      "     Training Step: 151 Training Loss: 0.614251971244812 \n",
      "     Training Step: 152 Training Loss: 0.6113853454589844 \n",
      "     Training Step: 153 Training Loss: 0.614741325378418 \n",
      "     Training Step: 154 Training Loss: 0.6146549582481384 \n",
      "     Training Step: 155 Training Loss: 0.6171814203262329 \n",
      "     Training Step: 156 Training Loss: 0.6164326071739197 \n",
      "     Training Step: 157 Training Loss: 0.615240216255188 \n",
      "     Training Step: 158 Training Loss: 0.6105908751487732 \n",
      "     Training Step: 159 Training Loss: 0.6169371008872986 \n",
      "     Training Step: 160 Training Loss: 0.6137426495552063 \n",
      "     Training Step: 161 Training Loss: 0.6104130148887634 \n",
      "     Training Step: 162 Training Loss: 0.612541675567627 \n",
      "     Training Step: 163 Training Loss: 0.6121720671653748 \n",
      "     Training Step: 164 Training Loss: 0.6164857149124146 \n",
      "     Training Step: 165 Training Loss: 0.609721302986145 \n",
      "     Training Step: 166 Training Loss: 0.6166164875030518 \n",
      "     Training Step: 167 Training Loss: 0.6186023354530334 \n",
      "     Training Step: 168 Training Loss: 0.6146707534790039 \n",
      "     Training Step: 169 Training Loss: 0.6127661466598511 \n",
      "     Training Step: 170 Training Loss: 0.6170975565910339 \n",
      "     Training Step: 171 Training Loss: 0.6176862120628357 \n",
      "     Training Step: 172 Training Loss: 0.6202138662338257 \n",
      "     Training Step: 173 Training Loss: 0.6123883724212646 \n",
      "     Training Step: 174 Training Loss: 0.6155261397361755 \n",
      "     Training Step: 175 Training Loss: 0.6143399477005005 \n",
      "     Training Step: 176 Training Loss: 0.615156888961792 \n",
      "     Training Step: 177 Training Loss: 0.6129851937294006 \n",
      "     Training Step: 178 Training Loss: 0.6167398691177368 \n",
      "     Training Step: 179 Training Loss: 0.6155113577842712 \n",
      "     Training Step: 180 Training Loss: 0.6150649785995483 \n",
      "     Training Step: 181 Training Loss: 0.6176865696907043 \n",
      "     Training Step: 182 Training Loss: 0.6116200685501099 \n",
      "     Training Step: 183 Training Loss: 0.6147463917732239 \n",
      "     Training Step: 184 Training Loss: 0.6138281226158142 \n",
      "     Training Step: 185 Training Loss: 0.6132088899612427 \n",
      "     Training Step: 186 Training Loss: 0.611846387386322 \n",
      "     Training Step: 187 Training Loss: 0.6114478707313538 \n",
      "     Training Step: 188 Training Loss: 0.618037760257721 \n",
      "     Training Step: 189 Training Loss: 0.6122255921363831 \n",
      "     Training Step: 190 Training Loss: 0.6127941608428955 \n",
      "     Training Step: 191 Training Loss: 0.6196433901786804 \n",
      "     Training Step: 192 Training Loss: 0.6141549944877625 \n",
      "     Training Step: 193 Training Loss: 0.6082611680030823 \n",
      "     Training Step: 194 Training Loss: 0.6099860668182373 \n",
      "     Training Step: 195 Training Loss: 0.6184314489364624 \n",
      "     Training Step: 196 Training Loss: 0.6135910153388977 \n",
      "     Training Step: 197 Training Loss: 0.6117992401123047 \n",
      "     Training Step: 198 Training Loss: 0.6152867078781128 \n",
      "     Training Step: 199 Training Loss: 0.6134647727012634 \n",
      "     Training Step: 200 Training Loss: 0.6144380569458008 \n",
      "     Training Step: 201 Training Loss: 0.6167096495628357 \n",
      "     Training Step: 202 Training Loss: 0.6168032288551331 \n",
      "     Training Step: 203 Training Loss: 0.6141068935394287 \n",
      "     Training Step: 204 Training Loss: 0.6141542196273804 \n",
      "     Training Step: 205 Training Loss: 0.6150938868522644 \n",
      "     Training Step: 206 Training Loss: 0.6102041602134705 \n",
      "     Training Step: 207 Training Loss: 0.6118162274360657 \n",
      "     Training Step: 208 Training Loss: 0.6118424534797668 \n",
      "     Training Step: 209 Training Loss: 0.614203691482544 \n",
      "     Training Step: 210 Training Loss: 0.6116220355033875 \n",
      "     Training Step: 211 Training Loss: 0.6152661442756653 \n",
      "     Training Step: 212 Training Loss: 0.6135132908821106 \n",
      "     Training Step: 213 Training Loss: 0.6116377711296082 \n",
      "     Training Step: 214 Training Loss: 0.6186019778251648 \n",
      "     Training Step: 215 Training Loss: 0.6136513948440552 \n",
      "     Training Step: 216 Training Loss: 0.6167317032814026 \n",
      "     Training Step: 217 Training Loss: 0.6160106658935547 \n",
      "     Training Step: 218 Training Loss: 0.6105664372444153 \n",
      "     Training Step: 219 Training Loss: 0.6115242838859558 \n",
      "     Training Step: 220 Training Loss: 0.6184505820274353 \n",
      "     Training Step: 221 Training Loss: 0.6146765947341919 \n",
      "     Training Step: 222 Training Loss: 0.6128644943237305 \n",
      "     Training Step: 223 Training Loss: 0.6143425703048706 \n",
      "     Training Step: 224 Training Loss: 0.6128430962562561 \n",
      "     Training Step: 225 Training Loss: 0.6122454404830933 \n",
      "     Training Step: 226 Training Loss: 0.6147055625915527 \n",
      "     Training Step: 227 Training Loss: 0.6147705912590027 \n",
      "     Training Step: 228 Training Loss: 0.6155955791473389 \n",
      "     Training Step: 229 Training Loss: 0.6174854040145874 \n",
      "     Training Step: 230 Training Loss: 0.6122305989265442 \n",
      "     Training Step: 231 Training Loss: 0.6188619136810303 \n",
      "     Training Step: 232 Training Loss: 0.614687442779541 \n",
      "     Training Step: 233 Training Loss: 0.6177493333816528 \n",
      "     Training Step: 234 Training Loss: 0.6118521094322205 \n",
      "     Training Step: 235 Training Loss: 0.6146103143692017 \n",
      "     Training Step: 236 Training Loss: 0.6131321787834167 \n",
      "     Training Step: 237 Training Loss: 0.6125442385673523 \n",
      "     Training Step: 238 Training Loss: 0.6097478270530701 \n",
      "     Training Step: 239 Training Loss: 0.6114850640296936 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6156186461448669 \n",
      "     Validation Step: 1 Validation Loss: 0.6162464618682861 \n",
      "     Validation Step: 2 Validation Loss: 0.6148932576179504 \n",
      "     Validation Step: 3 Validation Loss: 0.6136705875396729 \n",
      "     Validation Step: 4 Validation Loss: 0.6160065531730652 \n",
      "     Validation Step: 5 Validation Loss: 0.6141308546066284 \n",
      "     Validation Step: 6 Validation Loss: 0.6111618876457214 \n",
      "     Validation Step: 7 Validation Loss: 0.615581214427948 \n",
      "     Validation Step: 8 Validation Loss: 0.618476390838623 \n",
      "     Validation Step: 9 Validation Loss: 0.6173081398010254 \n",
      "     Validation Step: 10 Validation Loss: 0.6136593818664551 \n",
      "     Validation Step: 11 Validation Loss: 0.6145357489585876 \n",
      "     Validation Step: 12 Validation Loss: 0.6180686354637146 \n",
      "     Validation Step: 13 Validation Loss: 0.6136321425437927 \n",
      "     Validation Step: 14 Validation Loss: 0.6175974011421204 \n",
      "     Validation Step: 15 Validation Loss: 0.6182399392127991 \n",
      "     Validation Step: 16 Validation Loss: 0.6116387248039246 \n",
      "     Validation Step: 17 Validation Loss: 0.6157971620559692 \n",
      "     Validation Step: 18 Validation Loss: 0.6101328134536743 \n",
      "     Validation Step: 19 Validation Loss: 0.6118776798248291 \n",
      "     Validation Step: 20 Validation Loss: 0.6148366332054138 \n",
      "     Validation Step: 21 Validation Loss: 0.61051344871521 \n",
      "     Validation Step: 22 Validation Loss: 0.6145697236061096 \n",
      "     Validation Step: 23 Validation Loss: 0.6142545342445374 \n",
      "     Validation Step: 24 Validation Loss: 0.6170312166213989 \n",
      "     Validation Step: 25 Validation Loss: 0.6150469779968262 \n",
      "     Validation Step: 26 Validation Loss: 0.6153228878974915 \n",
      "     Validation Step: 27 Validation Loss: 0.611177384853363 \n",
      "     Validation Step: 28 Validation Loss: 0.6129853129386902 \n",
      "     Validation Step: 29 Validation Loss: 0.6142007112503052 \n",
      "     Validation Step: 30 Validation Loss: 0.6101563572883606 \n",
      "     Validation Step: 31 Validation Loss: 0.6141131520271301 \n",
      "     Validation Step: 32 Validation Loss: 0.6075171828269958 \n",
      "     Validation Step: 33 Validation Loss: 0.6101070046424866 \n",
      "     Validation Step: 34 Validation Loss: 0.6106266379356384 \n",
      "     Validation Step: 35 Validation Loss: 0.6183522939682007 \n",
      "     Validation Step: 36 Validation Loss: 0.615229070186615 \n",
      "     Validation Step: 37 Validation Loss: 0.6177032589912415 \n",
      "     Validation Step: 38 Validation Loss: 0.6128249764442444 \n",
      "     Validation Step: 39 Validation Loss: 0.6146392822265625 \n",
      "     Validation Step: 40 Validation Loss: 0.6133036017417908 \n",
      "     Validation Step: 41 Validation Loss: 0.61048424243927 \n",
      "     Validation Step: 42 Validation Loss: 0.6121255159378052 \n",
      "     Validation Step: 43 Validation Loss: 0.6185010671615601 \n",
      "     Validation Step: 44 Validation Loss: 0.611553430557251 \n",
      "Epoch: 103\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6121253371238708 \n",
      "     Training Step: 1 Training Loss: 0.6176614761352539 \n",
      "     Training Step: 2 Training Loss: 0.6186091303825378 \n",
      "     Training Step: 3 Training Loss: 0.6099756956100464 \n",
      "     Training Step: 4 Training Loss: 0.612694263458252 \n",
      "     Training Step: 5 Training Loss: 0.6137658357620239 \n",
      "     Training Step: 6 Training Loss: 0.6096832752227783 \n",
      "     Training Step: 7 Training Loss: 0.6115738749504089 \n",
      "     Training Step: 8 Training Loss: 0.614671528339386 \n",
      "     Training Step: 9 Training Loss: 0.6178389191627502 \n",
      "     Training Step: 10 Training Loss: 0.6196764707565308 \n",
      "     Training Step: 11 Training Loss: 0.619739830493927 \n",
      "     Training Step: 12 Training Loss: 0.6128406524658203 \n",
      "     Training Step: 13 Training Loss: 0.6100451946258545 \n",
      "     Training Step: 14 Training Loss: 0.6101441979408264 \n",
      "     Training Step: 15 Training Loss: 0.6131216883659363 \n",
      "     Training Step: 16 Training Loss: 0.616655707359314 \n",
      "     Training Step: 17 Training Loss: 0.6122871041297913 \n",
      "     Training Step: 18 Training Loss: 0.6153727173805237 \n",
      "     Training Step: 19 Training Loss: 0.6184029579162598 \n",
      "     Training Step: 20 Training Loss: 0.6127668619155884 \n",
      "     Training Step: 21 Training Loss: 0.6097414493560791 \n",
      "     Training Step: 22 Training Loss: 0.6133182644844055 \n",
      "     Training Step: 23 Training Loss: 0.6148694753646851 \n",
      "     Training Step: 24 Training Loss: 0.6168909668922424 \n",
      "     Training Step: 25 Training Loss: 0.6157824397087097 \n",
      "     Training Step: 26 Training Loss: 0.612069308757782 \n",
      "     Training Step: 27 Training Loss: 0.6141063570976257 \n",
      "     Training Step: 28 Training Loss: 0.6150855422019958 \n",
      "     Training Step: 29 Training Loss: 0.6121509075164795 \n",
      "     Training Step: 30 Training Loss: 0.6181073188781738 \n",
      "     Training Step: 31 Training Loss: 0.6111994981765747 \n",
      "     Training Step: 32 Training Loss: 0.6194484233856201 \n",
      "     Training Step: 33 Training Loss: 0.6146052479743958 \n",
      "     Training Step: 34 Training Loss: 0.6146017909049988 \n",
      "     Training Step: 35 Training Loss: 0.6174800992012024 \n",
      "     Training Step: 36 Training Loss: 0.6105195879936218 \n",
      "     Training Step: 37 Training Loss: 0.6147434711456299 \n",
      "     Training Step: 38 Training Loss: 0.6120294332504272 \n",
      "     Training Step: 39 Training Loss: 0.6167086362838745 \n",
      "     Training Step: 40 Training Loss: 0.6142423152923584 \n",
      "     Training Step: 41 Training Loss: 0.6144799590110779 \n",
      "     Training Step: 42 Training Loss: 0.6140396595001221 \n",
      "     Training Step: 43 Training Loss: 0.6118407249450684 \n",
      "     Training Step: 44 Training Loss: 0.6121437549591064 \n",
      "     Training Step: 45 Training Loss: 0.6161130666732788 \n",
      "     Training Step: 46 Training Loss: 0.6124646067619324 \n",
      "     Training Step: 47 Training Loss: 0.6129140257835388 \n",
      "     Training Step: 48 Training Loss: 0.6104773283004761 \n",
      "     Training Step: 49 Training Loss: 0.6153977513313293 \n",
      "     Training Step: 50 Training Loss: 0.6144523620605469 \n",
      "     Training Step: 51 Training Loss: 0.6147745847702026 \n",
      "     Training Step: 52 Training Loss: 0.6164222955703735 \n",
      "     Training Step: 53 Training Loss: 0.6138895153999329 \n",
      "     Training Step: 54 Training Loss: 0.6111522912979126 \n",
      "     Training Step: 55 Training Loss: 0.6116361021995544 \n",
      "     Training Step: 56 Training Loss: 0.6114262938499451 \n",
      "     Training Step: 57 Training Loss: 0.6137354969978333 \n",
      "     Training Step: 58 Training Loss: 0.6124845743179321 \n",
      "     Training Step: 59 Training Loss: 0.6116222143173218 \n",
      "     Training Step: 60 Training Loss: 0.6082383394241333 \n",
      "     Training Step: 61 Training Loss: 0.6122267246246338 \n",
      "     Training Step: 62 Training Loss: 0.6105723977088928 \n",
      "     Training Step: 63 Training Loss: 0.6132814288139343 \n",
      "     Training Step: 64 Training Loss: 0.6162383556365967 \n",
      "     Training Step: 65 Training Loss: 0.6167513728141785 \n",
      "     Training Step: 66 Training Loss: 0.6177994608879089 \n",
      "     Training Step: 67 Training Loss: 0.6152964234352112 \n",
      "     Training Step: 68 Training Loss: 0.6168180704116821 \n",
      "     Training Step: 69 Training Loss: 0.6152383089065552 \n",
      "     Training Step: 70 Training Loss: 0.6180354952812195 \n",
      "     Training Step: 71 Training Loss: 0.6092597246170044 \n",
      "     Training Step: 72 Training Loss: 0.6184514760971069 \n",
      "     Training Step: 73 Training Loss: 0.6130816340446472 \n",
      "     Training Step: 74 Training Loss: 0.610101580619812 \n",
      "     Training Step: 75 Training Loss: 0.6130833625793457 \n",
      "     Training Step: 76 Training Loss: 0.613629937171936 \n",
      "     Training Step: 77 Training Loss: 0.6157403588294983 \n",
      "     Training Step: 78 Training Loss: 0.6151719093322754 \n",
      "     Training Step: 79 Training Loss: 0.617378294467926 \n",
      "     Training Step: 80 Training Loss: 0.616802453994751 \n",
      "     Training Step: 81 Training Loss: 0.6114144325256348 \n",
      "     Training Step: 82 Training Loss: 0.6157428622245789 \n",
      "     Training Step: 83 Training Loss: 0.6131409406661987 \n",
      "     Training Step: 84 Training Loss: 0.6114799976348877 \n",
      "     Training Step: 85 Training Loss: 0.6105722188949585 \n",
      "     Training Step: 86 Training Loss: 0.6094505190849304 \n",
      "     Training Step: 87 Training Loss: 0.6116023659706116 \n",
      "     Training Step: 88 Training Loss: 0.6155408620834351 \n",
      "     Training Step: 89 Training Loss: 0.6166866421699524 \n",
      "     Training Step: 90 Training Loss: 0.6143574118614197 \n",
      "     Training Step: 91 Training Loss: 0.6129631400108337 \n",
      "     Training Step: 92 Training Loss: 0.6169641017913818 \n",
      "     Training Step: 93 Training Loss: 0.6126372218132019 \n",
      "     Training Step: 94 Training Loss: 0.610569953918457 \n",
      "     Training Step: 95 Training Loss: 0.6147955656051636 \n",
      "     Training Step: 96 Training Loss: 0.6144230365753174 \n",
      "     Training Step: 97 Training Loss: 0.6123764514923096 \n",
      "     Training Step: 98 Training Loss: 0.6160027980804443 \n",
      "     Training Step: 99 Training Loss: 0.615602433681488 \n",
      "     Training Step: 100 Training Loss: 0.6136361360549927 \n",
      "     Training Step: 101 Training Loss: 0.6114404797554016 \n",
      "     Training Step: 102 Training Loss: 0.6147037744522095 \n",
      "     Training Step: 103 Training Loss: 0.6106697916984558 \n",
      "     Training Step: 104 Training Loss: 0.6154211163520813 \n",
      "     Training Step: 105 Training Loss: 0.6157543659210205 \n",
      "     Training Step: 106 Training Loss: 0.6151018738746643 \n",
      "     Training Step: 107 Training Loss: 0.6183221340179443 \n",
      "     Training Step: 108 Training Loss: 0.6185972690582275 \n",
      "     Training Step: 109 Training Loss: 0.6176841855049133 \n",
      "     Training Step: 110 Training Loss: 0.6107437610626221 \n",
      "     Training Step: 111 Training Loss: 0.6150656342506409 \n",
      "     Training Step: 112 Training Loss: 0.6114405989646912 \n",
      "     Training Step: 113 Training Loss: 0.6184284687042236 \n",
      "     Training Step: 114 Training Loss: 0.6147338151931763 \n",
      "     Training Step: 115 Training Loss: 0.6152920126914978 \n",
      "     Training Step: 116 Training Loss: 0.6152784824371338 \n",
      "     Training Step: 117 Training Loss: 0.6117972731590271 \n",
      "     Training Step: 118 Training Loss: 0.6143379211425781 \n",
      "     Training Step: 119 Training Loss: 0.616746187210083 \n",
      "     Training Step: 120 Training Loss: 0.612871527671814 \n",
      "     Training Step: 121 Training Loss: 0.6115479469299316 \n",
      "     Training Step: 122 Training Loss: 0.6142868995666504 \n",
      "     Training Step: 123 Training Loss: 0.6116918325424194 \n",
      "     Training Step: 124 Training Loss: 0.6170955300331116 \n",
      "     Training Step: 125 Training Loss: 0.6141541004180908 \n",
      "     Training Step: 126 Training Loss: 0.6118305325508118 \n",
      "     Training Step: 127 Training Loss: 0.6154442429542542 \n",
      "     Training Step: 128 Training Loss: 0.6171663999557495 \n",
      "     Training Step: 129 Training Loss: 0.6125319004058838 \n",
      "     Training Step: 130 Training Loss: 0.6140219569206238 \n",
      "     Training Step: 131 Training Loss: 0.6159469485282898 \n",
      "     Training Step: 132 Training Loss: 0.6176884174346924 \n",
      "     Training Step: 133 Training Loss: 0.6152684688568115 \n",
      "     Training Step: 134 Training Loss: 0.6118404269218445 \n",
      "     Training Step: 135 Training Loss: 0.6166737079620361 \n",
      "     Training Step: 136 Training Loss: 0.6140201687812805 \n",
      "     Training Step: 137 Training Loss: 0.6132904887199402 \n",
      "     Training Step: 138 Training Loss: 0.6103998422622681 \n",
      "     Training Step: 139 Training Loss: 0.6128038167953491 \n",
      "     Training Step: 140 Training Loss: 0.6141485571861267 \n",
      "     Training Step: 141 Training Loss: 0.6167211532592773 \n",
      "     Training Step: 142 Training Loss: 0.6167169809341431 \n",
      "     Training Step: 143 Training Loss: 0.6182252168655396 \n",
      "     Training Step: 144 Training Loss: 0.6154090762138367 \n",
      "     Training Step: 145 Training Loss: 0.6123808026313782 \n",
      "     Training Step: 146 Training Loss: 0.6101939082145691 \n",
      "     Training Step: 147 Training Loss: 0.6180139780044556 \n",
      "     Training Step: 148 Training Loss: 0.6155288815498352 \n",
      "     Training Step: 149 Training Loss: 0.6166698932647705 \n",
      "     Training Step: 150 Training Loss: 0.6198586821556091 \n",
      "     Training Step: 151 Training Loss: 0.6100950837135315 \n",
      "     Training Step: 152 Training Loss: 0.6115787029266357 \n",
      "     Training Step: 153 Training Loss: 0.6188245415687561 \n",
      "     Training Step: 154 Training Loss: 0.6188514232635498 \n",
      "     Training Step: 155 Training Loss: 0.6146067380905151 \n",
      "     Training Step: 156 Training Loss: 0.6158280372619629 \n",
      "     Training Step: 157 Training Loss: 0.6182214617729187 \n",
      "     Training Step: 158 Training Loss: 0.6139367818832397 \n",
      "     Training Step: 159 Training Loss: 0.6176820397377014 \n",
      "     Training Step: 160 Training Loss: 0.6121877431869507 \n",
      "     Training Step: 161 Training Loss: 0.6142276525497437 \n",
      "     Training Step: 162 Training Loss: 0.609442949295044 \n",
      "     Training Step: 163 Training Loss: 0.6161991357803345 \n",
      "     Training Step: 164 Training Loss: 0.6139171123504639 \n",
      "     Training Step: 165 Training Loss: 0.612536609172821 \n",
      "     Training Step: 166 Training Loss: 0.6136481761932373 \n",
      "     Training Step: 167 Training Loss: 0.6172041296958923 \n",
      "     Training Step: 168 Training Loss: 0.6107324957847595 \n",
      "     Training Step: 169 Training Loss: 0.6127555966377258 \n",
      "     Training Step: 170 Training Loss: 0.6143845915794373 \n",
      "     Training Step: 171 Training Loss: 0.6137440800666809 \n",
      "     Training Step: 172 Training Loss: 0.6155537366867065 \n",
      "     Training Step: 173 Training Loss: 0.6134113073348999 \n",
      "     Training Step: 174 Training Loss: 0.61222904920578 \n",
      "     Training Step: 175 Training Loss: 0.6164998412132263 \n",
      "     Training Step: 176 Training Loss: 0.6162461042404175 \n",
      "     Training Step: 177 Training Loss: 0.6134963035583496 \n",
      "     Training Step: 178 Training Loss: 0.614929735660553 \n",
      "     Training Step: 179 Training Loss: 0.6156823635101318 \n",
      "     Training Step: 180 Training Loss: 0.6114122867584229 \n",
      "     Training Step: 181 Training Loss: 0.6144992113113403 \n",
      "     Training Step: 182 Training Loss: 0.6143471598625183 \n",
      "     Training Step: 183 Training Loss: 0.6180592775344849 \n",
      "     Training Step: 184 Training Loss: 0.6156722903251648 \n",
      "     Training Step: 185 Training Loss: 0.6146675944328308 \n",
      "     Training Step: 186 Training Loss: 0.6201978921890259 \n",
      "     Training Step: 187 Training Loss: 0.6118205189704895 \n",
      "     Training Step: 188 Training Loss: 0.6140794157981873 \n",
      "     Training Step: 189 Training Loss: 0.6115500926971436 \n",
      "     Training Step: 190 Training Loss: 0.6122550368309021 \n",
      "     Training Step: 191 Training Loss: 0.6135952472686768 \n",
      "     Training Step: 192 Training Loss: 0.6123006939888 \n",
      "     Training Step: 193 Training Loss: 0.6171414852142334 \n",
      "     Training Step: 194 Training Loss: 0.615153968334198 \n",
      "     Training Step: 195 Training Loss: 0.6135096549987793 \n",
      "     Training Step: 196 Training Loss: 0.6108938455581665 \n",
      "     Training Step: 197 Training Loss: 0.6209638714790344 \n",
      "     Training Step: 198 Training Loss: 0.6105653643608093 \n",
      "     Training Step: 199 Training Loss: 0.6108707785606384 \n",
      "     Training Step: 200 Training Loss: 0.6166357398033142 \n",
      "     Training Step: 201 Training Loss: 0.6128681898117065 \n",
      "     Training Step: 202 Training Loss: 0.6147093176841736 \n",
      "     Training Step: 203 Training Loss: 0.6138162016868591 \n",
      "     Training Step: 204 Training Loss: 0.6118735671043396 \n",
      "     Training Step: 205 Training Loss: 0.6146866679191589 \n",
      "     Training Step: 206 Training Loss: 0.6132027506828308 \n",
      "     Training Step: 207 Training Loss: 0.6149566769599915 \n",
      "     Training Step: 208 Training Loss: 0.6163555979728699 \n",
      "     Training Step: 209 Training Loss: 0.6153867244720459 \n",
      "     Training Step: 210 Training Loss: 0.6115257740020752 \n",
      "     Training Step: 211 Training Loss: 0.6116308569908142 \n",
      "     Training Step: 212 Training Loss: 0.6160325407981873 \n",
      "     Training Step: 213 Training Loss: 0.6153358817100525 \n",
      "     Training Step: 214 Training Loss: 0.6164143681526184 \n",
      "     Training Step: 215 Training Loss: 0.611808717250824 \n",
      "     Training Step: 216 Training Loss: 0.612525224685669 \n",
      "     Training Step: 217 Training Loss: 0.6154732704162598 \n",
      "     Training Step: 218 Training Loss: 0.6146355271339417 \n",
      "     Training Step: 219 Training Loss: 0.6104089617729187 \n",
      "     Training Step: 220 Training Loss: 0.6133022904396057 \n",
      "     Training Step: 221 Training Loss: 0.6171432733535767 \n",
      "     Training Step: 222 Training Loss: 0.6144393086433411 \n",
      "     Training Step: 223 Training Loss: 0.6123781204223633 \n",
      "     Training Step: 224 Training Loss: 0.61491858959198 \n",
      "     Training Step: 225 Training Loss: 0.6132045984268188 \n",
      "     Training Step: 226 Training Loss: 0.6133531332015991 \n",
      "     Training Step: 227 Training Loss: 0.6142112016677856 \n",
      "     Training Step: 228 Training Loss: 0.6134552359580994 \n",
      "     Training Step: 229 Training Loss: 0.6132875680923462 \n",
      "     Training Step: 230 Training Loss: 0.6129189729690552 \n",
      "     Training Step: 231 Training Loss: 0.6134706139564514 \n",
      "     Training Step: 232 Training Loss: 0.6178261041641235 \n",
      "     Training Step: 233 Training Loss: 0.6097232103347778 \n",
      "     Training Step: 234 Training Loss: 0.6106809973716736 \n",
      "     Training Step: 235 Training Loss: 0.6146525740623474 \n",
      "     Training Step: 236 Training Loss: 0.6131827235221863 \n",
      "     Training Step: 237 Training Loss: 0.6111323237419128 \n",
      "     Training Step: 238 Training Loss: 0.6168193817138672 \n",
      "     Training Step: 239 Training Loss: 0.6146906614303589 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.61732017993927 \n",
      "     Validation Step: 1 Validation Loss: 0.6185123920440674 \n",
      "     Validation Step: 2 Validation Loss: 0.6075028777122498 \n",
      "     Validation Step: 3 Validation Loss: 0.6115472912788391 \n",
      "     Validation Step: 4 Validation Loss: 0.6162489056587219 \n",
      "     Validation Step: 5 Validation Loss: 0.6145377159118652 \n",
      "     Validation Step: 6 Validation Loss: 0.611636221408844 \n",
      "     Validation Step: 7 Validation Loss: 0.613662600517273 \n",
      "     Validation Step: 8 Validation Loss: 0.6129785180091858 \n",
      "     Validation Step: 9 Validation Loss: 0.618076741695404 \n",
      "     Validation Step: 10 Validation Loss: 0.6111533045768738 \n",
      "     Validation Step: 11 Validation Loss: 0.610471785068512 \n",
      "     Validation Step: 12 Validation Loss: 0.6145766377449036 \n",
      "     Validation Step: 13 Validation Loss: 0.6141157150268555 \n",
      "     Validation Step: 14 Validation Loss: 0.6105037927627563 \n",
      "     Validation Step: 15 Validation Loss: 0.6170362830162048 \n",
      "     Validation Step: 16 Validation Loss: 0.6111718416213989 \n",
      "     Validation Step: 17 Validation Loss: 0.6149013042449951 \n",
      "     Validation Step: 18 Validation Loss: 0.6136384606361389 \n",
      "     Validation Step: 19 Validation Loss: 0.6142018437385559 \n",
      "     Validation Step: 20 Validation Loss: 0.6101233959197998 \n",
      "     Validation Step: 21 Validation Loss: 0.6184902191162109 \n",
      "     Validation Step: 22 Validation Loss: 0.6155790090560913 \n",
      "     Validation Step: 23 Validation Loss: 0.6176120638847351 \n",
      "     Validation Step: 24 Validation Loss: 0.6142578721046448 \n",
      "     Validation Step: 25 Validation Loss: 0.6148372292518616 \n",
      "     Validation Step: 26 Validation Loss: 0.6157972812652588 \n",
      "     Validation Step: 27 Validation Loss: 0.6153225302696228 \n",
      "     Validation Step: 28 Validation Loss: 0.6136490702629089 \n",
      "     Validation Step: 29 Validation Loss: 0.6100940108299255 \n",
      "     Validation Step: 30 Validation Loss: 0.6118741035461426 \n",
      "     Validation Step: 31 Validation Loss: 0.612826943397522 \n",
      "     Validation Step: 32 Validation Loss: 0.6160003542900085 \n",
      "     Validation Step: 33 Validation Loss: 0.6133058071136475 \n",
      "     Validation Step: 34 Validation Loss: 0.6183552145957947 \n",
      "     Validation Step: 35 Validation Loss: 0.6101541519165039 \n",
      "     Validation Step: 36 Validation Loss: 0.6121220588684082 \n",
      "     Validation Step: 37 Validation Loss: 0.6156187653541565 \n",
      "     Validation Step: 38 Validation Loss: 0.6182441115379333 \n",
      "     Validation Step: 39 Validation Loss: 0.6141248941421509 \n",
      "     Validation Step: 40 Validation Loss: 0.6150389909744263 \n",
      "     Validation Step: 41 Validation Loss: 0.61463862657547 \n",
      "     Validation Step: 42 Validation Loss: 0.6152318120002747 \n",
      "     Validation Step: 43 Validation Loss: 0.6106224656105042 \n",
      "     Validation Step: 44 Validation Loss: 0.617706298828125 \n",
      "Epoch: 104\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6125274300575256 \n",
      "     Training Step: 1 Training Loss: 0.6178109645843506 \n",
      "     Training Step: 2 Training Loss: 0.614608645439148 \n",
      "     Training Step: 3 Training Loss: 0.6153020858764648 \n",
      "     Training Step: 4 Training Loss: 0.6156817078590393 \n",
      "     Training Step: 5 Training Loss: 0.6097460389137268 \n",
      "     Training Step: 6 Training Loss: 0.6149309277534485 \n",
      "     Training Step: 7 Training Loss: 0.611210286617279 \n",
      "     Training Step: 8 Training Loss: 0.6149546504020691 \n",
      "     Training Step: 9 Training Loss: 0.615334153175354 \n",
      "     Training Step: 10 Training Loss: 0.6152660250663757 \n",
      "     Training Step: 11 Training Loss: 0.6114245057106018 \n",
      "     Training Step: 12 Training Loss: 0.6094110012054443 \n",
      "     Training Step: 13 Training Loss: 0.6117897629737854 \n",
      "     Training Step: 14 Training Loss: 0.6127495169639587 \n",
      "     Training Step: 15 Training Loss: 0.6162242889404297 \n",
      "     Training Step: 16 Training Loss: 0.6103866100311279 \n",
      "     Training Step: 17 Training Loss: 0.6132052540779114 \n",
      "     Training Step: 18 Training Loss: 0.6154422760009766 \n",
      "     Training Step: 19 Training Loss: 0.6111305356025696 \n",
      "     Training Step: 20 Training Loss: 0.6144446134567261 \n",
      "     Training Step: 21 Training Loss: 0.6171987056732178 \n",
      "     Training Step: 22 Training Loss: 0.6160539388656616 \n",
      "     Training Step: 23 Training Loss: 0.6177091598510742 \n",
      "     Training Step: 24 Training Loss: 0.6096938848495483 \n",
      "     Training Step: 25 Training Loss: 0.6148702502250671 \n",
      "     Training Step: 26 Training Loss: 0.6122900247573853 \n",
      "     Training Step: 27 Training Loss: 0.6139121651649475 \n",
      "     Training Step: 28 Training Loss: 0.6162437200546265 \n",
      "     Training Step: 29 Training Loss: 0.6144512891769409 \n",
      "     Training Step: 30 Training Loss: 0.6115306615829468 \n",
      "     Training Step: 31 Training Loss: 0.6183214783668518 \n",
      "     Training Step: 32 Training Loss: 0.6157569289207458 \n",
      "     Training Step: 33 Training Loss: 0.609242856502533 \n",
      "     Training Step: 34 Training Loss: 0.6097365021705627 \n",
      "     Training Step: 35 Training Loss: 0.612229585647583 \n",
      "     Training Step: 36 Training Loss: 0.6167085766792297 \n",
      "     Training Step: 37 Training Loss: 0.6129229068756104 \n",
      "     Training Step: 38 Training Loss: 0.6122271418571472 \n",
      "     Training Step: 39 Training Loss: 0.6154147982597351 \n",
      "     Training Step: 40 Training Loss: 0.6180306673049927 \n",
      "     Training Step: 41 Training Loss: 0.6118396520614624 \n",
      "     Training Step: 42 Training Loss: 0.6152824759483337 \n",
      "     Training Step: 43 Training Loss: 0.61143958568573 \n",
      "     Training Step: 44 Training Loss: 0.6150975823402405 \n",
      "     Training Step: 45 Training Loss: 0.6120172142982483 \n",
      "     Training Step: 46 Training Loss: 0.6188436150550842 \n",
      "     Training Step: 47 Training Loss: 0.6121227741241455 \n",
      "     Training Step: 48 Training Loss: 0.6155980825424194 \n",
      "     Training Step: 49 Training Loss: 0.6143427491188049 \n",
      "     Training Step: 50 Training Loss: 0.614602267742157 \n",
      "     Training Step: 51 Training Loss: 0.6166678071022034 \n",
      "     Training Step: 52 Training Loss: 0.6082914471626282 \n",
      "     Training Step: 53 Training Loss: 0.6139234304428101 \n",
      "     Training Step: 54 Training Loss: 0.6100662350654602 \n",
      "     Training Step: 55 Training Loss: 0.6141509413719177 \n",
      "     Training Step: 56 Training Loss: 0.6181056499481201 \n",
      "     Training Step: 57 Training Loss: 0.6202338933944702 \n",
      "     Training Step: 58 Training Loss: 0.6147542595863342 \n",
      "     Training Step: 59 Training Loss: 0.6126934289932251 \n",
      "     Training Step: 60 Training Loss: 0.6162027716636658 \n",
      "     Training Step: 61 Training Loss: 0.6147081255912781 \n",
      "     Training Step: 62 Training Loss: 0.6146138310432434 \n",
      "     Training Step: 63 Training Loss: 0.6198875308036804 \n",
      "     Training Step: 64 Training Loss: 0.611890435218811 \n",
      "     Training Step: 65 Training Loss: 0.6152812242507935 \n",
      "     Training Step: 66 Training Loss: 0.6116216778755188 \n",
      "     Training Step: 67 Training Loss: 0.613642156124115 \n",
      "     Training Step: 68 Training Loss: 0.6111709475517273 \n",
      "     Training Step: 69 Training Loss: 0.6100006699562073 \n",
      "     Training Step: 70 Training Loss: 0.6127632856369019 \n",
      "     Training Step: 71 Training Loss: 0.6100420951843262 \n",
      "     Training Step: 72 Training Loss: 0.6105746030807495 \n",
      "     Training Step: 73 Training Loss: 0.6181077361106873 \n",
      "     Training Step: 74 Training Loss: 0.6133137345314026 \n",
      "     Training Step: 75 Training Loss: 0.6155430674552917 \n",
      "     Training Step: 76 Training Loss: 0.6108832359313965 \n",
      "     Training Step: 77 Training Loss: 0.6163808703422546 \n",
      "     Training Step: 78 Training Loss: 0.6135023236274719 \n",
      "     Training Step: 79 Training Loss: 0.6167441606521606 \n",
      "     Training Step: 80 Training Loss: 0.6167374849319458 \n",
      "     Training Step: 81 Training Loss: 0.6165043115615845 \n",
      "     Training Step: 82 Training Loss: 0.6152390837669373 \n",
      "     Training Step: 83 Training Loss: 0.614212155342102 \n",
      "     Training Step: 84 Training Loss: 0.6114950180053711 \n",
      "     Training Step: 85 Training Loss: 0.6140382885932922 \n",
      "     Training Step: 86 Training Loss: 0.6167181134223938 \n",
      "     Training Step: 87 Training Loss: 0.6094920039176941 \n",
      "     Training Step: 88 Training Loss: 0.6105886101722717 \n",
      "     Training Step: 89 Training Loss: 0.6126486659049988 \n",
      "     Training Step: 90 Training Loss: 0.6151729822158813 \n",
      "     Training Step: 91 Training Loss: 0.61448073387146 \n",
      "     Training Step: 92 Training Loss: 0.6120654940605164 \n",
      "     Training Step: 93 Training Loss: 0.6104767322540283 \n",
      "     Training Step: 94 Training Loss: 0.6136181354522705 \n",
      "     Training Step: 95 Training Loss: 0.6135116815567017 \n",
      "     Training Step: 96 Training Loss: 0.6132075786590576 \n",
      "     Training Step: 97 Training Loss: 0.6182698011398315 \n",
      "     Training Step: 98 Training Loss: 0.6115400195121765 \n",
      "     Training Step: 99 Training Loss: 0.6131334900856018 \n",
      "     Training Step: 100 Training Loss: 0.614651083946228 \n",
      "     Training Step: 101 Training Loss: 0.6146984696388245 \n",
      "     Training Step: 102 Training Loss: 0.6138208508491516 \n",
      "     Training Step: 103 Training Loss: 0.611402153968811 \n",
      "     Training Step: 104 Training Loss: 0.6168292164802551 \n",
      "     Training Step: 105 Training Loss: 0.6133122444152832 \n",
      "     Training Step: 106 Training Loss: 0.6134843230247498 \n",
      "     Training Step: 107 Training Loss: 0.6142812967300415 \n",
      "     Training Step: 108 Training Loss: 0.6106929183006287 \n",
      "     Training Step: 109 Training Loss: 0.6142412424087524 \n",
      "     Training Step: 110 Training Loss: 0.6168935298919678 \n",
      "     Training Step: 111 Training Loss: 0.6121586561203003 \n",
      "     Training Step: 112 Training Loss: 0.6154736876487732 \n",
      "     Training Step: 113 Training Loss: 0.6130641102790833 \n",
      "     Training Step: 114 Training Loss: 0.6167998313903809 \n",
      "     Training Step: 115 Training Loss: 0.6158295273780823 \n",
      "     Training Step: 116 Training Loss: 0.619437038898468 \n",
      "     Training Step: 117 Training Loss: 0.6114116311073303 \n",
      "     Training Step: 118 Training Loss: 0.6129722595214844 \n",
      "     Training Step: 119 Training Loss: 0.6159973740577698 \n",
      "     Training Step: 120 Training Loss: 0.6147013306617737 \n",
      "     Training Step: 121 Training Loss: 0.6150846481323242 \n",
      "     Training Step: 122 Training Loss: 0.6141508221626282 \n",
      "     Training Step: 123 Training Loss: 0.6123833656311035 \n",
      "     Training Step: 124 Training Loss: 0.6107482314109802 \n",
      "     Training Step: 125 Training Loss: 0.6166496872901917 \n",
      "     Training Step: 126 Training Loss: 0.610506534576416 \n",
      "     Training Step: 127 Training Loss: 0.6185810565948486 \n",
      "     Training Step: 128 Training Loss: 0.617811918258667 \n",
      "     Training Step: 129 Training Loss: 0.6115310192108154 \n",
      "     Training Step: 130 Training Loss: 0.6146494746208191 \n",
      "     Training Step: 131 Training Loss: 0.6174878478050232 \n",
      "     Training Step: 132 Training Loss: 0.6123802065849304 \n",
      "     Training Step: 133 Training Loss: 0.6196950674057007 \n",
      "     Training Step: 134 Training Loss: 0.6143350601196289 \n",
      "     Training Step: 135 Training Loss: 0.6154022216796875 \n",
      "     Training Step: 136 Training Loss: 0.6124706864356995 \n",
      "     Training Step: 137 Training Loss: 0.6159401535987854 \n",
      "     Training Step: 138 Training Loss: 0.6104012727737427 \n",
      "     Training Step: 139 Training Loss: 0.6153725385665894 \n",
      "     Training Step: 140 Training Loss: 0.616797685623169 \n",
      "     Training Step: 141 Training Loss: 0.6157408952713013 \n",
      "     Training Step: 142 Training Loss: 0.6155269145965576 \n",
      "     Training Step: 143 Training Loss: 0.6137418746948242 \n",
      "     Training Step: 144 Training Loss: 0.610897958278656 \n",
      "     Training Step: 145 Training Loss: 0.610587477684021 \n",
      "     Training Step: 146 Training Loss: 0.6169213652610779 \n",
      "     Training Step: 147 Training Loss: 0.6177611947059631 \n",
      "     Training Step: 148 Training Loss: 0.6177060008049011 \n",
      "     Training Step: 149 Training Loss: 0.6142092347145081 \n",
      "     Training Step: 150 Training Loss: 0.6173768043518066 \n",
      "     Training Step: 151 Training Loss: 0.6124957203865051 \n",
      "     Training Step: 152 Training Loss: 0.6153691411018372 \n",
      "     Training Step: 153 Training Loss: 0.6141073107719421 \n",
      "     Training Step: 154 Training Loss: 0.6149156093597412 \n",
      "     Training Step: 155 Training Loss: 0.617084264755249 \n",
      "     Training Step: 156 Training Loss: 0.6164072155952454 \n",
      "     Training Step: 157 Training Loss: 0.6131347417831421 \n",
      "     Training Step: 158 Training Loss: 0.6121532917022705 \n",
      "     Training Step: 159 Training Loss: 0.6167443990707397 \n",
      "     Training Step: 160 Training Loss: 0.6166033744812012 \n",
      "     Training Step: 161 Training Loss: 0.6147306561470032 \n",
      "     Training Step: 162 Training Loss: 0.6161124110221863 \n",
      "     Training Step: 163 Training Loss: 0.6146765947341919 \n",
      "     Training Step: 164 Training Loss: 0.6114524006843567 \n",
      "     Training Step: 165 Training Loss: 0.610741376876831 \n",
      "     Training Step: 166 Training Loss: 0.613282322883606 \n",
      "     Training Step: 167 Training Loss: 0.6155363917350769 \n",
      "     Training Step: 168 Training Loss: 0.6164087653160095 \n",
      "     Training Step: 169 Training Loss: 0.6130669116973877 \n",
      "     Training Step: 170 Training Loss: 0.6184532642364502 \n",
      "     Training Step: 171 Training Loss: 0.6101341247558594 \n",
      "     Training Step: 172 Training Loss: 0.6132892966270447 \n",
      "     Training Step: 173 Training Loss: 0.6100597381591797 \n",
      "     Training Step: 174 Training Loss: 0.6135923266410828 \n",
      "     Training Step: 175 Training Loss: 0.6115157604217529 \n",
      "     Training Step: 176 Training Loss: 0.6186276078224182 \n",
      "     Training Step: 177 Training Loss: 0.6177108287811279 \n",
      "     Training Step: 178 Training Loss: 0.6182395815849304 \n",
      "     Training Step: 179 Training Loss: 0.6129117608070374 \n",
      "     Training Step: 180 Training Loss: 0.6144981384277344 \n",
      "     Training Step: 181 Training Loss: 0.6116475462913513 \n",
      "     Training Step: 182 Training Loss: 0.6156722903251648 \n",
      "     Training Step: 183 Training Loss: 0.6196024417877197 \n",
      "     Training Step: 184 Training Loss: 0.6140750050544739 \n",
      "     Training Step: 185 Training Loss: 0.6102234125137329 \n",
      "     Training Step: 186 Training Loss: 0.6144275069236755 \n",
      "     Training Step: 187 Training Loss: 0.6184432506561279 \n",
      "     Training Step: 188 Training Loss: 0.6133695244789124 \n",
      "     Training Step: 189 Training Loss: 0.6180180907249451 \n",
      "     Training Step: 190 Training Loss: 0.6121913194656372 \n",
      "     Training Step: 191 Training Loss: 0.6183838844299316 \n",
      "     Training Step: 192 Training Loss: 0.612305760383606 \n",
      "     Training Step: 193 Training Loss: 0.6136547923088074 \n",
      "     Training Step: 194 Training Loss: 0.6147737503051758 \n",
      "     Training Step: 195 Training Loss: 0.6118097901344299 \n",
      "     Training Step: 196 Training Loss: 0.6115971803665161 \n",
      "     Training Step: 197 Training Loss: 0.6151537299156189 \n",
      "     Training Step: 198 Training Loss: 0.6122361421585083 \n",
      "     Training Step: 199 Training Loss: 0.6166641712188721 \n",
      "     Training Step: 200 Training Loss: 0.6171540021896362 \n",
      "     Training Step: 201 Training Loss: 0.614383339881897 \n",
      "     Training Step: 202 Training Loss: 0.6128414869308472 \n",
      "     Training Step: 203 Training Loss: 0.614682674407959 \n",
      "     Training Step: 204 Training Loss: 0.6116891503334045 \n",
      "     Training Step: 205 Training Loss: 0.611826479434967 \n",
      "     Training Step: 206 Training Loss: 0.6140152812004089 \n",
      "     Training Step: 207 Training Loss: 0.6140228509902954 \n",
      "     Training Step: 208 Training Loss: 0.6116201281547546 \n",
      "     Training Step: 209 Training Loss: 0.6128632426261902 \n",
      "     Training Step: 210 Training Loss: 0.6157504916191101 \n",
      "     Training Step: 211 Training Loss: 0.6117777824401855 \n",
      "     Training Step: 212 Training Loss: 0.6134588122367859 \n",
      "     Training Step: 213 Training Loss: 0.6150714755058289 \n",
      "     Training Step: 214 Training Loss: 0.6146721243858337 \n",
      "     Training Step: 215 Training Loss: 0.6209527850151062 \n",
      "     Training Step: 216 Training Loss: 0.6127967834472656 \n",
      "     Training Step: 217 Training Loss: 0.6105996966362 \n",
      "     Training Step: 218 Training Loss: 0.6176301836967468 \n",
      "     Training Step: 219 Training Loss: 0.6125190258026123 \n",
      "     Training Step: 220 Training Loss: 0.6166677474975586 \n",
      "     Training Step: 221 Training Loss: 0.6188610196113586 \n",
      "     Training Step: 222 Training Loss: 0.6171849966049194 \n",
      "     Training Step: 223 Training Loss: 0.6116536855697632 \n",
      "     Training Step: 224 Training Loss: 0.6138886213302612 \n",
      "     Training Step: 225 Training Loss: 0.6125478744506836 \n",
      "     Training Step: 226 Training Loss: 0.6131987571716309 \n",
      "     Training Step: 227 Training Loss: 0.6132969856262207 \n",
      "     Training Step: 228 Training Loss: 0.6123892664909363 \n",
      "     Training Step: 229 Training Loss: 0.6143437027931213 \n",
      "     Training Step: 230 Training Loss: 0.6137396693229675 \n",
      "     Training Step: 231 Training Loss: 0.6171492338180542 \n",
      "     Training Step: 232 Training Loss: 0.6106581687927246 \n",
      "     Training Step: 233 Training Loss: 0.6157877445220947 \n",
      "     Training Step: 234 Training Loss: 0.6147971749305725 \n",
      "     Training Step: 235 Training Loss: 0.6118329167366028 \n",
      "     Training Step: 236 Training Loss: 0.6128677129745483 \n",
      "     Training Step: 237 Training Loss: 0.6137663125991821 \n",
      "     Training Step: 238 Training Loss: 0.6134127378463745 \n",
      "     Training Step: 239 Training Loss: 0.6154522895812988 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6142002940177917 \n",
      "     Validation Step: 1 Validation Loss: 0.6160100102424622 \n",
      "     Validation Step: 2 Validation Loss: 0.612119197845459 \n",
      "     Validation Step: 3 Validation Loss: 0.617607593536377 \n",
      "     Validation Step: 4 Validation Loss: 0.6111699938774109 \n",
      "     Validation Step: 5 Validation Loss: 0.6155844330787659 \n",
      "     Validation Step: 6 Validation Loss: 0.6182509064674377 \n",
      "     Validation Step: 7 Validation Loss: 0.6184873580932617 \n",
      "     Validation Step: 8 Validation Loss: 0.6105005145072937 \n",
      "     Validation Step: 9 Validation Loss: 0.6136653423309326 \n",
      "     Validation Step: 10 Validation Loss: 0.6148340106010437 \n",
      "     Validation Step: 11 Validation Loss: 0.6152327060699463 \n",
      "     Validation Step: 12 Validation Loss: 0.6146358847618103 \n",
      "     Validation Step: 13 Validation Loss: 0.6145750880241394 \n",
      "     Validation Step: 14 Validation Loss: 0.6101212501525879 \n",
      "     Validation Step: 15 Validation Loss: 0.6177108883857727 \n",
      "     Validation Step: 16 Validation Loss: 0.616253137588501 \n",
      "     Validation Step: 17 Validation Loss: 0.615328311920166 \n",
      "     Validation Step: 18 Validation Loss: 0.6116305589675903 \n",
      "     Validation Step: 19 Validation Loss: 0.6106191277503967 \n",
      "     Validation Step: 20 Validation Loss: 0.60749351978302 \n",
      "     Validation Step: 21 Validation Loss: 0.6180813312530518 \n",
      "     Validation Step: 22 Validation Loss: 0.6128169298171997 \n",
      "     Validation Step: 23 Validation Loss: 0.6170412302017212 \n",
      "     Validation Step: 24 Validation Loss: 0.6173210144042969 \n",
      "     Validation Step: 25 Validation Loss: 0.6141247153282166 \n",
      "     Validation Step: 26 Validation Loss: 0.6183629631996155 \n",
      "     Validation Step: 27 Validation Loss: 0.6118688583374023 \n",
      "     Validation Step: 28 Validation Loss: 0.6115447282791138 \n",
      "     Validation Step: 29 Validation Loss: 0.6104705929756165 \n",
      "     Validation Step: 30 Validation Loss: 0.6142527461051941 \n",
      "     Validation Step: 31 Validation Loss: 0.6100935935974121 \n",
      "     Validation Step: 32 Validation Loss: 0.6157966256141663 \n",
      "     Validation Step: 33 Validation Loss: 0.6132981181144714 \n",
      "     Validation Step: 34 Validation Loss: 0.6156226992607117 \n",
      "     Validation Step: 35 Validation Loss: 0.6136274933815002 \n",
      "     Validation Step: 36 Validation Loss: 0.6141119003295898 \n",
      "     Validation Step: 37 Validation Loss: 0.6150467395782471 \n",
      "     Validation Step: 38 Validation Loss: 0.6185147762298584 \n",
      "     Validation Step: 39 Validation Loss: 0.6136587858200073 \n",
      "     Validation Step: 40 Validation Loss: 0.6145337224006653 \n",
      "     Validation Step: 41 Validation Loss: 0.6129770874977112 \n",
      "     Validation Step: 42 Validation Loss: 0.6101455092430115 \n",
      "     Validation Step: 43 Validation Loss: 0.6111522316932678 \n",
      "     Validation Step: 44 Validation Loss: 0.6148973107337952 \n",
      "Epoch: 105\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6097221374511719 \n",
      "     Training Step: 1 Training Loss: 0.6093830466270447 \n",
      "     Training Step: 2 Training Loss: 0.6129611730575562 \n",
      "     Training Step: 3 Training Loss: 0.6166747212409973 \n",
      "     Training Step: 4 Training Loss: 0.618252158164978 \n",
      "     Training Step: 5 Training Loss: 0.6107099652290344 \n",
      "     Training Step: 6 Training Loss: 0.6118206977844238 \n",
      "     Training Step: 7 Training Loss: 0.6146151423454285 \n",
      "     Training Step: 8 Training Loss: 0.6152899265289307 \n",
      "     Training Step: 9 Training Loss: 0.6128689050674438 \n",
      "     Training Step: 10 Training Loss: 0.616215705871582 \n",
      "     Training Step: 11 Training Loss: 0.6149552464485168 \n",
      "     Training Step: 12 Training Loss: 0.616672158241272 \n",
      "     Training Step: 13 Training Loss: 0.6140156388282776 \n",
      "     Training Step: 14 Training Loss: 0.6147500276565552 \n",
      "     Training Step: 15 Training Loss: 0.6174830198287964 \n",
      "     Training Step: 16 Training Loss: 0.6094862222671509 \n",
      "     Training Step: 17 Training Loss: 0.617185652256012 \n",
      "     Training Step: 18 Training Loss: 0.6180598139762878 \n",
      "     Training Step: 19 Training Loss: 0.6155938506126404 \n",
      "     Training Step: 20 Training Loss: 0.6135243773460388 \n",
      "     Training Step: 21 Training Loss: 0.6104046106338501 \n",
      "     Training Step: 22 Training Loss: 0.61020427942276 \n",
      "     Training Step: 23 Training Loss: 0.6157789826393127 \n",
      "     Training Step: 24 Training Loss: 0.6202148199081421 \n",
      "     Training Step: 25 Training Loss: 0.6127035021781921 \n",
      "     Training Step: 26 Training Loss: 0.6104868054389954 \n",
      "     Training Step: 27 Training Loss: 0.616245687007904 \n",
      "     Training Step: 28 Training Loss: 0.6185895204544067 \n",
      "     Training Step: 29 Training Loss: 0.6106579899787903 \n",
      "     Training Step: 30 Training Loss: 0.6136376261711121 \n",
      "     Training Step: 31 Training Loss: 0.6116397976875305 \n",
      "     Training Step: 32 Training Loss: 0.6160060167312622 \n",
      "     Training Step: 33 Training Loss: 0.6104913353919983 \n",
      "     Training Step: 34 Training Loss: 0.616500735282898 \n",
      "     Training Step: 35 Training Loss: 0.6147069931030273 \n",
      "     Training Step: 36 Training Loss: 0.6181116700172424 \n",
      "     Training Step: 37 Training Loss: 0.6159501075744629 \n",
      "     Training Step: 38 Training Loss: 0.6114001870155334 \n",
      "     Training Step: 39 Training Loss: 0.6097361445426941 \n",
      "     Training Step: 40 Training Loss: 0.6144394874572754 \n",
      "     Training Step: 41 Training Loss: 0.613741934299469 \n",
      "     Training Step: 42 Training Loss: 0.6196241974830627 \n",
      "     Training Step: 43 Training Loss: 0.6115327477455139 \n",
      "     Training Step: 44 Training Loss: 0.6115331649780273 \n",
      "     Training Step: 45 Training Loss: 0.6166192889213562 \n",
      "     Training Step: 46 Training Loss: 0.6117904782295227 \n",
      "     Training Step: 47 Training Loss: 0.613471269607544 \n",
      "     Training Step: 48 Training Loss: 0.6176882386207581 \n",
      "     Training Step: 49 Training Loss: 0.6150646209716797 \n",
      "     Training Step: 50 Training Loss: 0.6134967803955078 \n",
      "     Training Step: 51 Training Loss: 0.6140381097793579 \n",
      "     Training Step: 52 Training Loss: 0.6161177754402161 \n",
      "     Training Step: 53 Training Loss: 0.6146746277809143 \n",
      "     Training Step: 54 Training Loss: 0.6142058372497559 \n",
      "     Training Step: 55 Training Loss: 0.6140697598457336 \n",
      "     Training Step: 56 Training Loss: 0.6158270835876465 \n",
      "     Training Step: 57 Training Loss: 0.6177902817726135 \n",
      "     Training Step: 58 Training Loss: 0.6100080609321594 \n",
      "     Training Step: 59 Training Loss: 0.6101527214050293 \n",
      "     Training Step: 60 Training Loss: 0.6154422163963318 \n",
      "     Training Step: 61 Training Loss: 0.6134265065193176 \n",
      "     Training Step: 62 Training Loss: 0.6180177330970764 \n",
      "     Training Step: 63 Training Loss: 0.6150999665260315 \n",
      "     Training Step: 64 Training Loss: 0.6147018671035767 \n",
      "     Training Step: 65 Training Loss: 0.6115968227386475 \n",
      "     Training Step: 66 Training Loss: 0.6122869253158569 \n",
      "     Training Step: 67 Training Loss: 0.6169271469116211 \n",
      "     Training Step: 68 Training Loss: 0.6111658811569214 \n",
      "     Training Step: 69 Training Loss: 0.6104065179824829 \n",
      "     Training Step: 70 Training Loss: 0.6164228916168213 \n",
      "     Training Step: 71 Training Loss: 0.6100579500198364 \n",
      "     Training Step: 72 Training Loss: 0.6127519011497498 \n",
      "     Training Step: 73 Training Loss: 0.6167258024215698 \n",
      "     Training Step: 74 Training Loss: 0.6130561828613281 \n",
      "     Training Step: 75 Training Loss: 0.6153090000152588 \n",
      "     Training Step: 76 Training Loss: 0.6136122941970825 \n",
      "     Training Step: 77 Training Loss: 0.6128385663032532 \n",
      "     Training Step: 78 Training Loss: 0.6150884628295898 \n",
      "     Training Step: 79 Training Loss: 0.6162080764770508 \n",
      "     Training Step: 80 Training Loss: 0.6130678653717041 \n",
      "     Training Step: 81 Training Loss: 0.615684449672699 \n",
      "     Training Step: 82 Training Loss: 0.6116310358047485 \n",
      "     Training Step: 83 Training Loss: 0.6153834462165833 \n",
      "     Training Step: 84 Training Loss: 0.6176325678825378 \n",
      "     Training Step: 85 Training Loss: 0.6129112243652344 \n",
      "     Training Step: 86 Training Loss: 0.6143396496772766 \n",
      "     Training Step: 87 Training Loss: 0.6167976260185242 \n",
      "     Training Step: 88 Training Loss: 0.617678701877594 \n",
      "     Training Step: 89 Training Loss: 0.6185775995254517 \n",
      "     Training Step: 90 Training Loss: 0.6109521985054016 \n",
      "     Training Step: 91 Training Loss: 0.6121472120285034 \n",
      "     Training Step: 92 Training Loss: 0.6121599078178406 \n",
      "     Training Step: 93 Training Loss: 0.6126595735549927 \n",
      "     Training Step: 94 Training Loss: 0.6157419085502625 \n",
      "     Training Step: 95 Training Loss: 0.6133039593696594 \n",
      "     Training Step: 96 Training Loss: 0.6144232153892517 \n",
      "     Training Step: 97 Training Loss: 0.6132956743240356 \n",
      "     Training Step: 98 Training Loss: 0.6100740432739258 \n",
      "     Training Step: 99 Training Loss: 0.6143724918365479 \n",
      "     Training Step: 100 Training Loss: 0.6118015050888062 \n",
      "     Training Step: 101 Training Loss: 0.6123718023300171 \n",
      "     Training Step: 102 Training Loss: 0.6184730529785156 \n",
      "     Training Step: 103 Training Loss: 0.6188702583312988 \n",
      "     Training Step: 104 Training Loss: 0.6082472801208496 \n",
      "     Training Step: 105 Training Loss: 0.6116044521331787 \n",
      "     Training Step: 106 Training Loss: 0.6105696558952332 \n",
      "     Training Step: 107 Training Loss: 0.6184362173080444 \n",
      "     Training Step: 108 Training Loss: 0.6125309467315674 \n",
      "     Training Step: 109 Training Loss: 0.616905927658081 \n",
      "     Training Step: 110 Training Loss: 0.6120637059211731 \n",
      "     Training Step: 111 Training Loss: 0.6135928630828857 \n",
      "     Training Step: 112 Training Loss: 0.6146910190582275 \n",
      "     Training Step: 113 Training Loss: 0.6132012605667114 \n",
      "     Training Step: 114 Training Loss: 0.6114097833633423 \n",
      "     Training Step: 115 Training Loss: 0.6144511103630066 \n",
      "     Training Step: 116 Training Loss: 0.6141508221626282 \n",
      "     Training Step: 117 Training Loss: 0.6132786273956299 \n",
      "     Training Step: 118 Training Loss: 0.615757942199707 \n",
      "     Training Step: 119 Training Loss: 0.6149179935455322 \n",
      "     Training Step: 120 Training Loss: 0.6144785284996033 \n",
      "     Training Step: 121 Training Loss: 0.6153709888458252 \n",
      "     Training Step: 122 Training Loss: 0.6108801960945129 \n",
      "     Training Step: 123 Training Loss: 0.6153340935707092 \n",
      "     Training Step: 124 Training Loss: 0.6156876087188721 \n",
      "     Training Step: 125 Training Loss: 0.6178112030029297 \n",
      "     Training Step: 126 Training Loss: 0.6147241592407227 \n",
      "     Training Step: 127 Training Loss: 0.6143410205841064 \n",
      "     Training Step: 128 Training Loss: 0.6146615147590637 \n",
      "     Training Step: 129 Training Loss: 0.6146365404129028 \n",
      "     Training Step: 130 Training Loss: 0.6155305504798889 \n",
      "     Training Step: 131 Training Loss: 0.6147951483726501 \n",
      "     Training Step: 132 Training Loss: 0.6152811646461487 \n",
      "     Training Step: 133 Training Loss: 0.6194226145744324 \n",
      "     Training Step: 134 Training Loss: 0.6118162870407104 \n",
      "     Training Step: 135 Training Loss: 0.6163445115089417 \n",
      "     Training Step: 136 Training Loss: 0.6143444776535034 \n",
      "     Training Step: 137 Training Loss: 0.6142803430557251 \n",
      "     Training Step: 138 Training Loss: 0.6145979762077332 \n",
      "     Training Step: 139 Training Loss: 0.6171287298202515 \n",
      "     Training Step: 140 Training Loss: 0.6177443861961365 \n",
      "     Training Step: 141 Training Loss: 0.6115394234657288 \n",
      "     Training Step: 142 Training Loss: 0.6124998331069946 \n",
      "     Training Step: 143 Training Loss: 0.6196868419647217 \n",
      "     Training Step: 144 Training Loss: 0.6176982522010803 \n",
      "     Training Step: 145 Training Loss: 0.6149329543113708 \n",
      "     Training Step: 146 Training Loss: 0.6146082282066345 \n",
      "     Training Step: 147 Training Loss: 0.6114596128463745 \n",
      "     Training Step: 148 Training Loss: 0.6107104420661926 \n",
      "     Training Step: 149 Training Loss: 0.6114319562911987 \n",
      "     Training Step: 150 Training Loss: 0.611831545829773 \n",
      "     Training Step: 151 Training Loss: 0.6154120564460754 \n",
      "     Training Step: 152 Training Loss: 0.6131167411804199 \n",
      "     Training Step: 153 Training Loss: 0.616050660610199 \n",
      "     Training Step: 154 Training Loss: 0.6147785186767578 \n",
      "     Training Step: 155 Training Loss: 0.6164217591285706 \n",
      "     Training Step: 156 Training Loss: 0.6133098006248474 \n",
      "     Training Step: 157 Training Loss: 0.6182535886764526 \n",
      "     Training Step: 158 Training Loss: 0.6118326783180237 \n",
      "     Training Step: 159 Training Loss: 0.613919198513031 \n",
      "     Training Step: 160 Training Loss: 0.6123771071434021 \n",
      "     Training Step: 161 Training Loss: 0.6141060590744019 \n",
      "     Training Step: 162 Training Loss: 0.6137405037879944 \n",
      "     Training Step: 163 Training Loss: 0.6118826866149902 \n",
      "     Training Step: 164 Training Loss: 0.6153883934020996 \n",
      "     Training Step: 165 Training Loss: 0.6198996305465698 \n",
      "     Training Step: 166 Training Loss: 0.6154241561889648 \n",
      "     Training Step: 167 Training Loss: 0.6124705672264099 \n",
      "     Training Step: 168 Training Loss: 0.6125150322914124 \n",
      "     Training Step: 169 Training Loss: 0.6154746413230896 \n",
      "     Training Step: 170 Training Loss: 0.6134597063064575 \n",
      "     Training Step: 171 Training Loss: 0.6100519895553589 \n",
      "     Training Step: 172 Training Loss: 0.6142401695251465 \n",
      "     Training Step: 173 Training Loss: 0.6132040023803711 \n",
      "     Training Step: 174 Training Loss: 0.6121593117713928 \n",
      "     Training Step: 175 Training Loss: 0.6131827235221863 \n",
      "     Training Step: 176 Training Loss: 0.6167804002761841 \n",
      "     Training Step: 177 Training Loss: 0.614656388759613 \n",
      "     Training Step: 178 Training Loss: 0.6171110272407532 \n",
      "     Training Step: 179 Training Loss: 0.6142078638076782 \n",
      "     Training Step: 180 Training Loss: 0.6092215180397034 \n",
      "     Training Step: 181 Training Loss: 0.6152692437171936 \n",
      "     Training Step: 182 Training Loss: 0.6168118119239807 \n",
      "     Training Step: 183 Training Loss: 0.6114742159843445 \n",
      "     Training Step: 184 Training Loss: 0.6132772564888 \n",
      "     Training Step: 185 Training Loss: 0.6105676889419556 \n",
      "     Training Step: 186 Training Loss: 0.6105890870094299 \n",
      "     Training Step: 187 Training Loss: 0.6115481853485107 \n",
      "     Training Step: 188 Training Loss: 0.6167351007461548 \n",
      "     Training Step: 189 Training Loss: 0.6180636286735535 \n",
      "     Training Step: 190 Training Loss: 0.6168181896209717 \n",
      "     Training Step: 191 Training Loss: 0.6111928224563599 \n",
      "     Training Step: 192 Training Loss: 0.6123697757720947 \n",
      "     Training Step: 193 Training Loss: 0.6129169464111328 \n",
      "     Training Step: 194 Training Loss: 0.6122322082519531 \n",
      "     Training Step: 195 Training Loss: 0.6111382842063904 \n",
      "     Training Step: 196 Training Loss: 0.6171723008155823 \n",
      "     Training Step: 197 Training Loss: 0.6116869449615479 \n",
      "     Training Step: 198 Training Loss: 0.6137650012969971 \n",
      "     Training Step: 199 Training Loss: 0.6122345924377441 \n",
      "     Training Step: 200 Training Loss: 0.61466383934021 \n",
      "     Training Step: 201 Training Loss: 0.612762451171875 \n",
      "     Training Step: 202 Training Loss: 0.6114283204078674 \n",
      "     Training Step: 203 Training Loss: 0.6167188286781311 \n",
      "     Training Step: 204 Training Loss: 0.6122269630432129 \n",
      "     Training Step: 205 Training Loss: 0.6136496663093567 \n",
      "     Training Step: 206 Training Loss: 0.6151781678199768 \n",
      "     Training Step: 207 Training Loss: 0.6125290989875793 \n",
      "     Training Step: 208 Training Loss: 0.6138181090354919 \n",
      "     Training Step: 209 Training Loss: 0.6140222549438477 \n",
      "     Training Step: 210 Training Loss: 0.6152431964874268 \n",
      "     Training Step: 211 Training Loss: 0.617146909236908 \n",
      "     Training Step: 212 Training Loss: 0.616654634475708 \n",
      "     Training Step: 213 Training Loss: 0.618459165096283 \n",
      "     Training Step: 214 Training Loss: 0.6120256781578064 \n",
      "     Training Step: 215 Training Loss: 0.6133633255958557 \n",
      "     Training Step: 216 Training Loss: 0.6107580661773682 \n",
      "     Training Step: 217 Training Loss: 0.6097370982170105 \n",
      "     Training Step: 218 Training Loss: 0.6144970655441284 \n",
      "     Training Step: 219 Training Loss: 0.6155297756195068 \n",
      "     Training Step: 220 Training Loss: 0.6151542663574219 \n",
      "     Training Step: 221 Training Loss: 0.6166735291481018 \n",
      "     Training Step: 222 Training Loss: 0.6122960448265076 \n",
      "     Training Step: 223 Training Loss: 0.6105734705924988 \n",
      "     Training Step: 224 Training Loss: 0.612154483795166 \n",
      "     Training Step: 225 Training Loss: 0.6183345317840576 \n",
      "     Training Step: 226 Training Loss: 0.6131384968757629 \n",
      "     Training Step: 227 Training Loss: 0.6116189956665039 \n",
      "     Training Step: 228 Training Loss: 0.6141549348831177 \n",
      "     Training Step: 229 Training Loss: 0.6157486438751221 \n",
      "     Training Step: 230 Training Loss: 0.6148706674575806 \n",
      "     Training Step: 231 Training Loss: 0.616711437702179 \n",
      "     Training Step: 232 Training Loss: 0.6127982139587402 \n",
      "     Training Step: 233 Training Loss: 0.6209343075752258 \n",
      "     Training Step: 234 Training Loss: 0.6138854026794434 \n",
      "     Training Step: 235 Training Loss: 0.6128693222999573 \n",
      "     Training Step: 236 Training Loss: 0.6155111789703369 \n",
      "     Training Step: 237 Training Loss: 0.6188510656356812 \n",
      "     Training Step: 238 Training Loss: 0.6139328479766846 \n",
      "     Training Step: 239 Training Loss: 0.6173599362373352 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6145508885383606 \n",
      "     Validation Step: 1 Validation Loss: 0.6157845854759216 \n",
      "     Validation Step: 2 Validation Loss: 0.6102088689804077 \n",
      "     Validation Step: 3 Validation Loss: 0.6106702089309692 \n",
      "     Validation Step: 4 Validation Loss: 0.6121612787246704 \n",
      "     Validation Step: 5 Validation Loss: 0.615225076675415 \n",
      "     Validation Step: 6 Validation Loss: 0.615034818649292 \n",
      "     Validation Step: 7 Validation Loss: 0.6112080216407776 \n",
      "     Validation Step: 8 Validation Loss: 0.6128636002540588 \n",
      "     Validation Step: 9 Validation Loss: 0.6145706176757812 \n",
      "     Validation Step: 10 Validation Loss: 0.6119153499603271 \n",
      "     Validation Step: 11 Validation Loss: 0.6075973510742188 \n",
      "     Validation Step: 12 Validation Loss: 0.611591100692749 \n",
      "     Validation Step: 13 Validation Loss: 0.6183156967163086 \n",
      "     Validation Step: 14 Validation Loss: 0.6133314371109009 \n",
      "     Validation Step: 15 Validation Loss: 0.6182016134262085 \n",
      "     Validation Step: 16 Validation Loss: 0.6141330003738403 \n",
      "     Validation Step: 17 Validation Loss: 0.6159765720367432 \n",
      "     Validation Step: 18 Validation Loss: 0.6184511780738831 \n",
      "     Validation Step: 19 Validation Loss: 0.6142731308937073 \n",
      "     Validation Step: 20 Validation Loss: 0.6153162717819214 \n",
      "     Validation Step: 21 Validation Loss: 0.6184647679328918 \n",
      "     Validation Step: 22 Validation Loss: 0.6101900935173035 \n",
      "     Validation Step: 23 Validation Loss: 0.6156063079833984 \n",
      "     Validation Step: 24 Validation Loss: 0.613651692867279 \n",
      "     Validation Step: 25 Validation Loss: 0.6146494746208191 \n",
      "     Validation Step: 26 Validation Loss: 0.610160231590271 \n",
      "     Validation Step: 27 Validation Loss: 0.6172873973846436 \n",
      "     Validation Step: 28 Validation Loss: 0.614845871925354 \n",
      "     Validation Step: 29 Validation Loss: 0.6136787533760071 \n",
      "     Validation Step: 30 Validation Loss: 0.613010585308075 \n",
      "     Validation Step: 31 Validation Loss: 0.6180384159088135 \n",
      "     Validation Step: 32 Validation Loss: 0.616237461566925 \n",
      "     Validation Step: 33 Validation Loss: 0.6136590838432312 \n",
      "     Validation Step: 34 Validation Loss: 0.6170012950897217 \n",
      "     Validation Step: 35 Validation Loss: 0.6149043440818787 \n",
      "     Validation Step: 36 Validation Loss: 0.6105232834815979 \n",
      "     Validation Step: 37 Validation Loss: 0.6176710724830627 \n",
      "     Validation Step: 38 Validation Loss: 0.6142163872718811 \n",
      "     Validation Step: 39 Validation Loss: 0.6116749048233032 \n",
      "     Validation Step: 40 Validation Loss: 0.6111962795257568 \n",
      "     Validation Step: 41 Validation Loss: 0.6141462922096252 \n",
      "     Validation Step: 42 Validation Loss: 0.6155627965927124 \n",
      "     Validation Step: 43 Validation Loss: 0.6175873279571533 \n",
      "     Validation Step: 44 Validation Loss: 0.6105563044548035 \n",
      "Epoch: 106\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6194162964820862 \n",
      "     Training Step: 1 Training Loss: 0.6122023463249207 \n",
      "     Training Step: 2 Training Loss: 0.6092944145202637 \n",
      "     Training Step: 3 Training Loss: 0.6097822785377502 \n",
      "     Training Step: 4 Training Loss: 0.611637532711029 \n",
      "     Training Step: 5 Training Loss: 0.6140175461769104 \n",
      "     Training Step: 6 Training Loss: 0.614337682723999 \n",
      "     Training Step: 7 Training Loss: 0.6117811799049377 \n",
      "     Training Step: 8 Training Loss: 0.6130686402320862 \n",
      "     Training Step: 9 Training Loss: 0.617185652256012 \n",
      "     Training Step: 10 Training Loss: 0.6166821122169495 \n",
      "     Training Step: 11 Training Loss: 0.6136432886123657 \n",
      "     Training Step: 12 Training Loss: 0.614081621170044 \n",
      "     Training Step: 13 Training Loss: 0.6118327975273132 \n",
      "     Training Step: 14 Training Loss: 0.6114288568496704 \n",
      "     Training Step: 15 Training Loss: 0.6124593615531921 \n",
      "     Training Step: 16 Training Loss: 0.617657482624054 \n",
      "     Training Step: 17 Training Loss: 0.6168150901794434 \n",
      "     Training Step: 18 Training Loss: 0.6152676939964294 \n",
      "     Training Step: 19 Training Loss: 0.6173821687698364 \n",
      "     Training Step: 20 Training Loss: 0.6114270687103271 \n",
      "     Training Step: 21 Training Loss: 0.6180089712142944 \n",
      "     Training Step: 22 Training Loss: 0.6183868646621704 \n",
      "     Training Step: 23 Training Loss: 0.6149206161499023 \n",
      "     Training Step: 24 Training Loss: 0.6137775182723999 \n",
      "     Training Step: 25 Training Loss: 0.6097791790962219 \n",
      "     Training Step: 26 Training Loss: 0.6140583753585815 \n",
      "     Training Step: 27 Training Loss: 0.6114720106124878 \n",
      "     Training Step: 28 Training Loss: 0.611849844455719 \n",
      "     Training Step: 29 Training Loss: 0.6139261722564697 \n",
      "     Training Step: 30 Training Loss: 0.6125248670578003 \n",
      "     Training Step: 31 Training Loss: 0.6122357845306396 \n",
      "     Training Step: 32 Training Loss: 0.6114028692245483 \n",
      "     Training Step: 33 Training Loss: 0.6167004108428955 \n",
      "     Training Step: 34 Training Loss: 0.6154125928878784 \n",
      "     Training Step: 35 Training Loss: 0.6132120490074158 \n",
      "     Training Step: 36 Training Loss: 0.6153935790061951 \n",
      "     Training Step: 37 Training Loss: 0.6164606809616089 \n",
      "     Training Step: 38 Training Loss: 0.6143603920936584 \n",
      "     Training Step: 39 Training Loss: 0.6160496473312378 \n",
      "     Training Step: 40 Training Loss: 0.6106548309326172 \n",
      "     Training Step: 41 Training Loss: 0.6185978651046753 \n",
      "     Training Step: 42 Training Loss: 0.6094629168510437 \n",
      "     Training Step: 43 Training Loss: 0.6150891780853271 \n",
      "     Training Step: 44 Training Loss: 0.6176889538764954 \n",
      "     Training Step: 45 Training Loss: 0.616711437702179 \n",
      "     Training Step: 46 Training Loss: 0.6082963943481445 \n",
      "     Training Step: 47 Training Loss: 0.6116542220115662 \n",
      "     Training Step: 48 Training Loss: 0.6104192733764648 \n",
      "     Training Step: 49 Training Loss: 0.6114055514335632 \n",
      "     Training Step: 50 Training Loss: 0.6153831481933594 \n",
      "     Training Step: 51 Training Loss: 0.6160053014755249 \n",
      "     Training Step: 52 Training Loss: 0.6143450140953064 \n",
      "     Training Step: 53 Training Loss: 0.6132028102874756 \n",
      "     Training Step: 54 Training Loss: 0.6115207076072693 \n",
      "     Training Step: 55 Training Loss: 0.6121343970298767 \n",
      "     Training Step: 56 Training Loss: 0.6141560673713684 \n",
      "     Training Step: 57 Training Loss: 0.6114670634269714 \n",
      "     Training Step: 58 Training Loss: 0.6155420541763306 \n",
      "     Training Step: 59 Training Loss: 0.6100440621376038 \n",
      "     Training Step: 60 Training Loss: 0.6157974004745483 \n",
      "     Training Step: 61 Training Loss: 0.6156904697418213 \n",
      "     Training Step: 62 Training Loss: 0.6120618581771851 \n",
      "     Training Step: 63 Training Loss: 0.6139209866523743 \n",
      "     Training Step: 64 Training Loss: 0.61415034532547 \n",
      "     Training Step: 65 Training Loss: 0.615289568901062 \n",
      "     Training Step: 66 Training Loss: 0.6096934080123901 \n",
      "     Training Step: 67 Training Loss: 0.6121207475662231 \n",
      "     Training Step: 68 Training Loss: 0.6169033050537109 \n",
      "     Training Step: 69 Training Loss: 0.6149576902389526 \n",
      "     Training Step: 70 Training Loss: 0.6152937412261963 \n",
      "     Training Step: 71 Training Loss: 0.6128690838813782 \n",
      "     Training Step: 72 Training Loss: 0.6116912364959717 \n",
      "     Training Step: 73 Training Loss: 0.6157453060150146 \n",
      "     Training Step: 74 Training Loss: 0.610594630241394 \n",
      "     Training Step: 75 Training Loss: 0.6131831407546997 \n",
      "     Training Step: 76 Training Loss: 0.614450991153717 \n",
      "     Training Step: 77 Training Loss: 0.6158376336097717 \n",
      "     Training Step: 78 Training Loss: 0.6129189729690552 \n",
      "     Training Step: 79 Training Loss: 0.6101727485656738 \n",
      "     Training Step: 80 Training Loss: 0.6100562810897827 \n",
      "     Training Step: 81 Training Loss: 0.6171130537986755 \n",
      "     Training Step: 82 Training Loss: 0.6162546277046204 \n",
      "     Training Step: 83 Training Loss: 0.6136164665222168 \n",
      "     Training Step: 84 Training Loss: 0.6180573105812073 \n",
      "     Training Step: 85 Training Loss: 0.6134706139564514 \n",
      "     Training Step: 86 Training Loss: 0.6115261316299438 \n",
      "     Training Step: 87 Training Loss: 0.6136478185653687 \n",
      "     Training Step: 88 Training Loss: 0.6118795871734619 \n",
      "     Training Step: 89 Training Loss: 0.610568642616272 \n",
      "     Training Step: 90 Training Loss: 0.6146897077560425 \n",
      "     Training Step: 91 Training Loss: 0.6164100766181946 \n",
      "     Training Step: 92 Training Loss: 0.6149313449859619 \n",
      "     Training Step: 93 Training Loss: 0.6146082282066345 \n",
      "     Training Step: 94 Training Loss: 0.6198837161064148 \n",
      "     Training Step: 95 Training Loss: 0.6116255521774292 \n",
      "     Training Step: 96 Training Loss: 0.6118384599685669 \n",
      "     Training Step: 97 Training Loss: 0.6123000979423523 \n",
      "     Training Step: 98 Training Loss: 0.6123918294906616 \n",
      "     Training Step: 99 Training Loss: 0.6154765486717224 \n",
      "     Training Step: 100 Training Loss: 0.6106910109519958 \n",
      "     Training Step: 101 Training Loss: 0.6130582094192505 \n",
      "     Training Step: 102 Training Loss: 0.6135143041610718 \n",
      "     Training Step: 103 Training Loss: 0.6133511066436768 \n",
      "     Training Step: 104 Training Loss: 0.6168126463890076 \n",
      "     Training Step: 105 Training Loss: 0.6150740385055542 \n",
      "     Training Step: 106 Training Loss: 0.6162191033363342 \n",
      "     Training Step: 107 Training Loss: 0.6117902398109436 \n",
      "     Training Step: 108 Training Loss: 0.6166643500328064 \n",
      "     Training Step: 109 Training Loss: 0.6174939274787903 \n",
      "     Training Step: 110 Training Loss: 0.6142851114273071 \n",
      "     Training Step: 111 Training Loss: 0.6125341057777405 \n",
      "     Training Step: 112 Training Loss: 0.615942120552063 \n",
      "     Training Step: 113 Training Loss: 0.6120281219482422 \n",
      "     Training Step: 114 Training Loss: 0.614704966545105 \n",
      "     Training Step: 115 Training Loss: 0.6182181239128113 \n",
      "     Training Step: 116 Training Loss: 0.6118166446685791 \n",
      "     Training Step: 117 Training Loss: 0.6122485399246216 \n",
      "     Training Step: 118 Training Loss: 0.6107478737831116 \n",
      "     Training Step: 119 Training Loss: 0.6177882552146912 \n",
      "     Training Step: 120 Training Loss: 0.6168027520179749 \n",
      "     Training Step: 121 Training Loss: 0.6146779656410217 \n",
      "     Training Step: 122 Training Loss: 0.6100531220436096 \n",
      "     Training Step: 123 Training Loss: 0.6147275567054749 \n",
      "     Training Step: 124 Training Loss: 0.6144405603408813 \n",
      "     Training Step: 125 Training Loss: 0.6141044497489929 \n",
      "     Training Step: 126 Training Loss: 0.6132835149765015 \n",
      "     Training Step: 127 Training Loss: 0.6156891584396362 \n",
      "     Training Step: 128 Training Loss: 0.6169373989105225 \n",
      "     Training Step: 129 Training Loss: 0.6107357144355774 \n",
      "     Training Step: 130 Training Loss: 0.6154463887214661 \n",
      "     Training Step: 131 Training Loss: 0.6167177557945251 \n",
      "     Training Step: 132 Training Loss: 0.6184611320495605 \n",
      "     Training Step: 133 Training Loss: 0.6202089786529541 \n",
      "     Training Step: 134 Training Loss: 0.6142199039459229 \n",
      "     Training Step: 135 Training Loss: 0.6133030652999878 \n",
      "     Training Step: 136 Training Loss: 0.6123071908950806 \n",
      "     Training Step: 137 Training Loss: 0.6176865696907043 \n",
      "     Training Step: 138 Training Loss: 0.6111779808998108 \n",
      "     Training Step: 139 Training Loss: 0.6131572127342224 \n",
      "     Training Step: 140 Training Loss: 0.6123905181884766 \n",
      "     Training Step: 141 Training Loss: 0.6138889193534851 \n",
      "     Training Step: 142 Training Loss: 0.6129168272018433 \n",
      "     Training Step: 143 Training Loss: 0.6094086170196533 \n",
      "     Training Step: 144 Training Loss: 0.618230402469635 \n",
      "     Training Step: 145 Training Loss: 0.6196418404579163 \n",
      "     Training Step: 146 Training Loss: 0.6133104562759399 \n",
      "     Training Step: 147 Training Loss: 0.6127946972846985 \n",
      "     Training Step: 148 Training Loss: 0.615609347820282 \n",
      "     Training Step: 149 Training Loss: 0.6123725771903992 \n",
      "     Training Step: 150 Training Loss: 0.6111549139022827 \n",
      "     Training Step: 151 Training Loss: 0.6188881397247314 \n",
      "     Training Step: 152 Training Loss: 0.6128624677658081 \n",
      "     Training Step: 153 Training Loss: 0.6186066269874573 \n",
      "     Training Step: 154 Training Loss: 0.6127659678459167 \n",
      "     Training Step: 155 Training Loss: 0.6105826497077942 \n",
      "     Training Step: 156 Training Loss: 0.6148658394813538 \n",
      "     Training Step: 157 Training Loss: 0.6151589751243591 \n",
      "     Training Step: 158 Training Loss: 0.6146028637886047 \n",
      "     Training Step: 159 Training Loss: 0.6155288815498352 \n",
      "     Training Step: 160 Training Loss: 0.610500693321228 \n",
      "     Training Step: 161 Training Loss: 0.6152404546737671 \n",
      "     Training Step: 162 Training Loss: 0.6154112815856934 \n",
      "     Training Step: 163 Training Loss: 0.6138198375701904 \n",
      "     Training Step: 164 Training Loss: 0.6171393990516663 \n",
      "     Training Step: 165 Training Loss: 0.614213764667511 \n",
      "     Training Step: 166 Training Loss: 0.6146335601806641 \n",
      "     Training Step: 167 Training Loss: 0.6180606484413147 \n",
      "     Training Step: 168 Training Loss: 0.6167101263999939 \n",
      "     Training Step: 169 Training Loss: 0.6177473664283752 \n",
      "     Training Step: 170 Training Loss: 0.6171261668205261 \n",
      "     Training Step: 171 Training Loss: 0.6147764921188354 \n",
      "     Training Step: 172 Training Loss: 0.6125556230545044 \n",
      "     Training Step: 173 Training Loss: 0.6121740341186523 \n",
      "     Training Step: 174 Training Loss: 0.6184190511703491 \n",
      "     Training Step: 175 Training Loss: 0.6146985292434692 \n",
      "     Training Step: 176 Training Loss: 0.6180840134620667 \n",
      "     Training Step: 177 Training Loss: 0.6126638054847717 \n",
      "     Training Step: 178 Training Loss: 0.6182932257652283 \n",
      "     Training Step: 179 Training Loss: 0.612765908241272 \n",
      "     Training Step: 180 Training Loss: 0.6128522753715515 \n",
      "     Training Step: 181 Training Loss: 0.6135044097900391 \n",
      "     Training Step: 182 Training Loss: 0.6188200116157532 \n",
      "     Training Step: 183 Training Loss: 0.6176817417144775 \n",
      "     Training Step: 184 Training Loss: 0.6144236326217651 \n",
      "     Training Step: 185 Training Loss: 0.6146031022071838 \n",
      "     Training Step: 186 Training Loss: 0.6166947484016418 \n",
      "     Training Step: 187 Training Loss: 0.6143671274185181 \n",
      "     Training Step: 188 Training Loss: 0.6134347319602966 \n",
      "     Training Step: 189 Training Loss: 0.6134697198867798 \n",
      "     Training Step: 190 Training Loss: 0.6100092530250549 \n",
      "     Training Step: 191 Training Loss: 0.6137427687644958 \n",
      "     Training Step: 192 Training Loss: 0.6209375262260437 \n",
      "     Training Step: 193 Training Loss: 0.6144816279411316 \n",
      "     Training Step: 194 Training Loss: 0.6140214204788208 \n",
      "     Training Step: 195 Training Loss: 0.615298867225647 \n",
      "     Training Step: 196 Training Loss: 0.6157522201538086 \n",
      "     Training Step: 197 Training Loss: 0.6105912327766418 \n",
      "     Training Step: 198 Training Loss: 0.6162002682685852 \n",
      "     Training Step: 199 Training Loss: 0.6135962009429932 \n",
      "     Training Step: 200 Training Loss: 0.615424394607544 \n",
      "     Training Step: 201 Training Loss: 0.6127023696899414 \n",
      "     Training Step: 202 Training Loss: 0.6112043261528015 \n",
      "     Training Step: 203 Training Loss: 0.6142425537109375 \n",
      "     Training Step: 204 Training Loss: 0.6124874353408813 \n",
      "     Training Step: 205 Training Loss: 0.6146528124809265 \n",
      "     Training Step: 206 Training Loss: 0.611579418182373 \n",
      "     Training Step: 207 Training Loss: 0.6155291199684143 \n",
      "     Training Step: 208 Training Loss: 0.616361677646637 \n",
      "     Training Step: 209 Training Loss: 0.6122249364852905 \n",
      "     Training Step: 210 Training Loss: 0.6129618883132935 \n",
      "     Training Step: 211 Training Loss: 0.6116254329681396 \n",
      "     Training Step: 212 Training Loss: 0.6166293025016785 \n",
      "     Training Step: 213 Training Loss: 0.6108934283256531 \n",
      "     Training Step: 214 Training Loss: 0.6115484833717346 \n",
      "     Training Step: 215 Training Loss: 0.6151787638664246 \n",
      "     Training Step: 216 Training Loss: 0.6101240515708923 \n",
      "     Training Step: 217 Training Loss: 0.6132907271385193 \n",
      "     Training Step: 218 Training Loss: 0.6137380003929138 \n",
      "     Training Step: 219 Training Loss: 0.6146640777587891 \n",
      "     Training Step: 220 Training Loss: 0.610360324382782 \n",
      "     Training Step: 221 Training Loss: 0.6145017743110657 \n",
      "     Training Step: 222 Training Loss: 0.6197417974472046 \n",
      "     Training Step: 223 Training Loss: 0.6157480478286743 \n",
      "     Training Step: 224 Training Loss: 0.6166794300079346 \n",
      "     Training Step: 225 Training Loss: 0.6153349280357361 \n",
      "     Training Step: 226 Training Loss: 0.6133070588111877 \n",
      "     Training Step: 227 Training Loss: 0.6164892315864563 \n",
      "     Training Step: 228 Training Loss: 0.614660382270813 \n",
      "     Training Step: 229 Training Loss: 0.6177980303764343 \n",
      "     Training Step: 230 Training Loss: 0.6131409406661987 \n",
      "     Training Step: 231 Training Loss: 0.6105403900146484 \n",
      "     Training Step: 232 Training Loss: 0.6147540211677551 \n",
      "     Training Step: 233 Training Loss: 0.6147958040237427 \n",
      "     Training Step: 234 Training Loss: 0.6167482733726501 \n",
      "     Training Step: 235 Training Loss: 0.6151015162467957 \n",
      "     Training Step: 236 Training Loss: 0.6171883940696716 \n",
      "     Training Step: 237 Training Loss: 0.6108975410461426 \n",
      "     Training Step: 238 Training Loss: 0.616111695766449 \n",
      "     Training Step: 239 Training Loss: 0.6115392446517944 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6118825078010559 \n",
      "     Validation Step: 1 Validation Loss: 0.616241455078125 \n",
      "     Validation Step: 2 Validation Loss: 0.6133108735084534 \n",
      "     Validation Step: 3 Validation Loss: 0.6145720481872559 \n",
      "     Validation Step: 4 Validation Loss: 0.6129856109619141 \n",
      "     Validation Step: 5 Validation Loss: 0.6180583834648132 \n",
      "     Validation Step: 6 Validation Loss: 0.610112726688385 \n",
      "     Validation Step: 7 Validation Loss: 0.6142014265060425 \n",
      "     Validation Step: 8 Validation Loss: 0.6156112551689148 \n",
      "     Validation Step: 9 Validation Loss: 0.6106370687484741 \n",
      "     Validation Step: 10 Validation Loss: 0.615311861038208 \n",
      "     Validation Step: 11 Validation Loss: 0.610518217086792 \n",
      "     Validation Step: 12 Validation Loss: 0.6184705495834351 \n",
      "     Validation Step: 13 Validation Loss: 0.6175990104675293 \n",
      "     Validation Step: 14 Validation Loss: 0.612834632396698 \n",
      "     Validation Step: 15 Validation Loss: 0.6111811399459839 \n",
      "     Validation Step: 16 Validation Loss: 0.617305338382721 \n",
      "     Validation Step: 17 Validation Loss: 0.6183336973190308 \n",
      "     Validation Step: 18 Validation Loss: 0.613663911819458 \n",
      "     Validation Step: 19 Validation Loss: 0.6159871816635132 \n",
      "     Validation Step: 20 Validation Loss: 0.6136435866355896 \n",
      "     Validation Step: 21 Validation Loss: 0.6115598082542419 \n",
      "     Validation Step: 22 Validation Loss: 0.6136457920074463 \n",
      "     Validation Step: 23 Validation Loss: 0.6111651659011841 \n",
      "     Validation Step: 24 Validation Loss: 0.6145384907722473 \n",
      "     Validation Step: 25 Validation Loss: 0.6116486191749573 \n",
      "     Validation Step: 26 Validation Loss: 0.6157876253128052 \n",
      "     Validation Step: 27 Validation Loss: 0.6170196533203125 \n",
      "     Validation Step: 28 Validation Loss: 0.6142581105232239 \n",
      "     Validation Step: 29 Validation Loss: 0.6155743598937988 \n",
      "     Validation Step: 30 Validation Loss: 0.610145628452301 \n",
      "     Validation Step: 31 Validation Loss: 0.6075312495231628 \n",
      "     Validation Step: 32 Validation Loss: 0.6104891300201416 \n",
      "     Validation Step: 33 Validation Loss: 0.6146376132965088 \n",
      "     Validation Step: 34 Validation Loss: 0.6152273416519165 \n",
      "     Validation Step: 35 Validation Loss: 0.6101720929145813 \n",
      "     Validation Step: 36 Validation Loss: 0.6141266822814941 \n",
      "     Validation Step: 37 Validation Loss: 0.6150326728820801 \n",
      "     Validation Step: 38 Validation Loss: 0.6148358583450317 \n",
      "     Validation Step: 39 Validation Loss: 0.614895224571228 \n",
      "     Validation Step: 40 Validation Loss: 0.6184915900230408 \n",
      "     Validation Step: 41 Validation Loss: 0.6141166090965271 \n",
      "     Validation Step: 42 Validation Loss: 0.6182209849357605 \n",
      "     Validation Step: 43 Validation Loss: 0.6121305823326111 \n",
      "     Validation Step: 44 Validation Loss: 0.6176909804344177 \n",
      "Epoch: 107\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6180704832077026 \n",
      "     Training Step: 1 Training Loss: 0.6144998073577881 \n",
      "     Training Step: 2 Training Loss: 0.6115337014198303 \n",
      "     Training Step: 3 Training Loss: 0.6185992956161499 \n",
      "     Training Step: 4 Training Loss: 0.6109067797660828 \n",
      "     Training Step: 5 Training Loss: 0.6128638982772827 \n",
      "     Training Step: 6 Training Loss: 0.6150854825973511 \n",
      "     Training Step: 7 Training Loss: 0.6136153340339661 \n",
      "     Training Step: 8 Training Loss: 0.6133124232292175 \n",
      "     Training Step: 9 Training Loss: 0.6114734411239624 \n",
      "     Training Step: 10 Training Loss: 0.6101741194725037 \n",
      "     Training Step: 11 Training Loss: 0.6146174669265747 \n",
      "     Training Step: 12 Training Loss: 0.6116410493850708 \n",
      "     Training Step: 13 Training Loss: 0.6199286580085754 \n",
      "     Training Step: 14 Training Loss: 0.616736888885498 \n",
      "     Training Step: 15 Training Loss: 0.6177794933319092 \n",
      "     Training Step: 16 Training Loss: 0.6183308959007263 \n",
      "     Training Step: 17 Training Loss: 0.6147950887680054 \n",
      "     Training Step: 18 Training Loss: 0.6104193329811096 \n",
      "     Training Step: 19 Training Loss: 0.6164025664329529 \n",
      "     Training Step: 20 Training Loss: 0.6137676239013672 \n",
      "     Training Step: 21 Training Loss: 0.6141653060913086 \n",
      "     Training Step: 22 Training Loss: 0.6161951422691345 \n",
      "     Training Step: 23 Training Loss: 0.613143265247345 \n",
      "     Training Step: 24 Training Loss: 0.6151765584945679 \n",
      "     Training Step: 25 Training Loss: 0.6105244159698486 \n",
      "     Training Step: 26 Training Loss: 0.6122393608093262 \n",
      "     Training Step: 27 Training Loss: 0.6120743155479431 \n",
      "     Training Step: 28 Training Loss: 0.6120209097862244 \n",
      "     Training Step: 29 Training Loss: 0.6174960732460022 \n",
      "     Training Step: 30 Training Loss: 0.6177225112915039 \n",
      "     Training Step: 31 Training Loss: 0.6167259812355042 \n",
      "     Training Step: 32 Training Loss: 0.614649772644043 \n",
      "     Training Step: 33 Training Loss: 0.6164268255233765 \n",
      "     Training Step: 34 Training Loss: 0.612156867980957 \n",
      "     Training Step: 35 Training Loss: 0.6194422841072083 \n",
      "     Training Step: 36 Training Loss: 0.6188310384750366 \n",
      "     Training Step: 37 Training Loss: 0.6176133155822754 \n",
      "     Training Step: 38 Training Loss: 0.6155160665512085 \n",
      "     Training Step: 39 Training Loss: 0.617772102355957 \n",
      "     Training Step: 40 Training Loss: 0.613549530506134 \n",
      "     Training Step: 41 Training Loss: 0.6142505407333374 \n",
      "     Training Step: 42 Training Loss: 0.6150747537612915 \n",
      "     Training Step: 43 Training Loss: 0.6093164086341858 \n",
      "     Training Step: 44 Training Loss: 0.6188419461250305 \n",
      "     Training Step: 45 Training Loss: 0.6166425347328186 \n",
      "     Training Step: 46 Training Loss: 0.6159377098083496 \n",
      "     Training Step: 47 Training Loss: 0.6167881488800049 \n",
      "     Training Step: 48 Training Loss: 0.6154216527938843 \n",
      "     Training Step: 49 Training Loss: 0.612140953540802 \n",
      "     Training Step: 50 Training Loss: 0.6107610464096069 \n",
      "     Training Step: 51 Training Loss: 0.6111738681793213 \n",
      "     Training Step: 52 Training Loss: 0.6164904236793518 \n",
      "     Training Step: 53 Training Loss: 0.6114165782928467 \n",
      "     Training Step: 54 Training Loss: 0.614039421081543 \n",
      "     Training Step: 55 Training Loss: 0.6129117608070374 \n",
      "     Training Step: 56 Training Loss: 0.6167230010032654 \n",
      "     Training Step: 57 Training Loss: 0.6118313074111938 \n",
      "     Training Step: 58 Training Loss: 0.6143476963043213 \n",
      "     Training Step: 59 Training Loss: 0.6138192415237427 \n",
      "     Training Step: 60 Training Loss: 0.6129204034805298 \n",
      "     Training Step: 61 Training Loss: 0.6106774806976318 \n",
      "     Training Step: 62 Training Loss: 0.6104831695556641 \n",
      "     Training Step: 63 Training Loss: 0.6171662211418152 \n",
      "     Training Step: 64 Training Loss: 0.6133036613464355 \n",
      "     Training Step: 65 Training Loss: 0.6115174889564514 \n",
      "     Training Step: 66 Training Loss: 0.611600399017334 \n",
      "     Training Step: 67 Training Loss: 0.6136487126350403 \n",
      "     Training Step: 68 Training Loss: 0.6148752570152283 \n",
      "     Training Step: 69 Training Loss: 0.6123692989349365 \n",
      "     Training Step: 70 Training Loss: 0.615339994430542 \n",
      "     Training Step: 71 Training Loss: 0.6115778088569641 \n",
      "     Training Step: 72 Training Loss: 0.6182394027709961 \n",
      "     Training Step: 73 Training Loss: 0.6106560826301575 \n",
      "     Training Step: 74 Training Loss: 0.6141546368598938 \n",
      "     Training Step: 75 Training Loss: 0.6152790188789368 \n",
      "     Training Step: 76 Training Loss: 0.6157469749450684 \n",
      "     Training Step: 77 Training Loss: 0.6097230911254883 \n",
      "     Training Step: 78 Training Loss: 0.6117900609970093 \n",
      "     Training Step: 79 Training Loss: 0.6124666929244995 \n",
      "     Training Step: 80 Training Loss: 0.6097220778465271 \n",
      "     Training Step: 81 Training Loss: 0.6105622053146362 \n",
      "     Training Step: 82 Training Loss: 0.6146175861358643 \n",
      "     Training Step: 83 Training Loss: 0.6184958815574646 \n",
      "     Training Step: 84 Training Loss: 0.6118002533912659 \n",
      "     Training Step: 85 Training Loss: 0.6130684614181519 \n",
      "     Training Step: 86 Training Loss: 0.6154096722602844 \n",
      "     Training Step: 87 Training Loss: 0.6127007007598877 \n",
      "     Training Step: 88 Training Loss: 0.6114113926887512 \n",
      "     Training Step: 89 Training Loss: 0.6168153882026672 \n",
      "     Training Step: 90 Training Loss: 0.6178215146064758 \n",
      "     Training Step: 91 Training Loss: 0.6105919480323792 \n",
      "     Training Step: 92 Training Loss: 0.618445634841919 \n",
      "     Training Step: 93 Training Loss: 0.6147459745407104 \n",
      "     Training Step: 94 Training Loss: 0.6144208312034607 \n",
      "     Training Step: 95 Training Loss: 0.6100613474845886 \n",
      "     Training Step: 96 Training Loss: 0.6173754930496216 \n",
      "     Training Step: 97 Training Loss: 0.6122917532920837 \n",
      "     Training Step: 98 Training Loss: 0.6158308982849121 \n",
      "     Training Step: 99 Training Loss: 0.6142814755439758 \n",
      "     Training Step: 100 Training Loss: 0.6196058988571167 \n",
      "     Training Step: 101 Training Loss: 0.6143360733985901 \n",
      "     Training Step: 102 Training Loss: 0.6131500005722046 \n",
      "     Training Step: 103 Training Loss: 0.6146759390830994 \n",
      "     Training Step: 104 Training Loss: 0.6153698563575745 \n",
      "     Training Step: 105 Training Loss: 0.615293562412262 \n",
      "     Training Step: 106 Training Loss: 0.6139376759529114 \n",
      "     Training Step: 107 Training Loss: 0.6209080815315247 \n",
      "     Training Step: 108 Training Loss: 0.6134747266769409 \n",
      "     Training Step: 109 Training Loss: 0.6155906915664673 \n",
      "     Training Step: 110 Training Loss: 0.613364577293396 \n",
      "     Training Step: 111 Training Loss: 0.616191565990448 \n",
      "     Training Step: 112 Training Loss: 0.6101714372634888 \n",
      "     Training Step: 113 Training Loss: 0.6155290603637695 \n",
      "     Training Step: 114 Training Loss: 0.6138852834701538 \n",
      "     Training Step: 115 Training Loss: 0.6146749258041382 \n",
      "     Training Step: 116 Training Loss: 0.6157468557357788 \n",
      "     Training Step: 117 Training Loss: 0.6176836490631104 \n",
      "     Training Step: 118 Training Loss: 0.6142451763153076 \n",
      "     Training Step: 119 Training Loss: 0.6168875098228455 \n",
      "     Training Step: 120 Training Loss: 0.6114458441734314 \n",
      "     Training Step: 121 Training Loss: 0.6171914339065552 \n",
      "     Training Step: 122 Training Loss: 0.6115618944168091 \n",
      "     Training Step: 123 Training Loss: 0.6122410893440247 \n",
      "     Training Step: 124 Training Loss: 0.618583619594574 \n",
      "     Training Step: 125 Training Loss: 0.6128038763999939 \n",
      "     Training Step: 126 Training Loss: 0.6143429279327393 \n",
      "     Training Step: 127 Training Loss: 0.6146355271339417 \n",
      "     Training Step: 128 Training Loss: 0.6127510666847229 \n",
      "     Training Step: 129 Training Loss: 0.6152381300926208 \n",
      "     Training Step: 130 Training Loss: 0.6125296354293823 \n",
      "     Training Step: 131 Training Loss: 0.6168098449707031 \n",
      "     Training Step: 132 Training Loss: 0.6180435419082642 \n",
      "     Training Step: 133 Training Loss: 0.6166756749153137 \n",
      "     Training Step: 134 Training Loss: 0.6118295788764954 \n",
      "     Training Step: 135 Training Loss: 0.6180158853530884 \n",
      "     Training Step: 136 Training Loss: 0.6151546239852905 \n",
      "     Training Step: 137 Training Loss: 0.6118485927581787 \n",
      "     Training Step: 138 Training Loss: 0.6157793402671814 \n",
      "     Training Step: 139 Training Loss: 0.6160296201705933 \n",
      "     Training Step: 140 Training Loss: 0.6144808530807495 \n",
      "     Training Step: 141 Training Loss: 0.6171252727508545 \n",
      "     Training Step: 142 Training Loss: 0.613213300704956 \n",
      "     Training Step: 143 Training Loss: 0.6116418242454529 \n",
      "     Training Step: 144 Training Loss: 0.6144580245018005 \n",
      "     Training Step: 145 Training Loss: 0.6162289977073669 \n",
      "     Training Step: 146 Training Loss: 0.6141068935394287 \n",
      "     Training Step: 147 Training Loss: 0.6154032945632935 \n",
      "     Training Step: 148 Training Loss: 0.6132100224494934 \n",
      "     Training Step: 149 Training Loss: 0.6149603128433228 \n",
      "     Training Step: 150 Training Loss: 0.6129672527313232 \n",
      "     Training Step: 151 Training Loss: 0.6142027378082275 \n",
      "     Training Step: 152 Training Loss: 0.61529541015625 \n",
      "     Training Step: 153 Training Loss: 0.6181105971336365 \n",
      "     Training Step: 154 Training Loss: 0.6128398180007935 \n",
      "     Training Step: 155 Training Loss: 0.613284707069397 \n",
      "     Training Step: 156 Training Loss: 0.6116365194320679 \n",
      "     Training Step: 157 Training Loss: 0.6116883158683777 \n",
      "     Training Step: 158 Training Loss: 0.6105840802192688 \n",
      "     Training Step: 159 Training Loss: 0.61057049036026 \n",
      "     Training Step: 160 Training Loss: 0.6202413439750671 \n",
      "     Training Step: 161 Training Loss: 0.6166343092918396 \n",
      "     Training Step: 162 Training Loss: 0.6122877597808838 \n",
      "     Training Step: 163 Training Loss: 0.6124849319458008 \n",
      "     Training Step: 164 Training Loss: 0.6111934185028076 \n",
      "     Training Step: 165 Training Loss: 0.6163622140884399 \n",
      "     Training Step: 166 Training Loss: 0.6123759746551514 \n",
      "     Training Step: 167 Training Loss: 0.6147306561470032 \n",
      "     Training Step: 168 Training Loss: 0.6132713556289673 \n",
      "     Training Step: 169 Training Loss: 0.6108661890029907 \n",
      "     Training Step: 170 Training Loss: 0.6094414591789246 \n",
      "     Training Step: 171 Training Loss: 0.6151155233383179 \n",
      "     Training Step: 172 Training Loss: 0.6096800565719604 \n",
      "     Training Step: 173 Training Loss: 0.608224630355835 \n",
      "     Training Step: 174 Training Loss: 0.6117745637893677 \n",
      "     Training Step: 175 Training Loss: 0.614033579826355 \n",
      "     Training Step: 176 Training Loss: 0.613749623298645 \n",
      "     Training Step: 177 Training Loss: 0.6147162318229675 \n",
      "     Training Step: 178 Training Loss: 0.6168261766433716 \n",
      "     Training Step: 179 Training Loss: 0.609358012676239 \n",
      "     Training Step: 180 Training Loss: 0.6136496663093567 \n",
      "     Training Step: 181 Training Loss: 0.6153034567832947 \n",
      "     Training Step: 182 Training Loss: 0.6146695017814636 \n",
      "     Training Step: 183 Training Loss: 0.6107045412063599 \n",
      "     Training Step: 184 Training Loss: 0.6149334907531738 \n",
      "     Training Step: 185 Training Loss: 0.6134554147720337 \n",
      "     Training Step: 186 Training Loss: 0.6156913638114929 \n",
      "     Training Step: 187 Training Loss: 0.6166755557060242 \n",
      "     Training Step: 188 Training Loss: 0.6166564226150513 \n",
      "     Training Step: 189 Training Loss: 0.6125208139419556 \n",
      "     Training Step: 190 Training Loss: 0.6182100772857666 \n",
      "     Training Step: 191 Training Loss: 0.6127751469612122 \n",
      "     Training Step: 192 Training Loss: 0.6114605069160461 \n",
      "     Training Step: 193 Training Loss: 0.6154778003692627 \n",
      "     Training Step: 194 Training Loss: 0.61224365234375 \n",
      "     Training Step: 195 Training Loss: 0.6161117553710938 \n",
      "     Training Step: 196 Training Loss: 0.6118910908699036 \n",
      "     Training Step: 197 Training Loss: 0.61329585313797 \n",
      "     Training Step: 198 Training Loss: 0.6100738644599915 \n",
      "     Training Step: 199 Training Loss: 0.6143723726272583 \n",
      "     Training Step: 200 Training Loss: 0.6139100790023804 \n",
      "     Training Step: 201 Training Loss: 0.6144400238990784 \n",
      "     Training Step: 202 Training Loss: 0.6126372814178467 \n",
      "     Training Step: 203 Training Loss: 0.6155577898025513 \n",
      "     Training Step: 204 Training Loss: 0.6103642582893372 \n",
      "     Training Step: 205 Training Loss: 0.6121568083763123 \n",
      "     Training Step: 206 Training Loss: 0.6100399494171143 \n",
      "     Training Step: 207 Training Loss: 0.6154620051383972 \n",
      "     Training Step: 208 Training Loss: 0.6137481331825256 \n",
      "     Training Step: 209 Training Loss: 0.6147183775901794 \n",
      "     Training Step: 210 Training Loss: 0.6134061813354492 \n",
      "     Training Step: 211 Training Loss: 0.6099668741226196 \n",
      "     Training Step: 212 Training Loss: 0.6128670573234558 \n",
      "     Training Step: 213 Training Loss: 0.6121433973312378 \n",
      "     Training Step: 214 Training Loss: 0.6171138882637024 \n",
      "     Training Step: 215 Training Loss: 0.6157575845718384 \n",
      "     Training Step: 216 Training Loss: 0.6131841540336609 \n",
      "     Training Step: 217 Training Loss: 0.6177036166191101 \n",
      "     Training Step: 218 Training Loss: 0.6169379949569702 \n",
      "     Training Step: 219 Training Loss: 0.6184060573577881 \n",
      "     Training Step: 220 Training Loss: 0.6159954071044922 \n",
      "     Training Step: 221 Training Loss: 0.6153765320777893 \n",
      "     Training Step: 222 Training Loss: 0.6146062612533569 \n",
      "     Training Step: 223 Training Loss: 0.6125668287277222 \n",
      "     Training Step: 224 Training Loss: 0.6130927205085754 \n",
      "     Training Step: 225 Training Loss: 0.6111839413642883 \n",
      "     Training Step: 226 Training Loss: 0.6196721792221069 \n",
      "     Training Step: 227 Training Loss: 0.6149193048477173 \n",
      "     Training Step: 228 Training Loss: 0.6147075295448303 \n",
      "     Training Step: 229 Training Loss: 0.6135035157203674 \n",
      "     Training Step: 230 Training Loss: 0.6171509623527527 \n",
      "     Training Step: 231 Training Loss: 0.6167179346084595 \n",
      "     Training Step: 232 Training Loss: 0.6140232086181641 \n",
      "     Training Step: 233 Training Loss: 0.6114087104797363 \n",
      "     Training Step: 234 Training Loss: 0.6123904585838318 \n",
      "     Training Step: 235 Training Loss: 0.615683376789093 \n",
      "     Training Step: 236 Training Loss: 0.6135879755020142 \n",
      "     Training Step: 237 Training Loss: 0.6115218997001648 \n",
      "     Training Step: 238 Training Loss: 0.6147751212120056 \n",
      "     Training Step: 239 Training Loss: 0.6140783429145813 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6136332750320435 \n",
      "     Validation Step: 1 Validation Loss: 0.615620493888855 \n",
      "     Validation Step: 2 Validation Loss: 0.6162510514259338 \n",
      "     Validation Step: 3 Validation Loss: 0.6153260469436646 \n",
      "     Validation Step: 4 Validation Loss: 0.6145761013031006 \n",
      "     Validation Step: 5 Validation Loss: 0.6111716628074646 \n",
      "     Validation Step: 6 Validation Loss: 0.6182447671890259 \n",
      "     Validation Step: 7 Validation Loss: 0.6160047054290771 \n",
      "     Validation Step: 8 Validation Loss: 0.61062091588974 \n",
      "     Validation Step: 9 Validation Loss: 0.6101251244544983 \n",
      "     Validation Step: 10 Validation Loss: 0.6155830025672913 \n",
      "     Validation Step: 11 Validation Loss: 0.6148977875709534 \n",
      "     Validation Step: 12 Validation Loss: 0.6105048656463623 \n",
      "     Validation Step: 13 Validation Loss: 0.6173180341720581 \n",
      "     Validation Step: 14 Validation Loss: 0.6183571219444275 \n",
      "     Validation Step: 15 Validation Loss: 0.6136664152145386 \n",
      "     Validation Step: 16 Validation Loss: 0.6136564612388611 \n",
      "     Validation Step: 17 Validation Loss: 0.6128216981887817 \n",
      "     Validation Step: 18 Validation Loss: 0.6145366430282593 \n",
      "     Validation Step: 19 Validation Loss: 0.6118711829185486 \n",
      "     Validation Step: 20 Validation Loss: 0.613303542137146 \n",
      "     Validation Step: 21 Validation Loss: 0.6176071166992188 \n",
      "     Validation Step: 22 Validation Loss: 0.6148352026939392 \n",
      "     Validation Step: 23 Validation Loss: 0.618485689163208 \n",
      "     Validation Step: 24 Validation Loss: 0.6101511120796204 \n",
      "     Validation Step: 25 Validation Loss: 0.6157932281494141 \n",
      "     Validation Step: 26 Validation Loss: 0.6129783391952515 \n",
      "     Validation Step: 27 Validation Loss: 0.6177056431770325 \n",
      "     Validation Step: 28 Validation Loss: 0.6111523509025574 \n",
      "     Validation Step: 29 Validation Loss: 0.6170397996902466 \n",
      "     Validation Step: 30 Validation Loss: 0.6152313351631165 \n",
      "     Validation Step: 31 Validation Loss: 0.6185097694396973 \n",
      "     Validation Step: 32 Validation Loss: 0.6104753613471985 \n",
      "     Validation Step: 33 Validation Loss: 0.6180768609046936 \n",
      "     Validation Step: 34 Validation Loss: 0.6115469932556152 \n",
      "     Validation Step: 35 Validation Loss: 0.6150436401367188 \n",
      "     Validation Step: 36 Validation Loss: 0.6075006723403931 \n",
      "     Validation Step: 37 Validation Loss: 0.6116362810134888 \n",
      "     Validation Step: 38 Validation Loss: 0.6141125559806824 \n",
      "     Validation Step: 39 Validation Loss: 0.6146380305290222 \n",
      "     Validation Step: 40 Validation Loss: 0.610096275806427 \n",
      "     Validation Step: 41 Validation Loss: 0.6141259670257568 \n",
      "     Validation Step: 42 Validation Loss: 0.6142562627792358 \n",
      "     Validation Step: 43 Validation Loss: 0.6121216416358948 \n",
      "     Validation Step: 44 Validation Loss: 0.6142005920410156 \n",
      "Epoch: 108\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.615391731262207 \n",
      "     Training Step: 1 Training Loss: 0.6167726516723633 \n",
      "     Training Step: 2 Training Loss: 0.6167243719100952 \n",
      "     Training Step: 3 Training Loss: 0.6122308373451233 \n",
      "     Training Step: 4 Training Loss: 0.618018388748169 \n",
      "     Training Step: 5 Training Loss: 0.6146879196166992 \n",
      "     Training Step: 6 Training Loss: 0.611646294593811 \n",
      "     Training Step: 7 Training Loss: 0.6116998791694641 \n",
      "     Training Step: 8 Training Loss: 0.6132883429527283 \n",
      "     Training Step: 9 Training Loss: 0.61363685131073 \n",
      "     Training Step: 10 Training Loss: 0.6150639653205872 \n",
      "     Training Step: 11 Training Loss: 0.6159412860870361 \n",
      "     Training Step: 12 Training Loss: 0.6146096587181091 \n",
      "     Training Step: 13 Training Loss: 0.6116244792938232 \n",
      "     Training Step: 14 Training Loss: 0.614452600479126 \n",
      "     Training Step: 15 Training Loss: 0.6176855564117432 \n",
      "     Training Step: 16 Training Loss: 0.6114026308059692 \n",
      "     Training Step: 17 Training Loss: 0.615298867225647 \n",
      "     Training Step: 18 Training Loss: 0.6125319600105286 \n",
      "     Training Step: 19 Training Loss: 0.6182276606559753 \n",
      "     Training Step: 20 Training Loss: 0.6176356077194214 \n",
      "     Training Step: 21 Training Loss: 0.6123774647712708 \n",
      "     Training Step: 22 Training Loss: 0.6132912039756775 \n",
      "     Training Step: 23 Training Loss: 0.6101505160331726 \n",
      "     Training Step: 24 Training Loss: 0.6147027611732483 \n",
      "     Training Step: 25 Training Loss: 0.6129682064056396 \n",
      "     Training Step: 26 Training Loss: 0.609713077545166 \n",
      "     Training Step: 27 Training Loss: 0.6123790740966797 \n",
      "     Training Step: 28 Training Loss: 0.6115236878395081 \n",
      "     Training Step: 29 Training Loss: 0.6136501431465149 \n",
      "     Training Step: 30 Training Loss: 0.6129192113876343 \n",
      "     Training Step: 31 Training Loss: 0.61626797914505 \n",
      "     Training Step: 32 Training Loss: 0.6157594919204712 \n",
      "     Training Step: 33 Training Loss: 0.6152983903884888 \n",
      "     Training Step: 34 Training Loss: 0.6141528487205505 \n",
      "     Training Step: 35 Training Loss: 0.6114222407341003 \n",
      "     Training Step: 36 Training Loss: 0.6128397583961487 \n",
      "     Training Step: 37 Training Loss: 0.6168183088302612 \n",
      "     Training Step: 38 Training Loss: 0.6154490113258362 \n",
      "     Training Step: 39 Training Loss: 0.6140315532684326 \n",
      "     Training Step: 40 Training Loss: 0.6105963587760925 \n",
      "     Training Step: 41 Training Loss: 0.6173884868621826 \n",
      "     Training Step: 42 Training Loss: 0.6122978329658508 \n",
      "     Training Step: 43 Training Loss: 0.6118290424346924 \n",
      "     Training Step: 44 Training Loss: 0.6100587844848633 \n",
      "     Training Step: 45 Training Loss: 0.613501787185669 \n",
      "     Training Step: 46 Training Loss: 0.616716742515564 \n",
      "     Training Step: 47 Training Loss: 0.6146621108055115 \n",
      "     Training Step: 48 Training Loss: 0.6093912124633789 \n",
      "     Training Step: 49 Training Loss: 0.6134523153305054 \n",
      "     Training Step: 50 Training Loss: 0.6100262999534607 \n",
      "     Training Step: 51 Training Loss: 0.6105583310127258 \n",
      "     Training Step: 52 Training Loss: 0.616136372089386 \n",
      "     Training Step: 53 Training Loss: 0.6177316308021545 \n",
      "     Training Step: 54 Training Loss: 0.6202812790870667 \n",
      "     Training Step: 55 Training Loss: 0.6132679581642151 \n",
      "     Training Step: 56 Training Loss: 0.6175088882446289 \n",
      "     Training Step: 57 Training Loss: 0.6122798919677734 \n",
      "     Training Step: 58 Training Loss: 0.6116411089897156 \n",
      "     Training Step: 59 Training Loss: 0.6130589842796326 \n",
      "     Training Step: 60 Training Loss: 0.6101827621459961 \n",
      "     Training Step: 61 Training Loss: 0.6122294664382935 \n",
      "     Training Step: 62 Training Loss: 0.6152706742286682 \n",
      "     Training Step: 63 Training Loss: 0.6114172339439392 \n",
      "     Training Step: 64 Training Loss: 0.6180766224861145 \n",
      "     Training Step: 65 Training Loss: 0.6167206764221191 \n",
      "     Training Step: 66 Training Loss: 0.6132029294967651 \n",
      "     Training Step: 67 Training Loss: 0.6151538491249084 \n",
      "     Training Step: 68 Training Loss: 0.6139246225357056 \n",
      "     Training Step: 69 Training Loss: 0.6124652624130249 \n",
      "     Training Step: 70 Training Loss: 0.61775803565979 \n",
      "     Training Step: 71 Training Loss: 0.6146058440208435 \n",
      "     Training Step: 72 Training Loss: 0.6166101694107056 \n",
      "     Training Step: 73 Training Loss: 0.616403341293335 \n",
      "     Training Step: 74 Training Loss: 0.6138213276863098 \n",
      "     Training Step: 75 Training Loss: 0.6134709119796753 \n",
      "     Training Step: 76 Training Loss: 0.6138859391212463 \n",
      "     Training Step: 77 Training Loss: 0.6108995079994202 \n",
      "     Training Step: 78 Training Loss: 0.619434654712677 \n",
      "     Training Step: 79 Training Loss: 0.6114471554756165 \n",
      "     Training Step: 80 Training Loss: 0.6107338666915894 \n",
      "     Training Step: 81 Training Loss: 0.6094667911529541 \n",
      "     Training Step: 82 Training Loss: 0.6105754971504211 \n",
      "     Training Step: 83 Training Loss: 0.6162167191505432 \n",
      "     Training Step: 84 Training Loss: 0.6169120073318481 \n",
      "     Training Step: 85 Training Loss: 0.6196610331535339 \n",
      "     Training Step: 86 Training Loss: 0.6100543737411499 \n",
      "     Training Step: 87 Training Loss: 0.6115216016769409 \n",
      "     Training Step: 88 Training Loss: 0.6118754148483276 \n",
      "     Training Step: 89 Training Loss: 0.6122292280197144 \n",
      "     Training Step: 90 Training Loss: 0.6142910122871399 \n",
      "     Training Step: 91 Training Loss: 0.6147540211677551 \n",
      "     Training Step: 92 Training Loss: 0.6117745041847229 \n",
      "     Training Step: 93 Training Loss: 0.6199234127998352 \n",
      "     Training Step: 94 Training Loss: 0.6128039360046387 \n",
      "     Training Step: 95 Training Loss: 0.6106560826301575 \n",
      "     Training Step: 96 Training Loss: 0.6185938715934753 \n",
      "     Training Step: 97 Training Loss: 0.6146619915962219 \n",
      "     Training Step: 98 Training Loss: 0.6129173040390015 \n",
      "     Training Step: 99 Training Loss: 0.6154196858406067 \n",
      "     Training Step: 100 Training Loss: 0.6155334115028381 \n",
      "     Training Step: 101 Training Loss: 0.6183063983917236 \n",
      "     Training Step: 102 Training Loss: 0.6164842247962952 \n",
      "     Training Step: 103 Training Loss: 0.6121890544891357 \n",
      "     Training Step: 104 Training Loss: 0.619671642780304 \n",
      "     Training Step: 105 Training Loss: 0.6107710003852844 \n",
      "     Training Step: 106 Training Loss: 0.6118265986442566 \n",
      "     Training Step: 107 Training Loss: 0.6142574548721313 \n",
      "     Training Step: 108 Training Loss: 0.6131430268287659 \n",
      "     Training Step: 109 Training Loss: 0.6100163459777832 \n",
      "     Training Step: 110 Training Loss: 0.6150872707366943 \n",
      "     Training Step: 111 Training Loss: 0.6184670925140381 \n",
      "     Training Step: 112 Training Loss: 0.6143727898597717 \n",
      "     Training Step: 113 Training Loss: 0.6124898791313171 \n",
      "     Training Step: 114 Training Loss: 0.6184539198875427 \n",
      "     Training Step: 115 Training Loss: 0.6160064339637756 \n",
      "     Training Step: 116 Training Loss: 0.6142151355743408 \n",
      "     Training Step: 117 Training Loss: 0.6188462376594543 \n",
      "     Training Step: 118 Training Loss: 0.6120227575302124 \n",
      "     Training Step: 119 Training Loss: 0.6162059307098389 \n",
      "     Training Step: 120 Training Loss: 0.6166693568229675 \n",
      "     Training Step: 121 Training Loss: 0.6167948842048645 \n",
      "     Training Step: 122 Training Loss: 0.6141120195388794 \n",
      "     Training Step: 123 Training Loss: 0.6169078350067139 \n",
      "     Training Step: 124 Training Loss: 0.6152486205101013 \n",
      "     Training Step: 125 Training Loss: 0.6124078631401062 \n",
      "     Training Step: 126 Training Loss: 0.6121702790260315 \n",
      "     Training Step: 127 Training Loss: 0.6130759119987488 \n",
      "     Training Step: 128 Training Loss: 0.6154788136482239 \n",
      "     Training Step: 129 Training Loss: 0.6144236922264099 \n",
      "     Training Step: 130 Training Loss: 0.6209148168563843 \n",
      "     Training Step: 131 Training Loss: 0.6163427829742432 \n",
      "     Training Step: 132 Training Loss: 0.6148676872253418 \n",
      "     Training Step: 133 Training Loss: 0.6154060959815979 \n",
      "     Training Step: 134 Training Loss: 0.6149588823318481 \n",
      "     Training Step: 135 Training Loss: 0.6133599281311035 \n",
      "     Training Step: 136 Training Loss: 0.6140722036361694 \n",
      "     Training Step: 137 Training Loss: 0.6144412755966187 \n",
      "     Training Step: 138 Training Loss: 0.6141507029533386 \n",
      "     Training Step: 139 Training Loss: 0.6125357151031494 \n",
      "     Training Step: 140 Training Loss: 0.6133027076721191 \n",
      "     Training Step: 141 Training Loss: 0.6156826615333557 \n",
      "     Training Step: 142 Training Loss: 0.6152875423431396 \n",
      "     Training Step: 143 Training Loss: 0.6131383776664734 \n",
      "     Training Step: 144 Training Loss: 0.6186139583587646 \n",
      "     Training Step: 145 Training Loss: 0.6153694987297058 \n",
      "     Training Step: 146 Training Loss: 0.6120713353157043 \n",
      "     Training Step: 147 Training Loss: 0.6126399040222168 \n",
      "     Training Step: 148 Training Loss: 0.6143426895141602 \n",
      "     Training Step: 149 Training Loss: 0.61492520570755 \n",
      "     Training Step: 150 Training Loss: 0.6139094829559326 \n",
      "     Training Step: 151 Training Loss: 0.6147741079330444 \n",
      "     Training Step: 152 Training Loss: 0.6108985543251038 \n",
      "     Training Step: 153 Training Loss: 0.6171691417694092 \n",
      "     Training Step: 154 Training Loss: 0.6132046580314636 \n",
      "     Training Step: 155 Training Loss: 0.6140184998512268 \n",
      "     Training Step: 156 Training Loss: 0.6137385964393616 \n",
      "     Training Step: 157 Training Loss: 0.6145983934402466 \n",
      "     Training Step: 158 Training Loss: 0.6121423840522766 \n",
      "     Training Step: 159 Training Loss: 0.6127486824989319 \n",
      "     Training Step: 160 Training Loss: 0.6111990809440613 \n",
      "     Training Step: 161 Training Loss: 0.6105765700340271 \n",
      "     Training Step: 162 Training Loss: 0.6151766777038574 \n",
      "     Training Step: 163 Training Loss: 0.6106788516044617 \n",
      "     Training Step: 164 Training Loss: 0.6147292852401733 \n",
      "     Training Step: 165 Training Loss: 0.6171169877052307 \n",
      "     Training Step: 166 Training Loss: 0.6181257367134094 \n",
      "     Training Step: 167 Training Loss: 0.6168206334114075 \n",
      "     Training Step: 168 Training Loss: 0.6178093552589417 \n",
      "     Training Step: 169 Training Loss: 0.612514853477478 \n",
      "     Training Step: 170 Training Loss: 0.6136195659637451 \n",
      "     Training Step: 171 Training Loss: 0.6146368384361267 \n",
      "     Training Step: 172 Training Loss: 0.6135962605476379 \n",
      "     Training Step: 173 Training Loss: 0.6167118549346924 \n",
      "     Training Step: 174 Training Loss: 0.6127750277519226 \n",
      "     Training Step: 175 Training Loss: 0.614916205406189 \n",
      "     Training Step: 176 Training Loss: 0.6121361255645752 \n",
      "     Training Step: 177 Training Loss: 0.6116375923156738 \n",
      "     Training Step: 178 Training Loss: 0.61822909116745 \n",
      "     Training Step: 179 Training Loss: 0.6137393712997437 \n",
      "     Training Step: 180 Training Loss: 0.6142039895057678 \n",
      "     Training Step: 181 Training Loss: 0.6164121627807617 \n",
      "     Training Step: 182 Training Loss: 0.6118320822715759 \n",
      "     Training Step: 183 Training Loss: 0.615746021270752 \n",
      "     Training Step: 184 Training Loss: 0.615764319896698 \n",
      "     Training Step: 185 Training Loss: 0.6127123236656189 \n",
      "     Training Step: 186 Training Loss: 0.6160428524017334 \n",
      "     Training Step: 187 Training Loss: 0.6144995093345642 \n",
      "     Training Step: 188 Training Loss: 0.6114795207977295 \n",
      "     Training Step: 189 Training Loss: 0.6166724562644958 \n",
      "     Training Step: 190 Training Loss: 0.6103768348693848 \n",
      "     Training Step: 191 Training Loss: 0.6146838665008545 \n",
      "     Training Step: 192 Training Loss: 0.6135078072547913 \n",
      "     Training Step: 193 Training Loss: 0.6111424565315247 \n",
      "     Training Step: 194 Training Loss: 0.6115161776542664 \n",
      "     Training Step: 195 Training Loss: 0.6172086000442505 \n",
      "     Training Step: 196 Training Loss: 0.6146591305732727 \n",
      "     Training Step: 197 Training Loss: 0.6104864478111267 \n",
      "     Training Step: 198 Training Loss: 0.6115425229072571 \n",
      "     Training Step: 199 Training Loss: 0.6103862524032593 \n",
      "     Training Step: 200 Training Loss: 0.6143499612808228 \n",
      "     Training Step: 201 Training Loss: 0.6115713715553284 \n",
      "     Training Step: 202 Training Loss: 0.6144891381263733 \n",
      "     Training Step: 203 Training Loss: 0.6158501505851746 \n",
      "     Training Step: 204 Training Loss: 0.6143516898155212 \n",
      "     Training Step: 205 Training Loss: 0.617839515209198 \n",
      "     Training Step: 206 Training Loss: 0.6114026308059692 \n",
      "     Training Step: 207 Training Loss: 0.6166651844978333 \n",
      "     Training Step: 208 Training Loss: 0.6097227931022644 \n",
      "     Training Step: 209 Training Loss: 0.6133118271827698 \n",
      "     Training Step: 210 Training Loss: 0.6092301607131958 \n",
      "     Training Step: 211 Training Loss: 0.6151018738746643 \n",
      "     Training Step: 212 Training Loss: 0.6128712892532349 \n",
      "     Training Step: 213 Training Loss: 0.6104800701141357 \n",
      "     Training Step: 214 Training Loss: 0.6147040128707886 \n",
      "     Training Step: 215 Training Loss: 0.6157929301261902 \n",
      "     Training Step: 216 Training Loss: 0.6156091094017029 \n",
      "     Training Step: 217 Training Loss: 0.6171499490737915 \n",
      "     Training Step: 218 Training Loss: 0.6118360757827759 \n",
      "     Training Step: 219 Training Loss: 0.6134124398231506 \n",
      "     Training Step: 220 Training Loss: 0.6117854714393616 \n",
      "     Training Step: 221 Training Loss: 0.6166523694992065 \n",
      "     Training Step: 222 Training Loss: 0.6111636161804199 \n",
      "     Training Step: 223 Training Loss: 0.6128657460212708 \n",
      "     Training Step: 224 Training Loss: 0.6188693046569824 \n",
      "     Training Step: 225 Training Loss: 0.6153340339660645 \n",
      "     Training Step: 226 Training Loss: 0.6156782507896423 \n",
      "     Training Step: 227 Training Loss: 0.6131898760795593 \n",
      "     Training Step: 228 Training Loss: 0.6176945567131042 \n",
      "     Training Step: 229 Training Loss: 0.6082950830459595 \n",
      "     Training Step: 230 Training Loss: 0.6180256009101868 \n",
      "     Training Step: 231 Training Loss: 0.6140232086181641 \n",
      "     Training Step: 232 Training Loss: 0.614793598651886 \n",
      "     Training Step: 233 Training Loss: 0.613766610622406 \n",
      "     Training Step: 234 Training Loss: 0.6154059767723083 \n",
      "     Training Step: 235 Training Loss: 0.6155131459236145 \n",
      "     Training Step: 236 Training Loss: 0.6183934807777405 \n",
      "     Training Step: 237 Training Loss: 0.6097453832626343 \n",
      "     Training Step: 238 Training Loss: 0.6155256032943726 \n",
      "     Training Step: 239 Training Loss: 0.6171289086341858 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.610651433467865 \n",
      "     Validation Step: 1 Validation Loss: 0.6101874709129333 \n",
      "     Validation Step: 2 Validation Loss: 0.6148390769958496 \n",
      "     Validation Step: 3 Validation Loss: 0.6153110265731812 \n",
      "     Validation Step: 4 Validation Loss: 0.6184589266777039 \n",
      "     Validation Step: 5 Validation Loss: 0.6142077445983887 \n",
      "     Validation Step: 6 Validation Loss: 0.6156060099601746 \n",
      "     Validation Step: 7 Validation Loss: 0.6183205842971802 \n",
      "     Validation Step: 8 Validation Loss: 0.6142642498016357 \n",
      "     Validation Step: 9 Validation Loss: 0.6133207678794861 \n",
      "     Validation Step: 10 Validation Loss: 0.6129955053329468 \n",
      "     Validation Step: 11 Validation Loss: 0.6111908555030823 \n",
      "     Validation Step: 12 Validation Loss: 0.6159765720367432 \n",
      "     Validation Step: 13 Validation Loss: 0.6148970723152161 \n",
      "     Validation Step: 14 Validation Loss: 0.613645076751709 \n",
      "     Validation Step: 15 Validation Loss: 0.6145710349082947 \n",
      "     Validation Step: 16 Validation Loss: 0.6141354441642761 \n",
      "     Validation Step: 17 Validation Loss: 0.6172932386398315 \n",
      "     Validation Step: 18 Validation Loss: 0.6118976473808289 \n",
      "     Validation Step: 19 Validation Loss: 0.6111783981323242 \n",
      "     Validation Step: 20 Validation Loss: 0.6136500835418701 \n",
      "     Validation Step: 21 Validation Loss: 0.6141213178634644 \n",
      "     Validation Step: 22 Validation Loss: 0.6157827973365784 \n",
      "     Validation Step: 23 Validation Loss: 0.6115735769271851 \n",
      "     Validation Step: 24 Validation Loss: 0.612143874168396 \n",
      "     Validation Step: 25 Validation Loss: 0.6136685013771057 \n",
      "     Validation Step: 26 Validation Loss: 0.6150304079055786 \n",
      "     Validation Step: 27 Validation Loss: 0.6175916790962219 \n",
      "     Validation Step: 28 Validation Loss: 0.6170068979263306 \n",
      "     Validation Step: 29 Validation Loss: 0.6101657748222351 \n",
      "     Validation Step: 30 Validation Loss: 0.6105369329452515 \n",
      "     Validation Step: 31 Validation Loss: 0.6182072758674622 \n",
      "     Validation Step: 32 Validation Loss: 0.6176770329475403 \n",
      "     Validation Step: 33 Validation Loss: 0.6152238845825195 \n",
      "     Validation Step: 34 Validation Loss: 0.6145427823066711 \n",
      "     Validation Step: 35 Validation Loss: 0.6162380576133728 \n",
      "     Validation Step: 36 Validation Loss: 0.6128480434417725 \n",
      "     Validation Step: 37 Validation Loss: 0.6105032563209534 \n",
      "     Validation Step: 38 Validation Loss: 0.6075627207756042 \n",
      "     Validation Step: 39 Validation Loss: 0.610133945941925 \n",
      "     Validation Step: 40 Validation Loss: 0.6180450916290283 \n",
      "     Validation Step: 41 Validation Loss: 0.6184734106063843 \n",
      "     Validation Step: 42 Validation Loss: 0.6116605401039124 \n",
      "     Validation Step: 43 Validation Loss: 0.6155648231506348 \n",
      "     Validation Step: 44 Validation Loss: 0.6146435737609863 \n",
      "Epoch: 109\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6136525273323059 \n",
      "     Training Step: 1 Training Loss: 0.6150659918785095 \n",
      "     Training Step: 2 Training Loss: 0.6130725741386414 \n",
      "     Training Step: 3 Training Loss: 0.6173614859580994 \n",
      "     Training Step: 4 Training Loss: 0.6155269145965576 \n",
      "     Training Step: 5 Training Loss: 0.6118273735046387 \n",
      "     Training Step: 6 Training Loss: 0.6123884916305542 \n",
      "     Training Step: 7 Training Loss: 0.6137449741363525 \n",
      "     Training Step: 8 Training Loss: 0.6103875637054443 \n",
      "     Training Step: 9 Training Loss: 0.6151536703109741 \n",
      "     Training Step: 10 Training Loss: 0.6143442392349243 \n",
      "     Training Step: 11 Training Loss: 0.6146710515022278 \n",
      "     Training Step: 12 Training Loss: 0.6142148971557617 \n",
      "     Training Step: 13 Training Loss: 0.6161229610443115 \n",
      "     Training Step: 14 Training Loss: 0.6147940754890442 \n",
      "     Training Step: 15 Training Loss: 0.614206850528717 \n",
      "     Training Step: 16 Training Loss: 0.6174986362457275 \n",
      "     Training Step: 17 Training Loss: 0.6114765405654907 \n",
      "     Training Step: 18 Training Loss: 0.6123818755149841 \n",
      "     Training Step: 19 Training Loss: 0.6120715737342834 \n",
      "     Training Step: 20 Training Loss: 0.6176994442939758 \n",
      "     Training Step: 21 Training Loss: 0.6154763102531433 \n",
      "     Training Step: 22 Training Loss: 0.6158345341682434 \n",
      "     Training Step: 23 Training Loss: 0.6128422617912292 \n",
      "     Training Step: 24 Training Loss: 0.6162415742874146 \n",
      "     Training Step: 25 Training Loss: 0.6114009618759155 \n",
      "     Training Step: 26 Training Loss: 0.6157588362693787 \n",
      "     Training Step: 27 Training Loss: 0.6125234961509705 \n",
      "     Training Step: 28 Training Loss: 0.6134981513023376 \n",
      "     Training Step: 29 Training Loss: 0.6133535504341125 \n",
      "     Training Step: 30 Training Loss: 0.6164153814315796 \n",
      "     Training Step: 31 Training Loss: 0.6152434945106506 \n",
      "     Training Step: 32 Training Loss: 0.6177921295166016 \n",
      "     Training Step: 33 Training Loss: 0.6147722601890564 \n",
      "     Training Step: 34 Training Loss: 0.6101495027542114 \n",
      "     Training Step: 35 Training Loss: 0.6115358471870422 \n",
      "     Training Step: 36 Training Loss: 0.6178056001663208 \n",
      "     Training Step: 37 Training Loss: 0.6147254109382629 \n",
      "     Training Step: 38 Training Loss: 0.6100512146949768 \n",
      "     Training Step: 39 Training Loss: 0.612294614315033 \n",
      "     Training Step: 40 Training Loss: 0.6105917096138 \n",
      "     Training Step: 41 Training Loss: 0.610172688961029 \n",
      "     Training Step: 42 Training Loss: 0.612006425857544 \n",
      "     Training Step: 43 Training Loss: 0.6114248037338257 \n",
      "     Training Step: 44 Training Loss: 0.6118180751800537 \n",
      "     Training Step: 45 Training Loss: 0.6096659898757935 \n",
      "     Training Step: 46 Training Loss: 0.6124730706214905 \n",
      "     Training Step: 47 Training Loss: 0.6197351217269897 \n",
      "     Training Step: 48 Training Loss: 0.6154671311378479 \n",
      "     Training Step: 49 Training Loss: 0.6121544241905212 \n",
      "     Training Step: 50 Training Loss: 0.6146607995033264 \n",
      "     Training Step: 51 Training Loss: 0.6123762130737305 \n",
      "     Training Step: 52 Training Loss: 0.6171880960464478 \n",
      "     Training Step: 53 Training Loss: 0.6138208508491516 \n",
      "     Training Step: 54 Training Loss: 0.6164973974227905 \n",
      "     Training Step: 55 Training Loss: 0.6176919341087341 \n",
      "     Training Step: 56 Training Loss: 0.6180938482284546 \n",
      "     Training Step: 57 Training Loss: 0.6156913042068481 \n",
      "     Training Step: 58 Training Loss: 0.6180209517478943 \n",
      "     Training Step: 59 Training Loss: 0.6135032773017883 \n",
      "     Training Step: 60 Training Loss: 0.6198360919952393 \n",
      "     Training Step: 61 Training Loss: 0.6109715700149536 \n",
      "     Training Step: 62 Training Loss: 0.6141735911369324 \n",
      "     Training Step: 63 Training Loss: 0.6185728907585144 \n",
      "     Training Step: 64 Training Loss: 0.6153831481933594 \n",
      "     Training Step: 65 Training Loss: 0.6129328012466431 \n",
      "     Training Step: 66 Training Loss: 0.6166929006576538 \n",
      "     Training Step: 67 Training Loss: 0.6147099137306213 \n",
      "     Training Step: 68 Training Loss: 0.6166415810585022 \n",
      "     Training Step: 69 Training Loss: 0.618387758731842 \n",
      "     Training Step: 70 Training Loss: 0.6101080775260925 \n",
      "     Training Step: 71 Training Loss: 0.6171343326568604 \n",
      "     Training Step: 72 Training Loss: 0.6114581227302551 \n",
      "     Training Step: 73 Training Loss: 0.6135880351066589 \n",
      "     Training Step: 74 Training Loss: 0.6105857491493225 \n",
      "     Training Step: 75 Training Loss: 0.6177127361297607 \n",
      "     Training Step: 76 Training Loss: 0.6167250275611877 \n",
      "     Training Step: 77 Training Loss: 0.6202325224876404 \n",
      "     Training Step: 78 Training Loss: 0.6141067147254944 \n",
      "     Training Step: 79 Training Loss: 0.6135166883468628 \n",
      "     Training Step: 80 Training Loss: 0.6146459579467773 \n",
      "     Training Step: 81 Training Loss: 0.6167996525764465 \n",
      "     Training Step: 82 Training Loss: 0.6111581325531006 \n",
      "     Training Step: 83 Training Loss: 0.6142861247062683 \n",
      "     Training Step: 84 Training Loss: 0.6122340559959412 \n",
      "     Training Step: 85 Training Loss: 0.6116541624069214 \n",
      "     Training Step: 86 Training Loss: 0.6116950511932373 \n",
      "     Training Step: 87 Training Loss: 0.6176273822784424 \n",
      "     Training Step: 88 Training Loss: 0.6146074533462524 \n",
      "     Training Step: 89 Training Loss: 0.6161975860595703 \n",
      "     Training Step: 90 Training Loss: 0.6132054328918457 \n",
      "     Training Step: 91 Training Loss: 0.6154000163078308 \n",
      "     Training Step: 92 Training Loss: 0.6122437715530396 \n",
      "     Training Step: 93 Training Loss: 0.614601194858551 \n",
      "     Training Step: 94 Training Loss: 0.6115253567695618 \n",
      "     Training Step: 95 Training Loss: 0.6094599962234497 \n",
      "     Training Step: 96 Training Loss: 0.611781120300293 \n",
      "     Training Step: 97 Training Loss: 0.6111909747123718 \n",
      "     Training Step: 98 Training Loss: 0.6167318820953369 \n",
      "     Training Step: 99 Training Loss: 0.6182595491409302 \n",
      "     Training Step: 100 Training Loss: 0.6140331625938416 \n",
      "     Training Step: 101 Training Loss: 0.6209787726402283 \n",
      "     Training Step: 102 Training Loss: 0.6155511736869812 \n",
      "     Training Step: 103 Training Loss: 0.6177687644958496 \n",
      "     Training Step: 104 Training Loss: 0.6132915019989014 \n",
      "     Training Step: 105 Training Loss: 0.6116368174552917 \n",
      "     Training Step: 106 Training Loss: 0.6121624708175659 \n",
      "     Training Step: 107 Training Loss: 0.6116523146629333 \n",
      "     Training Step: 108 Training Loss: 0.6128807663917542 \n",
      "     Training Step: 109 Training Loss: 0.6106817722320557 \n",
      "     Training Step: 110 Training Loss: 0.6128718256950378 \n",
      "     Training Step: 111 Training Loss: 0.6144213080406189 \n",
      "     Training Step: 112 Training Loss: 0.6147019863128662 \n",
      "     Training Step: 113 Training Loss: 0.612703800201416 \n",
      "     Training Step: 114 Training Loss: 0.618852436542511 \n",
      "     Training Step: 115 Training Loss: 0.6115840077400208 \n",
      "     Training Step: 116 Training Loss: 0.616043210029602 \n",
      "     Training Step: 117 Training Loss: 0.6131842136383057 \n",
      "     Training Step: 118 Training Loss: 0.6169021725654602 \n",
      "     Training Step: 119 Training Loss: 0.6157872676849365 \n",
      "     Training Step: 120 Training Loss: 0.6144528388977051 \n",
      "     Training Step: 121 Training Loss: 0.614928662776947 \n",
      "     Training Step: 122 Training Loss: 0.6115304827690125 \n",
      "     Training Step: 123 Training Loss: 0.6140221953392029 \n",
      "     Training Step: 124 Training Loss: 0.6122379302978516 \n",
      "     Training Step: 125 Training Loss: 0.6170940399169922 \n",
      "     Training Step: 126 Training Loss: 0.6166713833808899 \n",
      "     Training Step: 127 Training Loss: 0.6152849793434143 \n",
      "     Training Step: 128 Training Loss: 0.6097408533096313 \n",
      "     Training Step: 129 Training Loss: 0.6171367764472961 \n",
      "     Training Step: 130 Training Loss: 0.6107462048530579 \n",
      "     Training Step: 131 Training Loss: 0.611886203289032 \n",
      "     Training Step: 132 Training Loss: 0.6134178042411804 \n",
      "     Training Step: 133 Training Loss: 0.616205096244812 \n",
      "     Training Step: 134 Training Loss: 0.610500693321228 \n",
      "     Training Step: 135 Training Loss: 0.6127973198890686 \n",
      "     Training Step: 136 Training Loss: 0.613912045955658 \n",
      "     Training Step: 137 Training Loss: 0.6180358529090881 \n",
      "     Training Step: 138 Training Loss: 0.6168117523193359 \n",
      "     Training Step: 139 Training Loss: 0.614481508731842 \n",
      "     Training Step: 140 Training Loss: 0.61508709192276 \n",
      "     Training Step: 141 Training Loss: 0.6126387119293213 \n",
      "     Training Step: 142 Training Loss: 0.6183220744132996 \n",
      "     Training Step: 143 Training Loss: 0.6133153438568115 \n",
      "     Training Step: 144 Training Loss: 0.6097376942634583 \n",
      "     Training Step: 145 Training Loss: 0.6144981384277344 \n",
      "     Training Step: 146 Training Loss: 0.6146692633628845 \n",
      "     Training Step: 147 Training Loss: 0.6127541661262512 \n",
      "     Training Step: 148 Training Loss: 0.6143388152122498 \n",
      "     Training Step: 149 Training Loss: 0.6132819652557373 \n",
      "     Training Step: 150 Training Loss: 0.6114097237586975 \n",
      "     Training Step: 151 Training Loss: 0.6194503903388977 \n",
      "     Training Step: 152 Training Loss: 0.6082585453987122 \n",
      "     Training Step: 153 Training Loss: 0.6100490093231201 \n",
      "     Training Step: 154 Training Loss: 0.615276038646698 \n",
      "     Training Step: 155 Training Loss: 0.6149235963821411 \n",
      "     Training Step: 156 Training Loss: 0.6127629280090332 \n",
      "     Training Step: 157 Training Loss: 0.6197404265403748 \n",
      "     Training Step: 158 Training Loss: 0.6121402382850647 \n",
      "     Training Step: 159 Training Loss: 0.6107082962989807 \n",
      "     Training Step: 160 Training Loss: 0.6180914640426636 \n",
      "     Training Step: 161 Training Loss: 0.6159587502479553 \n",
      "     Training Step: 162 Training Loss: 0.6164105534553528 \n",
      "     Training Step: 163 Training Loss: 0.6166165471076965 \n",
      "     Training Step: 164 Training Loss: 0.6132071614265442 \n",
      "     Training Step: 165 Training Loss: 0.6142460703849792 \n",
      "     Training Step: 166 Training Loss: 0.6121305823326111 \n",
      "     Training Step: 167 Training Loss: 0.6104224920272827 \n",
      "     Training Step: 168 Training Loss: 0.6154404878616333 \n",
      "     Training Step: 169 Training Loss: 0.6182305812835693 \n",
      "     Training Step: 170 Training Loss: 0.6167495846748352 \n",
      "     Training Step: 171 Training Loss: 0.6163451075553894 \n",
      "     Training Step: 172 Training Loss: 0.6146911382675171 \n",
      "     Training Step: 173 Training Loss: 0.6146069765090942 \n",
      "     Training Step: 174 Training Loss: 0.6125038266181946 \n",
      "     Training Step: 175 Training Loss: 0.6156694889068604 \n",
      "     Training Step: 176 Training Loss: 0.6157388687133789 \n",
      "     Training Step: 177 Training Loss: 0.6185614466667175 \n",
      "     Training Step: 178 Training Loss: 0.6167088150978088 \n",
      "     Training Step: 179 Training Loss: 0.6131375432014465 \n",
      "     Training Step: 180 Training Loss: 0.6111845970153809 \n",
      "     Training Step: 181 Training Loss: 0.6129329204559326 \n",
      "     Training Step: 182 Training Loss: 0.613639771938324 \n",
      "     Training Step: 183 Training Loss: 0.6092444658279419 \n",
      "     Training Step: 184 Training Loss: 0.6114216446876526 \n",
      "     Training Step: 185 Training Loss: 0.6122825741767883 \n",
      "     Training Step: 186 Training Loss: 0.6184713244438171 \n",
      "     Training Step: 187 Training Loss: 0.6118407845497131 \n",
      "     Training Step: 188 Training Loss: 0.6099749803543091 \n",
      "     Training Step: 189 Training Loss: 0.6154431104660034 \n",
      "     Training Step: 190 Training Loss: 0.6146942377090454 \n",
      "     Training Step: 191 Training Loss: 0.6125301122665405 \n",
      "     Training Step: 192 Training Loss: 0.6148787140846252 \n",
      "     Training Step: 193 Training Loss: 0.6147570610046387 \n",
      "     Training Step: 194 Training Loss: 0.6160197257995605 \n",
      "     Training Step: 195 Training Loss: 0.6153170466423035 \n",
      "     Training Step: 196 Training Loss: 0.6166777610778809 \n",
      "     Training Step: 197 Training Loss: 0.6116160154342651 \n",
      "     Training Step: 198 Training Loss: 0.6166427731513977 \n",
      "     Training Step: 199 Training Loss: 0.6153716444969177 \n",
      "     Training Step: 200 Training Loss: 0.6153399348258972 \n",
      "     Training Step: 201 Training Loss: 0.6141660213470459 \n",
      "     Training Step: 202 Training Loss: 0.6188474893569946 \n",
      "     Training Step: 203 Training Loss: 0.6140281558036804 \n",
      "     Training Step: 204 Training Loss: 0.6106448769569397 \n",
      "     Training Step: 205 Training Loss: 0.6136426329612732 \n",
      "     Training Step: 206 Training Loss: 0.6143680214881897 \n",
      "     Training Step: 207 Training Loss: 0.6129805445671082 \n",
      "     Training Step: 208 Training Loss: 0.6150957942008972 \n",
      "     Training Step: 209 Training Loss: 0.6168009638786316 \n",
      "     Training Step: 210 Training Loss: 0.6108853816986084 \n",
      "     Training Step: 211 Training Loss: 0.6106885671615601 \n",
      "     Training Step: 212 Training Loss: 0.6130682229995728 \n",
      "     Training Step: 213 Training Loss: 0.6125297546386719 \n",
      "     Training Step: 214 Training Loss: 0.6118025183677673 \n",
      "     Training Step: 215 Training Loss: 0.6155380606651306 \n",
      "     Training Step: 216 Training Loss: 0.6131409406661987 \n",
      "     Training Step: 217 Training Loss: 0.6138964295387268 \n",
      "     Training Step: 218 Training Loss: 0.6093693971633911 \n",
      "     Training Step: 219 Training Loss: 0.6105583310127258 \n",
      "     Training Step: 220 Training Loss: 0.6157647967338562 \n",
      "     Training Step: 221 Training Loss: 0.6137378215789795 \n",
      "     Training Step: 222 Training Loss: 0.6134539842605591 \n",
      "     Training Step: 223 Training Loss: 0.6140837669372559 \n",
      "     Training Step: 224 Training Loss: 0.6137651205062866 \n",
      "     Training Step: 225 Training Loss: 0.6144396066665649 \n",
      "     Training Step: 226 Training Loss: 0.6169425249099731 \n",
      "     Training Step: 227 Training Loss: 0.6118276119232178 \n",
      "     Training Step: 228 Training Loss: 0.6143408417701721 \n",
      "     Training Step: 229 Training Loss: 0.6132808327674866 \n",
      "     Training Step: 230 Training Loss: 0.615175724029541 \n",
      "     Training Step: 231 Training Loss: 0.6149557828903198 \n",
      "     Training Step: 232 Training Loss: 0.6184580326080322 \n",
      "     Training Step: 233 Training Loss: 0.6171864867210388 \n",
      "     Training Step: 234 Training Loss: 0.6105141639709473 \n",
      "     Training Step: 235 Training Loss: 0.6133105158805847 \n",
      "     Training Step: 236 Training Loss: 0.6139296293258667 \n",
      "     Training Step: 237 Training Loss: 0.6115710735321045 \n",
      "     Training Step: 238 Training Loss: 0.615283727645874 \n",
      "     Training Step: 239 Training Loss: 0.6155967712402344 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6162428855895996 \n",
      "     Validation Step: 1 Validation Loss: 0.6148356795310974 \n",
      "     Validation Step: 2 Validation Loss: 0.6183419823646545 \n",
      "     Validation Step: 3 Validation Loss: 0.615997314453125 \n",
      "     Validation Step: 4 Validation Loss: 0.6075274348258972 \n",
      "     Validation Step: 5 Validation Loss: 0.6184896230697632 \n",
      "     Validation Step: 6 Validation Loss: 0.6142513155937195 \n",
      "     Validation Step: 7 Validation Loss: 0.6111688613891602 \n",
      "     Validation Step: 8 Validation Loss: 0.6136695146560669 \n",
      "     Validation Step: 9 Validation Loss: 0.6152268648147583 \n",
      "     Validation Step: 10 Validation Loss: 0.6105198860168457 \n",
      "     Validation Step: 11 Validation Loss: 0.6133027076721191 \n",
      "     Validation Step: 12 Validation Loss: 0.6182280778884888 \n",
      "     Validation Step: 13 Validation Loss: 0.6180596947669983 \n",
      "     Validation Step: 14 Validation Loss: 0.6173021793365479 \n",
      "     Validation Step: 15 Validation Loss: 0.6150419116020203 \n",
      "     Validation Step: 16 Validation Loss: 0.6145325303077698 \n",
      "     Validation Step: 17 Validation Loss: 0.6101402640342712 \n",
      "     Validation Step: 18 Validation Loss: 0.61298668384552 \n",
      "     Validation Step: 19 Validation Loss: 0.6156148314476013 \n",
      "     Validation Step: 20 Validation Loss: 0.6118844151496887 \n",
      "     Validation Step: 21 Validation Loss: 0.6142006516456604 \n",
      "     Validation Step: 22 Validation Loss: 0.6170204281806946 \n",
      "     Validation Step: 23 Validation Loss: 0.6146385669708252 \n",
      "     Validation Step: 24 Validation Loss: 0.610633373260498 \n",
      "     Validation Step: 25 Validation Loss: 0.6184663772583008 \n",
      "     Validation Step: 26 Validation Loss: 0.6148922443389893 \n",
      "     Validation Step: 27 Validation Loss: 0.6111798882484436 \n",
      "     Validation Step: 28 Validation Loss: 0.6128283739089966 \n",
      "     Validation Step: 29 Validation Loss: 0.6145691871643066 \n",
      "     Validation Step: 30 Validation Loss: 0.6136324405670166 \n",
      "     Validation Step: 31 Validation Loss: 0.6115590333938599 \n",
      "     Validation Step: 32 Validation Loss: 0.6157942414283752 \n",
      "     Validation Step: 33 Validation Loss: 0.6101582050323486 \n",
      "     Validation Step: 34 Validation Loss: 0.6136569380760193 \n",
      "     Validation Step: 35 Validation Loss: 0.6141316294670105 \n",
      "     Validation Step: 36 Validation Loss: 0.6175910234451294 \n",
      "     Validation Step: 37 Validation Loss: 0.6104859709739685 \n",
      "     Validation Step: 38 Validation Loss: 0.6101146936416626 \n",
      "     Validation Step: 39 Validation Loss: 0.6116418838500977 \n",
      "     Validation Step: 40 Validation Loss: 0.612130343914032 \n",
      "     Validation Step: 41 Validation Loss: 0.6155726909637451 \n",
      "     Validation Step: 42 Validation Loss: 0.6141127347946167 \n",
      "     Validation Step: 43 Validation Loss: 0.6153200268745422 \n",
      "     Validation Step: 44 Validation Loss: 0.6176958084106445 \n",
      "Epoch: 110\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6161975860595703 \n",
      "     Training Step: 1 Training Loss: 0.6154131293296814 \n",
      "     Training Step: 2 Training Loss: 0.616716742515564 \n",
      "     Training Step: 3 Training Loss: 0.6105029582977295 \n",
      "     Training Step: 4 Training Loss: 0.6184072494506836 \n",
      "     Training Step: 5 Training Loss: 0.6146789789199829 \n",
      "     Training Step: 6 Training Loss: 0.6154199242591858 \n",
      "     Training Step: 7 Training Loss: 0.614601194858551 \n",
      "     Training Step: 8 Training Loss: 0.6150988936424255 \n",
      "     Training Step: 9 Training Loss: 0.6170884966850281 \n",
      "     Training Step: 10 Training Loss: 0.6126517653465271 \n",
      "     Training Step: 11 Training Loss: 0.6171311736106873 \n",
      "     Training Step: 12 Training Loss: 0.6167946457862854 \n",
      "     Training Step: 13 Training Loss: 0.6152815818786621 \n",
      "     Training Step: 14 Training Loss: 0.6120358109474182 \n",
      "     Training Step: 15 Training Loss: 0.612405002117157 \n",
      "     Training Step: 16 Training Loss: 0.6118495464324951 \n",
      "     Training Step: 17 Training Loss: 0.6177950501441956 \n",
      "     Training Step: 18 Training Loss: 0.6141514778137207 \n",
      "     Training Step: 19 Training Loss: 0.6132821440696716 \n",
      "     Training Step: 20 Training Loss: 0.6097389459609985 \n",
      "     Training Step: 21 Training Loss: 0.6144964098930359 \n",
      "     Training Step: 22 Training Loss: 0.6135108470916748 \n",
      "     Training Step: 23 Training Loss: 0.6147074103355408 \n",
      "     Training Step: 24 Training Loss: 0.6105865240097046 \n",
      "     Training Step: 25 Training Loss: 0.6100420951843262 \n",
      "     Training Step: 26 Training Loss: 0.6155368685722351 \n",
      "     Training Step: 27 Training Loss: 0.6137394309043884 \n",
      "     Training Step: 28 Training Loss: 0.6131336688995361 \n",
      "     Training Step: 29 Training Loss: 0.6146245002746582 \n",
      "     Training Step: 30 Training Loss: 0.6154617667198181 \n",
      "     Training Step: 31 Training Loss: 0.6122287511825562 \n",
      "     Training Step: 32 Training Loss: 0.6154007911682129 \n",
      "     Training Step: 33 Training Loss: 0.6143832802772522 \n",
      "     Training Step: 34 Training Loss: 0.6168175339698792 \n",
      "     Training Step: 35 Training Loss: 0.6169314980506897 \n",
      "     Training Step: 36 Training Loss: 0.6177624464035034 \n",
      "     Training Step: 37 Training Loss: 0.6137486696243286 \n",
      "     Training Step: 38 Training Loss: 0.6128536462783813 \n",
      "     Training Step: 39 Training Loss: 0.6150974631309509 \n",
      "     Training Step: 40 Training Loss: 0.6120986938476562 \n",
      "     Training Step: 41 Training Loss: 0.6139481663703918 \n",
      "     Training Step: 42 Training Loss: 0.6106377243995667 \n",
      "     Training Step: 43 Training Loss: 0.6155946850776672 \n",
      "     Training Step: 44 Training Loss: 0.614337146282196 \n",
      "     Training Step: 45 Training Loss: 0.6180076599121094 \n",
      "     Training Step: 46 Training Loss: 0.6114891767501831 \n",
      "     Training Step: 47 Training Loss: 0.6173755526542664 \n",
      "     Training Step: 48 Training Loss: 0.6171921491622925 \n",
      "     Training Step: 49 Training Loss: 0.6134616136550903 \n",
      "     Training Step: 50 Training Loss: 0.6162351369857788 \n",
      "     Training Step: 51 Training Loss: 0.6138206124305725 \n",
      "     Training Step: 52 Training Loss: 0.6164916753768921 \n",
      "     Training Step: 53 Training Loss: 0.6182196140289307 \n",
      "     Training Step: 54 Training Loss: 0.6097452044487 \n",
      "     Training Step: 55 Training Loss: 0.6167156100273132 \n",
      "     Training Step: 56 Training Loss: 0.6158261299133301 \n",
      "     Training Step: 57 Training Loss: 0.6161109805107117 \n",
      "     Training Step: 58 Training Loss: 0.6104208827018738 \n",
      "     Training Step: 59 Training Loss: 0.6155275106430054 \n",
      "     Training Step: 60 Training Loss: 0.6115714907646179 \n",
      "     Training Step: 61 Training Loss: 0.6097269654273987 \n",
      "     Training Step: 62 Training Loss: 0.6166130304336548 \n",
      "     Training Step: 63 Training Loss: 0.6111450791358948 \n",
      "     Training Step: 64 Training Loss: 0.6124853491783142 \n",
      "     Training Step: 65 Training Loss: 0.6106540560722351 \n",
      "     Training Step: 66 Training Loss: 0.6100481152534485 \n",
      "     Training Step: 67 Training Loss: 0.6140868663787842 \n",
      "     Training Step: 68 Training Loss: 0.6180833578109741 \n",
      "     Training Step: 69 Training Loss: 0.6139116883277893 \n",
      "     Training Step: 70 Training Loss: 0.6094245910644531 \n",
      "     Training Step: 71 Training Loss: 0.6130560040473938 \n",
      "     Training Step: 72 Training Loss: 0.6167532205581665 \n",
      "     Training Step: 73 Training Loss: 0.6144331097602844 \n",
      "     Training Step: 74 Training Loss: 0.6188818216323853 \n",
      "     Training Step: 75 Training Loss: 0.6116214990615845 \n",
      "     Training Step: 76 Training Loss: 0.6149305701255798 \n",
      "     Training Step: 77 Training Loss: 0.6104801893234253 \n",
      "     Training Step: 78 Training Loss: 0.6160038113594055 \n",
      "     Training Step: 79 Training Loss: 0.6194430589675903 \n",
      "     Training Step: 80 Training Loss: 0.6164164543151855 \n",
      "     Training Step: 81 Training Loss: 0.6135030388832092 \n",
      "     Training Step: 82 Training Loss: 0.6159417629241943 \n",
      "     Training Step: 83 Training Loss: 0.6144827604293823 \n",
      "     Training Step: 84 Training Loss: 0.6196714639663696 \n",
      "     Training Step: 85 Training Loss: 0.6127337217330933 \n",
      "     Training Step: 86 Training Loss: 0.6141232848167419 \n",
      "     Training Step: 87 Training Loss: 0.6157441735267639 \n",
      "     Training Step: 88 Training Loss: 0.6176799535751343 \n",
      "     Training Step: 89 Training Loss: 0.6107742190361023 \n",
      "     Training Step: 90 Training Loss: 0.611821174621582 \n",
      "     Training Step: 91 Training Loss: 0.613364577293396 \n",
      "     Training Step: 92 Training Loss: 0.6151717305183411 \n",
      "     Training Step: 93 Training Loss: 0.6176241636276245 \n",
      "     Training Step: 94 Training Loss: 0.6147040128707886 \n",
      "     Training Step: 95 Training Loss: 0.612538754940033 \n",
      "     Training Step: 96 Training Loss: 0.6114397644996643 \n",
      "     Training Step: 97 Training Loss: 0.6166779398918152 \n",
      "     Training Step: 98 Training Loss: 0.6105730533599854 \n",
      "     Training Step: 99 Training Loss: 0.6146907806396484 \n",
      "     Training Step: 100 Training Loss: 0.6171501278877258 \n",
      "     Training Step: 101 Training Loss: 0.6101745963096619 \n",
      "     Training Step: 102 Training Loss: 0.6140320897102356 \n",
      "     Training Step: 103 Training Loss: 0.6202414035797119 \n",
      "     Training Step: 104 Training Loss: 0.6140227913856506 \n",
      "     Training Step: 105 Training Loss: 0.6122823357582092 \n",
      "     Training Step: 106 Training Loss: 0.6160303950309753 \n",
      "     Training Step: 107 Training Loss: 0.6116932034492493 \n",
      "     Training Step: 108 Training Loss: 0.6107308864593506 \n",
      "     Training Step: 109 Training Loss: 0.6134803295135498 \n",
      "     Training Step: 110 Training Loss: 0.6157506704330444 \n",
      "     Training Step: 111 Training Loss: 0.6147233843803406 \n",
      "     Training Step: 112 Training Loss: 0.6168943047523499 \n",
      "     Training Step: 113 Training Loss: 0.6116472482681274 \n",
      "     Training Step: 114 Training Loss: 0.6133043169975281 \n",
      "     Training Step: 115 Training Loss: 0.6149188280105591 \n",
      "     Training Step: 116 Training Loss: 0.6115207076072693 \n",
      "     Training Step: 117 Training Loss: 0.6118276715278625 \n",
      "     Training Step: 118 Training Loss: 0.6122896671295166 \n",
      "     Training Step: 119 Training Loss: 0.6100334525108337 \n",
      "     Training Step: 120 Training Loss: 0.6166686415672302 \n",
      "     Training Step: 121 Training Loss: 0.6142091751098633 \n",
      "     Training Step: 122 Training Loss: 0.6131815910339355 \n",
      "     Training Step: 123 Training Loss: 0.6105571389198303 \n",
      "     Training Step: 124 Training Loss: 0.6135963797569275 \n",
      "     Training Step: 125 Training Loss: 0.6146628856658936 \n",
      "     Training Step: 126 Training Loss: 0.6127615571022034 \n",
      "     Training Step: 127 Training Loss: 0.6152459979057312 \n",
      "     Training Step: 128 Training Loss: 0.6147756576538086 \n",
      "     Training Step: 129 Training Loss: 0.6138885617256165 \n",
      "     Training Step: 130 Training Loss: 0.6157910823822021 \n",
      "     Training Step: 131 Training Loss: 0.6149576306343079 \n",
      "     Training Step: 132 Training Loss: 0.6128643155097961 \n",
      "     Training Step: 133 Training Loss: 0.6128702163696289 \n",
      "     Training Step: 134 Training Loss: 0.6152665019035339 \n",
      "     Training Step: 135 Training Loss: 0.6153708696365356 \n",
      "     Training Step: 136 Training Loss: 0.6176857352256775 \n",
      "     Training Step: 137 Training Loss: 0.6155297160148621 \n",
      "     Training Step: 138 Training Loss: 0.6184269189834595 \n",
      "     Training Step: 139 Training Loss: 0.6128168702125549 \n",
      "     Training Step: 140 Training Loss: 0.6094520688056946 \n",
      "     Training Step: 141 Training Loss: 0.6109305620193481 \n",
      "     Training Step: 142 Training Loss: 0.6134286522865295 \n",
      "     Training Step: 143 Training Loss: 0.6146076917648315 \n",
      "     Training Step: 144 Training Loss: 0.6122322082519531 \n",
      "     Training Step: 145 Training Loss: 0.6114169955253601 \n",
      "     Training Step: 146 Training Loss: 0.6116082072257996 \n",
      "     Training Step: 147 Training Loss: 0.6199095845222473 \n",
      "     Training Step: 148 Training Loss: 0.6103644967079163 \n",
      "     Training Step: 149 Training Loss: 0.6114262938499451 \n",
      "     Training Step: 150 Training Loss: 0.6144583821296692 \n",
      "     Training Step: 151 Training Loss: 0.6175248622894287 \n",
      "     Training Step: 152 Training Loss: 0.6146546006202698 \n",
      "     Training Step: 153 Training Loss: 0.6168239712715149 \n",
      "     Training Step: 154 Training Loss: 0.6108699440956116 \n",
      "     Training Step: 155 Training Loss: 0.6166820526123047 \n",
      "     Training Step: 156 Training Loss: 0.6121593117713928 \n",
      "     Training Step: 157 Training Loss: 0.6163511872291565 \n",
      "     Training Step: 158 Training Loss: 0.6112019419670105 \n",
      "     Training Step: 159 Training Loss: 0.6147459149360657 \n",
      "     Training Step: 160 Training Loss: 0.6167513132095337 \n",
      "     Training Step: 161 Training Loss: 0.6131269335746765 \n",
      "     Training Step: 162 Training Loss: 0.6146710515022278 \n",
      "     Training Step: 163 Training Loss: 0.612542450428009 \n",
      "     Training Step: 164 Training Loss: 0.6136506199836731 \n",
      "     Training Step: 165 Training Loss: 0.612521231174469 \n",
      "     Training Step: 166 Training Loss: 0.6164053678512573 \n",
      "     Training Step: 167 Training Loss: 0.6111642122268677 \n",
      "     Training Step: 168 Training Loss: 0.6154755353927612 \n",
      "     Training Step: 169 Training Loss: 0.6147930026054382 \n",
      "     Training Step: 170 Training Loss: 0.6124622225761414 \n",
      "     Training Step: 171 Training Loss: 0.6115246415138245 \n",
      "     Training Step: 172 Training Loss: 0.6115787625312805 \n",
      "     Training Step: 173 Training Loss: 0.615073561668396 \n",
      "     Training Step: 174 Training Loss: 0.6184894442558289 \n",
      "     Training Step: 175 Training Loss: 0.6171817183494568 \n",
      "     Training Step: 176 Training Loss: 0.6154003739356995 \n",
      "     Training Step: 177 Training Loss: 0.6146654486656189 \n",
      "     Training Step: 178 Training Loss: 0.6148688197135925 \n",
      "     Training Step: 179 Training Loss: 0.6132907867431641 \n",
      "     Training Step: 180 Training Loss: 0.6140177249908447 \n",
      "     Training Step: 181 Training Loss: 0.6153346300125122 \n",
      "     Training Step: 182 Training Loss: 0.6118373274803162 \n",
      "     Training Step: 183 Training Loss: 0.6121591329574585 \n",
      "     Training Step: 184 Training Loss: 0.6129235029220581 \n",
      "     Training Step: 185 Training Loss: 0.6188632845878601 \n",
      "     Training Step: 186 Training Loss: 0.6136380434036255 \n",
      "     Training Step: 187 Training Loss: 0.6117986440658569 \n",
      "     Training Step: 188 Training Loss: 0.6209293007850647 \n",
      "     Training Step: 189 Training Loss: 0.6185747981071472 \n",
      "     Training Step: 190 Training Loss: 0.6100013852119446 \n",
      "     Training Step: 191 Training Loss: 0.6133161187171936 \n",
      "     Training Step: 192 Training Loss: 0.6122442483901978 \n",
      "     Training Step: 193 Training Loss: 0.6143462061882019 \n",
      "     Training Step: 194 Training Loss: 0.6117874383926392 \n",
      "     Training Step: 195 Training Loss: 0.612121045589447 \n",
      "     Training Step: 196 Training Loss: 0.613278865814209 \n",
      "     Training Step: 197 Training Loss: 0.6132016777992249 \n",
      "     Training Step: 198 Training Loss: 0.6129130125045776 \n",
      "     Training Step: 199 Training Loss: 0.6137657165527344 \n",
      "     Training Step: 200 Training Loss: 0.616230309009552 \n",
      "     Training Step: 201 Training Loss: 0.6143523454666138 \n",
      "     Training Step: 202 Training Loss: 0.6166654825210571 \n",
      "     Training Step: 203 Training Loss: 0.6142488121986389 \n",
      "     Training Step: 204 Training Loss: 0.6101323366165161 \n",
      "     Training Step: 205 Training Loss: 0.6151613593101501 \n",
      "     Training Step: 206 Training Loss: 0.6167095899581909 \n",
      "     Training Step: 207 Training Loss: 0.617798388004303 \n",
      "     Training Step: 208 Training Loss: 0.6144394874572754 \n",
      "     Training Step: 209 Training Loss: 0.618093729019165 \n",
      "     Training Step: 210 Training Loss: 0.614284336566925 \n",
      "     Training Step: 211 Training Loss: 0.6121617555618286 \n",
      "     Training Step: 212 Training Loss: 0.6152907609939575 \n",
      "     Training Step: 213 Training Loss: 0.6083252429962158 \n",
      "     Training Step: 214 Training Loss: 0.6182965040206909 \n",
      "     Training Step: 215 Training Loss: 0.614212691783905 \n",
      "     Training Step: 216 Training Loss: 0.611420214176178 \n",
      "     Training Step: 217 Training Loss: 0.6180549263954163 \n",
      "     Training Step: 218 Training Loss: 0.6092553734779358 \n",
      "     Training Step: 219 Training Loss: 0.6129705905914307 \n",
      "     Training Step: 220 Training Loss: 0.6157421469688416 \n",
      "     Training Step: 221 Training Loss: 0.6186022758483887 \n",
      "     Training Step: 222 Training Loss: 0.6118806004524231 \n",
      "     Training Step: 223 Training Loss: 0.6156864166259766 \n",
      "     Training Step: 224 Training Loss: 0.6132072806358337 \n",
      "     Training Step: 225 Training Loss: 0.6115283370018005 \n",
      "     Training Step: 226 Training Loss: 0.6176977753639221 \n",
      "     Training Step: 227 Training Loss: 0.612375020980835 \n",
      "     Training Step: 228 Training Loss: 0.6116173267364502 \n",
      "     Training Step: 229 Training Loss: 0.6123731136322021 \n",
      "     Training Step: 230 Training Loss: 0.6152901649475098 \n",
      "     Training Step: 231 Training Loss: 0.6127539277076721 \n",
      "     Training Step: 232 Training Loss: 0.6106806993484497 \n",
      "     Training Step: 233 Training Loss: 0.6182428598403931 \n",
      "     Training Step: 234 Training Loss: 0.6196424961090088 \n",
      "     Training Step: 235 Training Loss: 0.6114053726196289 \n",
      "     Training Step: 236 Training Loss: 0.6141481995582581 \n",
      "     Training Step: 237 Training Loss: 0.6130691170692444 \n",
      "     Training Step: 238 Training Loss: 0.6156803965568542 \n",
      "     Training Step: 239 Training Loss: 0.6136170029640198 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6106359362602234 \n",
      "     Validation Step: 1 Validation Loss: 0.6170202493667603 \n",
      "     Validation Step: 2 Validation Loss: 0.6148353219032288 \n",
      "     Validation Step: 3 Validation Loss: 0.6129857897758484 \n",
      "     Validation Step: 4 Validation Loss: 0.6173015832901001 \n",
      "     Validation Step: 5 Validation Loss: 0.6180606484413147 \n",
      "     Validation Step: 6 Validation Loss: 0.6162426471710205 \n",
      "     Validation Step: 7 Validation Loss: 0.6111811995506287 \n",
      "     Validation Step: 8 Validation Loss: 0.6118836998939514 \n",
      "     Validation Step: 9 Validation Loss: 0.6148924231529236 \n",
      "     Validation Step: 10 Validation Loss: 0.6116418242454529 \n",
      "     Validation Step: 11 Validation Loss: 0.6136693358421326 \n",
      "     Validation Step: 12 Validation Loss: 0.6101600527763367 \n",
      "     Validation Step: 13 Validation Loss: 0.6156156063079834 \n",
      "     Validation Step: 14 Validation Loss: 0.6121320724487305 \n",
      "     Validation Step: 15 Validation Loss: 0.6142531633377075 \n",
      "     Validation Step: 16 Validation Loss: 0.615227997303009 \n",
      "     Validation Step: 17 Validation Loss: 0.6141129732131958 \n",
      "     Validation Step: 18 Validation Loss: 0.6141332983970642 \n",
      "     Validation Step: 19 Validation Loss: 0.6155730485916138 \n",
      "     Validation Step: 20 Validation Loss: 0.6182286739349365 \n",
      "     Validation Step: 21 Validation Loss: 0.6184898614883423 \n",
      "     Validation Step: 22 Validation Loss: 0.6115607619285583 \n",
      "     Validation Step: 23 Validation Loss: 0.6133023500442505 \n",
      "     Validation Step: 24 Validation Loss: 0.6184661388397217 \n",
      "     Validation Step: 25 Validation Loss: 0.6153213977813721 \n",
      "     Validation Step: 26 Validation Loss: 0.6104859113693237 \n",
      "     Validation Step: 27 Validation Loss: 0.6157925724983215 \n",
      "     Validation Step: 28 Validation Loss: 0.6176961660385132 \n",
      "     Validation Step: 29 Validation Loss: 0.6105201244354248 \n",
      "     Validation Step: 30 Validation Loss: 0.6136302947998047 \n",
      "     Validation Step: 31 Validation Loss: 0.6111722588539124 \n",
      "     Validation Step: 32 Validation Loss: 0.6175903677940369 \n",
      "     Validation Step: 33 Validation Loss: 0.6075305938720703 \n",
      "     Validation Step: 34 Validation Loss: 0.6101189851760864 \n",
      "     Validation Step: 35 Validation Loss: 0.6142030358314514 \n",
      "     Validation Step: 36 Validation Loss: 0.615044116973877 \n",
      "     Validation Step: 37 Validation Loss: 0.6128268241882324 \n",
      "     Validation Step: 38 Validation Loss: 0.614533543586731 \n",
      "     Validation Step: 39 Validation Loss: 0.61834317445755 \n",
      "     Validation Step: 40 Validation Loss: 0.6159957051277161 \n",
      "     Validation Step: 41 Validation Loss: 0.614639401435852 \n",
      "     Validation Step: 42 Validation Loss: 0.6136573553085327 \n",
      "     Validation Step: 43 Validation Loss: 0.6101452112197876 \n",
      "     Validation Step: 44 Validation Loss: 0.6145688891410828 \n",
      "Epoch: 111\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6154755353927612 \n",
      "     Training Step: 1 Training Loss: 0.6105776429176331 \n",
      "     Training Step: 2 Training Loss: 0.6136419773101807 \n",
      "     Training Step: 3 Training Loss: 0.6133593320846558 \n",
      "     Training Step: 4 Training Loss: 0.616030752658844 \n",
      "     Training Step: 5 Training Loss: 0.6116082072257996 \n",
      "     Training Step: 6 Training Loss: 0.6128413677215576 \n",
      "     Training Step: 7 Training Loss: 0.6129157543182373 \n",
      "     Training Step: 8 Training Loss: 0.6197210550308228 \n",
      "     Training Step: 9 Training Loss: 0.6171729564666748 \n",
      "     Training Step: 10 Training Loss: 0.6169357895851135 \n",
      "     Training Step: 11 Training Loss: 0.6122283339500427 \n",
      "     Training Step: 12 Training Loss: 0.6157417297363281 \n",
      "     Training Step: 13 Training Loss: 0.6177554726600647 \n",
      "     Training Step: 14 Training Loss: 0.6142071485519409 \n",
      "     Training Step: 15 Training Loss: 0.6105917096138 \n",
      "     Training Step: 16 Training Loss: 0.6122583746910095 \n",
      "     Training Step: 17 Training Loss: 0.6130720973014832 \n",
      "     Training Step: 18 Training Loss: 0.6144233345985413 \n",
      "     Training Step: 19 Training Loss: 0.6109199523925781 \n",
      "     Training Step: 20 Training Loss: 0.612969696521759 \n",
      "     Training Step: 21 Training Loss: 0.612017035484314 \n",
      "     Training Step: 22 Training Loss: 0.6125342845916748 \n",
      "     Training Step: 23 Training Loss: 0.6118270754814148 \n",
      "     Training Step: 24 Training Loss: 0.6138153672218323 \n",
      "     Training Step: 25 Training Loss: 0.6120610237121582 \n",
      "     Training Step: 26 Training Loss: 0.6114176511764526 \n",
      "     Training Step: 27 Training Loss: 0.6167498826980591 \n",
      "     Training Step: 28 Training Loss: 0.6115115880966187 \n",
      "     Training Step: 29 Training Loss: 0.6118720769882202 \n",
      "     Training Step: 30 Training Loss: 0.6185033321380615 \n",
      "     Training Step: 31 Training Loss: 0.6152915954589844 \n",
      "     Training Step: 32 Training Loss: 0.6182572841644287 \n",
      "     Training Step: 33 Training Loss: 0.6153935790061951 \n",
      "     Training Step: 34 Training Loss: 0.6105902194976807 \n",
      "     Training Step: 35 Training Loss: 0.6107224225997925 \n",
      "     Training Step: 36 Training Loss: 0.6153051257133484 \n",
      "     Training Step: 37 Training Loss: 0.6143431663513184 \n",
      "     Training Step: 38 Training Loss: 0.6143768429756165 \n",
      "     Training Step: 39 Training Loss: 0.6134611368179321 \n",
      "     Training Step: 40 Training Loss: 0.6104111075401306 \n",
      "     Training Step: 41 Training Loss: 0.614698588848114 \n",
      "     Training Step: 42 Training Loss: 0.6180181503295898 \n",
      "     Training Step: 43 Training Loss: 0.6157428622245789 \n",
      "     Training Step: 44 Training Loss: 0.6116307377815247 \n",
      "     Training Step: 45 Training Loss: 0.610511064529419 \n",
      "     Training Step: 46 Training Loss: 0.6167551279067993 \n",
      "     Training Step: 47 Training Loss: 0.6147047877311707 \n",
      "     Training Step: 48 Training Loss: 0.6128736138343811 \n",
      "     Training Step: 49 Training Loss: 0.6133165955543518 \n",
      "     Training Step: 50 Training Loss: 0.6146734356880188 \n",
      "     Training Step: 51 Training Loss: 0.6158314347267151 \n",
      "     Training Step: 52 Training Loss: 0.6176412105560303 \n",
      "     Training Step: 53 Training Loss: 0.6114339232444763 \n",
      "     Training Step: 54 Training Loss: 0.6141056418418884 \n",
      "     Training Step: 55 Training Loss: 0.6103770732879639 \n",
      "     Training Step: 56 Training Loss: 0.6142849326133728 \n",
      "     Training Step: 57 Training Loss: 0.6115256547927856 \n",
      "     Training Step: 58 Training Loss: 0.6166738271713257 \n",
      "     Training Step: 59 Training Loss: 0.6146066188812256 \n",
      "     Training Step: 60 Training Loss: 0.6108769774436951 \n",
      "     Training Step: 61 Training Loss: 0.6180455684661865 \n",
      "     Training Step: 62 Training Loss: 0.6154223680496216 \n",
      "     Training Step: 63 Training Loss: 0.6121535897254944 \n",
      "     Training Step: 64 Training Loss: 0.6156793832778931 \n",
      "     Training Step: 65 Training Loss: 0.6146878004074097 \n",
      "     Training Step: 66 Training Loss: 0.6131466031074524 \n",
      "     Training Step: 67 Training Loss: 0.6173723936080933 \n",
      "     Training Step: 68 Training Loss: 0.6163450479507446 \n",
      "     Training Step: 69 Training Loss: 0.6140730977058411 \n",
      "     Training Step: 70 Training Loss: 0.6164814233779907 \n",
      "     Training Step: 71 Training Loss: 0.6097487211227417 \n",
      "     Training Step: 72 Training Loss: 0.6146388053894043 \n",
      "     Training Step: 73 Training Loss: 0.6122372150421143 \n",
      "     Training Step: 74 Training Loss: 0.6185621023178101 \n",
      "     Training Step: 75 Training Loss: 0.6138836145401001 \n",
      "     Training Step: 76 Training Loss: 0.6188235282897949 \n",
      "     Training Step: 77 Training Loss: 0.6144427061080933 \n",
      "     Training Step: 78 Training Loss: 0.6161976456642151 \n",
      "     Training Step: 79 Training Loss: 0.6141605973243713 \n",
      "     Training Step: 80 Training Loss: 0.615085244178772 \n",
      "     Training Step: 81 Training Loss: 0.6163992881774902 \n",
      "     Training Step: 82 Training Loss: 0.6198464035987854 \n",
      "     Training Step: 83 Training Loss: 0.6116102337837219 \n",
      "     Training Step: 84 Training Loss: 0.6176761984825134 \n",
      "     Training Step: 85 Training Loss: 0.6156859397888184 \n",
      "     Training Step: 86 Training Loss: 0.6107115745544434 \n",
      "     Training Step: 87 Training Loss: 0.6118459701538086 \n",
      "     Training Step: 88 Training Loss: 0.6083003878593445 \n",
      "     Training Step: 89 Training Loss: 0.6155136227607727 \n",
      "     Training Step: 90 Training Loss: 0.6157607436180115 \n",
      "     Training Step: 91 Training Loss: 0.6111394166946411 \n",
      "     Training Step: 92 Training Loss: 0.6118035316467285 \n",
      "     Training Step: 93 Training Loss: 0.6181272864341736 \n",
      "     Training Step: 94 Training Loss: 0.6099770069122314 \n",
      "     Training Step: 95 Training Loss: 0.6184931993484497 \n",
      "     Training Step: 96 Training Loss: 0.6153991222381592 \n",
      "     Training Step: 97 Training Loss: 0.6128649711608887 \n",
      "     Training Step: 98 Training Loss: 0.612372100353241 \n",
      "     Training Step: 99 Training Loss: 0.6144542694091797 \n",
      "     Training Step: 100 Training Loss: 0.6132808923721313 \n",
      "     Training Step: 101 Training Loss: 0.6100434064865112 \n",
      "     Training Step: 102 Training Loss: 0.6139209866523743 \n",
      "     Training Step: 103 Training Loss: 0.6159517765045166 \n",
      "     Training Step: 104 Training Loss: 0.6127623319625854 \n",
      "     Training Step: 105 Training Loss: 0.6093940734863281 \n",
      "     Training Step: 106 Training Loss: 0.6121381521224976 \n",
      "     Training Step: 107 Training Loss: 0.6115426421165466 \n",
      "     Training Step: 108 Training Loss: 0.6141570806503296 \n",
      "     Training Step: 109 Training Loss: 0.6184448003768921 \n",
      "     Training Step: 110 Training Loss: 0.6142065525054932 \n",
      "     Training Step: 111 Training Loss: 0.6168217062950134 \n",
      "     Training Step: 112 Training Loss: 0.6183464527130127 \n",
      "     Training Step: 113 Training Loss: 0.6167202591896057 \n",
      "     Training Step: 114 Training Loss: 0.6152852177619934 \n",
      "     Training Step: 115 Training Loss: 0.6154451966285706 \n",
      "     Training Step: 116 Training Loss: 0.6132135391235352 \n",
      "     Training Step: 117 Training Loss: 0.6146774888038635 \n",
      "     Training Step: 118 Training Loss: 0.6182027459144592 \n",
      "     Training Step: 119 Training Loss: 0.6142624020576477 \n",
      "     Training Step: 120 Training Loss: 0.6123340129852295 \n",
      "     Training Step: 121 Training Loss: 0.6152554750442505 \n",
      "     Training Step: 122 Training Loss: 0.6171234846115112 \n",
      "     Training Step: 123 Training Loss: 0.611232578754425 \n",
      "     Training Step: 124 Training Loss: 0.6167981624603271 \n",
      "     Training Step: 125 Training Loss: 0.6140193939208984 \n",
      "     Training Step: 126 Training Loss: 0.6135225296020508 \n",
      "     Training Step: 127 Training Loss: 0.6150964498519897 \n",
      "     Training Step: 128 Training Loss: 0.6136488914489746 \n",
      "     Training Step: 129 Training Loss: 0.6092352867126465 \n",
      "     Training Step: 130 Training Loss: 0.6126417517662048 \n",
      "     Training Step: 131 Training Loss: 0.6153775453567505 \n",
      "     Training Step: 132 Training Loss: 0.6132750511169434 \n",
      "     Training Step: 133 Training Loss: 0.6115158200263977 \n",
      "     Training Step: 134 Training Loss: 0.6168304085731506 \n",
      "     Training Step: 135 Training Loss: 0.6130558252334595 \n",
      "     Training Step: 136 Training Loss: 0.6117774844169617 \n",
      "     Training Step: 137 Training Loss: 0.6146131753921509 \n",
      "     Training Step: 138 Training Loss: 0.6140354871749878 \n",
      "     Training Step: 139 Training Loss: 0.612483561038971 \n",
      "     Training Step: 140 Training Loss: 0.6097201108932495 \n",
      "     Training Step: 141 Training Loss: 0.611639678478241 \n",
      "     Training Step: 142 Training Loss: 0.6164461374282837 \n",
      "     Training Step: 143 Training Loss: 0.6125025749206543 \n",
      "     Training Step: 144 Training Loss: 0.6150729060173035 \n",
      "     Training Step: 145 Training Loss: 0.6136097311973572 \n",
      "     Training Step: 146 Training Loss: 0.6132001876831055 \n",
      "     Training Step: 147 Training Loss: 0.6194553971290588 \n",
      "     Training Step: 148 Training Loss: 0.6122828125953674 \n",
      "     Training Step: 149 Training Loss: 0.6097299456596375 \n",
      "     Training Step: 150 Training Loss: 0.6101831197738647 \n",
      "     Training Step: 151 Training Loss: 0.6155350208282471 \n",
      "     Training Step: 152 Training Loss: 0.6180793642997742 \n",
      "     Training Step: 153 Training Loss: 0.6153357028961182 \n",
      "     Training Step: 154 Training Loss: 0.6134735345840454 \n",
      "     Training Step: 155 Training Loss: 0.6129156351089478 \n",
      "     Training Step: 156 Training Loss: 0.6188728213310242 \n",
      "     Training Step: 157 Training Loss: 0.6143438816070557 \n",
      "     Training Step: 158 Training Loss: 0.6177904009819031 \n",
      "     Training Step: 159 Training Loss: 0.6104995012283325 \n",
      "     Training Step: 160 Training Loss: 0.6149560809135437 \n",
      "     Training Step: 161 Training Loss: 0.6167101860046387 \n",
      "     Training Step: 162 Training Loss: 0.6114106774330139 \n",
      "     Training Step: 163 Training Loss: 0.6133034229278564 \n",
      "     Training Step: 164 Training Loss: 0.611489474773407 \n",
      "     Training Step: 165 Training Loss: 0.6144971251487732 \n",
      "     Training Step: 166 Training Loss: 0.610062837600708 \n",
      "     Training Step: 167 Training Loss: 0.6111580729484558 \n",
      "     Training Step: 168 Training Loss: 0.6126999855041504 \n",
      "     Training Step: 169 Training Loss: 0.6151800751686096 \n",
      "     Training Step: 170 Training Loss: 0.6160195469856262 \n",
      "     Training Step: 171 Training Loss: 0.6161322593688965 \n",
      "     Training Step: 172 Training Loss: 0.6149259805679321 \n",
      "     Training Step: 173 Training Loss: 0.6140279769897461 \n",
      "     Training Step: 174 Training Loss: 0.610569179058075 \n",
      "     Training Step: 175 Training Loss: 0.6107304692268372 \n",
      "     Training Step: 176 Training Loss: 0.6114128232002258 \n",
      "     Training Step: 177 Training Loss: 0.6146594285964966 \n",
      "     Training Step: 178 Training Loss: 0.6156130433082581 \n",
      "     Training Step: 179 Training Loss: 0.6151579022407532 \n",
      "     Training Step: 180 Training Loss: 0.6100497245788574 \n",
      "     Training Step: 181 Training Loss: 0.6131832003593445 \n",
      "     Training Step: 182 Training Loss: 0.6139075756072998 \n",
      "     Training Step: 183 Training Loss: 0.6171550750732422 \n",
      "     Training Step: 184 Training Loss: 0.6137401461601257 \n",
      "     Training Step: 185 Training Loss: 0.6125253438949585 \n",
      "     Training Step: 186 Training Loss: 0.6157879829406738 \n",
      "     Training Step: 187 Training Loss: 0.6128013134002686 \n",
      "     Training Step: 188 Training Loss: 0.6209444999694824 \n",
      "     Training Step: 189 Training Loss: 0.6166174411773682 \n",
      "     Training Step: 190 Training Loss: 0.612386167049408 \n",
      "     Training Step: 191 Training Loss: 0.6101551651954651 \n",
      "     Training Step: 192 Training Loss: 0.6148682236671448 \n",
      "     Training Step: 193 Training Loss: 0.6168898344039917 \n",
      "     Training Step: 194 Training Loss: 0.6121278405189514 \n",
      "     Training Step: 195 Training Loss: 0.6147950291633606 \n",
      "     Training Step: 196 Training Loss: 0.6134990453720093 \n",
      "     Training Step: 197 Training Loss: 0.6135883927345276 \n",
      "     Training Step: 198 Training Loss: 0.6185962557792664 \n",
      "     Training Step: 199 Training Loss: 0.6166731715202332 \n",
      "     Training Step: 200 Training Loss: 0.6155313849449158 \n",
      "     Training Step: 201 Training Loss: 0.6116929650306702 \n",
      "     Training Step: 202 Training Loss: 0.6114223003387451 \n",
      "     Training Step: 203 Training Loss: 0.6166380047798157 \n",
      "     Training Step: 204 Training Loss: 0.6146038770675659 \n",
      "     Training Step: 205 Training Loss: 0.6146698594093323 \n",
      "     Training Step: 206 Training Loss: 0.6127615571022034 \n",
      "     Training Step: 207 Training Loss: 0.6152878403663635 \n",
      "     Training Step: 208 Training Loss: 0.6143358945846558 \n",
      "     Training Step: 209 Training Loss: 0.6116406321525574 \n",
      "     Training Step: 210 Training Loss: 0.6147283911705017 \n",
      "     Training Step: 211 Training Loss: 0.6174833178520203 \n",
      "     Training Step: 212 Training Loss: 0.6176849603652954 \n",
      "     Training Step: 213 Training Loss: 0.6137661933898926 \n",
      "     Training Step: 214 Training Loss: 0.6154128313064575 \n",
      "     Training Step: 215 Training Loss: 0.6162365674972534 \n",
      "     Training Step: 216 Training Loss: 0.6170855164527893 \n",
      "     Training Step: 217 Training Loss: 0.6171836853027344 \n",
      "     Training Step: 218 Training Loss: 0.6134271621704102 \n",
      "     Training Step: 219 Training Loss: 0.6124762296676636 \n",
      "     Training Step: 220 Training Loss: 0.6201925277709961 \n",
      "     Training Step: 221 Training Loss: 0.6124041676521301 \n",
      "     Training Step: 222 Training Loss: 0.6147480607032776 \n",
      "     Training Step: 223 Training Loss: 0.6149340271949768 \n",
      "     Training Step: 224 Training Loss: 0.6195903420448303 \n",
      "     Training Step: 225 Training Loss: 0.6133004426956177 \n",
      "     Training Step: 226 Training Loss: 0.6137468814849854 \n",
      "     Training Step: 227 Training Loss: 0.6144795417785645 \n",
      "     Training Step: 228 Training Loss: 0.6176890730857849 \n",
      "     Training Step: 229 Training Loss: 0.6131289005279541 \n",
      "     Training Step: 230 Training Loss: 0.6177965402603149 \n",
      "     Training Step: 231 Training Loss: 0.6166437268257141 \n",
      "     Training Step: 232 Training Loss: 0.6147735714912415 \n",
      "     Training Step: 233 Training Loss: 0.6167094707489014 \n",
      "     Training Step: 234 Training Loss: 0.6094890236854553 \n",
      "     Training Step: 235 Training Loss: 0.6161937713623047 \n",
      "     Training Step: 236 Training Loss: 0.6118425726890564 \n",
      "     Training Step: 237 Training Loss: 0.6118043661117554 \n",
      "     Training Step: 238 Training Loss: 0.610666036605835 \n",
      "     Training Step: 239 Training Loss: 0.6121596693992615 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6111724972724915 \n",
      "     Validation Step: 1 Validation Loss: 0.6104757785797119 \n",
      "     Validation Step: 2 Validation Loss: 0.6074996590614319 \n",
      "     Validation Step: 3 Validation Loss: 0.6153251528739929 \n",
      "     Validation Step: 4 Validation Loss: 0.6173182725906372 \n",
      "     Validation Step: 5 Validation Loss: 0.6111570596694946 \n",
      "     Validation Step: 6 Validation Loss: 0.6146377921104431 \n",
      "     Validation Step: 7 Validation Loss: 0.6136332154273987 \n",
      "     Validation Step: 8 Validation Loss: 0.6177088022232056 \n",
      "     Validation Step: 9 Validation Loss: 0.6148377656936646 \n",
      "     Validation Step: 10 Validation Loss: 0.6121222376823425 \n",
      "     Validation Step: 11 Validation Loss: 0.6100949048995972 \n",
      "     Validation Step: 12 Validation Loss: 0.6148973107337952 \n",
      "     Validation Step: 13 Validation Loss: 0.6141287088394165 \n",
      "     Validation Step: 14 Validation Loss: 0.618485152721405 \n",
      "     Validation Step: 15 Validation Loss: 0.6133018732070923 \n",
      "     Validation Step: 16 Validation Loss: 0.6152337193489075 \n",
      "     Validation Step: 17 Validation Loss: 0.6142003536224365 \n",
      "     Validation Step: 18 Validation Loss: 0.6150466799736023 \n",
      "     Validation Step: 19 Validation Loss: 0.615798830986023 \n",
      "     Validation Step: 20 Validation Loss: 0.6162483096122742 \n",
      "     Validation Step: 21 Validation Loss: 0.6155825853347778 \n",
      "     Validation Step: 22 Validation Loss: 0.6116341948509216 \n",
      "     Validation Step: 23 Validation Loss: 0.6176049113273621 \n",
      "     Validation Step: 24 Validation Loss: 0.6160068511962891 \n",
      "     Validation Step: 25 Validation Loss: 0.6145364046096802 \n",
      "     Validation Step: 26 Validation Loss: 0.6136576533317566 \n",
      "     Validation Step: 27 Validation Loss: 0.617038369178772 \n",
      "     Validation Step: 28 Validation Loss: 0.6182448267936707 \n",
      "     Validation Step: 29 Validation Loss: 0.6180756688117981 \n",
      "     Validation Step: 30 Validation Loss: 0.6106206178665161 \n",
      "     Validation Step: 31 Validation Loss: 0.6156216859817505 \n",
      "     Validation Step: 32 Validation Loss: 0.610120415687561 \n",
      "     Validation Step: 33 Validation Loss: 0.6128233075141907 \n",
      "     Validation Step: 34 Validation Loss: 0.6145761609077454 \n",
      "     Validation Step: 35 Validation Loss: 0.6115475296974182 \n",
      "     Validation Step: 36 Validation Loss: 0.610507071018219 \n",
      "     Validation Step: 37 Validation Loss: 0.6141133904457092 \n",
      "     Validation Step: 38 Validation Loss: 0.6142555475234985 \n",
      "     Validation Step: 39 Validation Loss: 0.6185099482536316 \n",
      "     Validation Step: 40 Validation Loss: 0.6129806041717529 \n",
      "     Validation Step: 41 Validation Loss: 0.611873209476471 \n",
      "     Validation Step: 42 Validation Loss: 0.610148549079895 \n",
      "     Validation Step: 43 Validation Loss: 0.6183568239212036 \n",
      "     Validation Step: 44 Validation Loss: 0.6136668920516968 \n",
      "Epoch: 112\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6185954213142395 \n",
      "     Training Step: 1 Training Loss: 0.6114119291305542 \n",
      "     Training Step: 2 Training Loss: 0.6126403212547302 \n",
      "     Training Step: 3 Training Loss: 0.6156184077262878 \n",
      "     Training Step: 4 Training Loss: 0.6154031157493591 \n",
      "     Training Step: 5 Training Loss: 0.6141518354415894 \n",
      "     Training Step: 6 Training Loss: 0.6117761135101318 \n",
      "     Training Step: 7 Training Loss: 0.6163636445999146 \n",
      "     Training Step: 8 Training Loss: 0.6120131015777588 \n",
      "     Training Step: 9 Training Loss: 0.6184225678443909 \n",
      "     Training Step: 10 Training Loss: 0.6132808923721313 \n",
      "     Training Step: 11 Training Loss: 0.6134589314460754 \n",
      "     Training Step: 12 Training Loss: 0.6129652261734009 \n",
      "     Training Step: 13 Training Loss: 0.6115291714668274 \n",
      "     Training Step: 14 Training Loss: 0.6160318851470947 \n",
      "     Training Step: 15 Training Loss: 0.6145969033241272 \n",
      "     Training Step: 16 Training Loss: 0.6092329025268555 \n",
      "     Training Step: 17 Training Loss: 0.6171989440917969 \n",
      "     Training Step: 18 Training Loss: 0.6121573448181152 \n",
      "     Training Step: 19 Training Loss: 0.6146154999732971 \n",
      "     Training Step: 20 Training Loss: 0.6135974526405334 \n",
      "     Training Step: 21 Training Loss: 0.6100441813468933 \n",
      "     Training Step: 22 Training Loss: 0.6108806133270264 \n",
      "     Training Step: 23 Training Loss: 0.6097167134284973 \n",
      "     Training Step: 24 Training Loss: 0.6209778785705566 \n",
      "     Training Step: 25 Training Loss: 0.615800142288208 \n",
      "     Training Step: 26 Training Loss: 0.6177133321762085 \n",
      "     Training Step: 27 Training Loss: 0.6125273108482361 \n",
      "     Training Step: 28 Training Loss: 0.6171744465827942 \n",
      "     Training Step: 29 Training Loss: 0.6121204495429993 \n",
      "     Training Step: 30 Training Loss: 0.6127517223358154 \n",
      "     Training Step: 31 Training Loss: 0.6115871667861938 \n",
      "     Training Step: 32 Training Loss: 0.6151014566421509 \n",
      "     Training Step: 33 Training Loss: 0.6146795749664307 \n",
      "     Training Step: 34 Training Loss: 0.6136201620101929 \n",
      "     Training Step: 35 Training Loss: 0.6122344136238098 \n",
      "     Training Step: 36 Training Loss: 0.618310272693634 \n",
      "     Training Step: 37 Training Loss: 0.6177563071250916 \n",
      "     Training Step: 38 Training Loss: 0.6152876615524292 \n",
      "     Training Step: 39 Training Loss: 0.611842930316925 \n",
      "     Training Step: 40 Training Loss: 0.6140422821044922 \n",
      "     Training Step: 41 Training Loss: 0.6196799278259277 \n",
      "     Training Step: 42 Training Loss: 0.6144242882728577 \n",
      "     Training Step: 43 Training Loss: 0.6147297024726868 \n",
      "     Training Step: 44 Training Loss: 0.6144992709159851 \n",
      "     Training Step: 45 Training Loss: 0.6114271283149719 \n",
      "     Training Step: 46 Training Loss: 0.611499547958374 \n",
      "     Training Step: 47 Training Loss: 0.6097440123558044 \n",
      "     Training Step: 48 Training Loss: 0.6122862100601196 \n",
      "     Training Step: 49 Training Loss: 0.613923192024231 \n",
      "     Training Step: 50 Training Loss: 0.6115498542785645 \n",
      "     Training Step: 51 Training Loss: 0.6143578290939331 \n",
      "     Training Step: 52 Training Loss: 0.61351078748703 \n",
      "     Training Step: 53 Training Loss: 0.6143594980239868 \n",
      "     Training Step: 54 Training Loss: 0.6133154630661011 \n",
      "     Training Step: 55 Training Loss: 0.610454797744751 \n",
      "     Training Step: 56 Training Loss: 0.6137775778770447 \n",
      "     Training Step: 57 Training Loss: 0.6107262969017029 \n",
      "     Training Step: 58 Training Loss: 0.618496835231781 \n",
      "     Training Step: 59 Training Loss: 0.6122251749038696 \n",
      "     Training Step: 60 Training Loss: 0.614666223526001 \n",
      "     Training Step: 61 Training Loss: 0.6103853583335876 \n",
      "     Training Step: 62 Training Loss: 0.6128418445587158 \n",
      "     Training Step: 63 Training Loss: 0.6154312491416931 \n",
      "     Training Step: 64 Training Loss: 0.6162110567092896 \n",
      "     Training Step: 65 Training Loss: 0.6166287660598755 \n",
      "     Training Step: 66 Training Loss: 0.6202262043952942 \n",
      "     Training Step: 67 Training Loss: 0.6128080487251282 \n",
      "     Training Step: 68 Training Loss: 0.6136539578437805 \n",
      "     Training Step: 69 Training Loss: 0.6144486665725708 \n",
      "     Training Step: 70 Training Loss: 0.6138301491737366 \n",
      "     Training Step: 71 Training Loss: 0.6170847415924072 \n",
      "     Training Step: 72 Training Loss: 0.611551821231842 \n",
      "     Training Step: 73 Training Loss: 0.6111826300621033 \n",
      "     Training Step: 74 Training Loss: 0.6198472380638123 \n",
      "     Training Step: 75 Training Loss: 0.6169103384017944 \n",
      "     Training Step: 76 Training Loss: 0.6136372089385986 \n",
      "     Training Step: 77 Training Loss: 0.6149178743362427 \n",
      "     Training Step: 78 Training Loss: 0.6105958819389343 \n",
      "     Training Step: 79 Training Loss: 0.612500011920929 \n",
      "     Training Step: 80 Training Loss: 0.6129088997840881 \n",
      "     Training Step: 81 Training Loss: 0.6150846481323242 \n",
      "     Training Step: 82 Training Loss: 0.6166784167289734 \n",
      "     Training Step: 83 Training Loss: 0.6152611970901489 \n",
      "     Training Step: 84 Training Loss: 0.619450569152832 \n",
      "     Training Step: 85 Training Loss: 0.6125322580337524 \n",
      "     Training Step: 86 Training Loss: 0.612379789352417 \n",
      "     Training Step: 87 Training Loss: 0.6118884682655334 \n",
      "     Training Step: 88 Training Loss: 0.6118404269218445 \n",
      "     Training Step: 89 Training Loss: 0.6117990016937256 \n",
      "     Training Step: 90 Training Loss: 0.6122910380363464 \n",
      "     Training Step: 91 Training Loss: 0.616816520690918 \n",
      "     Training Step: 92 Training Loss: 0.6186266541481018 \n",
      "     Training Step: 93 Training Loss: 0.6140827536582947 \n",
      "     Training Step: 94 Training Loss: 0.6150702238082886 \n",
      "     Training Step: 95 Training Loss: 0.618230938911438 \n",
      "     Training Step: 96 Training Loss: 0.6105015873908997 \n",
      "     Training Step: 97 Training Loss: 0.6151512265205383 \n",
      "     Training Step: 98 Training Loss: 0.6159437298774719 \n",
      "     Training Step: 99 Training Loss: 0.6103934645652771 \n",
      "     Training Step: 100 Training Loss: 0.6127690076828003 \n",
      "     Training Step: 101 Training Loss: 0.6146060228347778 \n",
      "     Training Step: 102 Training Loss: 0.6132039427757263 \n",
      "     Training Step: 103 Training Loss: 0.6105758547782898 \n",
      "     Training Step: 104 Training Loss: 0.6188364028930664 \n",
      "     Training Step: 105 Training Loss: 0.6140133142471313 \n",
      "     Training Step: 106 Training Loss: 0.6118299961090088 \n",
      "     Training Step: 107 Training Loss: 0.6116865873336792 \n",
      "     Training Step: 108 Training Loss: 0.6142475605010986 \n",
      "     Training Step: 109 Training Loss: 0.6125181317329407 \n",
      "     Training Step: 110 Training Loss: 0.6111335754394531 \n",
      "     Training Step: 111 Training Loss: 0.615768551826477 \n",
      "     Training Step: 112 Training Loss: 0.6161311268806458 \n",
      "     Training Step: 113 Training Loss: 0.6156996488571167 \n",
      "     Training Step: 114 Training Loss: 0.6128635406494141 \n",
      "     Training Step: 115 Training Loss: 0.6151783466339111 \n",
      "     Training Step: 116 Training Loss: 0.6143743991851807 \n",
      "     Training Step: 117 Training Loss: 0.6167201995849609 \n",
      "     Training Step: 118 Training Loss: 0.6101421117782593 \n",
      "     Training Step: 119 Training Loss: 0.6094107031822205 \n",
      "     Training Step: 120 Training Loss: 0.6174848675727844 \n",
      "     Training Step: 121 Training Loss: 0.6114001274108887 \n",
      "     Training Step: 122 Training Loss: 0.6180685758590698 \n",
      "     Training Step: 123 Training Loss: 0.6142086386680603 \n",
      "     Training Step: 124 Training Loss: 0.6153331398963928 \n",
      "     Training Step: 125 Training Loss: 0.6167579293251038 \n",
      "     Training Step: 126 Training Loss: 0.6155340075492859 \n",
      "     Training Step: 127 Training Loss: 0.6106735467910767 \n",
      "     Training Step: 128 Training Loss: 0.6154375672340393 \n",
      "     Training Step: 129 Training Loss: 0.6124704480171204 \n",
      "     Training Step: 130 Training Loss: 0.6159945726394653 \n",
      "     Training Step: 131 Training Loss: 0.6116390228271484 \n",
      "     Training Step: 132 Training Loss: 0.6147445440292358 \n",
      "     Training Step: 133 Training Loss: 0.6112052202224731 \n",
      "     Training Step: 134 Training Loss: 0.6100460290908813 \n",
      "     Training Step: 135 Training Loss: 0.6132760047912598 \n",
      "     Training Step: 136 Training Loss: 0.6173936724662781 \n",
      "     Training Step: 137 Training Loss: 0.6126959919929504 \n",
      "     Training Step: 138 Training Loss: 0.6143467426300049 \n",
      "     Training Step: 139 Training Loss: 0.6131128072738647 \n",
      "     Training Step: 140 Training Loss: 0.6178330183029175 \n",
      "     Training Step: 141 Training Loss: 0.6114232540130615 \n",
      "     Training Step: 142 Training Loss: 0.6116170883178711 \n",
      "     Training Step: 143 Training Loss: 0.6134117245674133 \n",
      "     Training Step: 144 Training Loss: 0.6167203783988953 \n",
      "     Training Step: 145 Training Loss: 0.6130664348602295 \n",
      "     Training Step: 146 Training Loss: 0.6128699779510498 \n",
      "     Training Step: 147 Training Loss: 0.6140213012695312 \n",
      "     Training Step: 148 Training Loss: 0.609700620174408 \n",
      "     Training Step: 149 Training Loss: 0.6122262477874756 \n",
      "     Training Step: 150 Training Loss: 0.6184718608856201 \n",
      "     Training Step: 151 Training Loss: 0.6116068363189697 \n",
      "     Training Step: 152 Training Loss: 0.6146379709243774 \n",
      "     Training Step: 153 Training Loss: 0.6144510507583618 \n",
      "     Training Step: 154 Training Loss: 0.6131356954574585 \n",
      "     Training Step: 155 Training Loss: 0.6166771054267883 \n",
      "     Training Step: 156 Training Loss: 0.6157451272010803 \n",
      "     Training Step: 157 Training Loss: 0.6137375831604004 \n",
      "     Training Step: 158 Training Loss: 0.617688775062561 \n",
      "     Training Step: 159 Training Loss: 0.6177896857261658 \n",
      "     Training Step: 160 Training Loss: 0.6123806834220886 \n",
      "     Training Step: 161 Training Loss: 0.6118091940879822 \n",
      "     Training Step: 162 Training Loss: 0.6161978244781494 \n",
      "     Training Step: 163 Training Loss: 0.6141505241394043 \n",
      "     Training Step: 164 Training Loss: 0.6180220246315002 \n",
      "     Training Step: 165 Training Loss: 0.618852436542511 \n",
      "     Training Step: 166 Training Loss: 0.6133275628089905 \n",
      "     Training Step: 167 Training Loss: 0.6137486696243286 \n",
      "     Training Step: 168 Training Loss: 0.6152887940406799 \n",
      "     Training Step: 169 Training Loss: 0.6166387796401978 \n",
      "     Training Step: 170 Training Loss: 0.6101064085960388 \n",
      "     Training Step: 171 Training Loss: 0.6180844306945801 \n",
      "     Training Step: 172 Training Loss: 0.6157389283180237 \n",
      "     Training Step: 173 Training Loss: 0.6139212846755981 \n",
      "     Training Step: 174 Training Loss: 0.6130689978599548 \n",
      "     Training Step: 175 Training Loss: 0.614479124546051 \n",
      "     Training Step: 176 Training Loss: 0.6154748201370239 \n",
      "     Training Step: 177 Training Loss: 0.6132069230079651 \n",
      "     Training Step: 178 Training Loss: 0.6106005907058716 \n",
      "     Training Step: 179 Training Loss: 0.6132910251617432 \n",
      "     Training Step: 180 Training Loss: 0.610173225402832 \n",
      "     Training Step: 181 Training Loss: 0.6147774457931519 \n",
      "     Training Step: 182 Training Loss: 0.6121572256088257 \n",
      "     Training Step: 183 Training Loss: 0.6171754002571106 \n",
      "     Training Step: 184 Training Loss: 0.6177507638931274 \n",
      "     Training Step: 185 Training Loss: 0.6182553172111511 \n",
      "     Training Step: 186 Training Loss: 0.6169083714485168 \n",
      "     Training Step: 187 Training Loss: 0.6155262589454651 \n",
      "     Training Step: 188 Training Loss: 0.6168039441108704 \n",
      "     Training Step: 189 Training Loss: 0.6116527915000916 \n",
      "     Training Step: 190 Training Loss: 0.6107481122016907 \n",
      "     Training Step: 191 Training Loss: 0.6134787201881409 \n",
      "     Training Step: 192 Training Loss: 0.6094968914985657 \n",
      "     Training Step: 193 Training Loss: 0.6115382313728333 \n",
      "     Training Step: 194 Training Loss: 0.613501250743866 \n",
      "     Training Step: 195 Training Loss: 0.6156849265098572 \n",
      "     Training Step: 196 Training Loss: 0.6166532039642334 \n",
      "     Training Step: 197 Training Loss: 0.6158313751220703 \n",
      "     Training Step: 198 Training Loss: 0.6147063970565796 \n",
      "     Training Step: 199 Training Loss: 0.6152840256690979 \n",
      "     Training Step: 200 Training Loss: 0.619629979133606 \n",
      "     Training Step: 201 Training Loss: 0.6153759956359863 \n",
      "     Training Step: 202 Training Loss: 0.6141035556793213 \n",
      "     Training Step: 203 Training Loss: 0.610893726348877 \n",
      "     Training Step: 204 Training Loss: 0.6164920926094055 \n",
      "     Training Step: 205 Training Loss: 0.6106991767883301 \n",
      "     Training Step: 206 Training Loss: 0.6100038290023804 \n",
      "     Training Step: 207 Training Loss: 0.6082735657691956 \n",
      "     Training Step: 208 Training Loss: 0.6153985261917114 \n",
      "     Training Step: 209 Training Loss: 0.6176547408103943 \n",
      "     Training Step: 210 Training Loss: 0.6149623394012451 \n",
      "     Training Step: 211 Training Loss: 0.6180368661880493 \n",
      "     Training Step: 212 Training Loss: 0.6147943139076233 \n",
      "     Training Step: 213 Training Loss: 0.6154224872589111 \n",
      "     Training Step: 214 Training Loss: 0.613358199596405 \n",
      "     Training Step: 215 Training Loss: 0.614660382270813 \n",
      "     Training Step: 216 Training Loss: 0.6148685216903687 \n",
      "     Training Step: 217 Training Loss: 0.6167089343070984 \n",
      "     Training Step: 218 Training Loss: 0.6114542484283447 \n",
      "     Training Step: 219 Training Loss: 0.6149320602416992 \n",
      "     Training Step: 220 Training Loss: 0.6171317100524902 \n",
      "     Training Step: 221 Training Loss: 0.6142836213111877 \n",
      "     Training Step: 222 Training Loss: 0.6129318475723267 \n",
      "     Training Step: 223 Training Loss: 0.6146866679191589 \n",
      "     Training Step: 224 Training Loss: 0.6167135238647461 \n",
      "     Training Step: 225 Training Loss: 0.6152440309524536 \n",
      "     Training Step: 226 Training Loss: 0.6168006062507629 \n",
      "     Training Step: 227 Training Loss: 0.613190770149231 \n",
      "     Training Step: 228 Training Loss: 0.6162353754043579 \n",
      "     Training Step: 229 Training Loss: 0.6164072751998901 \n",
      "     Training Step: 230 Training Loss: 0.6120779514312744 \n",
      "     Training Step: 231 Training Loss: 0.6138849854469299 \n",
      "     Training Step: 232 Training Loss: 0.612388551235199 \n",
      "     Training Step: 233 Training Loss: 0.6147012710571289 \n",
      "     Training Step: 234 Training Loss: 0.6146484017372131 \n",
      "     Training Step: 235 Training Loss: 0.6164160370826721 \n",
      "     Training Step: 236 Training Loss: 0.6155300140380859 \n",
      "     Training Step: 237 Training Loss: 0.6121398210525513 \n",
      "     Training Step: 238 Training Loss: 0.6142043471336365 \n",
      "     Training Step: 239 Training Loss: 0.6105881333351135 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6183383464813232 \n",
      "     Validation Step: 1 Validation Loss: 0.6129837036132812 \n",
      "     Validation Step: 2 Validation Loss: 0.6180622577667236 \n",
      "     Validation Step: 3 Validation Loss: 0.6182268261909485 \n",
      "     Validation Step: 4 Validation Loss: 0.6155714988708496 \n",
      "     Validation Step: 5 Validation Loss: 0.611642599105835 \n",
      "     Validation Step: 6 Validation Loss: 0.6162427067756653 \n",
      "     Validation Step: 7 Validation Loss: 0.6176021099090576 \n",
      "     Validation Step: 8 Validation Loss: 0.6106292605400085 \n",
      "     Validation Step: 9 Validation Loss: 0.6157873868942261 \n",
      "     Validation Step: 10 Validation Loss: 0.6111576557159424 \n",
      "     Validation Step: 11 Validation Loss: 0.6075190901756287 \n",
      "     Validation Step: 12 Validation Loss: 0.6145375370979309 \n",
      "     Validation Step: 13 Validation Loss: 0.6184760332107544 \n",
      "     Validation Step: 14 Validation Loss: 0.6133100390434265 \n",
      "     Validation Step: 15 Validation Loss: 0.6148352026939392 \n",
      "     Validation Step: 16 Validation Loss: 0.6136449575424194 \n",
      "     Validation Step: 17 Validation Loss: 0.6111767292022705 \n",
      "     Validation Step: 18 Validation Loss: 0.614124596118927 \n",
      "     Validation Step: 19 Validation Loss: 0.6184957027435303 \n",
      "     Validation Step: 20 Validation Loss: 0.6156119108200073 \n",
      "     Validation Step: 21 Validation Loss: 0.6148977875709534 \n",
      "     Validation Step: 22 Validation Loss: 0.6152259707450867 \n",
      "     Validation Step: 23 Validation Loss: 0.6136629581451416 \n",
      "     Validation Step: 24 Validation Loss: 0.6104819774627686 \n",
      "     Validation Step: 25 Validation Loss: 0.6159879565238953 \n",
      "     Validation Step: 26 Validation Loss: 0.613641083240509 \n",
      "     Validation Step: 27 Validation Loss: 0.6173083186149597 \n",
      "     Validation Step: 28 Validation Loss: 0.6101344227790833 \n",
      "     Validation Step: 29 Validation Loss: 0.6150320172309875 \n",
      "     Validation Step: 30 Validation Loss: 0.6145718693733215 \n",
      "     Validation Step: 31 Validation Loss: 0.6128324270248413 \n",
      "     Validation Step: 32 Validation Loss: 0.6170238852500916 \n",
      "     Validation Step: 33 Validation Loss: 0.6121277809143066 \n",
      "     Validation Step: 34 Validation Loss: 0.6142019629478455 \n",
      "     Validation Step: 35 Validation Loss: 0.6176905035972595 \n",
      "     Validation Step: 36 Validation Loss: 0.6101052761077881 \n",
      "     Validation Step: 37 Validation Loss: 0.610163152217865 \n",
      "     Validation Step: 38 Validation Loss: 0.6146376132965088 \n",
      "     Validation Step: 39 Validation Loss: 0.6118792295455933 \n",
      "     Validation Step: 40 Validation Loss: 0.6141161918640137 \n",
      "     Validation Step: 41 Validation Loss: 0.6142585277557373 \n",
      "     Validation Step: 42 Validation Loss: 0.6105131506919861 \n",
      "     Validation Step: 43 Validation Loss: 0.6115536689758301 \n",
      "     Validation Step: 44 Validation Loss: 0.6153160333633423 \n",
      "Epoch: 113\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6143369078636169 \n",
      "     Training Step: 1 Training Loss: 0.6167622208595276 \n",
      "     Training Step: 2 Training Loss: 0.6164036393165588 \n",
      "     Training Step: 3 Training Loss: 0.6183145642280579 \n",
      "     Training Step: 4 Training Loss: 0.6123917102813721 \n",
      "     Training Step: 5 Training Loss: 0.6155111193656921 \n",
      "     Training Step: 6 Training Loss: 0.6106035709381104 \n",
      "     Training Step: 7 Training Loss: 0.6118938326835632 \n",
      "     Training Step: 8 Training Loss: 0.6127160787582397 \n",
      "     Training Step: 9 Training Loss: 0.6126441955566406 \n",
      "     Training Step: 10 Training Loss: 0.6171455383300781 \n",
      "     Training Step: 11 Training Loss: 0.6140154004096985 \n",
      "     Training Step: 12 Training Loss: 0.620940625667572 \n",
      "     Training Step: 13 Training Loss: 0.6177973747253418 \n",
      "     Training Step: 14 Training Loss: 0.6153796911239624 \n",
      "     Training Step: 15 Training Loss: 0.6180904507637024 \n",
      "     Training Step: 16 Training Loss: 0.6146653890609741 \n",
      "     Training Step: 17 Training Loss: 0.612480878829956 \n",
      "     Training Step: 18 Training Loss: 0.618384838104248 \n",
      "     Training Step: 19 Training Loss: 0.6157421469688416 \n",
      "     Training Step: 20 Training Loss: 0.6201834678649902 \n",
      "     Training Step: 21 Training Loss: 0.6177856922149658 \n",
      "     Training Step: 22 Training Loss: 0.6167934536933899 \n",
      "     Training Step: 23 Training Loss: 0.6109598278999329 \n",
      "     Training Step: 24 Training Loss: 0.614477276802063 \n",
      "     Training Step: 25 Training Loss: 0.6168797612190247 \n",
      "     Training Step: 26 Training Loss: 0.6118690967559814 \n",
      "     Training Step: 27 Training Loss: 0.6171298027038574 \n",
      "     Training Step: 28 Training Loss: 0.6161136031150818 \n",
      "     Training Step: 29 Training Loss: 0.6100819706916809 \n",
      "     Training Step: 30 Training Loss: 0.6136503219604492 \n",
      "     Training Step: 31 Training Loss: 0.615599513053894 \n",
      "     Training Step: 32 Training Loss: 0.6180427074432373 \n",
      "     Training Step: 33 Training Loss: 0.6113989949226379 \n",
      "     Training Step: 34 Training Loss: 0.6103713512420654 \n",
      "     Training Step: 35 Training Loss: 0.6149343848228455 \n",
      "     Training Step: 36 Training Loss: 0.616222083568573 \n",
      "     Training Step: 37 Training Loss: 0.6100295782089233 \n",
      "     Training Step: 38 Training Loss: 0.6127560138702393 \n",
      "     Training Step: 39 Training Loss: 0.6113996505737305 \n",
      "     Training Step: 40 Training Loss: 0.6177103519439697 \n",
      "     Training Step: 41 Training Loss: 0.6107091903686523 \n",
      "     Training Step: 42 Training Loss: 0.6117792725563049 \n",
      "     Training Step: 43 Training Loss: 0.6137384176254272 \n",
      "     Training Step: 44 Training Loss: 0.6132740378379822 \n",
      "     Training Step: 45 Training Loss: 0.6149266958236694 \n",
      "     Training Step: 46 Training Loss: 0.6127611994743347 \n",
      "     Training Step: 47 Training Loss: 0.6097041964530945 \n",
      "     Training Step: 48 Training Loss: 0.6134979724884033 \n",
      "     Training Step: 49 Training Loss: 0.6147103905677795 \n",
      "     Training Step: 50 Training Loss: 0.6116199493408203 \n",
      "     Training Step: 51 Training Loss: 0.6188969612121582 \n",
      "     Training Step: 52 Training Loss: 0.616249144077301 \n",
      "     Training Step: 53 Training Loss: 0.6152423024177551 \n",
      "     Training Step: 54 Training Loss: 0.6171656250953674 \n",
      "     Training Step: 55 Training Loss: 0.6166474223136902 \n",
      "     Training Step: 56 Training Loss: 0.6182335615158081 \n",
      "     Training Step: 57 Training Loss: 0.6166039705276489 \n",
      "     Training Step: 58 Training Loss: 0.6123265624046326 \n",
      "     Training Step: 59 Training Loss: 0.6167085766792297 \n",
      "     Training Step: 60 Training Loss: 0.6128265857696533 \n",
      "     Training Step: 61 Training Loss: 0.6176722049713135 \n",
      "     Training Step: 62 Training Loss: 0.611477255821228 \n",
      "     Training Step: 63 Training Loss: 0.6132080554962158 \n",
      "     Training Step: 64 Training Loss: 0.6094503402709961 \n",
      "     Training Step: 65 Training Loss: 0.6188161373138428 \n",
      "     Training Step: 66 Training Loss: 0.6155301928520203 \n",
      "     Training Step: 67 Training Loss: 0.6152955293655396 \n",
      "     Training Step: 68 Training Loss: 0.6146472692489624 \n",
      "     Training Step: 69 Training Loss: 0.6137474179267883 \n",
      "     Training Step: 70 Training Loss: 0.6129106283187866 \n",
      "     Training Step: 71 Training Loss: 0.6122419834136963 \n",
      "     Training Step: 72 Training Loss: 0.6106859445571899 \n",
      "     Training Step: 73 Training Loss: 0.6139140129089355 \n",
      "     Training Step: 74 Training Loss: 0.6118270754814148 \n",
      "     Training Step: 75 Training Loss: 0.6155573129653931 \n",
      "     Training Step: 76 Training Loss: 0.6099746823310852 \n",
      "     Training Step: 77 Training Loss: 0.6142969131469727 \n",
      "     Training Step: 78 Training Loss: 0.6114639043807983 \n",
      "     Training Step: 79 Training Loss: 0.6148831248283386 \n",
      "     Training Step: 80 Training Loss: 0.6124851107597351 \n",
      "     Training Step: 81 Training Loss: 0.6166746616363525 \n",
      "     Training Step: 82 Training Loss: 0.6151120662689209 \n",
      "     Training Step: 83 Training Loss: 0.6174991726875305 \n",
      "     Training Step: 84 Training Loss: 0.6133126616477966 \n",
      "     Training Step: 85 Training Loss: 0.6147719621658325 \n",
      "     Training Step: 86 Training Loss: 0.6130709648132324 \n",
      "     Training Step: 87 Training Loss: 0.6122949719429016 \n",
      "     Training Step: 88 Training Loss: 0.6140391826629639 \n",
      "     Training Step: 89 Training Loss: 0.6123816967010498 \n",
      "     Training Step: 90 Training Loss: 0.6147249341011047 \n",
      "     Training Step: 91 Training Loss: 0.61207515001297 \n",
      "     Training Step: 92 Training Loss: 0.6196224093437195 \n",
      "     Training Step: 93 Training Loss: 0.6149532198905945 \n",
      "     Training Step: 94 Training Loss: 0.6150686740875244 \n",
      "     Training Step: 95 Training Loss: 0.6142426133155823 \n",
      "     Training Step: 96 Training Loss: 0.6180695295333862 \n",
      "     Training Step: 97 Training Loss: 0.6154729723930359 \n",
      "     Training Step: 98 Training Loss: 0.6184333562850952 \n",
      "     Training Step: 99 Training Loss: 0.6135168671607971 \n",
      "     Training Step: 100 Training Loss: 0.611623227596283 \n",
      "     Training Step: 101 Training Loss: 0.6146709322929382 \n",
      "     Training Step: 102 Training Loss: 0.6139233112335205 \n",
      "     Training Step: 103 Training Loss: 0.6125215291976929 \n",
      "     Training Step: 104 Training Loss: 0.6168004274368286 \n",
      "     Training Step: 105 Training Loss: 0.6151747107505798 \n",
      "     Training Step: 106 Training Loss: 0.6143801808357239 \n",
      "     Training Step: 107 Training Loss: 0.61040198802948 \n",
      "     Training Step: 108 Training Loss: 0.6159496903419495 \n",
      "     Training Step: 109 Training Loss: 0.610057532787323 \n",
      "     Training Step: 110 Training Loss: 0.6133064031600952 \n",
      "     Training Step: 111 Training Loss: 0.6163591146469116 \n",
      "     Training Step: 112 Training Loss: 0.6160403490066528 \n",
      "     Training Step: 113 Training Loss: 0.6166811585426331 \n",
      "     Training Step: 114 Training Loss: 0.6114132404327393 \n",
      "     Training Step: 115 Training Loss: 0.6144977807998657 \n",
      "     Training Step: 116 Training Loss: 0.6146897673606873 \n",
      "     Training Step: 117 Training Loss: 0.6141510605812073 \n",
      "     Training Step: 118 Training Loss: 0.6128703355789185 \n",
      "     Training Step: 119 Training Loss: 0.6137635111808777 \n",
      "     Training Step: 120 Training Loss: 0.6167086958885193 \n",
      "     Training Step: 121 Training Loss: 0.6128653287887573 \n",
      "     Training Step: 122 Training Loss: 0.6159976720809937 \n",
      "     Training Step: 123 Training Loss: 0.6141542792320251 \n",
      "     Training Step: 124 Training Loss: 0.614479660987854 \n",
      "     Training Step: 125 Training Loss: 0.6143330931663513 \n",
      "     Training Step: 126 Training Loss: 0.6131276488304138 \n",
      "     Training Step: 127 Training Loss: 0.6105006337165833 \n",
      "     Training Step: 128 Training Loss: 0.613882839679718 \n",
      "     Training Step: 129 Training Loss: 0.6198694109916687 \n",
      "     Training Step: 130 Training Loss: 0.6117995381355286 \n",
      "     Training Step: 131 Training Loss: 0.6152819395065308 \n",
      "     Training Step: 132 Training Loss: 0.6186005473136902 \n",
      "     Training Step: 133 Training Loss: 0.6097119450569153 \n",
      "     Training Step: 134 Training Loss: 0.6112014055252075 \n",
      "     Training Step: 135 Training Loss: 0.6146391034126282 \n",
      "     Training Step: 136 Training Loss: 0.6146030426025391 \n",
      "     Training Step: 137 Training Loss: 0.613589346408844 \n",
      "     Training Step: 138 Training Loss: 0.6111376881599426 \n",
      "     Training Step: 139 Training Loss: 0.6136377453804016 \n",
      "     Training Step: 140 Training Loss: 0.613815426826477 \n",
      "     Training Step: 141 Training Loss: 0.6115783452987671 \n",
      "     Training Step: 142 Training Loss: 0.6140789985656738 \n",
      "     Training Step: 143 Training Loss: 0.6115179061889648 \n",
      "     Training Step: 144 Training Loss: 0.613351583480835 \n",
      "     Training Step: 145 Training Loss: 0.6153820157051086 \n",
      "     Training Step: 146 Training Loss: 0.6154524087905884 \n",
      "     Training Step: 147 Training Loss: 0.614425003528595 \n",
      "     Training Step: 148 Training Loss: 0.6101245284080505 \n",
      "     Training Step: 149 Training Loss: 0.6132889986038208 \n",
      "     Training Step: 150 Training Loss: 0.6158370971679688 \n",
      "     Training Step: 151 Training Loss: 0.6167258024215698 \n",
      "     Training Step: 152 Training Loss: 0.6150875687599182 \n",
      "     Training Step: 153 Training Loss: 0.6132035255432129 \n",
      "     Training Step: 154 Training Loss: 0.6120169758796692 \n",
      "     Training Step: 155 Training Loss: 0.6157808303833008 \n",
      "     Training Step: 156 Training Loss: 0.6146994233131409 \n",
      "     Training Step: 157 Training Loss: 0.611534059047699 \n",
      "     Training Step: 158 Training Loss: 0.6125398278236389 \n",
      "     Training Step: 159 Training Loss: 0.6134724617004395 \n",
      "     Training Step: 160 Training Loss: 0.6116465330123901 \n",
      "     Training Step: 161 Training Loss: 0.6121394634246826 \n",
      "     Training Step: 162 Training Loss: 0.6118014454841614 \n",
      "     Training Step: 163 Training Loss: 0.6152684092521667 \n",
      "     Training Step: 164 Training Loss: 0.6184804439544678 \n",
      "     Training Step: 165 Training Loss: 0.6132825613021851 \n",
      "     Training Step: 166 Training Loss: 0.6125326156616211 \n",
      "     Training Step: 167 Training Loss: 0.6185905933380127 \n",
      "     Training Step: 168 Training Loss: 0.610572099685669 \n",
      "     Training Step: 169 Training Loss: 0.61670982837677 \n",
      "     Training Step: 170 Training Loss: 0.6101795434951782 \n",
      "     Training Step: 171 Training Loss: 0.6176373362541199 \n",
      "     Training Step: 172 Training Loss: 0.6115580201148987 \n",
      "     Training Step: 173 Training Loss: 0.6122308373451233 \n",
      "     Training Step: 174 Training Loss: 0.6141047477722168 \n",
      "     Training Step: 175 Training Loss: 0.6097325682640076 \n",
      "     Training Step: 176 Training Loss: 0.6121219396591187 \n",
      "     Training Step: 177 Training Loss: 0.609226644039154 \n",
      "     Training Step: 178 Training Loss: 0.6182340383529663 \n",
      "     Training Step: 179 Training Loss: 0.6142093539237976 \n",
      "     Training Step: 180 Training Loss: 0.6168148517608643 \n",
      "     Training Step: 181 Training Loss: 0.6197168231010437 \n",
      "     Training Step: 182 Training Loss: 0.6131386756896973 \n",
      "     Training Step: 183 Training Loss: 0.6132009029388428 \n",
      "     Training Step: 184 Training Loss: 0.6180182695388794 \n",
      "     Training Step: 185 Training Loss: 0.6122302412986755 \n",
      "     Training Step: 186 Training Loss: 0.6154158711433411 \n",
      "     Training Step: 187 Training Loss: 0.615294337272644 \n",
      "     Training Step: 188 Training Loss: 0.6128486394882202 \n",
      "     Training Step: 189 Training Loss: 0.6140252947807312 \n",
      "     Training Step: 190 Training Loss: 0.6130732297897339 \n",
      "     Training Step: 191 Training Loss: 0.6143444180488586 \n",
      "     Training Step: 192 Training Loss: 0.6144443154335022 \n",
      "     Training Step: 193 Training Loss: 0.6151537895202637 \n",
      "     Training Step: 194 Training Loss: 0.6094779372215271 \n",
      "     Training Step: 195 Training Loss: 0.6177546977996826 \n",
      "     Training Step: 196 Training Loss: 0.6146095395088196 \n",
      "     Training Step: 197 Training Loss: 0.6164237260818481 \n",
      "     Training Step: 198 Training Loss: 0.6170973777770996 \n",
      "     Training Step: 199 Training Loss: 0.6104999780654907 \n",
      "     Training Step: 200 Training Loss: 0.611433207988739 \n",
      "     Training Step: 201 Training Loss: 0.6153354644775391 \n",
      "     Training Step: 202 Training Loss: 0.6115198135375977 \n",
      "     Training Step: 203 Training Loss: 0.6173923015594482 \n",
      "     Training Step: 204 Training Loss: 0.6153953075408936 \n",
      "     Training Step: 205 Training Loss: 0.6147477626800537 \n",
      "     Training Step: 206 Training Loss: 0.6116290092468262 \n",
      "     Training Step: 207 Training Loss: 0.6105706691741943 \n",
      "     Training Step: 208 Training Loss: 0.6154186725616455 \n",
      "     Training Step: 209 Training Loss: 0.6164973974227905 \n",
      "     Training Step: 210 Training Loss: 0.6123788356781006 \n",
      "     Training Step: 211 Training Loss: 0.6134556531906128 \n",
      "     Training Step: 212 Training Loss: 0.6169323921203613 \n",
      "     Training Step: 213 Training Loss: 0.6162012219429016 \n",
      "     Training Step: 214 Training Loss: 0.6121639013290405 \n",
      "     Training Step: 215 Training Loss: 0.6134157776832581 \n",
      "     Training Step: 216 Training Loss: 0.612156331539154 \n",
      "     Training Step: 217 Training Loss: 0.608273446559906 \n",
      "     Training Step: 218 Training Loss: 0.6106613874435425 \n",
      "     Training Step: 219 Training Loss: 0.6156840324401855 \n",
      "     Training Step: 220 Training Loss: 0.6129171848297119 \n",
      "     Training Step: 221 Training Loss: 0.6108646988868713 \n",
      "     Training Step: 222 Training Loss: 0.6157627701759338 \n",
      "     Training Step: 223 Training Loss: 0.6146958470344543 \n",
      "     Training Step: 224 Training Loss: 0.6111499667167664 \n",
      "     Training Step: 225 Training Loss: 0.6172248721122742 \n",
      "     Training Step: 226 Training Loss: 0.6157135367393494 \n",
      "     Training Step: 227 Training Loss: 0.6107281446456909 \n",
      "     Training Step: 228 Training Loss: 0.6157523393630981 \n",
      "     Training Step: 229 Training Loss: 0.6194601058959961 \n",
      "     Training Step: 230 Training Loss: 0.613614022731781 \n",
      "     Training Step: 231 Training Loss: 0.6166751384735107 \n",
      "     Training Step: 232 Training Loss: 0.6118413209915161 \n",
      "     Training Step: 233 Training Loss: 0.6146078109741211 \n",
      "     Training Step: 234 Training Loss: 0.6106219291687012 \n",
      "     Training Step: 235 Training Loss: 0.6147963404655457 \n",
      "     Training Step: 236 Training Loss: 0.6117017865180969 \n",
      "     Training Step: 237 Training Loss: 0.6176916360855103 \n",
      "     Training Step: 238 Training Loss: 0.6129739880561829 \n",
      "     Training Step: 239 Training Loss: 0.6142207980155945 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6111701130867004 \n",
      "     Validation Step: 1 Validation Loss: 0.6148973107337952 \n",
      "     Validation Step: 2 Validation Loss: 0.6136659979820251 \n",
      "     Validation Step: 3 Validation Loss: 0.6141210198402405 \n",
      "     Validation Step: 4 Validation Loss: 0.6148394346237183 \n",
      "     Validation Step: 5 Validation Loss: 0.6105251908302307 \n",
      "     Validation Step: 6 Validation Loss: 0.6170154213905334 \n",
      "     Validation Step: 7 Validation Loss: 0.6153129935264587 \n",
      "     Validation Step: 8 Validation Loss: 0.6176846623420715 \n",
      "     Validation Step: 9 Validation Loss: 0.6136457920074463 \n",
      "     Validation Step: 10 Validation Loss: 0.6142610907554626 \n",
      "     Validation Step: 11 Validation Loss: 0.6101177334785461 \n",
      "     Validation Step: 12 Validation Loss: 0.6152253746986389 \n",
      "     Validation Step: 13 Validation Loss: 0.6101459264755249 \n",
      "     Validation Step: 14 Validation Loss: 0.6075392961502075 \n",
      "     Validation Step: 15 Validation Loss: 0.6136469841003418 \n",
      "     Validation Step: 16 Validation Loss: 0.6121354103088379 \n",
      "     Validation Step: 17 Validation Loss: 0.6116506457328796 \n",
      "     Validation Step: 18 Validation Loss: 0.6104931235313416 \n",
      "     Validation Step: 19 Validation Loss: 0.6184678673744202 \n",
      "     Validation Step: 20 Validation Loss: 0.6142048239707947 \n",
      "     Validation Step: 21 Validation Loss: 0.6156083941459656 \n",
      "     Validation Step: 22 Validation Loss: 0.6111840009689331 \n",
      "     Validation Step: 23 Validation Loss: 0.6101735830307007 \n",
      "     Validation Step: 24 Validation Loss: 0.6128412485122681 \n",
      "     Validation Step: 25 Validation Loss: 0.6155676245689392 \n",
      "     Validation Step: 26 Validation Loss: 0.6180532574653625 \n",
      "     Validation Step: 27 Validation Loss: 0.614639937877655 \n",
      "     Validation Step: 28 Validation Loss: 0.615787923336029 \n",
      "     Validation Step: 29 Validation Loss: 0.6133139729499817 \n",
      "     Validation Step: 30 Validation Loss: 0.617301881313324 \n",
      "     Validation Step: 31 Validation Loss: 0.6115637421607971 \n",
      "     Validation Step: 32 Validation Loss: 0.6175953149795532 \n",
      "     Validation Step: 33 Validation Loss: 0.6118907332420349 \n",
      "     Validation Step: 34 Validation Loss: 0.6162384748458862 \n",
      "     Validation Step: 35 Validation Loss: 0.6145411133766174 \n",
      "     Validation Step: 36 Validation Loss: 0.618329644203186 \n",
      "     Validation Step: 37 Validation Loss: 0.614570140838623 \n",
      "     Validation Step: 38 Validation Loss: 0.6159845590591431 \n",
      "     Validation Step: 39 Validation Loss: 0.612991213798523 \n",
      "     Validation Step: 40 Validation Loss: 0.6182166337966919 \n",
      "     Validation Step: 41 Validation Loss: 0.6184839010238647 \n",
      "     Validation Step: 42 Validation Loss: 0.6150325536727905 \n",
      "     Validation Step: 43 Validation Loss: 0.6141310930252075 \n",
      "     Validation Step: 44 Validation Loss: 0.610640287399292 \n",
      "Epoch: 114\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6112093329429626 \n",
      "     Training Step: 1 Training Loss: 0.6167008280754089 \n",
      "     Training Step: 2 Training Loss: 0.6133005619049072 \n",
      "     Training Step: 3 Training Loss: 0.6125282049179077 \n",
      "     Training Step: 4 Training Loss: 0.614288330078125 \n",
      "     Training Step: 5 Training Loss: 0.613919198513031 \n",
      "     Training Step: 6 Training Loss: 0.6123719215393066 \n",
      "     Training Step: 7 Training Loss: 0.6115237474441528 \n",
      "     Training Step: 8 Training Loss: 0.6121359467506409 \n",
      "     Training Step: 9 Training Loss: 0.6156154870986938 \n",
      "     Training Step: 10 Training Loss: 0.6144514679908752 \n",
      "     Training Step: 11 Training Loss: 0.6104699969291687 \n",
      "     Training Step: 12 Training Loss: 0.6196545362472534 \n",
      "     Training Step: 13 Training Loss: 0.6123753786087036 \n",
      "     Training Step: 14 Training Loss: 0.611638069152832 \n",
      "     Training Step: 15 Training Loss: 0.6166799068450928 \n",
      "     Training Step: 16 Training Loss: 0.6142063736915588 \n",
      "     Training Step: 17 Training Loss: 0.6140217781066895 \n",
      "     Training Step: 18 Training Loss: 0.6181023120880127 \n",
      "     Training Step: 19 Training Loss: 0.6114829182624817 \n",
      "     Training Step: 20 Training Loss: 0.6092437505722046 \n",
      "     Training Step: 21 Training Loss: 0.614421010017395 \n",
      "     Training Step: 22 Training Loss: 0.6120139956474304 \n",
      "     Training Step: 23 Training Loss: 0.6130585074424744 \n",
      "     Training Step: 24 Training Loss: 0.6184167861938477 \n",
      "     Training Step: 25 Training Loss: 0.6166656613349915 \n",
      "     Training Step: 26 Training Loss: 0.6157853603363037 \n",
      "     Training Step: 27 Training Loss: 0.6154146194458008 \n",
      "     Training Step: 28 Training Loss: 0.6149546504020691 \n",
      "     Training Step: 29 Training Loss: 0.6150647401809692 \n",
      "     Training Step: 30 Training Loss: 0.6125238537788391 \n",
      "     Training Step: 31 Training Loss: 0.6106758713722229 \n",
      "     Training Step: 32 Training Loss: 0.6160330176353455 \n",
      "     Training Step: 33 Training Loss: 0.6144976615905762 \n",
      "     Training Step: 34 Training Loss: 0.6141501665115356 \n",
      "     Training Step: 35 Training Loss: 0.6136190891265869 \n",
      "     Training Step: 36 Training Loss: 0.6127512454986572 \n",
      "     Training Step: 37 Training Loss: 0.611789882183075 \n",
      "     Training Step: 38 Training Loss: 0.6177588105201721 \n",
      "     Training Step: 39 Training Loss: 0.6138864159584045 \n",
      "     Training Step: 40 Training Loss: 0.6141517162322998 \n",
      "     Training Step: 41 Training Loss: 0.6180275678634644 \n",
      "     Training Step: 42 Training Loss: 0.6164109110832214 \n",
      "     Training Step: 43 Training Loss: 0.6097282767295837 \n",
      "     Training Step: 44 Training Loss: 0.6168033480644226 \n",
      "     Training Step: 45 Training Loss: 0.6188669204711914 \n",
      "     Training Step: 46 Training Loss: 0.6148679256439209 \n",
      "     Training Step: 47 Training Loss: 0.6100791692733765 \n",
      "     Training Step: 48 Training Loss: 0.6127090454101562 \n",
      "     Training Step: 49 Training Loss: 0.6124657988548279 \n",
      "     Training Step: 50 Training Loss: 0.6194375157356262 \n",
      "     Training Step: 51 Training Loss: 0.6152960658073425 \n",
      "     Training Step: 52 Training Loss: 0.6150951981544495 \n",
      "     Training Step: 53 Training Loss: 0.6153703331947327 \n",
      "     Training Step: 54 Training Loss: 0.6111673712730408 \n",
      "     Training Step: 55 Training Loss: 0.6137624382972717 \n",
      "     Training Step: 56 Training Loss: 0.6118333339691162 \n",
      "     Training Step: 57 Training Loss: 0.6114423871040344 \n",
      "     Training Step: 58 Training Loss: 0.6124899387359619 \n",
      "     Training Step: 59 Training Loss: 0.6180428862571716 \n",
      "     Training Step: 60 Training Loss: 0.6123742461204529 \n",
      "     Training Step: 61 Training Loss: 0.6147056818008423 \n",
      "     Training Step: 62 Training Loss: 0.6161202192306519 \n",
      "     Training Step: 63 Training Loss: 0.6116845607757568 \n",
      "     Training Step: 64 Training Loss: 0.6105664968490601 \n",
      "     Training Step: 65 Training Loss: 0.616356611251831 \n",
      "     Training Step: 66 Training Loss: 0.6093894243240356 \n",
      "     Training Step: 67 Training Loss: 0.6134969592094421 \n",
      "     Training Step: 68 Training Loss: 0.6180867552757263 \n",
      "     Training Step: 69 Training Loss: 0.6147941946983337 \n",
      "     Training Step: 70 Training Loss: 0.6133111119270325 \n",
      "     Training Step: 71 Training Loss: 0.6140171885490417 \n",
      "     Training Step: 72 Training Loss: 0.6114131212234497 \n",
      "     Training Step: 73 Training Loss: 0.6118335723876953 \n",
      "     Training Step: 74 Training Loss: 0.6146649122238159 \n",
      "     Training Step: 75 Training Loss: 0.6103929877281189 \n",
      "     Training Step: 76 Training Loss: 0.6186153888702393 \n",
      "     Training Step: 77 Training Loss: 0.6146530508995056 \n",
      "     Training Step: 78 Training Loss: 0.6168099045753479 \n",
      "     Training Step: 79 Training Loss: 0.6150871515274048 \n",
      "     Training Step: 80 Training Loss: 0.6171599626541138 \n",
      "     Training Step: 81 Training Loss: 0.6176841259002686 \n",
      "     Training Step: 82 Training Loss: 0.6114110350608826 \n",
      "     Training Step: 83 Training Loss: 0.6167963147163391 \n",
      "     Training Step: 84 Training Loss: 0.613299548625946 \n",
      "     Training Step: 85 Training Loss: 0.6154388785362244 \n",
      "     Training Step: 86 Training Loss: 0.6146008372306824 \n",
      "     Training Step: 87 Training Loss: 0.6101661324501038 \n",
      "     Training Step: 88 Training Loss: 0.6128807663917542 \n",
      "     Training Step: 89 Training Loss: 0.6140782237052917 \n",
      "     Training Step: 90 Training Loss: 0.6156866550445557 \n",
      "     Training Step: 91 Training Loss: 0.6171955466270447 \n",
      "     Training Step: 92 Training Loss: 0.61620032787323 \n",
      "     Training Step: 93 Training Loss: 0.6183217763900757 \n",
      "     Training Step: 94 Training Loss: 0.6122400164604187 \n",
      "     Training Step: 95 Training Loss: 0.614478349685669 \n",
      "     Training Step: 96 Training Loss: 0.6152397990226746 \n",
      "     Training Step: 97 Training Loss: 0.6135159730911255 \n",
      "     Training Step: 98 Training Loss: 0.6209222078323364 \n",
      "     Training Step: 99 Training Loss: 0.6184521317481995 \n",
      "     Training Step: 100 Training Loss: 0.6149154305458069 \n",
      "     Training Step: 101 Training Loss: 0.619673490524292 \n",
      "     Training Step: 102 Training Loss: 0.6100953817367554 \n",
      "     Training Step: 103 Training Loss: 0.6143471002578735 \n",
      "     Training Step: 104 Training Loss: 0.6136578917503357 \n",
      "     Training Step: 105 Training Loss: 0.6122502684593201 \n",
      "     Training Step: 106 Training Loss: 0.6161959767341614 \n",
      "     Training Step: 107 Training Loss: 0.6155298352241516 \n",
      "     Training Step: 108 Training Loss: 0.6131523847579956 \n",
      "     Training Step: 109 Training Loss: 0.6142470240592957 \n",
      "     Training Step: 110 Training Loss: 0.6143392324447632 \n",
      "     Training Step: 111 Training Loss: 0.6132074594497681 \n",
      "     Training Step: 112 Training Loss: 0.6100475788116455 \n",
      "     Training Step: 113 Training Loss: 0.6151755452156067 \n",
      "     Training Step: 114 Training Loss: 0.617821991443634 \n",
      "     Training Step: 115 Training Loss: 0.6108970642089844 \n",
      "     Training Step: 116 Training Loss: 0.6128391027450562 \n",
      "     Training Step: 117 Training Loss: 0.6154283881187439 \n",
      "     Training Step: 118 Training Loss: 0.6155504584312439 \n",
      "     Training Step: 119 Training Loss: 0.6147280931472778 \n",
      "     Training Step: 120 Training Loss: 0.6096935272216797 \n",
      "     Training Step: 121 Training Loss: 0.6118218898773193 \n",
      "     Training Step: 122 Training Loss: 0.6177253723144531 \n",
      "     Training Step: 123 Training Loss: 0.6125251054763794 \n",
      "     Training Step: 124 Training Loss: 0.6143792867660522 \n",
      "     Training Step: 125 Training Loss: 0.6115466356277466 \n",
      "     Training Step: 126 Training Loss: 0.6131830215454102 \n",
      "     Training Step: 127 Training Loss: 0.613067626953125 \n",
      "     Training Step: 128 Training Loss: 0.6146369576454163 \n",
      "     Training Step: 129 Training Loss: 0.6146857142448425 \n",
      "     Training Step: 130 Training Loss: 0.6188527941703796 \n",
      "     Training Step: 131 Training Loss: 0.6121622920036316 \n",
      "     Training Step: 132 Training Loss: 0.6111423373222351 \n",
      "     Training Step: 133 Training Loss: 0.6122280955314636 \n",
      "     Training Step: 134 Training Loss: 0.6115278601646423 \n",
      "     Training Step: 135 Training Loss: 0.6122936606407166 \n",
      "     Training Step: 136 Training Loss: 0.612641453742981 \n",
      "     Training Step: 137 Training Loss: 0.613745927810669 \n",
      "     Training Step: 138 Training Loss: 0.6101675033569336 \n",
      "     Training Step: 139 Training Loss: 0.6142109632492065 \n",
      "     Training Step: 140 Training Loss: 0.6147143244743347 \n",
      "     Training Step: 141 Training Loss: 0.6108613610267639 \n",
      "     Training Step: 142 Training Loss: 0.6141127943992615 \n",
      "     Training Step: 143 Training Loss: 0.6167026162147522 \n",
      "     Training Step: 144 Training Loss: 0.6146972179412842 \n",
      "     Training Step: 145 Training Loss: 0.6127591133117676 \n",
      "     Training Step: 146 Training Loss: 0.6182519197463989 \n",
      "     Training Step: 147 Training Loss: 0.6167715191841125 \n",
      "     Training Step: 148 Training Loss: 0.6138206124305725 \n",
      "     Training Step: 149 Training Loss: 0.6147719025611877 \n",
      "     Training Step: 150 Training Loss: 0.6157426834106445 \n",
      "     Training Step: 151 Training Loss: 0.6122961044311523 \n",
      "     Training Step: 152 Training Loss: 0.6114291548728943 \n",
      "     Training Step: 153 Training Loss: 0.6144459843635559 \n",
      "     Training Step: 154 Training Loss: 0.6097549796104431 \n",
      "     Training Step: 155 Training Loss: 0.6149314045906067 \n",
      "     Training Step: 156 Training Loss: 0.6169238686561584 \n",
      "     Training Step: 157 Training Loss: 0.6152856349945068 \n",
      "     Training Step: 158 Training Loss: 0.6173791885375977 \n",
      "     Training Step: 159 Training Loss: 0.6105921864509583 \n",
      "     Training Step: 160 Training Loss: 0.6132804155349731 \n",
      "     Training Step: 161 Training Loss: 0.6157565116882324 \n",
      "     Training Step: 162 Training Loss: 0.6153979897499084 \n",
      "     Training Step: 163 Training Loss: 0.6120669841766357 \n",
      "     Training Step: 164 Training Loss: 0.6116070747375488 \n",
      "     Training Step: 165 Training Loss: 0.6171502470970154 \n",
      "     Training Step: 166 Training Loss: 0.6139185428619385 \n",
      "     Training Step: 167 Training Loss: 0.6118742227554321 \n",
      "     Training Step: 168 Training Loss: 0.6135939955711365 \n",
      "     Training Step: 169 Training Loss: 0.6115777492523193 \n",
      "     Training Step: 170 Training Loss: 0.6153377890586853 \n",
      "     Training Step: 171 Training Loss: 0.6159583330154419 \n",
      "     Training Step: 172 Training Loss: 0.6129193305969238 \n",
      "     Training Step: 173 Training Loss: 0.6129153966903687 \n",
      "     Training Step: 174 Training Loss: 0.6116295456886292 \n",
      "     Training Step: 175 Training Loss: 0.6132023930549622 \n",
      "     Training Step: 176 Training Loss: 0.6176448464393616 \n",
      "     Training Step: 177 Training Loss: 0.615526020526886 \n",
      "     Training Step: 178 Training Loss: 0.610500693321228 \n",
      "     Training Step: 179 Training Loss: 0.6164914965629578 \n",
      "     Training Step: 180 Training Loss: 0.6167172193527222 \n",
      "     Training Step: 181 Training Loss: 0.6159973740577698 \n",
      "     Training Step: 182 Training Loss: 0.6170876622200012 \n",
      "     Training Step: 183 Training Loss: 0.6184264421463013 \n",
      "     Training Step: 184 Training Loss: 0.6166938543319702 \n",
      "     Training Step: 185 Training Loss: 0.6162269115447998 \n",
      "     Training Step: 186 Training Loss: 0.6100437641143799 \n",
      "     Training Step: 187 Training Loss: 0.6156640648841858 \n",
      "     Training Step: 188 Training Loss: 0.613378643989563 \n",
      "     Training Step: 189 Training Loss: 0.6134898066520691 \n",
      "     Training Step: 190 Training Loss: 0.6106116771697998 \n",
      "     Training Step: 191 Training Loss: 0.612810492515564 \n",
      "     Training Step: 192 Training Loss: 0.6157446503639221 \n",
      "     Training Step: 193 Training Loss: 0.61163729429245 \n",
      "     Training Step: 194 Training Loss: 0.6202273368835449 \n",
      "     Training Step: 195 Training Loss: 0.6107354760169983 \n",
      "     Training Step: 196 Training Loss: 0.6178098320960999 \n",
      "     Training Step: 197 Training Loss: 0.6136372685432434 \n",
      "     Training Step: 198 Training Loss: 0.61212158203125 \n",
      "     Training Step: 199 Training Loss: 0.611801028251648 \n",
      "     Training Step: 200 Training Loss: 0.6151582598686218 \n",
      "     Training Step: 201 Training Loss: 0.6082451939582825 \n",
      "     Training Step: 202 Training Loss: 0.6143451929092407 \n",
      "     Training Step: 203 Training Loss: 0.610367476940155 \n",
      "     Training Step: 204 Training Loss: 0.6158419251441956 \n",
      "     Training Step: 205 Training Loss: 0.6182486414909363 \n",
      "     Training Step: 206 Training Loss: 0.6146637201309204 \n",
      "     Training Step: 207 Training Loss: 0.6166622042655945 \n",
      "     Training Step: 208 Training Loss: 0.6153843402862549 \n",
      "     Training Step: 209 Training Loss: 0.6198734045028687 \n",
      "     Training Step: 210 Training Loss: 0.6107378602027893 \n",
      "     Training Step: 211 Training Loss: 0.6185618042945862 \n",
      "     Training Step: 212 Training Loss: 0.615288257598877 \n",
      "     Training Step: 213 Training Loss: 0.6107268929481506 \n",
      "     Training Step: 214 Training Loss: 0.6114734411239624 \n",
      "     Training Step: 215 Training Loss: 0.6146110892295837 \n",
      "     Training Step: 216 Training Loss: 0.6167076826095581 \n",
      "     Training Step: 217 Training Loss: 0.6166011691093445 \n",
      "     Training Step: 218 Training Loss: 0.6133021712303162 \n",
      "     Training Step: 219 Training Loss: 0.6171292066574097 \n",
      "     Training Step: 220 Training Loss: 0.6146070957183838 \n",
      "     Training Step: 221 Training Loss: 0.6137385964393616 \n",
      "     Training Step: 222 Training Loss: 0.6118165850639343 \n",
      "     Training Step: 223 Training Loss: 0.6121564507484436 \n",
      "     Training Step: 224 Training Loss: 0.6115257740020752 \n",
      "     Training Step: 225 Training Loss: 0.6164212822914124 \n",
      "     Training Step: 226 Training Loss: 0.6147482395172119 \n",
      "     Training Step: 227 Training Loss: 0.6175004839897156 \n",
      "     Training Step: 228 Training Loss: 0.6176959276199341 \n",
      "     Training Step: 229 Training Loss: 0.6134677529335022 \n",
      "     Training Step: 230 Training Loss: 0.6168943643569946 \n",
      "     Training Step: 231 Training Loss: 0.6131181716918945 \n",
      "     Training Step: 232 Training Loss: 0.6134158372879028 \n",
      "     Training Step: 233 Training Loss: 0.6154741644859314 \n",
      "     Training Step: 234 Training Loss: 0.6129739880561829 \n",
      "     Training Step: 235 Training Loss: 0.6140368580818176 \n",
      "     Training Step: 236 Training Loss: 0.615269124507904 \n",
      "     Training Step: 237 Training Loss: 0.6128718852996826 \n",
      "     Training Step: 238 Training Loss: 0.6106033325195312 \n",
      "     Training Step: 239 Training Loss: 0.6094616651535034 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6152285933494568 \n",
      "     Validation Step: 1 Validation Loss: 0.6148325204849243 \n",
      "     Validation Step: 2 Validation Loss: 0.6105077266693115 \n",
      "     Validation Step: 3 Validation Loss: 0.6075060367584229 \n",
      "     Validation Step: 4 Validation Loss: 0.6175992488861084 \n",
      "     Validation Step: 5 Validation Loss: 0.6104775071144104 \n",
      "     Validation Step: 6 Validation Loss: 0.614571213722229 \n",
      "     Validation Step: 7 Validation Loss: 0.6128219366073608 \n",
      "     Validation Step: 8 Validation Loss: 0.6176990270614624 \n",
      "     Validation Step: 9 Validation Loss: 0.6145329475402832 \n",
      "     Validation Step: 10 Validation Loss: 0.6133008599281311 \n",
      "     Validation Step: 11 Validation Loss: 0.6136647462844849 \n",
      "     Validation Step: 12 Validation Loss: 0.6146351099014282 \n",
      "     Validation Step: 13 Validation Loss: 0.618068277835846 \n",
      "     Validation Step: 14 Validation Loss: 0.6150405406951904 \n",
      "     Validation Step: 15 Validation Loss: 0.6153217554092407 \n",
      "     Validation Step: 16 Validation Loss: 0.6111575365066528 \n",
      "     Validation Step: 17 Validation Loss: 0.6159985065460205 \n",
      "     Validation Step: 18 Validation Loss: 0.6111742258071899 \n",
      "     Validation Step: 19 Validation Loss: 0.6182354092597961 \n",
      "     Validation Step: 20 Validation Loss: 0.6183483600616455 \n",
      "     Validation Step: 21 Validation Loss: 0.6101516485214233 \n",
      "     Validation Step: 22 Validation Loss: 0.614111602306366 \n",
      "     Validation Step: 23 Validation Loss: 0.6141246557235718 \n",
      "     Validation Step: 24 Validation Loss: 0.610101580619812 \n",
      "     Validation Step: 25 Validation Loss: 0.6115500330924988 \n",
      "     Validation Step: 26 Validation Loss: 0.6173107028007507 \n",
      "     Validation Step: 27 Validation Loss: 0.6142519116401672 \n",
      "     Validation Step: 28 Validation Loss: 0.6141989231109619 \n",
      "     Validation Step: 29 Validation Loss: 0.6121224164962769 \n",
      "     Validation Step: 30 Validation Loss: 0.61702960729599 \n",
      "     Validation Step: 31 Validation Loss: 0.613653838634491 \n",
      "     Validation Step: 32 Validation Loss: 0.6155769228935242 \n",
      "     Validation Step: 33 Validation Loss: 0.6162456274032593 \n",
      "     Validation Step: 34 Validation Loss: 0.6118736863136292 \n",
      "     Validation Step: 35 Validation Loss: 0.6101275086402893 \n",
      "     Validation Step: 36 Validation Loss: 0.6106234788894653 \n",
      "     Validation Step: 37 Validation Loss: 0.6116346716880798 \n",
      "     Validation Step: 38 Validation Loss: 0.615790605545044 \n",
      "     Validation Step: 39 Validation Loss: 0.6136298179626465 \n",
      "     Validation Step: 40 Validation Loss: 0.618475615978241 \n",
      "     Validation Step: 41 Validation Loss: 0.6148951053619385 \n",
      "     Validation Step: 42 Validation Loss: 0.6185014247894287 \n",
      "     Validation Step: 43 Validation Loss: 0.6129801273345947 \n",
      "     Validation Step: 44 Validation Loss: 0.6156167387962341 \n",
      "Epoch: 115\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6147050857543945 \n",
      "     Training Step: 1 Training Loss: 0.6151047348976135 \n",
      "     Training Step: 2 Training Loss: 0.6188896298408508 \n",
      "     Training Step: 3 Training Loss: 0.6105830669403076 \n",
      "     Training Step: 4 Training Loss: 0.6150738000869751 \n",
      "     Training Step: 5 Training Loss: 0.6159592270851135 \n",
      "     Training Step: 6 Training Loss: 0.6092160940170288 \n",
      "     Training Step: 7 Training Loss: 0.6154502630233765 \n",
      "     Training Step: 8 Training Loss: 0.6131836175918579 \n",
      "     Training Step: 9 Training Loss: 0.6101729869842529 \n",
      "     Training Step: 10 Training Loss: 0.6185909509658813 \n",
      "     Training Step: 11 Training Loss: 0.6124863028526306 \n",
      "     Training Step: 12 Training Loss: 0.6209473609924316 \n",
      "     Training Step: 13 Training Loss: 0.6167194247245789 \n",
      "     Training Step: 14 Training Loss: 0.6106930375099182 \n",
      "     Training Step: 15 Training Loss: 0.6100676655769348 \n",
      "     Training Step: 16 Training Loss: 0.6106702089309692 \n",
      "     Training Step: 17 Training Loss: 0.6171569228172302 \n",
      "     Training Step: 18 Training Loss: 0.6136181354522705 \n",
      "     Training Step: 19 Training Loss: 0.6082701086997986 \n",
      "     Training Step: 20 Training Loss: 0.6171951293945312 \n",
      "     Training Step: 21 Training Loss: 0.6104726791381836 \n",
      "     Training Step: 22 Training Loss: 0.612119197845459 \n",
      "     Training Step: 23 Training Loss: 0.6130713820457458 \n",
      "     Training Step: 24 Training Loss: 0.6144278049468994 \n",
      "     Training Step: 25 Training Loss: 0.615385115146637 \n",
      "     Training Step: 26 Training Loss: 0.6150956749916077 \n",
      "     Training Step: 27 Training Loss: 0.6120616793632507 \n",
      "     Training Step: 28 Training Loss: 0.6137375235557556 \n",
      "     Training Step: 29 Training Loss: 0.6101208925247192 \n",
      "     Training Step: 30 Training Loss: 0.6136419773101807 \n",
      "     Training Step: 31 Training Loss: 0.6164199709892273 \n",
      "     Training Step: 32 Training Loss: 0.6128683686256409 \n",
      "     Training Step: 33 Training Loss: 0.613509476184845 \n",
      "     Training Step: 34 Training Loss: 0.6143496036529541 \n",
      "     Training Step: 35 Training Loss: 0.6094438433647156 \n",
      "     Training Step: 36 Training Loss: 0.6161255836486816 \n",
      "     Training Step: 37 Training Loss: 0.6166651844978333 \n",
      "     Training Step: 38 Training Loss: 0.6107332110404968 \n",
      "     Training Step: 39 Training Loss: 0.6126388907432556 \n",
      "     Training Step: 40 Training Loss: 0.6125108003616333 \n",
      "     Training Step: 41 Training Loss: 0.6184148788452148 \n",
      "     Training Step: 42 Training Loss: 0.6178127527236938 \n",
      "     Training Step: 43 Training Loss: 0.612751305103302 \n",
      "     Training Step: 44 Training Loss: 0.6144552826881409 \n",
      "     Training Step: 45 Training Loss: 0.6194309592247009 \n",
      "     Training Step: 46 Training Loss: 0.6157790422439575 \n",
      "     Training Step: 47 Training Loss: 0.6176120638847351 \n",
      "     Training Step: 48 Training Loss: 0.6129871010780334 \n",
      "     Training Step: 49 Training Loss: 0.6115884184837341 \n",
      "     Training Step: 50 Training Loss: 0.6176791787147522 \n",
      "     Training Step: 51 Training Loss: 0.6174712181091309 \n",
      "     Training Step: 52 Training Loss: 0.613313615322113 \n",
      "     Training Step: 53 Training Loss: 0.6115478277206421 \n",
      "     Training Step: 54 Training Loss: 0.6116303205490112 \n",
      "     Training Step: 55 Training Loss: 0.6114497184753418 \n",
      "     Training Step: 56 Training Loss: 0.6121653914451599 \n",
      "     Training Step: 57 Training Loss: 0.6196271777153015 \n",
      "     Training Step: 58 Training Loss: 0.6146803498268127 \n",
      "     Training Step: 59 Training Loss: 0.6143428087234497 \n",
      "     Training Step: 60 Training Loss: 0.6146703958511353 \n",
      "     Training Step: 61 Training Loss: 0.6141581535339355 \n",
      "     Training Step: 62 Training Loss: 0.611138641834259 \n",
      "     Training Step: 63 Training Loss: 0.6104897856712341 \n",
      "     Training Step: 64 Training Loss: 0.6164304614067078 \n",
      "     Training Step: 65 Training Loss: 0.6131992936134338 \n",
      "     Training Step: 66 Training Loss: 0.6149361729621887 \n",
      "     Training Step: 67 Training Loss: 0.6100565195083618 \n",
      "     Training Step: 68 Training Loss: 0.610390841960907 \n",
      "     Training Step: 69 Training Loss: 0.6131129264831543 \n",
      "     Training Step: 70 Training Loss: 0.6160131692886353 \n",
      "     Training Step: 71 Training Loss: 0.6202497482299805 \n",
      "     Training Step: 72 Training Loss: 0.6146599650382996 \n",
      "     Training Step: 73 Training Loss: 0.6171519756317139 \n",
      "     Training Step: 74 Training Loss: 0.6134752035140991 \n",
      "     Training Step: 75 Training Loss: 0.6167610287666321 \n",
      "     Training Step: 76 Training Loss: 0.612240195274353 \n",
      "     Training Step: 77 Training Loss: 0.6147958636283875 \n",
      "     Training Step: 78 Training Loss: 0.6138257384300232 \n",
      "     Training Step: 79 Training Loss: 0.6137479543685913 \n",
      "     Training Step: 80 Training Loss: 0.6133677363395691 \n",
      "     Training Step: 81 Training Loss: 0.6117021441459656 \n",
      "     Training Step: 82 Training Loss: 0.6135932207107544 \n",
      "     Training Step: 83 Training Loss: 0.6180285215377808 \n",
      "     Training Step: 84 Training Loss: 0.6105812788009644 \n",
      "     Training Step: 85 Training Loss: 0.616202712059021 \n",
      "     Training Step: 86 Training Loss: 0.6118000745773315 \n",
      "     Training Step: 87 Training Loss: 0.6123754382133484 \n",
      "     Training Step: 88 Training Loss: 0.6107162237167358 \n",
      "     Training Step: 89 Training Loss: 0.6142073273658752 \n",
      "     Training Step: 90 Training Loss: 0.6142126321792603 \n",
      "     Training Step: 91 Training Loss: 0.6122786998748779 \n",
      "     Training Step: 92 Training Loss: 0.6177919507026672 \n",
      "     Training Step: 93 Training Loss: 0.6117762923240662 \n",
      "     Training Step: 94 Training Loss: 0.6166968941688538 \n",
      "     Training Step: 95 Training Loss: 0.6114192605018616 \n",
      "     Training Step: 96 Training Loss: 0.6153423190116882 \n",
      "     Training Step: 97 Training Loss: 0.6132744550704956 \n",
      "     Training Step: 98 Training Loss: 0.6111557483673096 \n",
      "     Training Step: 99 Training Loss: 0.6116255521774292 \n",
      "     Training Step: 100 Training Loss: 0.6147693395614624 \n",
      "     Training Step: 101 Training Loss: 0.612464189529419 \n",
      "     Training Step: 102 Training Loss: 0.6134957671165466 \n",
      "     Training Step: 103 Training Loss: 0.6188502907752991 \n",
      "     Training Step: 104 Training Loss: 0.6118019819259644 \n",
      "     Training Step: 105 Training Loss: 0.6147438883781433 \n",
      "     Training Step: 106 Training Loss: 0.6180681586265564 \n",
      "     Training Step: 107 Training Loss: 0.6180989742279053 \n",
      "     Training Step: 108 Training Loss: 0.6134278178215027 \n",
      "     Training Step: 109 Training Loss: 0.6157431602478027 \n",
      "     Training Step: 110 Training Loss: 0.6154777407646179 \n",
      "     Training Step: 111 Training Loss: 0.6154005527496338 \n",
      "     Training Step: 112 Training Loss: 0.6166030764579773 \n",
      "     Training Step: 113 Training Loss: 0.6143389344215393 \n",
      "     Training Step: 114 Training Loss: 0.6115519404411316 \n",
      "     Training Step: 115 Training Loss: 0.611900269985199 \n",
      "     Training Step: 116 Training Loss: 0.6146355867385864 \n",
      "     Training Step: 117 Training Loss: 0.6183072924613953 \n",
      "     Training Step: 118 Training Loss: 0.614690899848938 \n",
      "     Training Step: 119 Training Loss: 0.6115379929542542 \n",
      "     Training Step: 120 Training Loss: 0.6147047281265259 \n",
      "     Training Step: 121 Training Loss: 0.6116456985473633 \n",
      "     Training Step: 122 Training Loss: 0.6132807731628418 \n",
      "     Training Step: 123 Training Loss: 0.6103724241256714 \n",
      "     Training Step: 124 Training Loss: 0.6123801469802856 \n",
      "     Training Step: 125 Training Loss: 0.6118203997612 \n",
      "     Training Step: 126 Training Loss: 0.619744598865509 \n",
      "     Training Step: 127 Training Loss: 0.6136544942855835 \n",
      "     Training Step: 128 Training Loss: 0.6199265122413635 \n",
      "     Training Step: 129 Training Loss: 0.6114017367362976 \n",
      "     Training Step: 130 Training Loss: 0.6151620745658875 \n",
      "     Training Step: 131 Training Loss: 0.6152414679527283 \n",
      "     Training Step: 132 Training Loss: 0.6141505241394043 \n",
      "     Training Step: 133 Training Loss: 0.6127666234970093 \n",
      "     Training Step: 134 Training Loss: 0.6143726706504822 \n",
      "     Training Step: 135 Training Loss: 0.6144962310791016 \n",
      "     Training Step: 136 Training Loss: 0.6151731610298157 \n",
      "     Training Step: 137 Training Loss: 0.6170859932899475 \n",
      "     Training Step: 138 Training Loss: 0.615530788898468 \n",
      "     Training Step: 139 Training Loss: 0.6139220595359802 \n",
      "     Training Step: 140 Training Loss: 0.6149183511734009 \n",
      "     Training Step: 141 Training Loss: 0.6156700253486633 \n",
      "     Training Step: 142 Training Loss: 0.6140430569648743 \n",
      "     Training Step: 143 Training Loss: 0.6164802312850952 \n",
      "     Training Step: 144 Training Loss: 0.6100180149078369 \n",
      "     Training Step: 145 Training Loss: 0.6162304878234863 \n",
      "     Training Step: 146 Training Loss: 0.6167961955070496 \n",
      "     Training Step: 147 Training Loss: 0.6141082644462585 \n",
      "     Training Step: 148 Training Loss: 0.6128026843070984 \n",
      "     Training Step: 149 Training Loss: 0.6185894012451172 \n",
      "     Training Step: 150 Training Loss: 0.61163729429245 \n",
      "     Training Step: 151 Training Loss: 0.6115921139717102 \n",
      "     Training Step: 152 Training Loss: 0.6152939796447754 \n",
      "     Training Step: 153 Training Loss: 0.6157631278038025 \n",
      "     Training Step: 154 Training Loss: 0.6120163202285767 \n",
      "     Training Step: 155 Training Loss: 0.6142884492874146 \n",
      "     Training Step: 156 Training Loss: 0.616814374923706 \n",
      "     Training Step: 157 Training Loss: 0.6152855753898621 \n",
      "     Training Step: 158 Training Loss: 0.6152662634849548 \n",
      "     Training Step: 159 Training Loss: 0.6105824708938599 \n",
      "     Training Step: 160 Training Loss: 0.6145990490913391 \n",
      "     Training Step: 161 Training Loss: 0.6153016686439514 \n",
      "     Training Step: 162 Training Loss: 0.6123814582824707 \n",
      "     Training Step: 163 Training Loss: 0.6122362017631531 \n",
      "     Training Step: 164 Training Loss: 0.6169291138648987 \n",
      "     Training Step: 165 Training Loss: 0.6100476384162903 \n",
      "     Training Step: 166 Training Loss: 0.6097286343574524 \n",
      "     Training Step: 167 Training Loss: 0.6125280261039734 \n",
      "     Training Step: 168 Training Loss: 0.6093910336494446 \n",
      "     Training Step: 169 Training Loss: 0.6158405542373657 \n",
      "     Training Step: 170 Training Loss: 0.6128670573234558 \n",
      "     Training Step: 171 Training Loss: 0.611190915107727 \n",
      "     Training Step: 172 Training Loss: 0.6122285723686218 \n",
      "     Training Step: 173 Training Loss: 0.6133145689964294 \n",
      "     Training Step: 174 Training Loss: 0.6149759888648987 \n",
      "     Training Step: 175 Training Loss: 0.6128430366516113 \n",
      "     Training Step: 176 Training Loss: 0.6156330704689026 \n",
      "     Training Step: 177 Training Loss: 0.6146238446235657 \n",
      "     Training Step: 178 Training Loss: 0.6130557060241699 \n",
      "     Training Step: 179 Training Loss: 0.6114084124565125 \n",
      "     Training Step: 180 Training Loss: 0.6096868515014648 \n",
      "     Training Step: 181 Training Loss: 0.615530788898468 \n",
      "     Training Step: 182 Training Loss: 0.6129160523414612 \n",
      "     Training Step: 183 Training Loss: 0.612913966178894 \n",
      "     Training Step: 184 Training Loss: 0.6146117448806763 \n",
      "     Training Step: 185 Training Loss: 0.6118245124816895 \n",
      "     Training Step: 186 Training Loss: 0.6147273182868958 \n",
      "     Training Step: 187 Training Loss: 0.6182388067245483 \n",
      "     Training Step: 188 Training Loss: 0.6153846979141235 \n",
      "     Training Step: 189 Training Loss: 0.6121552586555481 \n",
      "     Training Step: 190 Training Loss: 0.6166533827781677 \n",
      "     Training Step: 191 Training Loss: 0.6140753030776978 \n",
      "     Training Step: 192 Training Loss: 0.6132087111473083 \n",
      "     Training Step: 193 Training Loss: 0.6173691749572754 \n",
      "     Training Step: 194 Training Loss: 0.6176775097846985 \n",
      "     Training Step: 195 Training Loss: 0.6097528338432312 \n",
      "     Training Step: 196 Training Loss: 0.6154162883758545 \n",
      "     Training Step: 197 Training Loss: 0.615685224533081 \n",
      "     Training Step: 198 Training Loss: 0.6184253096580505 \n",
      "     Training Step: 199 Training Loss: 0.6161909699440002 \n",
      "     Training Step: 200 Training Loss: 0.6144505143165588 \n",
      "     Training Step: 201 Training Loss: 0.6167056560516357 \n",
      "     Training Step: 202 Training Loss: 0.6160331964492798 \n",
      "     Training Step: 203 Training Loss: 0.6109152436256409 \n",
      "     Training Step: 204 Training Loss: 0.613768994808197 \n",
      "     Training Step: 205 Training Loss: 0.6139312982559204 \n",
      "     Training Step: 206 Training Loss: 0.6167990565299988 \n",
      "     Training Step: 207 Training Loss: 0.6166998744010925 \n",
      "     Training Step: 208 Training Loss: 0.6105824112892151 \n",
      "     Training Step: 209 Training Loss: 0.6142433881759644 \n",
      "     Training Step: 210 Training Loss: 0.6182199120521545 \n",
      "     Training Step: 211 Training Loss: 0.6138868927955627 \n",
      "     Training Step: 212 Training Loss: 0.6163468360900879 \n",
      "     Training Step: 213 Training Loss: 0.6144793033599854 \n",
      "     Training Step: 214 Training Loss: 0.6146487593650818 \n",
      "     Training Step: 215 Training Loss: 0.6167033314704895 \n",
      "     Training Step: 216 Training Loss: 0.6154050827026367 \n",
      "     Training Step: 217 Training Loss: 0.6127143502235413 \n",
      "     Training Step: 218 Training Loss: 0.614016056060791 \n",
      "     Training Step: 219 Training Loss: 0.6157428026199341 \n",
      "     Training Step: 220 Training Loss: 0.6140241026878357 \n",
      "     Training Step: 221 Training Loss: 0.6114935874938965 \n",
      "     Training Step: 222 Training Loss: 0.6121436357498169 \n",
      "     Training Step: 223 Training Loss: 0.610910952091217 \n",
      "     Training Step: 224 Training Loss: 0.6118374466896057 \n",
      "     Training Step: 225 Training Loss: 0.6178045868873596 \n",
      "     Training Step: 226 Training Loss: 0.6166813969612122 \n",
      "     Training Step: 227 Training Loss: 0.6125297546386719 \n",
      "     Training Step: 228 Training Loss: 0.6122871041297913 \n",
      "     Training Step: 229 Training Loss: 0.617724597454071 \n",
      "     Training Step: 230 Training Loss: 0.6180359721183777 \n",
      "     Training Step: 231 Training Loss: 0.6168983578681946 \n",
      "     Training Step: 232 Training Loss: 0.6131402850151062 \n",
      "     Training Step: 233 Training Loss: 0.6184574365615845 \n",
      "     Training Step: 234 Training Loss: 0.613294243812561 \n",
      "     Training Step: 235 Training Loss: 0.613467276096344 \n",
      "     Training Step: 236 Training Loss: 0.6155281662940979 \n",
      "     Training Step: 237 Training Loss: 0.611416757106781 \n",
      "     Training Step: 238 Training Loss: 0.614870548248291 \n",
      "     Training Step: 239 Training Loss: 0.6171283721923828 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6183258891105652 \n",
      "     Validation Step: 1 Validation Loss: 0.6145398020744324 \n",
      "     Validation Step: 2 Validation Loss: 0.6136433482170105 \n",
      "     Validation Step: 3 Validation Loss: 0.6141201853752136 \n",
      "     Validation Step: 4 Validation Loss: 0.6105369925498962 \n",
      "     Validation Step: 5 Validation Loss: 0.6145661473274231 \n",
      "     Validation Step: 6 Validation Loss: 0.6111829876899719 \n",
      "     Validation Step: 7 Validation Loss: 0.6180450916290283 \n",
      "     Validation Step: 8 Validation Loss: 0.6170071959495544 \n",
      "     Validation Step: 9 Validation Loss: 0.6172894835472107 \n",
      "     Validation Step: 10 Validation Loss: 0.6148391366004944 \n",
      "     Validation Step: 11 Validation Loss: 0.6141394972801208 \n",
      "     Validation Step: 12 Validation Loss: 0.6101799607276917 \n",
      "     Validation Step: 13 Validation Loss: 0.6116570234298706 \n",
      "     Validation Step: 14 Validation Loss: 0.6156087517738342 \n",
      "     Validation Step: 15 Validation Loss: 0.6136741042137146 \n",
      "     Validation Step: 16 Validation Loss: 0.6175848245620728 \n",
      "     Validation Step: 17 Validation Loss: 0.6133147478103638 \n",
      "     Validation Step: 18 Validation Loss: 0.6184733510017395 \n",
      "     Validation Step: 19 Validation Loss: 0.6118980050086975 \n",
      "     Validation Step: 20 Validation Loss: 0.6146440505981445 \n",
      "     Validation Step: 21 Validation Loss: 0.6150380969047546 \n",
      "     Validation Step: 22 Validation Loss: 0.6148936748504639 \n",
      "     Validation Step: 23 Validation Loss: 0.6155667304992676 \n",
      "     Validation Step: 24 Validation Loss: 0.6157886981964111 \n",
      "     Validation Step: 25 Validation Loss: 0.6115736365318298 \n",
      "     Validation Step: 26 Validation Loss: 0.6101635694503784 \n",
      "     Validation Step: 27 Validation Loss: 0.6182107329368591 \n",
      "     Validation Step: 28 Validation Loss: 0.6111935377120972 \n",
      "     Validation Step: 29 Validation Loss: 0.6136534810066223 \n",
      "     Validation Step: 30 Validation Loss: 0.6142064332962036 \n",
      "     Validation Step: 31 Validation Loss: 0.6075621843338013 \n",
      "     Validation Step: 32 Validation Loss: 0.6153159141540527 \n",
      "     Validation Step: 33 Validation Loss: 0.6142590641975403 \n",
      "     Validation Step: 34 Validation Loss: 0.618453860282898 \n",
      "     Validation Step: 35 Validation Loss: 0.6176819205284119 \n",
      "     Validation Step: 36 Validation Loss: 0.6128430962562561 \n",
      "     Validation Step: 37 Validation Loss: 0.615983784198761 \n",
      "     Validation Step: 38 Validation Loss: 0.6162366271018982 \n",
      "     Validation Step: 39 Validation Loss: 0.6152241826057434 \n",
      "     Validation Step: 40 Validation Loss: 0.6106491684913635 \n",
      "     Validation Step: 41 Validation Loss: 0.6101375818252563 \n",
      "     Validation Step: 42 Validation Loss: 0.6129980683326721 \n",
      "     Validation Step: 43 Validation Loss: 0.6121436357498169 \n",
      "     Validation Step: 44 Validation Loss: 0.6105045080184937 \n",
      "Epoch: 116\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6156706213951111 \n",
      "     Training Step: 1 Training Loss: 0.6137688755989075 \n",
      "     Training Step: 2 Training Loss: 0.6100735664367676 \n",
      "     Training Step: 3 Training Loss: 0.6138861179351807 \n",
      "     Training Step: 4 Training Loss: 0.611539900302887 \n",
      "     Training Step: 5 Training Loss: 0.612069308757782 \n",
      "     Training Step: 6 Training Loss: 0.6141055226325989 \n",
      "     Training Step: 7 Training Loss: 0.6122242212295532 \n",
      "     Training Step: 8 Training Loss: 0.6131837964057922 \n",
      "     Training Step: 9 Training Loss: 0.6162239909172058 \n",
      "     Training Step: 10 Training Loss: 0.6125267148017883 \n",
      "     Training Step: 11 Training Loss: 0.6118200421333313 \n",
      "     Training Step: 12 Training Loss: 0.6115206480026245 \n",
      "     Training Step: 13 Training Loss: 0.6118013858795166 \n",
      "     Training Step: 14 Training Loss: 0.6115131974220276 \n",
      "     Training Step: 15 Training Loss: 0.6104769110679626 \n",
      "     Training Step: 16 Training Loss: 0.6100429892539978 \n",
      "     Training Step: 17 Training Loss: 0.6199411749839783 \n",
      "     Training Step: 18 Training Loss: 0.6113995313644409 \n",
      "     Training Step: 19 Training Loss: 0.6118168830871582 \n",
      "     Training Step: 20 Training Loss: 0.6202631592750549 \n",
      "     Training Step: 21 Training Loss: 0.6147735118865967 \n",
      "     Training Step: 22 Training Loss: 0.6099724769592285 \n",
      "     Training Step: 23 Training Loss: 0.6139066219329834 \n",
      "     Training Step: 24 Training Loss: 0.6141509413719177 \n",
      "     Training Step: 25 Training Loss: 0.612127959728241 \n",
      "     Training Step: 26 Training Loss: 0.6126948595046997 \n",
      "     Training Step: 27 Training Loss: 0.6130619049072266 \n",
      "     Training Step: 28 Training Loss: 0.6178041100502014 \n",
      "     Training Step: 29 Training Loss: 0.6153925657272339 \n",
      "     Training Step: 30 Training Loss: 0.6092391014099121 \n",
      "     Training Step: 31 Training Loss: 0.6082600355148315 \n",
      "     Training Step: 32 Training Loss: 0.6180425882339478 \n",
      "     Training Step: 33 Training Loss: 0.6135079860687256 \n",
      "     Training Step: 34 Training Loss: 0.6178238391876221 \n",
      "     Training Step: 35 Training Loss: 0.6147273778915405 \n",
      "     Training Step: 36 Training Loss: 0.6139201521873474 \n",
      "     Training Step: 37 Training Loss: 0.6149185299873352 \n",
      "     Training Step: 38 Training Loss: 0.6174941062927246 \n",
      "     Training Step: 39 Training Loss: 0.6177079081535339 \n",
      "     Training Step: 40 Training Loss: 0.6149300932884216 \n",
      "     Training Step: 41 Training Loss: 0.6124941110610962 \n",
      "     Training Step: 42 Training Loss: 0.6157811284065247 \n",
      "     Training Step: 43 Training Loss: 0.6166422367095947 \n",
      "     Training Step: 44 Training Loss: 0.6123989224433899 \n",
      "     Training Step: 45 Training Loss: 0.615065336227417 \n",
      "     Training Step: 46 Training Loss: 0.6140437722206116 \n",
      "     Training Step: 47 Training Loss: 0.6147054433822632 \n",
      "     Training Step: 48 Training Loss: 0.6138238906860352 \n",
      "     Training Step: 49 Training Loss: 0.6111577749252319 \n",
      "     Training Step: 50 Training Loss: 0.6146672964096069 \n",
      "     Training Step: 51 Training Loss: 0.6104057431221008 \n",
      "     Training Step: 52 Training Loss: 0.6137411594390869 \n",
      "     Training Step: 53 Training Loss: 0.612280011177063 \n",
      "     Training Step: 54 Training Loss: 0.6126397252082825 \n",
      "     Training Step: 55 Training Loss: 0.6134536862373352 \n",
      "     Training Step: 56 Training Loss: 0.6106770634651184 \n",
      "     Training Step: 57 Training Loss: 0.6127998232841492 \n",
      "     Training Step: 58 Training Loss: 0.6153572797775269 \n",
      "     Training Step: 59 Training Loss: 0.614978551864624 \n",
      "     Training Step: 60 Training Loss: 0.6154698729515076 \n",
      "     Training Step: 61 Training Loss: 0.6153014302253723 \n",
      "     Training Step: 62 Training Loss: 0.6125233769416809 \n",
      "     Training Step: 63 Training Loss: 0.6113900542259216 \n",
      "     Training Step: 64 Training Loss: 0.6155420541763306 \n",
      "     Training Step: 65 Training Loss: 0.6130680441856384 \n",
      "     Training Step: 66 Training Loss: 0.6150906085968018 \n",
      "     Training Step: 67 Training Loss: 0.6118390560150146 \n",
      "     Training Step: 68 Training Loss: 0.6108902096748352 \n",
      "     Training Step: 69 Training Loss: 0.6171941757202148 \n",
      "     Training Step: 70 Training Loss: 0.6121466159820557 \n",
      "     Training Step: 71 Training Loss: 0.6144549250602722 \n",
      "     Training Step: 72 Training Loss: 0.6143377423286438 \n",
      "     Training Step: 73 Training Loss: 0.6132755875587463 \n",
      "     Training Step: 74 Training Loss: 0.616925835609436 \n",
      "     Training Step: 75 Training Loss: 0.6209295392036438 \n",
      "     Training Step: 76 Training Loss: 0.6116287708282471 \n",
      "     Training Step: 77 Training Loss: 0.614676833152771 \n",
      "     Training Step: 78 Training Loss: 0.6154026389122009 \n",
      "     Training Step: 79 Training Loss: 0.6167474985122681 \n",
      "     Training Step: 80 Training Loss: 0.6124730706214905 \n",
      "     Training Step: 81 Training Loss: 0.6124062538146973 \n",
      "     Training Step: 82 Training Loss: 0.6195896863937378 \n",
      "     Training Step: 83 Training Loss: 0.6180009245872498 \n",
      "     Training Step: 84 Training Loss: 0.6167962551116943 \n",
      "     Training Step: 85 Training Loss: 0.6182319521903992 \n",
      "     Training Step: 86 Training Loss: 0.6185473203659058 \n",
      "     Training Step: 87 Training Loss: 0.6162204146385193 \n",
      "     Training Step: 88 Training Loss: 0.6154817342758179 \n",
      "     Training Step: 89 Training Loss: 0.6166836619377136 \n",
      "     Training Step: 90 Training Loss: 0.6143581867218018 \n",
      "     Training Step: 91 Training Loss: 0.61647629737854 \n",
      "     Training Step: 92 Training Loss: 0.6171181201934814 \n",
      "     Training Step: 93 Training Loss: 0.6181888580322266 \n",
      "     Training Step: 94 Training Loss: 0.618270993232727 \n",
      "     Training Step: 95 Training Loss: 0.6142168045043945 \n",
      "     Training Step: 96 Training Loss: 0.6157485842704773 \n",
      "     Training Step: 97 Training Loss: 0.6118626594543457 \n",
      "     Training Step: 98 Training Loss: 0.6128627061843872 \n",
      "     Training Step: 99 Training Loss: 0.617669939994812 \n",
      "     Training Step: 100 Training Loss: 0.6112275719642639 \n",
      "     Training Step: 101 Training Loss: 0.6184272170066833 \n",
      "     Training Step: 102 Training Loss: 0.6120287775993347 \n",
      "     Training Step: 103 Training Loss: 0.6127703189849854 \n",
      "     Training Step: 104 Training Loss: 0.6122308373451233 \n",
      "     Training Step: 105 Training Loss: 0.6160067319869995 \n",
      "     Training Step: 106 Training Loss: 0.6168098449707031 \n",
      "     Training Step: 107 Training Loss: 0.6156877279281616 \n",
      "     Training Step: 108 Training Loss: 0.6164100170135498 \n",
      "     Training Step: 109 Training Loss: 0.6132774353027344 \n",
      "     Training Step: 110 Training Loss: 0.6100538969039917 \n",
      "     Training Step: 111 Training Loss: 0.6194548010826111 \n",
      "     Training Step: 112 Training Loss: 0.6105896234512329 \n",
      "     Training Step: 113 Training Loss: 0.6133022904396057 \n",
      "     Training Step: 114 Training Loss: 0.6143435835838318 \n",
      "     Training Step: 115 Training Loss: 0.6151530742645264 \n",
      "     Training Step: 116 Training Loss: 0.6167218089103699 \n",
      "     Training Step: 117 Training Loss: 0.6146380305290222 \n",
      "     Training Step: 118 Training Loss: 0.6106621623039246 \n",
      "     Training Step: 119 Training Loss: 0.6142086982727051 \n",
      "     Training Step: 120 Training Loss: 0.613204300403595 \n",
      "     Training Step: 121 Training Loss: 0.6142805218696594 \n",
      "     Training Step: 122 Training Loss: 0.6141497492790222 \n",
      "     Training Step: 123 Training Loss: 0.6129181981086731 \n",
      "     Training Step: 124 Training Loss: 0.6105690002441406 \n",
      "     Training Step: 125 Training Loss: 0.615605890750885 \n",
      "     Training Step: 126 Training Loss: 0.6143763065338135 \n",
      "     Training Step: 127 Training Loss: 0.6105659008026123 \n",
      "     Training Step: 128 Training Loss: 0.6121528148651123 \n",
      "     Training Step: 129 Training Loss: 0.6144801378250122 \n",
      "     Training Step: 130 Training Loss: 0.6125079989433289 \n",
      "     Training Step: 131 Training Loss: 0.6155465841293335 \n",
      "     Training Step: 132 Training Loss: 0.6122311949729919 \n",
      "     Training Step: 133 Training Loss: 0.6154254078865051 \n",
      "     Training Step: 134 Training Loss: 0.6107125282287598 \n",
      "     Training Step: 135 Training Loss: 0.6133534908294678 \n",
      "     Training Step: 136 Training Loss: 0.610124945640564 \n",
      "     Training Step: 137 Training Loss: 0.6134966611862183 \n",
      "     Training Step: 138 Training Loss: 0.6157642602920532 \n",
      "     Training Step: 139 Training Loss: 0.6117752194404602 \n",
      "     Training Step: 140 Training Loss: 0.6186307072639465 \n",
      "     Training Step: 141 Training Loss: 0.6177037954330444 \n",
      "     Training Step: 142 Training Loss: 0.609443724155426 \n",
      "     Training Step: 143 Training Loss: 0.6105726361274719 \n",
      "     Training Step: 144 Training Loss: 0.6128623485565186 \n",
      "     Training Step: 145 Training Loss: 0.6114675998687744 \n",
      "     Training Step: 146 Training Loss: 0.6116366386413574 \n",
      "     Training Step: 147 Training Loss: 0.6152704954147339 \n",
      "     Training Step: 148 Training Loss: 0.6131356954574585 \n",
      "     Training Step: 149 Training Loss: 0.6146090030670166 \n",
      "     Training Step: 150 Training Loss: 0.6097166538238525 \n",
      "     Training Step: 151 Training Loss: 0.6176642179489136 \n",
      "     Training Step: 152 Training Loss: 0.6152435541152954 \n",
      "     Training Step: 153 Training Loss: 0.6153748631477356 \n",
      "     Training Step: 154 Training Loss: 0.6116050481796265 \n",
      "     Training Step: 155 Training Loss: 0.6116307973861694 \n",
      "     Training Step: 156 Training Loss: 0.6129140257835388 \n",
      "     Training Step: 157 Training Loss: 0.6132899522781372 \n",
      "     Training Step: 158 Training Loss: 0.6101808547973633 \n",
      "     Training Step: 159 Training Loss: 0.6093962788581848 \n",
      "     Training Step: 160 Training Loss: 0.6140167117118835 \n",
      "     Training Step: 161 Training Loss: 0.6114217042922974 \n",
      "     Training Step: 162 Training Loss: 0.6181267499923706 \n",
      "     Training Step: 163 Training Loss: 0.6121516227722168 \n",
      "     Training Step: 164 Training Loss: 0.6163693070411682 \n",
      "     Training Step: 165 Training Loss: 0.6111487150192261 \n",
      "     Training Step: 166 Training Loss: 0.6166399121284485 \n",
      "     Training Step: 167 Training Loss: 0.6169085502624512 \n",
      "     Training Step: 168 Training Loss: 0.6152908802032471 \n",
      "     Training Step: 169 Training Loss: 0.6096962094306946 \n",
      "     Training Step: 170 Training Loss: 0.6128693222999573 \n",
      "     Training Step: 171 Training Loss: 0.61037278175354 \n",
      "     Training Step: 172 Training Loss: 0.6133102774620056 \n",
      "     Training Step: 173 Training Loss: 0.6151016354560852 \n",
      "     Training Step: 174 Training Loss: 0.6164237260818481 \n",
      "     Training Step: 175 Training Loss: 0.6108766794204712 \n",
      "     Training Step: 176 Training Loss: 0.613635241985321 \n",
      "     Training Step: 177 Training Loss: 0.6171040534973145 \n",
      "     Training Step: 178 Training Loss: 0.6115549206733704 \n",
      "     Training Step: 179 Training Loss: 0.6107332706451416 \n",
      "     Training Step: 180 Training Loss: 0.61141437292099 \n",
      "     Training Step: 181 Training Loss: 0.614498496055603 \n",
      "     Training Step: 182 Training Loss: 0.6136479377746582 \n",
      "     Training Step: 183 Training Loss: 0.6166604161262512 \n",
      "     Training Step: 184 Training Loss: 0.6132035255432129 \n",
      "     Training Step: 185 Training Loss: 0.6188792586326599 \n",
      "     Training Step: 186 Training Loss: 0.6147921681404114 \n",
      "     Training Step: 187 Training Loss: 0.6160315871238708 \n",
      "     Training Step: 188 Training Loss: 0.6180639266967773 \n",
      "     Training Step: 189 Training Loss: 0.6166666746139526 \n",
      "     Training Step: 190 Training Loss: 0.6146913170814514 \n",
      "     Training Step: 191 Training Loss: 0.616191565990448 \n",
      "     Training Step: 192 Training Loss: 0.6097783446311951 \n",
      "     Training Step: 193 Training Loss: 0.6152909994125366 \n",
      "     Training Step: 194 Training Loss: 0.6144526600837708 \n",
      "     Training Step: 195 Training Loss: 0.6146141886711121 \n",
      "     Training Step: 196 Training Loss: 0.6173627972602844 \n",
      "     Training Step: 197 Training Loss: 0.6167116165161133 \n",
      "     Training Step: 198 Training Loss: 0.6116125583648682 \n",
      "     Training Step: 199 Training Loss: 0.6137433052062988 \n",
      "     Training Step: 200 Training Loss: 0.6158262491226196 \n",
      "     Training Step: 201 Training Loss: 0.6151725053787231 \n",
      "     Training Step: 202 Training Loss: 0.6146092414855957 \n",
      "     Training Step: 203 Training Loss: 0.6184050440788269 \n",
      "     Training Step: 204 Training Loss: 0.614251971244812 \n",
      "     Training Step: 205 Training Loss: 0.6104927659034729 \n",
      "     Training Step: 206 Training Loss: 0.6144245862960815 \n",
      "     Training Step: 207 Training Loss: 0.6140219569206238 \n",
      "     Training Step: 208 Training Loss: 0.6147449612617493 \n",
      "     Training Step: 209 Training Loss: 0.6168085336685181 \n",
      "     Training Step: 210 Training Loss: 0.6166771054267883 \n",
      "     Training Step: 211 Training Loss: 0.6167070865631104 \n",
      "     Training Step: 212 Training Loss: 0.6147019267082214 \n",
      "     Training Step: 213 Training Loss: 0.6134145259857178 \n",
      "     Training Step: 214 Training Loss: 0.6153908967971802 \n",
      "     Training Step: 215 Training Loss: 0.6117016077041626 \n",
      "     Training Step: 216 Training Loss: 0.6134811043739319 \n",
      "     Training Step: 217 Training Loss: 0.6171334981918335 \n",
      "     Training Step: 218 Training Loss: 0.6114546656608582 \n",
      "     Training Step: 219 Training Loss: 0.6188328266143799 \n",
      "     Training Step: 220 Training Loss: 0.6118884682655334 \n",
      "     Training Step: 221 Training Loss: 0.614868700504303 \n",
      "     Training Step: 222 Training Loss: 0.6123782396316528 \n",
      "     Training Step: 223 Training Loss: 0.6136218905448914 \n",
      "     Training Step: 224 Training Loss: 0.6161161661148071 \n",
      "     Training Step: 225 Training Loss: 0.6155178546905518 \n",
      "     Training Step: 226 Training Loss: 0.6122909188270569 \n",
      "     Training Step: 227 Training Loss: 0.6135879755020142 \n",
      "     Training Step: 228 Training Loss: 0.6146501302719116 \n",
      "     Training Step: 229 Training Loss: 0.6159542798995972 \n",
      "     Training Step: 230 Training Loss: 0.6140719652175903 \n",
      "     Training Step: 231 Training Loss: 0.6157475709915161 \n",
      "     Training Step: 232 Training Loss: 0.6127551198005676 \n",
      "     Training Step: 233 Training Loss: 0.6197000741958618 \n",
      "     Training Step: 234 Training Loss: 0.612969696521759 \n",
      "     Training Step: 235 Training Loss: 0.6177541017532349 \n",
      "     Training Step: 236 Training Loss: 0.6184488534927368 \n",
      "     Training Step: 237 Training Loss: 0.613131582736969 \n",
      "     Training Step: 238 Training Loss: 0.6146754026412964 \n",
      "     Training Step: 239 Training Loss: 0.6171467304229736 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6159847974777222 \n",
      "     Validation Step: 1 Validation Loss: 0.6141495108604431 \n",
      "     Validation Step: 2 Validation Loss: 0.6136826872825623 \n",
      "     Validation Step: 3 Validation Loss: 0.6172853112220764 \n",
      "     Validation Step: 4 Validation Loss: 0.6176778078079224 \n",
      "     Validation Step: 5 Validation Loss: 0.6141268014907837 \n",
      "     Validation Step: 6 Validation Loss: 0.6148986220359802 \n",
      "     Validation Step: 7 Validation Loss: 0.6121551990509033 \n",
      "     Validation Step: 8 Validation Loss: 0.6133225560188293 \n",
      "     Validation Step: 9 Validation Loss: 0.614212691783905 \n",
      "     Validation Step: 10 Validation Loss: 0.6170052289962769 \n",
      "     Validation Step: 11 Validation Loss: 0.6175822615623474 \n",
      "     Validation Step: 12 Validation Loss: 0.6130083203315735 \n",
      "     Validation Step: 13 Validation Loss: 0.6115859150886536 \n",
      "     Validation Step: 14 Validation Loss: 0.6148437261581421 \n",
      "     Validation Step: 15 Validation Loss: 0.6142656207084656 \n",
      "     Validation Step: 16 Validation Loss: 0.61066073179245 \n",
      "     Validation Step: 17 Validation Loss: 0.6145676374435425 \n",
      "     Validation Step: 18 Validation Loss: 0.6145456433296204 \n",
      "     Validation Step: 19 Validation Loss: 0.6150436401367188 \n",
      "     Validation Step: 20 Validation Loss: 0.6112045049667358 \n",
      "     Validation Step: 21 Validation Loss: 0.6152248978614807 \n",
      "     Validation Step: 22 Validation Loss: 0.6101554036140442 \n",
      "     Validation Step: 23 Validation Loss: 0.6182080507278442 \n",
      "     Validation Step: 24 Validation Loss: 0.6111956238746643 \n",
      "     Validation Step: 25 Validation Loss: 0.6184483766555786 \n",
      "     Validation Step: 26 Validation Loss: 0.6180410981178284 \n",
      "     Validation Step: 27 Validation Loss: 0.6162389516830444 \n",
      "     Validation Step: 28 Validation Loss: 0.6136479377746582 \n",
      "     Validation Step: 29 Validation Loss: 0.618323802947998 \n",
      "     Validation Step: 30 Validation Loss: 0.6157889366149902 \n",
      "     Validation Step: 31 Validation Loss: 0.6128534078598022 \n",
      "     Validation Step: 32 Validation Loss: 0.6101797223091125 \n",
      "     Validation Step: 33 Validation Loss: 0.6105520725250244 \n",
      "     Validation Step: 34 Validation Loss: 0.6156099438667297 \n",
      "     Validation Step: 35 Validation Loss: 0.6101924777030945 \n",
      "     Validation Step: 36 Validation Loss: 0.6119101643562317 \n",
      "     Validation Step: 37 Validation Loss: 0.6116666793823242 \n",
      "     Validation Step: 38 Validation Loss: 0.6146500706672668 \n",
      "     Validation Step: 39 Validation Loss: 0.6075844764709473 \n",
      "     Validation Step: 40 Validation Loss: 0.610519289970398 \n",
      "     Validation Step: 41 Validation Loss: 0.6153218746185303 \n",
      "     Validation Step: 42 Validation Loss: 0.6155661940574646 \n",
      "     Validation Step: 43 Validation Loss: 0.6136611700057983 \n",
      "     Validation Step: 44 Validation Loss: 0.6184662580490112 \n",
      "Epoch: 117\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6143420338630676 \n",
      "     Training Step: 1 Training Loss: 0.6166985034942627 \n",
      "     Training Step: 2 Training Loss: 0.6147318482398987 \n",
      "     Training Step: 3 Training Loss: 0.6118288636207581 \n",
      "     Training Step: 4 Training Loss: 0.6146030426025391 \n",
      "     Training Step: 5 Training Loss: 0.6118530035018921 \n",
      "     Training Step: 6 Training Loss: 0.61460942029953 \n",
      "     Training Step: 7 Training Loss: 0.6140180230140686 \n",
      "     Training Step: 8 Training Loss: 0.6115304231643677 \n",
      "     Training Step: 9 Training Loss: 0.6176422834396362 \n",
      "     Training Step: 10 Training Loss: 0.6137630343437195 \n",
      "     Training Step: 11 Training Loss: 0.6151574850082397 \n",
      "     Training Step: 12 Training Loss: 0.6117765307426453 \n",
      "     Training Step: 13 Training Loss: 0.6152869462966919 \n",
      "     Training Step: 14 Training Loss: 0.6114116311073303 \n",
      "     Training Step: 15 Training Loss: 0.6150895953178406 \n",
      "     Training Step: 16 Training Loss: 0.6164286732673645 \n",
      "     Training Step: 17 Training Loss: 0.6153778433799744 \n",
      "     Training Step: 18 Training Loss: 0.6168133020401001 \n",
      "     Training Step: 19 Training Loss: 0.6125176548957825 \n",
      "     Training Step: 20 Training Loss: 0.6142851114273071 \n",
      "     Training Step: 21 Training Loss: 0.6196123361587524 \n",
      "     Training Step: 22 Training Loss: 0.6143440008163452 \n",
      "     Training Step: 23 Training Loss: 0.6209088563919067 \n",
      "     Training Step: 24 Training Loss: 0.6133112907409668 \n",
      "     Training Step: 25 Training Loss: 0.6125582456588745 \n",
      "     Training Step: 26 Training Loss: 0.6135161519050598 \n",
      "     Training Step: 27 Training Loss: 0.6124120950698853 \n",
      "     Training Step: 28 Training Loss: 0.6144518256187439 \n",
      "     Training Step: 29 Training Loss: 0.6176770329475403 \n",
      "     Training Step: 30 Training Loss: 0.6146080493927002 \n",
      "     Training Step: 31 Training Loss: 0.6107531189918518 \n",
      "     Training Step: 32 Training Loss: 0.6114055514335632 \n",
      "     Training Step: 33 Training Loss: 0.6136341691017151 \n",
      "     Training Step: 34 Training Loss: 0.6171475648880005 \n",
      "     Training Step: 35 Training Loss: 0.6134664416313171 \n",
      "     Training Step: 36 Training Loss: 0.6120624542236328 \n",
      "     Training Step: 37 Training Loss: 0.6126996874809265 \n",
      "     Training Step: 38 Training Loss: 0.6105666756629944 \n",
      "     Training Step: 39 Training Loss: 0.6129232048988342 \n",
      "     Training Step: 40 Training Loss: 0.6169638633728027 \n",
      "     Training Step: 41 Training Loss: 0.6153542995452881 \n",
      "     Training Step: 42 Training Loss: 0.6094316840171814 \n",
      "     Training Step: 43 Training Loss: 0.6167405247688293 \n",
      "     Training Step: 44 Training Loss: 0.6182535290718079 \n",
      "     Training Step: 45 Training Loss: 0.609686017036438 \n",
      "     Training Step: 46 Training Loss: 0.6130533218383789 \n",
      "     Training Step: 47 Training Loss: 0.6153936386108398 \n",
      "     Training Step: 48 Training Loss: 0.6106862425804138 \n",
      "     Training Step: 49 Training Loss: 0.6143778562545776 \n",
      "     Training Step: 50 Training Loss: 0.6124657392501831 \n",
      "     Training Step: 51 Training Loss: 0.6133108139038086 \n",
      "     Training Step: 52 Training Loss: 0.6105909943580627 \n",
      "     Training Step: 53 Training Loss: 0.6097159385681152 \n",
      "     Training Step: 54 Training Loss: 0.6103631258010864 \n",
      "     Training Step: 55 Training Loss: 0.6180482506752014 \n",
      "     Training Step: 56 Training Loss: 0.6138159036636353 \n",
      "     Training Step: 57 Training Loss: 0.6100329160690308 \n",
      "     Training Step: 58 Training Loss: 0.616682767868042 \n",
      "     Training Step: 59 Training Loss: 0.6128415465354919 \n",
      "     Training Step: 60 Training Loss: 0.6144874095916748 \n",
      "     Training Step: 61 Training Loss: 0.6127956509590149 \n",
      "     Training Step: 62 Training Loss: 0.6202502846717834 \n",
      "     Training Step: 63 Training Loss: 0.6140262484550476 \n",
      "     Training Step: 64 Training Loss: 0.6146393418312073 \n",
      "     Training Step: 65 Training Loss: 0.6144547462463379 \n",
      "     Training Step: 66 Training Loss: 0.6118868589401245 \n",
      "     Training Step: 67 Training Loss: 0.6132068634033203 \n",
      "     Training Step: 68 Training Loss: 0.6123809218406677 \n",
      "     Training Step: 69 Training Loss: 0.6137445569038391 \n",
      "     Training Step: 70 Training Loss: 0.6167157888412476 \n",
      "     Training Step: 71 Training Loss: 0.6111683249473572 \n",
      "     Training Step: 72 Training Loss: 0.611636757850647 \n",
      "     Training Step: 73 Training Loss: 0.614208459854126 \n",
      "     Training Step: 74 Training Loss: 0.6154403686523438 \n",
      "     Training Step: 75 Training Loss: 0.6156874299049377 \n",
      "     Training Step: 76 Training Loss: 0.6163527369499207 \n",
      "     Training Step: 77 Training Loss: 0.6138870716094971 \n",
      "     Training Step: 78 Training Loss: 0.6101821064949036 \n",
      "     Training Step: 79 Training Loss: 0.6194444298744202 \n",
      "     Training Step: 80 Training Loss: 0.617192804813385 \n",
      "     Training Step: 81 Training Loss: 0.6168011426925659 \n",
      "     Training Step: 82 Training Loss: 0.6122472286224365 \n",
      "     Training Step: 83 Training Loss: 0.6176903247833252 \n",
      "     Training Step: 84 Training Loss: 0.6146749258041382 \n",
      "     Training Step: 85 Training Loss: 0.6183878779411316 \n",
      "     Training Step: 86 Training Loss: 0.6150653958320618 \n",
      "     Training Step: 87 Training Loss: 0.6184185147285461 \n",
      "     Training Step: 88 Training Loss: 0.6104423403739929 \n",
      "     Training Step: 89 Training Loss: 0.6130790114402771 \n",
      "     Training Step: 90 Training Loss: 0.6141196489334106 \n",
      "     Training Step: 91 Training Loss: 0.6167411804199219 \n",
      "     Training Step: 92 Training Loss: 0.6120367646217346 \n",
      "     Training Step: 93 Training Loss: 0.6159375905990601 \n",
      "     Training Step: 94 Training Loss: 0.6147437691688538 \n",
      "     Training Step: 95 Training Loss: 0.6152593493461609 \n",
      "     Training Step: 96 Training Loss: 0.6177527904510498 \n",
      "     Training Step: 97 Training Loss: 0.614149272441864 \n",
      "     Training Step: 98 Training Loss: 0.6106737852096558 \n",
      "     Training Step: 99 Training Loss: 0.6166070699691772 \n",
      "     Training Step: 100 Training Loss: 0.6144211888313293 \n",
      "     Training Step: 101 Training Loss: 0.6146447658538818 \n",
      "     Training Step: 102 Training Loss: 0.617158830165863 \n",
      "     Training Step: 103 Training Loss: 0.6121684312820435 \n",
      "     Training Step: 104 Training Loss: 0.6116333603858948 \n",
      "     Training Step: 105 Training Loss: 0.6139271855354309 \n",
      "     Training Step: 106 Training Loss: 0.619871973991394 \n",
      "     Training Step: 107 Training Loss: 0.6155344843864441 \n",
      "     Training Step: 108 Training Loss: 0.6116894483566284 \n",
      "     Training Step: 109 Training Loss: 0.6154730319976807 \n",
      "     Training Step: 110 Training Loss: 0.616195797920227 \n",
      "     Training Step: 111 Training Loss: 0.6129125952720642 \n",
      "     Training Step: 112 Training Loss: 0.6134215593338013 \n",
      "     Training Step: 113 Training Loss: 0.6115996241569519 \n",
      "     Training Step: 114 Training Loss: 0.6114446520805359 \n",
      "     Training Step: 115 Training Loss: 0.610886812210083 \n",
      "     Training Step: 116 Training Loss: 0.6154217720031738 \n",
      "     Training Step: 117 Training Loss: 0.6161161661148071 \n",
      "     Training Step: 118 Training Loss: 0.6107144355773926 \n",
      "     Training Step: 119 Training Loss: 0.6188908815383911 \n",
      "     Training Step: 120 Training Loss: 0.6133547425270081 \n",
      "     Training Step: 121 Training Loss: 0.6105600595474243 \n",
      "     Training Step: 122 Training Loss: 0.6115984320640564 \n",
      "     Training Step: 123 Training Loss: 0.6118037700653076 \n",
      "     Training Step: 124 Training Loss: 0.6140318512916565 \n",
      "     Training Step: 125 Training Loss: 0.6148797869682312 \n",
      "     Training Step: 126 Training Loss: 0.6122831702232361 \n",
      "     Training Step: 127 Training Loss: 0.6166897416114807 \n",
      "     Training Step: 128 Training Loss: 0.6125227212905884 \n",
      "     Training Step: 129 Training Loss: 0.6175084710121155 \n",
      "     Training Step: 130 Training Loss: 0.6142150163650513 \n",
      "     Training Step: 131 Training Loss: 0.6173935532569885 \n",
      "     Training Step: 132 Training Loss: 0.611409604549408 \n",
      "     Training Step: 133 Training Loss: 0.6176889538764954 \n",
      "     Training Step: 134 Training Loss: 0.6180927157402039 \n",
      "     Training Step: 135 Training Loss: 0.6116573214530945 \n",
      "     Training Step: 136 Training Loss: 0.6158289909362793 \n",
      "     Training Step: 137 Training Loss: 0.6092717051506042 \n",
      "     Training Step: 138 Training Loss: 0.6180189847946167 \n",
      "     Training Step: 139 Training Loss: 0.609441876411438 \n",
      "     Training Step: 140 Training Loss: 0.612147331237793 \n",
      "     Training Step: 141 Training Loss: 0.6097387075424194 \n",
      "     Training Step: 142 Training Loss: 0.6127638816833496 \n",
      "     Training Step: 143 Training Loss: 0.6082600355148315 \n",
      "     Training Step: 144 Training Loss: 0.6149271726608276 \n",
      "     Training Step: 145 Training Loss: 0.6145122051239014 \n",
      "     Training Step: 146 Training Loss: 0.6142572164535522 \n",
      "     Training Step: 147 Training Loss: 0.6185178756713867 \n",
      "     Training Step: 148 Training Loss: 0.6101160645484924 \n",
      "     Training Step: 149 Training Loss: 0.6148065328598022 \n",
      "     Training Step: 150 Training Loss: 0.6136558651924133 \n",
      "     Training Step: 151 Training Loss: 0.6122768521308899 \n",
      "     Training Step: 152 Training Loss: 0.6099657416343689 \n",
      "     Training Step: 153 Training Loss: 0.6186156272888184 \n",
      "     Training Step: 154 Training Loss: 0.6143545508384705 \n",
      "     Training Step: 155 Training Loss: 0.6111395955085754 \n",
      "     Training Step: 156 Training Loss: 0.6128655672073364 \n",
      "     Training Step: 157 Training Loss: 0.613909125328064 \n",
      "     Training Step: 158 Training Loss: 0.6186062693595886 \n",
      "     Training Step: 159 Training Loss: 0.6132799386978149 \n",
      "     Training Step: 160 Training Loss: 0.6157558560371399 \n",
      "     Training Step: 161 Training Loss: 0.6168032288551331 \n",
      "     Training Step: 162 Training Loss: 0.6170881986618042 \n",
      "     Training Step: 163 Training Loss: 0.6157400608062744 \n",
      "     Training Step: 164 Training Loss: 0.6105057001113892 \n",
      "     Training Step: 165 Training Loss: 0.6159929633140564 \n",
      "     Training Step: 166 Training Loss: 0.6196784973144531 \n",
      "     Training Step: 167 Training Loss: 0.6153754591941833 \n",
      "     Training Step: 168 Training Loss: 0.6115501523017883 \n",
      "     Training Step: 169 Training Loss: 0.6182955503463745 \n",
      "     Training Step: 170 Training Loss: 0.6156823635101318 \n",
      "     Training Step: 171 Training Loss: 0.6146758794784546 \n",
      "     Training Step: 172 Training Loss: 0.614776611328125 \n",
      "     Training Step: 173 Training Loss: 0.6141595840454102 \n",
      "     Training Step: 174 Training Loss: 0.6155107617378235 \n",
      "     Training Step: 175 Training Loss: 0.6131518483161926 \n",
      "     Training Step: 176 Training Loss: 0.617133378982544 \n",
      "     Training Step: 177 Training Loss: 0.6121290922164917 \n",
      "     Training Step: 178 Training Loss: 0.6147046089172363 \n",
      "     Training Step: 179 Training Loss: 0.6164891719818115 \n",
      "     Training Step: 180 Training Loss: 0.6152886152267456 \n",
      "     Training Step: 181 Training Loss: 0.6132814288139343 \n",
      "     Training Step: 182 Training Loss: 0.6135135293006897 \n",
      "     Training Step: 183 Training Loss: 0.6112033128738403 \n",
      "     Training Step: 184 Training Loss: 0.6114394068717957 \n",
      "     Training Step: 185 Training Loss: 0.6134543418884277 \n",
      "     Training Step: 186 Training Loss: 0.6127511858940125 \n",
      "     Training Step: 187 Training Loss: 0.6128674745559692 \n",
      "     Training Step: 188 Training Loss: 0.612374484539032 \n",
      "     Training Step: 189 Training Loss: 0.6147141456604004 \n",
      "     Training Step: 190 Training Loss: 0.6146975159645081 \n",
      "     Training Step: 191 Training Loss: 0.6151843667030334 \n",
      "     Training Step: 192 Training Loss: 0.610871434211731 \n",
      "     Training Step: 193 Training Loss: 0.6167019009590149 \n",
      "     Training Step: 194 Training Loss: 0.6152414679527283 \n",
      "     Training Step: 195 Training Loss: 0.6167230606079102 \n",
      "     Training Step: 196 Training Loss: 0.6114711761474609 \n",
      "     Training Step: 197 Training Loss: 0.6140772700309753 \n",
      "     Training Step: 198 Training Loss: 0.6149317622184753 \n",
      "     Training Step: 199 Training Loss: 0.6153002977371216 \n",
      "     Training Step: 200 Training Loss: 0.6131243705749512 \n",
      "     Training Step: 201 Training Loss: 0.6155324578285217 \n",
      "     Training Step: 202 Training Loss: 0.6136259436607361 \n",
      "     Training Step: 203 Training Loss: 0.6132951974868774 \n",
      "     Training Step: 204 Training Loss: 0.6105124950408936 \n",
      "     Training Step: 205 Training Loss: 0.6154088377952576 \n",
      "     Training Step: 206 Training Loss: 0.6168903112411499 \n",
      "     Training Step: 207 Training Loss: 0.6118366718292236 \n",
      "     Training Step: 208 Training Loss: 0.6182354092597961 \n",
      "     Training Step: 209 Training Loss: 0.6162359714508057 \n",
      "     Training Step: 210 Training Loss: 0.6135890483856201 \n",
      "     Training Step: 211 Training Loss: 0.6131877303123474 \n",
      "     Training Step: 212 Training Loss: 0.6160328388214111 \n",
      "     Training Step: 213 Training Loss: 0.6177868247032166 \n",
      "     Training Step: 214 Training Loss: 0.6124929189682007 \n",
      "     Training Step: 215 Training Loss: 0.6161988377571106 \n",
      "     Training Step: 216 Training Loss: 0.6100839972496033 \n",
      "     Training Step: 217 Training Loss: 0.6118370890617371 \n",
      "     Training Step: 218 Training Loss: 0.6157395243644714 \n",
      "     Training Step: 219 Training Loss: 0.6157816648483276 \n",
      "     Training Step: 220 Training Loss: 0.6126462817192078 \n",
      "     Training Step: 221 Training Loss: 0.613739550113678 \n",
      "     Training Step: 222 Training Loss: 0.6149559617042542 \n",
      "     Training Step: 223 Training Loss: 0.618072509765625 \n",
      "     Training Step: 224 Training Loss: 0.6100437045097351 \n",
      "     Training Step: 225 Training Loss: 0.6164078116416931 \n",
      "     Training Step: 226 Training Loss: 0.6115549206733704 \n",
      "     Training Step: 227 Training Loss: 0.6115211844444275 \n",
      "     Training Step: 228 Training Loss: 0.6122254729270935 \n",
      "     Training Step: 229 Training Loss: 0.6188530921936035 \n",
      "     Training Step: 230 Training Loss: 0.616657018661499 \n",
      "     Training Step: 231 Training Loss: 0.6121526956558228 \n",
      "     Training Step: 232 Training Loss: 0.614665150642395 \n",
      "     Training Step: 233 Training Loss: 0.6178118586540222 \n",
      "     Training Step: 234 Training Loss: 0.6132016181945801 \n",
      "     Training Step: 235 Training Loss: 0.6105780601501465 \n",
      "     Training Step: 236 Training Loss: 0.6155961155891418 \n",
      "     Training Step: 237 Training Loss: 0.6122323274612427 \n",
      "     Training Step: 238 Training Loss: 0.6129697561264038 \n",
      "     Training Step: 239 Training Loss: 0.6150925755500793 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6121352314949036 \n",
      "     Validation Step: 1 Validation Loss: 0.6146406531333923 \n",
      "     Validation Step: 2 Validation Loss: 0.6176837682723999 \n",
      "     Validation Step: 3 Validation Loss: 0.616239070892334 \n",
      "     Validation Step: 4 Validation Loss: 0.6159814596176147 \n",
      "     Validation Step: 5 Validation Loss: 0.6182172298431396 \n",
      "     Validation Step: 6 Validation Loss: 0.6101445555686951 \n",
      "     Validation Step: 7 Validation Loss: 0.6133145689964294 \n",
      "     Validation Step: 8 Validation Loss: 0.6101707816123962 \n",
      "     Validation Step: 9 Validation Loss: 0.6136457920074463 \n",
      "     Validation Step: 10 Validation Loss: 0.6141303181648254 \n",
      "     Validation Step: 11 Validation Loss: 0.6145691871643066 \n",
      "     Validation Step: 12 Validation Loss: 0.6118878722190857 \n",
      "     Validation Step: 13 Validation Loss: 0.6183298230171204 \n",
      "     Validation Step: 14 Validation Loss: 0.6175966858863831 \n",
      "     Validation Step: 15 Validation Loss: 0.6101164817810059 \n",
      "     Validation Step: 16 Validation Loss: 0.6155669093132019 \n",
      "     Validation Step: 17 Validation Loss: 0.614205539226532 \n",
      "     Validation Step: 18 Validation Loss: 0.6128400564193726 \n",
      "     Validation Step: 19 Validation Loss: 0.6142599582672119 \n",
      "     Validation Step: 20 Validation Loss: 0.6145395040512085 \n",
      "     Validation Step: 21 Validation Loss: 0.6180534362792969 \n",
      "     Validation Step: 22 Validation Loss: 0.6075367331504822 \n",
      "     Validation Step: 23 Validation Loss: 0.6184677481651306 \n",
      "     Validation Step: 24 Validation Loss: 0.6152244210243225 \n",
      "     Validation Step: 25 Validation Loss: 0.6105231642723083 \n",
      "     Validation Step: 26 Validation Loss: 0.6141191124916077 \n",
      "     Validation Step: 27 Validation Loss: 0.6184853315353394 \n",
      "     Validation Step: 28 Validation Loss: 0.6157872676849365 \n",
      "     Validation Step: 29 Validation Loss: 0.6104902625083923 \n",
      "     Validation Step: 30 Validation Loss: 0.6129910349845886 \n",
      "     Validation Step: 31 Validation Loss: 0.6170138120651245 \n",
      "     Validation Step: 32 Validation Loss: 0.611183226108551 \n",
      "     Validation Step: 33 Validation Loss: 0.6116487383842468 \n",
      "     Validation Step: 34 Validation Loss: 0.6173009872436523 \n",
      "     Validation Step: 35 Validation Loss: 0.6156085133552551 \n",
      "     Validation Step: 36 Validation Loss: 0.613643229007721 \n",
      "     Validation Step: 37 Validation Loss: 0.6136660575866699 \n",
      "     Validation Step: 38 Validation Loss: 0.6150307059288025 \n",
      "     Validation Step: 39 Validation Loss: 0.6111673712730408 \n",
      "     Validation Step: 40 Validation Loss: 0.6148375868797302 \n",
      "     Validation Step: 41 Validation Loss: 0.6148967742919922 \n",
      "     Validation Step: 42 Validation Loss: 0.6153140068054199 \n",
      "     Validation Step: 43 Validation Loss: 0.6106369495391846 \n",
      "     Validation Step: 44 Validation Loss: 0.6115615963935852 \n",
      "Epoch: 118\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.612383246421814 \n",
      "     Training Step: 1 Training Loss: 0.6136500239372253 \n",
      "     Training Step: 2 Training Loss: 0.6125363707542419 \n",
      "     Training Step: 3 Training Loss: 0.6116155385971069 \n",
      "     Training Step: 4 Training Loss: 0.6160442233085632 \n",
      "     Training Step: 5 Training Loss: 0.6147060990333557 \n",
      "     Training Step: 6 Training Loss: 0.6154460310935974 \n",
      "     Training Step: 7 Training Loss: 0.611833930015564 \n",
      "     Training Step: 8 Training Loss: 0.6151765584945679 \n",
      "     Training Step: 9 Training Loss: 0.6122788786888123 \n",
      "     Training Step: 10 Training Loss: 0.6097151041030884 \n",
      "     Training Step: 11 Training Loss: 0.6181104779243469 \n",
      "     Training Step: 12 Training Loss: 0.6153887510299683 \n",
      "     Training Step: 13 Training Loss: 0.6168984174728394 \n",
      "     Training Step: 14 Training Loss: 0.614724338054657 \n",
      "     Training Step: 15 Training Loss: 0.6115486025810242 \n",
      "     Training Step: 16 Training Loss: 0.614242672920227 \n",
      "     Training Step: 17 Training Loss: 0.6177023649215698 \n",
      "     Training Step: 18 Training Loss: 0.6125478744506836 \n",
      "     Training Step: 19 Training Loss: 0.6153320670127869 \n",
      "     Training Step: 20 Training Loss: 0.6141500473022461 \n",
      "     Training Step: 21 Training Loss: 0.6128488183021545 \n",
      "     Training Step: 22 Training Loss: 0.6108937859535217 \n",
      "     Training Step: 23 Training Loss: 0.6111637949943542 \n",
      "     Training Step: 24 Training Loss: 0.6149283647537231 \n",
      "     Training Step: 25 Training Loss: 0.6130666136741638 \n",
      "     Training Step: 26 Training Loss: 0.6129613518714905 \n",
      "     Training Step: 27 Training Loss: 0.6168156862258911 \n",
      "     Training Step: 28 Training Loss: 0.6104841828346252 \n",
      "     Training Step: 29 Training Loss: 0.613452672958374 \n",
      "     Training Step: 30 Training Loss: 0.6131812334060669 \n",
      "     Training Step: 31 Training Loss: 0.6114621162414551 \n",
      "     Training Step: 32 Training Loss: 0.6113881468772888 \n",
      "     Training Step: 33 Training Loss: 0.6142139434814453 \n",
      "     Training Step: 34 Training Loss: 0.619492769241333 \n",
      "     Training Step: 35 Training Loss: 0.6137388944625854 \n",
      "     Training Step: 36 Training Loss: 0.6149636507034302 \n",
      "     Training Step: 37 Training Loss: 0.6165082454681396 \n",
      "     Training Step: 38 Training Loss: 0.613815426826477 \n",
      "     Training Step: 39 Training Loss: 0.6133546233177185 \n",
      "     Training Step: 40 Training Loss: 0.6142094731330872 \n",
      "     Training Step: 41 Training Loss: 0.61822509765625 \n",
      "     Training Step: 42 Training Loss: 0.6171312928199768 \n",
      "     Training Step: 43 Training Loss: 0.6198524236679077 \n",
      "     Training Step: 44 Training Loss: 0.6116531491279602 \n",
      "     Training Step: 45 Training Loss: 0.6121932864189148 \n",
      "     Training Step: 46 Training Loss: 0.6152911186218262 \n",
      "     Training Step: 47 Training Loss: 0.610109806060791 \n",
      "     Training Step: 48 Training Loss: 0.6150707006454468 \n",
      "     Training Step: 49 Training Loss: 0.6106054186820984 \n",
      "     Training Step: 50 Training Loss: 0.615746021270752 \n",
      "     Training Step: 51 Training Loss: 0.613637387752533 \n",
      "     Training Step: 52 Training Loss: 0.6146370768547058 \n",
      "     Training Step: 53 Training Loss: 0.6177610754966736 \n",
      "     Training Step: 54 Training Loss: 0.6146016716957092 \n",
      "     Training Step: 55 Training Loss: 0.609394907951355 \n",
      "     Training Step: 56 Training Loss: 0.6105744242668152 \n",
      "     Training Step: 57 Training Loss: 0.6180408000946045 \n",
      "     Training Step: 58 Training Loss: 0.6162589192390442 \n",
      "     Training Step: 59 Training Loss: 0.6196535229682922 \n",
      "     Training Step: 60 Training Loss: 0.6134726405143738 \n",
      "     Training Step: 61 Training Loss: 0.6140173673629761 \n",
      "     Training Step: 62 Training Loss: 0.6118323802947998 \n",
      "     Training Step: 63 Training Loss: 0.6122303009033203 \n",
      "     Training Step: 64 Training Loss: 0.6183139085769653 \n",
      "     Training Step: 65 Training Loss: 0.617483377456665 \n",
      "     Training Step: 66 Training Loss: 0.6100627183914185 \n",
      "     Training Step: 67 Training Loss: 0.6120767593383789 \n",
      "     Training Step: 68 Training Loss: 0.6150862574577332 \n",
      "     Training Step: 69 Training Loss: 0.6106090545654297 \n",
      "     Training Step: 70 Training Loss: 0.6152799725532532 \n",
      "     Training Step: 71 Training Loss: 0.6137452721595764 \n",
      "     Training Step: 72 Training Loss: 0.6092299222946167 \n",
      "     Training Step: 73 Training Loss: 0.6132037043571472 \n",
      "     Training Step: 74 Training Loss: 0.6185927391052246 \n",
      "     Training Step: 75 Training Loss: 0.6155225038528442 \n",
      "     Training Step: 76 Training Loss: 0.618459165096283 \n",
      "     Training Step: 77 Training Loss: 0.6157630085945129 \n",
      "     Training Step: 78 Training Loss: 0.6202239394187927 \n",
      "     Training Step: 79 Training Loss: 0.6167987585067749 \n",
      "     Training Step: 80 Training Loss: 0.6155261993408203 \n",
      "     Training Step: 81 Training Loss: 0.6140753626823425 \n",
      "     Training Step: 82 Training Loss: 0.610605776309967 \n",
      "     Training Step: 83 Training Loss: 0.6154199242591858 \n",
      "     Training Step: 84 Training Loss: 0.6152868866920471 \n",
      "     Training Step: 85 Training Loss: 0.614467442035675 \n",
      "     Training Step: 86 Training Loss: 0.6171262264251709 \n",
      "     Training Step: 87 Training Loss: 0.6185732483863831 \n",
      "     Training Step: 88 Training Loss: 0.6127794981002808 \n",
      "     Training Step: 89 Training Loss: 0.6167093515396118 \n",
      "     Training Step: 90 Training Loss: 0.614692211151123 \n",
      "     Training Step: 91 Training Loss: 0.611661434173584 \n",
      "     Training Step: 92 Training Loss: 0.6156644821166992 \n",
      "     Training Step: 93 Training Loss: 0.6161108016967773 \n",
      "     Training Step: 94 Training Loss: 0.6121308207511902 \n",
      "     Training Step: 95 Training Loss: 0.6128675937652588 \n",
      "     Training Step: 96 Training Loss: 0.6146733164787292 \n",
      "     Training Step: 97 Training Loss: 0.613287627696991 \n",
      "     Training Step: 98 Training Loss: 0.6132071018218994 \n",
      "     Training Step: 99 Training Loss: 0.6134166121482849 \n",
      "     Training Step: 100 Training Loss: 0.6147957444190979 \n",
      "     Training Step: 101 Training Loss: 0.6118199825286865 \n",
      "     Training Step: 102 Training Loss: 0.6158428192138672 \n",
      "     Training Step: 103 Training Loss: 0.6156898140907288 \n",
      "     Training Step: 104 Training Loss: 0.6182501316070557 \n",
      "     Training Step: 105 Training Loss: 0.610125720500946 \n",
      "     Training Step: 106 Training Loss: 0.6197192668914795 \n",
      "     Training Step: 107 Training Loss: 0.6166826486587524 \n",
      "     Training Step: 108 Training Loss: 0.6167182922363281 \n",
      "     Training Step: 109 Training Loss: 0.615379810333252 \n",
      "     Training Step: 110 Training Loss: 0.6171494126319885 \n",
      "     Training Step: 111 Training Loss: 0.616793692111969 \n",
      "     Training Step: 112 Training Loss: 0.6151672005653381 \n",
      "     Training Step: 113 Training Loss: 0.6144372224807739 \n",
      "     Training Step: 114 Training Loss: 0.614621102809906 \n",
      "     Training Step: 115 Training Loss: 0.6118286848068237 \n",
      "     Training Step: 116 Training Loss: 0.6180473566055298 \n",
      "     Training Step: 117 Training Loss: 0.6119212508201599 \n",
      "     Training Step: 118 Training Loss: 0.6183791160583496 \n",
      "     Training Step: 119 Training Loss: 0.61574387550354 \n",
      "     Training Step: 120 Training Loss: 0.6169045567512512 \n",
      "     Training Step: 121 Training Loss: 0.6164036393165588 \n",
      "     Training Step: 122 Training Loss: 0.6149187088012695 \n",
      "     Training Step: 123 Training Loss: 0.6148689985275269 \n",
      "     Training Step: 124 Training Loss: 0.6136320233345032 \n",
      "     Training Step: 125 Training Loss: 0.6107423305511475 \n",
      "     Training Step: 126 Training Loss: 0.6130664944648743 \n",
      "     Training Step: 127 Training Loss: 0.6120251417160034 \n",
      "     Training Step: 128 Training Loss: 0.6139134168624878 \n",
      "     Training Step: 129 Training Loss: 0.6114144921302795 \n",
      "     Training Step: 130 Training Loss: 0.6122861504554749 \n",
      "     Training Step: 131 Training Loss: 0.6178327798843384 \n",
      "     Training Step: 132 Training Loss: 0.6133151650428772 \n",
      "     Training Step: 133 Training Loss: 0.6159750819206238 \n",
      "     Training Step: 134 Training Loss: 0.6158033609390259 \n",
      "     Training Step: 135 Training Loss: 0.6129185557365417 \n",
      "     Training Step: 136 Training Loss: 0.6114282011985779 \n",
      "     Training Step: 137 Training Loss: 0.6209641098976135 \n",
      "     Training Step: 138 Training Loss: 0.615546464920044 \n",
      "     Training Step: 139 Training Loss: 0.6171942353248596 \n",
      "     Training Step: 140 Training Loss: 0.6143446564674377 \n",
      "     Training Step: 141 Training Loss: 0.6133102178573608 \n",
      "     Training Step: 142 Training Loss: 0.6140305399894714 \n",
      "     Training Step: 143 Training Loss: 0.6147108674049377 \n",
      "     Training Step: 144 Training Loss: 0.6122748255729675 \n",
      "     Training Step: 145 Training Loss: 0.6138933300971985 \n",
      "     Training Step: 146 Training Loss: 0.613510012626648 \n",
      "     Training Step: 147 Training Loss: 0.6170859336853027 \n",
      "     Training Step: 148 Training Loss: 0.6115448474884033 \n",
      "     Training Step: 149 Training Loss: 0.6144993305206299 \n",
      "     Training Step: 150 Training Loss: 0.6154765486717224 \n",
      "     Training Step: 151 Training Loss: 0.6127089262008667 \n",
      "     Training Step: 152 Training Loss: 0.6115223169326782 \n",
      "     Training Step: 153 Training Loss: 0.6125141978263855 \n",
      "     Training Step: 154 Training Loss: 0.6097201108932495 \n",
      "     Training Step: 155 Training Loss: 0.611575722694397 \n",
      "     Training Step: 156 Training Loss: 0.6144927740097046 \n",
      "     Training Step: 157 Training Loss: 0.6176921129226685 \n",
      "     Training Step: 158 Training Loss: 0.61237633228302 \n",
      "     Training Step: 159 Training Loss: 0.6111887693405151 \n",
      "     Training Step: 160 Training Loss: 0.6117768883705139 \n",
      "     Training Step: 161 Training Loss: 0.6166958212852478 \n",
      "     Training Step: 162 Training Loss: 0.6188862323760986 \n",
      "     Training Step: 163 Training Loss: 0.6143894791603088 \n",
      "     Training Step: 164 Training Loss: 0.6100482940673828 \n",
      "     Training Step: 165 Training Loss: 0.6128032803535461 \n",
      "     Training Step: 166 Training Loss: 0.617694079875946 \n",
      "     Training Step: 167 Training Loss: 0.6123740673065186 \n",
      "     Training Step: 168 Training Loss: 0.6155997514724731 \n",
      "     Training Step: 169 Training Loss: 0.6146646738052368 \n",
      "     Training Step: 170 Training Loss: 0.6184560060501099 \n",
      "     Training Step: 171 Training Loss: 0.6118142604827881 \n",
      "     Training Step: 172 Training Loss: 0.6188541054725647 \n",
      "     Training Step: 173 Training Loss: 0.6132883429527283 \n",
      "     Training Step: 174 Training Loss: 0.6128878593444824 \n",
      "     Training Step: 175 Training Loss: 0.6154018044471741 \n",
      "     Training Step: 176 Training Loss: 0.6164057850837708 \n",
      "     Training Step: 177 Training Loss: 0.6140442490577698 \n",
      "     Training Step: 178 Training Loss: 0.6147745847702026 \n",
      "     Training Step: 179 Training Loss: 0.612234890460968 \n",
      "     Training Step: 180 Training Loss: 0.6141512989997864 \n",
      "     Training Step: 181 Training Loss: 0.610385537147522 \n",
      "     Training Step: 182 Training Loss: 0.6146485209465027 \n",
      "     Training Step: 183 Training Loss: 0.6108754873275757 \n",
      "     Training Step: 184 Training Loss: 0.6146110892295837 \n",
      "     Training Step: 185 Training Loss: 0.6163620352745056 \n",
      "     Training Step: 186 Training Loss: 0.6153818368911743 \n",
      "     Training Step: 187 Training Loss: 0.6167802214622498 \n",
      "     Training Step: 188 Training Loss: 0.6151082515716553 \n",
      "     Training Step: 189 Training Loss: 0.6166568398475647 \n",
      "     Training Step: 190 Training Loss: 0.6116312742233276 \n",
      "     Training Step: 191 Training Loss: 0.6124900579452515 \n",
      "     Training Step: 192 Training Loss: 0.6143485307693481 \n",
      "     Training Step: 193 Training Loss: 0.6139311790466309 \n",
      "     Training Step: 194 Training Loss: 0.6115379929542542 \n",
      "     Training Step: 195 Training Loss: 0.6121526956558228 \n",
      "     Training Step: 196 Training Loss: 0.6135895252227783 \n",
      "     Training Step: 197 Training Loss: 0.6116892695426941 \n",
      "     Training Step: 198 Training Loss: 0.6152405142784119 \n",
      "     Training Step: 199 Training Loss: 0.6094574332237244 \n",
      "     Training Step: 200 Training Loss: 0.6137640476226807 \n",
      "     Training Step: 201 Training Loss: 0.6101716160774231 \n",
      "     Training Step: 202 Training Loss: 0.6167265772819519 \n",
      "     Training Step: 203 Training Loss: 0.6142922639846802 \n",
      "     Training Step: 204 Training Loss: 0.6131129264831543 \n",
      "     Training Step: 205 Training Loss: 0.6166335940361023 \n",
      "     Training Step: 206 Training Loss: 0.6129131317138672 \n",
      "     Training Step: 207 Training Loss: 0.6132894158363342 \n",
      "     Training Step: 208 Training Loss: 0.6099780797958374 \n",
      "     Training Step: 209 Training Loss: 0.6104751825332642 \n",
      "     Training Step: 210 Training Loss: 0.6177191734313965 \n",
      "     Training Step: 211 Training Loss: 0.6082484126091003 \n",
      "     Training Step: 212 Training Loss: 0.6160107851028442 \n",
      "     Training Step: 213 Training Loss: 0.6152696013450623 \n",
      "     Training Step: 214 Training Loss: 0.6131349205970764 \n",
      "     Training Step: 215 Training Loss: 0.6143405437469482 \n",
      "     Training Step: 216 Training Loss: 0.6103920340538025 \n",
      "     Training Step: 217 Training Loss: 0.6180459856987 \n",
      "     Training Step: 218 Training Loss: 0.6107334494590759 \n",
      "     Training Step: 219 Training Loss: 0.6111387014389038 \n",
      "     Training Step: 220 Training Loss: 0.612747311592102 \n",
      "     Training Step: 221 Training Loss: 0.616205096244812 \n",
      "     Training Step: 222 Training Loss: 0.6097010970115662 \n",
      "     Training Step: 223 Training Loss: 0.6166604161262512 \n",
      "     Training Step: 224 Training Loss: 0.612138569355011 \n",
      "     Training Step: 225 Training Loss: 0.612638533115387 \n",
      "     Training Step: 226 Training Loss: 0.6144390106201172 \n",
      "     Training Step: 227 Training Loss: 0.6124615669250488 \n",
      "     Training Step: 228 Training Loss: 0.6106777787208557 \n",
      "     Training Step: 229 Training Loss: 0.6147512197494507 \n",
      "     Training Step: 230 Training Loss: 0.6167196035385132 \n",
      "     Training Step: 231 Training Loss: 0.6146702766418457 \n",
      "     Training Step: 232 Training Loss: 0.6178250312805176 \n",
      "     Training Step: 233 Training Loss: 0.6106544137001038 \n",
      "     Training Step: 234 Training Loss: 0.6135116219520569 \n",
      "     Training Step: 235 Training Loss: 0.6173831820487976 \n",
      "     Training Step: 236 Training Loss: 0.6141058206558228 \n",
      "     Training Step: 237 Training Loss: 0.6162031292915344 \n",
      "     Training Step: 238 Training Loss: 0.611440896987915 \n",
      "     Training Step: 239 Training Loss: 0.6114166975021362 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6182171702384949 \n",
      "     Validation Step: 1 Validation Loss: 0.6136378645896912 \n",
      "     Validation Step: 2 Validation Loss: 0.6136665940284729 \n",
      "     Validation Step: 3 Validation Loss: 0.6180524230003357 \n",
      "     Validation Step: 4 Validation Loss: 0.6115618348121643 \n",
      "     Validation Step: 5 Validation Loss: 0.6145685911178589 \n",
      "     Validation Step: 6 Validation Loss: 0.6105234026908875 \n",
      "     Validation Step: 7 Validation Loss: 0.6159843802452087 \n",
      "     Validation Step: 8 Validation Loss: 0.6170142889022827 \n",
      "     Validation Step: 9 Validation Loss: 0.6184629797935486 \n",
      "     Validation Step: 10 Validation Loss: 0.6141315698623657 \n",
      "     Validation Step: 11 Validation Loss: 0.6142030954360962 \n",
      "     Validation Step: 12 Validation Loss: 0.6146391034126282 \n",
      "     Validation Step: 13 Validation Loss: 0.6162384748458862 \n",
      "     Validation Step: 14 Validation Loss: 0.615567147731781 \n",
      "     Validation Step: 15 Validation Loss: 0.6157857179641724 \n",
      "     Validation Step: 16 Validation Loss: 0.6175901889801025 \n",
      "     Validation Step: 17 Validation Loss: 0.6075354218482971 \n",
      "     Validation Step: 18 Validation Loss: 0.6106371879577637 \n",
      "     Validation Step: 19 Validation Loss: 0.610146164894104 \n",
      "     Validation Step: 20 Validation Loss: 0.6184813976287842 \n",
      "     Validation Step: 21 Validation Loss: 0.6118860840797424 \n",
      "     Validation Step: 22 Validation Loss: 0.612833559513092 \n",
      "     Validation Step: 23 Validation Loss: 0.6101657152175903 \n",
      "     Validation Step: 24 Validation Loss: 0.6129876375198364 \n",
      "     Validation Step: 25 Validation Loss: 0.6152238845825195 \n",
      "     Validation Step: 26 Validation Loss: 0.6141147017478943 \n",
      "     Validation Step: 27 Validation Loss: 0.6136487126350403 \n",
      "     Validation Step: 28 Validation Loss: 0.611169695854187 \n",
      "     Validation Step: 29 Validation Loss: 0.6148926019668579 \n",
      "     Validation Step: 30 Validation Loss: 0.6153159141540527 \n",
      "     Validation Step: 31 Validation Loss: 0.6101185083389282 \n",
      "     Validation Step: 32 Validation Loss: 0.6148359179496765 \n",
      "     Validation Step: 33 Validation Loss: 0.6183316111564636 \n",
      "     Validation Step: 34 Validation Loss: 0.6176854372024536 \n",
      "     Validation Step: 35 Validation Loss: 0.615034818649292 \n",
      "     Validation Step: 36 Validation Loss: 0.6156094074249268 \n",
      "     Validation Step: 37 Validation Loss: 0.6104886531829834 \n",
      "     Validation Step: 38 Validation Loss: 0.6116464734077454 \n",
      "     Validation Step: 39 Validation Loss: 0.6142561435699463 \n",
      "     Validation Step: 40 Validation Loss: 0.617297887802124 \n",
      "     Validation Step: 41 Validation Loss: 0.6111816167831421 \n",
      "     Validation Step: 42 Validation Loss: 0.6145355105400085 \n",
      "     Validation Step: 43 Validation Loss: 0.6133084297180176 \n",
      "     Validation Step: 44 Validation Loss: 0.6121343374252319 \n",
      "Epoch: 119\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6115297675132751 \n",
      "     Training Step: 1 Training Loss: 0.6176856756210327 \n",
      "     Training Step: 2 Training Loss: 0.6097296476364136 \n",
      "     Training Step: 3 Training Loss: 0.6167083978652954 \n",
      "     Training Step: 4 Training Loss: 0.6153801679611206 \n",
      "     Training Step: 5 Training Loss: 0.6168018579483032 \n",
      "     Training Step: 6 Training Loss: 0.6146755218505859 \n",
      "     Training Step: 7 Training Loss: 0.6166611909866333 \n",
      "     Training Step: 8 Training Loss: 0.6188204884529114 \n",
      "     Training Step: 9 Training Loss: 0.6173637509346008 \n",
      "     Training Step: 10 Training Loss: 0.615943193435669 \n",
      "     Training Step: 11 Training Loss: 0.6137492656707764 \n",
      "     Training Step: 12 Training Loss: 0.6183804869651794 \n",
      "     Training Step: 13 Training Loss: 0.6166427135467529 \n",
      "     Training Step: 14 Training Loss: 0.6176846623420715 \n",
      "     Training Step: 15 Training Loss: 0.6133572459220886 \n",
      "     Training Step: 16 Training Loss: 0.6125860214233398 \n",
      "     Training Step: 17 Training Loss: 0.6109716296195984 \n",
      "     Training Step: 18 Training Loss: 0.6167996525764465 \n",
      "     Training Step: 19 Training Loss: 0.6159982681274414 \n",
      "     Training Step: 20 Training Loss: 0.6136587262153625 \n",
      "     Training Step: 21 Training Loss: 0.6157421469688416 \n",
      "     Training Step: 22 Training Loss: 0.613517701625824 \n",
      "     Training Step: 23 Training Loss: 0.6114046573638916 \n",
      "     Training Step: 24 Training Loss: 0.6151781678199768 \n",
      "     Training Step: 25 Training Loss: 0.6121515035629272 \n",
      "     Training Step: 26 Training Loss: 0.6124700307846069 \n",
      "     Training Step: 27 Training Loss: 0.611194372177124 \n",
      "     Training Step: 28 Training Loss: 0.6153274178504944 \n",
      "     Training Step: 29 Training Loss: 0.611596941947937 \n",
      "     Training Step: 30 Training Loss: 0.6118326783180237 \n",
      "     Training Step: 31 Training Loss: 0.6142507195472717 \n",
      "     Training Step: 32 Training Loss: 0.6142991781234741 \n",
      "     Training Step: 33 Training Loss: 0.6121562123298645 \n",
      "     Training Step: 34 Training Loss: 0.6115397214889526 \n",
      "     Training Step: 35 Training Loss: 0.6167397499084473 \n",
      "     Training Step: 36 Training Loss: 0.6132025718688965 \n",
      "     Training Step: 37 Training Loss: 0.6104646921157837 \n",
      "     Training Step: 38 Training Loss: 0.612373948097229 \n",
      "     Training Step: 39 Training Loss: 0.6133025884628296 \n",
      "     Training Step: 40 Training Loss: 0.6151087880134583 \n",
      "     Training Step: 41 Training Loss: 0.6142081618309021 \n",
      "     Training Step: 42 Training Loss: 0.6131173968315125 \n",
      "     Training Step: 43 Training Loss: 0.6209502220153809 \n",
      "     Training Step: 44 Training Loss: 0.6144533753395081 \n",
      "     Training Step: 45 Training Loss: 0.612490713596344 \n",
      "     Training Step: 46 Training Loss: 0.6144219040870667 \n",
      "     Training Step: 47 Training Loss: 0.6155332326889038 \n",
      "     Training Step: 48 Training Loss: 0.6116330623626709 \n",
      "     Training Step: 49 Training Loss: 0.6152839064598083 \n",
      "     Training Step: 50 Training Loss: 0.6156767010688782 \n",
      "     Training Step: 51 Training Loss: 0.6122912168502808 \n",
      "     Training Step: 52 Training Loss: 0.6134630441665649 \n",
      "     Training Step: 53 Training Loss: 0.6097363233566284 \n",
      "     Training Step: 54 Training Loss: 0.6132829785346985 \n",
      "     Training Step: 55 Training Loss: 0.6122915148735046 \n",
      "     Training Step: 56 Training Loss: 0.6153988838195801 \n",
      "     Training Step: 57 Training Loss: 0.6155251860618591 \n",
      "     Training Step: 58 Training Loss: 0.6147483587265015 \n",
      "     Training Step: 59 Training Loss: 0.6130679845809937 \n",
      "     Training Step: 60 Training Loss: 0.6146052479743958 \n",
      "     Training Step: 61 Training Loss: 0.616951048374176 \n",
      "     Training Step: 62 Training Loss: 0.6125050187110901 \n",
      "     Training Step: 63 Training Loss: 0.6147729754447937 \n",
      "     Training Step: 64 Training Loss: 0.6147958636283875 \n",
      "     Training Step: 65 Training Loss: 0.612869143486023 \n",
      "     Training Step: 66 Training Loss: 0.6114808320999146 \n",
      "     Training Step: 67 Training Loss: 0.6120167970657349 \n",
      "     Training Step: 68 Training Loss: 0.6163521409034729 \n",
      "     Training Step: 69 Training Loss: 0.614725649356842 \n",
      "     Training Step: 70 Training Loss: 0.6168903112411499 \n",
      "     Training Step: 71 Training Loss: 0.6146998405456543 \n",
      "     Training Step: 72 Training Loss: 0.614608108997345 \n",
      "     Training Step: 73 Training Loss: 0.6107334494590759 \n",
      "     Training Step: 74 Training Loss: 0.6117910742759705 \n",
      "     Training Step: 75 Training Loss: 0.6162042021751404 \n",
      "     Training Step: 76 Training Loss: 0.6155348420143127 \n",
      "     Training Step: 77 Training Loss: 0.6136363744735718 \n",
      "     Training Step: 78 Training Loss: 0.6118847727775574 \n",
      "     Training Step: 79 Training Loss: 0.614646852016449 \n",
      "     Training Step: 80 Training Loss: 0.6171595454216003 \n",
      "     Training Step: 81 Training Loss: 0.612123429775238 \n",
      "     Training Step: 82 Training Loss: 0.6167567372322083 \n",
      "     Training Step: 83 Training Loss: 0.6111446022987366 \n",
      "     Training Step: 84 Training Loss: 0.6151601672172546 \n",
      "     Training Step: 85 Training Loss: 0.6140230298042297 \n",
      "     Training Step: 86 Training Loss: 0.6116339564323425 \n",
      "     Training Step: 87 Training Loss: 0.6118005514144897 \n",
      "     Training Step: 88 Training Loss: 0.611416757106781 \n",
      "     Training Step: 89 Training Loss: 0.6168111562728882 \n",
      "     Training Step: 90 Training Loss: 0.6139073371887207 \n",
      "     Training Step: 91 Training Loss: 0.6107337474822998 \n",
      "     Training Step: 92 Training Loss: 0.6099740862846375 \n",
      "     Training Step: 93 Training Loss: 0.6164204478263855 \n",
      "     Training Step: 94 Training Loss: 0.6153421401977539 \n",
      "     Training Step: 95 Training Loss: 0.6123752593994141 \n",
      "     Training Step: 96 Training Loss: 0.6140870451927185 \n",
      "     Training Step: 97 Training Loss: 0.6103848814964294 \n",
      "     Training Step: 98 Training Loss: 0.6180984973907471 \n",
      "     Training Step: 99 Training Loss: 0.613610029220581 \n",
      "     Training Step: 100 Training Loss: 0.6171590089797974 \n",
      "     Training Step: 101 Training Loss: 0.610654890537262 \n",
      "     Training Step: 102 Training Loss: 0.6138871312141418 \n",
      "     Training Step: 103 Training Loss: 0.612533450126648 \n",
      "     Training Step: 104 Training Loss: 0.6120667457580566 \n",
      "     Training Step: 105 Training Loss: 0.6152399778366089 \n",
      "     Training Step: 106 Training Loss: 0.6167064905166626 \n",
      "     Training Step: 107 Training Loss: 0.61115962266922 \n",
      "     Training Step: 108 Training Loss: 0.6105937957763672 \n",
      "     Training Step: 109 Training Loss: 0.6146695613861084 \n",
      "     Training Step: 110 Training Loss: 0.6115860939025879 \n",
      "     Training Step: 111 Training Loss: 0.6154242753982544 \n",
      "     Training Step: 112 Training Loss: 0.6140352487564087 \n",
      "     Training Step: 113 Training Loss: 0.6132897138595581 \n",
      "     Training Step: 114 Training Loss: 0.6137471795082092 \n",
      "     Training Step: 115 Training Loss: 0.6118207573890686 \n",
      "     Training Step: 116 Training Loss: 0.6121535897254944 \n",
      "     Training Step: 117 Training Loss: 0.6096834540367126 \n",
      "     Training Step: 118 Training Loss: 0.6105630397796631 \n",
      "     Training Step: 119 Training Loss: 0.6154872179031372 \n",
      "     Training Step: 120 Training Loss: 0.6157720685005188 \n",
      "     Training Step: 121 Training Loss: 0.6122227907180786 \n",
      "     Training Step: 122 Training Loss: 0.6181454658508301 \n",
      "     Training Step: 123 Training Loss: 0.6196861267089844 \n",
      "     Training Step: 124 Training Loss: 0.6116884350776672 \n",
      "     Training Step: 125 Training Loss: 0.6166369915008545 \n",
      "     Training Step: 126 Training Loss: 0.6129193305969238 \n",
      "     Training Step: 127 Training Loss: 0.6148701906204224 \n",
      "     Training Step: 128 Training Loss: 0.612648606300354 \n",
      "     Training Step: 129 Training Loss: 0.6182213425636292 \n",
      "     Training Step: 130 Training Loss: 0.6150920391082764 \n",
      "     Training Step: 131 Training Loss: 0.6094459295272827 \n",
      "     Training Step: 132 Training Loss: 0.6129353046417236 \n",
      "     Training Step: 133 Training Loss: 0.6146074533462524 \n",
      "     Training Step: 134 Training Loss: 0.6149153113365173 \n",
      "     Training Step: 135 Training Loss: 0.6185862421989441 \n",
      "     Training Step: 136 Training Loss: 0.6100585460662842 \n",
      "     Training Step: 137 Training Loss: 0.6114432215690613 \n",
      "     Training Step: 138 Training Loss: 0.6196998953819275 \n",
      "     Training Step: 139 Training Loss: 0.6135876178741455 \n",
      "     Training Step: 140 Training Loss: 0.6131848096847534 \n",
      "     Training Step: 141 Training Loss: 0.6146703958511353 \n",
      "     Training Step: 142 Training Loss: 0.6134967803955078 \n",
      "     Training Step: 143 Training Loss: 0.6141062378883362 \n",
      "     Training Step: 144 Training Loss: 0.6122291684150696 \n",
      "     Training Step: 145 Training Loss: 0.612697422504425 \n",
      "     Training Step: 146 Training Loss: 0.614346981048584 \n",
      "     Training Step: 147 Training Loss: 0.6178250908851624 \n",
      "     Training Step: 148 Training Loss: 0.6118021011352539 \n",
      "     Training Step: 149 Training Loss: 0.6167234182357788 \n",
      "     Training Step: 150 Training Loss: 0.6116417050361633 \n",
      "     Training Step: 151 Training Loss: 0.6118274927139282 \n",
      "     Training Step: 152 Training Loss: 0.6188709735870361 \n",
      "     Training Step: 153 Training Loss: 0.6157419085502625 \n",
      "     Training Step: 154 Training Loss: 0.6180289387702942 \n",
      "     Training Step: 155 Training Loss: 0.6141530871391296 \n",
      "     Training Step: 156 Training Loss: 0.6171284317970276 \n",
      "     Training Step: 157 Training Loss: 0.6160230040550232 \n",
      "     Training Step: 158 Training Loss: 0.6095061302185059 \n",
      "     Training Step: 159 Training Loss: 0.6182977557182312 \n",
      "     Training Step: 160 Training Loss: 0.6185564994812012 \n",
      "     Training Step: 161 Training Loss: 0.6176042556762695 \n",
      "     Training Step: 162 Training Loss: 0.6144548058509827 \n",
      "     Training Step: 163 Training Loss: 0.6142139434814453 \n",
      "     Training Step: 164 Training Loss: 0.6129905581474304 \n",
      "     Training Step: 165 Training Loss: 0.6141599416732788 \n",
      "     Training Step: 166 Training Loss: 0.6138282418251038 \n",
      "     Training Step: 167 Training Loss: 0.6158252954483032 \n",
      "     Training Step: 168 Training Loss: 0.6140168309211731 \n",
      "     Training Step: 169 Training Loss: 0.6152613759040833 \n",
      "     Training Step: 170 Training Loss: 0.6143343448638916 \n",
      "     Training Step: 171 Training Loss: 0.6144833564758301 \n",
      "     Training Step: 172 Training Loss: 0.6130647659301758 \n",
      "     Training Step: 173 Training Loss: 0.6127946972846985 \n",
      "     Training Step: 174 Training Loss: 0.6143351793289185 \n",
      "     Training Step: 175 Training Loss: 0.6182375550270081 \n",
      "     Training Step: 176 Training Loss: 0.6152833700180054 \n",
      "     Training Step: 177 Training Loss: 0.6132115125656128 \n",
      "     Training Step: 178 Training Loss: 0.6194487810134888 \n",
      "     Training Step: 179 Training Loss: 0.6170939207077026 \n",
      "     Training Step: 180 Training Loss: 0.6100813150405884 \n",
      "     Training Step: 181 Training Loss: 0.6149290204048157 \n",
      "     Training Step: 182 Training Loss: 0.6131482124328613 \n",
      "     Training Step: 183 Training Loss: 0.6171835660934448 \n",
      "     Training Step: 184 Training Loss: 0.6105893850326538 \n",
      "     Training Step: 185 Training Loss: 0.6123847365379333 \n",
      "     Training Step: 186 Training Loss: 0.6114193797111511 \n",
      "     Training Step: 187 Training Loss: 0.6133662462234497 \n",
      "     Training Step: 188 Training Loss: 0.6154406666755676 \n",
      "     Training Step: 189 Training Loss: 0.6115264296531677 \n",
      "     Training Step: 190 Training Loss: 0.6137694716453552 \n",
      "     Training Step: 191 Training Loss: 0.6128682494163513 \n",
      "     Training Step: 192 Training Loss: 0.6147112846374512 \n",
      "     Training Step: 193 Training Loss: 0.6114262342453003 \n",
      "     Training Step: 194 Training Loss: 0.6153820157051086 \n",
      "     Training Step: 195 Training Loss: 0.6115185618400574 \n",
      "     Training Step: 196 Training Loss: 0.6178311705589294 \n",
      "     Training Step: 197 Training Loss: 0.6177934408187866 \n",
      "     Training Step: 198 Training Loss: 0.6166853308677673 \n",
      "     Training Step: 199 Training Loss: 0.6128381490707397 \n",
      "     Training Step: 200 Training Loss: 0.6134161353111267 \n",
      "     Training Step: 201 Training Loss: 0.6150674223899841 \n",
      "     Training Step: 202 Training Loss: 0.6106947660446167 \n",
      "     Training Step: 203 Training Loss: 0.615782618522644 \n",
      "     Training Step: 204 Training Loss: 0.6184537410736084 \n",
      "     Training Step: 205 Training Loss: 0.6162331700325012 \n",
      "     Training Step: 206 Training Loss: 0.6161925792694092 \n",
      "     Training Step: 207 Training Loss: 0.6145016551017761 \n",
      "     Training Step: 208 Training Loss: 0.6127790808677673 \n",
      "     Training Step: 209 Training Loss: 0.6179978251457214 \n",
      "     Training Step: 210 Training Loss: 0.6149593591690063 \n",
      "     Training Step: 211 Training Loss: 0.6155906319618225 \n",
      "     Training Step: 212 Training Loss: 0.6101812720298767 \n",
      "     Training Step: 213 Training Loss: 0.616634726524353 \n",
      "     Training Step: 214 Training Loss: 0.6109077334403992 \n",
      "     Training Step: 215 Training Loss: 0.6154024004936218 \n",
      "     Training Step: 216 Training Loss: 0.6105124950408936 \n",
      "     Training Step: 217 Training Loss: 0.6100664734840393 \n",
      "     Training Step: 218 Training Loss: 0.6184521913528442 \n",
      "     Training Step: 219 Training Loss: 0.6164286732673645 \n",
      "     Training Step: 220 Training Loss: 0.6165050864219666 \n",
      "     Training Step: 221 Training Loss: 0.6156895160675049 \n",
      "     Training Step: 222 Training Loss: 0.616122841835022 \n",
      "     Training Step: 223 Training Loss: 0.619881808757782 \n",
      "     Training Step: 224 Training Loss: 0.6202176809310913 \n",
      "     Training Step: 225 Training Loss: 0.6139274835586548 \n",
      "     Training Step: 226 Training Loss: 0.6174741387367249 \n",
      "     Training Step: 227 Training Loss: 0.6132948994636536 \n",
      "     Training Step: 228 Training Loss: 0.6127740740776062 \n",
      "     Training Step: 229 Training Loss: 0.6106184720993042 \n",
      "     Training Step: 230 Training Loss: 0.6176801919937134 \n",
      "     Training Step: 231 Training Loss: 0.6104302406311035 \n",
      "     Training Step: 232 Training Loss: 0.6083230972290039 \n",
      "     Training Step: 233 Training Loss: 0.6143739223480225 \n",
      "     Training Step: 234 Training Loss: 0.6146354079246521 \n",
      "     Training Step: 235 Training Loss: 0.613476574420929 \n",
      "     Training Step: 236 Training Loss: 0.6122288703918457 \n",
      "     Training Step: 237 Training Loss: 0.6101711988449097 \n",
      "     Training Step: 238 Training Loss: 0.6146941184997559 \n",
      "     Training Step: 239 Training Loss: 0.6092013716697693 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6101399064064026 \n",
      "     Validation Step: 1 Validation Loss: 0.6116339564323425 \n",
      "     Validation Step: 2 Validation Loss: 0.6183834671974182 \n",
      "     Validation Step: 3 Validation Loss: 0.6141283512115479 \n",
      "     Validation Step: 4 Validation Loss: 0.6133093237876892 \n",
      "     Validation Step: 5 Validation Loss: 0.6156020164489746 \n",
      "     Validation Step: 6 Validation Loss: 0.6142094731330872 \n",
      "     Validation Step: 7 Validation Loss: 0.6104939579963684 \n",
      "     Validation Step: 8 Validation Loss: 0.6145455837249756 \n",
      "     Validation Step: 9 Validation Loss: 0.6173514723777771 \n",
      "     Validation Step: 10 Validation Loss: 0.6136699318885803 \n",
      "     Validation Step: 11 Validation Loss: 0.6148478984832764 \n",
      "     Validation Step: 12 Validation Loss: 0.6170678734779358 \n",
      "     Validation Step: 13 Validation Loss: 0.6181069016456604 \n",
      "     Validation Step: 14 Validation Loss: 0.6162709593772888 \n",
      "     Validation Step: 15 Validation Loss: 0.6153396964073181 \n",
      "     Validation Step: 16 Validation Loss: 0.6177336573600769 \n",
      "     Validation Step: 17 Validation Loss: 0.6118675470352173 \n",
      "     Validation Step: 18 Validation Loss: 0.6156386733055115 \n",
      "     Validation Step: 19 Validation Loss: 0.6074654459953308 \n",
      "     Validation Step: 20 Validation Loss: 0.6141209006309509 \n",
      "     Validation Step: 21 Validation Loss: 0.6149130463600159 \n",
      "     Validation Step: 22 Validation Loss: 0.6185457706451416 \n",
      "     Validation Step: 23 Validation Loss: 0.6145966649055481 \n",
      "     Validation Step: 24 Validation Loss: 0.61366206407547 \n",
      "     Validation Step: 25 Validation Loss: 0.6100738644599915 \n",
      "     Validation Step: 26 Validation Loss: 0.6150526404380798 \n",
      "     Validation Step: 27 Validation Loss: 0.6152496337890625 \n",
      "     Validation Step: 28 Validation Loss: 0.6185213327407837 \n",
      "     Validation Step: 29 Validation Loss: 0.6121182441711426 \n",
      "     Validation Step: 30 Validation Loss: 0.6106127500534058 \n",
      "     Validation Step: 31 Validation Loss: 0.6136448979377747 \n",
      "     Validation Step: 32 Validation Loss: 0.6111457347869873 \n",
      "     Validation Step: 33 Validation Loss: 0.6176379323005676 \n",
      "     Validation Step: 34 Validation Loss: 0.6142644286155701 \n",
      "     Validation Step: 35 Validation Loss: 0.6146477460861206 \n",
      "     Validation Step: 36 Validation Loss: 0.611167848110199 \n",
      "     Validation Step: 37 Validation Loss: 0.6128240823745728 \n",
      "     Validation Step: 38 Validation Loss: 0.6182729005813599 \n",
      "     Validation Step: 39 Validation Loss: 0.610462486743927 \n",
      "     Validation Step: 40 Validation Loss: 0.612978458404541 \n",
      "     Validation Step: 41 Validation Loss: 0.6160221099853516 \n",
      "     Validation Step: 42 Validation Loss: 0.6115408539772034 \n",
      "     Validation Step: 43 Validation Loss: 0.6101037263870239 \n",
      "     Validation Step: 44 Validation Loss: 0.615813136100769 \n",
      "Epoch: 120\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6091969013214111 \n",
      "     Training Step: 1 Training Loss: 0.6210228204727173 \n",
      "     Training Step: 2 Training Loss: 0.6172471642494202 \n",
      "     Training Step: 3 Training Loss: 0.6122851967811584 \n",
      "     Training Step: 4 Training Loss: 0.6125169396400452 \n",
      "     Training Step: 5 Training Loss: 0.6141701936721802 \n",
      "     Training Step: 6 Training Loss: 0.611127495765686 \n",
      "     Training Step: 7 Training Loss: 0.615800142288208 \n",
      "     Training Step: 8 Training Loss: 0.6153978109359741 \n",
      "     Training Step: 9 Training Loss: 0.6123695373535156 \n",
      "     Training Step: 10 Training Loss: 0.6101719737052917 \n",
      "     Training Step: 11 Training Loss: 0.6147502064704895 \n",
      "     Training Step: 12 Training Loss: 0.6122445464134216 \n",
      "     Training Step: 13 Training Loss: 0.6129166483879089 \n",
      "     Training Step: 14 Training Loss: 0.6097396016120911 \n",
      "     Training Step: 15 Training Loss: 0.6162539124488831 \n",
      "     Training Step: 16 Training Loss: 0.6135083436965942 \n",
      "     Training Step: 17 Training Loss: 0.6094534993171692 \n",
      "     Training Step: 18 Training Loss: 0.6114128232002258 \n",
      "     Training Step: 19 Training Loss: 0.6142436265945435 \n",
      "     Training Step: 20 Training Loss: 0.6143512725830078 \n",
      "     Training Step: 21 Training Loss: 0.6150943636894226 \n",
      "     Training Step: 22 Training Loss: 0.6134125590324402 \n",
      "     Training Step: 23 Training Loss: 0.613111674785614 \n",
      "     Training Step: 24 Training Loss: 0.6105602383613586 \n",
      "     Training Step: 25 Training Loss: 0.6143506765365601 \n",
      "     Training Step: 26 Training Loss: 0.6124849915504456 \n",
      "     Training Step: 27 Training Loss: 0.6105570793151855 \n",
      "     Training Step: 28 Training Loss: 0.6117743253707886 \n",
      "     Training Step: 29 Training Loss: 0.6154353022575378 \n",
      "     Training Step: 30 Training Loss: 0.613206148147583 \n",
      "     Training Step: 31 Training Loss: 0.6146171689033508 \n",
      "     Training Step: 32 Training Loss: 0.6146552562713623 \n",
      "     Training Step: 33 Training Loss: 0.6166781187057495 \n",
      "     Training Step: 34 Training Loss: 0.6155351996421814 \n",
      "     Training Step: 35 Training Loss: 0.6133178472518921 \n",
      "     Training Step: 36 Training Loss: 0.61005699634552 \n",
      "     Training Step: 37 Training Loss: 0.6097422242164612 \n",
      "     Training Step: 38 Training Loss: 0.6146880388259888 \n",
      "     Training Step: 39 Training Loss: 0.6154186725616455 \n",
      "     Training Step: 40 Training Loss: 0.6198631525039673 \n",
      "     Training Step: 41 Training Loss: 0.6166994571685791 \n",
      "     Training Step: 42 Training Loss: 0.6106077432632446 \n",
      "     Training Step: 43 Training Loss: 0.6094219088554382 \n",
      "     Training Step: 44 Training Loss: 0.610385000705719 \n",
      "     Training Step: 45 Training Loss: 0.6107254028320312 \n",
      "     Training Step: 46 Training Loss: 0.6100647449493408 \n",
      "     Training Step: 47 Training Loss: 0.6127589344978333 \n",
      "     Training Step: 48 Training Loss: 0.6169567704200745 \n",
      "     Training Step: 49 Training Loss: 0.6100366115570068 \n",
      "     Training Step: 50 Training Loss: 0.6123737692832947 \n",
      "     Training Step: 51 Training Loss: 0.6154137253761292 \n",
      "     Training Step: 52 Training Loss: 0.6178456544876099 \n",
      "     Training Step: 53 Training Loss: 0.6155685782432556 \n",
      "     Training Step: 54 Training Loss: 0.6135945320129395 \n",
      "     Training Step: 55 Training Loss: 0.6139081716537476 \n",
      "     Training Step: 56 Training Loss: 0.6108695864677429 \n",
      "     Training Step: 57 Training Loss: 0.6168094277381897 \n",
      "     Training Step: 58 Training Loss: 0.6106924414634705 \n",
      "     Training Step: 59 Training Loss: 0.6144536733627319 \n",
      "     Training Step: 60 Training Loss: 0.6152830719947815 \n",
      "     Training Step: 61 Training Loss: 0.6122990250587463 \n",
      "     Training Step: 62 Training Loss: 0.6132044196128845 \n",
      "     Training Step: 63 Training Loss: 0.6133577823638916 \n",
      "     Training Step: 64 Training Loss: 0.6115551590919495 \n",
      "     Training Step: 65 Training Loss: 0.6167208552360535 \n",
      "     Training Step: 66 Training Loss: 0.6118020415306091 \n",
      "     Training Step: 67 Training Loss: 0.6157457828521729 \n",
      "     Training Step: 68 Training Loss: 0.6147264242172241 \n",
      "     Training Step: 69 Training Loss: 0.6114046573638916 \n",
      "     Training Step: 70 Training Loss: 0.6184549331665039 \n",
      "     Training Step: 71 Training Loss: 0.6171679496765137 \n",
      "     Training Step: 72 Training Loss: 0.6146364808082581 \n",
      "     Training Step: 73 Training Loss: 0.6166513562202454 \n",
      "     Training Step: 74 Training Loss: 0.6145984530448914 \n",
      "     Training Step: 75 Training Loss: 0.618304967880249 \n",
      "     Training Step: 76 Training Loss: 0.6167948842048645 \n",
      "     Training Step: 77 Training Loss: 0.6129211783409119 \n",
      "     Training Step: 78 Training Loss: 0.6168838739395142 \n",
      "     Training Step: 79 Training Loss: 0.6148759126663208 \n",
      "     Training Step: 80 Training Loss: 0.6185672283172607 \n",
      "     Training Step: 81 Training Loss: 0.6128349900245667 \n",
      "     Training Step: 82 Training Loss: 0.6139019131660461 \n",
      "     Training Step: 83 Training Loss: 0.616401731967926 \n",
      "     Training Step: 84 Training Loss: 0.6167938709259033 \n",
      "     Training Step: 85 Training Loss: 0.6183763742446899 \n",
      "     Training Step: 86 Training Loss: 0.6118610501289368 \n",
      "     Training Step: 87 Training Loss: 0.6121587157249451 \n",
      "     Training Step: 88 Training Loss: 0.6107630729675293 \n",
      "     Training Step: 89 Training Loss: 0.6083052158355713 \n",
      "     Training Step: 90 Training Loss: 0.6120701432228088 \n",
      "     Training Step: 91 Training Loss: 0.6140162944793701 \n",
      "     Training Step: 92 Training Loss: 0.6166934967041016 \n",
      "     Training Step: 93 Training Loss: 0.6177144646644592 \n",
      "     Training Step: 94 Training Loss: 0.6143872737884521 \n",
      "     Training Step: 95 Training Loss: 0.6144356727600098 \n",
      "     Training Step: 96 Training Loss: 0.6130728721618652 \n",
      "     Training Step: 97 Training Loss: 0.619733989238739 \n",
      "     Training Step: 98 Training Loss: 0.6138178110122681 \n",
      "     Training Step: 99 Training Loss: 0.6159515976905823 \n",
      "     Training Step: 100 Training Loss: 0.611437976360321 \n",
      "     Training Step: 101 Training Loss: 0.6147003173828125 \n",
      "     Training Step: 102 Training Loss: 0.6154395341873169 \n",
      "     Training Step: 103 Training Loss: 0.6164819598197937 \n",
      "     Training Step: 104 Training Loss: 0.6180193424224854 \n",
      "     Training Step: 105 Training Loss: 0.6146694421768188 \n",
      "     Training Step: 106 Training Loss: 0.6148021817207336 \n",
      "     Training Step: 107 Training Loss: 0.6134917140007019 \n",
      "     Training Step: 108 Training Loss: 0.612729012966156 \n",
      "     Training Step: 109 Training Loss: 0.614083468914032 \n",
      "     Training Step: 110 Training Loss: 0.6142215132713318 \n",
      "     Training Step: 111 Training Loss: 0.612241268157959 \n",
      "     Training Step: 112 Training Loss: 0.6115989089012146 \n",
      "     Training Step: 113 Training Loss: 0.6142078042030334 \n",
      "     Training Step: 114 Training Loss: 0.6164079308509827 \n",
      "     Training Step: 115 Training Loss: 0.6136375069618225 \n",
      "     Training Step: 116 Training Loss: 0.6156877875328064 \n",
      "     Training Step: 117 Training Loss: 0.611875057220459 \n",
      "     Training Step: 118 Training Loss: 0.6115174889564514 \n",
      "     Training Step: 119 Training Loss: 0.6108865737915039 \n",
      "     Training Step: 120 Training Loss: 0.6126387715339661 \n",
      "     Training Step: 121 Training Loss: 0.6115178465843201 \n",
      "     Training Step: 122 Training Loss: 0.6167446970939636 \n",
      "     Training Step: 123 Training Loss: 0.6132814884185791 \n",
      "     Training Step: 124 Training Loss: 0.6135033369064331 \n",
      "     Training Step: 125 Training Loss: 0.6137688755989075 \n",
      "     Training Step: 126 Training Loss: 0.6141512393951416 \n",
      "     Training Step: 127 Training Loss: 0.6147745847702026 \n",
      "     Training Step: 128 Training Loss: 0.6117761135101318 \n",
      "     Training Step: 129 Training Loss: 0.6116847991943359 \n",
      "     Training Step: 130 Training Loss: 0.6140233874320984 \n",
      "     Training Step: 131 Training Loss: 0.6105663180351257 \n",
      "     Training Step: 132 Training Loss: 0.6163608431816101 \n",
      "     Training Step: 133 Training Loss: 0.615523099899292 \n",
      "     Training Step: 134 Training Loss: 0.6132757663726807 \n",
      "     Training Step: 135 Training Loss: 0.6173906326293945 \n",
      "     Training Step: 136 Training Loss: 0.6111660003662109 \n",
      "     Training Step: 137 Training Loss: 0.6127638220787048 \n",
      "     Training Step: 138 Training Loss: 0.6149554252624512 \n",
      "     Training Step: 139 Training Loss: 0.6144774556159973 \n",
      "     Training Step: 140 Training Loss: 0.6153707504272461 \n",
      "     Training Step: 141 Training Loss: 0.6166486740112305 \n",
      "     Training Step: 142 Training Loss: 0.6152427792549133 \n",
      "     Training Step: 143 Training Loss: 0.6116470098495483 \n",
      "     Training Step: 144 Training Loss: 0.6121252775192261 \n",
      "     Training Step: 145 Training Loss: 0.6166088581085205 \n",
      "     Training Step: 146 Training Loss: 0.6114039421081543 \n",
      "     Training Step: 147 Training Loss: 0.6128395199775696 \n",
      "     Training Step: 148 Training Loss: 0.6149348616600037 \n",
      "     Training Step: 149 Training Loss: 0.614918053150177 \n",
      "     Training Step: 150 Training Loss: 0.6128714680671692 \n",
      "     Training Step: 151 Training Loss: 0.6139248609542847 \n",
      "     Training Step: 152 Training Loss: 0.6140351891517639 \n",
      "     Training Step: 153 Training Loss: 0.6146799921989441 \n",
      "     Training Step: 154 Training Loss: 0.6125271320343018 \n",
      "     Training Step: 155 Training Loss: 0.610130250453949 \n",
      "     Training Step: 156 Training Loss: 0.6132887601852417 \n",
      "     Training Step: 157 Training Loss: 0.6144385933876038 \n",
      "     Training Step: 158 Training Loss: 0.6176601648330688 \n",
      "     Training Step: 159 Training Loss: 0.6161209940910339 \n",
      "     Training Step: 160 Training Loss: 0.616215705871582 \n",
      "     Training Step: 161 Training Loss: 0.6129669547080994 \n",
      "     Training Step: 162 Training Loss: 0.6137408018112183 \n",
      "     Training Step: 163 Training Loss: 0.610485315322876 \n",
      "     Training Step: 164 Training Loss: 0.6136481165885925 \n",
      "     Training Step: 165 Training Loss: 0.6170927286148071 \n",
      "     Training Step: 166 Training Loss: 0.612153947353363 \n",
      "     Training Step: 167 Training Loss: 0.6180177330970764 \n",
      "     Training Step: 168 Training Loss: 0.6128695607185364 \n",
      "     Training Step: 169 Training Loss: 0.6134626269340515 \n",
      "     Training Step: 170 Training Loss: 0.609996497631073 \n",
      "     Training Step: 171 Training Loss: 0.6122409701347351 \n",
      "     Training Step: 172 Training Loss: 0.6158309578895569 \n",
      "     Training Step: 173 Training Loss: 0.61434006690979 \n",
      "     Training Step: 174 Training Loss: 0.610490620136261 \n",
      "     Training Step: 175 Training Loss: 0.6153078079223633 \n",
      "     Training Step: 176 Training Loss: 0.6157515048980713 \n",
      "     Training Step: 177 Training Loss: 0.6118260025978088 \n",
      "     Training Step: 178 Training Loss: 0.6177782416343689 \n",
      "     Training Step: 179 Training Loss: 0.6111916303634644 \n",
      "     Training Step: 180 Training Loss: 0.614107608795166 \n",
      "     Training Step: 181 Training Loss: 0.6123736500740051 \n",
      "     Training Step: 182 Training Loss: 0.6174978613853455 \n",
      "     Training Step: 183 Training Loss: 0.6147055625915527 \n",
      "     Training Step: 184 Training Loss: 0.6181023716926575 \n",
      "     Training Step: 185 Training Loss: 0.6160295009613037 \n",
      "     Training Step: 186 Training Loss: 0.6167147755622864 \n",
      "     Training Step: 187 Training Loss: 0.6157523989677429 \n",
      "     Training Step: 188 Training Loss: 0.615064799785614 \n",
      "     Training Step: 189 Training Loss: 0.6125552654266357 \n",
      "     Training Step: 190 Training Loss: 0.6142846941947937 \n",
      "     Training Step: 191 Training Loss: 0.6181989312171936 \n",
      "     Training Step: 192 Training Loss: 0.6161916255950928 \n",
      "     Training Step: 193 Training Loss: 0.6177911758422852 \n",
      "     Training Step: 194 Training Loss: 0.6131643056869507 \n",
      "     Training Step: 195 Training Loss: 0.6155916452407837 \n",
      "     Training Step: 196 Training Loss: 0.6185504198074341 \n",
      "     Training Step: 197 Training Loss: 0.6137559413909912 \n",
      "     Training Step: 198 Training Loss: 0.6184350252151489 \n",
      "     Training Step: 199 Training Loss: 0.6180486679077148 \n",
      "     Training Step: 200 Training Loss: 0.6120455861091614 \n",
      "     Training Step: 201 Training Loss: 0.6188414692878723 \n",
      "     Training Step: 202 Training Loss: 0.6176729798316956 \n",
      "     Training Step: 203 Training Loss: 0.616740345954895 \n",
      "     Training Step: 204 Training Loss: 0.6195772886276245 \n",
      "     Training Step: 205 Training Loss: 0.617115318775177 \n",
      "     Training Step: 206 Training Loss: 0.6122052669525146 \n",
      "     Training Step: 207 Training Loss: 0.6130974292755127 \n",
      "     Training Step: 208 Training Loss: 0.6136476993560791 \n",
      "     Training Step: 209 Training Loss: 0.6152621507644653 \n",
      "     Training Step: 210 Training Loss: 0.6201882362365723 \n",
      "     Training Step: 211 Training Loss: 0.6150965094566345 \n",
      "     Training Step: 212 Training Loss: 0.6097418665885925 \n",
      "     Training Step: 213 Training Loss: 0.6133056282997131 \n",
      "     Training Step: 214 Training Loss: 0.6118344664573669 \n",
      "     Training Step: 215 Training Loss: 0.6182372570037842 \n",
      "     Training Step: 216 Training Loss: 0.6160043478012085 \n",
      "     Training Step: 217 Training Loss: 0.6114308834075928 \n",
      "     Training Step: 218 Training Loss: 0.6115183234214783 \n",
      "     Training Step: 219 Training Loss: 0.6146115064620972 \n",
      "     Training Step: 220 Training Loss: 0.6171622276306152 \n",
      "     Training Step: 221 Training Loss: 0.6152927875518799 \n",
      "     Training Step: 222 Training Loss: 0.6151579022407532 \n",
      "     Training Step: 223 Training Loss: 0.6194576025009155 \n",
      "     Training Step: 224 Training Loss: 0.615474283695221 \n",
      "     Training Step: 225 Training Loss: 0.6151736974716187 \n",
      "     Training Step: 226 Training Loss: 0.6116140484809875 \n",
      "     Training Step: 227 Training Loss: 0.6114911437034607 \n",
      "     Training Step: 228 Training Loss: 0.6131897568702698 \n",
      "     Training Step: 229 Training Loss: 0.610410749912262 \n",
      "     Training Step: 230 Training Loss: 0.6116358041763306 \n",
      "     Training Step: 231 Training Loss: 0.6156866550445557 \n",
      "     Training Step: 232 Training Loss: 0.6176974177360535 \n",
      "     Training Step: 233 Training Loss: 0.6106571555137634 \n",
      "     Training Step: 234 Training Loss: 0.6116219162940979 \n",
      "     Training Step: 235 Training Loss: 0.614666223526001 \n",
      "     Training Step: 236 Training Loss: 0.6145018935203552 \n",
      "     Training Step: 237 Training Loss: 0.6124594807624817 \n",
      "     Training Step: 238 Training Loss: 0.6188564896583557 \n",
      "     Training Step: 239 Training Loss: 0.6153451204299927 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6104715466499329 \n",
      "     Validation Step: 1 Validation Loss: 0.6185069680213928 \n",
      "     Validation Step: 2 Validation Loss: 0.6145358085632324 \n",
      "     Validation Step: 3 Validation Loss: 0.6105048060417175 \n",
      "     Validation Step: 4 Validation Loss: 0.6106219291687012 \n",
      "     Validation Step: 5 Validation Loss: 0.617700457572937 \n",
      "     Validation Step: 6 Validation Loss: 0.6184857487678528 \n",
      "     Validation Step: 7 Validation Loss: 0.6162498593330383 \n",
      "     Validation Step: 8 Validation Loss: 0.6153221130371094 \n",
      "     Validation Step: 9 Validation Loss: 0.6136624813079834 \n",
      "     Validation Step: 10 Validation Loss: 0.6148359179496765 \n",
      "     Validation Step: 11 Validation Loss: 0.6133056282997131 \n",
      "     Validation Step: 12 Validation Loss: 0.6150363683700562 \n",
      "     Validation Step: 13 Validation Loss: 0.6176090836524963 \n",
      "     Validation Step: 14 Validation Loss: 0.6136484742164612 \n",
      "     Validation Step: 15 Validation Loss: 0.6146381497383118 \n",
      "     Validation Step: 16 Validation Loss: 0.6101229786872864 \n",
      "     Validation Step: 17 Validation Loss: 0.611547589302063 \n",
      "     Validation Step: 18 Validation Loss: 0.615994930267334 \n",
      "     Validation Step: 19 Validation Loss: 0.6141136288642883 \n",
      "     Validation Step: 20 Validation Loss: 0.6129786968231201 \n",
      "     Validation Step: 21 Validation Loss: 0.6180732250213623 \n",
      "     Validation Step: 22 Validation Loss: 0.6100940108299255 \n",
      "     Validation Step: 23 Validation Loss: 0.6170341968536377 \n",
      "     Validation Step: 24 Validation Loss: 0.6148988008499146 \n",
      "     Validation Step: 25 Validation Loss: 0.6152310967445374 \n",
      "     Validation Step: 26 Validation Loss: 0.6173189282417297 \n",
      "     Validation Step: 27 Validation Loss: 0.6142022013664246 \n",
      "     Validation Step: 28 Validation Loss: 0.6136366128921509 \n",
      "     Validation Step: 29 Validation Loss: 0.6111705899238586 \n",
      "     Validation Step: 30 Validation Loss: 0.6074977517127991 \n",
      "     Validation Step: 31 Validation Loss: 0.614123523235321 \n",
      "     Validation Step: 32 Validation Loss: 0.6116359829902649 \n",
      "     Validation Step: 33 Validation Loss: 0.618350088596344 \n",
      "     Validation Step: 34 Validation Loss: 0.6156180500984192 \n",
      "     Validation Step: 35 Validation Loss: 0.6157907843589783 \n",
      "     Validation Step: 36 Validation Loss: 0.6118726134300232 \n",
      "     Validation Step: 37 Validation Loss: 0.6111525297164917 \n",
      "     Validation Step: 38 Validation Loss: 0.6101499199867249 \n",
      "     Validation Step: 39 Validation Loss: 0.6121227145195007 \n",
      "     Validation Step: 40 Validation Loss: 0.6142562627792358 \n",
      "     Validation Step: 41 Validation Loss: 0.6182380318641663 \n",
      "     Validation Step: 42 Validation Loss: 0.6155776977539062 \n",
      "     Validation Step: 43 Validation Loss: 0.6128249764442444 \n",
      "     Validation Step: 44 Validation Loss: 0.6145774126052856 \n",
      "Epoch: 121\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6114272475242615 \n",
      "     Training Step: 1 Training Loss: 0.6151593327522278 \n",
      "     Training Step: 2 Training Loss: 0.6105764508247375 \n",
      "     Training Step: 3 Training Loss: 0.6107344031333923 \n",
      "     Training Step: 4 Training Loss: 0.6105651259422302 \n",
      "     Training Step: 5 Training Loss: 0.6082388758659363 \n",
      "     Training Step: 6 Training Loss: 0.6167269349098206 \n",
      "     Training Step: 7 Training Loss: 0.6181014180183411 \n",
      "     Training Step: 8 Training Loss: 0.6140233278274536 \n",
      "     Training Step: 9 Training Loss: 0.6175071001052856 \n",
      "     Training Step: 10 Training Loss: 0.6172055006027222 \n",
      "     Training Step: 11 Training Loss: 0.6128414273262024 \n",
      "     Training Step: 12 Training Loss: 0.617763102054596 \n",
      "     Training Step: 13 Training Loss: 0.6146501898765564 \n",
      "     Training Step: 14 Training Loss: 0.6111757159233093 \n",
      "     Training Step: 15 Training Loss: 0.6118003726005554 \n",
      "     Training Step: 16 Training Loss: 0.6127158403396606 \n",
      "     Training Step: 17 Training Loss: 0.6094282865524292 \n",
      "     Training Step: 18 Training Loss: 0.6116495728492737 \n",
      "     Training Step: 19 Training Loss: 0.6146052479743958 \n",
      "     Training Step: 20 Training Loss: 0.6155170202255249 \n",
      "     Training Step: 21 Training Loss: 0.6171054840087891 \n",
      "     Training Step: 22 Training Loss: 0.6113942265510559 \n",
      "     Training Step: 23 Training Loss: 0.614339292049408 \n",
      "     Training Step: 24 Training Loss: 0.6178105473518372 \n",
      "     Training Step: 25 Training Loss: 0.6171560883522034 \n",
      "     Training Step: 26 Training Loss: 0.6100561618804932 \n",
      "     Training Step: 27 Training Loss: 0.6152810454368591 \n",
      "     Training Step: 28 Training Loss: 0.614105224609375 \n",
      "     Training Step: 29 Training Loss: 0.6109048128128052 \n",
      "     Training Step: 30 Training Loss: 0.6188662648200989 \n",
      "     Training Step: 31 Training Loss: 0.6146749258041382 \n",
      "     Training Step: 32 Training Loss: 0.6114442348480225 \n",
      "     Training Step: 33 Training Loss: 0.610191285610199 \n",
      "     Training Step: 34 Training Loss: 0.6182265877723694 \n",
      "     Training Step: 35 Training Loss: 0.6133134365081787 \n",
      "     Training Step: 36 Training Loss: 0.6122949719429016 \n",
      "     Training Step: 37 Training Loss: 0.6143407821655273 \n",
      "     Training Step: 38 Training Loss: 0.6108744144439697 \n",
      "     Training Step: 39 Training Loss: 0.6154272556304932 \n",
      "     Training Step: 40 Training Loss: 0.6126394867897034 \n",
      "     Training Step: 41 Training Loss: 0.6101229786872864 \n",
      "     Training Step: 42 Training Loss: 0.6116226315498352 \n",
      "     Training Step: 43 Training Loss: 0.6150941848754883 \n",
      "     Training Step: 44 Training Loss: 0.6154009103775024 \n",
      "     Training Step: 45 Training Loss: 0.6134527325630188 \n",
      "     Training Step: 46 Training Loss: 0.6137343049049377 \n",
      "     Training Step: 47 Training Loss: 0.6122269630432129 \n",
      "     Training Step: 48 Training Loss: 0.6105589866638184 \n",
      "     Training Step: 49 Training Loss: 0.6120123267173767 \n",
      "     Training Step: 50 Training Loss: 0.6176672577857971 \n",
      "     Training Step: 51 Training Loss: 0.6114084124565125 \n",
      "     Training Step: 52 Training Loss: 0.6154214143753052 \n",
      "     Training Step: 53 Training Loss: 0.6116828918457031 \n",
      "     Training Step: 54 Training Loss: 0.6116124391555786 \n",
      "     Training Step: 55 Training Loss: 0.6118355989456177 \n",
      "     Training Step: 56 Training Loss: 0.6184684038162231 \n",
      "     Training Step: 57 Training Loss: 0.6124619841575623 \n",
      "     Training Step: 58 Training Loss: 0.6128726005554199 \n",
      "     Training Step: 59 Training Loss: 0.6196120977401733 \n",
      "     Training Step: 60 Training Loss: 0.6163470149040222 \n",
      "     Training Step: 61 Training Loss: 0.619679868221283 \n",
      "     Training Step: 62 Training Loss: 0.618812620639801 \n",
      "     Training Step: 63 Training Loss: 0.6092780828475952 \n",
      "     Training Step: 64 Training Loss: 0.6116662621498108 \n",
      "     Training Step: 65 Training Loss: 0.6115917563438416 \n",
      "     Training Step: 66 Training Loss: 0.6135958433151245 \n",
      "     Training Step: 67 Training Loss: 0.6201929450035095 \n",
      "     Training Step: 68 Training Loss: 0.6147454977035522 \n",
      "     Training Step: 69 Training Loss: 0.6167919039726257 \n",
      "     Training Step: 70 Training Loss: 0.6100847125053406 \n",
      "     Training Step: 71 Training Loss: 0.6180279850959778 \n",
      "     Training Step: 72 Training Loss: 0.6132056713104248 \n",
      "     Training Step: 73 Training Loss: 0.6164018511772156 \n",
      "     Training Step: 74 Training Loss: 0.6141539216041565 \n",
      "     Training Step: 75 Training Loss: 0.6132832765579224 \n",
      "     Training Step: 76 Training Loss: 0.6183152198791504 \n",
      "     Training Step: 77 Training Loss: 0.6168918013572693 \n",
      "     Training Step: 78 Training Loss: 0.6097384095191956 \n",
      "     Training Step: 79 Training Loss: 0.6180129051208496 \n",
      "     Training Step: 80 Training Loss: 0.615439772605896 \n",
      "     Training Step: 81 Training Loss: 0.6139156818389893 \n",
      "     Training Step: 82 Training Loss: 0.6150633096694946 \n",
      "     Training Step: 83 Training Loss: 0.6180892586708069 \n",
      "     Training Step: 84 Training Loss: 0.6157786250114441 \n",
      "     Training Step: 85 Training Loss: 0.6157386898994446 \n",
      "     Training Step: 86 Training Loss: 0.614499032497406 \n",
      "     Training Step: 87 Training Loss: 0.6161922216415405 \n",
      "     Training Step: 88 Training Loss: 0.612506628036499 \n",
      "     Training Step: 89 Training Loss: 0.6146945357322693 \n",
      "     Training Step: 90 Training Loss: 0.6132950186729431 \n",
      "     Training Step: 91 Training Loss: 0.6157500743865967 \n",
      "     Training Step: 92 Training Loss: 0.6146069169044495 \n",
      "     Training Step: 93 Training Loss: 0.6151736378669739 \n",
      "     Training Step: 94 Training Loss: 0.6185755729675293 \n",
      "     Training Step: 95 Training Loss: 0.6152984499931335 \n",
      "     Training Step: 96 Training Loss: 0.6149142980575562 \n",
      "     Training Step: 97 Training Loss: 0.6157401204109192 \n",
      "     Training Step: 98 Training Loss: 0.611485481262207 \n",
      "     Training Step: 99 Training Loss: 0.6122884154319763 \n",
      "     Training Step: 100 Training Loss: 0.6146685481071472 \n",
      "     Training Step: 101 Training Loss: 0.6173787713050842 \n",
      "     Training Step: 102 Training Loss: 0.6118305921554565 \n",
      "     Training Step: 103 Training Loss: 0.6134979128837585 \n",
      "     Training Step: 104 Training Loss: 0.610397219657898 \n",
      "     Training Step: 105 Training Loss: 0.6167249083518982 \n",
      "     Training Step: 106 Training Loss: 0.6121542453765869 \n",
      "     Training Step: 107 Training Loss: 0.6134065389633179 \n",
      "     Training Step: 108 Training Loss: 0.6123731136322021 \n",
      "     Training Step: 109 Training Loss: 0.6166819334030151 \n",
      "     Training Step: 110 Training Loss: 0.6162235736846924 \n",
      "     Training Step: 111 Training Loss: 0.6147935390472412 \n",
      "     Training Step: 112 Training Loss: 0.6165030002593994 \n",
      "     Training Step: 113 Training Loss: 0.6143812537193298 \n",
      "     Training Step: 114 Training Loss: 0.6136398315429688 \n",
      "     Training Step: 115 Training Loss: 0.614439070224762 \n",
      "     Training Step: 116 Training Loss: 0.617687463760376 \n",
      "     Training Step: 117 Training Loss: 0.6105117797851562 \n",
      "     Training Step: 118 Training Loss: 0.6144786477088928 \n",
      "     Training Step: 119 Training Loss: 0.6104972958564758 \n",
      "     Training Step: 120 Training Loss: 0.6167116761207581 \n",
      "     Training Step: 121 Training Loss: 0.6166439056396484 \n",
      "     Training Step: 122 Training Loss: 0.6158276200294495 \n",
      "     Training Step: 123 Training Loss: 0.6125360131263733 \n",
      "     Training Step: 124 Training Loss: 0.6168004274368286 \n",
      "     Training Step: 125 Training Loss: 0.6140409708023071 \n",
      "     Training Step: 126 Training Loss: 0.6121586561203003 \n",
      "     Training Step: 127 Training Loss: 0.6146671175956726 \n",
      "     Training Step: 128 Training Loss: 0.6167534589767456 \n",
      "     Training Step: 129 Training Loss: 0.6133018732070923 \n",
      "     Training Step: 130 Training Loss: 0.6097390055656433 \n",
      "     Training Step: 131 Training Loss: 0.6128658056259155 \n",
      "     Training Step: 132 Training Loss: 0.6167068481445312 \n",
      "     Training Step: 133 Training Loss: 0.615336000919342 \n",
      "     Training Step: 134 Training Loss: 0.6155310869216919 \n",
      "     Training Step: 135 Training Loss: 0.6156831979751587 \n",
      "     Training Step: 136 Training Loss: 0.6152406930923462 \n",
      "     Training Step: 137 Training Loss: 0.6118842959403992 \n",
      "     Training Step: 138 Training Loss: 0.615397036075592 \n",
      "     Training Step: 139 Training Loss: 0.6120719909667969 \n",
      "     Training Step: 140 Training Loss: 0.614955484867096 \n",
      "     Training Step: 141 Training Loss: 0.6135125756263733 \n",
      "     Training Step: 142 Training Loss: 0.6138179898262024 \n",
      "     Training Step: 143 Training Loss: 0.615368664264679 \n",
      "     Training Step: 144 Training Loss: 0.6209334135055542 \n",
      "     Training Step: 145 Training Loss: 0.6166144013404846 \n",
      "     Training Step: 146 Training Loss: 0.6125312447547913 \n",
      "     Training Step: 147 Training Loss: 0.6152809262275696 \n",
      "     Training Step: 148 Training Loss: 0.610704779624939 \n",
      "     Training Step: 149 Training Loss: 0.615100085735321 \n",
      "     Training Step: 150 Training Loss: 0.6144207715988159 \n",
      "     Training Step: 151 Training Loss: 0.6164136528968811 \n",
      "     Training Step: 152 Training Loss: 0.6114183664321899 \n",
      "     Training Step: 153 Training Loss: 0.6129692792892456 \n",
      "     Training Step: 154 Training Loss: 0.6106669902801514 \n",
      "     Training Step: 155 Training Loss: 0.609458327293396 \n",
      "     Training Step: 156 Training Loss: 0.611791729927063 \n",
      "     Training Step: 157 Training Loss: 0.6136201024055481 \n",
      "     Training Step: 158 Training Loss: 0.614454984664917 \n",
      "     Training Step: 159 Training Loss: 0.6194978356361389 \n",
      "     Training Step: 160 Training Loss: 0.6147394180297852 \n",
      "     Training Step: 161 Training Loss: 0.6115172505378723 \n",
      "     Training Step: 162 Training Loss: 0.6136543154716492 \n",
      "     Training Step: 163 Training Loss: 0.6155645251274109 \n",
      "     Training Step: 164 Training Loss: 0.6138913631439209 \n",
      "     Training Step: 165 Training Loss: 0.6107112765312195 \n",
      "     Training Step: 166 Training Loss: 0.611137330532074 \n",
      "     Training Step: 167 Training Loss: 0.6142434477806091 \n",
      "     Training Step: 168 Training Loss: 0.6115190386772156 \n",
      "     Training Step: 169 Training Loss: 0.6115157008171082 \n",
      "     Training Step: 170 Training Loss: 0.6166608929634094 \n",
      "     Training Step: 171 Training Loss: 0.6198935508728027 \n",
      "     Training Step: 172 Training Loss: 0.6130589842796326 \n",
      "     Training Step: 173 Training Loss: 0.6127511262893677 \n",
      "     Training Step: 174 Training Loss: 0.6146990060806274 \n",
      "     Training Step: 175 Training Loss: 0.6143440008163452 \n",
      "     Training Step: 176 Training Loss: 0.6123840808868408 \n",
      "     Training Step: 177 Training Loss: 0.6147701144218445 \n",
      "     Training Step: 178 Training Loss: 0.6169198155403137 \n",
      "     Training Step: 179 Training Loss: 0.6160280704498291 \n",
      "     Training Step: 180 Training Loss: 0.613071858882904 \n",
      "     Training Step: 181 Training Loss: 0.6121317148208618 \n",
      "     Training Step: 182 Training Loss: 0.6176811456680298 \n",
      "     Training Step: 183 Training Loss: 0.6123830676078796 \n",
      "     Training Step: 184 Training Loss: 0.6137450337409973 \n",
      "     Training Step: 185 Training Loss: 0.6106054782867432 \n",
      "     Training Step: 186 Training Loss: 0.6171358823776245 \n",
      "     Training Step: 187 Training Loss: 0.6147042512893677 \n",
      "     Training Step: 188 Training Loss: 0.6132033467292786 \n",
      "     Training Step: 189 Training Loss: 0.6132884621620178 \n",
      "     Training Step: 190 Training Loss: 0.6141517162322998 \n",
      "     Training Step: 191 Training Loss: 0.6160054206848145 \n",
      "     Training Step: 192 Training Loss: 0.6140220761299133 \n",
      "     Training Step: 193 Training Loss: 0.61407470703125 \n",
      "     Training Step: 194 Training Loss: 0.6122345328330994 \n",
      "     Training Step: 195 Training Loss: 0.6156818866729736 \n",
      "     Training Step: 196 Training Loss: 0.6171643733978271 \n",
      "     Training Step: 197 Training Loss: 0.6159462332725525 \n",
      "     Training Step: 198 Training Loss: 0.6142824292182922 \n",
      "     Training Step: 199 Training Loss: 0.6142120361328125 \n",
      "     Training Step: 200 Training Loss: 0.6129140257835388 \n",
      "     Training Step: 201 Training Loss: 0.6152623891830444 \n",
      "     Training Step: 202 Training Loss: 0.6100612282752991 \n",
      "     Training Step: 203 Training Loss: 0.6133580207824707 \n",
      "     Training Step: 204 Training Loss: 0.6166600584983826 \n",
      "     Training Step: 205 Training Loss: 0.6146005392074585 \n",
      "     Training Step: 206 Training Loss: 0.6115920543670654 \n",
      "     Training Step: 207 Training Loss: 0.6146380305290222 \n",
      "     Training Step: 208 Training Loss: 0.6099891066551208 \n",
      "     Training Step: 209 Training Loss: 0.6182265281677246 \n",
      "     Training Step: 210 Training Loss: 0.6118238568305969 \n",
      "     Training Step: 211 Training Loss: 0.6154755353927612 \n",
      "     Training Step: 212 Training Loss: 0.612795889377594 \n",
      "     Training Step: 213 Training Loss: 0.610365092754364 \n",
      "     Training Step: 214 Training Loss: 0.6184235215187073 \n",
      "     Training Step: 215 Training Loss: 0.616815447807312 \n",
      "     Training Step: 216 Training Loss: 0.6122234463691711 \n",
      "     Training Step: 217 Training Loss: 0.6137643456459045 \n",
      "     Training Step: 218 Training Loss: 0.6177074313163757 \n",
      "     Training Step: 219 Training Loss: 0.6155974268913269 \n",
      "     Training Step: 220 Training Loss: 0.6121430993080139 \n",
      "     Training Step: 221 Training Loss: 0.611208975315094 \n",
      "     Training Step: 222 Training Loss: 0.6131489276885986 \n",
      "     Training Step: 223 Training Loss: 0.6127712726593018 \n",
      "     Training Step: 224 Training Loss: 0.6131286025047302 \n",
      "     Training Step: 225 Training Loss: 0.6184394359588623 \n",
      "     Training Step: 226 Training Loss: 0.6134729981422424 \n",
      "     Training Step: 227 Training Loss: 0.6185950636863708 \n",
      "     Training Step: 228 Training Loss: 0.6178053021430969 \n",
      "     Training Step: 229 Training Loss: 0.6129248738288879 \n",
      "     Training Step: 230 Training Loss: 0.6131905317306519 \n",
      "     Training Step: 231 Training Loss: 0.6161099672317505 \n",
      "     Training Step: 232 Training Loss: 0.6149300336837769 \n",
      "     Training Step: 233 Training Loss: 0.6162309050559998 \n",
      "     Training Step: 234 Training Loss: 0.609733521938324 \n",
      "     Training Step: 235 Training Loss: 0.6142085194587708 \n",
      "     Training Step: 236 Training Loss: 0.6125211715698242 \n",
      "     Training Step: 237 Training Loss: 0.613923966884613 \n",
      "     Training Step: 238 Training Loss: 0.6148682236671448 \n",
      "     Training Step: 239 Training Loss: 0.6118030548095703 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6141997575759888 \n",
      "     Validation Step: 1 Validation Loss: 0.610620379447937 \n",
      "     Validation Step: 2 Validation Loss: 0.6176031231880188 \n",
      "     Validation Step: 3 Validation Loss: 0.6185055375099182 \n",
      "     Validation Step: 4 Validation Loss: 0.6128201484680176 \n",
      "     Validation Step: 5 Validation Loss: 0.6153238415718079 \n",
      "     Validation Step: 6 Validation Loss: 0.6142542958259583 \n",
      "     Validation Step: 7 Validation Loss: 0.6157900094985962 \n",
      "     Validation Step: 8 Validation Loss: 0.61412513256073 \n",
      "     Validation Step: 9 Validation Loss: 0.6146367788314819 \n",
      "     Validation Step: 10 Validation Loss: 0.6148963570594788 \n",
      "     Validation Step: 11 Validation Loss: 0.6159965395927429 \n",
      "     Validation Step: 12 Validation Loss: 0.6111722588539124 \n",
      "     Validation Step: 13 Validation Loss: 0.61457359790802 \n",
      "     Validation Step: 14 Validation Loss: 0.614534854888916 \n",
      "     Validation Step: 15 Validation Loss: 0.6115468144416809 \n",
      "     Validation Step: 16 Validation Loss: 0.6129770874977112 \n",
      "     Validation Step: 17 Validation Loss: 0.6136302351951599 \n",
      "     Validation Step: 18 Validation Loss: 0.6121212244033813 \n",
      "     Validation Step: 19 Validation Loss: 0.6105040311813354 \n",
      "     Validation Step: 20 Validation Loss: 0.6156182289123535 \n",
      "     Validation Step: 21 Validation Loss: 0.6173137426376343 \n",
      "     Validation Step: 22 Validation Loss: 0.6150409579277039 \n",
      "     Validation Step: 23 Validation Loss: 0.6118707060813904 \n",
      "     Validation Step: 24 Validation Loss: 0.6162457466125488 \n",
      "     Validation Step: 25 Validation Loss: 0.6074996590614319 \n",
      "     Validation Step: 26 Validation Loss: 0.6148326396942139 \n",
      "     Validation Step: 27 Validation Loss: 0.6136629581451416 \n",
      "     Validation Step: 28 Validation Loss: 0.6177006959915161 \n",
      "     Validation Step: 29 Validation Loss: 0.6184803247451782 \n",
      "     Validation Step: 30 Validation Loss: 0.6152306795120239 \n",
      "     Validation Step: 31 Validation Loss: 0.6100979447364807 \n",
      "     Validation Step: 32 Validation Loss: 0.6101238131523132 \n",
      "     Validation Step: 33 Validation Loss: 0.6104734539985657 \n",
      "     Validation Step: 34 Validation Loss: 0.6170333027839661 \n",
      "     Validation Step: 35 Validation Loss: 0.6136521697044373 \n",
      "     Validation Step: 36 Validation Loss: 0.6101478934288025 \n",
      "     Validation Step: 37 Validation Loss: 0.6116334199905396 \n",
      "     Validation Step: 38 Validation Loss: 0.6182379126548767 \n",
      "     Validation Step: 39 Validation Loss: 0.6183520555496216 \n",
      "     Validation Step: 40 Validation Loss: 0.6180720925331116 \n",
      "     Validation Step: 41 Validation Loss: 0.6141118407249451 \n",
      "     Validation Step: 42 Validation Loss: 0.6155781149864197 \n",
      "     Validation Step: 43 Validation Loss: 0.6111549139022827 \n",
      "     Validation Step: 44 Validation Loss: 0.6133018732070923 \n",
      "Epoch: 122\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6184746623039246 \n",
      "     Training Step: 1 Training Loss: 0.6100471615791321 \n",
      "     Training Step: 2 Training Loss: 0.6146030426025391 \n",
      "     Training Step: 3 Training Loss: 0.6158369183540344 \n",
      "     Training Step: 4 Training Loss: 0.6097087860107422 \n",
      "     Training Step: 5 Training Loss: 0.612153947353363 \n",
      "     Training Step: 6 Training Loss: 0.6147094368934631 \n",
      "     Training Step: 7 Training Loss: 0.611515998840332 \n",
      "     Training Step: 8 Training Loss: 0.6152915358543396 \n",
      "     Training Step: 9 Training Loss: 0.6105596423149109 \n",
      "     Training Step: 10 Training Loss: 0.6135051846504211 \n",
      "     Training Step: 11 Training Loss: 0.6131116151809692 \n",
      "     Training Step: 12 Training Loss: 0.6101628541946411 \n",
      "     Training Step: 13 Training Loss: 0.616422176361084 \n",
      "     Training Step: 14 Training Loss: 0.6113875508308411 \n",
      "     Training Step: 15 Training Loss: 0.6113978028297424 \n",
      "     Training Step: 16 Training Loss: 0.6111515760421753 \n",
      "     Training Step: 17 Training Loss: 0.6176810264587402 \n",
      "     Training Step: 18 Training Loss: 0.6118199229240417 \n",
      "     Training Step: 19 Training Loss: 0.6189029216766357 \n",
      "     Training Step: 20 Training Loss: 0.6118173003196716 \n",
      "     Training Step: 21 Training Loss: 0.6143409013748169 \n",
      "     Training Step: 22 Training Loss: 0.6092233061790466 \n",
      "     Training Step: 23 Training Loss: 0.61649489402771 \n",
      "     Training Step: 24 Training Loss: 0.6140326261520386 \n",
      "     Training Step: 25 Training Loss: 0.6198737621307373 \n",
      "     Training Step: 26 Training Loss: 0.6151567101478577 \n",
      "     Training Step: 27 Training Loss: 0.6120765805244446 \n",
      "     Training Step: 28 Training Loss: 0.6151726841926575 \n",
      "     Training Step: 29 Training Loss: 0.6128795146942139 \n",
      "     Training Step: 30 Training Loss: 0.6144785284996033 \n",
      "     Training Step: 31 Training Loss: 0.6107479333877563 \n",
      "     Training Step: 32 Training Loss: 0.6140243411064148 \n",
      "     Training Step: 33 Training Loss: 0.6159421801567078 \n",
      "     Training Step: 34 Training Loss: 0.6137652397155762 \n",
      "     Training Step: 35 Training Loss: 0.6147909164428711 \n",
      "     Training Step: 36 Training Loss: 0.6164162755012512 \n",
      "     Training Step: 37 Training Loss: 0.6124629378318787 \n",
      "     Training Step: 38 Training Loss: 0.6105740666389465 \n",
      "     Training Step: 39 Training Loss: 0.6153369545936584 \n",
      "     Training Step: 40 Training Loss: 0.6137426495552063 \n",
      "     Training Step: 41 Training Loss: 0.6150673627853394 \n",
      "     Training Step: 42 Training Loss: 0.6202260851860046 \n",
      "     Training Step: 43 Training Loss: 0.6139109134674072 \n",
      "     Training Step: 44 Training Loss: 0.6154201626777649 \n",
      "     Training Step: 45 Training Loss: 0.614702045917511 \n",
      "     Training Step: 46 Training Loss: 0.6153752207756042 \n",
      "     Training Step: 47 Training Loss: 0.610599160194397 \n",
      "     Training Step: 48 Training Loss: 0.6109041571617126 \n",
      "     Training Step: 49 Training Loss: 0.6097415685653687 \n",
      "     Training Step: 50 Training Loss: 0.6128414869308472 \n",
      "     Training Step: 51 Training Loss: 0.6132007241249084 \n",
      "     Training Step: 52 Training Loss: 0.614650547504425 \n",
      "     Training Step: 53 Training Loss: 0.6146834492683411 \n",
      "     Training Step: 54 Training Loss: 0.6146090030670166 \n",
      "     Training Step: 55 Training Loss: 0.6166682839393616 \n",
      "     Training Step: 56 Training Loss: 0.6125078797340393 \n",
      "     Training Step: 57 Training Loss: 0.6168144345283508 \n",
      "     Training Step: 58 Training Loss: 0.6131393313407898 \n",
      "     Training Step: 59 Training Loss: 0.6132883429527283 \n",
      "     Training Step: 60 Training Loss: 0.6082552075386047 \n",
      "     Training Step: 61 Training Loss: 0.6169331073760986 \n",
      "     Training Step: 62 Training Loss: 0.6122890114784241 \n",
      "     Training Step: 63 Training Loss: 0.6116241812705994 \n",
      "     Training Step: 64 Training Loss: 0.6114109754562378 \n",
      "     Training Step: 65 Training Loss: 0.6177250742912292 \n",
      "     Training Step: 66 Training Loss: 0.6134694218635559 \n",
      "     Training Step: 67 Training Loss: 0.6100516319274902 \n",
      "     Training Step: 68 Training Loss: 0.6125308275222778 \n",
      "     Training Step: 69 Training Loss: 0.611637532711029 \n",
      "     Training Step: 70 Training Loss: 0.6186001300811768 \n",
      "     Training Step: 71 Training Loss: 0.6184622645378113 \n",
      "     Training Step: 72 Training Loss: 0.6136492490768433 \n",
      "     Training Step: 73 Training Loss: 0.6155391335487366 \n",
      "     Training Step: 74 Training Loss: 0.6150885224342346 \n",
      "     Training Step: 75 Training Loss: 0.6177874207496643 \n",
      "     Training Step: 76 Training Loss: 0.6174795627593994 \n",
      "     Training Step: 77 Training Loss: 0.6115438342094421 \n",
      "     Training Step: 78 Training Loss: 0.6155899167060852 \n",
      "     Training Step: 79 Training Loss: 0.6152641177177429 \n",
      "     Training Step: 80 Training Loss: 0.6124114990234375 \n",
      "     Training Step: 81 Training Loss: 0.6146386861801147 \n",
      "     Training Step: 82 Training Loss: 0.6118226647377014 \n",
      "     Training Step: 83 Training Loss: 0.6156875491142273 \n",
      "     Training Step: 84 Training Loss: 0.6144246459007263 \n",
      "     Training Step: 85 Training Loss: 0.6135929822921753 \n",
      "     Training Step: 86 Training Loss: 0.6142815351486206 \n",
      "     Training Step: 87 Training Loss: 0.6180635690689087 \n",
      "     Training Step: 88 Training Loss: 0.6166625618934631 \n",
      "     Training Step: 89 Training Loss: 0.6137384176254272 \n",
      "     Training Step: 90 Training Loss: 0.6161112785339355 \n",
      "     Training Step: 91 Training Loss: 0.6142185926437378 \n",
      "     Training Step: 92 Training Loss: 0.6127662062644958 \n",
      "     Training Step: 93 Training Loss: 0.6147732734680176 \n",
      "     Training Step: 94 Training Loss: 0.6155163049697876 \n",
      "     Training Step: 95 Training Loss: 0.6161988973617554 \n",
      "     Training Step: 96 Training Loss: 0.6144516468048096 \n",
      "     Training Step: 97 Training Loss: 0.6104869246482849 \n",
      "     Training Step: 98 Training Loss: 0.617378294467926 \n",
      "     Training Step: 99 Training Loss: 0.6122405529022217 \n",
      "     Training Step: 100 Training Loss: 0.6183168292045593 \n",
      "     Training Step: 101 Training Loss: 0.615397036075592 \n",
      "     Training Step: 102 Training Loss: 0.6166700124740601 \n",
      "     Training Step: 103 Training Loss: 0.6141063570976257 \n",
      "     Training Step: 104 Training Loss: 0.6171324849128723 \n",
      "     Training Step: 105 Training Loss: 0.6160274147987366 \n",
      "     Training Step: 106 Training Loss: 0.6180075407028198 \n",
      "     Training Step: 107 Training Loss: 0.6107161045074463 \n",
      "     Training Step: 108 Training Loss: 0.6095015406608582 \n",
      "     Training Step: 109 Training Loss: 0.6146098971366882 \n",
      "     Training Step: 110 Training Loss: 0.6122945547103882 \n",
      "     Training Step: 111 Training Loss: 0.6133037209510803 \n",
      "     Training Step: 112 Training Loss: 0.6167998313903809 \n",
      "     Training Step: 113 Training Loss: 0.6111393570899963 \n",
      "     Training Step: 114 Training Loss: 0.614206850528717 \n",
      "     Training Step: 115 Training Loss: 0.6093890070915222 \n",
      "     Training Step: 116 Training Loss: 0.6153861284255981 \n",
      "     Training Step: 117 Training Loss: 0.6178381443023682 \n",
      "     Training Step: 118 Training Loss: 0.6136407256126404 \n",
      "     Training Step: 119 Training Loss: 0.611681342124939 \n",
      "     Training Step: 120 Training Loss: 0.6133515238761902 \n",
      "     Training Step: 121 Training Loss: 0.6186217069625854 \n",
      "     Training Step: 122 Training Loss: 0.6118786334991455 \n",
      "     Training Step: 123 Training Loss: 0.6118318438529968 \n",
      "     Training Step: 124 Training Loss: 0.6154480576515198 \n",
      "     Training Step: 125 Training Loss: 0.6154133677482605 \n",
      "     Training Step: 126 Training Loss: 0.6099876761436462 \n",
      "     Training Step: 127 Training Loss: 0.6159992218017578 \n",
      "     Training Step: 128 Training Loss: 0.6157528758049011 \n",
      "     Training Step: 129 Training Loss: 0.6188285946846008 \n",
      "     Training Step: 130 Training Loss: 0.6143437027931213 \n",
      "     Training Step: 131 Training Loss: 0.6166996359825134 \n",
      "     Training Step: 132 Training Loss: 0.6123983263969421 \n",
      "     Training Step: 133 Training Loss: 0.6134259104728699 \n",
      "     Training Step: 134 Training Loss: 0.6149170994758606 \n",
      "     Training Step: 135 Training Loss: 0.6180840730667114 \n",
      "     Training Step: 136 Training Loss: 0.6146660447120667 \n",
      "     Training Step: 137 Training Loss: 0.6183854341506958 \n",
      "     Training Step: 138 Training Loss: 0.6152922511100769 \n",
      "     Training Step: 139 Training Loss: 0.61332768201828 \n",
      "     Training Step: 140 Training Loss: 0.6120308041572571 \n",
      "     Training Step: 141 Training Loss: 0.614077091217041 \n",
      "     Training Step: 142 Training Loss: 0.6107457876205444 \n",
      "     Training Step: 143 Training Loss: 0.6129158139228821 \n",
      "     Training Step: 144 Training Loss: 0.6167044043540955 \n",
      "     Training Step: 145 Training Loss: 0.6115555167198181 \n",
      "     Training Step: 146 Training Loss: 0.6136165857315063 \n",
      "     Training Step: 147 Training Loss: 0.6145015954971313 \n",
      "     Training Step: 148 Training Loss: 0.6166269779205322 \n",
      "     Training Step: 149 Training Loss: 0.6140156984329224 \n",
      "     Training Step: 150 Training Loss: 0.6155394315719604 \n",
      "     Training Step: 151 Training Loss: 0.617108166217804 \n",
      "     Training Step: 152 Training Loss: 0.6127949357032776 \n",
      "     Training Step: 153 Training Loss: 0.612485408782959 \n",
      "     Training Step: 154 Training Loss: 0.6132818460464478 \n",
      "     Training Step: 155 Training Loss: 0.6126397252082825 \n",
      "     Training Step: 156 Training Loss: 0.6134952306747437 \n",
      "     Training Step: 157 Training Loss: 0.6147276163101196 \n",
      "     Training Step: 158 Training Loss: 0.6167601346969604 \n",
      "     Training Step: 159 Training Loss: 0.6141481399536133 \n",
      "     Training Step: 160 Training Loss: 0.615740954875946 \n",
      "     Training Step: 161 Training Loss: 0.6105057001113892 \n",
      "     Training Step: 162 Training Loss: 0.6167176961898804 \n",
      "     Training Step: 163 Training Loss: 0.618215799331665 \n",
      "     Training Step: 164 Training Loss: 0.6122381687164307 \n",
      "     Training Step: 165 Training Loss: 0.6138861775398254 \n",
      "     Training Step: 166 Training Loss: 0.6209144592285156 \n",
      "     Training Step: 167 Training Loss: 0.6148687601089478 \n",
      "     Training Step: 168 Training Loss: 0.6182219386100769 \n",
      "     Training Step: 169 Training Loss: 0.6100802421569824 \n",
      "     Training Step: 170 Training Loss: 0.6162252426147461 \n",
      "     Training Step: 171 Training Loss: 0.6141613721847534 \n",
      "     Training Step: 172 Training Loss: 0.6166328191757202 \n",
      "     Training Step: 173 Training Loss: 0.6104019284248352 \n",
      "     Training Step: 174 Training Loss: 0.610162615776062 \n",
      "     Training Step: 175 Training Loss: 0.6143342852592468 \n",
      "     Training Step: 176 Training Loss: 0.6138165593147278 \n",
      "     Training Step: 177 Training Loss: 0.6121212840080261 \n",
      "     Training Step: 178 Training Loss: 0.6121606230735779 \n",
      "     Training Step: 179 Training Loss: 0.6171790361404419 \n",
      "     Training Step: 180 Training Loss: 0.6168245673179626 \n",
      "     Training Step: 181 Training Loss: 0.6157594919204712 \n",
      "     Training Step: 182 Training Loss: 0.6194697022438049 \n",
      "     Training Step: 183 Training Loss: 0.6103955507278442 \n",
      "     Training Step: 184 Training Loss: 0.6127569675445557 \n",
      "     Training Step: 185 Training Loss: 0.6157840490341187 \n",
      "     Training Step: 186 Training Loss: 0.612231433391571 \n",
      "     Training Step: 187 Training Loss: 0.6176874041557312 \n",
      "     Training Step: 188 Training Loss: 0.6115957498550415 \n",
      "     Training Step: 189 Training Loss: 0.6130728125572205 \n",
      "     Training Step: 190 Training Loss: 0.6139201521873474 \n",
      "     Training Step: 191 Training Loss: 0.6097249984741211 \n",
      "     Training Step: 192 Training Loss: 0.6171378493309021 \n",
      "     Training Step: 193 Training Loss: 0.611788272857666 \n",
      "     Training Step: 194 Training Loss: 0.6146613359451294 \n",
      "     Training Step: 195 Training Loss: 0.6116044521331787 \n",
      "     Training Step: 196 Training Loss: 0.6116288900375366 \n",
      "     Training Step: 197 Training Loss: 0.6131823062896729 \n",
      "     Training Step: 198 Training Loss: 0.6162296533584595 \n",
      "     Training Step: 199 Training Loss: 0.6129583716392517 \n",
      "     Training Step: 200 Training Loss: 0.6117754578590393 \n",
      "     Training Step: 201 Training Loss: 0.6177953481674194 \n",
      "     Training Step: 202 Training Loss: 0.6146937012672424 \n",
      "     Training Step: 203 Training Loss: 0.6180660724639893 \n",
      "     Training Step: 204 Training Loss: 0.6129195094108582 \n",
      "     Training Step: 205 Training Loss: 0.6105868220329285 \n",
      "     Training Step: 206 Training Loss: 0.6154747605323792 \n",
      "     Training Step: 207 Training Loss: 0.6147498488426208 \n",
      "     Training Step: 208 Training Loss: 0.6108826994895935 \n",
      "     Training Step: 209 Training Loss: 0.6134596467018127 \n",
      "     Training Step: 210 Training Loss: 0.6196209788322449 \n",
      "     Training Step: 211 Training Loss: 0.6176874041557312 \n",
      "     Training Step: 212 Training Loss: 0.6150994300842285 \n",
      "     Training Step: 213 Training Loss: 0.6163459420204163 \n",
      "     Training Step: 214 Training Loss: 0.6149293780326843 \n",
      "     Training Step: 215 Training Loss: 0.6144441962242126 \n",
      "     Training Step: 216 Training Loss: 0.616708517074585 \n",
      "     Training Step: 217 Training Loss: 0.6168822050094604 \n",
      "     Training Step: 218 Training Loss: 0.6130741238594055 \n",
      "     Training Step: 219 Training Loss: 0.6114588975906372 \n",
      "     Training Step: 220 Training Loss: 0.6156672239303589 \n",
      "     Training Step: 221 Training Loss: 0.6152824759483337 \n",
      "     Training Step: 222 Training Loss: 0.6149584054946899 \n",
      "     Training Step: 223 Training Loss: 0.6171846389770508 \n",
      "     Training Step: 224 Training Loss: 0.6128712296485901 \n",
      "     Training Step: 225 Training Loss: 0.6196824908256531 \n",
      "     Training Step: 226 Training Loss: 0.611211359500885 \n",
      "     Training Step: 227 Training Loss: 0.6142463684082031 \n",
      "     Training Step: 228 Training Loss: 0.6114467978477478 \n",
      "     Training Step: 229 Training Loss: 0.6106697916984558 \n",
      "     Training Step: 230 Training Loss: 0.6121387481689453 \n",
      "     Training Step: 231 Training Loss: 0.6127027273178101 \n",
      "     Training Step: 232 Training Loss: 0.6115221381187439 \n",
      "     Training Step: 233 Training Loss: 0.6152483820915222 \n",
      "     Training Step: 234 Training Loss: 0.612372100353241 \n",
      "     Training Step: 235 Training Loss: 0.6125276684761047 \n",
      "     Training Step: 236 Training Loss: 0.6114581227302551 \n",
      "     Training Step: 237 Training Loss: 0.6132799983024597 \n",
      "     Training Step: 238 Training Loss: 0.6143998503684998 \n",
      "     Training Step: 239 Training Loss: 0.6132096648216248 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6104905605316162 \n",
      "     Validation Step: 1 Validation Loss: 0.6173453330993652 \n",
      "     Validation Step: 2 Validation Loss: 0.6106082201004028 \n",
      "     Validation Step: 3 Validation Loss: 0.6146440505981445 \n",
      "     Validation Step: 4 Validation Loss: 0.616267204284668 \n",
      "     Validation Step: 5 Validation Loss: 0.6153463125228882 \n",
      "     Validation Step: 6 Validation Loss: 0.6170677542686462 \n",
      "     Validation Step: 7 Validation Loss: 0.6185446381568909 \n",
      "     Validation Step: 8 Validation Loss: 0.6177366971969604 \n",
      "     Validation Step: 9 Validation Loss: 0.6111679673194885 \n",
      "     Validation Step: 10 Validation Loss: 0.6101272702217102 \n",
      "     Validation Step: 11 Validation Loss: 0.610099196434021 \n",
      "     Validation Step: 12 Validation Loss: 0.6185131072998047 \n",
      "     Validation Step: 13 Validation Loss: 0.6136717796325684 \n",
      "     Validation Step: 14 Validation Loss: 0.6118645668029785 \n",
      "     Validation Step: 15 Validation Loss: 0.6132979989051819 \n",
      "     Validation Step: 16 Validation Loss: 0.6104604005813599 \n",
      "     Validation Step: 17 Validation Loss: 0.6152474284172058 \n",
      "     Validation Step: 18 Validation Loss: 0.6149079203605652 \n",
      "     Validation Step: 19 Validation Loss: 0.6158149838447571 \n",
      "     Validation Step: 20 Validation Loss: 0.6145879030227661 \n",
      "     Validation Step: 21 Validation Loss: 0.6181074380874634 \n",
      "     Validation Step: 22 Validation Loss: 0.6156014204025269 \n",
      "     Validation Step: 23 Validation Loss: 0.6141176223754883 \n",
      "     Validation Step: 24 Validation Loss: 0.6145394444465637 \n",
      "     Validation Step: 25 Validation Loss: 0.6156400442123413 \n",
      "     Validation Step: 26 Validation Loss: 0.6183916330337524 \n",
      "     Validation Step: 27 Validation Loss: 0.6129776835441589 \n",
      "     Validation Step: 28 Validation Loss: 0.6141308546066284 \n",
      "     Validation Step: 29 Validation Loss: 0.6176270842552185 \n",
      "     Validation Step: 30 Validation Loss: 0.6100771427154541 \n",
      "     Validation Step: 31 Validation Loss: 0.6160306930541992 \n",
      "     Validation Step: 32 Validation Loss: 0.6115376353263855 \n",
      "     Validation Step: 33 Validation Loss: 0.6128126978874207 \n",
      "     Validation Step: 34 Validation Loss: 0.6074608564376831 \n",
      "     Validation Step: 35 Validation Loss: 0.6116241216659546 \n",
      "     Validation Step: 36 Validation Loss: 0.6121153235435486 \n",
      "     Validation Step: 37 Validation Loss: 0.6148433089256287 \n",
      "     Validation Step: 38 Validation Loss: 0.6136291027069092 \n",
      "     Validation Step: 39 Validation Loss: 0.6136718988418579 \n",
      "     Validation Step: 40 Validation Loss: 0.61506187915802 \n",
      "     Validation Step: 41 Validation Loss: 0.6182780861854553 \n",
      "     Validation Step: 42 Validation Loss: 0.6142570972442627 \n",
      "     Validation Step: 43 Validation Loss: 0.6142063736915588 \n",
      "     Validation Step: 44 Validation Loss: 0.6111474633216858 \n",
      "Epoch: 123\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6143574714660645 \n",
      "     Training Step: 1 Training Loss: 0.6155678033828735 \n",
      "     Training Step: 2 Training Loss: 0.6134751439094543 \n",
      "     Training Step: 3 Training Loss: 0.6111337542533875 \n",
      "     Training Step: 4 Training Loss: 0.6162175536155701 \n",
      "     Training Step: 5 Training Loss: 0.6160354614257812 \n",
      "     Training Step: 6 Training Loss: 0.6131888628005981 \n",
      "     Training Step: 7 Training Loss: 0.6092446446418762 \n",
      "     Training Step: 8 Training Loss: 0.6136397123336792 \n",
      "     Training Step: 9 Training Loss: 0.6146007180213928 \n",
      "     Training Step: 10 Training Loss: 0.6176872253417969 \n",
      "     Training Step: 11 Training Loss: 0.6146367192268372 \n",
      "     Training Step: 12 Training Loss: 0.6128732562065125 \n",
      "     Training Step: 13 Training Loss: 0.6156863570213318 \n",
      "     Training Step: 14 Training Loss: 0.6138185262680054 \n",
      "     Training Step: 15 Training Loss: 0.6135931015014648 \n",
      "     Training Step: 16 Training Loss: 0.6146644353866577 \n",
      "     Training Step: 17 Training Loss: 0.6123801469802856 \n",
      "     Training Step: 18 Training Loss: 0.611553966999054 \n",
      "     Training Step: 19 Training Loss: 0.6127994060516357 \n",
      "     Training Step: 20 Training Loss: 0.6157472729682922 \n",
      "     Training Step: 21 Training Loss: 0.6166812777519226 \n",
      "     Training Step: 22 Training Loss: 0.6167733669281006 \n",
      "     Training Step: 23 Training Loss: 0.6115171313285828 \n",
      "     Training Step: 24 Training Loss: 0.6137629747390747 \n",
      "     Training Step: 25 Training Loss: 0.6173844337463379 \n",
      "     Training Step: 26 Training Loss: 0.6164184808731079 \n",
      "     Training Step: 27 Training Loss: 0.6157848238945007 \n",
      "     Training Step: 28 Training Loss: 0.6176191568374634 \n",
      "     Training Step: 29 Training Loss: 0.615280270576477 \n",
      "     Training Step: 30 Training Loss: 0.6102198958396912 \n",
      "     Training Step: 31 Training Loss: 0.61468905210495 \n",
      "     Training Step: 32 Training Loss: 0.6150683760643005 \n",
      "     Training Step: 33 Training Loss: 0.616111695766449 \n",
      "     Training Step: 34 Training Loss: 0.6142531037330627 \n",
      "     Training Step: 35 Training Loss: 0.6107627153396606 \n",
      "     Training Step: 36 Training Loss: 0.6115492582321167 \n",
      "     Training Step: 37 Training Loss: 0.6133190989494324 \n",
      "     Training Step: 38 Training Loss: 0.6147701740264893 \n",
      "     Training Step: 39 Training Loss: 0.617714524269104 \n",
      "     Training Step: 40 Training Loss: 0.6156085133552551 \n",
      "     Training Step: 41 Training Loss: 0.6113936901092529 \n",
      "     Training Step: 42 Training Loss: 0.6198903322219849 \n",
      "     Training Step: 43 Training Loss: 0.6116876006126404 \n",
      "     Training Step: 44 Training Loss: 0.614341139793396 \n",
      "     Training Step: 45 Training Loss: 0.6106859445571899 \n",
      "     Training Step: 46 Training Loss: 0.6147459745407104 \n",
      "     Training Step: 47 Training Loss: 0.6146054267883301 \n",
      "     Training Step: 48 Training Loss: 0.6093980669975281 \n",
      "     Training Step: 49 Training Loss: 0.614424467086792 \n",
      "     Training Step: 50 Training Loss: 0.617820680141449 \n",
      "     Training Step: 51 Training Loss: 0.611880898475647 \n",
      "     Training Step: 52 Training Loss: 0.6162084341049194 \n",
      "     Training Step: 53 Training Loss: 0.6151611804962158 \n",
      "     Training Step: 54 Training Loss: 0.6114752888679504 \n",
      "     Training Step: 55 Training Loss: 0.6209398508071899 \n",
      "     Training Step: 56 Training Loss: 0.6122836470603943 \n",
      "     Training Step: 57 Training Loss: 0.6097338795661926 \n",
      "     Training Step: 58 Training Loss: 0.6167178750038147 \n",
      "     Training Step: 59 Training Loss: 0.6143431663513184 \n",
      "     Training Step: 60 Training Loss: 0.6127683520317078 \n",
      "     Training Step: 61 Training Loss: 0.6188638806343079 \n",
      "     Training Step: 62 Training Loss: 0.6101498007774353 \n",
      "     Training Step: 63 Training Loss: 0.6147234439849854 \n",
      "     Training Step: 64 Training Loss: 0.6120173931121826 \n",
      "     Training Step: 65 Training Loss: 0.6118285059928894 \n",
      "     Training Step: 66 Training Loss: 0.6105728149414062 \n",
      "     Training Step: 67 Training Loss: 0.6128676533699036 \n",
      "     Training Step: 68 Training Loss: 0.6116419434547424 \n",
      "     Training Step: 69 Training Loss: 0.6140835881233215 \n",
      "     Training Step: 70 Training Loss: 0.6100219488143921 \n",
      "     Training Step: 71 Training Loss: 0.6165213584899902 \n",
      "     Training Step: 72 Training Loss: 0.6155530214309692 \n",
      "     Training Step: 73 Training Loss: 0.6158477663993835 \n",
      "     Training Step: 74 Training Loss: 0.6097012162208557 \n",
      "     Training Step: 75 Training Loss: 0.6181305646896362 \n",
      "     Training Step: 76 Training Loss: 0.6182540655136108 \n",
      "     Training Step: 77 Training Loss: 0.6147968769073486 \n",
      "     Training Step: 78 Training Loss: 0.6140168905258179 \n",
      "     Training Step: 79 Training Loss: 0.6149165034294128 \n",
      "     Training Step: 80 Training Loss: 0.6163489818572998 \n",
      "     Training Step: 81 Training Loss: 0.6149533987045288 \n",
      "     Training Step: 82 Training Loss: 0.6182048320770264 \n",
      "     Training Step: 83 Training Loss: 0.6159932613372803 \n",
      "     Training Step: 84 Training Loss: 0.6144670844078064 \n",
      "     Training Step: 85 Training Loss: 0.6122599244117737 \n",
      "     Training Step: 86 Training Loss: 0.6180486679077148 \n",
      "     Training Step: 87 Training Loss: 0.6154225468635559 \n",
      "     Training Step: 88 Training Loss: 0.6167954802513123 \n",
      "     Training Step: 89 Training Loss: 0.612545907497406 \n",
      "     Training Step: 90 Training Loss: 0.6135337948799133 \n",
      "     Training Step: 91 Training Loss: 0.6154390573501587 \n",
      "     Training Step: 92 Training Loss: 0.6143695712089539 \n",
      "     Training Step: 93 Training Loss: 0.6112132668495178 \n",
      "     Training Step: 94 Training Loss: 0.6157531142234802 \n",
      "     Training Step: 95 Training Loss: 0.614868700504303 \n",
      "     Training Step: 96 Training Loss: 0.6121422052383423 \n",
      "     Training Step: 97 Training Loss: 0.6127009987831116 \n",
      "     Training Step: 98 Training Loss: 0.6116065382957458 \n",
      "     Training Step: 99 Training Loss: 0.6105640530586243 \n",
      "     Training Step: 100 Training Loss: 0.6139246225357056 \n",
      "     Training Step: 101 Training Loss: 0.6147009134292603 \n",
      "     Training Step: 102 Training Loss: 0.6139107942581177 \n",
      "     Training Step: 103 Training Loss: 0.6131134033203125 \n",
      "     Training Step: 104 Training Loss: 0.6135039925575256 \n",
      "     Training Step: 105 Training Loss: 0.6130767464637756 \n",
      "     Training Step: 106 Training Loss: 0.6152468919754028 \n",
      "     Training Step: 107 Training Loss: 0.6122252345085144 \n",
      "     Training Step: 108 Training Loss: 0.6122817993164062 \n",
      "     Training Step: 109 Training Loss: 0.6147118806838989 \n",
      "     Training Step: 110 Training Loss: 0.6172110438346863 \n",
      "     Training Step: 111 Training Loss: 0.6114035844802856 \n",
      "     Training Step: 112 Training Loss: 0.6177730560302734 \n",
      "     Training Step: 113 Training Loss: 0.6141070127487183 \n",
      "     Training Step: 114 Training Loss: 0.6122320890426636 \n",
      "     Training Step: 115 Training Loss: 0.6146538257598877 \n",
      "     Training Step: 116 Training Loss: 0.616706132888794 \n",
      "     Training Step: 117 Training Loss: 0.6118029356002808 \n",
      "     Training Step: 118 Training Loss: 0.6174803972244263 \n",
      "     Training Step: 119 Training Loss: 0.6094838380813599 \n",
      "     Training Step: 120 Training Loss: 0.6132827401161194 \n",
      "     Training Step: 121 Training Loss: 0.6180289387702942 \n",
      "     Training Step: 122 Training Loss: 0.6099957227706909 \n",
      "     Training Step: 123 Training Loss: 0.6100678443908691 \n",
      "     Training Step: 124 Training Loss: 0.6142081022262573 \n",
      "     Training Step: 125 Training Loss: 0.6124635338783264 \n",
      "     Training Step: 126 Training Loss: 0.6140348315238953 \n",
      "     Training Step: 127 Training Loss: 0.6125344038009644 \n",
      "     Training Step: 128 Training Loss: 0.6166917085647583 \n",
      "     Training Step: 129 Training Loss: 0.6171707510948181 \n",
      "     Training Step: 130 Training Loss: 0.6138946413993835 \n",
      "     Training Step: 131 Training Loss: 0.6106497645378113 \n",
      "     Training Step: 132 Training Loss: 0.6167345643043518 \n",
      "     Training Step: 133 Training Loss: 0.6132052540779114 \n",
      "     Training Step: 134 Training Loss: 0.6126397252082825 \n",
      "     Training Step: 135 Training Loss: 0.6176952719688416 \n",
      "     Training Step: 136 Training Loss: 0.610400378704071 \n",
      "     Training Step: 137 Training Loss: 0.6124871969223022 \n",
      "     Training Step: 138 Training Loss: 0.6114371418952942 \n",
      "     Training Step: 139 Training Loss: 0.6147002577781677 \n",
      "     Training Step: 140 Training Loss: 0.6166515946388245 \n",
      "     Training Step: 141 Training Loss: 0.610497236251831 \n",
      "     Training Step: 142 Training Loss: 0.6118043661117554 \n",
      "     Training Step: 143 Training Loss: 0.6137442588806152 \n",
      "     Training Step: 144 Training Loss: 0.6133521199226379 \n",
      "     Training Step: 145 Training Loss: 0.6167110800743103 \n",
      "     Training Step: 146 Training Loss: 0.6115806698799133 \n",
      "     Training Step: 147 Training Loss: 0.6131365895271301 \n",
      "     Training Step: 148 Training Loss: 0.6134089231491089 \n",
      "     Training Step: 149 Training Loss: 0.6121542453765869 \n",
      "     Training Step: 150 Training Loss: 0.6196449398994446 \n",
      "     Training Step: 151 Training Loss: 0.6108688712120056 \n",
      "     Training Step: 152 Training Loss: 0.6132710576057434 \n",
      "     Training Step: 153 Training Loss: 0.6115249991416931 \n",
      "     Training Step: 154 Training Loss: 0.6183459758758545 \n",
      "     Training Step: 155 Training Loss: 0.611832320690155 \n",
      "     Training Step: 156 Training Loss: 0.6111545562744141 \n",
      "     Training Step: 157 Training Loss: 0.6142131090164185 \n",
      "     Training Step: 158 Training Loss: 0.6121203899383545 \n",
      "     Training Step: 159 Training Loss: 0.6129176020622253 \n",
      "     Training Step: 160 Training Loss: 0.6141496896743774 \n",
      "     Training Step: 161 Training Loss: 0.6146818995475769 \n",
      "     Training Step: 162 Training Loss: 0.6164088845252991 \n",
      "     Training Step: 163 Training Loss: 0.6184495091438293 \n",
      "     Training Step: 164 Training Loss: 0.6129650473594666 \n",
      "     Training Step: 165 Training Loss: 0.6104888916015625 \n",
      "     Training Step: 166 Training Loss: 0.6194382905960083 \n",
      "     Training Step: 167 Training Loss: 0.6177874207496643 \n",
      "     Training Step: 168 Training Loss: 0.6128437519073486 \n",
      "     Training Step: 169 Training Loss: 0.6146069765090942 \n",
      "     Training Step: 170 Training Loss: 0.6116483211517334 \n",
      "     Training Step: 171 Training Loss: 0.6106151342391968 \n",
      "     Training Step: 172 Training Loss: 0.6153692603111267 \n",
      "     Training Step: 173 Training Loss: 0.6130644679069519 \n",
      "     Training Step: 174 Training Loss: 0.6100627779960632 \n",
      "     Training Step: 175 Training Loss: 0.6185868978500366 \n",
      "     Training Step: 176 Training Loss: 0.6154758334159851 \n",
      "     Training Step: 177 Training Loss: 0.611413300037384 \n",
      "     Training Step: 178 Training Loss: 0.6197161674499512 \n",
      "     Training Step: 179 Training Loss: 0.6157474517822266 \n",
      "     Training Step: 180 Training Loss: 0.6140215992927551 \n",
      "     Training Step: 181 Training Loss: 0.6136147975921631 \n",
      "     Training Step: 182 Training Loss: 0.6136485934257507 \n",
      "     Training Step: 183 Training Loss: 0.6125317215919495 \n",
      "     Training Step: 184 Training Loss: 0.6184067130088806 \n",
      "     Training Step: 185 Training Loss: 0.6121545433998108 \n",
      "     Training Step: 186 Training Loss: 0.6156758069992065 \n",
      "     Training Step: 187 Training Loss: 0.6188255548477173 \n",
      "     Training Step: 188 Training Loss: 0.6167959570884705 \n",
      "     Training Step: 189 Training Loss: 0.6171483993530273 \n",
      "     Training Step: 190 Training Loss: 0.6109317541122437 \n",
      "     Training Step: 191 Training Loss: 0.6162222623825073 \n",
      "     Training Step: 192 Training Loss: 0.613752543926239 \n",
      "     Training Step: 193 Training Loss: 0.6124024391174316 \n",
      "     Training Step: 194 Training Loss: 0.610414445400238 \n",
      "     Training Step: 195 Training Loss: 0.6106157898902893 \n",
      "     Training Step: 196 Training Loss: 0.6152904629707336 \n",
      "     Training Step: 197 Training Loss: 0.6120675802230835 \n",
      "     Training Step: 198 Training Loss: 0.6153830885887146 \n",
      "     Training Step: 199 Training Loss: 0.6142914891242981 \n",
      "     Training Step: 200 Training Loss: 0.6169493198394775 \n",
      "     Training Step: 201 Training Loss: 0.6159619688987732 \n",
      "     Training Step: 202 Training Loss: 0.611613392829895 \n",
      "     Training Step: 203 Training Loss: 0.6151801943778992 \n",
      "     Training Step: 204 Training Loss: 0.6096893548965454 \n",
      "     Training Step: 205 Training Loss: 0.6171566247940063 \n",
      "     Training Step: 206 Training Loss: 0.614440381526947 \n",
      "     Training Step: 207 Training Loss: 0.6150867342948914 \n",
      "     Training Step: 208 Training Loss: 0.6150977611541748 \n",
      "     Training Step: 209 Training Loss: 0.6082623600959778 \n",
      "     Training Step: 210 Training Loss: 0.6152633428573608 \n",
      "     Training Step: 211 Training Loss: 0.6155175566673279 \n",
      "     Training Step: 212 Training Loss: 0.612751841545105 \n",
      "     Training Step: 213 Training Loss: 0.6134589314460754 \n",
      "     Training Step: 214 Training Loss: 0.6114338636398315 \n",
      "     Training Step: 215 Training Loss: 0.6186043620109558 \n",
      "     Training Step: 216 Training Loss: 0.6133009791374207 \n",
      "     Training Step: 217 Training Loss: 0.6129118800163269 \n",
      "     Training Step: 218 Training Loss: 0.6123807430267334 \n",
      "     Training Step: 219 Training Loss: 0.6118285655975342 \n",
      "     Training Step: 220 Training Loss: 0.6166149377822876 \n",
      "     Training Step: 221 Training Loss: 0.6144789457321167 \n",
      "     Training Step: 222 Training Loss: 0.6180240511894226 \n",
      "     Training Step: 223 Training Loss: 0.6117817163467407 \n",
      "     Training Step: 224 Training Loss: 0.6149299144744873 \n",
      "     Training Step: 225 Training Loss: 0.6132900714874268 \n",
      "     Training Step: 226 Training Loss: 0.6166443228721619 \n",
      "     Training Step: 227 Training Loss: 0.6144983768463135 \n",
      "     Training Step: 228 Training Loss: 0.6141521334648132 \n",
      "     Training Step: 229 Training Loss: 0.6202073693275452 \n",
      "     Training Step: 230 Training Loss: 0.6168859601020813 \n",
      "     Training Step: 231 Training Loss: 0.6107439398765564 \n",
      "     Training Step: 232 Training Loss: 0.616795003414154 \n",
      "     Training Step: 233 Training Loss: 0.6184430122375488 \n",
      "     Training Step: 234 Training Loss: 0.6154001951217651 \n",
      "     Training Step: 235 Training Loss: 0.6153336763381958 \n",
      "     Training Step: 236 Training Loss: 0.6132161021232605 \n",
      "     Training Step: 237 Training Loss: 0.6152912378311157 \n",
      "     Training Step: 238 Training Loss: 0.6154058575630188 \n",
      "     Training Step: 239 Training Loss: 0.6170805096626282 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.610175609588623 \n",
      "     Validation Step: 1 Validation Loss: 0.614839494228363 \n",
      "     Validation Step: 2 Validation Loss: 0.6184679865837097 \n",
      "     Validation Step: 3 Validation Loss: 0.61454176902771 \n",
      "     Validation Step: 4 Validation Loss: 0.6170045137405396 \n",
      "     Validation Step: 5 Validation Loss: 0.6130039691925049 \n",
      "     Validation Step: 6 Validation Loss: 0.6141423583030701 \n",
      "     Validation Step: 7 Validation Loss: 0.6155679821968079 \n",
      "     Validation Step: 8 Validation Loss: 0.6128471493721008 \n",
      "     Validation Step: 9 Validation Loss: 0.6176769137382507 \n",
      "     Validation Step: 10 Validation Loss: 0.6105149388313293 \n",
      "     Validation Step: 11 Validation Loss: 0.614260733127594 \n",
      "     Validation Step: 12 Validation Loss: 0.614567756652832 \n",
      "     Validation Step: 13 Validation Loss: 0.615784227848053 \n",
      "     Validation Step: 14 Validation Loss: 0.6182056069374084 \n",
      "     Validation Step: 15 Validation Loss: 0.6101880073547363 \n",
      "     Validation Step: 16 Validation Loss: 0.617285966873169 \n",
      "     Validation Step: 17 Validation Loss: 0.6115812063217163 \n",
      "     Validation Step: 18 Validation Loss: 0.6159813404083252 \n",
      "     Validation Step: 19 Validation Loss: 0.6142084002494812 \n",
      "     Validation Step: 20 Validation Loss: 0.6150393486022949 \n",
      "     Validation Step: 21 Validation Loss: 0.613644540309906 \n",
      "     Validation Step: 22 Validation Loss: 0.6183211803436279 \n",
      "     Validation Step: 23 Validation Loss: 0.6121500730514526 \n",
      "     Validation Step: 24 Validation Loss: 0.6184462904930115 \n",
      "     Validation Step: 25 Validation Loss: 0.6133195757865906 \n",
      "     Validation Step: 26 Validation Loss: 0.6136795282363892 \n",
      "     Validation Step: 27 Validation Loss: 0.6101499199867249 \n",
      "     Validation Step: 28 Validation Loss: 0.615225076675415 \n",
      "     Validation Step: 29 Validation Loss: 0.6116637587547302 \n",
      "     Validation Step: 30 Validation Loss: 0.6075732707977295 \n",
      "     Validation Step: 31 Validation Loss: 0.614122211933136 \n",
      "     Validation Step: 32 Validation Loss: 0.6175822615623474 \n",
      "     Validation Step: 33 Validation Loss: 0.615318238735199 \n",
      "     Validation Step: 34 Validation Loss: 0.6105461120605469 \n",
      "     Validation Step: 35 Validation Loss: 0.6136582493782043 \n",
      "     Validation Step: 36 Validation Loss: 0.6162394881248474 \n",
      "     Validation Step: 37 Validation Loss: 0.611903190612793 \n",
      "     Validation Step: 38 Validation Loss: 0.6156103610992432 \n",
      "     Validation Step: 39 Validation Loss: 0.6180410385131836 \n",
      "     Validation Step: 40 Validation Loss: 0.6111893057823181 \n",
      "     Validation Step: 41 Validation Loss: 0.610656201839447 \n",
      "     Validation Step: 42 Validation Loss: 0.6112010478973389 \n",
      "     Validation Step: 43 Validation Loss: 0.614646315574646 \n",
      "     Validation Step: 44 Validation Loss: 0.6148945689201355 \n",
      "Epoch: 124\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6143435835838318 \n",
      "     Training Step: 1 Training Loss: 0.6100220084190369 \n",
      "     Training Step: 2 Training Loss: 0.6198400259017944 \n",
      "     Training Step: 3 Training Loss: 0.6121777892112732 \n",
      "     Training Step: 4 Training Loss: 0.6131505370140076 \n",
      "     Training Step: 5 Training Loss: 0.6130681037902832 \n",
      "     Training Step: 6 Training Loss: 0.6153348684310913 \n",
      "     Training Step: 7 Training Loss: 0.6155350804328918 \n",
      "     Training Step: 8 Training Loss: 0.6171578764915466 \n",
      "     Training Step: 9 Training Loss: 0.6116326451301575 \n",
      "     Training Step: 10 Training Loss: 0.6196961402893066 \n",
      "     Training Step: 11 Training Loss: 0.6115602850914001 \n",
      "     Training Step: 12 Training Loss: 0.6136475205421448 \n",
      "     Training Step: 13 Training Loss: 0.6100685000419617 \n",
      "     Training Step: 14 Training Loss: 0.6118273138999939 \n",
      "     Training Step: 15 Training Loss: 0.6122816801071167 \n",
      "     Training Step: 16 Training Loss: 0.612375020980835 \n",
      "     Training Step: 17 Training Loss: 0.6105821132659912 \n",
      "     Training Step: 18 Training Loss: 0.6116195321083069 \n",
      "     Training Step: 19 Training Loss: 0.6129188537597656 \n",
      "     Training Step: 20 Training Loss: 0.6158085465431213 \n",
      "     Training Step: 21 Training Loss: 0.613057553768158 \n",
      "     Training Step: 22 Training Loss: 0.610451877117157 \n",
      "     Training Step: 23 Training Loss: 0.6129621267318726 \n",
      "     Training Step: 24 Training Loss: 0.6115095615386963 \n",
      "     Training Step: 25 Training Loss: 0.615766167640686 \n",
      "     Training Step: 26 Training Loss: 0.6167500019073486 \n",
      "     Training Step: 27 Training Loss: 0.610379159450531 \n",
      "     Training Step: 28 Training Loss: 0.6171716451644897 \n",
      "     Training Step: 29 Training Loss: 0.6106480956077576 \n",
      "     Training Step: 30 Training Loss: 0.6153932809829712 \n",
      "     Training Step: 31 Training Loss: 0.6133111715316772 \n",
      "     Training Step: 32 Training Loss: 0.6146385073661804 \n",
      "     Training Step: 33 Training Loss: 0.609461784362793 \n",
      "     Training Step: 34 Training Loss: 0.6176906824111938 \n",
      "     Training Step: 35 Training Loss: 0.6162400245666504 \n",
      "     Training Step: 36 Training Loss: 0.6154409646987915 \n",
      "     Training Step: 37 Training Loss: 0.6162031888961792 \n",
      "     Training Step: 38 Training Loss: 0.6124717593193054 \n",
      "     Training Step: 39 Training Loss: 0.6138849854469299 \n",
      "     Training Step: 40 Training Loss: 0.612238883972168 \n",
      "     Training Step: 41 Training Loss: 0.614669919013977 \n",
      "     Training Step: 42 Training Loss: 0.6137407422065735 \n",
      "     Training Step: 43 Training Loss: 0.6209148168563843 \n",
      "     Training Step: 44 Training Loss: 0.615511417388916 \n",
      "     Training Step: 45 Training Loss: 0.6152461171150208 \n",
      "     Training Step: 46 Training Loss: 0.6164065599441528 \n",
      "     Training Step: 47 Training Loss: 0.6147769093513489 \n",
      "     Training Step: 48 Training Loss: 0.6118525266647339 \n",
      "     Training Step: 49 Training Loss: 0.6176865100860596 \n",
      "     Training Step: 50 Training Loss: 0.6153694987297058 \n",
      "     Training Step: 51 Training Loss: 0.6160302758216858 \n",
      "     Training Step: 52 Training Loss: 0.6169108152389526 \n",
      "     Training Step: 53 Training Loss: 0.6127173900604248 \n",
      "     Training Step: 54 Training Loss: 0.618385910987854 \n",
      "     Training Step: 55 Training Loss: 0.61362624168396 \n",
      "     Training Step: 56 Training Loss: 0.6147286891937256 \n",
      "     Training Step: 57 Training Loss: 0.6153934001922607 \n",
      "     Training Step: 58 Training Loss: 0.6120893955230713 \n",
      "     Training Step: 59 Training Loss: 0.6144974827766418 \n",
      "     Training Step: 60 Training Loss: 0.6118158102035522 \n",
      "     Training Step: 61 Training Loss: 0.613122284412384 \n",
      "     Training Step: 62 Training Loss: 0.6173862814903259 \n",
      "     Training Step: 63 Training Loss: 0.611417293548584 \n",
      "     Training Step: 64 Training Loss: 0.6141509413719177 \n",
      "     Training Step: 65 Training Loss: 0.6142112612724304 \n",
      "     Training Step: 66 Training Loss: 0.6166804432868958 \n",
      "     Training Step: 67 Training Loss: 0.6157606840133667 \n",
      "     Training Step: 68 Training Loss: 0.6144234538078308 \n",
      "     Training Step: 69 Training Loss: 0.6128689646720886 \n",
      "     Training Step: 70 Training Loss: 0.6171968579292297 \n",
      "     Training Step: 71 Training Loss: 0.6149337291717529 \n",
      "     Training Step: 72 Training Loss: 0.6161993145942688 \n",
      "     Training Step: 73 Training Loss: 0.6112037301063538 \n",
      "     Training Step: 74 Training Loss: 0.6176217198371887 \n",
      "     Training Step: 75 Training Loss: 0.6114482879638672 \n",
      "     Training Step: 76 Training Loss: 0.6150636076927185 \n",
      "     Training Step: 77 Training Loss: 0.6167110800743103 \n",
      "     Training Step: 78 Training Loss: 0.6116573810577393 \n",
      "     Training Step: 79 Training Loss: 0.6139262914657593 \n",
      "     Training Step: 80 Training Loss: 0.6114212274551392 \n",
      "     Training Step: 81 Training Loss: 0.6152833104133606 \n",
      "     Training Step: 82 Training Loss: 0.6106948256492615 \n",
      "     Training Step: 83 Training Loss: 0.6114777326583862 \n",
      "     Training Step: 84 Training Loss: 0.6101283431053162 \n",
      "     Training Step: 85 Training Loss: 0.6158398389816284 \n",
      "     Training Step: 86 Training Loss: 0.6122280955314636 \n",
      "     Training Step: 87 Training Loss: 0.6154407262802124 \n",
      "     Training Step: 88 Training Loss: 0.6117777228355408 \n",
      "     Training Step: 89 Training Loss: 0.6108778119087219 \n",
      "     Training Step: 90 Training Loss: 0.6135045289993286 \n",
      "     Training Step: 91 Training Loss: 0.6133120656013489 \n",
      "     Training Step: 92 Training Loss: 0.6167104840278625 \n",
      "     Training Step: 93 Training Loss: 0.6147213578224182 \n",
      "     Training Step: 94 Training Loss: 0.6194778084754944 \n",
      "     Training Step: 95 Training Loss: 0.609688401222229 \n",
      "     Training Step: 96 Training Loss: 0.6115802526473999 \n",
      "     Training Step: 97 Training Loss: 0.6147480010986328 \n",
      "     Training Step: 98 Training Loss: 0.6180368065834045 \n",
      "     Training Step: 99 Training Loss: 0.6135134696960449 \n",
      "     Training Step: 100 Training Loss: 0.611169159412384 \n",
      "     Training Step: 101 Training Loss: 0.6156820058822632 \n",
      "     Training Step: 102 Training Loss: 0.6166099309921265 \n",
      "     Training Step: 103 Training Loss: 0.6135959029197693 \n",
      "     Training Step: 104 Training Loss: 0.6142129302024841 \n",
      "     Training Step: 105 Training Loss: 0.6146803498268127 \n",
      "     Training Step: 106 Training Loss: 0.6092466115951538 \n",
      "     Training Step: 107 Training Loss: 0.6107312440872192 \n",
      "     Training Step: 108 Training Loss: 0.6140710711479187 \n",
      "     Training Step: 109 Training Loss: 0.6157463192939758 \n",
      "     Training Step: 110 Training Loss: 0.6180830597877502 \n",
      "     Training Step: 111 Training Loss: 0.6161237359046936 \n",
      "     Training Step: 112 Training Loss: 0.6118311882019043 \n",
      "     Training Step: 113 Training Loss: 0.6147058010101318 \n",
      "     Training Step: 114 Training Loss: 0.6137447953224182 \n",
      "     Training Step: 115 Training Loss: 0.611686110496521 \n",
      "     Training Step: 116 Training Loss: 0.6140202283859253 \n",
      "     Training Step: 117 Training Loss: 0.6144394278526306 \n",
      "     Training Step: 118 Training Loss: 0.6182394623756409 \n",
      "     Training Step: 119 Training Loss: 0.6167179942131042 \n",
      "     Training Step: 120 Training Loss: 0.6131879687309265 \n",
      "     Training Step: 121 Training Loss: 0.6167011857032776 \n",
      "     Training Step: 122 Training Loss: 0.6097456812858582 \n",
      "     Training Step: 123 Training Loss: 0.6155979633331299 \n",
      "     Training Step: 124 Training Loss: 0.6155298948287964 \n",
      "     Training Step: 125 Training Loss: 0.6146047711372375 \n",
      "     Training Step: 126 Training Loss: 0.617082417011261 \n",
      "     Training Step: 127 Training Loss: 0.6152916550636292 \n",
      "     Training Step: 128 Training Loss: 0.6100895404815674 \n",
      "     Training Step: 129 Training Loss: 0.6154747009277344 \n",
      "     Training Step: 130 Training Loss: 0.6105214953422546 \n",
      "     Training Step: 131 Training Loss: 0.6082851886749268 \n",
      "     Training Step: 132 Training Loss: 0.6115249991416931 \n",
      "     Training Step: 133 Training Loss: 0.6122311949729919 \n",
      "     Training Step: 134 Training Loss: 0.6166712641716003 \n",
      "     Training Step: 135 Training Loss: 0.6166806221008301 \n",
      "     Training Step: 136 Training Loss: 0.618359386920929 \n",
      "     Training Step: 137 Training Loss: 0.6171626448631287 \n",
      "     Training Step: 138 Training Loss: 0.6103594303131104 \n",
      "     Training Step: 139 Training Loss: 0.6127962470054626 \n",
      "     Training Step: 140 Training Loss: 0.6165046095848083 \n",
      "     Training Step: 141 Training Loss: 0.613285481929779 \n",
      "     Training Step: 142 Training Loss: 0.611885666847229 \n",
      "     Training Step: 143 Training Loss: 0.6125211715698242 \n",
      "     Training Step: 144 Training Loss: 0.6168057918548584 \n",
      "     Training Step: 145 Training Loss: 0.6127533912658691 \n",
      "     Training Step: 146 Training Loss: 0.6140167713165283 \n",
      "     Training Step: 147 Training Loss: 0.6108801364898682 \n",
      "     Training Step: 148 Training Loss: 0.6136404871940613 \n",
      "     Training Step: 149 Training Loss: 0.6163569688796997 \n",
      "     Training Step: 150 Training Loss: 0.6147927641868591 \n",
      "     Training Step: 151 Training Loss: 0.6196317076683044 \n",
      "     Training Step: 152 Training Loss: 0.6126426458358765 \n",
      "     Training Step: 153 Training Loss: 0.6134762167930603 \n",
      "     Training Step: 154 Training Loss: 0.610188901424408 \n",
      "     Training Step: 155 Training Loss: 0.6123780608177185 \n",
      "     Training Step: 156 Training Loss: 0.6116095781326294 \n",
      "     Training Step: 157 Training Loss: 0.618446409702301 \n",
      "     Training Step: 158 Training Loss: 0.6152639985084534 \n",
      "     Training Step: 159 Training Loss: 0.6121211051940918 \n",
      "     Training Step: 160 Training Loss: 0.6141042113304138 \n",
      "     Training Step: 161 Training Loss: 0.6178175210952759 \n",
      "     Training Step: 162 Training Loss: 0.6120147705078125 \n",
      "     Training Step: 163 Training Loss: 0.6168054342269897 \n",
      "     Training Step: 164 Training Loss: 0.6143329739570618 \n",
      "     Training Step: 165 Training Loss: 0.6154069304466248 \n",
      "     Training Step: 166 Training Loss: 0.6159451007843018 \n",
      "     Training Step: 167 Training Loss: 0.6177537441253662 \n",
      "     Training Step: 168 Training Loss: 0.6146048903465271 \n",
      "     Training Step: 169 Training Loss: 0.6142509579658508 \n",
      "     Training Step: 170 Training Loss: 0.614956796169281 \n",
      "     Training Step: 171 Training Loss: 0.6146051287651062 \n",
      "     Training Step: 172 Training Loss: 0.6106051206588745 \n",
      "     Training Step: 173 Training Loss: 0.6149176359176636 \n",
      "     Training Step: 174 Training Loss: 0.6138243675231934 \n",
      "     Training Step: 175 Training Loss: 0.6188220381736755 \n",
      "     Training Step: 176 Training Loss: 0.6180046200752258 \n",
      "     Training Step: 177 Training Loss: 0.6133635640144348 \n",
      "     Training Step: 178 Training Loss: 0.6115425229072571 \n",
      "     Training Step: 179 Training Loss: 0.6202018857002258 \n",
      "     Training Step: 180 Training Loss: 0.6127685904502869 \n",
      "     Training Step: 181 Training Loss: 0.6144784092903137 \n",
      "     Training Step: 182 Training Loss: 0.6114435195922852 \n",
      "     Training Step: 183 Training Loss: 0.6132028102874756 \n",
      "     Training Step: 184 Training Loss: 0.6146696209907532 \n",
      "     Training Step: 185 Training Loss: 0.6156812906265259 \n",
      "     Training Step: 186 Training Loss: 0.6132885813713074 \n",
      "     Training Step: 187 Training Loss: 0.6097198128700256 \n",
      "     Training Step: 188 Training Loss: 0.6144523620605469 \n",
      "     Training Step: 189 Training Loss: 0.6174997687339783 \n",
      "     Training Step: 190 Training Loss: 0.6107333302497864 \n",
      "     Training Step: 191 Training Loss: 0.6146508455276489 \n",
      "     Training Step: 192 Training Loss: 0.6178123950958252 \n",
      "     Training Step: 193 Training Loss: 0.6123767495155334 \n",
      "     Training Step: 194 Training Loss: 0.6134102940559387 \n",
      "     Training Step: 195 Training Loss: 0.6143752336502075 \n",
      "     Training Step: 196 Training Loss: 0.6093959212303162 \n",
      "     Training Step: 197 Training Loss: 0.6139064431190491 \n",
      "     Training Step: 198 Training Loss: 0.6125232577323914 \n",
      "     Training Step: 199 Training Loss: 0.6185934543609619 \n",
      "     Training Step: 200 Training Loss: 0.6134526133537292 \n",
      "     Training Step: 201 Training Loss: 0.6143452525138855 \n",
      "     Training Step: 202 Training Loss: 0.6152848601341248 \n",
      "     Training Step: 203 Training Loss: 0.6117851734161377 \n",
      "     Training Step: 204 Training Loss: 0.611139714717865 \n",
      "     Training Step: 205 Training Loss: 0.6146872043609619 \n",
      "     Training Step: 206 Training Loss: 0.6148691773414612 \n",
      "     Training Step: 207 Training Loss: 0.6188797950744629 \n",
      "     Training Step: 208 Training Loss: 0.6168988347053528 \n",
      "     Training Step: 209 Training Loss: 0.6125330328941345 \n",
      "     Training Step: 210 Training Loss: 0.6176857948303223 \n",
      "     Training Step: 211 Training Loss: 0.6100543737411499 \n",
      "     Training Step: 212 Training Loss: 0.6114053130149841 \n",
      "     Training Step: 213 Training Loss: 0.6185886859893799 \n",
      "     Training Step: 214 Training Loss: 0.6128431558609009 \n",
      "     Training Step: 215 Training Loss: 0.6150858998298645 \n",
      "     Training Step: 216 Training Loss: 0.6124935150146484 \n",
      "     Training Step: 217 Training Loss: 0.6167521476745605 \n",
      "     Training Step: 218 Training Loss: 0.6180969476699829 \n",
      "     Training Step: 219 Training Loss: 0.6142823696136475 \n",
      "     Training Step: 220 Training Loss: 0.6105836629867554 \n",
      "     Training Step: 221 Training Loss: 0.6151559948921204 \n",
      "     Training Step: 222 Training Loss: 0.6164023280143738 \n",
      "     Training Step: 223 Training Loss: 0.610601007938385 \n",
      "     Training Step: 224 Training Loss: 0.6121565699577332 \n",
      "     Training Step: 225 Training Loss: 0.6140345335006714 \n",
      "     Training Step: 226 Training Loss: 0.616802990436554 \n",
      "     Training Step: 227 Training Loss: 0.6182238459587097 \n",
      "     Training Step: 228 Training Loss: 0.6132752299308777 \n",
      "     Training Step: 229 Training Loss: 0.6141512393951416 \n",
      "     Training Step: 230 Training Loss: 0.6129144430160522 \n",
      "     Training Step: 231 Training Loss: 0.6151005625724792 \n",
      "     Training Step: 232 Training Loss: 0.6128669381141663 \n",
      "     Training Step: 233 Training Loss: 0.6122957468032837 \n",
      "     Training Step: 234 Training Loss: 0.6137621998786926 \n",
      "     Training Step: 235 Training Loss: 0.6160007119178772 \n",
      "     Training Step: 236 Training Loss: 0.6132051944732666 \n",
      "     Training Step: 237 Training Loss: 0.6151745915412903 \n",
      "     Training Step: 238 Training Loss: 0.6184647083282471 \n",
      "     Training Step: 239 Training Loss: 0.6121383905410767 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6141266822814941 \n",
      "     Validation Step: 1 Validation Loss: 0.6159836649894714 \n",
      "     Validation Step: 2 Validation Loss: 0.6173054575920105 \n",
      "     Validation Step: 3 Validation Loss: 0.6075227856636047 \n",
      "     Validation Step: 4 Validation Loss: 0.6142562031745911 \n",
      "     Validation Step: 5 Validation Loss: 0.6145712733268738 \n",
      "     Validation Step: 6 Validation Loss: 0.6146369576454163 \n",
      "     Validation Step: 7 Validation Loss: 0.6101088523864746 \n",
      "     Validation Step: 8 Validation Loss: 0.6145363450050354 \n",
      "     Validation Step: 9 Validation Loss: 0.6128321290016174 \n",
      "     Validation Step: 10 Validation Loss: 0.6136623024940491 \n",
      "     Validation Step: 11 Validation Loss: 0.6111779808998108 \n",
      "     Validation Step: 12 Validation Loss: 0.6136402487754822 \n",
      "     Validation Step: 13 Validation Loss: 0.6152260899543762 \n",
      "     Validation Step: 14 Validation Loss: 0.6176878809928894 \n",
      "     Validation Step: 15 Validation Loss: 0.6155678033828735 \n",
      "     Validation Step: 16 Validation Loss: 0.6175959706306458 \n",
      "     Validation Step: 17 Validation Loss: 0.615609884262085 \n",
      "     Validation Step: 18 Validation Loss: 0.6111648082733154 \n",
      "     Validation Step: 19 Validation Loss: 0.6133073568344116 \n",
      "     Validation Step: 20 Validation Loss: 0.6115576028823853 \n",
      "     Validation Step: 21 Validation Loss: 0.6170182824134827 \n",
      "     Validation Step: 22 Validation Loss: 0.618488609790802 \n",
      "     Validation Step: 23 Validation Loss: 0.6141160726547241 \n",
      "     Validation Step: 24 Validation Loss: 0.610482931137085 \n",
      "     Validation Step: 25 Validation Loss: 0.6183334589004517 \n",
      "     Validation Step: 26 Validation Loss: 0.6180576086044312 \n",
      "     Validation Step: 27 Validation Loss: 0.6182191371917725 \n",
      "     Validation Step: 28 Validation Loss: 0.6157864332199097 \n",
      "     Validation Step: 29 Validation Loss: 0.6142030358314514 \n",
      "     Validation Step: 30 Validation Loss: 0.6153149008750916 \n",
      "     Validation Step: 31 Validation Loss: 0.6136440634727478 \n",
      "     Validation Step: 32 Validation Loss: 0.6162389516830444 \n",
      "     Validation Step: 33 Validation Loss: 0.6148955821990967 \n",
      "     Validation Step: 34 Validation Loss: 0.6150318384170532 \n",
      "     Validation Step: 35 Validation Loss: 0.6184701919555664 \n",
      "     Validation Step: 36 Validation Loss: 0.6116431355476379 \n",
      "     Validation Step: 37 Validation Loss: 0.6118828058242798 \n",
      "     Validation Step: 38 Validation Loss: 0.6105168461799622 \n",
      "     Validation Step: 39 Validation Loss: 0.6148360967636108 \n",
      "     Validation Step: 40 Validation Loss: 0.6129851937294006 \n",
      "     Validation Step: 41 Validation Loss: 0.6106327772140503 \n",
      "     Validation Step: 42 Validation Loss: 0.6121305823326111 \n",
      "     Validation Step: 43 Validation Loss: 0.6101610064506531 \n",
      "     Validation Step: 44 Validation Loss: 0.6101362109184265 \n",
      "Epoch: 125\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6142815351486206 \n",
      "     Training Step: 1 Training Loss: 0.6132871508598328 \n",
      "     Training Step: 2 Training Loss: 0.6177572011947632 \n",
      "     Training Step: 3 Training Loss: 0.6124929189682007 \n",
      "     Training Step: 4 Training Loss: 0.6154727935791016 \n",
      "     Training Step: 5 Training Loss: 0.6116926670074463 \n",
      "     Training Step: 6 Training Loss: 0.6122868061065674 \n",
      "     Training Step: 7 Training Loss: 0.6129218339920044 \n",
      "     Training Step: 8 Training Loss: 0.6138855218887329 \n",
      "     Training Step: 9 Training Loss: 0.6153354644775391 \n",
      "     Training Step: 10 Training Loss: 0.6106864213943481 \n",
      "     Training Step: 11 Training Loss: 0.6100322008132935 \n",
      "     Training Step: 12 Training Loss: 0.6180537939071655 \n",
      "     Training Step: 13 Training Loss: 0.6136490702629089 \n",
      "     Training Step: 14 Training Loss: 0.6196507811546326 \n",
      "     Training Step: 15 Training Loss: 0.6114002466201782 \n",
      "     Training Step: 16 Training Loss: 0.61048823595047 \n",
      "     Training Step: 17 Training Loss: 0.618241548538208 \n",
      "     Training Step: 18 Training Loss: 0.6115174889564514 \n",
      "     Training Step: 19 Training Loss: 0.6104698777198792 \n",
      "     Training Step: 20 Training Loss: 0.6159526705741882 \n",
      "     Training Step: 21 Training Loss: 0.6143406629562378 \n",
      "     Training Step: 22 Training Loss: 0.6121538877487183 \n",
      "     Training Step: 23 Training Loss: 0.6122902631759644 \n",
      "     Training Step: 24 Training Loss: 0.6166759133338928 \n",
      "     Training Step: 25 Training Loss: 0.6134694218635559 \n",
      "     Training Step: 26 Training Loss: 0.6135894656181335 \n",
      "     Training Step: 27 Training Loss: 0.6147461533546448 \n",
      "     Training Step: 28 Training Loss: 0.6116296052932739 \n",
      "     Training Step: 29 Training Loss: 0.6134187579154968 \n",
      "     Training Step: 30 Training Loss: 0.6151576042175293 \n",
      "     Training Step: 31 Training Loss: 0.6153774857521057 \n",
      "     Training Step: 32 Training Loss: 0.6144391894340515 \n",
      "     Training Step: 33 Training Loss: 0.6132897734642029 \n",
      "     Training Step: 34 Training Loss: 0.6136176586151123 \n",
      "     Training Step: 35 Training Loss: 0.6147720217704773 \n",
      "     Training Step: 36 Training Loss: 0.6146382689476013 \n",
      "     Training Step: 37 Training Loss: 0.6128665208816528 \n",
      "     Training Step: 38 Training Loss: 0.612374484539032 \n",
      "     Training Step: 39 Training Loss: 0.6186037063598633 \n",
      "     Training Step: 40 Training Loss: 0.6142111420631409 \n",
      "     Training Step: 41 Training Loss: 0.6131173968315125 \n",
      "     Training Step: 42 Training Loss: 0.6105989217758179 \n",
      "     Training Step: 43 Training Loss: 0.6094581484794617 \n",
      "     Training Step: 44 Training Loss: 0.6140218377113342 \n",
      "     Training Step: 45 Training Loss: 0.6138138771057129 \n",
      "     Training Step: 46 Training Loss: 0.6151783466339111 \n",
      "     Training Step: 47 Training Loss: 0.6146133542060852 \n",
      "     Training Step: 48 Training Loss: 0.6150748133659363 \n",
      "     Training Step: 49 Training Loss: 0.6165094971656799 \n",
      "     Training Step: 50 Training Loss: 0.608243465423584 \n",
      "     Training Step: 51 Training Loss: 0.6161214113235474 \n",
      "     Training Step: 52 Training Loss: 0.6115202903747559 \n",
      "     Training Step: 53 Training Loss: 0.6150878667831421 \n",
      "     Training Step: 54 Training Loss: 0.6105640530586243 \n",
      "     Training Step: 55 Training Loss: 0.6176955699920654 \n",
      "     Training Step: 56 Training Loss: 0.615756094455719 \n",
      "     Training Step: 57 Training Loss: 0.6152681708335876 \n",
      "     Training Step: 58 Training Loss: 0.6115267276763916 \n",
      "     Training Step: 59 Training Loss: 0.6140322089195251 \n",
      "     Training Step: 60 Training Loss: 0.6105697751045227 \n",
      "     Training Step: 61 Training Loss: 0.6133551001548767 \n",
      "     Training Step: 62 Training Loss: 0.6134965419769287 \n",
      "     Training Step: 63 Training Loss: 0.611392080783844 \n",
      "     Training Step: 64 Training Loss: 0.6166614890098572 \n",
      "     Training Step: 65 Training Loss: 0.6166219711303711 \n",
      "     Training Step: 66 Training Loss: 0.6103692054748535 \n",
      "     Training Step: 67 Training Loss: 0.614014744758606 \n",
      "     Training Step: 68 Training Loss: 0.6168080568313599 \n",
      "     Training Step: 69 Training Loss: 0.6160418391227722 \n",
      "     Training Step: 70 Training Loss: 0.6128389239311218 \n",
      "     Training Step: 71 Training Loss: 0.6146743297576904 \n",
      "     Training Step: 72 Training Loss: 0.6125380396842957 \n",
      "     Training Step: 73 Training Loss: 0.618400514125824 \n",
      "     Training Step: 74 Training Loss: 0.6115649342536926 \n",
      "     Training Step: 75 Training Loss: 0.6167104840278625 \n",
      "     Training Step: 76 Training Loss: 0.6137643456459045 \n",
      "     Training Step: 77 Training Loss: 0.6167979836463928 \n",
      "     Training Step: 78 Training Loss: 0.6107479929924011 \n",
      "     Training Step: 79 Training Loss: 0.6174783706665039 \n",
      "     Training Step: 80 Training Loss: 0.6100037097930908 \n",
      "     Training Step: 81 Training Loss: 0.6112103462219238 \n",
      "     Training Step: 82 Training Loss: 0.6116490960121155 \n",
      "     Training Step: 83 Training Loss: 0.6116092205047607 \n",
      "     Training Step: 84 Training Loss: 0.6122317910194397 \n",
      "     Training Step: 85 Training Loss: 0.6136394143104553 \n",
      "     Training Step: 86 Training Loss: 0.6114082932472229 \n",
      "     Training Step: 87 Training Loss: 0.6118038892745972 \n",
      "     Training Step: 88 Training Loss: 0.6199352145195007 \n",
      "     Training Step: 89 Training Loss: 0.6143918633460999 \n",
      "     Training Step: 90 Training Loss: 0.6118166446685791 \n",
      "     Training Step: 91 Training Loss: 0.6152473092079163 \n",
      "     Training Step: 92 Training Loss: 0.6118709444999695 \n",
      "     Training Step: 93 Training Loss: 0.6147315502166748 \n",
      "     Training Step: 94 Training Loss: 0.6144242882728577 \n",
      "     Training Step: 95 Training Loss: 0.6181133389472961 \n",
      "     Training Step: 96 Training Loss: 0.6131378412246704 \n",
      "     Training Step: 97 Training Loss: 0.6101824641227722 \n",
      "     Training Step: 98 Training Loss: 0.6155356168746948 \n",
      "     Training Step: 99 Training Loss: 0.6156795024871826 \n",
      "     Training Step: 100 Training Loss: 0.6108913421630859 \n",
      "     Training Step: 101 Training Loss: 0.6142452359199524 \n",
      "     Training Step: 102 Training Loss: 0.6149535775184631 \n",
      "     Training Step: 103 Training Loss: 0.6137442588806152 \n",
      "     Training Step: 104 Training Loss: 0.6157805919647217 \n",
      "     Training Step: 105 Training Loss: 0.6120745539665222 \n",
      "     Training Step: 106 Training Loss: 0.6167557239532471 \n",
      "     Training Step: 107 Training Loss: 0.6122326254844666 \n",
      "     Training Step: 108 Training Loss: 0.6180179119110107 \n",
      "     Training Step: 109 Training Loss: 0.6152831315994263 \n",
      "     Training Step: 110 Training Loss: 0.6168917417526245 \n",
      "     Training Step: 111 Training Loss: 0.6176842451095581 \n",
      "     Training Step: 112 Training Loss: 0.6147039532661438 \n",
      "     Training Step: 113 Training Loss: 0.6146076917648315 \n",
      "     Training Step: 114 Training Loss: 0.6209065914154053 \n",
      "     Training Step: 115 Training Loss: 0.6148694753646851 \n",
      "     Training Step: 116 Training Loss: 0.6122451424598694 \n",
      "     Training Step: 117 Training Loss: 0.6120437383651733 \n",
      "     Training Step: 118 Training Loss: 0.6118557453155518 \n",
      "     Training Step: 119 Training Loss: 0.6140739917755127 \n",
      "     Training Step: 120 Training Loss: 0.6139330267906189 \n",
      "     Training Step: 121 Training Loss: 0.6130713820457458 \n",
      "     Training Step: 122 Training Loss: 0.6164016127586365 \n",
      "     Training Step: 123 Training Loss: 0.612165629863739 \n",
      "     Training Step: 124 Training Loss: 0.6114357709884644 \n",
      "     Training Step: 125 Training Loss: 0.6117889881134033 \n",
      "     Training Step: 126 Training Loss: 0.6153994798660278 \n",
      "     Training Step: 127 Training Loss: 0.6189005970954895 \n",
      "     Training Step: 128 Training Loss: 0.6105625629425049 \n",
      "     Training Step: 129 Training Loss: 0.6092090606689453 \n",
      "     Training Step: 130 Training Loss: 0.6127675771713257 \n",
      "     Training Step: 131 Training Loss: 0.6194935441017151 \n",
      "     Training Step: 132 Training Loss: 0.6172258257865906 \n",
      "     Training Step: 133 Training Loss: 0.613206684589386 \n",
      "     Training Step: 134 Training Loss: 0.6164441108703613 \n",
      "     Training Step: 135 Training Loss: 0.6123759150505066 \n",
      "     Training Step: 136 Training Loss: 0.6184511184692383 \n",
      "     Training Step: 137 Training Loss: 0.6154425740242004 \n",
      "     Training Step: 138 Training Loss: 0.6155976057052612 \n",
      "     Training Step: 139 Training Loss: 0.6146141886711121 \n",
      "     Training Step: 140 Training Loss: 0.6134772300720215 \n",
      "     Training Step: 141 Training Loss: 0.6109262704849243 \n",
      "     Training Step: 142 Training Loss: 0.6118136048316956 \n",
      "     Training Step: 143 Training Loss: 0.6141549944877625 \n",
      "     Training Step: 144 Training Loss: 0.6139246225357056 \n",
      "     Training Step: 145 Training Loss: 0.6125407218933105 \n",
      "     Training Step: 146 Training Loss: 0.6143394112586975 \n",
      "     Training Step: 147 Training Loss: 0.6093987822532654 \n",
      "     Training Step: 148 Training Loss: 0.6144511103630066 \n",
      "     Training Step: 149 Training Loss: 0.6143501400947571 \n",
      "     Training Step: 150 Training Loss: 0.6157586574554443 \n",
      "     Training Step: 151 Training Loss: 0.6153026819229126 \n",
      "     Training Step: 152 Training Loss: 0.6174165606498718 \n",
      "     Training Step: 153 Training Loss: 0.6121191382408142 \n",
      "     Training Step: 154 Training Loss: 0.6130550503730774 \n",
      "     Training Step: 155 Training Loss: 0.6147162318229675 \n",
      "     Training Step: 156 Training Loss: 0.6144838333129883 \n",
      "     Training Step: 157 Training Loss: 0.6121388673782349 \n",
      "     Training Step: 158 Training Loss: 0.6202272176742554 \n",
      "     Training Step: 159 Training Loss: 0.6146641969680786 \n",
      "     Training Step: 160 Training Loss: 0.6129695773124695 \n",
      "     Training Step: 161 Training Loss: 0.6166678667068481 \n",
      "     Training Step: 162 Training Loss: 0.6161990165710449 \n",
      "     Training Step: 163 Training Loss: 0.6097450852394104 \n",
      "     Training Step: 164 Training Loss: 0.6166406273841858 \n",
      "     Training Step: 165 Training Loss: 0.6146548986434937 \n",
      "     Training Step: 166 Training Loss: 0.6196717619895935 \n",
      "     Training Step: 167 Training Loss: 0.6133254170417786 \n",
      "     Training Step: 168 Training Loss: 0.6116410493850708 \n",
      "     Training Step: 169 Training Loss: 0.612883448600769 \n",
      "     Training Step: 170 Training Loss: 0.6111560463905334 \n",
      "     Training Step: 171 Training Loss: 0.6127578020095825 \n",
      "     Training Step: 172 Training Loss: 0.6144987940788269 \n",
      "     Training Step: 173 Training Loss: 0.6160056591033936 \n",
      "     Training Step: 174 Training Loss: 0.6180843710899353 \n",
      "     Training Step: 175 Training Loss: 0.6162069439888 \n",
      "     Training Step: 176 Training Loss: 0.6171024441719055 \n",
      "     Training Step: 177 Training Loss: 0.6147921681404114 \n",
      "     Training Step: 178 Training Loss: 0.6177904009819031 \n",
      "     Training Step: 179 Training Loss: 0.6126453280448914 \n",
      "     Training Step: 180 Training Loss: 0.6127070784568787 \n",
      "     Training Step: 181 Training Loss: 0.6185632348060608 \n",
      "     Training Step: 182 Training Loss: 0.6153699159622192 \n",
      "     Training Step: 183 Training Loss: 0.6131983399391174 \n",
      "     Training Step: 184 Training Loss: 0.6111828684806824 \n",
      "     Training Step: 185 Training Loss: 0.6132121086120605 \n",
      "     Training Step: 186 Training Loss: 0.6142155528068542 \n",
      "     Training Step: 187 Training Loss: 0.6146616339683533 \n",
      "     Training Step: 188 Training Loss: 0.6100660562515259 \n",
      "     Training Step: 189 Training Loss: 0.6155195236206055 \n",
      "     Training Step: 190 Training Loss: 0.6100471615791321 \n",
      "     Training Step: 191 Training Loss: 0.6106517314910889 \n",
      "     Training Step: 192 Training Loss: 0.6097006797790527 \n",
      "     Training Step: 193 Training Loss: 0.6167513728141785 \n",
      "     Training Step: 194 Training Loss: 0.6171869039535522 \n",
      "     Training Step: 195 Training Loss: 0.6176957488059998 \n",
      "     Training Step: 196 Training Loss: 0.6185050010681152 \n",
      "     Training Step: 197 Training Loss: 0.6141549348831177 \n",
      "     Training Step: 198 Training Loss: 0.6125144958496094 \n",
      "     Training Step: 199 Training Loss: 0.6167156100273132 \n",
      "     Training Step: 200 Training Loss: 0.6146900653839111 \n",
      "     Training Step: 201 Training Loss: 0.614932656288147 \n",
      "     Training Step: 202 Training Loss: 0.6135213375091553 \n",
      "     Training Step: 203 Training Loss: 0.6107484102249146 \n",
      "     Training Step: 204 Training Loss: 0.6171501278877258 \n",
      "     Training Step: 205 Training Loss: 0.6169133186340332 \n",
      "     Training Step: 206 Training Loss: 0.6101629137992859 \n",
      "     Training Step: 207 Training Loss: 0.6154199838638306 \n",
      "     Training Step: 208 Training Loss: 0.6155322194099426 \n",
      "     Training Step: 209 Training Loss: 0.613306999206543 \n",
      "     Training Step: 210 Training Loss: 0.6182100176811218 \n",
      "     Training Step: 211 Training Loss: 0.6177994608879089 \n",
      "     Training Step: 212 Training Loss: 0.611602246761322 \n",
      "     Training Step: 213 Training Loss: 0.6118377447128296 \n",
      "     Training Step: 214 Training Loss: 0.6149154901504517 \n",
      "     Training Step: 215 Training Loss: 0.6154049038887024 \n",
      "     Training Step: 216 Training Loss: 0.6156834959983826 \n",
      "     Training Step: 217 Training Loss: 0.6150934100151062 \n",
      "     Training Step: 218 Training Loss: 0.6137483716011047 \n",
      "     Training Step: 219 Training Loss: 0.6141043305397034 \n",
      "     Training Step: 220 Training Loss: 0.6124624609947205 \n",
      "     Training Step: 221 Training Loss: 0.6123769879341125 \n",
      "     Training Step: 222 Training Loss: 0.6097269058227539 \n",
      "     Training Step: 223 Training Loss: 0.6103964447975159 \n",
      "     Training Step: 224 Training Loss: 0.6163572072982788 \n",
      "     Training Step: 225 Training Loss: 0.6177260875701904 \n",
      "     Training Step: 226 Training Loss: 0.6114272475242615 \n",
      "     Training Step: 227 Training Loss: 0.6162444949150085 \n",
      "     Training Step: 228 Training Loss: 0.6167089939117432 \n",
      "     Training Step: 229 Training Loss: 0.6183218955993652 \n",
      "     Training Step: 230 Training Loss: 0.6168068051338196 \n",
      "     Training Step: 231 Training Loss: 0.6171389818191528 \n",
      "     Training Step: 232 Training Loss: 0.6158257722854614 \n",
      "     Training Step: 233 Training Loss: 0.6133050322532654 \n",
      "     Training Step: 234 Training Loss: 0.6187965869903564 \n",
      "     Training Step: 235 Training Loss: 0.6129255294799805 \n",
      "     Training Step: 236 Training Loss: 0.6152918338775635 \n",
      "     Training Step: 237 Training Loss: 0.6115416884422302 \n",
      "     Training Step: 238 Training Loss: 0.6128325462341309 \n",
      "     Training Step: 239 Training Loss: 0.6157462000846863 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6101811528205872 \n",
      "     Validation Step: 1 Validation Loss: 0.6116730570793152 \n",
      "     Validation Step: 2 Validation Loss: 0.6106663346290588 \n",
      "     Validation Step: 3 Validation Loss: 0.613332211971283 \n",
      "     Validation Step: 4 Validation Loss: 0.6136800050735474 \n",
      "     Validation Step: 5 Validation Loss: 0.6153175234794617 \n",
      "     Validation Step: 6 Validation Loss: 0.6105560064315796 \n",
      "     Validation Step: 7 Validation Loss: 0.6172884106636047 \n",
      "     Validation Step: 8 Validation Loss: 0.6142188310623169 \n",
      "     Validation Step: 9 Validation Loss: 0.6149044036865234 \n",
      "     Validation Step: 10 Validation Loss: 0.616236686706543 \n",
      "     Validation Step: 11 Validation Loss: 0.6141358017921448 \n",
      "     Validation Step: 12 Validation Loss: 0.6112086176872253 \n",
      "     Validation Step: 13 Validation Loss: 0.6183168292045593 \n",
      "     Validation Step: 14 Validation Loss: 0.6111969947814941 \n",
      "     Validation Step: 15 Validation Loss: 0.6182008385658264 \n",
      "     Validation Step: 16 Validation Loss: 0.6150370240211487 \n",
      "     Validation Step: 17 Validation Loss: 0.6105237603187561 \n",
      "     Validation Step: 18 Validation Loss: 0.6155634522438049 \n",
      "     Validation Step: 19 Validation Loss: 0.6145702600479126 \n",
      "     Validation Step: 20 Validation Loss: 0.6130136251449585 \n",
      "     Validation Step: 21 Validation Loss: 0.6121599078178406 \n",
      "     Validation Step: 22 Validation Loss: 0.6119166016578674 \n",
      "     Validation Step: 23 Validation Loss: 0.6152262091636658 \n",
      "     Validation Step: 24 Validation Loss: 0.6184530258178711 \n",
      "     Validation Step: 25 Validation Loss: 0.615975558757782 \n",
      "     Validation Step: 26 Validation Loss: 0.6102022528648376 \n",
      "     Validation Step: 27 Validation Loss: 0.6075904965400696 \n",
      "     Validation Step: 28 Validation Loss: 0.615608274936676 \n",
      "     Validation Step: 29 Validation Loss: 0.614553689956665 \n",
      "     Validation Step: 30 Validation Loss: 0.6141484975814819 \n",
      "     Validation Step: 31 Validation Loss: 0.6157873272895813 \n",
      "     Validation Step: 32 Validation Loss: 0.611589789390564 \n",
      "     Validation Step: 33 Validation Loss: 0.6101564168930054 \n",
      "     Validation Step: 34 Validation Loss: 0.6136611104011536 \n",
      "     Validation Step: 35 Validation Loss: 0.6170024275779724 \n",
      "     Validation Step: 36 Validation Loss: 0.6148484945297241 \n",
      "     Validation Step: 37 Validation Loss: 0.6184657216072083 \n",
      "     Validation Step: 38 Validation Loss: 0.6136525273323059 \n",
      "     Validation Step: 39 Validation Loss: 0.6146532893180847 \n",
      "     Validation Step: 40 Validation Loss: 0.6128632426261902 \n",
      "     Validation Step: 41 Validation Loss: 0.6142743229866028 \n",
      "     Validation Step: 42 Validation Loss: 0.6176714301109314 \n",
      "     Validation Step: 43 Validation Loss: 0.6175881624221802 \n",
      "     Validation Step: 44 Validation Loss: 0.6180398464202881 \n",
      "Epoch: 126\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6116472482681274 \n",
      "     Training Step: 1 Training Loss: 0.6153417825698853 \n",
      "     Training Step: 2 Training Loss: 0.6097465753555298 \n",
      "     Training Step: 3 Training Loss: 0.6126466989517212 \n",
      "     Training Step: 4 Training Loss: 0.6182273626327515 \n",
      "     Training Step: 5 Training Loss: 0.6103708744049072 \n",
      "     Training Step: 6 Training Loss: 0.6177033185958862 \n",
      "     Training Step: 7 Training Loss: 0.6132912635803223 \n",
      "     Training Step: 8 Training Loss: 0.6117730736732483 \n",
      "     Training Step: 9 Training Loss: 0.6143553256988525 \n",
      "     Training Step: 10 Training Loss: 0.6194830536842346 \n",
      "     Training Step: 11 Training Loss: 0.61016845703125 \n",
      "     Training Step: 12 Training Loss: 0.6140179634094238 \n",
      "     Training Step: 13 Training Loss: 0.6133021712303162 \n",
      "     Training Step: 14 Training Loss: 0.6145015358924866 \n",
      "     Training Step: 15 Training Loss: 0.6137635111808777 \n",
      "     Training Step: 16 Training Loss: 0.6125321388244629 \n",
      "     Training Step: 17 Training Loss: 0.6196256875991821 \n",
      "     Training Step: 18 Training Loss: 0.6122294068336487 \n",
      "     Training Step: 19 Training Loss: 0.6118079423904419 \n",
      "     Training Step: 20 Training Loss: 0.6157812476158142 \n",
      "     Training Step: 21 Training Loss: 0.6155282258987427 \n",
      "     Training Step: 22 Training Loss: 0.6180059313774109 \n",
      "     Training Step: 23 Training Loss: 0.6140744090080261 \n",
      "     Training Step: 24 Training Loss: 0.618382453918457 \n",
      "     Training Step: 25 Training Loss: 0.6164028644561768 \n",
      "     Training Step: 26 Training Loss: 0.617141842842102 \n",
      "     Training Step: 27 Training Loss: 0.6128911972045898 \n",
      "     Training Step: 28 Training Loss: 0.6187942028045654 \n",
      "     Training Step: 29 Training Loss: 0.6121760010719299 \n",
      "     Training Step: 30 Training Loss: 0.6139571666717529 \n",
      "     Training Step: 31 Training Loss: 0.6141214370727539 \n",
      "     Training Step: 32 Training Loss: 0.6156603693962097 \n",
      "     Training Step: 33 Training Loss: 0.6133663058280945 \n",
      "     Training Step: 34 Training Loss: 0.6142019629478455 \n",
      "     Training Step: 35 Training Loss: 0.6097498536109924 \n",
      "     Training Step: 36 Training Loss: 0.6105754971504211 \n",
      "     Training Step: 37 Training Loss: 0.6165016889572144 \n",
      "     Training Step: 38 Training Loss: 0.6127065420150757 \n",
      "     Training Step: 39 Training Loss: 0.6127952337265015 \n",
      "     Training Step: 40 Training Loss: 0.6151871085166931 \n",
      "     Training Step: 41 Training Loss: 0.6136203408241272 \n",
      "     Training Step: 42 Training Loss: 0.6118337512016296 \n",
      "     Training Step: 43 Training Loss: 0.6181105375289917 \n",
      "     Training Step: 44 Training Loss: 0.609370231628418 \n",
      "     Training Step: 45 Training Loss: 0.6130541563034058 \n",
      "     Training Step: 46 Training Loss: 0.6131333112716675 \n",
      "     Training Step: 47 Training Loss: 0.614393413066864 \n",
      "     Training Step: 48 Training Loss: 0.616698145866394 \n",
      "     Training Step: 49 Training Loss: 0.6152730584144592 \n",
      "     Training Step: 50 Training Loss: 0.6162083745002747 \n",
      "     Training Step: 51 Training Loss: 0.6134725213050842 \n",
      "     Training Step: 52 Training Loss: 0.6101433038711548 \n",
      "     Training Step: 53 Training Loss: 0.6116921305656433 \n",
      "     Training Step: 54 Training Loss: 0.6120734810829163 \n",
      "     Training Step: 55 Training Loss: 0.6140251755714417 \n",
      "     Training Step: 56 Training Loss: 0.6147301197052002 \n",
      "     Training Step: 57 Training Loss: 0.6144232153892517 \n",
      "     Training Step: 58 Training Loss: 0.6168098449707031 \n",
      "     Training Step: 59 Training Loss: 0.6171397566795349 \n",
      "     Training Step: 60 Training Loss: 0.6109031438827515 \n",
      "     Training Step: 61 Training Loss: 0.6167157292366028 \n",
      "     Training Step: 62 Training Loss: 0.6106684803962708 \n",
      "     Training Step: 63 Training Loss: 0.6092391610145569 \n",
      "     Training Step: 64 Training Loss: 0.6185775995254517 \n",
      "     Training Step: 65 Training Loss: 0.6160008311271667 \n",
      "     Training Step: 66 Training Loss: 0.6177617907524109 \n",
      "     Training Step: 67 Training Loss: 0.6146091222763062 \n",
      "     Training Step: 68 Training Loss: 0.6142430901527405 \n",
      "     Training Step: 69 Training Loss: 0.6152378916740417 \n",
      "     Training Step: 70 Training Loss: 0.6144551634788513 \n",
      "     Training Step: 71 Training Loss: 0.6177879571914673 \n",
      "     Training Step: 72 Training Loss: 0.6127522587776184 \n",
      "     Training Step: 73 Training Loss: 0.615294873714447 \n",
      "     Training Step: 74 Training Loss: 0.612537682056427 \n",
      "     Training Step: 75 Training Loss: 0.6142795085906982 \n",
      "     Training Step: 76 Training Loss: 0.6123875975608826 \n",
      "     Training Step: 77 Training Loss: 0.6146975159645081 \n",
      "     Training Step: 78 Training Loss: 0.6176829934120178 \n",
      "     Training Step: 79 Training Loss: 0.6147051453590393 \n",
      "     Training Step: 80 Training Loss: 0.6185898780822754 \n",
      "     Training Step: 81 Training Loss: 0.6114287972450256 \n",
      "     Training Step: 82 Training Loss: 0.6167483329772949 \n",
      "     Training Step: 83 Training Loss: 0.6104985475540161 \n",
      "     Training Step: 84 Training Loss: 0.6153974533081055 \n",
      "     Training Step: 85 Training Loss: 0.6100550293922424 \n",
      "     Training Step: 86 Training Loss: 0.6132825016975403 \n",
      "     Training Step: 87 Training Loss: 0.615412175655365 \n",
      "     Training Step: 88 Training Loss: 0.6183289289474487 \n",
      "     Training Step: 89 Training Loss: 0.6120088696479797 \n",
      "     Training Step: 90 Training Loss: 0.6134962439537048 \n",
      "     Training Step: 91 Training Loss: 0.6148730516433716 \n",
      "     Training Step: 92 Training Loss: 0.6166212558746338 \n",
      "     Training Step: 93 Training Loss: 0.6147929430007935 \n",
      "     Training Step: 94 Training Loss: 0.6146632432937622 \n",
      "     Training Step: 95 Training Loss: 0.6155317425727844 \n",
      "     Training Step: 96 Training Loss: 0.6173737049102783 \n",
      "     Training Step: 97 Training Loss: 0.6157416105270386 \n",
      "     Training Step: 98 Training Loss: 0.6118083000183105 \n",
      "     Training Step: 99 Training Loss: 0.6082921028137207 \n",
      "     Training Step: 100 Training Loss: 0.6146715879440308 \n",
      "     Training Step: 101 Training Loss: 0.614440381526947 \n",
      "     Training Step: 102 Training Loss: 0.6152873635292053 \n",
      "     Training Step: 103 Training Loss: 0.6154395341873169 \n",
      "     Training Step: 104 Training Loss: 0.6113945245742798 \n",
      "     Training Step: 105 Training Loss: 0.6115279793739319 \n",
      "     Training Step: 106 Training Loss: 0.6129201054573059 \n",
      "     Training Step: 107 Training Loss: 0.6123696565628052 \n",
      "     Training Step: 108 Training Loss: 0.6133081316947937 \n",
      "     Training Step: 109 Training Loss: 0.6132011413574219 \n",
      "     Training Step: 110 Training Loss: 0.616727352142334 \n",
      "     Training Step: 111 Training Loss: 0.6094360947608948 \n",
      "     Training Step: 112 Training Loss: 0.6107009649276733 \n",
      "     Training Step: 113 Training Loss: 0.6137510538101196 \n",
      "     Training Step: 114 Training Loss: 0.611516535282135 \n",
      "     Training Step: 115 Training Loss: 0.6178504824638367 \n",
      "     Training Step: 116 Training Loss: 0.617676317691803 \n",
      "     Training Step: 117 Training Loss: 0.6138215065002441 \n",
      "     Training Step: 118 Training Loss: 0.6122809648513794 \n",
      "     Training Step: 119 Training Loss: 0.6180486679077148 \n",
      "     Training Step: 120 Training Loss: 0.6138830780982971 \n",
      "     Training Step: 121 Training Loss: 0.6105760931968689 \n",
      "     Training Step: 122 Training Loss: 0.6146098971366882 \n",
      "     Training Step: 123 Training Loss: 0.6150981783866882 \n",
      "     Training Step: 124 Training Loss: 0.6118391752243042 \n",
      "     Training Step: 125 Training Loss: 0.6150643229484558 \n",
      "     Training Step: 126 Training Loss: 0.6134671568870544 \n",
      "     Training Step: 127 Training Loss: 0.6122484803199768 \n",
      "     Training Step: 128 Training Loss: 0.6171860098838806 \n",
      "     Training Step: 129 Training Loss: 0.6176975965499878 \n",
      "     Training Step: 130 Training Loss: 0.6114445328712463 \n",
      "     Training Step: 131 Training Loss: 0.6184359192848206 \n",
      "     Training Step: 132 Training Loss: 0.6134132742881775 \n",
      "     Training Step: 133 Training Loss: 0.6168891787528992 \n",
      "     Training Step: 134 Training Loss: 0.6129679083824158 \n",
      "     Training Step: 135 Training Loss: 0.6169226169586182 \n",
      "     Training Step: 136 Training Loss: 0.6151514649391174 \n",
      "     Training Step: 137 Training Loss: 0.6141522526741028 \n",
      "     Training Step: 138 Training Loss: 0.6137420535087585 \n",
      "     Training Step: 139 Training Loss: 0.6149155497550964 \n",
      "     Training Step: 140 Training Loss: 0.6202062368392944 \n",
      "     Training Step: 141 Training Loss: 0.6136376857757568 \n",
      "     Training Step: 142 Training Loss: 0.6157398819923401 \n",
      "     Training Step: 143 Training Loss: 0.6116539239883423 \n",
      "     Training Step: 144 Training Loss: 0.6166378259658813 \n",
      "     Training Step: 145 Training Loss: 0.6170840859413147 \n",
      "     Training Step: 146 Training Loss: 0.6123902201652527 \n",
      "     Training Step: 147 Training Loss: 0.6114973425865173 \n",
      "     Training Step: 148 Training Loss: 0.6131291389465332 \n",
      "     Training Step: 149 Training Loss: 0.6171337962150574 \n",
      "     Training Step: 150 Training Loss: 0.6135878562927246 \n",
      "     Training Step: 151 Training Loss: 0.6181022524833679 \n",
      "     Training Step: 152 Training Loss: 0.617487370967865 \n",
      "     Training Step: 153 Training Loss: 0.6116339564323425 \n",
      "     Training Step: 154 Training Loss: 0.6100638508796692 \n",
      "     Training Step: 155 Training Loss: 0.612872302532196 \n",
      "     Training Step: 156 Training Loss: 0.6124616265296936 \n",
      "     Training Step: 157 Training Loss: 0.6114286184310913 \n",
      "     Training Step: 158 Training Loss: 0.6167279481887817 \n",
      "     Training Step: 159 Training Loss: 0.612509548664093 \n",
      "     Training Step: 160 Training Loss: 0.6135064363479614 \n",
      "     Training Step: 161 Training Loss: 0.6197267174720764 \n",
      "     Training Step: 162 Training Loss: 0.6160398125648499 \n",
      "     Training Step: 163 Training Loss: 0.6132045984268188 \n",
      "     Training Step: 164 Training Loss: 0.615425705909729 \n",
      "     Training Step: 165 Training Loss: 0.6153867244720459 \n",
      "     Training Step: 166 Training Loss: 0.6158331036567688 \n",
      "     Training Step: 167 Training Loss: 0.6184552311897278 \n",
      "     Training Step: 168 Training Loss: 0.6118964552879333 \n",
      "     Training Step: 169 Training Loss: 0.6163458228111267 \n",
      "     Training Step: 170 Training Loss: 0.6111844182014465 \n",
      "     Training Step: 171 Training Loss: 0.6146014332771301 \n",
      "     Training Step: 172 Training Loss: 0.610712468624115 \n",
      "     Training Step: 173 Training Loss: 0.61619633436203 \n",
      "     Training Step: 174 Training Loss: 0.6106089949607849 \n",
      "     Training Step: 175 Training Loss: 0.6128453612327576 \n",
      "     Training Step: 176 Training Loss: 0.6115924119949341 \n",
      "     Training Step: 177 Training Loss: 0.6143398284912109 \n",
      "     Training Step: 178 Training Loss: 0.6161251068115234 \n",
      "     Training Step: 179 Training Loss: 0.6147508025169373 \n",
      "     Training Step: 180 Training Loss: 0.6122868657112122 \n",
      "     Training Step: 181 Training Loss: 0.6162519454956055 \n",
      "     Training Step: 182 Training Loss: 0.6144884824752808 \n",
      "     Training Step: 183 Training Loss: 0.6146515607833862 \n",
      "     Training Step: 184 Training Loss: 0.6164132952690125 \n",
      "     Training Step: 185 Training Loss: 0.6154797077178955 \n",
      "     Training Step: 186 Training Loss: 0.6116065382957458 \n",
      "     Training Step: 187 Training Loss: 0.6121215224266052 \n",
      "     Training Step: 188 Training Loss: 0.6103984117507935 \n",
      "     Training Step: 189 Training Loss: 0.6097047924995422 \n",
      "     Training Step: 190 Training Loss: 0.6108720302581787 \n",
      "     Training Step: 191 Training Loss: 0.6146920919418335 \n",
      "     Training Step: 192 Training Loss: 0.6115138530731201 \n",
      "     Training Step: 193 Training Loss: 0.6131824851036072 \n",
      "     Training Step: 194 Training Loss: 0.6121550798416138 \n",
      "     Training Step: 195 Training Loss: 0.6156272292137146 \n",
      "     Training Step: 196 Training Loss: 0.6107248663902283 \n",
      "     Training Step: 197 Training Loss: 0.6166929006576538 \n",
      "     Training Step: 198 Training Loss: 0.6209762692451477 \n",
      "     Training Step: 199 Training Loss: 0.6124826073646545 \n",
      "     Training Step: 200 Training Loss: 0.611822247505188 \n",
      "     Training Step: 201 Training Loss: 0.6166571378707886 \n",
      "     Training Step: 202 Training Loss: 0.6132780313491821 \n",
      "     Training Step: 203 Training Loss: 0.6141543388366699 \n",
      "     Training Step: 204 Training Loss: 0.6150883436203003 \n",
      "     Training Step: 205 Training Loss: 0.6100757718086243 \n",
      "     Training Step: 206 Training Loss: 0.6139201521873474 \n",
      "     Training Step: 207 Training Loss: 0.6167052984237671 \n",
      "     Training Step: 208 Training Loss: 0.618866503238678 \n",
      "     Training Step: 209 Training Loss: 0.6121667623519897 \n",
      "     Training Step: 210 Training Loss: 0.611153781414032 \n",
      "     Training Step: 211 Training Loss: 0.6142104268074036 \n",
      "     Training Step: 212 Training Loss: 0.6146520376205444 \n",
      "     Training Step: 213 Training Loss: 0.6105956435203552 \n",
      "     Training Step: 214 Training Loss: 0.6156823635101318 \n",
      "     Training Step: 215 Training Loss: 0.6159515976905823 \n",
      "     Training Step: 216 Training Loss: 0.6115463972091675 \n",
      "     Training Step: 217 Training Loss: 0.6127604246139526 \n",
      "     Training Step: 218 Training Loss: 0.6143435835838318 \n",
      "     Training Step: 219 Training Loss: 0.6146844029426575 \n",
      "     Training Step: 220 Training Loss: 0.6129202246665955 \n",
      "     Training Step: 221 Training Loss: 0.6122251749038696 \n",
      "     Training Step: 222 Training Loss: 0.6111923456192017 \n",
      "     Training Step: 223 Training Loss: 0.6168175339698792 \n",
      "     Training Step: 224 Training Loss: 0.614935576915741 \n",
      "     Training Step: 225 Training Loss: 0.6157644987106323 \n",
      "     Training Step: 226 Training Loss: 0.6155174374580383 \n",
      "     Training Step: 227 Training Loss: 0.6136484146118164 \n",
      "     Training Step: 228 Training Loss: 0.609991192817688 \n",
      "     Training Step: 229 Training Loss: 0.6152843236923218 \n",
      "     Training Step: 230 Training Loss: 0.6182289719581604 \n",
      "     Training Step: 231 Training Loss: 0.6167985200881958 \n",
      "     Training Step: 232 Training Loss: 0.6140372157096863 \n",
      "     Training Step: 233 Training Loss: 0.6114237904548645 \n",
      "     Training Step: 234 Training Loss: 0.6130759716033936 \n",
      "     Training Step: 235 Training Loss: 0.6105194091796875 \n",
      "     Training Step: 236 Training Loss: 0.6153677701950073 \n",
      "     Training Step: 237 Training Loss: 0.6149540543556213 \n",
      "     Training Step: 238 Training Loss: 0.6198727488517761 \n",
      "     Training Step: 239 Training Loss: 0.6147699952125549 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6155713796615601 \n",
      "     Validation Step: 1 Validation Loss: 0.6133031249046326 \n",
      "     Validation Step: 2 Validation Loss: 0.6075210571289062 \n",
      "     Validation Step: 3 Validation Loss: 0.6159896850585938 \n",
      "     Validation Step: 4 Validation Loss: 0.6101366281509399 \n",
      "     Validation Step: 5 Validation Loss: 0.6141291260719299 \n",
      "     Validation Step: 6 Validation Loss: 0.6136317849159241 \n",
      "     Validation Step: 7 Validation Loss: 0.6121286153793335 \n",
      "     Validation Step: 8 Validation Loss: 0.6152265071868896 \n",
      "     Validation Step: 9 Validation Loss: 0.6105173826217651 \n",
      "     Validation Step: 10 Validation Loss: 0.6101555824279785 \n",
      "     Validation Step: 11 Validation Loss: 0.6129850149154663 \n",
      "     Validation Step: 12 Validation Loss: 0.611640214920044 \n",
      "     Validation Step: 13 Validation Loss: 0.6146372556686401 \n",
      "     Validation Step: 14 Validation Loss: 0.6173008680343628 \n",
      "     Validation Step: 15 Validation Loss: 0.6111658811569214 \n",
      "     Validation Step: 16 Validation Loss: 0.618463933467865 \n",
      "     Validation Step: 17 Validation Loss: 0.6180567145347595 \n",
      "     Validation Step: 18 Validation Loss: 0.615787923336029 \n",
      "     Validation Step: 19 Validation Loss: 0.6141120195388794 \n",
      "     Validation Step: 20 Validation Loss: 0.6176902055740356 \n",
      "     Validation Step: 21 Validation Loss: 0.6145322918891907 \n",
      "     Validation Step: 22 Validation Loss: 0.6101123094558716 \n",
      "     Validation Step: 23 Validation Loss: 0.6142512559890747 \n",
      "     Validation Step: 24 Validation Loss: 0.6153183579444885 \n",
      "     Validation Step: 25 Validation Loss: 0.6142004728317261 \n",
      "     Validation Step: 26 Validation Loss: 0.6115565896034241 \n",
      "     Validation Step: 27 Validation Loss: 0.6150380373001099 \n",
      "     Validation Step: 28 Validation Loss: 0.6162398457527161 \n",
      "     Validation Step: 29 Validation Loss: 0.613652229309082 \n",
      "     Validation Step: 30 Validation Loss: 0.6156129837036133 \n",
      "     Validation Step: 31 Validation Loss: 0.6104838848114014 \n",
      "     Validation Step: 32 Validation Loss: 0.6170181632041931 \n",
      "     Validation Step: 33 Validation Loss: 0.6128260493278503 \n",
      "     Validation Step: 34 Validation Loss: 0.611880898475647 \n",
      "     Validation Step: 35 Validation Loss: 0.6136664748191833 \n",
      "     Validation Step: 36 Validation Loss: 0.6148337125778198 \n",
      "     Validation Step: 37 Validation Loss: 0.614568293094635 \n",
      "     Validation Step: 38 Validation Loss: 0.6175900101661682 \n",
      "     Validation Step: 39 Validation Loss: 0.6148907542228699 \n",
      "     Validation Step: 40 Validation Loss: 0.618222177028656 \n",
      "     Validation Step: 41 Validation Loss: 0.6106307506561279 \n",
      "     Validation Step: 42 Validation Loss: 0.6183376908302307 \n",
      "     Validation Step: 43 Validation Loss: 0.6111792922019958 \n",
      "     Validation Step: 44 Validation Loss: 0.6184878349304199 \n",
      "Epoch: 127\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6108930110931396 \n",
      "     Training Step: 1 Training Loss: 0.6113963723182678 \n",
      "     Training Step: 2 Training Loss: 0.6170948147773743 \n",
      "     Training Step: 3 Training Loss: 0.6167066693305969 \n",
      "     Training Step: 4 Training Loss: 0.6173802614212036 \n",
      "     Training Step: 5 Training Loss: 0.6178066730499268 \n",
      "     Training Step: 6 Training Loss: 0.6111652255058289 \n",
      "     Training Step: 7 Training Loss: 0.6134172081947327 \n",
      "     Training Step: 8 Training Loss: 0.6169147491455078 \n",
      "     Training Step: 9 Training Loss: 0.6201987862586975 \n",
      "     Training Step: 10 Training Loss: 0.6180003881454468 \n",
      "     Training Step: 11 Training Loss: 0.6176796555519104 \n",
      "     Training Step: 12 Training Loss: 0.616405189037323 \n",
      "     Training Step: 13 Training Loss: 0.616644561290741 \n",
      "     Training Step: 14 Training Loss: 0.6168839335441589 \n",
      "     Training Step: 15 Training Loss: 0.6133092045783997 \n",
      "     Training Step: 16 Training Loss: 0.6167119741439819 \n",
      "     Training Step: 17 Training Loss: 0.6122868657112122 \n",
      "     Training Step: 18 Training Loss: 0.6177338361740112 \n",
      "     Training Step: 19 Training Loss: 0.6139443516731262 \n",
      "     Training Step: 20 Training Loss: 0.6115541458129883 \n",
      "     Training Step: 21 Training Loss: 0.6151733994483948 \n",
      "     Training Step: 22 Training Loss: 0.6108977198600769 \n",
      "     Training Step: 23 Training Loss: 0.6176315546035767 \n",
      "     Training Step: 24 Training Loss: 0.6114335656166077 \n",
      "     Training Step: 25 Training Loss: 0.6147488951683044 \n",
      "     Training Step: 26 Training Loss: 0.61222904920578 \n",
      "     Training Step: 27 Training Loss: 0.6105846762657166 \n",
      "     Training Step: 28 Training Loss: 0.6094359755516052 \n",
      "     Training Step: 29 Training Loss: 0.6163750290870667 \n",
      "     Training Step: 30 Training Loss: 0.6114082336425781 \n",
      "     Training Step: 31 Training Loss: 0.6115413904190063 \n",
      "     Training Step: 32 Training Loss: 0.6100285053253174 \n",
      "     Training Step: 33 Training Loss: 0.6096922159194946 \n",
      "     Training Step: 34 Training Loss: 0.6129360795021057 \n",
      "     Training Step: 35 Training Loss: 0.6129688620567322 \n",
      "     Training Step: 36 Training Loss: 0.6134597659111023 \n",
      "     Training Step: 37 Training Loss: 0.6125100255012512 \n",
      "     Training Step: 38 Training Loss: 0.6157861948013306 \n",
      "     Training Step: 39 Training Loss: 0.613912045955658 \n",
      "     Training Step: 40 Training Loss: 0.618375837802887 \n",
      "     Training Step: 41 Training Loss: 0.6160163879394531 \n",
      "     Training Step: 42 Training Loss: 0.6143448948860168 \n",
      "     Training Step: 43 Training Loss: 0.6168110370635986 \n",
      "     Training Step: 44 Training Loss: 0.6157852411270142 \n",
      "     Training Step: 45 Training Loss: 0.6132165193557739 \n",
      "     Training Step: 46 Training Loss: 0.6127874851226807 \n",
      "     Training Step: 47 Training Loss: 0.6155301928520203 \n",
      "     Training Step: 48 Training Loss: 0.6181978583335876 \n",
      "     Training Step: 49 Training Loss: 0.6135234236717224 \n",
      "     Training Step: 50 Training Loss: 0.6161175966262817 \n",
      "     Training Step: 51 Training Loss: 0.6154873967170715 \n",
      "     Training Step: 52 Training Loss: 0.6147379875183105 \n",
      "     Training Step: 53 Training Loss: 0.6130868196487427 \n",
      "     Training Step: 54 Training Loss: 0.6165990829467773 \n",
      "     Training Step: 55 Training Loss: 0.6161923408508301 \n",
      "     Training Step: 56 Training Loss: 0.6185759902000427 \n",
      "     Training Step: 57 Training Loss: 0.612763524055481 \n",
      "     Training Step: 58 Training Loss: 0.6146053671836853 \n",
      "     Training Step: 59 Training Loss: 0.6164097189903259 \n",
      "     Training Step: 60 Training Loss: 0.6130699515342712 \n",
      "     Training Step: 61 Training Loss: 0.6140707731246948 \n",
      "     Training Step: 62 Training Loss: 0.6156836152076721 \n",
      "     Training Step: 63 Training Loss: 0.6121575832366943 \n",
      "     Training Step: 64 Training Loss: 0.6146734356880188 \n",
      "     Training Step: 65 Training Loss: 0.6131214499473572 \n",
      "     Training Step: 66 Training Loss: 0.6171928644180298 \n",
      "     Training Step: 67 Training Loss: 0.6182377934455872 \n",
      "     Training Step: 68 Training Loss: 0.6150960922241211 \n",
      "     Training Step: 69 Training Loss: 0.6118062734603882 \n",
      "     Training Step: 70 Training Loss: 0.6167125701904297 \n",
      "     Training Step: 71 Training Loss: 0.6180241703987122 \n",
      "     Training Step: 72 Training Loss: 0.6188157796859741 \n",
      "     Training Step: 73 Training Loss: 0.6152923107147217 \n",
      "     Training Step: 74 Training Loss: 0.6162251830101013 \n",
      "     Training Step: 75 Training Loss: 0.6141647696495056 \n",
      "     Training Step: 76 Training Loss: 0.6142336130142212 \n",
      "     Training Step: 77 Training Loss: 0.6118656396865845 \n",
      "     Training Step: 78 Training Loss: 0.6166346669197083 \n",
      "     Training Step: 79 Training Loss: 0.6152825951576233 \n",
      "     Training Step: 80 Training Loss: 0.614777147769928 \n",
      "     Training Step: 81 Training Loss: 0.6151582598686218 \n",
      "     Training Step: 82 Training Loss: 0.6144552826881409 \n",
      "     Training Step: 83 Training Loss: 0.6209121346473694 \n",
      "     Training Step: 84 Training Loss: 0.6114215850830078 \n",
      "     Training Step: 85 Training Loss: 0.6140220165252686 \n",
      "     Training Step: 86 Training Loss: 0.6196078062057495 \n",
      "     Training Step: 87 Training Loss: 0.6148665547370911 \n",
      "     Training Step: 88 Training Loss: 0.6174798607826233 \n",
      "     Training Step: 89 Training Loss: 0.6106743812561035 \n",
      "     Training Step: 90 Training Loss: 0.6146876811981201 \n",
      "     Training Step: 91 Training Loss: 0.6105824708938599 \n",
      "     Training Step: 92 Training Loss: 0.6128429770469666 \n",
      "     Training Step: 93 Training Loss: 0.6106911897659302 \n",
      "     Training Step: 94 Training Loss: 0.6118306517601013 \n",
      "     Training Step: 95 Training Loss: 0.6178046464920044 \n",
      "     Training Step: 96 Training Loss: 0.6154469847679138 \n",
      "     Training Step: 97 Training Loss: 0.6144821047782898 \n",
      "     Training Step: 98 Training Loss: 0.6096904873847961 \n",
      "     Training Step: 99 Training Loss: 0.6132723093032837 \n",
      "     Training Step: 100 Training Loss: 0.6154259443283081 \n",
      "     Training Step: 101 Training Loss: 0.6114266514778137 \n",
      "     Training Step: 102 Training Loss: 0.6156147718429565 \n",
      "     Training Step: 103 Training Loss: 0.6159594655036926 \n",
      "     Training Step: 104 Training Loss: 0.6180856823921204 \n",
      "     Training Step: 105 Training Loss: 0.6100525856018066 \n",
      "     Training Step: 106 Training Loss: 0.6121217608451843 \n",
      "     Training Step: 107 Training Loss: 0.6125354170799255 \n",
      "     Training Step: 108 Training Loss: 0.6101405620574951 \n",
      "     Training Step: 109 Training Loss: 0.614342451095581 \n",
      "     Training Step: 110 Training Loss: 0.6134717464447021 \n",
      "     Training Step: 111 Training Loss: 0.6131822466850281 \n",
      "     Training Step: 112 Training Loss: 0.6146647334098816 \n",
      "     Training Step: 113 Training Loss: 0.6146129369735718 \n",
      "     Training Step: 114 Training Loss: 0.6107285618782043 \n",
      "     Training Step: 115 Training Loss: 0.6146401166915894 \n",
      "     Training Step: 116 Training Loss: 0.6136128902435303 \n",
      "     Training Step: 117 Training Loss: 0.6116012930870056 \n",
      "     Training Step: 118 Training Loss: 0.6166658997535706 \n",
      "     Training Step: 119 Training Loss: 0.6133031249046326 \n",
      "     Training Step: 120 Training Loss: 0.6194573044776917 \n",
      "     Training Step: 121 Training Loss: 0.6152681112289429 \n",
      "     Training Step: 122 Training Loss: 0.61603182554245 \n",
      "     Training Step: 123 Training Loss: 0.6143781542778015 \n",
      "     Training Step: 124 Training Loss: 0.6117033958435059 \n",
      "     Training Step: 125 Training Loss: 0.6105867028236389 \n",
      "     Training Step: 126 Training Loss: 0.6121516227722168 \n",
      "     Training Step: 127 Training Loss: 0.6137440204620361 \n",
      "     Training Step: 128 Training Loss: 0.613296389579773 \n",
      "     Training Step: 129 Training Loss: 0.6135166883468628 \n",
      "     Training Step: 130 Training Loss: 0.6164916753768921 \n",
      "     Training Step: 131 Training Loss: 0.6154229044914246 \n",
      "     Training Step: 132 Training Loss: 0.6144213080406189 \n",
      "     Training Step: 133 Training Loss: 0.6097275614738464 \n",
      "     Training Step: 134 Training Loss: 0.6185805797576904 \n",
      "     Training Step: 135 Training Loss: 0.6149160861968994 \n",
      "     Training Step: 136 Training Loss: 0.6167585253715515 \n",
      "     Training Step: 137 Training Loss: 0.6171327829360962 \n",
      "     Training Step: 138 Training Loss: 0.6118398308753967 \n",
      "     Training Step: 139 Training Loss: 0.6116501688957214 \n",
      "     Training Step: 140 Training Loss: 0.6131505370140076 \n",
      "     Training Step: 141 Training Loss: 0.6138856410980225 \n",
      "     Training Step: 142 Training Loss: 0.609412431716919 \n",
      "     Training Step: 143 Training Loss: 0.6161967515945435 \n",
      "     Training Step: 144 Training Loss: 0.6122953295707703 \n",
      "     Training Step: 145 Training Loss: 0.6157467365264893 \n",
      "     Training Step: 146 Training Loss: 0.6111372113227844 \n",
      "     Training Step: 147 Training Loss: 0.6177273392677307 \n",
      "     Training Step: 148 Training Loss: 0.6124867796897888 \n",
      "     Training Step: 149 Training Loss: 0.6114669442176819 \n",
      "     Training Step: 150 Training Loss: 0.6099699139595032 \n",
      "     Training Step: 151 Training Loss: 0.6171780228614807 \n",
      "     Training Step: 152 Training Loss: 0.6115218997001648 \n",
      "     Training Step: 153 Training Loss: 0.611515998840332 \n",
      "     Training Step: 154 Training Loss: 0.6171636581420898 \n",
      "     Training Step: 155 Training Loss: 0.6141058206558228 \n",
      "     Training Step: 156 Training Loss: 0.61335289478302 \n",
      "     Training Step: 157 Training Loss: 0.6115759015083313 \n",
      "     Training Step: 158 Training Loss: 0.6117768287658691 \n",
      "     Training Step: 159 Training Loss: 0.6133081912994385 \n",
      "     Training Step: 160 Training Loss: 0.6116359829902649 \n",
      "     Training Step: 161 Training Loss: 0.6117792129516602 \n",
      "     Training Step: 162 Training Loss: 0.6145029067993164 \n",
      "     Training Step: 163 Training Loss: 0.6125244498252869 \n",
      "     Training Step: 164 Training Loss: 0.6146062612533569 \n",
      "     Training Step: 165 Training Loss: 0.6181281208992004 \n",
      "     Training Step: 166 Training Loss: 0.6137404441833496 \n",
      "     Training Step: 167 Training Loss: 0.6199012398719788 \n",
      "     Training Step: 168 Training Loss: 0.6105777621269226 \n",
      "     Training Step: 169 Training Loss: 0.6092312335968018 \n",
      "     Training Step: 170 Training Loss: 0.6149532794952393 \n",
      "     Training Step: 171 Training Loss: 0.614031195640564 \n",
      "     Training Step: 172 Training Loss: 0.6103999614715576 \n",
      "     Training Step: 173 Training Loss: 0.6152387857437134 \n",
      "     Training Step: 174 Training Loss: 0.6144389510154724 \n",
      "     Training Step: 175 Training Loss: 0.6153344511985779 \n",
      "     Training Step: 176 Training Loss: 0.6120666265487671 \n",
      "     Training Step: 177 Training Loss: 0.6141513586044312 \n",
      "     Training Step: 178 Training Loss: 0.616679310798645 \n",
      "     Training Step: 179 Training Loss: 0.6126395463943481 \n",
      "     Training Step: 180 Training Loss: 0.6146540641784668 \n",
      "     Training Step: 181 Training Loss: 0.6184179782867432 \n",
      "     Training Step: 182 Training Loss: 0.6082509756088257 \n",
      "     Training Step: 183 Training Loss: 0.6140143871307373 \n",
      "     Training Step: 184 Training Loss: 0.6123773455619812 \n",
      "     Training Step: 185 Training Loss: 0.6167064905166626 \n",
      "     Training Step: 186 Training Loss: 0.6184651851654053 \n",
      "     Training Step: 187 Training Loss: 0.6153731942176819 \n",
      "     Training Step: 188 Training Loss: 0.6132025718688965 \n",
      "     Training Step: 189 Training Loss: 0.6150892376899719 \n",
      "     Training Step: 190 Training Loss: 0.6149369478225708 \n",
      "     Training Step: 191 Training Loss: 0.6153753399848938 \n",
      "     Training Step: 192 Training Loss: 0.6116278171539307 \n",
      "     Training Step: 193 Training Loss: 0.6123901605606079 \n",
      "     Training Step: 194 Training Loss: 0.6105013489723206 \n",
      "     Training Step: 195 Training Loss: 0.6152894496917725 \n",
      "     Training Step: 196 Training Loss: 0.6147960424423218 \n",
      "     Training Step: 197 Training Loss: 0.618873655796051 \n",
      "     Training Step: 198 Training Loss: 0.6136497855186462 \n",
      "     Training Step: 199 Training Loss: 0.6168063282966614 \n",
      "     Training Step: 200 Training Loss: 0.6184422373771667 \n",
      "     Training Step: 201 Training Loss: 0.6157376766204834 \n",
      "     Training Step: 202 Training Loss: 0.6120232343673706 \n",
      "     Training Step: 203 Training Loss: 0.6155164241790771 \n",
      "     Training Step: 204 Training Loss: 0.6176815629005432 \n",
      "     Training Step: 205 Training Loss: 0.6102064847946167 \n",
      "     Training Step: 206 Training Loss: 0.6142128705978394 \n",
      "     Training Step: 207 Training Loss: 0.6121736168861389 \n",
      "     Training Step: 208 Training Loss: 0.613593339920044 \n",
      "     Training Step: 209 Training Loss: 0.6128748655319214 \n",
      "     Training Step: 210 Training Loss: 0.615530252456665 \n",
      "     Training Step: 211 Training Loss: 0.6127986311912537 \n",
      "     Training Step: 212 Training Loss: 0.6128647923469543 \n",
      "     Training Step: 213 Training Loss: 0.6129232048988342 \n",
      "     Training Step: 214 Training Loss: 0.6111947298049927 \n",
      "     Training Step: 215 Training Loss: 0.6118764877319336 \n",
      "     Training Step: 216 Training Loss: 0.6150778532028198 \n",
      "     Training Step: 217 Training Loss: 0.6104786992073059 \n",
      "     Training Step: 218 Training Loss: 0.6154102087020874 \n",
      "     Training Step: 219 Training Loss: 0.6106985807418823 \n",
      "     Training Step: 220 Training Loss: 0.6122313141822815 \n",
      "     Training Step: 221 Training Loss: 0.612277090549469 \n",
      "     Training Step: 222 Training Loss: 0.6147190928459167 \n",
      "     Training Step: 223 Training Loss: 0.616837739944458 \n",
      "     Training Step: 224 Training Loss: 0.6123701930046082 \n",
      "     Training Step: 225 Training Loss: 0.6136431097984314 \n",
      "     Training Step: 226 Training Loss: 0.6158438324928284 \n",
      "     Training Step: 227 Training Loss: 0.6138172745704651 \n",
      "     Training Step: 228 Training Loss: 0.6137664914131165 \n",
      "     Training Step: 229 Training Loss: 0.6143448948860168 \n",
      "     Training Step: 230 Training Loss: 0.6156839728355408 \n",
      "     Training Step: 231 Training Loss: 0.6127038598060608 \n",
      "     Training Step: 232 Training Loss: 0.6142444014549255 \n",
      "     Training Step: 233 Training Loss: 0.6100647449493408 \n",
      "     Training Step: 234 Training Loss: 0.6103932857513428 \n",
      "     Training Step: 235 Training Loss: 0.6196886301040649 \n",
      "     Training Step: 236 Training Loss: 0.6146772503852844 \n",
      "     Training Step: 237 Training Loss: 0.6124645471572876 \n",
      "     Training Step: 238 Training Loss: 0.6147022247314453 \n",
      "     Training Step: 239 Training Loss: 0.6142830848693848 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6128340363502502 \n",
      "     Validation Step: 1 Validation Loss: 0.6153149604797363 \n",
      "     Validation Step: 2 Validation Loss: 0.6111781001091003 \n",
      "     Validation Step: 3 Validation Loss: 0.616240382194519 \n",
      "     Validation Step: 4 Validation Loss: 0.6182194352149963 \n",
      "     Validation Step: 5 Validation Loss: 0.6116436719894409 \n",
      "     Validation Step: 6 Validation Loss: 0.6184706091880798 \n",
      "     Validation Step: 7 Validation Loss: 0.6145718097686768 \n",
      "     Validation Step: 8 Validation Loss: 0.6176870465278625 \n",
      "     Validation Step: 9 Validation Loss: 0.6156105399131775 \n",
      "     Validation Step: 10 Validation Loss: 0.6136423945426941 \n",
      "     Validation Step: 11 Validation Loss: 0.6148959994316101 \n",
      "     Validation Step: 12 Validation Loss: 0.6159830689430237 \n",
      "     Validation Step: 13 Validation Loss: 0.6101340055465698 \n",
      "     Validation Step: 14 Validation Loss: 0.6118834018707275 \n",
      "     Validation Step: 15 Validation Loss: 0.6150299906730652 \n",
      "     Validation Step: 16 Validation Loss: 0.6133086681365967 \n",
      "     Validation Step: 17 Validation Loss: 0.6101612448692322 \n",
      "     Validation Step: 18 Validation Loss: 0.6141169667243958 \n",
      "     Validation Step: 19 Validation Loss: 0.6145361661911011 \n",
      "     Validation Step: 20 Validation Loss: 0.6129862070083618 \n",
      "     Validation Step: 21 Validation Loss: 0.6183321475982666 \n",
      "     Validation Step: 22 Validation Loss: 0.611556351184845 \n",
      "     Validation Step: 23 Validation Loss: 0.6075206995010376 \n",
      "     Validation Step: 24 Validation Loss: 0.6148371696472168 \n",
      "     Validation Step: 25 Validation Loss: 0.6173064112663269 \n",
      "     Validation Step: 26 Validation Loss: 0.614637017250061 \n",
      "     Validation Step: 27 Validation Loss: 0.6136441230773926 \n",
      "     Validation Step: 28 Validation Loss: 0.6141256093978882 \n",
      "     Validation Step: 29 Validation Loss: 0.6184886693954468 \n",
      "     Validation Step: 30 Validation Loss: 0.6121309399604797 \n",
      "     Validation Step: 31 Validation Loss: 0.615786612033844 \n",
      "     Validation Step: 32 Validation Loss: 0.6104825735092163 \n",
      "     Validation Step: 33 Validation Loss: 0.6105170845985413 \n",
      "     Validation Step: 34 Validation Loss: 0.6180567145347595 \n",
      "     Validation Step: 35 Validation Loss: 0.6142562031745911 \n",
      "     Validation Step: 36 Validation Loss: 0.6155670285224915 \n",
      "     Validation Step: 37 Validation Loss: 0.6106314659118652 \n",
      "     Validation Step: 38 Validation Loss: 0.6170186400413513 \n",
      "     Validation Step: 39 Validation Loss: 0.610106885433197 \n",
      "     Validation Step: 40 Validation Loss: 0.6175976395606995 \n",
      "     Validation Step: 41 Validation Loss: 0.6142023205757141 \n",
      "     Validation Step: 42 Validation Loss: 0.6111629605293274 \n",
      "     Validation Step: 43 Validation Loss: 0.6152256727218628 \n",
      "     Validation Step: 44 Validation Loss: 0.6136636137962341 \n",
      "Epoch: 128\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6202154755592346 \n",
      "     Training Step: 1 Training Loss: 0.6106910109519958 \n",
      "     Training Step: 2 Training Loss: 0.613293468952179 \n",
      "     Training Step: 3 Training Loss: 0.6142022609710693 \n",
      "     Training Step: 4 Training Loss: 0.6128662824630737 \n",
      "     Training Step: 5 Training Loss: 0.616802990436554 \n",
      "     Training Step: 6 Training Loss: 0.6118265986442566 \n",
      "     Training Step: 7 Training Loss: 0.6107383370399475 \n",
      "     Training Step: 8 Training Loss: 0.6162050366401672 \n",
      "     Training Step: 9 Training Loss: 0.6153767108917236 \n",
      "     Training Step: 10 Training Loss: 0.6116885542869568 \n",
      "     Training Step: 11 Training Loss: 0.6120126247406006 \n",
      "     Training Step: 12 Training Loss: 0.614072322845459 \n",
      "     Training Step: 13 Training Loss: 0.6139121651649475 \n",
      "     Training Step: 14 Training Loss: 0.6152410507202148 \n",
      "     Training Step: 15 Training Loss: 0.6131353974342346 \n",
      "     Training Step: 16 Training Loss: 0.6151532530784607 \n",
      "     Training Step: 17 Training Loss: 0.6133110523223877 \n",
      "     Training Step: 18 Training Loss: 0.6115196943283081 \n",
      "     Training Step: 19 Training Loss: 0.6132038235664368 \n",
      "     Training Step: 20 Training Loss: 0.6152864694595337 \n",
      "     Training Step: 21 Training Loss: 0.6118718385696411 \n",
      "     Training Step: 22 Training Loss: 0.6186055541038513 \n",
      "     Training Step: 23 Training Loss: 0.6117729544639587 \n",
      "     Training Step: 24 Training Loss: 0.6137422323226929 \n",
      "     Training Step: 25 Training Loss: 0.6162077784538269 \n",
      "     Training Step: 26 Training Loss: 0.6168986558914185 \n",
      "     Training Step: 27 Training Loss: 0.6151754856109619 \n",
      "     Training Step: 28 Training Loss: 0.615682065486908 \n",
      "     Training Step: 29 Training Loss: 0.6121438145637512 \n",
      "     Training Step: 30 Training Loss: 0.6127693057060242 \n",
      "     Training Step: 31 Training Loss: 0.6101872324943542 \n",
      "     Training Step: 32 Training Loss: 0.6177575588226318 \n",
      "     Training Step: 33 Training Loss: 0.6146990656852722 \n",
      "     Training Step: 34 Training Loss: 0.6157464981079102 \n",
      "     Training Step: 35 Training Loss: 0.6176982522010803 \n",
      "     Training Step: 36 Training Loss: 0.6155312061309814 \n",
      "     Training Step: 37 Training Loss: 0.6140232086181641 \n",
      "     Training Step: 38 Training Loss: 0.6111558079719543 \n",
      "     Training Step: 39 Training Loss: 0.612912118434906 \n",
      "     Training Step: 40 Training Loss: 0.6100789308547974 \n",
      "     Training Step: 41 Training Loss: 0.6153365969657898 \n",
      "     Training Step: 42 Training Loss: 0.6113996505737305 \n",
      "     Training Step: 43 Training Loss: 0.6142446398735046 \n",
      "     Training Step: 44 Training Loss: 0.6136471629142761 \n",
      "     Training Step: 45 Training Loss: 0.6092163920402527 \n",
      "     Training Step: 46 Training Loss: 0.6106504797935486 \n",
      "     Training Step: 47 Training Loss: 0.6146482229232788 \n",
      "     Training Step: 48 Training Loss: 0.6136442422866821 \n",
      "     Training Step: 49 Training Loss: 0.618267297744751 \n",
      "     Training Step: 50 Training Loss: 0.6184844970703125 \n",
      "     Training Step: 51 Training Loss: 0.6134037375450134 \n",
      "     Training Step: 52 Training Loss: 0.6197247505187988 \n",
      "     Training Step: 53 Training Loss: 0.6157876253128052 \n",
      "     Training Step: 54 Training Loss: 0.6114235520362854 \n",
      "     Training Step: 55 Training Loss: 0.6108925938606262 \n",
      "     Training Step: 56 Training Loss: 0.6167968511581421 \n",
      "     Training Step: 57 Training Loss: 0.6100761890411377 \n",
      "     Training Step: 58 Training Loss: 0.6162367463111877 \n",
      "     Training Step: 59 Training Loss: 0.6152946352958679 \n",
      "     Training Step: 60 Training Loss: 0.615516722202301 \n",
      "     Training Step: 61 Training Loss: 0.6167464852333069 \n",
      "     Training Step: 62 Training Loss: 0.6150894165039062 \n",
      "     Training Step: 63 Training Loss: 0.6140451431274414 \n",
      "     Training Step: 64 Training Loss: 0.6147745251655579 \n",
      "     Training Step: 65 Training Loss: 0.6164809465408325 \n",
      "     Training Step: 66 Training Loss: 0.616399884223938 \n",
      "     Training Step: 67 Training Loss: 0.6142827272415161 \n",
      "     Training Step: 68 Training Loss: 0.6152580380439758 \n",
      "     Training Step: 69 Training Loss: 0.6131320595741272 \n",
      "     Training Step: 70 Training Loss: 0.6097278594970703 \n",
      "     Training Step: 71 Training Loss: 0.614601731300354 \n",
      "     Training Step: 72 Training Loss: 0.6146678328514099 \n",
      "     Training Step: 73 Training Loss: 0.618021547794342 \n",
      "     Training Step: 74 Training Loss: 0.6101378798484802 \n",
      "     Training Step: 75 Training Loss: 0.6104753017425537 \n",
      "     Training Step: 76 Training Loss: 0.6183345317840576 \n",
      "     Training Step: 77 Training Loss: 0.6143800020217896 \n",
      "     Training Step: 78 Training Loss: 0.6105811595916748 \n",
      "     Training Step: 79 Training Loss: 0.6114670038223267 \n",
      "     Training Step: 80 Training Loss: 0.6148779988288879 \n",
      "     Training Step: 81 Training Loss: 0.6142120957374573 \n",
      "     Training Step: 82 Training Loss: 0.6127936244010925 \n",
      "     Training Step: 83 Training Loss: 0.6127532720565796 \n",
      "     Training Step: 84 Training Loss: 0.6115198135375977 \n",
      "     Training Step: 85 Training Loss: 0.6134672164916992 \n",
      "     Training Step: 86 Training Loss: 0.6147075295448303 \n",
      "     Training Step: 87 Training Loss: 0.6115458011627197 \n",
      "     Training Step: 88 Training Loss: 0.6103872656822205 \n",
      "     Training Step: 89 Training Loss: 0.6114211082458496 \n",
      "     Training Step: 90 Training Loss: 0.6144524216651917 \n",
      "     Training Step: 91 Training Loss: 0.6186316609382629 \n",
      "     Training Step: 92 Training Loss: 0.6166730523109436 \n",
      "     Training Step: 93 Training Loss: 0.616010844707489 \n",
      "     Training Step: 94 Training Loss: 0.6123729944229126 \n",
      "     Training Step: 95 Training Loss: 0.6099828481674194 \n",
      "     Training Step: 96 Training Loss: 0.6146796941757202 \n",
      "     Training Step: 97 Training Loss: 0.6124891638755798 \n",
      "     Training Step: 98 Training Loss: 0.613511323928833 \n",
      "     Training Step: 99 Training Loss: 0.6100438237190247 \n",
      "     Training Step: 100 Training Loss: 0.6129171848297119 \n",
      "     Training Step: 101 Training Loss: 0.6146513223648071 \n",
      "     Training Step: 102 Training Loss: 0.615691602230072 \n",
      "     Training Step: 103 Training Loss: 0.6180818676948547 \n",
      "     Training Step: 104 Training Loss: 0.6122891306877136 \n",
      "     Training Step: 105 Training Loss: 0.6136131882667542 \n",
      "     Training Step: 106 Training Loss: 0.6154753565788269 \n",
      "     Training Step: 107 Training Loss: 0.6154202818870544 \n",
      "     Training Step: 108 Training Loss: 0.6097218990325928 \n",
      "     Training Step: 109 Training Loss: 0.6168047785758972 \n",
      "     Training Step: 110 Training Loss: 0.6147444844245911 \n",
      "     Training Step: 111 Training Loss: 0.6163477897644043 \n",
      "     Training Step: 112 Training Loss: 0.6122381687164307 \n",
      "     Training Step: 113 Training Loss: 0.6141071915626526 \n",
      "     Training Step: 114 Training Loss: 0.6121622323989868 \n",
      "     Training Step: 115 Training Loss: 0.618033766746521 \n",
      "     Training Step: 116 Training Loss: 0.6123807430267334 \n",
      "     Training Step: 117 Training Loss: 0.6116296648979187 \n",
      "     Training Step: 118 Training Loss: 0.6120683550834656 \n",
      "     Training Step: 119 Training Loss: 0.613201916217804 \n",
      "     Training Step: 120 Training Loss: 0.6103702783584595 \n",
      "     Training Step: 121 Training Loss: 0.6093834042549133 \n",
      "     Training Step: 122 Training Loss: 0.6166885495185852 \n",
      "     Training Step: 123 Training Loss: 0.6117744445800781 \n",
      "     Training Step: 124 Training Loss: 0.6146978735923767 \n",
      "     Training Step: 125 Training Loss: 0.6184520125389099 \n",
      "     Training Step: 126 Training Loss: 0.614348828792572 \n",
      "     Training Step: 127 Training Loss: 0.6115737557411194 \n",
      "     Training Step: 128 Training Loss: 0.6137493848800659 \n",
      "     Training Step: 129 Training Loss: 0.6116152405738831 \n",
      "     Training Step: 130 Training Loss: 0.6134964823722839 \n",
      "     Training Step: 131 Training Loss: 0.6105624437332153 \n",
      "     Training Step: 132 Training Loss: 0.6173874735832214 \n",
      "     Training Step: 133 Training Loss: 0.616658091545105 \n",
      "     Training Step: 134 Training Loss: 0.610585629940033 \n",
      "     Training Step: 135 Training Loss: 0.6177945733070374 \n",
      "     Training Step: 136 Training Loss: 0.6160337328910828 \n",
      "     Training Step: 137 Training Loss: 0.6133029460906982 \n",
      "     Training Step: 138 Training Loss: 0.6176156401634216 \n",
      "     Training Step: 139 Training Loss: 0.6184441447257996 \n",
      "     Training Step: 140 Training Loss: 0.6109253764152527 \n",
      "     Training Step: 141 Training Loss: 0.6116381287574768 \n",
      "     Training Step: 142 Training Loss: 0.6166960000991821 \n",
      "     Training Step: 143 Training Loss: 0.6153739094734192 \n",
      "     Training Step: 144 Training Loss: 0.6171212196350098 \n",
      "     Training Step: 145 Training Loss: 0.6169075965881348 \n",
      "     Training Step: 146 Training Loss: 0.6176794767379761 \n",
      "     Training Step: 147 Training Loss: 0.6157416105270386 \n",
      "     Training Step: 148 Training Loss: 0.613596498966217 \n",
      "     Training Step: 149 Training Loss: 0.6097584962844849 \n",
      "     Training Step: 150 Training Loss: 0.6105257868766785 \n",
      "     Training Step: 151 Training Loss: 0.6166467666625977 \n",
      "     Training Step: 152 Training Loss: 0.6178024411201477 \n",
      "     Training Step: 153 Training Loss: 0.6196078062057495 \n",
      "     Training Step: 154 Training Loss: 0.6157614588737488 \n",
      "     Training Step: 155 Training Loss: 0.6105892658233643 \n",
      "     Training Step: 156 Training Loss: 0.6149425506591797 \n",
      "     Training Step: 157 Training Loss: 0.6194356083869934 \n",
      "     Training Step: 158 Training Loss: 0.6167086958885193 \n",
      "     Training Step: 159 Training Loss: 0.6159399151802063 \n",
      "     Training Step: 160 Training Loss: 0.6154006719589233 \n",
      "     Training Step: 161 Training Loss: 0.6134724617004395 \n",
      "     Training Step: 162 Training Loss: 0.6121652126312256 \n",
      "     Training Step: 163 Training Loss: 0.6154383420944214 \n",
      "     Training Step: 164 Training Loss: 0.6171525120735168 \n",
      "     Training Step: 165 Training Loss: 0.6144981980323792 \n",
      "     Training Step: 166 Training Loss: 0.6115407943725586 \n",
      "     Training Step: 167 Training Loss: 0.6138933897018433 \n",
      "     Training Step: 168 Training Loss: 0.6174811124801636 \n",
      "     Training Step: 169 Training Loss: 0.6107398271560669 \n",
      "     Training Step: 170 Training Loss: 0.6167165040969849 \n",
      "     Training Step: 171 Training Loss: 0.6147254109382629 \n",
      "     Training Step: 172 Training Loss: 0.6139188408851624 \n",
      "     Training Step: 173 Training Loss: 0.6170960068702698 \n",
      "     Training Step: 174 Training Loss: 0.6118279695510864 \n",
      "     Training Step: 175 Training Loss: 0.6082557439804077 \n",
      "     Training Step: 176 Training Loss: 0.6122324466705322 \n",
      "     Training Step: 177 Training Loss: 0.6114014387130737 \n",
      "     Training Step: 178 Training Loss: 0.6161266565322876 \n",
      "     Training Step: 179 Training Loss: 0.6146166920661926 \n",
      "     Training Step: 180 Training Loss: 0.6126376986503601 \n",
      "     Training Step: 181 Training Loss: 0.6137670874595642 \n",
      "     Training Step: 182 Training Loss: 0.614348828792572 \n",
      "     Training Step: 183 Training Loss: 0.6125208735466003 \n",
      "     Training Step: 184 Training Loss: 0.6166318655014038 \n",
      "     Training Step: 185 Training Loss: 0.614605188369751 \n",
      "     Training Step: 186 Training Loss: 0.6144228577613831 \n",
      "     Training Step: 187 Training Loss: 0.6131864190101624 \n",
      "     Training Step: 188 Training Loss: 0.6118344068527222 \n",
      "     Training Step: 189 Training Loss: 0.6176910996437073 \n",
      "     Training Step: 190 Training Loss: 0.6121264696121216 \n",
      "     Training Step: 191 Training Loss: 0.6144785284996033 \n",
      "     Training Step: 192 Training Loss: 0.6154075264930725 \n",
      "     Training Step: 193 Training Loss: 0.6188571453094482 \n",
      "     Training Step: 194 Training Loss: 0.6111749410629272 \n",
      "     Training Step: 195 Training Loss: 0.6144455671310425 \n",
      "     Training Step: 196 Training Loss: 0.6127137541770935 \n",
      "     Training Step: 197 Training Loss: 0.6166937351226807 \n",
      "     Training Step: 198 Training Loss: 0.6123893857002258 \n",
      "     Training Step: 199 Training Loss: 0.617184042930603 \n",
      "     Training Step: 200 Training Loss: 0.6138167977333069 \n",
      "     Training Step: 201 Training Loss: 0.614151120185852 \n",
      "     Training Step: 202 Training Loss: 0.6182159781455994 \n",
      "     Training Step: 203 Training Loss: 0.6143414378166199 \n",
      "     Training Step: 204 Training Loss: 0.6118058562278748 \n",
      "     Training Step: 205 Training Loss: 0.6130635738372803 \n",
      "     Training Step: 206 Training Loss: 0.6132789850234985 \n",
      "     Training Step: 207 Training Loss: 0.6114392280578613 \n",
      "     Training Step: 208 Training Loss: 0.6132786870002747 \n",
      "     Training Step: 209 Training Loss: 0.6198782920837402 \n",
      "     Training Step: 210 Training Loss: 0.6125321388244629 \n",
      "     Training Step: 211 Training Loss: 0.6188395619392395 \n",
      "     Training Step: 212 Training Loss: 0.615602970123291 \n",
      "     Training Step: 213 Training Loss: 0.6140141487121582 \n",
      "     Training Step: 214 Training Loss: 0.6141533851623535 \n",
      "     Training Step: 215 Training Loss: 0.6149159073829651 \n",
      "     Training Step: 216 Training Loss: 0.6150647401809692 \n",
      "     Training Step: 217 Training Loss: 0.6125283241271973 \n",
      "     Training Step: 218 Training Loss: 0.6150919198989868 \n",
      "     Training Step: 219 Training Loss: 0.6129741072654724 \n",
      "     Training Step: 220 Training Loss: 0.6124668717384338 \n",
      "     Training Step: 221 Training Loss: 0.6149550080299377 \n",
      "     Training Step: 222 Training Loss: 0.6152808666229248 \n",
      "     Training Step: 223 Training Loss: 0.6094651818275452 \n",
      "     Training Step: 224 Training Loss: 0.6130650639533997 \n",
      "     Training Step: 225 Training Loss: 0.6116407513618469 \n",
      "     Training Step: 226 Training Loss: 0.6111945509910583 \n",
      "     Training Step: 227 Training Loss: 0.6146670579910278 \n",
      "     Training Step: 228 Training Loss: 0.6133518815040588 \n",
      "     Training Step: 229 Training Loss: 0.6181354522705078 \n",
      "     Training Step: 230 Training Loss: 0.6148003935813904 \n",
      "     Training Step: 231 Training Loss: 0.6155596971511841 \n",
      "     Training Step: 232 Training Loss: 0.6171621680259705 \n",
      "     Training Step: 233 Training Loss: 0.6122793555259705 \n",
      "     Training Step: 234 Training Loss: 0.6209399104118347 \n",
      "     Training Step: 235 Training Loss: 0.6128717064857483 \n",
      "     Training Step: 236 Training Loss: 0.6128467917442322 \n",
      "     Training Step: 237 Training Loss: 0.6122395992279053 \n",
      "     Training Step: 238 Training Loss: 0.6164116859436035 \n",
      "     Training Step: 239 Training Loss: 0.6158303022384644 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6121512055397034 \n",
      "     Validation Step: 1 Validation Loss: 0.6136561036109924 \n",
      "     Validation Step: 2 Validation Loss: 0.6175832748413086 \n",
      "     Validation Step: 3 Validation Loss: 0.6159821152687073 \n",
      "     Validation Step: 4 Validation Loss: 0.6184512972831726 \n",
      "     Validation Step: 5 Validation Loss: 0.6141253709793091 \n",
      "     Validation Step: 6 Validation Loss: 0.611191987991333 \n",
      "     Validation Step: 7 Validation Loss: 0.6155642867088318 \n",
      "     Validation Step: 8 Validation Loss: 0.6130066514015198 \n",
      "     Validation Step: 9 Validation Loss: 0.6145681738853455 \n",
      "     Validation Step: 10 Validation Loss: 0.6182073354721069 \n",
      "     Validation Step: 11 Validation Loss: 0.6180431842803955 \n",
      "     Validation Step: 12 Validation Loss: 0.6136789917945862 \n",
      "     Validation Step: 13 Validation Loss: 0.6152259707450867 \n",
      "     Validation Step: 14 Validation Loss: 0.6150398254394531 \n",
      "     Validation Step: 15 Validation Loss: 0.6156108975410461 \n",
      "     Validation Step: 16 Validation Loss: 0.61018306016922 \n",
      "     Validation Step: 17 Validation Loss: 0.6119071245193481 \n",
      "     Validation Step: 18 Validation Loss: 0.610546350479126 \n",
      "     Validation Step: 19 Validation Loss: 0.6183232069015503 \n",
      "     Validation Step: 20 Validation Loss: 0.6184693574905396 \n",
      "     Validation Step: 21 Validation Loss: 0.616237998008728 \n",
      "     Validation Step: 22 Validation Loss: 0.6176797151565552 \n",
      "     Validation Step: 23 Validation Loss: 0.6115813851356506 \n",
      "     Validation Step: 24 Validation Loss: 0.6146487593650818 \n",
      "     Validation Step: 25 Validation Loss: 0.6145420670509338 \n",
      "     Validation Step: 26 Validation Loss: 0.6101464629173279 \n",
      "     Validation Step: 27 Validation Loss: 0.6172888278961182 \n",
      "     Validation Step: 28 Validation Loss: 0.6101688742637634 \n",
      "     Validation Step: 29 Validation Loss: 0.6148437261581421 \n",
      "     Validation Step: 30 Validation Loss: 0.6142125725746155 \n",
      "     Validation Step: 31 Validation Loss: 0.6157914996147156 \n",
      "     Validation Step: 32 Validation Loss: 0.6148964762687683 \n",
      "     Validation Step: 33 Validation Loss: 0.6136475205421448 \n",
      "     Validation Step: 34 Validation Loss: 0.6106559038162231 \n",
      "     Validation Step: 35 Validation Loss: 0.6128491759300232 \n",
      "     Validation Step: 36 Validation Loss: 0.6075712442398071 \n",
      "     Validation Step: 37 Validation Loss: 0.6116614937782288 \n",
      "     Validation Step: 38 Validation Loss: 0.6153194904327393 \n",
      "     Validation Step: 39 Validation Loss: 0.6133186221122742 \n",
      "     Validation Step: 40 Validation Loss: 0.6105102896690369 \n",
      "     Validation Step: 41 Validation Loss: 0.6170036792755127 \n",
      "     Validation Step: 42 Validation Loss: 0.6141442060470581 \n",
      "     Validation Step: 43 Validation Loss: 0.6111999750137329 \n",
      "     Validation Step: 44 Validation Loss: 0.6142613887786865 \n",
      "Epoch: 129\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6177768111228943 \n",
      "     Training Step: 1 Training Loss: 0.6134286522865295 \n",
      "     Training Step: 2 Training Loss: 0.6171435713768005 \n",
      "     Training Step: 3 Training Loss: 0.6159927845001221 \n",
      "     Training Step: 4 Training Loss: 0.6166662573814392 \n",
      "     Training Step: 5 Training Loss: 0.6157563924789429 \n",
      "     Training Step: 6 Training Loss: 0.6166362166404724 \n",
      "     Training Step: 7 Training Loss: 0.6121623516082764 \n",
      "     Training Step: 8 Training Loss: 0.6154190301895142 \n",
      "     Training Step: 9 Training Loss: 0.6144492030143738 \n",
      "     Training Step: 10 Training Loss: 0.610011875629425 \n",
      "     Training Step: 11 Training Loss: 0.6198461651802063 \n",
      "     Training Step: 12 Training Loss: 0.6106962561607361 \n",
      "     Training Step: 13 Training Loss: 0.6135154962539673 \n",
      "     Training Step: 14 Training Loss: 0.6138848662376404 \n",
      "     Training Step: 15 Training Loss: 0.611431360244751 \n",
      "     Training Step: 16 Training Loss: 0.618887186050415 \n",
      "     Training Step: 17 Training Loss: 0.6105716824531555 \n",
      "     Training Step: 18 Training Loss: 0.6103894710540771 \n",
      "     Training Step: 19 Training Loss: 0.6157035231590271 \n",
      "     Training Step: 20 Training Loss: 0.6118302941322327 \n",
      "     Training Step: 21 Training Loss: 0.6125295758247375 \n",
      "     Training Step: 22 Training Loss: 0.6123703122138977 \n",
      "     Training Step: 23 Training Loss: 0.6132729649543762 \n",
      "     Training Step: 24 Training Loss: 0.6146156191825867 \n",
      "     Training Step: 25 Training Loss: 0.614346444606781 \n",
      "     Training Step: 26 Training Loss: 0.613183856010437 \n",
      "     Training Step: 27 Training Loss: 0.6128394603729248 \n",
      "     Training Step: 28 Training Loss: 0.6169431209564209 \n",
      "     Training Step: 29 Training Loss: 0.6129633784294128 \n",
      "     Training Step: 30 Training Loss: 0.611778974533081 \n",
      "     Training Step: 31 Training Loss: 0.6127514839172363 \n",
      "     Training Step: 32 Training Loss: 0.6142836213111877 \n",
      "     Training Step: 33 Training Loss: 0.6185985207557678 \n",
      "     Training Step: 34 Training Loss: 0.6114209890365601 \n",
      "     Training Step: 35 Training Loss: 0.6142462491989136 \n",
      "     Training Step: 36 Training Loss: 0.6178011298179626 \n",
      "     Training Step: 37 Training Loss: 0.6129133701324463 \n",
      "     Training Step: 38 Training Loss: 0.6135929226875305 \n",
      "     Training Step: 39 Training Loss: 0.6115407347679138 \n",
      "     Training Step: 40 Training Loss: 0.6141569018363953 \n",
      "     Training Step: 41 Training Loss: 0.6121276617050171 \n",
      "     Training Step: 42 Training Loss: 0.6140711903572083 \n",
      "     Training Step: 43 Training Loss: 0.613497793674469 \n",
      "     Training Step: 44 Training Loss: 0.609396755695343 \n",
      "     Training Step: 45 Training Loss: 0.614104688167572 \n",
      "     Training Step: 46 Training Loss: 0.6153443455696106 \n",
      "     Training Step: 47 Training Loss: 0.6101220846176147 \n",
      "     Training Step: 48 Training Loss: 0.613054096698761 \n",
      "     Training Step: 49 Training Loss: 0.6137475967407227 \n",
      "     Training Step: 50 Training Loss: 0.6101572513580322 \n",
      "     Training Step: 51 Training Loss: 0.6136143803596497 \n",
      "     Training Step: 52 Training Loss: 0.6137444972991943 \n",
      "     Training Step: 53 Training Loss: 0.6131359934806824 \n",
      "     Training Step: 54 Training Loss: 0.6147081851959229 \n",
      "     Training Step: 55 Training Loss: 0.6105753183364868 \n",
      "     Training Step: 56 Training Loss: 0.6175236105918884 \n",
      "     Training Step: 57 Training Loss: 0.611685574054718 \n",
      "     Training Step: 58 Training Loss: 0.6166988611221313 \n",
      "     Training Step: 59 Training Loss: 0.6150922179222107 \n",
      "     Training Step: 60 Training Loss: 0.6157463192939758 \n",
      "     Training Step: 61 Training Loss: 0.6100454330444336 \n",
      "     Training Step: 62 Training Loss: 0.6140218377113342 \n",
      "     Training Step: 63 Training Loss: 0.6182136535644531 \n",
      "     Training Step: 64 Training Loss: 0.6123067736625671 \n",
      "     Training Step: 65 Training Loss: 0.6116536259651184 \n",
      "     Training Step: 66 Training Loss: 0.60973060131073 \n",
      "     Training Step: 67 Training Loss: 0.6139209866523743 \n",
      "     Training Step: 68 Training Loss: 0.613278865814209 \n",
      "     Training Step: 69 Training Loss: 0.6122352480888367 \n",
      "     Training Step: 70 Training Loss: 0.6144511699676514 \n",
      "     Training Step: 71 Training Loss: 0.6194648742675781 \n",
      "     Training Step: 72 Training Loss: 0.6116016507148743 \n",
      "     Training Step: 73 Training Loss: 0.6153971552848816 \n",
      "     Training Step: 74 Training Loss: 0.6166321039199829 \n",
      "     Training Step: 75 Training Loss: 0.6118213534355164 \n",
      "     Training Step: 76 Training Loss: 0.6126890182495117 \n",
      "     Training Step: 77 Training Loss: 0.6122397780418396 \n",
      "     Training Step: 78 Training Loss: 0.6151131987571716 \n",
      "     Training Step: 79 Training Loss: 0.618050217628479 \n",
      "     Training Step: 80 Training Loss: 0.6118053793907166 \n",
      "     Training Step: 81 Training Loss: 0.6129165887832642 \n",
      "     Training Step: 82 Training Loss: 0.6115507483482361 \n",
      "     Training Step: 83 Training Loss: 0.6146636009216309 \n",
      "     Training Step: 84 Training Loss: 0.6116287708282471 \n",
      "     Training Step: 85 Training Loss: 0.6133114099502563 \n",
      "     Training Step: 86 Training Loss: 0.6184690594673157 \n",
      "     Training Step: 87 Training Loss: 0.613636314868927 \n",
      "     Training Step: 88 Training Loss: 0.6196997761726379 \n",
      "     Training Step: 89 Training Loss: 0.6140329837799072 \n",
      "     Training Step: 90 Training Loss: 0.6094692349433899 \n",
      "     Training Step: 91 Training Loss: 0.6118834018707275 \n",
      "     Training Step: 92 Training Loss: 0.6153713464736938 \n",
      "     Training Step: 93 Training Loss: 0.611199140548706 \n",
      "     Training Step: 94 Training Loss: 0.6181036233901978 \n",
      "     Training Step: 95 Training Loss: 0.6121523976325989 \n",
      "     Training Step: 96 Training Loss: 0.6155332922935486 \n",
      "     Training Step: 97 Training Loss: 0.6177651882171631 \n",
      "     Training Step: 98 Training Loss: 0.6137633323669434 \n",
      "     Training Step: 99 Training Loss: 0.6107385754585266 \n",
      "     Training Step: 100 Training Loss: 0.6164045929908752 \n",
      "     Training Step: 101 Training Loss: 0.6152841448783875 \n",
      "     Training Step: 102 Training Loss: 0.6132012009620667 \n",
      "     Training Step: 103 Training Loss: 0.6132911443710327 \n",
      "     Training Step: 104 Training Loss: 0.6128000617027283 \n",
      "     Training Step: 105 Training Loss: 0.6138176918029785 \n",
      "     Training Step: 106 Training Loss: 0.613459050655365 \n",
      "     Training Step: 107 Training Loss: 0.6156826019287109 \n",
      "     Training Step: 108 Training Loss: 0.6168051362037659 \n",
      "     Training Step: 109 Training Loss: 0.6133003234863281 \n",
      "     Training Step: 110 Training Loss: 0.6103729605674744 \n",
      "     Training Step: 111 Training Loss: 0.619625985622406 \n",
      "     Training Step: 112 Training Loss: 0.6146767139434814 \n",
      "     Training Step: 113 Training Loss: 0.616698145866394 \n",
      "     Training Step: 114 Training Loss: 0.6139268279075623 \n",
      "     Training Step: 115 Training Loss: 0.6148651242256165 \n",
      "     Training Step: 116 Training Loss: 0.6150669455528259 \n",
      "     Training Step: 117 Training Loss: 0.6105836629867554 \n",
      "     Training Step: 118 Training Loss: 0.6109176874160767 \n",
      "     Training Step: 119 Training Loss: 0.6155133843421936 \n",
      "     Training Step: 120 Training Loss: 0.6142160296440125 \n",
      "     Training Step: 121 Training Loss: 0.6185703277587891 \n",
      "     Training Step: 122 Training Loss: 0.611400842666626 \n",
      "     Training Step: 123 Training Loss: 0.614496111869812 \n",
      "     Training Step: 124 Training Loss: 0.6118269562721252 \n",
      "     Training Step: 125 Training Loss: 0.6164229512214661 \n",
      "     Training Step: 126 Training Loss: 0.6133577823638916 \n",
      "     Training Step: 127 Training Loss: 0.6152371764183044 \n",
      "     Training Step: 128 Training Loss: 0.6168075203895569 \n",
      "     Training Step: 129 Training Loss: 0.6120110154151917 \n",
      "     Training Step: 130 Training Loss: 0.6152868866920471 \n",
      "     Training Step: 131 Training Loss: 0.6105688810348511 \n",
      "     Training Step: 132 Training Loss: 0.6122826337814331 \n",
      "     Training Step: 133 Training Loss: 0.6147475242614746 \n",
      "     Training Step: 134 Training Loss: 0.6184175610542297 \n",
      "     Training Step: 135 Training Loss: 0.6171983480453491 \n",
      "     Training Step: 136 Training Loss: 0.613646388053894 \n",
      "     Training Step: 137 Training Loss: 0.6123737692832947 \n",
      "     Training Step: 138 Training Loss: 0.6188321113586426 \n",
      "     Training Step: 139 Training Loss: 0.6144198179244995 \n",
      "     Training Step: 140 Training Loss: 0.6180610060691833 \n",
      "     Training Step: 141 Training Loss: 0.6170852184295654 \n",
      "     Training Step: 142 Training Loss: 0.6149556636810303 \n",
      "     Training Step: 143 Training Loss: 0.616883397102356 \n",
      "     Training Step: 144 Training Loss: 0.6125112771987915 \n",
      "     Training Step: 145 Training Loss: 0.6158286929130554 \n",
      "     Training Step: 146 Training Loss: 0.6157793998718262 \n",
      "     Training Step: 147 Training Loss: 0.6163408160209656 \n",
      "     Training Step: 148 Training Loss: 0.6149196028709412 \n",
      "     Training Step: 149 Training Loss: 0.6147995591163635 \n",
      "     Training Step: 150 Training Loss: 0.6167048811912537 \n",
      "     Training Step: 151 Training Loss: 0.6083041429519653 \n",
      "     Training Step: 152 Training Loss: 0.6120796799659729 \n",
      "     Training Step: 153 Training Loss: 0.6171355247497559 \n",
      "     Training Step: 154 Training Loss: 0.6143379807472229 \n",
      "     Training Step: 155 Training Loss: 0.6147282719612122 \n",
      "     Training Step: 156 Training Loss: 0.6115274429321289 \n",
      "     Training Step: 157 Training Loss: 0.6107133626937866 \n",
      "     Training Step: 158 Training Loss: 0.6165095567703247 \n",
      "     Training Step: 159 Training Loss: 0.6162181496620178 \n",
      "     Training Step: 160 Training Loss: 0.6097168326377869 \n",
      "     Training Step: 161 Training Loss: 0.6182568669319153 \n",
      "     Training Step: 162 Training Loss: 0.616218090057373 \n",
      "     Training Step: 163 Training Loss: 0.6162426471710205 \n",
      "     Training Step: 164 Training Loss: 0.6114333271980286 \n",
      "     Training Step: 165 Training Loss: 0.6155354976654053 \n",
      "     Training Step: 166 Training Loss: 0.6160314083099365 \n",
      "     Training Step: 167 Training Loss: 0.6167131066322327 \n",
      "     Training Step: 168 Training Loss: 0.6144784092903137 \n",
      "     Training Step: 169 Training Loss: 0.6146774291992188 \n",
      "     Training Step: 170 Training Loss: 0.6147762537002563 \n",
      "     Training Step: 171 Training Loss: 0.6100980639457703 \n",
      "     Training Step: 172 Training Loss: 0.6124822497367859 \n",
      "     Training Step: 173 Training Loss: 0.6183019876480103 \n",
      "     Training Step: 174 Training Loss: 0.6159400343894958 \n",
      "     Training Step: 175 Training Loss: 0.6146342754364014 \n",
      "     Training Step: 176 Training Loss: 0.6121686697006226 \n",
      "     Training Step: 177 Training Loss: 0.6184307932853699 \n",
      "     Training Step: 178 Training Loss: 0.6166480183601379 \n",
      "     Training Step: 179 Training Loss: 0.6151716709136963 \n",
      "     Training Step: 180 Training Loss: 0.6153742671012878 \n",
      "     Training Step: 181 Training Loss: 0.6141525506973267 \n",
      "     Training Step: 182 Training Loss: 0.6154399514198303 \n",
      "     Training Step: 183 Training Loss: 0.6151577830314636 \n",
      "     Training Step: 184 Training Loss: 0.6100743412971497 \n",
      "     Training Step: 185 Training Loss: 0.6104924082756042 \n",
      "     Training Step: 186 Training Loss: 0.6123858690261841 \n",
      "     Training Step: 187 Training Loss: 0.611622154712677 \n",
      "     Training Step: 188 Training Loss: 0.6128684282302856 \n",
      "     Training Step: 189 Training Loss: 0.6114010810852051 \n",
      "     Training Step: 190 Training Loss: 0.6176732778549194 \n",
      "     Training Step: 191 Training Loss: 0.6147139668464661 \n",
      "     Training Step: 192 Training Loss: 0.6143842339515686 \n",
      "     Training Step: 193 Training Loss: 0.6146146655082703 \n",
      "     Training Step: 194 Training Loss: 0.615268349647522 \n",
      "     Training Step: 195 Training Loss: 0.6173901557922363 \n",
      "     Training Step: 196 Training Loss: 0.6092293858528137 \n",
      "     Training Step: 197 Training Loss: 0.6115275621414185 \n",
      "     Training Step: 198 Training Loss: 0.6125204563140869 \n",
      "     Training Step: 199 Training Loss: 0.6167552471160889 \n",
      "     Training Step: 200 Training Loss: 0.6180151104927063 \n",
      "     Training Step: 201 Training Loss: 0.6125348806381226 \n",
      "     Training Step: 202 Training Loss: 0.6126471161842346 \n",
      "     Training Step: 203 Training Loss: 0.6122364401817322 \n",
      "     Training Step: 204 Training Loss: 0.6152949929237366 \n",
      "     Training Step: 205 Training Loss: 0.6161144971847534 \n",
      "     Training Step: 206 Training Loss: 0.614208459854126 \n",
      "     Training Step: 207 Training Loss: 0.6128735542297363 \n",
      "     Training Step: 208 Training Loss: 0.6145964860916138 \n",
      "     Training Step: 209 Training Loss: 0.616797685623169 \n",
      "     Training Step: 210 Training Loss: 0.609736442565918 \n",
      "     Training Step: 211 Training Loss: 0.6146998405456543 \n",
      "     Training Step: 212 Training Loss: 0.6111488938331604 \n",
      "     Training Step: 213 Training Loss: 0.6177040338516235 \n",
      "     Training Step: 214 Training Loss: 0.6209306716918945 \n",
      "     Training Step: 215 Training Loss: 0.6149283647537231 \n",
      "     Training Step: 216 Training Loss: 0.6154727935791016 \n",
      "     Training Step: 217 Training Loss: 0.611592173576355 \n",
      "     Training Step: 218 Training Loss: 0.6143422722816467 \n",
      "     Training Step: 219 Training Loss: 0.6106680035591125 \n",
      "     Training Step: 220 Training Loss: 0.6132033467292786 \n",
      "     Training Step: 221 Training Loss: 0.6140159964561462 \n",
      "     Training Step: 222 Training Loss: 0.6104945540428162 \n",
      "     Training Step: 223 Training Loss: 0.6130658984184265 \n",
      "     Training Step: 224 Training Loss: 0.6114683747291565 \n",
      "     Training Step: 225 Training Loss: 0.6131145358085632 \n",
      "     Training Step: 226 Training Loss: 0.6108633875846863 \n",
      "     Training Step: 227 Training Loss: 0.6146649122238159 \n",
      "     Training Step: 228 Training Loss: 0.6177249550819397 \n",
      "     Training Step: 229 Training Loss: 0.6202789545059204 \n",
      "     Training Step: 230 Training Loss: 0.6127617359161377 \n",
      "     Training Step: 231 Training Loss: 0.6157539486885071 \n",
      "     Training Step: 232 Training Loss: 0.6111536026000977 \n",
      "     Training Step: 233 Training Loss: 0.6134694814682007 \n",
      "     Training Step: 234 Training Loss: 0.6167083382606506 \n",
      "     Training Step: 235 Training Loss: 0.6154122948646545 \n",
      "     Training Step: 236 Training Loss: 0.6171364784240723 \n",
      "     Training Step: 237 Training Loss: 0.6155961155891418 \n",
      "     Training Step: 238 Training Loss: 0.6118209362030029 \n",
      "     Training Step: 239 Training Loss: 0.6176759004592896 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6112080812454224 \n",
      "     Validation Step: 1 Validation Loss: 0.6148460507392883 \n",
      "     Validation Step: 2 Validation Loss: 0.6141310334205627 \n",
      "     Validation Step: 3 Validation Loss: 0.6105555891990662 \n",
      "     Validation Step: 4 Validation Loss: 0.6153208017349243 \n",
      "     Validation Step: 5 Validation Loss: 0.6101956367492676 \n",
      "     Validation Step: 6 Validation Loss: 0.6184647083282471 \n",
      "     Validation Step: 7 Validation Loss: 0.6146548390388489 \n",
      "     Validation Step: 8 Validation Loss: 0.6141496300697327 \n",
      "     Validation Step: 9 Validation Loss: 0.6136830449104309 \n",
      "     Validation Step: 10 Validation Loss: 0.6145486831665039 \n",
      "     Validation Step: 11 Validation Loss: 0.6128583550453186 \n",
      "     Validation Step: 12 Validation Loss: 0.6183190941810608 \n",
      "     Validation Step: 13 Validation Loss: 0.6136549115180969 \n",
      "     Validation Step: 14 Validation Loss: 0.6182029247283936 \n",
      "     Validation Step: 15 Validation Loss: 0.6136556267738342 \n",
      "     Validation Step: 16 Validation Loss: 0.6180397868156433 \n",
      "     Validation Step: 17 Validation Loss: 0.6116707921028137 \n",
      "     Validation Step: 18 Validation Loss: 0.6157885193824768 \n",
      "     Validation Step: 19 Validation Loss: 0.6111981868743896 \n",
      "     Validation Step: 20 Validation Loss: 0.61700040102005 \n",
      "     Validation Step: 21 Validation Loss: 0.6162390112876892 \n",
      "     Validation Step: 22 Validation Loss: 0.6156100034713745 \n",
      "     Validation Step: 23 Validation Loss: 0.6175851821899414 \n",
      "     Validation Step: 24 Validation Loss: 0.611589252948761 \n",
      "     Validation Step: 25 Validation Loss: 0.6106644868850708 \n",
      "     Validation Step: 26 Validation Loss: 0.6130130290985107 \n",
      "     Validation Step: 27 Validation Loss: 0.612159252166748 \n",
      "     Validation Step: 28 Validation Loss: 0.6176735162734985 \n",
      "     Validation Step: 29 Validation Loss: 0.6133288741111755 \n",
      "     Validation Step: 30 Validation Loss: 0.6172850131988525 \n",
      "     Validation Step: 31 Validation Loss: 0.6159765720367432 \n",
      "     Validation Step: 32 Validation Loss: 0.618448793888092 \n",
      "     Validation Step: 33 Validation Loss: 0.6142684817314148 \n",
      "     Validation Step: 34 Validation Loss: 0.6152257323265076 \n",
      "     Validation Step: 35 Validation Loss: 0.6105209589004517 \n",
      "     Validation Step: 36 Validation Loss: 0.6155633926391602 \n",
      "     Validation Step: 37 Validation Loss: 0.6142175197601318 \n",
      "     Validation Step: 38 Validation Loss: 0.6150394678115845 \n",
      "     Validation Step: 39 Validation Loss: 0.610159695148468 \n",
      "     Validation Step: 40 Validation Loss: 0.6119149923324585 \n",
      "     Validation Step: 41 Validation Loss: 0.6101831197738647 \n",
      "     Validation Step: 42 Validation Loss: 0.6075897216796875 \n",
      "     Validation Step: 43 Validation Loss: 0.6149003505706787 \n",
      "     Validation Step: 44 Validation Loss: 0.6145692467689514 \n",
      "Epoch: 130\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6148741245269775 \n",
      "     Training Step: 1 Training Loss: 0.6155316233634949 \n",
      "     Training Step: 2 Training Loss: 0.6198217868804932 \n",
      "     Training Step: 3 Training Loss: 0.6109343767166138 \n",
      "     Training Step: 4 Training Loss: 0.6146819591522217 \n",
      "     Training Step: 5 Training Loss: 0.6123151779174805 \n",
      "     Training Step: 6 Training Loss: 0.6137493252754211 \n",
      "     Training Step: 7 Training Loss: 0.6142845749855042 \n",
      "     Training Step: 8 Training Loss: 0.6129108667373657 \n",
      "     Training Step: 9 Training Loss: 0.6116217970848083 \n",
      "     Training Step: 10 Training Loss: 0.6185953617095947 \n",
      "     Training Step: 11 Training Loss: 0.6171624660491943 \n",
      "     Training Step: 12 Training Loss: 0.61460942029953 \n",
      "     Training Step: 13 Training Loss: 0.6150882244110107 \n",
      "     Training Step: 14 Training Loss: 0.6152984499931335 \n",
      "     Training Step: 15 Training Loss: 0.6159511208534241 \n",
      "     Training Step: 16 Training Loss: 0.6166565418243408 \n",
      "     Training Step: 17 Training Loss: 0.6150665879249573 \n",
      "     Training Step: 18 Training Loss: 0.6111542582511902 \n",
      "     Training Step: 19 Training Loss: 0.6171325445175171 \n",
      "     Training Step: 20 Training Loss: 0.6146368384361267 \n",
      "     Training Step: 21 Training Loss: 0.6125308275222778 \n",
      "     Training Step: 22 Training Loss: 0.611609160900116 \n",
      "     Training Step: 23 Training Loss: 0.6118424534797668 \n",
      "     Training Step: 24 Training Loss: 0.6140394806861877 \n",
      "     Training Step: 25 Training Loss: 0.6178043484687805 \n",
      "     Training Step: 26 Training Loss: 0.6130616068840027 \n",
      "     Training Step: 27 Training Loss: 0.6105855107307434 \n",
      "     Training Step: 28 Training Loss: 0.6144529581069946 \n",
      "     Training Step: 29 Training Loss: 0.6153904795646667 \n",
      "     Training Step: 30 Training Loss: 0.6115170121192932 \n",
      "     Training Step: 31 Training Loss: 0.6132732629776001 \n",
      "     Training Step: 32 Training Loss: 0.6152433753013611 \n",
      "     Training Step: 33 Training Loss: 0.6169164776802063 \n",
      "     Training Step: 34 Training Loss: 0.613915741443634 \n",
      "     Training Step: 35 Training Loss: 0.6147564649581909 \n",
      "     Training Step: 36 Training Loss: 0.6188766360282898 \n",
      "     Training Step: 37 Training Loss: 0.6100279688835144 \n",
      "     Training Step: 38 Training Loss: 0.6121565103530884 \n",
      "     Training Step: 39 Training Loss: 0.6157568097114563 \n",
      "     Training Step: 40 Training Loss: 0.6106574535369873 \n",
      "     Training Step: 41 Training Loss: 0.6115472316741943 \n",
      "     Training Step: 42 Training Loss: 0.6121577620506287 \n",
      "     Training Step: 43 Training Loss: 0.614702045917511 \n",
      "     Training Step: 44 Training Loss: 0.6093899011611938 \n",
      "     Training Step: 45 Training Loss: 0.6182352900505066 \n",
      "     Training Step: 46 Training Loss: 0.6123692393302917 \n",
      "     Training Step: 47 Training Loss: 0.6121351718902588 \n",
      "     Training Step: 48 Training Loss: 0.6147720813751221 \n",
      "     Training Step: 49 Training Loss: 0.616041362285614 \n",
      "     Training Step: 50 Training Loss: 0.6128401160240173 \n",
      "     Training Step: 51 Training Loss: 0.6171051263809204 \n",
      "     Training Step: 52 Training Loss: 0.6152689456939697 \n",
      "     Training Step: 53 Training Loss: 0.6171365976333618 \n",
      "     Training Step: 54 Training Loss: 0.6115331649780273 \n",
      "     Training Step: 55 Training Loss: 0.6122382879257202 \n",
      "     Training Step: 56 Training Loss: 0.6177530884742737 \n",
      "     Training Step: 57 Training Loss: 0.6167101263999939 \n",
      "     Training Step: 58 Training Loss: 0.6114338636398315 \n",
      "     Training Step: 59 Training Loss: 0.6164054274559021 \n",
      "     Training Step: 60 Training Loss: 0.6118485927581787 \n",
      "     Training Step: 61 Training Loss: 0.6107527017593384 \n",
      "     Training Step: 62 Training Loss: 0.6142204403877258 \n",
      "     Training Step: 63 Training Loss: 0.6121288537979126 \n",
      "     Training Step: 64 Training Loss: 0.6166986227035522 \n",
      "     Training Step: 65 Training Loss: 0.6137614250183105 \n",
      "     Training Step: 66 Training Loss: 0.6154418587684631 \n",
      "     Training Step: 67 Training Loss: 0.6155378818511963 \n",
      "     Training Step: 68 Training Loss: 0.610683798789978 \n",
      "     Training Step: 69 Training Loss: 0.6135881543159485 \n",
      "     Training Step: 70 Training Loss: 0.6147046685218811 \n",
      "     Training Step: 71 Training Loss: 0.6082548499107361 \n",
      "     Training Step: 72 Training Loss: 0.6177108287811279 \n",
      "     Training Step: 73 Training Loss: 0.6104905605316162 \n",
      "     Training Step: 74 Training Loss: 0.6114263534545898 \n",
      "     Training Step: 75 Training Loss: 0.6097180843353271 \n",
      "     Training Step: 76 Training Loss: 0.611771821975708 \n",
      "     Training Step: 77 Training Loss: 0.6154320240020752 \n",
      "     Training Step: 78 Training Loss: 0.6184805035591125 \n",
      "     Training Step: 79 Training Loss: 0.6178236603736877 \n",
      "     Training Step: 80 Training Loss: 0.616817831993103 \n",
      "     Training Step: 81 Training Loss: 0.6131360530853271 \n",
      "     Training Step: 82 Training Loss: 0.6122271418571472 \n",
      "     Training Step: 83 Training Loss: 0.6104036569595337 \n",
      "     Training Step: 84 Training Loss: 0.6188634037971497 \n",
      "     Training Step: 85 Training Loss: 0.6125390529632568 \n",
      "     Training Step: 86 Training Loss: 0.6100739240646362 \n",
      "     Training Step: 87 Training Loss: 0.61060631275177 \n",
      "     Training Step: 88 Training Loss: 0.617372989654541 \n",
      "     Training Step: 89 Training Loss: 0.6123819351196289 \n",
      "     Training Step: 90 Training Loss: 0.6116308569908142 \n",
      "     Training Step: 91 Training Loss: 0.6168091297149658 \n",
      "     Training Step: 92 Training Loss: 0.6209421753883362 \n",
      "     Training Step: 93 Training Loss: 0.6129623055458069 \n",
      "     Training Step: 94 Training Loss: 0.6116869449615479 \n",
      "     Training Step: 95 Training Loss: 0.6140744090080261 \n",
      "     Training Step: 96 Training Loss: 0.6128640174865723 \n",
      "     Training Step: 97 Training Loss: 0.6152822971343994 \n",
      "     Training Step: 98 Training Loss: 0.6132900714874268 \n",
      "     Training Step: 99 Training Loss: 0.6137452125549316 \n",
      "     Training Step: 100 Training Loss: 0.614956796169281 \n",
      "     Training Step: 101 Training Loss: 0.6157848834991455 \n",
      "     Training Step: 102 Training Loss: 0.61188143491745 \n",
      "     Training Step: 103 Training Loss: 0.615514874458313 \n",
      "     Training Step: 104 Training Loss: 0.6112005114555359 \n",
      "     Training Step: 105 Training Loss: 0.6147933006286621 \n",
      "     Training Step: 106 Training Loss: 0.6126994490623474 \n",
      "     Training Step: 107 Training Loss: 0.6157444715499878 \n",
      "     Training Step: 108 Training Loss: 0.6184700131416321 \n",
      "     Training Step: 109 Training Loss: 0.614495575428009 \n",
      "     Training Step: 110 Training Loss: 0.6142095923423767 \n",
      "     Training Step: 111 Training Loss: 0.6159968376159668 \n",
      "     Training Step: 112 Training Loss: 0.6132059097290039 \n",
      "     Training Step: 113 Training Loss: 0.6149153709411621 \n",
      "     Training Step: 114 Training Loss: 0.6125377416610718 \n",
      "     Training Step: 115 Training Loss: 0.6118114590644836 \n",
      "     Training Step: 116 Training Loss: 0.6131882667541504 \n",
      "     Training Step: 117 Training Loss: 0.6120154857635498 \n",
      "     Training Step: 118 Training Loss: 0.6156796216964722 \n",
      "     Training Step: 119 Training Loss: 0.6136359572410583 \n",
      "     Training Step: 120 Training Loss: 0.6134566068649292 \n",
      "     Training Step: 121 Training Loss: 0.6144394278526306 \n",
      "     Training Step: 122 Training Loss: 0.6107163429260254 \n",
      "     Training Step: 123 Training Loss: 0.6161211133003235 \n",
      "     Training Step: 124 Training Loss: 0.615309476852417 \n",
      "     Training Step: 125 Training Loss: 0.6131141781806946 \n",
      "     Training Step: 126 Training Loss: 0.6153424382209778 \n",
      "     Training Step: 127 Training Loss: 0.6197142004966736 \n",
      "     Training Step: 128 Training Loss: 0.6176403760910034 \n",
      "     Training Step: 129 Training Loss: 0.6116090416908264 \n",
      "     Training Step: 130 Training Loss: 0.6154078841209412 \n",
      "     Training Step: 131 Training Loss: 0.6136515140533447 \n",
      "     Training Step: 132 Training Loss: 0.6158278584480286 \n",
      "     Training Step: 133 Training Loss: 0.6162286996841431 \n",
      "     Training Step: 134 Training Loss: 0.6140260696411133 \n",
      "     Training Step: 135 Training Loss: 0.6104139089584351 \n",
      "     Training Step: 136 Training Loss: 0.6095020771026611 \n",
      "     Training Step: 137 Training Loss: 0.6166359782218933 \n",
      "     Training Step: 138 Training Loss: 0.6100037693977356 \n",
      "     Training Step: 139 Training Loss: 0.6105731725692749 \n",
      "     Training Step: 140 Training Loss: 0.6132859587669373 \n",
      "     Training Step: 141 Training Loss: 0.6101720929145813 \n",
      "     Training Step: 142 Training Loss: 0.6169566512107849 \n",
      "     Training Step: 143 Training Loss: 0.6151889562606812 \n",
      "     Training Step: 144 Training Loss: 0.6135146021842957 \n",
      "     Training Step: 145 Training Loss: 0.6184563040733337 \n",
      "     Training Step: 146 Training Loss: 0.6100322604179382 \n",
      "     Training Step: 147 Training Loss: 0.6177141070365906 \n",
      "     Training Step: 148 Training Loss: 0.6180503368377686 \n",
      "     Training Step: 149 Training Loss: 0.6142465472221375 \n",
      "     Training Step: 150 Training Loss: 0.6153873801231384 \n",
      "     Training Step: 151 Training Loss: 0.6166505813598633 \n",
      "     Training Step: 152 Training Loss: 0.6196035146713257 \n",
      "     Training Step: 153 Training Loss: 0.6115435361862183 \n",
      "     Training Step: 154 Training Loss: 0.6156885027885437 \n",
      "     Training Step: 155 Training Loss: 0.614736020565033 \n",
      "     Training Step: 156 Training Loss: 0.6106189489364624 \n",
      "     Training Step: 157 Training Loss: 0.6143450736999512 \n",
      "     Training Step: 158 Training Loss: 0.6180123090744019 \n",
      "     Training Step: 159 Training Loss: 0.6097728610038757 \n",
      "     Training Step: 160 Training Loss: 0.6146135330200195 \n",
      "     Training Step: 161 Training Loss: 0.6161922812461853 \n",
      "     Training Step: 162 Training Loss: 0.6101521253585815 \n",
      "     Training Step: 163 Training Loss: 0.613469123840332 \n",
      "     Training Step: 164 Training Loss: 0.6157459616661072 \n",
      "     Training Step: 165 Training Loss: 0.6114310026168823 \n",
      "     Training Step: 166 Training Loss: 0.6162173748016357 \n",
      "     Training Step: 167 Training Loss: 0.6141105890274048 \n",
      "     Training Step: 168 Training Loss: 0.6122851967811584 \n",
      "     Training Step: 169 Training Loss: 0.6134161353111267 \n",
      "     Training Step: 170 Training Loss: 0.6177197694778442 \n",
      "     Training Step: 171 Training Loss: 0.6165180802345276 \n",
      "     Training Step: 172 Training Loss: 0.6138206124305725 \n",
      "     Training Step: 173 Training Loss: 0.6153752207756042 \n",
      "     Training Step: 174 Training Loss: 0.6114040613174438 \n",
      "     Training Step: 175 Training Loss: 0.6144216656684875 \n",
      "     Training Step: 176 Training Loss: 0.6174860000610352 \n",
      "     Training Step: 177 Training Loss: 0.6130673885345459 \n",
      "     Training Step: 178 Training Loss: 0.6163457036018372 \n",
      "     Training Step: 179 Training Loss: 0.6171838641166687 \n",
      "     Training Step: 180 Training Loss: 0.61247718334198 \n",
      "     Training Step: 181 Training Loss: 0.6146930456161499 \n",
      "     Training Step: 182 Training Loss: 0.6167412996292114 \n",
      "     Training Step: 183 Training Loss: 0.6182863116264343 \n",
      "     Training Step: 184 Training Loss: 0.6182186603546143 \n",
      "     Training Step: 185 Training Loss: 0.6185467839241028 \n",
      "     Training Step: 186 Training Loss: 0.6194058656692505 \n",
      "     Training Step: 187 Training Loss: 0.6136549115180969 \n",
      "     Training Step: 188 Training Loss: 0.6143612861633301 \n",
      "     Training Step: 189 Training Loss: 0.6180769205093384 \n",
      "     Training Step: 190 Training Loss: 0.6154845356941223 \n",
      "     Training Step: 191 Training Loss: 0.6201745867729187 \n",
      "     Training Step: 192 Training Loss: 0.6092987656593323 \n",
      "     Training Step: 193 Training Loss: 0.6127910017967224 \n",
      "     Training Step: 194 Training Loss: 0.6129415035247803 \n",
      "     Training Step: 195 Training Loss: 0.6164050102233887 \n",
      "     Training Step: 196 Training Loss: 0.611405074596405 \n",
      "     Training Step: 197 Training Loss: 0.614604115486145 \n",
      "     Training Step: 198 Training Loss: 0.6146703362464905 \n",
      "     Training Step: 199 Training Loss: 0.6180899143218994 \n",
      "     Training Step: 200 Training Loss: 0.6114689707756042 \n",
      "     Training Step: 201 Training Loss: 0.610880434513092 \n",
      "     Training Step: 202 Training Loss: 0.6127999424934387 \n",
      "     Training Step: 203 Training Loss: 0.6156286597251892 \n",
      "     Training Step: 204 Training Loss: 0.6133588552474976 \n",
      "     Training Step: 205 Training Loss: 0.6146607995033264 \n",
      "     Training Step: 206 Training Loss: 0.6131983995437622 \n",
      "     Training Step: 207 Training Loss: 0.6122302412986755 \n",
      "     Training Step: 208 Training Loss: 0.6120638847351074 \n",
      "     Training Step: 209 Training Loss: 0.6133124828338623 \n",
      "     Training Step: 210 Training Loss: 0.6117778420448303 \n",
      "     Training Step: 211 Training Loss: 0.615108072757721 \n",
      "     Training Step: 212 Training Loss: 0.6124849915504456 \n",
      "     Training Step: 213 Training Loss: 0.6139106750488281 \n",
      "     Training Step: 214 Training Loss: 0.6143458485603333 \n",
      "     Training Step: 215 Training Loss: 0.6143773794174194 \n",
      "     Training Step: 216 Training Loss: 0.6133019328117371 \n",
      "     Training Step: 217 Training Loss: 0.6134973764419556 \n",
      "     Training Step: 218 Training Loss: 0.6126404404640198 \n",
      "     Training Step: 219 Training Loss: 0.6141527891159058 \n",
      "     Training Step: 220 Training Loss: 0.6146659255027771 \n",
      "     Training Step: 221 Training Loss: 0.6118297576904297 \n",
      "     Training Step: 222 Training Loss: 0.6149300336837769 \n",
      "     Training Step: 223 Training Loss: 0.609705924987793 \n",
      "     Training Step: 224 Training Loss: 0.6123788356781006 \n",
      "     Training Step: 225 Training Loss: 0.6104678511619568 \n",
      "     Training Step: 226 Training Loss: 0.6144835948944092 \n",
      "     Training Step: 227 Training Loss: 0.6168252825737 \n",
      "     Training Step: 228 Training Loss: 0.6111519932746887 \n",
      "     Training Step: 229 Training Loss: 0.6141526103019714 \n",
      "     Training Step: 230 Training Loss: 0.6151599884033203 \n",
      "     Training Step: 231 Training Loss: 0.6127504110336304 \n",
      "     Training Step: 232 Training Loss: 0.6167376041412354 \n",
      "     Training Step: 233 Training Loss: 0.6116389632225037 \n",
      "     Training Step: 234 Training Loss: 0.6140186190605164 \n",
      "     Training Step: 235 Training Loss: 0.6128666996955872 \n",
      "     Training Step: 236 Training Loss: 0.6166756749153137 \n",
      "     Training Step: 237 Training Loss: 0.6166152954101562 \n",
      "     Training Step: 238 Training Loss: 0.6138837933540344 \n",
      "     Training Step: 239 Training Loss: 0.6166967749595642 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6184775233268738 \n",
      "     Validation Step: 1 Validation Loss: 0.6111876368522644 \n",
      "     Validation Step: 2 Validation Loss: 0.614839494228363 \n",
      "     Validation Step: 3 Validation Loss: 0.615225076675415 \n",
      "     Validation Step: 4 Validation Loss: 0.6075448393821716 \n",
      "     Validation Step: 5 Validation Loss: 0.6141325235366821 \n",
      "     Validation Step: 6 Validation Loss: 0.6148954629898071 \n",
      "     Validation Step: 7 Validation Loss: 0.6175912022590637 \n",
      "     Validation Step: 8 Validation Loss: 0.6141200065612793 \n",
      "     Validation Step: 9 Validation Loss: 0.6145705580711365 \n",
      "     Validation Step: 10 Validation Loss: 0.6172975301742554 \n",
      "     Validation Step: 11 Validation Loss: 0.6142072081565857 \n",
      "     Validation Step: 12 Validation Loss: 0.6162394285202026 \n",
      "     Validation Step: 13 Validation Loss: 0.6105306148529053 \n",
      "     Validation Step: 14 Validation Loss: 0.6116529107093811 \n",
      "     Validation Step: 15 Validation Loss: 0.618460476398468 \n",
      "     Validation Step: 16 Validation Loss: 0.6101717352867126 \n",
      "     Validation Step: 17 Validation Loss: 0.6150311827659607 \n",
      "     Validation Step: 18 Validation Loss: 0.6128410696983337 \n",
      "     Validation Step: 19 Validation Loss: 0.6121400594711304 \n",
      "     Validation Step: 20 Validation Loss: 0.6183251142501831 \n",
      "     Validation Step: 21 Validation Loss: 0.6104942560195923 \n",
      "     Validation Step: 22 Validation Loss: 0.613313615322113 \n",
      "     Validation Step: 23 Validation Loss: 0.6106441020965576 \n",
      "     Validation Step: 24 Validation Loss: 0.6146426796913147 \n",
      "     Validation Step: 25 Validation Loss: 0.6111763715744019 \n",
      "     Validation Step: 26 Validation Loss: 0.6129947900772095 \n",
      "     Validation Step: 27 Validation Loss: 0.6170088052749634 \n",
      "     Validation Step: 28 Validation Loss: 0.6136463284492493 \n",
      "     Validation Step: 29 Validation Loss: 0.615564227104187 \n",
      "     Validation Step: 30 Validation Loss: 0.6159783601760864 \n",
      "     Validation Step: 31 Validation Loss: 0.6101257801055908 \n",
      "     Validation Step: 32 Validation Loss: 0.6101521849632263 \n",
      "     Validation Step: 33 Validation Loss: 0.6118948459625244 \n",
      "     Validation Step: 34 Validation Loss: 0.618048906326294 \n",
      "     Validation Step: 35 Validation Loss: 0.6153150200843811 \n",
      "     Validation Step: 36 Validation Loss: 0.6182101964950562 \n",
      "     Validation Step: 37 Validation Loss: 0.6157863140106201 \n",
      "     Validation Step: 38 Validation Loss: 0.6156089901924133 \n",
      "     Validation Step: 39 Validation Loss: 0.6115689873695374 \n",
      "     Validation Step: 40 Validation Loss: 0.6142573952674866 \n",
      "     Validation Step: 41 Validation Loss: 0.6145382523536682 \n",
      "     Validation Step: 42 Validation Loss: 0.6136690378189087 \n",
      "     Validation Step: 43 Validation Loss: 0.6136457324028015 \n",
      "     Validation Step: 44 Validation Loss: 0.6176813244819641 \n",
      "Epoch: 131\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6209107637405396 \n",
      "     Training Step: 1 Training Loss: 0.6143674254417419 \n",
      "     Training Step: 2 Training Loss: 0.6166828870773315 \n",
      "     Training Step: 3 Training Loss: 0.6122530102729797 \n",
      "     Training Step: 4 Training Loss: 0.6146508455276489 \n",
      "     Training Step: 5 Training Loss: 0.6118192672729492 \n",
      "     Training Step: 6 Training Loss: 0.617124080657959 \n",
      "     Training Step: 7 Training Loss: 0.6159447431564331 \n",
      "     Training Step: 8 Training Loss: 0.6136660575866699 \n",
      "     Training Step: 9 Training Loss: 0.6180199980735779 \n",
      "     Training Step: 10 Training Loss: 0.615999162197113 \n",
      "     Training Step: 11 Training Loss: 0.6135349869728088 \n",
      "     Training Step: 12 Training Loss: 0.613074004650116 \n",
      "     Training Step: 13 Training Loss: 0.6116301417350769 \n",
      "     Training Step: 14 Training Loss: 0.6118915677070618 \n",
      "     Training Step: 15 Training Loss: 0.6168005466461182 \n",
      "     Training Step: 16 Training Loss: 0.6171476244926453 \n",
      "     Training Step: 17 Training Loss: 0.6123814582824707 \n",
      "     Training Step: 18 Training Loss: 0.611143946647644 \n",
      "     Training Step: 19 Training Loss: 0.6100338697433472 \n",
      "     Training Step: 20 Training Loss: 0.6105861663818359 \n",
      "     Training Step: 21 Training Loss: 0.6094352006912231 \n",
      "     Training Step: 22 Training Loss: 0.6147032380104065 \n",
      "     Training Step: 23 Training Loss: 0.6142296195030212 \n",
      "     Training Step: 24 Training Loss: 0.6147311925888062 \n",
      "     Training Step: 25 Training Loss: 0.6172346472740173 \n",
      "     Training Step: 26 Training Loss: 0.611405074596405 \n",
      "     Training Step: 27 Training Loss: 0.6114596128463745 \n",
      "     Training Step: 28 Training Loss: 0.6093695163726807 \n",
      "     Training Step: 29 Training Loss: 0.6140384078025818 \n",
      "     Training Step: 30 Training Loss: 0.6129160523414612 \n",
      "     Training Step: 31 Training Loss: 0.6165177822113037 \n",
      "     Training Step: 32 Training Loss: 0.619472861289978 \n",
      "     Training Step: 33 Training Loss: 0.6153867244720459 \n",
      "     Training Step: 34 Training Loss: 0.6149351596832275 \n",
      "     Training Step: 35 Training Loss: 0.6180987358093262 \n",
      "     Training Step: 36 Training Loss: 0.6156708598136902 \n",
      "     Training Step: 37 Training Loss: 0.6111883521080017 \n",
      "     Training Step: 38 Training Loss: 0.6136403679847717 \n",
      "     Training Step: 39 Training Loss: 0.6168871521949768 \n",
      "     Training Step: 40 Training Loss: 0.6109325289726257 \n",
      "     Training Step: 41 Training Loss: 0.6147129535675049 \n",
      "     Training Step: 42 Training Loss: 0.6174741983413696 \n",
      "     Training Step: 43 Training Loss: 0.6147987246513367 \n",
      "     Training Step: 44 Training Loss: 0.6133149862289429 \n",
      "     Training Step: 45 Training Loss: 0.6128795742988586 \n",
      "     Training Step: 46 Training Loss: 0.6114385724067688 \n",
      "     Training Step: 47 Training Loss: 0.6100612878799438 \n",
      "     Training Step: 48 Training Loss: 0.6135950684547424 \n",
      "     Training Step: 49 Training Loss: 0.6147766709327698 \n",
      "     Training Step: 50 Training Loss: 0.6133138537406921 \n",
      "     Training Step: 51 Training Loss: 0.6151671409606934 \n",
      "     Training Step: 52 Training Loss: 0.6121582388877869 \n",
      "     Training Step: 53 Training Loss: 0.612464189529419 \n",
      "     Training Step: 54 Training Loss: 0.6132040619850159 \n",
      "     Training Step: 55 Training Loss: 0.6101589798927307 \n",
      "     Training Step: 56 Training Loss: 0.6158488392829895 \n",
      "     Training Step: 57 Training Loss: 0.6177442669868469 \n",
      "     Training Step: 58 Training Loss: 0.6148771643638611 \n",
      "     Training Step: 59 Training Loss: 0.6147494316101074 \n",
      "     Training Step: 60 Training Loss: 0.6166168451309204 \n",
      "     Training Step: 61 Training Loss: 0.6115252375602722 \n",
      "     Training Step: 62 Training Loss: 0.6146056056022644 \n",
      "     Training Step: 63 Training Loss: 0.6161993145942688 \n",
      "     Training Step: 64 Training Loss: 0.6180049777030945 \n",
      "     Training Step: 65 Training Loss: 0.6134772300720215 \n",
      "     Training Step: 66 Training Loss: 0.6173604130744934 \n",
      "     Training Step: 67 Training Loss: 0.615531325340271 \n",
      "     Training Step: 68 Training Loss: 0.6104507446289062 \n",
      "     Training Step: 69 Training Loss: 0.6157456040382385 \n",
      "     Training Step: 70 Training Loss: 0.6137572526931763 \n",
      "     Training Step: 71 Training Loss: 0.6130813956260681 \n",
      "     Training Step: 72 Training Loss: 0.6143372058868408 \n",
      "     Training Step: 73 Training Loss: 0.618424654006958 \n",
      "     Training Step: 74 Training Loss: 0.614334225654602 \n",
      "     Training Step: 75 Training Loss: 0.6157416105270386 \n",
      "     Training Step: 76 Training Loss: 0.6140198111534119 \n",
      "     Training Step: 77 Training Loss: 0.614155113697052 \n",
      "     Training Step: 78 Training Loss: 0.6139292120933533 \n",
      "     Training Step: 79 Training Loss: 0.6176315546035767 \n",
      "     Training Step: 80 Training Loss: 0.6146084666252136 \n",
      "     Training Step: 81 Training Loss: 0.6115613579750061 \n",
      "     Training Step: 82 Training Loss: 0.6121270060539246 \n",
      "     Training Step: 83 Training Loss: 0.6168022155761719 \n",
      "     Training Step: 84 Training Loss: 0.6198697090148926 \n",
      "     Training Step: 85 Training Loss: 0.6122308373451233 \n",
      "     Training Step: 86 Training Loss: 0.6144528985023499 \n",
      "     Training Step: 87 Training Loss: 0.6145995855331421 \n",
      "     Training Step: 88 Training Loss: 0.6177842020988464 \n",
      "     Training Step: 89 Training Loss: 0.6132075786590576 \n",
      "     Training Step: 90 Training Loss: 0.6120230555534363 \n",
      "     Training Step: 91 Training Loss: 0.6157530546188354 \n",
      "     Training Step: 92 Training Loss: 0.6137657165527344 \n",
      "     Training Step: 93 Training Loss: 0.6149159669876099 \n",
      "     Training Step: 94 Training Loss: 0.6107003092765808 \n",
      "     Training Step: 95 Training Loss: 0.6152632236480713 \n",
      "     Training Step: 96 Training Loss: 0.6182277798652649 \n",
      "     Training Step: 97 Training Loss: 0.6100594997406006 \n",
      "     Training Step: 98 Training Loss: 0.610572099685669 \n",
      "     Training Step: 99 Training Loss: 0.6125310063362122 \n",
      "     Training Step: 100 Training Loss: 0.6115762591362 \n",
      "     Training Step: 101 Training Loss: 0.6162118315696716 \n",
      "     Training Step: 102 Training Loss: 0.6123707294464111 \n",
      "     Training Step: 103 Training Loss: 0.6128714084625244 \n",
      "     Training Step: 104 Training Loss: 0.6122834086418152 \n",
      "     Training Step: 105 Training Loss: 0.6197463274002075 \n",
      "     Training Step: 106 Training Loss: 0.612150251865387 \n",
      "     Training Step: 107 Training Loss: 0.6161293387413025 \n",
      "     Training Step: 108 Training Loss: 0.6125245690345764 \n",
      "     Training Step: 109 Training Loss: 0.6138166189193726 \n",
      "     Training Step: 110 Training Loss: 0.6167181134223938 \n",
      "     Training Step: 111 Training Loss: 0.6122286319732666 \n",
      "     Training Step: 112 Training Loss: 0.6111962199211121 \n",
      "     Training Step: 113 Training Loss: 0.6134046912193298 \n",
      "     Training Step: 114 Training Loss: 0.6164119243621826 \n",
      "     Training Step: 115 Training Loss: 0.6131837964057922 \n",
      "     Training Step: 116 Training Loss: 0.6150651574134827 \n",
      "     Training Step: 117 Training Loss: 0.6101366877555847 \n",
      "     Training Step: 118 Training Loss: 0.6202215552330017 \n",
      "     Training Step: 119 Training Loss: 0.6116455793380737 \n",
      "     Training Step: 120 Training Loss: 0.6153906583786011 \n",
      "     Training Step: 121 Training Loss: 0.6170936822891235 \n",
      "     Training Step: 122 Training Loss: 0.6107395887374878 \n",
      "     Training Step: 123 Training Loss: 0.6176880598068237 \n",
      "     Training Step: 124 Training Loss: 0.6152976751327515 \n",
      "     Training Step: 125 Training Loss: 0.6099926233291626 \n",
      "     Training Step: 126 Training Loss: 0.6155142784118652 \n",
      "     Training Step: 127 Training Loss: 0.6134976744651794 \n",
      "     Training Step: 128 Training Loss: 0.6160297989845276 \n",
      "     Training Step: 129 Training Loss: 0.6167145371437073 \n",
      "     Training Step: 130 Training Loss: 0.6155951619148254 \n",
      "     Training Step: 131 Training Loss: 0.614071249961853 \n",
      "     Training Step: 132 Training Loss: 0.6154161691665649 \n",
      "     Training Step: 133 Training Loss: 0.6128453016281128 \n",
      "     Training Step: 134 Training Loss: 0.6153352856636047 \n",
      "     Training Step: 135 Training Loss: 0.6151724457740784 \n",
      "     Training Step: 136 Training Loss: 0.616743803024292 \n",
      "     Training Step: 137 Training Loss: 0.6092520356178284 \n",
      "     Training Step: 138 Training Loss: 0.6114069819450378 \n",
      "     Training Step: 139 Training Loss: 0.6180613040924072 \n",
      "     Training Step: 140 Training Loss: 0.6166559457778931 \n",
      "     Training Step: 141 Training Loss: 0.617752730846405 \n",
      "     Training Step: 142 Training Loss: 0.6126469969749451 \n",
      "     Training Step: 143 Training Loss: 0.618819534778595 \n",
      "     Training Step: 144 Training Loss: 0.6142829656600952 \n",
      "     Training Step: 145 Training Loss: 0.6132968068122864 \n",
      "     Training Step: 146 Training Loss: 0.6082934737205505 \n",
      "     Training Step: 147 Training Loss: 0.6127665638923645 \n",
      "     Training Step: 148 Training Loss: 0.6185857057571411 \n",
      "     Training Step: 149 Training Loss: 0.6114416718482971 \n",
      "     Training Step: 150 Training Loss: 0.615286111831665 \n",
      "     Training Step: 151 Training Loss: 0.6097284555435181 \n",
      "     Training Step: 152 Training Loss: 0.6127948760986328 \n",
      "     Training Step: 153 Training Loss: 0.6134552955627441 \n",
      "     Training Step: 154 Training Loss: 0.6184804439544678 \n",
      "     Training Step: 155 Training Loss: 0.6163597106933594 \n",
      "     Training Step: 156 Training Loss: 0.6131360530853271 \n",
      "     Training Step: 157 Training Loss: 0.6151024699211121 \n",
      "     Training Step: 158 Training Loss: 0.6103676557540894 \n",
      "     Training Step: 159 Training Loss: 0.610711932182312 \n",
      "     Training Step: 160 Training Loss: 0.6141533255577087 \n",
      "     Training Step: 161 Training Loss: 0.6122790575027466 \n",
      "     Training Step: 162 Training Loss: 0.6126924157142639 \n",
      "     Training Step: 163 Training Loss: 0.6152908802032471 \n",
      "     Training Step: 164 Training Loss: 0.611620306968689 \n",
      "     Training Step: 165 Training Loss: 0.6105584502220154 \n",
      "     Training Step: 166 Training Loss: 0.6171826720237732 \n",
      "     Training Step: 167 Training Loss: 0.6177046298980713 \n",
      "     Training Step: 168 Training Loss: 0.611832857131958 \n",
      "     Training Step: 169 Training Loss: 0.6144222617149353 \n",
      "     Training Step: 170 Training Loss: 0.6116220951080322 \n",
      "     Training Step: 171 Training Loss: 0.6132737994194031 \n",
      "     Training Step: 172 Training Loss: 0.6129167079925537 \n",
      "     Training Step: 173 Training Loss: 0.6129629015922546 \n",
      "     Training Step: 174 Training Loss: 0.6178168058395386 \n",
      "     Training Step: 175 Training Loss: 0.6184128522872925 \n",
      "     Training Step: 176 Training Loss: 0.6118255257606506 \n",
      "     Training Step: 177 Training Loss: 0.6166496276855469 \n",
      "     Training Step: 178 Training Loss: 0.6154395341873169 \n",
      "     Training Step: 179 Training Loss: 0.6131260991096497 \n",
      "     Training Step: 180 Training Loss: 0.613278329372406 \n",
      "     Training Step: 181 Training Loss: 0.6166666746139526 \n",
      "     Training Step: 182 Training Loss: 0.6142123937606812 \n",
      "     Training Step: 183 Training Loss: 0.612148642539978 \n",
      "     Training Step: 184 Training Loss: 0.6157817840576172 \n",
      "     Training Step: 185 Training Loss: 0.6146903038024902 \n",
      "     Training Step: 186 Training Loss: 0.614020049571991 \n",
      "     Training Step: 187 Training Loss: 0.6124920845031738 \n",
      "     Training Step: 188 Training Loss: 0.6185721158981323 \n",
      "     Training Step: 189 Training Loss: 0.6162335872650146 \n",
      "     Training Step: 190 Training Loss: 0.6139170527458191 \n",
      "     Training Step: 191 Training Loss: 0.6149559020996094 \n",
      "     Training Step: 192 Training Loss: 0.6152448654174805 \n",
      "     Training Step: 193 Training Loss: 0.6097357273101807 \n",
      "     Training Step: 194 Training Loss: 0.6166503429412842 \n",
      "     Training Step: 195 Training Loss: 0.6154075860977173 \n",
      "     Training Step: 196 Training Loss: 0.6146639585494995 \n",
      "     Training Step: 197 Training Loss: 0.6156821250915527 \n",
      "     Training Step: 198 Training Loss: 0.6115292310714722 \n",
      "     Training Step: 199 Training Loss: 0.611789345741272 \n",
      "     Training Step: 200 Training Loss: 0.6144775152206421 \n",
      "     Training Step: 201 Training Loss: 0.6133548021316528 \n",
      "     Training Step: 202 Training Loss: 0.6146607398986816 \n",
      "     Training Step: 203 Training Loss: 0.6154745817184448 \n",
      "     Training Step: 204 Training Loss: 0.6155499815940857 \n",
      "     Training Step: 205 Training Loss: 0.6105725765228271 \n",
      "     Training Step: 206 Training Loss: 0.615089476108551 \n",
      "     Training Step: 207 Training Loss: 0.6108660697937012 \n",
      "     Training Step: 208 Training Loss: 0.6138888001441956 \n",
      "     Training Step: 209 Training Loss: 0.6141074895858765 \n",
      "     Training Step: 210 Training Loss: 0.6167318820953369 \n",
      "     Training Step: 211 Training Loss: 0.611400306224823 \n",
      "     Training Step: 212 Training Loss: 0.6096853613853455 \n",
      "     Training Step: 213 Training Loss: 0.6118013262748718 \n",
      "     Training Step: 214 Training Loss: 0.615380048751831 \n",
      "     Training Step: 215 Training Loss: 0.6143503785133362 \n",
      "     Training Step: 216 Training Loss: 0.6136382818222046 \n",
      "     Training Step: 217 Training Loss: 0.6118200421333313 \n",
      "     Training Step: 218 Training Loss: 0.6104637384414673 \n",
      "     Training Step: 219 Training Loss: 0.6137336492538452 \n",
      "     Training Step: 220 Training Loss: 0.6115187406539917 \n",
      "     Training Step: 221 Training Loss: 0.6104796528816223 \n",
      "     Training Step: 222 Training Loss: 0.6145052313804626 \n",
      "     Training Step: 223 Training Loss: 0.618255078792572 \n",
      "     Training Step: 224 Training Loss: 0.6144402623176575 \n",
      "     Training Step: 225 Training Loss: 0.6116819381713867 \n",
      "     Training Step: 226 Training Loss: 0.6142443418502808 \n",
      "     Training Step: 227 Training Loss: 0.6127490997314453 \n",
      "     Training Step: 228 Training Loss: 0.6196309924125671 \n",
      "     Training Step: 229 Training Loss: 0.6168048977851868 \n",
      "     Training Step: 230 Training Loss: 0.6106680035591125 \n",
      "     Training Step: 231 Training Loss: 0.6183042526245117 \n",
      "     Training Step: 232 Training Loss: 0.6164098381996155 \n",
      "     Training Step: 233 Training Loss: 0.6146551370620728 \n",
      "     Training Step: 234 Training Loss: 0.6188443303108215 \n",
      "     Training Step: 235 Training Loss: 0.6125489473342896 \n",
      "     Training Step: 236 Training Loss: 0.6169003248214722 \n",
      "     Training Step: 237 Training Loss: 0.6121061444282532 \n",
      "     Training Step: 238 Training Loss: 0.6147387027740479 \n",
      "     Training Step: 239 Training Loss: 0.6124047636985779 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6075786352157593 \n",
      "     Validation Step: 1 Validation Loss: 0.6128559112548828 \n",
      "     Validation Step: 2 Validation Loss: 0.6180385947227478 \n",
      "     Validation Step: 3 Validation Loss: 0.6130073666572571 \n",
      "     Validation Step: 4 Validation Loss: 0.6101486682891846 \n",
      "     Validation Step: 5 Validation Loss: 0.617000937461853 \n",
      "     Validation Step: 6 Validation Loss: 0.6157851219177246 \n",
      "     Validation Step: 7 Validation Loss: 0.6112023591995239 \n",
      "     Validation Step: 8 Validation Loss: 0.6172873973846436 \n",
      "     Validation Step: 9 Validation Loss: 0.6115842461585999 \n",
      "     Validation Step: 10 Validation Loss: 0.6184664368629456 \n",
      "     Validation Step: 11 Validation Loss: 0.6152251958847046 \n",
      "     Validation Step: 12 Validation Loss: 0.6121534705162048 \n",
      "     Validation Step: 13 Validation Loss: 0.6136557459831238 \n",
      "     Validation Step: 14 Validation Loss: 0.6145473718643188 \n",
      "     Validation Step: 15 Validation Loss: 0.6133270263671875 \n",
      "     Validation Step: 16 Validation Loss: 0.6105490326881409 \n",
      "     Validation Step: 17 Validation Loss: 0.6105163097381592 \n",
      "     Validation Step: 18 Validation Loss: 0.6162363290786743 \n",
      "     Validation Step: 19 Validation Loss: 0.614898145198822 \n",
      "     Validation Step: 20 Validation Loss: 0.6136767268180847 \n",
      "     Validation Step: 21 Validation Loss: 0.6181990504264832 \n",
      "     Validation Step: 22 Validation Loss: 0.6148445010185242 \n",
      "     Validation Step: 23 Validation Loss: 0.6119080781936646 \n",
      "     Validation Step: 24 Validation Loss: 0.6145691275596619 \n",
      "     Validation Step: 25 Validation Loss: 0.6116681098937988 \n",
      "     Validation Step: 26 Validation Loss: 0.6106597781181335 \n",
      "     Validation Step: 27 Validation Loss: 0.6136488318443298 \n",
      "     Validation Step: 28 Validation Loss: 0.6101939082145691 \n",
      "     Validation Step: 29 Validation Loss: 0.6175864338874817 \n",
      "     Validation Step: 30 Validation Loss: 0.615607500076294 \n",
      "     Validation Step: 31 Validation Loss: 0.61556476354599 \n",
      "     Validation Step: 32 Validation Loss: 0.618314266204834 \n",
      "     Validation Step: 33 Validation Loss: 0.6141279935836792 \n",
      "     Validation Step: 34 Validation Loss: 0.6146500706672668 \n",
      "     Validation Step: 35 Validation Loss: 0.6153127551078796 \n",
      "     Validation Step: 36 Validation Loss: 0.6176722645759583 \n",
      "     Validation Step: 37 Validation Loss: 0.6142137050628662 \n",
      "     Validation Step: 38 Validation Loss: 0.614142894744873 \n",
      "     Validation Step: 39 Validation Loss: 0.6159725785255432 \n",
      "     Validation Step: 40 Validation Loss: 0.6142676472663879 \n",
      "     Validation Step: 41 Validation Loss: 0.6101754903793335 \n",
      "     Validation Step: 42 Validation Loss: 0.6150332093238831 \n",
      "     Validation Step: 43 Validation Loss: 0.6111910939216614 \n",
      "     Validation Step: 44 Validation Loss: 0.6184513568878174 \n",
      "Epoch: 132\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6121435165405273 \n",
      "     Training Step: 1 Training Loss: 0.6129785776138306 \n",
      "     Training Step: 2 Training Loss: 0.6109093427658081 \n",
      "     Training Step: 3 Training Loss: 0.6131201982498169 \n",
      "     Training Step: 4 Training Loss: 0.6097143292427063 \n",
      "     Training Step: 5 Training Loss: 0.6178248524665833 \n",
      "     Training Step: 6 Training Loss: 0.6153303384780884 \n",
      "     Training Step: 7 Training Loss: 0.6164296269416809 \n",
      "     Training Step: 8 Training Loss: 0.6129254102706909 \n",
      "     Training Step: 9 Training Loss: 0.614693284034729 \n",
      "     Training Step: 10 Training Loss: 0.6131860613822937 \n",
      "     Training Step: 11 Training Loss: 0.616665244102478 \n",
      "     Training Step: 12 Training Loss: 0.6171114444732666 \n",
      "     Training Step: 13 Training Loss: 0.6141051650047302 \n",
      "     Training Step: 14 Training Loss: 0.6184061765670776 \n",
      "     Training Step: 15 Training Loss: 0.6100841760635376 \n",
      "     Training Step: 16 Training Loss: 0.6152885556221008 \n",
      "     Training Step: 17 Training Loss: 0.6117053627967834 \n",
      "     Training Step: 18 Training Loss: 0.6135261654853821 \n",
      "     Training Step: 19 Training Loss: 0.6118524074554443 \n",
      "     Training Step: 20 Training Loss: 0.6118453145027161 \n",
      "     Training Step: 21 Training Loss: 0.6100032329559326 \n",
      "     Training Step: 22 Training Loss: 0.6134122610092163 \n",
      "     Training Step: 23 Training Loss: 0.6115888953208923 \n",
      "     Training Step: 24 Training Loss: 0.6128678321838379 \n",
      "     Training Step: 25 Training Loss: 0.6147149205207825 \n",
      "     Training Step: 26 Training Loss: 0.618107259273529 \n",
      "     Training Step: 27 Training Loss: 0.6094309687614441 \n",
      "     Training Step: 28 Training Loss: 0.6163817644119263 \n",
      "     Training Step: 29 Training Loss: 0.6169259548187256 \n",
      "     Training Step: 30 Training Loss: 0.6197472810745239 \n",
      "     Training Step: 31 Training Loss: 0.6105644702911377 \n",
      "     Training Step: 32 Training Loss: 0.61248379945755 \n",
      "     Training Step: 33 Training Loss: 0.6123768091201782 \n",
      "     Training Step: 34 Training Loss: 0.6146009564399719 \n",
      "     Training Step: 35 Training Loss: 0.6150887608528137 \n",
      "     Training Step: 36 Training Loss: 0.6141499876976013 \n",
      "     Training Step: 37 Training Loss: 0.6158306002616882 \n",
      "     Training Step: 38 Training Loss: 0.6117882132530212 \n",
      "     Training Step: 39 Training Loss: 0.6128429174423218 \n",
      "     Training Step: 40 Training Loss: 0.6127529144287109 \n",
      "     Training Step: 41 Training Loss: 0.6132860779762268 \n",
      "     Training Step: 42 Training Loss: 0.6136333346366882 \n",
      "     Training Step: 43 Training Loss: 0.6209292411804199 \n",
      "     Training Step: 44 Training Loss: 0.6142039895057678 \n",
      "     Training Step: 45 Training Loss: 0.6174829602241516 \n",
      "     Training Step: 46 Training Loss: 0.6166542768478394 \n",
      "     Training Step: 47 Training Loss: 0.6167115569114685 \n",
      "     Training Step: 48 Training Loss: 0.6150649785995483 \n",
      "     Training Step: 49 Training Loss: 0.6154176592826843 \n",
      "     Training Step: 50 Training Loss: 0.613297164440155 \n",
      "     Training Step: 51 Training Loss: 0.6195781230926514 \n",
      "     Training Step: 52 Training Loss: 0.6156880259513855 \n",
      "     Training Step: 53 Training Loss: 0.6169003844261169 \n",
      "     Training Step: 54 Training Loss: 0.6166372299194336 \n",
      "     Training Step: 55 Training Loss: 0.6141713261604309 \n",
      "     Training Step: 56 Training Loss: 0.612948477268219 \n",
      "     Training Step: 57 Training Loss: 0.6115960478782654 \n",
      "     Training Step: 58 Training Loss: 0.6139297485351562 \n",
      "     Training Step: 59 Training Loss: 0.6106221675872803 \n",
      "     Training Step: 60 Training Loss: 0.6152634024620056 \n",
      "     Training Step: 61 Training Loss: 0.612466037273407 \n",
      "     Training Step: 62 Training Loss: 0.6118255853652954 \n",
      "     Training Step: 63 Training Loss: 0.6147758960723877 \n",
      "     Training Step: 64 Training Loss: 0.6126385927200317 \n",
      "     Training Step: 65 Training Loss: 0.6151904463768005 \n",
      "     Training Step: 66 Training Loss: 0.6157678961753845 \n",
      "     Training Step: 67 Training Loss: 0.6111300587654114 \n",
      "     Training Step: 68 Training Loss: 0.61392742395401 \n",
      "     Training Step: 69 Training Loss: 0.611617922782898 \n",
      "     Training Step: 70 Training Loss: 0.6161375641822815 \n",
      "     Training Step: 71 Training Loss: 0.6140345335006714 \n",
      "     Training Step: 72 Training Loss: 0.6136112809181213 \n",
      "     Training Step: 73 Training Loss: 0.6125037670135498 \n",
      "     Training Step: 74 Training Loss: 0.6146412491798401 \n",
      "     Training Step: 75 Training Loss: 0.6199002861976624 \n",
      "     Training Step: 76 Training Loss: 0.6157864332199097 \n",
      "     Training Step: 77 Training Loss: 0.6138216257095337 \n",
      "     Training Step: 78 Training Loss: 0.6171355247497559 \n",
      "     Training Step: 79 Training Loss: 0.6146590113639832 \n",
      "     Training Step: 80 Training Loss: 0.6147539019584656 \n",
      "     Training Step: 81 Training Loss: 0.614730715751648 \n",
      "     Training Step: 82 Training Loss: 0.6130836009979248 \n",
      "     Training Step: 83 Training Loss: 0.6137776374816895 \n",
      "     Training Step: 84 Training Loss: 0.611514151096344 \n",
      "     Training Step: 85 Training Loss: 0.6105258464813232 \n",
      "     Training Step: 86 Training Loss: 0.613740861415863 \n",
      "     Training Step: 87 Training Loss: 0.614702582359314 \n",
      "     Training Step: 88 Training Loss: 0.6188356280326843 \n",
      "     Training Step: 89 Training Loss: 0.61253422498703 \n",
      "     Training Step: 90 Training Loss: 0.6154511570930481 \n",
      "     Training Step: 91 Training Loss: 0.6167064905166626 \n",
      "     Training Step: 92 Training Loss: 0.6154170632362366 \n",
      "     Training Step: 93 Training Loss: 0.6178195476531982 \n",
      "     Training Step: 94 Training Loss: 0.6168064475059509 \n",
      "     Training Step: 95 Training Loss: 0.6153447031974792 \n",
      "     Training Step: 96 Training Loss: 0.6144278049468994 \n",
      "     Training Step: 97 Training Loss: 0.6164821982383728 \n",
      "     Training Step: 98 Training Loss: 0.6159952878952026 \n",
      "     Training Step: 99 Training Loss: 0.6160274744033813 \n",
      "     Training Step: 100 Training Loss: 0.6165979504585266 \n",
      "     Training Step: 101 Training Loss: 0.6151010394096375 \n",
      "     Training Step: 102 Training Loss: 0.6179900765419006 \n",
      "     Training Step: 103 Training Loss: 0.6176724433898926 \n",
      "     Training Step: 104 Training Loss: 0.6137662529945374 \n",
      "     Training Step: 105 Training Loss: 0.610133945941925 \n",
      "     Training Step: 106 Training Loss: 0.6097873449325562 \n",
      "     Training Step: 107 Training Loss: 0.6142280697822571 \n",
      "     Training Step: 108 Training Loss: 0.6114119291305542 \n",
      "     Training Step: 109 Training Loss: 0.6144977807998657 \n",
      "     Training Step: 110 Training Loss: 0.611437201499939 \n",
      "     Training Step: 111 Training Loss: 0.6132078170776367 \n",
      "     Training Step: 112 Training Loss: 0.6177136301994324 \n",
      "     Training Step: 113 Training Loss: 0.6171848177909851 \n",
      "     Training Step: 114 Training Loss: 0.6122318506240845 \n",
      "     Training Step: 115 Training Loss: 0.6182713508605957 \n",
      "     Training Step: 116 Training Loss: 0.6115119457244873 \n",
      "     Training Step: 117 Training Loss: 0.6166896224021912 \n",
      "     Training Step: 118 Training Loss: 0.6105633974075317 \n",
      "     Training Step: 119 Training Loss: 0.6117831468582153 \n",
      "     Training Step: 120 Training Loss: 0.6188838481903076 \n",
      "     Training Step: 121 Training Loss: 0.6133547425270081 \n",
      "     Training Step: 122 Training Loss: 0.6115241050720215 \n",
      "     Training Step: 123 Training Loss: 0.6092303395271301 \n",
      "     Training Step: 124 Training Loss: 0.6168069243431091 \n",
      "     Training Step: 125 Training Loss: 0.6114318370819092 \n",
      "     Training Step: 126 Training Loss: 0.61215740442276 \n",
      "     Training Step: 127 Training Loss: 0.6143477559089661 \n",
      "     Training Step: 128 Training Loss: 0.6144542098045349 \n",
      "     Training Step: 129 Training Loss: 0.6111544966697693 \n",
      "     Training Step: 130 Training Loss: 0.6148728132247925 \n",
      "     Training Step: 131 Training Loss: 0.6152894496917725 \n",
      "     Training Step: 132 Training Loss: 0.6108683943748474 \n",
      "     Training Step: 133 Training Loss: 0.6116364002227783 \n",
      "     Training Step: 134 Training Loss: 0.6134557723999023 \n",
      "     Training Step: 135 Training Loss: 0.6107273697853088 \n",
      "     Training Step: 136 Training Loss: 0.6126940846443176 \n",
      "     Training Step: 137 Training Loss: 0.614801287651062 \n",
      "     Training Step: 138 Training Loss: 0.6157107353210449 \n",
      "     Training Step: 139 Training Loss: 0.6167874932289124 \n",
      "     Training Step: 140 Training Loss: 0.6177293062210083 \n",
      "     Training Step: 141 Training Loss: 0.6146889328956604 \n",
      "     Training Step: 142 Training Loss: 0.6152411699295044 \n",
      "     Training Step: 143 Training Loss: 0.6116124391555786 \n",
      "     Training Step: 144 Training Loss: 0.6153716444969177 \n",
      "     Training Step: 145 Training Loss: 0.6100645661354065 \n",
      "     Training Step: 146 Training Loss: 0.6118923425674438 \n",
      "     Training Step: 147 Training Loss: 0.6121688485145569 \n",
      "     Training Step: 148 Training Loss: 0.6120754480361938 \n",
      "     Training Step: 149 Training Loss: 0.6143425107002258 \n",
      "     Training Step: 150 Training Loss: 0.6185842752456665 \n",
      "     Training Step: 151 Training Loss: 0.6140772104263306 \n",
      "     Training Step: 152 Training Loss: 0.6164240837097168 \n",
      "     Training Step: 153 Training Loss: 0.6130591034889221 \n",
      "     Training Step: 154 Training Loss: 0.6157441139221191 \n",
      "     Training Step: 155 Training Loss: 0.616715669631958 \n",
      "     Training Step: 156 Training Loss: 0.6127644777297974 \n",
      "     Training Step: 157 Training Loss: 0.6114208698272705 \n",
      "     Training Step: 158 Training Loss: 0.6180987358093262 \n",
      "     Training Step: 159 Training Loss: 0.6154742240905762 \n",
      "     Training Step: 160 Training Loss: 0.6162275671958923 \n",
      "     Training Step: 161 Training Loss: 0.6118077635765076 \n",
      "     Training Step: 162 Training Loss: 0.6161949634552002 \n",
      "     Training Step: 163 Training Loss: 0.6132064461708069 \n",
      "     Training Step: 164 Training Loss: 0.6120226979255676 \n",
      "     Training Step: 165 Training Loss: 0.6097399592399597 \n",
      "     Training Step: 166 Training Loss: 0.6153998970985413 \n",
      "     Training Step: 167 Training Loss: 0.6171547174453735 \n",
      "     Training Step: 168 Training Loss: 0.6135890483856201 \n",
      "     Training Step: 169 Training Loss: 0.6132907867431641 \n",
      "     Training Step: 170 Training Loss: 0.6146051287651062 \n",
      "     Training Step: 171 Training Loss: 0.6105700731277466 \n",
      "     Training Step: 172 Training Loss: 0.6125300526618958 \n",
      "     Training Step: 173 Training Loss: 0.6123798489570618 \n",
      "     Training Step: 174 Training Loss: 0.614338219165802 \n",
      "     Training Step: 175 Training Loss: 0.6184792518615723 \n",
      "     Training Step: 176 Training Loss: 0.6116143465042114 \n",
      "     Training Step: 177 Training Loss: 0.6134664416313171 \n",
      "     Training Step: 178 Training Loss: 0.6128614544868469 \n",
      "     Training Step: 179 Training Loss: 0.6111943125724792 \n",
      "     Training Step: 180 Training Loss: 0.6146097183227539 \n",
      "     Training Step: 181 Training Loss: 0.6138836145401001 \n",
      "     Training Step: 182 Training Loss: 0.61229008436203 \n",
      "     Training Step: 183 Training Loss: 0.6104735732078552 \n",
      "     Training Step: 184 Training Loss: 0.6101690530776978 \n",
      "     Training Step: 185 Training Loss: 0.6122245788574219 \n",
      "     Training Step: 186 Training Loss: 0.617785632610321 \n",
      "     Training Step: 187 Training Loss: 0.6106473803520203 \n",
      "     Training Step: 188 Training Loss: 0.6133097410202026 \n",
      "     Training Step: 189 Training Loss: 0.6157650947570801 \n",
      "     Training Step: 190 Training Loss: 0.614963948726654 \n",
      "     Training Step: 191 Training Loss: 0.6136506199836731 \n",
      "     Training Step: 192 Training Loss: 0.612138032913208 \n",
      "     Training Step: 193 Training Loss: 0.6180474162101746 \n",
      "     Training Step: 194 Training Loss: 0.6142420768737793 \n",
      "     Training Step: 195 Training Loss: 0.6133041381835938 \n",
      "     Training Step: 196 Training Loss: 0.6123745441436768 \n",
      "     Training Step: 197 Training Loss: 0.6171887516975403 \n",
      "     Training Step: 198 Training Loss: 0.6135000586509705 \n",
      "     Training Step: 199 Training Loss: 0.6176196932792664 \n",
      "     Training Step: 200 Training Loss: 0.6155303716659546 \n",
      "     Training Step: 201 Training Loss: 0.6161980032920837 \n",
      "     Training Step: 202 Training Loss: 0.6128180623054504 \n",
      "     Training Step: 203 Training Loss: 0.6094475984573364 \n",
      "     Training Step: 204 Training Loss: 0.6201919913291931 \n",
      "     Training Step: 205 Training Loss: 0.610169529914856 \n",
      "     Training Step: 206 Training Loss: 0.6143646836280823 \n",
      "     Training Step: 207 Training Loss: 0.6151579022407532 \n",
      "     Training Step: 208 Training Loss: 0.6149378418922424 \n",
      "     Training Step: 209 Training Loss: 0.6122431755065918 \n",
      "     Training Step: 210 Training Loss: 0.6149194836616516 \n",
      "     Training Step: 211 Training Loss: 0.614286482334137 \n",
      "     Training Step: 212 Training Loss: 0.6155179142951965 \n",
      "     Training Step: 213 Training Loss: 0.618451714515686 \n",
      "     Training Step: 214 Training Loss: 0.6168058514595032 \n",
      "     Training Step: 215 Training Loss: 0.6146636009216309 \n",
      "     Training Step: 216 Training Loss: 0.6140210032463074 \n",
      "     Training Step: 217 Training Loss: 0.6183066368103027 \n",
      "     Training Step: 218 Training Loss: 0.6144443154335022 \n",
      "     Training Step: 219 Training Loss: 0.6140201091766357 \n",
      "     Training Step: 220 Training Loss: 0.6155929565429688 \n",
      "     Training Step: 221 Training Loss: 0.6159384846687317 \n",
      "     Training Step: 222 Training Loss: 0.6144816279411316 \n",
      "     Training Step: 223 Training Loss: 0.6104368567466736 \n",
      "     Training Step: 224 Training Loss: 0.6182218790054321 \n",
      "     Training Step: 225 Training Loss: 0.6173593997955322 \n",
      "     Training Step: 226 Training Loss: 0.6194165945053101 \n",
      "     Training Step: 227 Training Loss: 0.6131577491760254 \n",
      "     Training Step: 228 Training Loss: 0.6123049259185791 \n",
      "     Training Step: 229 Training Loss: 0.6153716444969177 \n",
      "     Training Step: 230 Training Loss: 0.6166936159133911 \n",
      "     Training Step: 231 Training Loss: 0.6155282855033875 \n",
      "     Training Step: 232 Training Loss: 0.6107428073883057 \n",
      "     Training Step: 233 Training Loss: 0.6082810759544373 \n",
      "     Training Step: 234 Training Loss: 0.6114104390144348 \n",
      "     Training Step: 235 Training Loss: 0.6146685481071472 \n",
      "     Training Step: 236 Training Loss: 0.6115216612815857 \n",
      "     Training Step: 237 Training Loss: 0.6106748580932617 \n",
      "     Training Step: 238 Training Loss: 0.6103562712669373 \n",
      "     Training Step: 239 Training Loss: 0.6186655163764954 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6136733889579773 \n",
      "     Validation Step: 1 Validation Loss: 0.6156169772148132 \n",
      "     Validation Step: 2 Validation Loss: 0.6185357570648193 \n",
      "     Validation Step: 3 Validation Loss: 0.6104896664619446 \n",
      "     Validation Step: 4 Validation Loss: 0.6106089949607849 \n",
      "     Validation Step: 5 Validation Loss: 0.6128203272819519 \n",
      "     Validation Step: 6 Validation Loss: 0.618402361869812 \n",
      "     Validation Step: 7 Validation Loss: 0.614652156829834 \n",
      "     Validation Step: 8 Validation Loss: 0.618289053440094 \n",
      "     Validation Step: 9 Validation Loss: 0.6150665283203125 \n",
      "     Validation Step: 10 Validation Loss: 0.6101340651512146 \n",
      "     Validation Step: 11 Validation Loss: 0.6104638576507568 \n",
      "     Validation Step: 12 Validation Loss: 0.6148516535758972 \n",
      "     Validation Step: 13 Validation Loss: 0.6156482696533203 \n",
      "     Validation Step: 14 Validation Loss: 0.6160364151000977 \n",
      "     Validation Step: 15 Validation Loss: 0.6115398406982422 \n",
      "     Validation Step: 16 Validation Loss: 0.6185637712478638 \n",
      "     Validation Step: 17 Validation Loss: 0.6111690998077393 \n",
      "     Validation Step: 18 Validation Loss: 0.6177454590797424 \n",
      "     Validation Step: 19 Validation Loss: 0.6181232929229736 \n",
      "     Validation Step: 20 Validation Loss: 0.6136419177055359 \n",
      "     Validation Step: 21 Validation Loss: 0.6133109927177429 \n",
      "     Validation Step: 22 Validation Loss: 0.6153528690338135 \n",
      "     Validation Step: 23 Validation Loss: 0.612979531288147 \n",
      "     Validation Step: 24 Validation Loss: 0.6141358017921448 \n",
      "     Validation Step: 25 Validation Loss: 0.6176475882530212 \n",
      "     Validation Step: 26 Validation Loss: 0.6173641085624695 \n",
      "     Validation Step: 27 Validation Loss: 0.6074503064155579 \n",
      "     Validation Step: 28 Validation Loss: 0.6142728328704834 \n",
      "     Validation Step: 29 Validation Loss: 0.6162795424461365 \n",
      "     Validation Step: 30 Validation Loss: 0.6146025061607361 \n",
      "     Validation Step: 31 Validation Loss: 0.6149216890335083 \n",
      "     Validation Step: 32 Validation Loss: 0.6152591705322266 \n",
      "     Validation Step: 33 Validation Loss: 0.6116307377815247 \n",
      "     Validation Step: 34 Validation Loss: 0.6145534515380859 \n",
      "     Validation Step: 35 Validation Loss: 0.6100972890853882 \n",
      "     Validation Step: 36 Validation Loss: 0.6170875430107117 \n",
      "     Validation Step: 37 Validation Loss: 0.6141252517700195 \n",
      "     Validation Step: 38 Validation Loss: 0.610069990158081 \n",
      "     Validation Step: 39 Validation Loss: 0.6142165064811707 \n",
      "     Validation Step: 40 Validation Loss: 0.6118628978729248 \n",
      "     Validation Step: 41 Validation Loss: 0.6111447215080261 \n",
      "     Validation Step: 42 Validation Loss: 0.615817666053772 \n",
      "     Validation Step: 43 Validation Loss: 0.6136743426322937 \n",
      "     Validation Step: 44 Validation Loss: 0.6121172904968262 \n",
      "Epoch: 133\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6133182048797607 \n",
      "     Training Step: 1 Training Loss: 0.609959602355957 \n",
      "     Training Step: 2 Training Loss: 0.6123751401901245 \n",
      "     Training Step: 3 Training Loss: 0.6114148497581482 \n",
      "     Training Step: 4 Training Loss: 0.6139018535614014 \n",
      "     Training Step: 5 Training Loss: 0.6143971681594849 \n",
      "     Training Step: 6 Training Loss: 0.6146765947341919 \n",
      "     Training Step: 7 Training Loss: 0.6146184206008911 \n",
      "     Training Step: 8 Training Loss: 0.6141059398651123 \n",
      "     Training Step: 9 Training Loss: 0.6131359338760376 \n",
      "     Training Step: 10 Training Loss: 0.6164238452911377 \n",
      "     Training Step: 11 Training Loss: 0.6171393394470215 \n",
      "     Training Step: 12 Training Loss: 0.6166508197784424 \n",
      "     Training Step: 13 Training Loss: 0.6176856160163879 \n",
      "     Training Step: 14 Training Loss: 0.6154046058654785 \n",
      "     Training Step: 15 Training Loss: 0.6188068985939026 \n",
      "     Training Step: 16 Training Loss: 0.6155296564102173 \n",
      "     Training Step: 17 Training Loss: 0.6153775453567505 \n",
      "     Training Step: 18 Training Loss: 0.6121959090232849 \n",
      "     Training Step: 19 Training Loss: 0.6132394075393677 \n",
      "     Training Step: 20 Training Loss: 0.6102260947227478 \n",
      "     Training Step: 21 Training Loss: 0.6139582991600037 \n",
      "     Training Step: 22 Training Loss: 0.6137542128562927 \n",
      "     Training Step: 23 Training Loss: 0.6121652722358704 \n",
      "     Training Step: 24 Training Loss: 0.6196083426475525 \n",
      "     Training Step: 25 Training Loss: 0.6143375635147095 \n",
      "     Training Step: 26 Training Loss: 0.6156867742538452 \n",
      "     Training Step: 27 Training Loss: 0.6133526563644409 \n",
      "     Training Step: 28 Training Loss: 0.6093904376029968 \n",
      "     Training Step: 29 Training Loss: 0.6124887466430664 \n",
      "     Training Step: 30 Training Loss: 0.6182519793510437 \n",
      "     Training Step: 31 Training Loss: 0.6146565675735474 \n",
      "     Training Step: 32 Training Loss: 0.6118063926696777 \n",
      "     Training Step: 33 Training Loss: 0.6152909398078918 \n",
      "     Training Step: 34 Training Loss: 0.6129241585731506 \n",
      "     Training Step: 35 Training Loss: 0.616042971611023 \n",
      "     Training Step: 36 Training Loss: 0.6149303913116455 \n",
      "     Training Step: 37 Training Loss: 0.6151000261306763 \n",
      "     Training Step: 38 Training Loss: 0.610383927822113 \n",
      "     Training Step: 39 Training Loss: 0.6146799921989441 \n",
      "     Training Step: 40 Training Loss: 0.6144199371337891 \n",
      "     Training Step: 41 Training Loss: 0.6176828742027283 \n",
      "     Training Step: 42 Training Loss: 0.619681715965271 \n",
      "     Training Step: 43 Training Loss: 0.617149829864502 \n",
      "     Training Step: 44 Training Loss: 0.6140769720077515 \n",
      "     Training Step: 45 Training Loss: 0.6105155944824219 \n",
      "     Training Step: 46 Training Loss: 0.6177884340286255 \n",
      "     Training Step: 47 Training Loss: 0.6104331016540527 \n",
      "     Training Step: 48 Training Loss: 0.6129788756370544 \n",
      "     Training Step: 49 Training Loss: 0.6176100969314575 \n",
      "     Training Step: 50 Training Loss: 0.6154394149780273 \n",
      "     Training Step: 51 Training Loss: 0.6155133247375488 \n",
      "     Training Step: 52 Training Loss: 0.6105850338935852 \n",
      "     Training Step: 53 Training Loss: 0.6147720217704773 \n",
      "     Training Step: 54 Training Loss: 0.6116886734962463 \n",
      "     Training Step: 55 Training Loss: 0.6167206764221191 \n",
      "     Training Step: 56 Training Loss: 0.6118364930152893 \n",
      "     Training Step: 57 Training Loss: 0.6111568212509155 \n",
      "     Training Step: 58 Training Loss: 0.6140326261520386 \n",
      "     Training Step: 59 Training Loss: 0.6182448863983154 \n",
      "     Training Step: 60 Training Loss: 0.6122782826423645 \n",
      "     Training Step: 61 Training Loss: 0.6122270226478577 \n",
      "     Training Step: 62 Training Loss: 0.6137424111366272 \n",
      "     Training Step: 63 Training Loss: 0.6144537329673767 \n",
      "     Training Step: 64 Training Loss: 0.6134514212608337 \n",
      "     Training Step: 65 Training Loss: 0.6114000082015991 \n",
      "     Training Step: 66 Training Loss: 0.6092151403427124 \n",
      "     Training Step: 67 Training Loss: 0.6094347834587097 \n",
      "     Training Step: 68 Training Loss: 0.616790235042572 \n",
      "     Training Step: 69 Training Loss: 0.6122353076934814 \n",
      "     Training Step: 70 Training Loss: 0.6142469644546509 \n",
      "     Training Step: 71 Training Loss: 0.617172360420227 \n",
      "     Training Step: 72 Training Loss: 0.6149643659591675 \n",
      "     Training Step: 73 Training Loss: 0.6153432130813599 \n",
      "     Training Step: 74 Training Loss: 0.6164102554321289 \n",
      "     Training Step: 75 Training Loss: 0.613200843334198 \n",
      "     Training Step: 76 Training Loss: 0.6157448887825012 \n",
      "     Training Step: 77 Training Loss: 0.6135179400444031 \n",
      "     Training Step: 78 Training Loss: 0.6153727173805237 \n",
      "     Training Step: 79 Training Loss: 0.6167106628417969 \n",
      "     Training Step: 80 Training Loss: 0.6156833171844482 \n",
      "     Training Step: 81 Training Loss: 0.6097503900527954 \n",
      "     Training Step: 82 Training Loss: 0.6166406869888306 \n",
      "     Training Step: 83 Training Loss: 0.6127116680145264 \n",
      "     Training Step: 84 Training Loss: 0.6134752631187439 \n",
      "     Training Step: 85 Training Loss: 0.6122450232505798 \n",
      "     Training Step: 86 Training Loss: 0.614871084690094 \n",
      "     Training Step: 87 Training Loss: 0.6135914325714111 \n",
      "     Training Step: 88 Training Loss: 0.6142056584358215 \n",
      "     Training Step: 89 Training Loss: 0.6121198534965515 \n",
      "     Training Step: 90 Training Loss: 0.6114292740821838 \n",
      "     Training Step: 91 Training Loss: 0.6107091903686523 \n",
      "     Training Step: 92 Training Loss: 0.6143422722816467 \n",
      "     Training Step: 93 Training Loss: 0.6174033284187317 \n",
      "     Training Step: 94 Training Loss: 0.6127930879592896 \n",
      "     Training Step: 95 Training Loss: 0.6153242588043213 \n",
      "     Training Step: 96 Training Loss: 0.6148048639297485 \n",
      "     Training Step: 97 Training Loss: 0.6128398180007935 \n",
      "     Training Step: 98 Training Loss: 0.6126432418823242 \n",
      "     Training Step: 99 Training Loss: 0.6143462061882019 \n",
      "     Training Step: 100 Training Loss: 0.6144410371780396 \n",
      "     Training Step: 101 Training Loss: 0.6168015599250793 \n",
      "     Training Step: 102 Training Loss: 0.6152616739273071 \n",
      "     Training Step: 103 Training Loss: 0.6154000163078308 \n",
      "     Training Step: 104 Training Loss: 0.6130683422088623 \n",
      "     Training Step: 105 Training Loss: 0.62019944190979 \n",
      "     Training Step: 106 Training Loss: 0.6180853843688965 \n",
      "     Training Step: 107 Training Loss: 0.6136457920074463 \n",
      "     Training Step: 108 Training Loss: 0.6146562099456787 \n",
      "     Training Step: 109 Training Loss: 0.61577969789505 \n",
      "     Training Step: 110 Training Loss: 0.6161925196647644 \n",
      "     Training Step: 111 Training Loss: 0.6150967478752136 \n",
      "     Training Step: 112 Training Loss: 0.6151573657989502 \n",
      "     Training Step: 113 Training Loss: 0.6151788830757141 \n",
      "     Training Step: 114 Training Loss: 0.6100846529006958 \n",
      "     Training Step: 115 Training Loss: 0.618051290512085 \n",
      "     Training Step: 116 Training Loss: 0.6147438883781433 \n",
      "     Training Step: 117 Training Loss: 0.6116377115249634 \n",
      "     Training Step: 118 Training Loss: 0.615474283695221 \n",
      "     Training Step: 119 Training Loss: 0.6105829477310181 \n",
      "     Training Step: 120 Training Loss: 0.6139091849327087 \n",
      "     Training Step: 121 Training Loss: 0.6172084808349609 \n",
      "     Training Step: 122 Training Loss: 0.6184887886047363 \n",
      "     Training Step: 123 Training Loss: 0.6097080111503601 \n",
      "     Training Step: 124 Training Loss: 0.6106799840927124 \n",
      "     Training Step: 125 Training Loss: 0.6199063062667847 \n",
      "     Training Step: 126 Training Loss: 0.611602783203125 \n",
      "     Training Step: 127 Training Loss: 0.6152938008308411 \n",
      "     Training Step: 128 Training Loss: 0.6133129000663757 \n",
      "     Training Step: 129 Training Loss: 0.6171026825904846 \n",
      "     Training Step: 130 Training Loss: 0.612528383731842 \n",
      "     Training Step: 131 Training Loss: 0.6121610403060913 \n",
      "     Training Step: 132 Training Loss: 0.6158291101455688 \n",
      "     Training Step: 133 Training Loss: 0.6159986853599548 \n",
      "     Training Step: 134 Training Loss: 0.6141486763954163 \n",
      "     Training Step: 135 Training Loss: 0.6114875078201294 \n",
      "     Training Step: 136 Training Loss: 0.6115290522575378 \n",
      "     Training Step: 137 Training Loss: 0.6144963502883911 \n",
      "     Training Step: 138 Training Loss: 0.614612340927124 \n",
      "     Training Step: 139 Training Loss: 0.610053539276123 \n",
      "     Training Step: 140 Training Loss: 0.6149171590805054 \n",
      "     Training Step: 141 Training Loss: 0.6168100833892822 \n",
      "     Training Step: 142 Training Loss: 0.6166759729385376 \n",
      "     Training Step: 143 Training Loss: 0.6096941828727722 \n",
      "     Training Step: 144 Training Loss: 0.6194554567337036 \n",
      "     Training Step: 145 Training Loss: 0.6183287501335144 \n",
      "     Training Step: 146 Training Loss: 0.6166704297065735 \n",
      "     Training Step: 147 Training Loss: 0.6146729588508606 \n",
      "     Training Step: 148 Training Loss: 0.6120169758796692 \n",
      "     Training Step: 149 Training Loss: 0.6144769787788391 \n",
      "     Training Step: 150 Training Loss: 0.6177825331687927 \n",
      "     Training Step: 151 Training Loss: 0.6116524934768677 \n",
      "     Training Step: 152 Training Loss: 0.6115706562995911 \n",
      "     Training Step: 153 Training Loss: 0.6120770573616028 \n",
      "     Training Step: 154 Training Loss: 0.6185620427131653 \n",
      "     Training Step: 155 Training Loss: 0.6111516952514648 \n",
      "     Training Step: 156 Training Loss: 0.614699125289917 \n",
      "     Training Step: 157 Training Loss: 0.6146875023841858 \n",
      "     Training Step: 158 Training Loss: 0.6112030148506165 \n",
      "     Training Step: 159 Training Loss: 0.6152427792549133 \n",
      "     Training Step: 160 Training Loss: 0.6129110455513 \n",
      "     Training Step: 161 Training Loss: 0.6104908585548401 \n",
      "     Training Step: 162 Training Loss: 0.6131182909011841 \n",
      "     Training Step: 163 Training Loss: 0.6136490702629089 \n",
      "     Training Step: 164 Training Loss: 0.6132712364196777 \n",
      "     Training Step: 165 Training Loss: 0.6123732924461365 \n",
      "     Training Step: 166 Training Loss: 0.6127641797065735 \n",
      "     Training Step: 167 Training Loss: 0.6177986264228821 \n",
      "     Training Step: 168 Training Loss: 0.6116230487823486 \n",
      "     Training Step: 169 Training Loss: 0.6127519607543945 \n",
      "     Training Step: 170 Training Loss: 0.6122825741767883 \n",
      "     Training Step: 171 Training Loss: 0.6140227317810059 \n",
      "     Training Step: 172 Training Loss: 0.6115742325782776 \n",
      "     Training Step: 173 Training Loss: 0.6162222623825073 \n",
      "     Training Step: 174 Training Loss: 0.6141498684883118 \n",
      "     Training Step: 175 Training Loss: 0.6180360317230225 \n",
      "     Training Step: 176 Training Loss: 0.6128641963005066 \n",
      "     Training Step: 177 Training Loss: 0.6128705739974976 \n",
      "     Training Step: 178 Training Loss: 0.6167006492614746 \n",
      "     Training Step: 179 Training Loss: 0.6166063547134399 \n",
      "     Training Step: 180 Training Loss: 0.6138258576393127 \n",
      "     Training Step: 181 Training Loss: 0.6136313676834106 \n",
      "     Training Step: 182 Training Loss: 0.6107550263404846 \n",
      "     Training Step: 183 Training Loss: 0.6142836809158325 \n",
      "     Training Step: 184 Training Loss: 0.6082863807678223 \n",
      "     Training Step: 185 Training Loss: 0.6132974028587341 \n",
      "     Training Step: 186 Training Loss: 0.6146025061607361 \n",
      "     Training Step: 187 Training Loss: 0.6125341057777405 \n",
      "     Training Step: 188 Training Loss: 0.6105846762657166 \n",
      "     Training Step: 189 Training Loss: 0.6124622821807861 \n",
      "     Training Step: 190 Training Loss: 0.616366982460022 \n",
      "     Training Step: 191 Training Loss: 0.6165179014205933 \n",
      "     Training Step: 192 Training Loss: 0.6169143319129944 \n",
      "     Training Step: 193 Training Loss: 0.6168218851089478 \n",
      "     Training Step: 194 Training Loss: 0.6157495975494385 \n",
      "     Training Step: 195 Training Loss: 0.6169371604919434 \n",
      "     Training Step: 196 Training Loss: 0.6130623817443848 \n",
      "     Training Step: 197 Training Loss: 0.6167005300521851 \n",
      "     Training Step: 198 Training Loss: 0.610680103302002 \n",
      "     Training Step: 199 Training Loss: 0.6132842898368835 \n",
      "     Training Step: 200 Training Loss: 0.618579626083374 \n",
      "     Training Step: 201 Training Loss: 0.6109158992767334 \n",
      "     Training Step: 202 Training Loss: 0.6180174350738525 \n",
      "     Training Step: 203 Training Loss: 0.6100941300392151 \n",
      "     Training Step: 204 Training Loss: 0.6105943322181702 \n",
      "     Training Step: 205 Training Loss: 0.6114070415496826 \n",
      "     Training Step: 206 Training Loss: 0.6118321418762207 \n",
      "     Training Step: 207 Training Loss: 0.6114184260368347 \n",
      "     Training Step: 208 Training Loss: 0.6184548735618591 \n",
      "     Training Step: 209 Training Loss: 0.6177068948745728 \n",
      "     Training Step: 210 Training Loss: 0.6118203997612 \n",
      "     Training Step: 211 Training Loss: 0.6117780208587646 \n",
      "     Training Step: 212 Training Loss: 0.6157644391059875 \n",
      "     Training Step: 213 Training Loss: 0.6147317886352539 \n",
      "     Training Step: 214 Training Loss: 0.6188966631889343 \n",
      "     Training Step: 215 Training Loss: 0.6161234974861145 \n",
      "     Training Step: 216 Training Loss: 0.6209459900856018 \n",
      "     Training Step: 217 Training Loss: 0.6150697469711304 \n",
      "     Training Step: 218 Training Loss: 0.611799955368042 \n",
      "     Training Step: 219 Training Loss: 0.615941047668457 \n",
      "     Training Step: 220 Training Loss: 0.6183861494064331 \n",
      "     Training Step: 221 Training Loss: 0.6147114038467407 \n",
      "     Training Step: 222 Training Loss: 0.6132192611694336 \n",
      "     Training Step: 223 Training Loss: 0.6142295002937317 \n",
      "     Training Step: 224 Training Loss: 0.613781213760376 \n",
      "     Training Step: 225 Training Loss: 0.6109198331832886 \n",
      "     Training Step: 226 Training Loss: 0.6135116815567017 \n",
      "     Training Step: 227 Training Loss: 0.6140258312225342 \n",
      "     Training Step: 228 Training Loss: 0.6125175952911377 \n",
      "     Training Step: 229 Training Loss: 0.6115306615829468 \n",
      "     Training Step: 230 Training Loss: 0.6156087517738342 \n",
      "     Training Step: 231 Training Loss: 0.6123734712600708 \n",
      "     Training Step: 232 Training Loss: 0.6101651787757874 \n",
      "     Training Step: 233 Training Loss: 0.611515998840332 \n",
      "     Training Step: 234 Training Loss: 0.6175254583358765 \n",
      "     Training Step: 235 Training Loss: 0.6155585050582886 \n",
      "     Training Step: 236 Training Loss: 0.6118752956390381 \n",
      "     Training Step: 237 Training Loss: 0.6162676811218262 \n",
      "     Training Step: 238 Training Loss: 0.6134156584739685 \n",
      "     Training Step: 239 Training Loss: 0.6154342889785767 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6162527799606323 \n",
      "     Validation Step: 1 Validation Loss: 0.6104649305343628 \n",
      "     Validation Step: 2 Validation Loss: 0.6118646264076233 \n",
      "     Validation Step: 3 Validation Loss: 0.6177092790603638 \n",
      "     Validation Step: 4 Validation Loss: 0.6101114749908447 \n",
      "     Validation Step: 5 Validation Loss: 0.6153294444084167 \n",
      "     Validation Step: 6 Validation Loss: 0.6183624863624573 \n",
      "     Validation Step: 7 Validation Loss: 0.6157937049865723 \n",
      "     Validation Step: 8 Validation Loss: 0.6173272132873535 \n",
      "     Validation Step: 9 Validation Loss: 0.6100847721099854 \n",
      "     Validation Step: 10 Validation Loss: 0.6116294264793396 \n",
      "     Validation Step: 11 Validation Loss: 0.6141128540039062 \n",
      "     Validation Step: 12 Validation Loss: 0.6136514544487 \n",
      "     Validation Step: 13 Validation Loss: 0.6111484169960022 \n",
      "     Validation Step: 14 Validation Loss: 0.6156231164932251 \n",
      "     Validation Step: 15 Validation Loss: 0.6136619448661804 \n",
      "     Validation Step: 16 Validation Loss: 0.6101389527320862 \n",
      "     Validation Step: 17 Validation Loss: 0.6170461773872375 \n",
      "     Validation Step: 18 Validation Loss: 0.6145808696746826 \n",
      "     Validation Step: 19 Validation Loss: 0.6115415692329407 \n",
      "     Validation Step: 20 Validation Loss: 0.6184965372085571 \n",
      "     Validation Step: 21 Validation Loss: 0.6136326789855957 \n",
      "     Validation Step: 22 Validation Loss: 0.6176154613494873 \n",
      "     Validation Step: 23 Validation Loss: 0.6180849671363831 \n",
      "     Validation Step: 24 Validation Loss: 0.6133031845092773 \n",
      "     Validation Step: 25 Validation Loss: 0.6149016618728638 \n",
      "     Validation Step: 26 Validation Loss: 0.6128178238868713 \n",
      "     Validation Step: 27 Validation Loss: 0.6155850291252136 \n",
      "     Validation Step: 28 Validation Loss: 0.6106134653091431 \n",
      "     Validation Step: 29 Validation Loss: 0.610495924949646 \n",
      "     Validation Step: 30 Validation Loss: 0.6074803471565247 \n",
      "     Validation Step: 31 Validation Loss: 0.6160007119178772 \n",
      "     Validation Step: 32 Validation Loss: 0.612118124961853 \n",
      "     Validation Step: 33 Validation Loss: 0.6150433421134949 \n",
      "     Validation Step: 34 Validation Loss: 0.6129747033119202 \n",
      "     Validation Step: 35 Validation Loss: 0.6152360439300537 \n",
      "     Validation Step: 36 Validation Loss: 0.614835798740387 \n",
      "     Validation Step: 37 Validation Loss: 0.6182487607002258 \n",
      "     Validation Step: 38 Validation Loss: 0.6146396994590759 \n",
      "     Validation Step: 39 Validation Loss: 0.6185201406478882 \n",
      "     Validation Step: 40 Validation Loss: 0.6111668944358826 \n",
      "     Validation Step: 41 Validation Loss: 0.6142043471336365 \n",
      "     Validation Step: 42 Validation Loss: 0.6145380139350891 \n",
      "     Validation Step: 43 Validation Loss: 0.6141248345375061 \n",
      "     Validation Step: 44 Validation Loss: 0.6142585873603821 \n",
      "Epoch: 134\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6117798686027527 \n",
      "     Training Step: 1 Training Loss: 0.6115439534187317 \n",
      "     Training Step: 2 Training Loss: 0.609722375869751 \n",
      "     Training Step: 3 Training Loss: 0.6143428683280945 \n",
      "     Training Step: 4 Training Loss: 0.6152706742286682 \n",
      "     Training Step: 5 Training Loss: 0.6171473264694214 \n",
      "     Training Step: 6 Training Loss: 0.6127633452415466 \n",
      "     Training Step: 7 Training Loss: 0.6182202696800232 \n",
      "     Training Step: 8 Training Loss: 0.6155333518981934 \n",
      "     Training Step: 9 Training Loss: 0.6152852773666382 \n",
      "     Training Step: 10 Training Loss: 0.6114580035209656 \n",
      "     Training Step: 11 Training Loss: 0.6131548285484314 \n",
      "     Training Step: 12 Training Loss: 0.6147502064704895 \n",
      "     Training Step: 13 Training Loss: 0.6097396016120911 \n",
      "     Training Step: 14 Training Loss: 0.6115373373031616 \n",
      "     Training Step: 15 Training Loss: 0.6101847290992737 \n",
      "     Training Step: 16 Training Loss: 0.6167197823524475 \n",
      "     Training Step: 17 Training Loss: 0.6198891997337341 \n",
      "     Training Step: 18 Training Loss: 0.6180353164672852 \n",
      "     Training Step: 19 Training Loss: 0.6124597191810608 \n",
      "     Training Step: 20 Training Loss: 0.6127936244010925 \n",
      "     Training Step: 21 Training Loss: 0.616766095161438 \n",
      "     Training Step: 22 Training Loss: 0.6176941394805908 \n",
      "     Training Step: 23 Training Loss: 0.6131203770637512 \n",
      "     Training Step: 24 Training Loss: 0.6154034733772278 \n",
      "     Training Step: 25 Training Loss: 0.6140139102935791 \n",
      "     Training Step: 26 Training Loss: 0.6104094386100769 \n",
      "     Training Step: 27 Training Loss: 0.6100515723228455 \n",
      "     Training Step: 28 Training Loss: 0.6144975423812866 \n",
      "     Training Step: 29 Training Loss: 0.6122301816940308 \n",
      "     Training Step: 30 Training Loss: 0.6150854229927063 \n",
      "     Training Step: 31 Training Loss: 0.6176418662071228 \n",
      "     Training Step: 32 Training Loss: 0.6116862297058105 \n",
      "     Training Step: 33 Training Loss: 0.6150994300842285 \n",
      "     Training Step: 34 Training Loss: 0.6147720813751221 \n",
      "     Training Step: 35 Training Loss: 0.6184056401252747 \n",
      "     Training Step: 36 Training Loss: 0.6154175996780396 \n",
      "     Training Step: 37 Training Loss: 0.6099950671195984 \n",
      "     Training Step: 38 Training Loss: 0.61809241771698 \n",
      "     Training Step: 39 Training Loss: 0.6135177612304688 \n",
      "     Training Step: 40 Training Loss: 0.6177801489830017 \n",
      "     Training Step: 41 Training Loss: 0.6139259338378906 \n",
      "     Training Step: 42 Training Loss: 0.6154729127883911 \n",
      "     Training Step: 43 Training Loss: 0.6157791614532471 \n",
      "     Training Step: 44 Training Loss: 0.610396683216095 \n",
      "     Training Step: 45 Training Loss: 0.6122927665710449 \n",
      "     Training Step: 46 Training Loss: 0.6170896887779236 \n",
      "     Training Step: 47 Training Loss: 0.6101441383361816 \n",
      "     Training Step: 48 Training Loss: 0.6154136061668396 \n",
      "     Training Step: 49 Training Loss: 0.6153889894485474 \n",
      "     Training Step: 50 Training Loss: 0.6149301528930664 \n",
      "     Training Step: 51 Training Loss: 0.6105653643608093 \n",
      "     Training Step: 52 Training Loss: 0.6134066581726074 \n",
      "     Training Step: 53 Training Loss: 0.6166767477989197 \n",
      "     Training Step: 54 Training Loss: 0.6122317910194397 \n",
      "     Training Step: 55 Training Loss: 0.6115782260894775 \n",
      "     Training Step: 56 Training Loss: 0.6186111569404602 \n",
      "     Training Step: 57 Training Loss: 0.6183263063430786 \n",
      "     Training Step: 58 Training Loss: 0.6163508296012878 \n",
      "     Training Step: 59 Training Loss: 0.614334225654602 \n",
      "     Training Step: 60 Training Loss: 0.6125340461730957 \n",
      "     Training Step: 61 Training Loss: 0.6121301054954529 \n",
      "     Training Step: 62 Training Loss: 0.6168001890182495 \n",
      "     Training Step: 63 Training Loss: 0.6127204895019531 \n",
      "     Training Step: 64 Training Loss: 0.613822877407074 \n",
      "     Training Step: 65 Training Loss: 0.6114526391029358 \n",
      "     Training Step: 66 Training Loss: 0.6100705862045288 \n",
      "     Training Step: 67 Training Loss: 0.611787736415863 \n",
      "     Training Step: 68 Training Loss: 0.6106877326965332 \n",
      "     Training Step: 69 Training Loss: 0.6129162311553955 \n",
      "     Training Step: 70 Training Loss: 0.6180598139762878 \n",
      "     Training Step: 71 Training Loss: 0.6092066168785095 \n",
      "     Training Step: 72 Training Loss: 0.6082274913787842 \n",
      "     Training Step: 73 Training Loss: 0.6135988831520081 \n",
      "     Training Step: 74 Training Loss: 0.6166860461235046 \n",
      "     Training Step: 75 Training Loss: 0.6169753670692444 \n",
      "     Training Step: 76 Training Loss: 0.615848958492279 \n",
      "     Training Step: 77 Training Loss: 0.610563337802887 \n",
      "     Training Step: 78 Training Loss: 0.6133549809455872 \n",
      "     Training Step: 79 Training Loss: 0.6185978055000305 \n",
      "     Training Step: 80 Training Loss: 0.6167283058166504 \n",
      "     Training Step: 81 Training Loss: 0.6167044639587402 \n",
      "     Training Step: 82 Training Loss: 0.6156728267669678 \n",
      "     Training Step: 83 Training Loss: 0.6128820180892944 \n",
      "     Training Step: 84 Training Loss: 0.6121835708618164 \n",
      "     Training Step: 85 Training Loss: 0.6146820783615112 \n",
      "     Training Step: 86 Training Loss: 0.6106945276260376 \n",
      "     Training Step: 87 Training Loss: 0.6111688017845154 \n",
      "     Training Step: 88 Training Loss: 0.6156872510910034 \n",
      "     Training Step: 89 Training Loss: 0.6108952760696411 \n",
      "     Training Step: 90 Training Loss: 0.6104869246482849 \n",
      "     Training Step: 91 Training Loss: 0.6093972325325012 \n",
      "     Training Step: 92 Training Loss: 0.6174057126045227 \n",
      "     Training Step: 93 Training Loss: 0.6146960854530334 \n",
      "     Training Step: 94 Training Loss: 0.6146210432052612 \n",
      "     Training Step: 95 Training Loss: 0.6140362620353699 \n",
      "     Training Step: 96 Training Loss: 0.6153361797332764 \n",
      "     Training Step: 97 Training Loss: 0.610480785369873 \n",
      "     Training Step: 98 Training Loss: 0.6157613396644592 \n",
      "     Training Step: 99 Training Loss: 0.615972101688385 \n",
      "     Training Step: 100 Training Loss: 0.6156203746795654 \n",
      "     Training Step: 101 Training Loss: 0.6137351393699646 \n",
      "     Training Step: 102 Training Loss: 0.6118766069412231 \n",
      "     Training Step: 103 Training Loss: 0.6128640174865723 \n",
      "     Training Step: 104 Training Loss: 0.6139128804206848 \n",
      "     Training Step: 105 Training Loss: 0.613762378692627 \n",
      "     Training Step: 106 Training Loss: 0.6167991757392883 \n",
      "     Training Step: 107 Training Loss: 0.6137434840202332 \n",
      "     Training Step: 108 Training Loss: 0.613885223865509 \n",
      "     Training Step: 109 Training Loss: 0.6177486777305603 \n",
      "     Training Step: 110 Training Loss: 0.6147063374519348 \n",
      "     Training Step: 111 Training Loss: 0.6132953763008118 \n",
      "     Training Step: 112 Training Loss: 0.616603672504425 \n",
      "     Training Step: 113 Training Loss: 0.6118404269218445 \n",
      "     Training Step: 114 Training Loss: 0.6162281632423401 \n",
      "     Training Step: 115 Training Loss: 0.6118110418319702 \n",
      "     Training Step: 116 Training Loss: 0.6124938726425171 \n",
      "     Training Step: 117 Training Loss: 0.6180590987205505 \n",
      "     Training Step: 118 Training Loss: 0.6141504049301147 \n",
      "     Training Step: 119 Training Loss: 0.6136193871498108 \n",
      "     Training Step: 120 Training Loss: 0.6176855564117432 \n",
      "     Training Step: 121 Training Loss: 0.6123806834220886 \n",
      "     Training Step: 122 Training Loss: 0.6108953952789307 \n",
      "     Training Step: 123 Training Loss: 0.6202210187911987 \n",
      "     Training Step: 124 Training Loss: 0.6121520400047302 \n",
      "     Training Step: 125 Training Loss: 0.6141073703765869 \n",
      "     Training Step: 126 Training Loss: 0.6151745915412903 \n",
      "     Training Step: 127 Training Loss: 0.6140202283859253 \n",
      "     Training Step: 128 Training Loss: 0.6188687682151794 \n",
      "     Training Step: 129 Training Loss: 0.6128408312797546 \n",
      "     Training Step: 130 Training Loss: 0.6178034543991089 \n",
      "     Training Step: 131 Training Loss: 0.6142089366912842 \n",
      "     Training Step: 132 Training Loss: 0.6166921854019165 \n",
      "     Training Step: 133 Training Loss: 0.614480197429657 \n",
      "     Training Step: 134 Training Loss: 0.6136544942855835 \n",
      "     Training Step: 135 Training Loss: 0.6146483421325684 \n",
      "     Training Step: 136 Training Loss: 0.6208962202072144 \n",
      "     Training Step: 137 Training Loss: 0.6148699522018433 \n",
      "     Training Step: 138 Training Loss: 0.6132211685180664 \n",
      "     Training Step: 139 Training Loss: 0.6116638779640198 \n",
      "     Training Step: 140 Training Loss: 0.6182281374931335 \n",
      "     Training Step: 141 Training Loss: 0.616401195526123 \n",
      "     Training Step: 142 Training Loss: 0.6115021705627441 \n",
      "     Training Step: 143 Training Loss: 0.6167946457862854 \n",
      "     Training Step: 144 Training Loss: 0.6144540309906006 \n",
      "     Training Step: 145 Training Loss: 0.6149561405181885 \n",
      "     Training Step: 146 Training Loss: 0.6157529354095459 \n",
      "     Training Step: 147 Training Loss: 0.6111637353897095 \n",
      "     Training Step: 148 Training Loss: 0.6171919107437134 \n",
      "     Training Step: 149 Training Loss: 0.616206169128418 \n",
      "     Training Step: 150 Training Loss: 0.6153349280357361 \n",
      "     Training Step: 151 Training Loss: 0.6107227802276611 \n",
      "     Training Step: 152 Training Loss: 0.6120697855949402 \n",
      "     Training Step: 153 Training Loss: 0.6105701327323914 \n",
      "     Training Step: 154 Training Loss: 0.6154459118843079 \n",
      "     Training Step: 155 Training Loss: 0.6161184310913086 \n",
      "     Training Step: 156 Training Loss: 0.6115232706069946 \n",
      "     Training Step: 157 Training Loss: 0.6115145087242126 \n",
      "     Training Step: 158 Training Loss: 0.6123685836791992 \n",
      "     Training Step: 159 Training Loss: 0.6129167079925537 \n",
      "     Training Step: 160 Training Loss: 0.6147955656051636 \n",
      "     Training Step: 161 Training Loss: 0.6164161562919617 \n",
      "     Training Step: 162 Training Loss: 0.6094356179237366 \n",
      "     Training Step: 163 Training Loss: 0.6142879128456116 \n",
      "     Training Step: 164 Training Loss: 0.6157531142234802 \n",
      "     Training Step: 165 Training Loss: 0.6125220656394958 \n",
      "     Training Step: 166 Training Loss: 0.6100416779518127 \n",
      "     Training Step: 167 Training Loss: 0.6188593506813049 \n",
      "     Training Step: 168 Training Loss: 0.6146641373634338 \n",
      "     Training Step: 169 Training Loss: 0.6132016181945801 \n",
      "     Training Step: 170 Training Loss: 0.6194515228271484 \n",
      "     Training Step: 171 Training Loss: 0.616672933101654 \n",
      "     Training Step: 172 Training Loss: 0.6123838424682617 \n",
      "     Training Step: 173 Training Loss: 0.6130729913711548 \n",
      "     Training Step: 174 Training Loss: 0.6114192605018616 \n",
      "     Training Step: 175 Training Loss: 0.6145952343940735 \n",
      "     Training Step: 176 Training Loss: 0.6161942481994629 \n",
      "     Training Step: 177 Training Loss: 0.616645097732544 \n",
      "     Training Step: 178 Training Loss: 0.6152408123016357 \n",
      "     Training Step: 179 Training Loss: 0.6132740378379822 \n",
      "     Training Step: 180 Training Loss: 0.6121514439582825 \n",
      "     Training Step: 181 Training Loss: 0.6131908893585205 \n",
      "     Training Step: 182 Training Loss: 0.6143736839294434 \n",
      "     Training Step: 183 Training Loss: 0.6152863502502441 \n",
      "     Training Step: 184 Training Loss: 0.6122348308563232 \n",
      "     Training Step: 185 Training Loss: 0.614700198173523 \n",
      "     Training Step: 186 Training Loss: 0.6171373724937439 \n",
      "     Training Step: 187 Training Loss: 0.6125185489654541 \n",
      "     Training Step: 188 Training Loss: 0.611834704875946 \n",
      "     Training Step: 189 Training Loss: 0.6113956570625305 \n",
      "     Training Step: 190 Training Loss: 0.6146401166915894 \n",
      "     Training Step: 191 Training Loss: 0.613497793674469 \n",
      "     Training Step: 192 Training Loss: 0.6129595637321472 \n",
      "     Training Step: 193 Training Loss: 0.6130583882331848 \n",
      "     Training Step: 194 Training Loss: 0.6097078919410706 \n",
      "     Training Step: 195 Training Loss: 0.6169081926345825 \n",
      "     Training Step: 196 Training Loss: 0.6107240319252014 \n",
      "     Training Step: 197 Training Loss: 0.6115998029708862 \n",
      "     Training Step: 198 Training Loss: 0.6132925748825073 \n",
      "     Training Step: 199 Training Loss: 0.61553555727005 \n",
      "     Training Step: 200 Training Loss: 0.6141526699066162 \n",
      "     Training Step: 201 Training Loss: 0.6153774857521057 \n",
      "     Training Step: 202 Training Loss: 0.6105811595916748 \n",
      "     Training Step: 203 Training Loss: 0.6196461915969849 \n",
      "     Training Step: 204 Training Loss: 0.6133137941360474 \n",
      "     Training Step: 205 Training Loss: 0.6116431355476379 \n",
      "     Training Step: 206 Training Loss: 0.611626386642456 \n",
      "     Training Step: 207 Training Loss: 0.61846524477005 \n",
      "     Training Step: 208 Training Loss: 0.6134594678878784 \n",
      "     Training Step: 209 Training Loss: 0.6122974157333374 \n",
      "     Training Step: 210 Training Loss: 0.6155310869216919 \n",
      "     Training Step: 211 Training Loss: 0.6144214868545532 \n",
      "     Training Step: 212 Training Loss: 0.6150649189949036 \n",
      "     Training Step: 213 Training Loss: 0.6143434643745422 \n",
      "     Training Step: 214 Training Loss: 0.6160342693328857 \n",
      "     Training Step: 215 Training Loss: 0.6151540875434875 \n",
      "     Training Step: 216 Training Loss: 0.6127529740333557 \n",
      "     Training Step: 217 Training Loss: 0.6147241592407227 \n",
      "     Training Step: 218 Training Loss: 0.6146891713142395 \n",
      "     Training Step: 219 Training Loss: 0.6159956455230713 \n",
      "     Training Step: 220 Training Loss: 0.6126469969749451 \n",
      "     Training Step: 221 Training Loss: 0.6118306517601013 \n",
      "     Training Step: 222 Training Loss: 0.6171578168869019 \n",
      "     Training Step: 223 Training Loss: 0.6144409775733948 \n",
      "     Training Step: 224 Training Loss: 0.6136419177055359 \n",
      "     Training Step: 225 Training Loss: 0.6196922063827515 \n",
      "     Training Step: 226 Training Loss: 0.6146095395088196 \n",
      "     Training Step: 227 Training Loss: 0.6142419576644897 \n",
      "     Training Step: 228 Training Loss: 0.6140730977058411 \n",
      "     Training Step: 229 Training Loss: 0.6174795627593994 \n",
      "     Training Step: 230 Training Loss: 0.6120197772979736 \n",
      "     Training Step: 231 Training Loss: 0.6134695410728455 \n",
      "     Training Step: 232 Training Loss: 0.6176884174346924 \n",
      "     Training Step: 233 Training Loss: 0.6164835691452026 \n",
      "     Training Step: 234 Training Loss: 0.6142228841781616 \n",
      "     Training Step: 235 Training Loss: 0.6112146973609924 \n",
      "     Training Step: 236 Training Loss: 0.6114296317100525 \n",
      "     Training Step: 237 Training Loss: 0.613301694393158 \n",
      "     Training Step: 238 Training Loss: 0.6184345483779907 \n",
      "     Training Step: 239 Training Loss: 0.6149167418479919 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.613663375377655 \n",
      "     Validation Step: 1 Validation Loss: 0.6105183362960815 \n",
      "     Validation Step: 2 Validation Loss: 0.6142579317092896 \n",
      "     Validation Step: 3 Validation Loss: 0.6184844374656677 \n",
      "     Validation Step: 4 Validation Loss: 0.6176823377609253 \n",
      "     Validation Step: 5 Validation Loss: 0.6157823801040649 \n",
      "     Validation Step: 6 Validation Loss: 0.6183295249938965 \n",
      "     Validation Step: 7 Validation Loss: 0.6115579605102539 \n",
      "     Validation Step: 8 Validation Loss: 0.6145374774932861 \n",
      "     Validation Step: 9 Validation Loss: 0.6142033338546753 \n",
      "     Validation Step: 10 Validation Loss: 0.6133099794387817 \n",
      "     Validation Step: 11 Validation Loss: 0.6101619005203247 \n",
      "     Validation Step: 12 Validation Loss: 0.6173009872436523 \n",
      "     Validation Step: 13 Validation Loss: 0.6153140068054199 \n",
      "     Validation Step: 14 Validation Loss: 0.6155679225921631 \n",
      "     Validation Step: 15 Validation Loss: 0.6180534362792969 \n",
      "     Validation Step: 16 Validation Loss: 0.6128336191177368 \n",
      "     Validation Step: 17 Validation Loss: 0.6111786365509033 \n",
      "     Validation Step: 18 Validation Loss: 0.612130880355835 \n",
      "     Validation Step: 19 Validation Loss: 0.6111640930175781 \n",
      "     Validation Step: 20 Validation Loss: 0.6129867434501648 \n",
      "     Validation Step: 21 Validation Loss: 0.6159800291061401 \n",
      "     Validation Step: 22 Validation Loss: 0.6101096272468567 \n",
      "     Validation Step: 23 Validation Loss: 0.6104868054389954 \n",
      "     Validation Step: 24 Validation Loss: 0.6145684719085693 \n",
      "     Validation Step: 25 Validation Loss: 0.6141287684440613 \n",
      "     Validation Step: 26 Validation Loss: 0.6170167326927185 \n",
      "     Validation Step: 27 Validation Loss: 0.614638090133667 \n",
      "     Validation Step: 28 Validation Loss: 0.6075241565704346 \n",
      "     Validation Step: 29 Validation Loss: 0.6162385940551758 \n",
      "     Validation Step: 30 Validation Loss: 0.6156075596809387 \n",
      "     Validation Step: 31 Validation Loss: 0.6148945093154907 \n",
      "     Validation Step: 32 Validation Loss: 0.6184669137001038 \n",
      "     Validation Step: 33 Validation Loss: 0.6118818521499634 \n",
      "     Validation Step: 34 Validation Loss: 0.6136438846588135 \n",
      "     Validation Step: 35 Validation Loss: 0.6150308847427368 \n",
      "     Validation Step: 36 Validation Loss: 0.6175947189331055 \n",
      "     Validation Step: 37 Validation Loss: 0.6148353815078735 \n",
      "     Validation Step: 38 Validation Loss: 0.6106308102607727 \n",
      "     Validation Step: 39 Validation Loss: 0.6101369261741638 \n",
      "     Validation Step: 40 Validation Loss: 0.6141152381896973 \n",
      "     Validation Step: 41 Validation Loss: 0.6152233481407166 \n",
      "     Validation Step: 42 Validation Loss: 0.6182153224945068 \n",
      "     Validation Step: 43 Validation Loss: 0.6136399507522583 \n",
      "     Validation Step: 44 Validation Loss: 0.6116436719894409 \n",
      "Epoch: 135\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6146461963653564 \n",
      "     Training Step: 1 Training Loss: 0.6103806495666504 \n",
      "     Training Step: 2 Training Loss: 0.6117811799049377 \n",
      "     Training Step: 3 Training Loss: 0.6164027452468872 \n",
      "     Training Step: 4 Training Loss: 0.6117901802062988 \n",
      "     Training Step: 5 Training Loss: 0.6202326416969299 \n",
      "     Training Step: 6 Training Loss: 0.6132998466491699 \n",
      "     Training Step: 7 Training Loss: 0.6115794777870178 \n",
      "     Training Step: 8 Training Loss: 0.6171682476997375 \n",
      "     Training Step: 9 Training Loss: 0.6114118695259094 \n",
      "     Training Step: 10 Training Loss: 0.6147930026054382 \n",
      "     Training Step: 11 Training Loss: 0.6120643019676208 \n",
      "     Training Step: 12 Training Loss: 0.6143367886543274 \n",
      "     Training Step: 13 Training Loss: 0.6167625784873962 \n",
      "     Training Step: 14 Training Loss: 0.6176895499229431 \n",
      "     Training Step: 15 Training Loss: 0.6155341267585754 \n",
      "     Training Step: 16 Training Loss: 0.6115614175796509 \n",
      "     Training Step: 17 Training Loss: 0.6131905913352966 \n",
      "     Training Step: 18 Training Loss: 0.6128764748573303 \n",
      "     Training Step: 19 Training Loss: 0.6194273829460144 \n",
      "     Training Step: 20 Training Loss: 0.616911768913269 \n",
      "     Training Step: 21 Training Loss: 0.6171814799308777 \n",
      "     Training Step: 22 Training Loss: 0.6094971299171448 \n",
      "     Training Step: 23 Training Loss: 0.6196691989898682 \n",
      "     Training Step: 24 Training Loss: 0.6130736470222473 \n",
      "     Training Step: 25 Training Loss: 0.6166486740112305 \n",
      "     Training Step: 26 Training Loss: 0.6155104041099548 \n",
      "     Training Step: 27 Training Loss: 0.6149169206619263 \n",
      "     Training Step: 28 Training Loss: 0.6156632900238037 \n",
      "     Training Step: 29 Training Loss: 0.6159934997558594 \n",
      "     Training Step: 30 Training Loss: 0.6100984215736389 \n",
      "     Training Step: 31 Training Loss: 0.6139363050460815 \n",
      "     Training Step: 32 Training Loss: 0.6182277798652649 \n",
      "     Training Step: 33 Training Loss: 0.6188104748725891 \n",
      "     Training Step: 34 Training Loss: 0.6097273826599121 \n",
      "     Training Step: 35 Training Loss: 0.6115297079086304 \n",
      "     Training Step: 36 Training Loss: 0.6141505241394043 \n",
      "     Training Step: 37 Training Loss: 0.6173802614212036 \n",
      "     Training Step: 38 Training Loss: 0.6093935370445251 \n",
      "     Training Step: 39 Training Loss: 0.6171520352363586 \n",
      "     Training Step: 40 Training Loss: 0.61415034532547 \n",
      "     Training Step: 41 Training Loss: 0.6114681959152222 \n",
      "     Training Step: 42 Training Loss: 0.6139081716537476 \n",
      "     Training Step: 43 Training Loss: 0.6138166189193726 \n",
      "     Training Step: 44 Training Loss: 0.6132903695106506 \n",
      "     Training Step: 45 Training Loss: 0.6146900057792664 \n",
      "     Training Step: 46 Training Loss: 0.6105597019195557 \n",
      "     Training Step: 47 Training Loss: 0.6137369275093079 \n",
      "     Training Step: 48 Training Loss: 0.6144511699676514 \n",
      "     Training Step: 49 Training Loss: 0.6132040023803711 \n",
      "     Training Step: 50 Training Loss: 0.6111301183700562 \n",
      "     Training Step: 51 Training Loss: 0.6143398284912109 \n",
      "     Training Step: 52 Training Loss: 0.615686297416687 \n",
      "     Training Step: 53 Training Loss: 0.615267276763916 \n",
      "     Training Step: 54 Training Loss: 0.6167104244232178 \n",
      "     Training Step: 55 Training Loss: 0.6178153157234192 \n",
      "     Training Step: 56 Training Loss: 0.6105967164039612 \n",
      "     Training Step: 57 Training Loss: 0.6118056178092957 \n",
      "     Training Step: 58 Training Loss: 0.6142841577529907 \n",
      "     Training Step: 59 Training Loss: 0.6104938387870789 \n",
      "     Training Step: 60 Training Loss: 0.6105093955993652 \n",
      "     Training Step: 61 Training Loss: 0.6158289909362793 \n",
      "     Training Step: 62 Training Loss: 0.6167221665382385 \n",
      "     Training Step: 63 Training Loss: 0.615338921546936 \n",
      "     Training Step: 64 Training Loss: 0.6113939881324768 \n",
      "     Training Step: 65 Training Loss: 0.6132029891014099 \n",
      "     Training Step: 66 Training Loss: 0.6153033375740051 \n",
      "     Training Step: 67 Training Loss: 0.614020586013794 \n",
      "     Training Step: 68 Training Loss: 0.6146888136863708 \n",
      "     Training Step: 69 Training Loss: 0.6145980358123779 \n",
      "     Training Step: 70 Training Loss: 0.6141059398651123 \n",
      "     Training Step: 71 Training Loss: 0.6127623915672302 \n",
      "     Training Step: 72 Training Loss: 0.6097298264503479 \n",
      "     Training Step: 73 Training Loss: 0.6159491539001465 \n",
      "     Training Step: 74 Training Loss: 0.618024468421936 \n",
      "     Training Step: 75 Training Loss: 0.6160318851470947 \n",
      "     Training Step: 76 Training Loss: 0.6137648820877075 \n",
      "     Training Step: 77 Training Loss: 0.6146626472473145 \n",
      "     Training Step: 78 Training Loss: 0.6106957197189331 \n",
      "     Training Step: 79 Training Loss: 0.6123887300491333 \n",
      "     Training Step: 80 Training Loss: 0.6114412546157837 \n",
      "     Training Step: 81 Training Loss: 0.6134980320930481 \n",
      "     Training Step: 82 Training Loss: 0.6142102479934692 \n",
      "     Training Step: 83 Training Loss: 0.6118226051330566 \n",
      "     Training Step: 84 Training Loss: 0.6121546030044556 \n",
      "     Training Step: 85 Training Loss: 0.6168132424354553 \n",
      "     Training Step: 86 Training Loss: 0.615401029586792 \n",
      "     Training Step: 87 Training Loss: 0.6124608516693115 \n",
      "     Training Step: 88 Training Loss: 0.6111509203910828 \n",
      "     Training Step: 89 Training Loss: 0.6143466830253601 \n",
      "     Training Step: 90 Training Loss: 0.6133529543876648 \n",
      "     Training Step: 91 Training Loss: 0.6115992069244385 \n",
      "     Training Step: 92 Training Loss: 0.6176567077636719 \n",
      "     Training Step: 93 Training Loss: 0.6116365194320679 \n",
      "     Training Step: 94 Training Loss: 0.6183294653892517 \n",
      "     Training Step: 95 Training Loss: 0.6166765093803406 \n",
      "     Training Step: 96 Training Loss: 0.6116934418678284 \n",
      "     Training Step: 97 Training Loss: 0.616800844669342 \n",
      "     Training Step: 98 Training Loss: 0.6116432547569275 \n",
      "     Training Step: 99 Training Loss: 0.6167974472045898 \n",
      "     Training Step: 100 Training Loss: 0.6140744090080261 \n",
      "     Training Step: 101 Training Loss: 0.6164798736572266 \n",
      "     Training Step: 102 Training Loss: 0.614365816116333 \n",
      "     Training Step: 103 Training Loss: 0.6151577830314636 \n",
      "     Training Step: 104 Training Loss: 0.6147083640098572 \n",
      "     Training Step: 105 Training Loss: 0.6177731156349182 \n",
      "     Training Step: 106 Training Loss: 0.6107578277587891 \n",
      "     Training Step: 107 Training Loss: 0.6105960607528687 \n",
      "     Training Step: 108 Training Loss: 0.6114212274551392 \n",
      "     Training Step: 109 Training Loss: 0.6209145188331604 \n",
      "     Training Step: 110 Training Loss: 0.6144971251487732 \n",
      "     Training Step: 111 Training Loss: 0.6157407164573669 \n",
      "     Training Step: 112 Training Loss: 0.6115312576293945 \n",
      "     Training Step: 113 Training Loss: 0.6171431541442871 \n",
      "     Training Step: 114 Training Loss: 0.6121601462364197 \n",
      "     Training Step: 115 Training Loss: 0.6166515350341797 \n",
      "     Training Step: 116 Training Loss: 0.6167187094688416 \n",
      "     Training Step: 117 Training Loss: 0.6156029105186462 \n",
      "     Training Step: 118 Training Loss: 0.6146690249443054 \n",
      "     Training Step: 119 Training Loss: 0.613639235496521 \n",
      "     Training Step: 120 Training Loss: 0.6126448512077332 \n",
      "     Training Step: 121 Training Loss: 0.6116279363632202 \n",
      "     Training Step: 122 Training Loss: 0.6125350594520569 \n",
      "     Training Step: 123 Training Loss: 0.6099885106086731 \n",
      "     Training Step: 124 Training Loss: 0.6177622079849243 \n",
      "     Training Step: 125 Training Loss: 0.6125309467315674 \n",
      "     Training Step: 126 Training Loss: 0.609717607498169 \n",
      "     Training Step: 127 Training Loss: 0.6138833165168762 \n",
      "     Training Step: 128 Training Loss: 0.612695574760437 \n",
      "     Training Step: 129 Training Loss: 0.6150908470153809 \n",
      "     Training Step: 130 Training Loss: 0.614615797996521 \n",
      "     Training Step: 131 Training Loss: 0.6100329756736755 \n",
      "     Training Step: 132 Training Loss: 0.6144400238990784 \n",
      "     Training Step: 133 Training Loss: 0.6101155281066895 \n",
      "     Training Step: 134 Training Loss: 0.610867440700531 \n",
      "     Training Step: 135 Training Loss: 0.6196750998497009 \n",
      "     Training Step: 136 Training Loss: 0.6180704832077026 \n",
      "     Training Step: 137 Training Loss: 0.6154324412345886 \n",
      "     Training Step: 138 Training Loss: 0.6147059798240662 \n",
      "     Training Step: 139 Training Loss: 0.6146383881568909 \n",
      "     Training Step: 140 Training Loss: 0.6082625985145569 \n",
      "     Training Step: 141 Training Loss: 0.6147258877754211 \n",
      "     Training Step: 142 Training Loss: 0.6128488183021545 \n",
      "     Training Step: 143 Training Loss: 0.6100578308105469 \n",
      "     Training Step: 144 Training Loss: 0.6176859140396118 \n",
      "     Training Step: 145 Training Loss: 0.6112052798271179 \n",
      "     Training Step: 146 Training Loss: 0.6136148571968079 \n",
      "     Training Step: 147 Training Loss: 0.610397458076477 \n",
      "     Training Step: 148 Training Loss: 0.6153748631477356 \n",
      "     Training Step: 149 Training Loss: 0.6171059608459473 \n",
      "     Training Step: 150 Training Loss: 0.6169010996818542 \n",
      "     Training Step: 151 Training Loss: 0.612230122089386 \n",
      "     Training Step: 152 Training Loss: 0.6129171252250671 \n",
      "     Training Step: 153 Training Loss: 0.6154769062995911 \n",
      "     Training Step: 154 Training Loss: 0.616713285446167 \n",
      "     Training Step: 155 Training Loss: 0.6121199727058411 \n",
      "     Training Step: 156 Training Loss: 0.6135103106498718 \n",
      "     Training Step: 157 Training Loss: 0.6128636598587036 \n",
      "     Training Step: 158 Training Loss: 0.6140139698982239 \n",
      "     Training Step: 159 Training Loss: 0.6166532635688782 \n",
      "     Training Step: 160 Training Loss: 0.6180672645568848 \n",
      "     Training Step: 161 Training Loss: 0.6114409565925598 \n",
      "     Training Step: 162 Training Loss: 0.6135882139205933 \n",
      "     Training Step: 163 Training Loss: 0.6122310161590576 \n",
      "     Training Step: 164 Training Loss: 0.6122921109199524 \n",
      "     Training Step: 165 Training Loss: 0.6133156418800354 \n",
      "     Training Step: 166 Training Loss: 0.6144771575927734 \n",
      "     Training Step: 167 Training Loss: 0.6132815480232239 \n",
      "     Training Step: 168 Training Loss: 0.6146082282066345 \n",
      "     Training Step: 169 Training Loss: 0.6155340671539307 \n",
      "     Training Step: 170 Training Loss: 0.6118748784065247 \n",
      "     Training Step: 171 Training Loss: 0.6186079382896423 \n",
      "     Training Step: 172 Training Loss: 0.6122304201126099 \n",
      "     Training Step: 173 Training Loss: 0.6115254163742065 \n",
      "     Training Step: 174 Training Loss: 0.6188787221908569 \n",
      "     Training Step: 175 Training Loss: 0.6129136085510254 \n",
      "     Training Step: 176 Training Loss: 0.6151003241539001 \n",
      "     Training Step: 177 Training Loss: 0.6147712469100952 \n",
      "     Training Step: 178 Training Loss: 0.6142452359199524 \n",
      "     Training Step: 179 Training Loss: 0.6129661202430725 \n",
      "     Training Step: 180 Training Loss: 0.6105912327766418 \n",
      "     Training Step: 181 Training Loss: 0.6198596358299255 \n",
      "     Training Step: 182 Training Loss: 0.6131421327590942 \n",
      "     Training Step: 183 Training Loss: 0.615440845489502 \n",
      "     Training Step: 184 Training Loss: 0.6108832955360413 \n",
      "     Training Step: 185 Training Loss: 0.6166101694107056 \n",
      "     Training Step: 186 Training Loss: 0.6181005239486694 \n",
      "     Training Step: 187 Training Loss: 0.6134591698646545 \n",
      "     Training Step: 188 Training Loss: 0.616414487361908 \n",
      "     Training Step: 189 Training Loss: 0.6142056584358215 \n",
      "     Training Step: 190 Training Loss: 0.615062415599823 \n",
      "     Training Step: 191 Training Loss: 0.6118372678756714 \n",
      "     Training Step: 192 Training Loss: 0.6163429617881775 \n",
      "     Training Step: 193 Training Loss: 0.614867091178894 \n",
      "     Training Step: 194 Training Loss: 0.6123011708259583 \n",
      "     Training Step: 195 Training Loss: 0.6149262189865112 \n",
      "     Training Step: 196 Training Loss: 0.6132774353027344 \n",
      "     Training Step: 197 Training Loss: 0.6157795786857605 \n",
      "     Training Step: 198 Training Loss: 0.6152827143669128 \n",
      "     Training Step: 199 Training Loss: 0.6137388944625854 \n",
      "     Training Step: 200 Training Loss: 0.6124904751777649 \n",
      "     Training Step: 201 Training Loss: 0.6161152124404907 \n",
      "     Training Step: 202 Training Loss: 0.6152384877204895 \n",
      "     Training Step: 203 Training Loss: 0.6177111864089966 \n",
      "     Training Step: 204 Training Loss: 0.6107402443885803 \n",
      "     Training Step: 205 Training Loss: 0.615176260471344 \n",
      "     Training Step: 206 Training Loss: 0.6174818873405457 \n",
      "     Training Step: 207 Training Loss: 0.6130601763725281 \n",
      "     Training Step: 208 Training Loss: 0.6140337586402893 \n",
      "     Training Step: 209 Training Loss: 0.6121456027030945 \n",
      "     Training Step: 210 Training Loss: 0.6127533912658691 \n",
      "     Training Step: 211 Training Loss: 0.6101824045181274 \n",
      "     Training Step: 212 Training Loss: 0.6125128865242004 \n",
      "     Training Step: 213 Training Loss: 0.6134066581726074 \n",
      "     Training Step: 214 Training Loss: 0.6123749017715454 \n",
      "     Training Step: 215 Training Loss: 0.6153959035873413 \n",
      "     Training Step: 216 Training Loss: 0.6184908151626587 \n",
      "     Training Step: 217 Training Loss: 0.6184664964675903 \n",
      "     Training Step: 218 Training Loss: 0.6127951145172119 \n",
      "     Training Step: 219 Training Loss: 0.615422785282135 \n",
      "     Training Step: 220 Training Loss: 0.6182259321212769 \n",
      "     Training Step: 221 Training Loss: 0.6120169162750244 \n",
      "     Training Step: 222 Training Loss: 0.6118394136428833 \n",
      "     Training Step: 223 Training Loss: 0.6134742498397827 \n",
      "     Training Step: 224 Training Loss: 0.6149547696113586 \n",
      "     Training Step: 225 Training Loss: 0.6157414317131042 \n",
      "     Training Step: 226 Training Loss: 0.614746630191803 \n",
      "     Training Step: 227 Training Loss: 0.6106768846511841 \n",
      "     Training Step: 228 Training Loss: 0.612384021282196 \n",
      "     Training Step: 229 Training Loss: 0.615753173828125 \n",
      "     Training Step: 230 Training Loss: 0.6161975264549255 \n",
      "     Training Step: 231 Training Loss: 0.6131207942962646 \n",
      "     Training Step: 232 Training Loss: 0.6184074878692627 \n",
      "     Training Step: 233 Training Loss: 0.618579626083374 \n",
      "     Training Step: 234 Training Loss: 0.613647997379303 \n",
      "     Training Step: 235 Training Loss: 0.6162341237068176 \n",
      "     Training Step: 236 Training Loss: 0.6144239902496338 \n",
      "     Training Step: 237 Training Loss: 0.6161980628967285 \n",
      "     Training Step: 238 Training Loss: 0.6092560887336731 \n",
      "     Training Step: 239 Training Loss: 0.61528080701828 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6145387887954712 \n",
      "     Validation Step: 1 Validation Loss: 0.6128420233726501 \n",
      "     Validation Step: 2 Validation Loss: 0.6121408343315125 \n",
      "     Validation Step: 3 Validation Loss: 0.6129946112632751 \n",
      "     Validation Step: 4 Validation Loss: 0.6159742474555969 \n",
      "     Validation Step: 5 Validation Loss: 0.6133154630661011 \n",
      "     Validation Step: 6 Validation Loss: 0.6157823801040649 \n",
      "     Validation Step: 7 Validation Loss: 0.6156082153320312 \n",
      "     Validation Step: 8 Validation Loss: 0.6155651807785034 \n",
      "     Validation Step: 9 Validation Loss: 0.6101277470588684 \n",
      "     Validation Step: 10 Validation Loss: 0.61489337682724 \n",
      "     Validation Step: 11 Validation Loss: 0.6172953248023987 \n",
      "     Validation Step: 12 Validation Loss: 0.6152251362800598 \n",
      "     Validation Step: 13 Validation Loss: 0.6148388385772705 \n",
      "     Validation Step: 14 Validation Loss: 0.617006778717041 \n",
      "     Validation Step: 15 Validation Loss: 0.6136454343795776 \n",
      "     Validation Step: 16 Validation Loss: 0.6075479984283447 \n",
      "     Validation Step: 17 Validation Loss: 0.6142581105232239 \n",
      "     Validation Step: 18 Validation Loss: 0.6116557717323303 \n",
      "     Validation Step: 19 Validation Loss: 0.6136691570281982 \n",
      "     Validation Step: 20 Validation Loss: 0.6104980111122131 \n",
      "     Validation Step: 21 Validation Loss: 0.6111893057823181 \n",
      "     Validation Step: 22 Validation Loss: 0.6118941903114319 \n",
      "     Validation Step: 23 Validation Loss: 0.6115700602531433 \n",
      "     Validation Step: 24 Validation Loss: 0.6180441379547119 \n",
      "     Validation Step: 25 Validation Loss: 0.6105324625968933 \n",
      "     Validation Step: 26 Validation Loss: 0.6175892353057861 \n",
      "     Validation Step: 27 Validation Loss: 0.6145710349082947 \n",
      "     Validation Step: 28 Validation Loss: 0.6106460094451904 \n",
      "     Validation Step: 29 Validation Loss: 0.6136471033096313 \n",
      "     Validation Step: 30 Validation Loss: 0.6183193325996399 \n",
      "     Validation Step: 31 Validation Loss: 0.6142053604125977 \n",
      "     Validation Step: 32 Validation Loss: 0.6111774444580078 \n",
      "     Validation Step: 33 Validation Loss: 0.6184739470481873 \n",
      "     Validation Step: 34 Validation Loss: 0.6101556420326233 \n",
      "     Validation Step: 35 Validation Loss: 0.6141193509101868 \n",
      "     Validation Step: 36 Validation Loss: 0.6141316294670105 \n",
      "     Validation Step: 37 Validation Loss: 0.6146417856216431 \n",
      "     Validation Step: 38 Validation Loss: 0.6184566617012024 \n",
      "     Validation Step: 39 Validation Loss: 0.6101760864257812 \n",
      "     Validation Step: 40 Validation Loss: 0.6153113842010498 \n",
      "     Validation Step: 41 Validation Loss: 0.6162383556365967 \n",
      "     Validation Step: 42 Validation Loss: 0.6182048320770264 \n",
      "     Validation Step: 43 Validation Loss: 0.6176779866218567 \n",
      "     Validation Step: 44 Validation Loss: 0.6150295734405518 \n",
      "Epoch: 136\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6157798171043396 \n",
      "     Training Step: 1 Training Loss: 0.618290901184082 \n",
      "     Training Step: 2 Training Loss: 0.610916793346405 \n",
      "     Training Step: 3 Training Loss: 0.6163409352302551 \n",
      "     Training Step: 4 Training Loss: 0.6167922616004944 \n",
      "     Training Step: 5 Training Loss: 0.6114492416381836 \n",
      "     Training Step: 6 Training Loss: 0.6128434538841248 \n",
      "     Training Step: 7 Training Loss: 0.6178020238876343 \n",
      "     Training Step: 8 Training Loss: 0.6114861369132996 \n",
      "     Training Step: 9 Training Loss: 0.6136208176612854 \n",
      "     Training Step: 10 Training Loss: 0.6176849007606506 \n",
      "     Training Step: 11 Training Loss: 0.612286388874054 \n",
      "     Training Step: 12 Training Loss: 0.6103983521461487 \n",
      "     Training Step: 13 Training Loss: 0.6126971244812012 \n",
      "     Training Step: 14 Training Loss: 0.6166608333587646 \n",
      "     Training Step: 15 Training Loss: 0.616412341594696 \n",
      "     Training Step: 16 Training Loss: 0.6116199493408203 \n",
      "     Training Step: 17 Training Loss: 0.6175016164779663 \n",
      "     Training Step: 18 Training Loss: 0.6196384429931641 \n",
      "     Training Step: 19 Training Loss: 0.6152852773666382 \n",
      "     Training Step: 20 Training Loss: 0.6143341064453125 \n",
      "     Training Step: 21 Training Loss: 0.6154162287712097 \n",
      "     Training Step: 22 Training Loss: 0.6147768497467041 \n",
      "     Training Step: 23 Training Loss: 0.6126529574394226 \n",
      "     Training Step: 24 Training Loss: 0.6166455149650574 \n",
      "     Training Step: 25 Training Loss: 0.6154010891914368 \n",
      "     Training Step: 26 Training Loss: 0.6154428124427795 \n",
      "     Training Step: 27 Training Loss: 0.6180528998374939 \n",
      "     Training Step: 28 Training Loss: 0.6142593622207642 \n",
      "     Training Step: 29 Training Loss: 0.6161931753158569 \n",
      "     Training Step: 30 Training Loss: 0.6152949929237366 \n",
      "     Training Step: 31 Training Loss: 0.6166684031486511 \n",
      "     Training Step: 32 Training Loss: 0.6122704744338989 \n",
      "     Training Step: 33 Training Loss: 0.6166873574256897 \n",
      "     Training Step: 34 Training Loss: 0.615336000919342 \n",
      "     Training Step: 35 Training Loss: 0.6097558736801147 \n",
      "     Training Step: 36 Training Loss: 0.6177791953086853 \n",
      "     Training Step: 37 Training Loss: 0.6149173378944397 \n",
      "     Training Step: 38 Training Loss: 0.6116527915000916 \n",
      "     Training Step: 39 Training Loss: 0.613120436668396 \n",
      "     Training Step: 40 Training Loss: 0.6209359169006348 \n",
      "     Training Step: 41 Training Loss: 0.6149593591690063 \n",
      "     Training Step: 42 Training Loss: 0.6148714423179626 \n",
      "     Training Step: 43 Training Loss: 0.6186014413833618 \n",
      "     Training Step: 44 Training Loss: 0.6144227981567383 \n",
      "     Training Step: 45 Training Loss: 0.6124879717826843 \n",
      "     Training Step: 46 Training Loss: 0.6143440008163452 \n",
      "     Training Step: 47 Training Loss: 0.6122286915779114 \n",
      "     Training Step: 48 Training Loss: 0.6142194271087646 \n",
      "     Training Step: 49 Training Loss: 0.6094698905944824 \n",
      "     Training Step: 50 Training Loss: 0.6094034910202026 \n",
      "     Training Step: 51 Training Loss: 0.6149337887763977 \n",
      "     Training Step: 52 Training Loss: 0.6138867139816284 \n",
      "     Training Step: 53 Training Loss: 0.6134533286094666 \n",
      "     Training Step: 54 Training Loss: 0.6184370517730713 \n",
      "     Training Step: 55 Training Loss: 0.6133070588111877 \n",
      "     Training Step: 56 Training Loss: 0.6127626895904541 \n",
      "     Training Step: 57 Training Loss: 0.6202483773231506 \n",
      "     Training Step: 58 Training Loss: 0.6133090853691101 \n",
      "     Training Step: 59 Training Loss: 0.6161173582077026 \n",
      "     Training Step: 60 Training Loss: 0.6171947717666626 \n",
      "     Training Step: 61 Training Loss: 0.6168022751808167 \n",
      "     Training Step: 62 Training Loss: 0.6125218272209167 \n",
      "     Training Step: 63 Training Loss: 0.6173686385154724 \n",
      "     Training Step: 64 Training Loss: 0.6182005405426025 \n",
      "     Training Step: 65 Training Loss: 0.6146668791770935 \n",
      "     Training Step: 66 Training Loss: 0.6153768301010132 \n",
      "     Training Step: 67 Training Loss: 0.6137581467628479 \n",
      "     Training Step: 68 Training Loss: 0.6184130907058716 \n",
      "     Training Step: 69 Training Loss: 0.6128920912742615 \n",
      "     Training Step: 70 Training Loss: 0.6135160326957703 \n",
      "     Training Step: 71 Training Loss: 0.6097643971443176 \n",
      "     Training Step: 72 Training Loss: 0.6118122935295105 \n",
      "     Training Step: 73 Training Loss: 0.6147430539131165 \n",
      "     Training Step: 74 Training Loss: 0.6129687428474426 \n",
      "     Training Step: 75 Training Loss: 0.6155441999435425 \n",
      "     Training Step: 76 Training Loss: 0.6115462183952332 \n",
      "     Training Step: 77 Training Loss: 0.6115216612815857 \n",
      "     Training Step: 78 Training Loss: 0.6180741786956787 \n",
      "     Training Step: 79 Training Loss: 0.6104564666748047 \n",
      "     Training Step: 80 Training Loss: 0.6139271259307861 \n",
      "     Training Step: 81 Training Loss: 0.6171793341636658 \n",
      "     Training Step: 82 Training Loss: 0.6105595827102661 \n",
      "     Training Step: 83 Training Loss: 0.6156927943229675 \n",
      "     Training Step: 84 Training Loss: 0.6115703582763672 \n",
      "     Training Step: 85 Training Loss: 0.6121559143066406 \n",
      "     Training Step: 86 Training Loss: 0.611407995223999 \n",
      "     Training Step: 87 Training Loss: 0.6155307292938232 \n",
      "     Training Step: 88 Training Loss: 0.6156101822853088 \n",
      "     Training Step: 89 Training Loss: 0.6099735498428345 \n",
      "     Training Step: 90 Training Loss: 0.6167214512825012 \n",
      "     Training Step: 91 Training Loss: 0.6153957843780518 \n",
      "     Training Step: 92 Training Loss: 0.6167070865631104 \n",
      "     Training Step: 93 Training Loss: 0.6138201355934143 \n",
      "     Training Step: 94 Training Loss: 0.6092425584793091 \n",
      "     Training Step: 95 Training Loss: 0.6166083812713623 \n",
      "     Training Step: 96 Training Loss: 0.6111661195755005 \n",
      "     Training Step: 97 Training Loss: 0.612535834312439 \n",
      "     Training Step: 98 Training Loss: 0.6114123463630676 \n",
      "     Training Step: 99 Training Loss: 0.6176888346672058 \n",
      "     Training Step: 100 Training Loss: 0.6118009090423584 \n",
      "     Training Step: 101 Training Loss: 0.6146761178970337 \n",
      "     Training Step: 102 Training Loss: 0.6146420240402222 \n",
      "     Training Step: 103 Training Loss: 0.6198715567588806 \n",
      "     Training Step: 104 Training Loss: 0.6107190847396851 \n",
      "     Training Step: 105 Training Loss: 0.6115285754203796 \n",
      "     Training Step: 106 Training Loss: 0.6118377447128296 \n",
      "     Training Step: 107 Training Loss: 0.6121620535850525 \n",
      "     Training Step: 108 Training Loss: 0.6116845607757568 \n",
      "     Training Step: 109 Training Loss: 0.6147297024726868 \n",
      "     Training Step: 110 Training Loss: 0.6135903596878052 \n",
      "     Training Step: 111 Training Loss: 0.6144526600837708 \n",
      "     Training Step: 112 Training Loss: 0.611602246761322 \n",
      "     Training Step: 113 Training Loss: 0.6127505302429199 \n",
      "     Training Step: 114 Training Loss: 0.6105587482452393 \n",
      "     Training Step: 115 Training Loss: 0.6132021546363831 \n",
      "     Training Step: 116 Training Loss: 0.6143476963043213 \n",
      "     Training Step: 117 Training Loss: 0.6120649576187134 \n",
      "     Training Step: 118 Training Loss: 0.6123673319816589 \n",
      "     Training Step: 119 Training Loss: 0.6104831695556641 \n",
      "     Training Step: 120 Training Loss: 0.6160456538200378 \n",
      "     Training Step: 121 Training Loss: 0.611511766910553 \n",
      "     Training Step: 122 Training Loss: 0.6152433156967163 \n",
      "     Training Step: 123 Training Loss: 0.6141067743301392 \n",
      "     Training Step: 124 Training Loss: 0.6108618974685669 \n",
      "     Training Step: 125 Training Loss: 0.6143831610679626 \n",
      "     Training Step: 126 Training Loss: 0.6169060468673706 \n",
      "     Training Step: 127 Training Loss: 0.6111909747123718 \n",
      "     Training Step: 128 Training Loss: 0.6153914928436279 \n",
      "     Training Step: 129 Training Loss: 0.6131343245506287 \n",
      "     Training Step: 130 Training Loss: 0.6105769276618958 \n",
      "     Training Step: 131 Training Loss: 0.6151750683784485 \n",
      "     Training Step: 132 Training Loss: 0.6147043108940125 \n",
      "     Training Step: 133 Training Loss: 0.6188762784004211 \n",
      "     Training Step: 134 Training Loss: 0.6130661964416504 \n",
      "     Training Step: 135 Training Loss: 0.61428302526474 \n",
      "     Training Step: 136 Training Loss: 0.6140156388282776 \n",
      "     Training Step: 137 Training Loss: 0.614151120185852 \n",
      "     Training Step: 138 Training Loss: 0.6182272434234619 \n",
      "     Training Step: 139 Training Loss: 0.6164095401763916 \n",
      "     Training Step: 140 Training Loss: 0.6171468496322632 \n",
      "     Training Step: 141 Training Loss: 0.6123958230018616 \n",
      "     Training Step: 142 Training Loss: 0.615937352180481 \n",
      "     Training Step: 143 Training Loss: 0.613749086856842 \n",
      "     Training Step: 144 Training Loss: 0.6176773905754089 \n",
      "     Training Step: 145 Training Loss: 0.6184380650520325 \n",
      "     Training Step: 146 Training Loss: 0.6140283346176147 \n",
      "     Training Step: 147 Training Loss: 0.6135326027870178 \n",
      "     Training Step: 148 Training Loss: 0.6120384335517883 \n",
      "     Training Step: 149 Training Loss: 0.6129158139228821 \n",
      "     Training Step: 150 Training Loss: 0.61179518699646 \n",
      "     Training Step: 151 Training Loss: 0.6147032380104065 \n",
      "     Training Step: 152 Training Loss: 0.6146031618118286 \n",
      "     Training Step: 153 Training Loss: 0.6140381097793579 \n",
      "     Training Step: 154 Training Loss: 0.6101804971694946 \n",
      "     Training Step: 155 Training Loss: 0.6125310659408569 \n",
      "     Training Step: 156 Training Loss: 0.6106767654418945 \n",
      "     Training Step: 157 Training Loss: 0.611872673034668 \n",
      "     Training Step: 158 Training Loss: 0.617180585861206 \n",
      "     Training Step: 159 Training Loss: 0.610644519329071 \n",
      "     Training Step: 160 Training Loss: 0.6155570149421692 \n",
      "     Training Step: 161 Training Loss: 0.6146137118339539 \n",
      "     Training Step: 162 Training Loss: 0.6162261962890625 \n",
      "     Training Step: 163 Training Loss: 0.6100186109542847 \n",
      "     Training Step: 164 Training Loss: 0.6082280874252319 \n",
      "     Training Step: 165 Training Loss: 0.6134760975837708 \n",
      "     Training Step: 166 Training Loss: 0.6144834756851196 \n",
      "     Training Step: 167 Training Loss: 0.6194665431976318 \n",
      "     Training Step: 168 Training Loss: 0.6128669381141663 \n",
      "     Training Step: 169 Training Loss: 0.6131834983825684 \n",
      "     Training Step: 170 Training Loss: 0.6151537895202637 \n",
      "     Training Step: 171 Training Loss: 0.6105964779853821 \n",
      "     Training Step: 172 Training Loss: 0.6116307377815247 \n",
      "     Training Step: 173 Training Loss: 0.6144968271255493 \n",
      "     Training Step: 174 Training Loss: 0.6151056289672852 \n",
      "     Training Step: 175 Training Loss: 0.61004638671875 \n",
      "     Training Step: 176 Training Loss: 0.6150679588317871 \n",
      "     Training Step: 177 Training Loss: 0.6122894883155823 \n",
      "     Training Step: 178 Training Loss: 0.6107296347618103 \n",
      "     Training Step: 179 Training Loss: 0.6137627959251404 \n",
      "     Training Step: 180 Training Loss: 0.615837574005127 \n",
      "     Training Step: 181 Training Loss: 0.6129177212715149 \n",
      "     Training Step: 182 Training Loss: 0.6114200949668884 \n",
      "     Training Step: 183 Training Loss: 0.6156997084617615 \n",
      "     Training Step: 184 Training Loss: 0.6157675385475159 \n",
      "     Training Step: 185 Training Loss: 0.6130536198616028 \n",
      "     Training Step: 186 Training Loss: 0.6177783608436584 \n",
      "     Training Step: 187 Training Loss: 0.6144388318061829 \n",
      "     Training Step: 188 Training Loss: 0.6188334226608276 \n",
      "     Training Step: 189 Training Loss: 0.618015468120575 \n",
      "     Training Step: 190 Training Loss: 0.6176154613494873 \n",
      "     Training Step: 191 Training Loss: 0.6167410612106323 \n",
      "     Training Step: 192 Training Loss: 0.6128273606300354 \n",
      "     Training Step: 193 Training Loss: 0.6148108839988708 \n",
      "     Training Step: 194 Training Loss: 0.6118776798248291 \n",
      "     Training Step: 195 Training Loss: 0.6157522797584534 \n",
      "     Training Step: 196 Training Loss: 0.6102026700973511 \n",
      "     Training Step: 197 Training Loss: 0.6146116852760315 \n",
      "     Training Step: 198 Training Loss: 0.6150912642478943 \n",
      "     Training Step: 199 Training Loss: 0.6146484017372131 \n",
      "     Training Step: 200 Training Loss: 0.6133586764335632 \n",
      "     Training Step: 201 Training Loss: 0.6139165163040161 \n",
      "     Training Step: 202 Training Loss: 0.6122263669967651 \n",
      "     Training Step: 203 Training Loss: 0.613279402256012 \n",
      "     Training Step: 204 Training Loss: 0.6121217012405396 \n",
      "     Training Step: 205 Training Loss: 0.6162570714950562 \n",
      "     Training Step: 206 Training Loss: 0.610363245010376 \n",
      "     Training Step: 207 Training Loss: 0.616753339767456 \n",
      "     Training Step: 208 Training Loss: 0.6097173690795898 \n",
      "     Training Step: 209 Training Loss: 0.6157670617103577 \n",
      "     Training Step: 210 Training Loss: 0.6121392846107483 \n",
      "     Training Step: 211 Training Loss: 0.6168298125267029 \n",
      "     Training Step: 212 Training Loss: 0.6100435853004456 \n",
      "     Training Step: 213 Training Loss: 0.6154806017875671 \n",
      "     Training Step: 214 Training Loss: 0.6141517162322998 \n",
      "     Training Step: 215 Training Loss: 0.6124616861343384 \n",
      "     Training Step: 216 Training Loss: 0.6171008944511414 \n",
      "     Training Step: 217 Training Loss: 0.6136474609375 \n",
      "     Training Step: 218 Training Loss: 0.6169242858886719 \n",
      "     Training Step: 219 Training Loss: 0.6132968664169312 \n",
      "     Training Step: 220 Training Loss: 0.6152659058570862 \n",
      "     Training Step: 221 Training Loss: 0.6132109761238098 \n",
      "     Training Step: 222 Training Loss: 0.6114131808280945 \n",
      "     Training Step: 223 Training Loss: 0.6118500232696533 \n",
      "     Training Step: 224 Training Loss: 0.6111595630645752 \n",
      "     Training Step: 225 Training Loss: 0.6166443228721619 \n",
      "     Training Step: 226 Training Loss: 0.6134119033813477 \n",
      "     Training Step: 227 Training Loss: 0.6146664023399353 \n",
      "     Training Step: 228 Training Loss: 0.6153014302253723 \n",
      "     Training Step: 229 Training Loss: 0.6123763918876648 \n",
      "     Training Step: 230 Training Loss: 0.6197115182876587 \n",
      "     Training Step: 231 Training Loss: 0.6164989471435547 \n",
      "     Training Step: 232 Training Loss: 0.6160026788711548 \n",
      "     Training Step: 233 Training Loss: 0.6146886348724365 \n",
      "     Training Step: 234 Training Loss: 0.6132739782333374 \n",
      "     Training Step: 235 Training Loss: 0.6140756011009216 \n",
      "     Training Step: 236 Training Loss: 0.6180889010429382 \n",
      "     Training Step: 237 Training Loss: 0.6142086386680603 \n",
      "     Training Step: 238 Training Loss: 0.6136438250541687 \n",
      "     Training Step: 239 Training Loss: 0.6185555458068848 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6155625581741333 \n",
      "     Validation Step: 1 Validation Loss: 0.6142102479934692 \n",
      "     Validation Step: 2 Validation Loss: 0.617290735244751 \n",
      "     Validation Step: 3 Validation Loss: 0.6145700216293335 \n",
      "     Validation Step: 4 Validation Loss: 0.6116602420806885 \n",
      "     Validation Step: 5 Validation Loss: 0.615226149559021 \n",
      "     Validation Step: 6 Validation Loss: 0.6180416941642761 \n",
      "     Validation Step: 7 Validation Loss: 0.6157863140106201 \n",
      "     Validation Step: 8 Validation Loss: 0.6184676885604858 \n",
      "     Validation Step: 9 Validation Loss: 0.6115784049034119 \n",
      "     Validation Step: 10 Validation Loss: 0.6148425340652466 \n",
      "     Validation Step: 11 Validation Loss: 0.6111888289451599 \n",
      "     Validation Step: 12 Validation Loss: 0.6162362098693848 \n",
      "     Validation Step: 13 Validation Loss: 0.6175832152366638 \n",
      "     Validation Step: 14 Validation Loss: 0.6170029640197754 \n",
      "     Validation Step: 15 Validation Loss: 0.6159762144088745 \n",
      "     Validation Step: 16 Validation Loss: 0.6183186769485474 \n",
      "     Validation Step: 17 Validation Loss: 0.6128465533256531 \n",
      "     Validation Step: 18 Validation Loss: 0.614645779132843 \n",
      "     Validation Step: 19 Validation Loss: 0.6182028651237488 \n",
      "     Validation Step: 20 Validation Loss: 0.6141230463981628 \n",
      "     Validation Step: 21 Validation Loss: 0.6105056405067444 \n",
      "     Validation Step: 22 Validation Loss: 0.6150351166725159 \n",
      "     Validation Step: 23 Validation Loss: 0.6101405620574951 \n",
      "     Validation Step: 24 Validation Loss: 0.6101651191711426 \n",
      "     Validation Step: 25 Validation Loss: 0.6156095266342163 \n",
      "     Validation Step: 26 Validation Loss: 0.613674521446228 \n",
      "     Validation Step: 27 Validation Loss: 0.6153149008750916 \n",
      "     Validation Step: 28 Validation Loss: 0.6111958026885986 \n",
      "     Validation Step: 29 Validation Loss: 0.6101806163787842 \n",
      "     Validation Step: 30 Validation Loss: 0.6184515357017517 \n",
      "     Validation Step: 31 Validation Loss: 0.6176770925521851 \n",
      "     Validation Step: 32 Validation Loss: 0.6075639724731445 \n",
      "     Validation Step: 33 Validation Loss: 0.6119035482406616 \n",
      "     Validation Step: 34 Validation Loss: 0.6142604947090149 \n",
      "     Validation Step: 35 Validation Loss: 0.6105428338050842 \n",
      "     Validation Step: 36 Validation Loss: 0.6136479377746582 \n",
      "     Validation Step: 37 Validation Loss: 0.6106541752815247 \n",
      "     Validation Step: 38 Validation Loss: 0.612148642539978 \n",
      "     Validation Step: 39 Validation Loss: 0.6130014061927795 \n",
      "     Validation Step: 40 Validation Loss: 0.6136511564254761 \n",
      "     Validation Step: 41 Validation Loss: 0.6141394972801208 \n",
      "     Validation Step: 42 Validation Loss: 0.6148945689201355 \n",
      "     Validation Step: 43 Validation Loss: 0.6133168339729309 \n",
      "     Validation Step: 44 Validation Loss: 0.6145411133766174 \n",
      "Epoch: 137\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6147266030311584 \n",
      "     Training Step: 1 Training Loss: 0.6118166446685791 \n",
      "     Training Step: 2 Training Loss: 0.6152616739273071 \n",
      "     Training Step: 3 Training Loss: 0.6136528253555298 \n",
      "     Training Step: 4 Training Loss: 0.6130713820457458 \n",
      "     Training Step: 5 Training Loss: 0.61217200756073 \n",
      "     Training Step: 6 Training Loss: 0.6154051423072815 \n",
      "     Training Step: 7 Training Loss: 0.6097298860549927 \n",
      "     Training Step: 8 Training Loss: 0.614107072353363 \n",
      "     Training Step: 9 Training Loss: 0.6146796345710754 \n",
      "     Training Step: 10 Training Loss: 0.6202366948127747 \n",
      "     Training Step: 11 Training Loss: 0.6094458103179932 \n",
      "     Training Step: 12 Training Loss: 0.6111918687820435 \n",
      "     Training Step: 13 Training Loss: 0.6137478947639465 \n",
      "     Training Step: 14 Training Loss: 0.6128712296485901 \n",
      "     Training Step: 15 Training Loss: 0.6151643991470337 \n",
      "     Training Step: 16 Training Loss: 0.6142147183418274 \n",
      "     Training Step: 17 Training Loss: 0.6122279763221741 \n",
      "     Training Step: 18 Training Loss: 0.6118683218955994 \n",
      "     Training Step: 19 Training Loss: 0.6105600595474243 \n",
      "     Training Step: 20 Training Loss: 0.616724967956543 \n",
      "     Training Step: 21 Training Loss: 0.6165081858634949 \n",
      "     Training Step: 22 Training Loss: 0.6111539006233215 \n",
      "     Training Step: 23 Training Loss: 0.6140351295471191 \n",
      "     Training Step: 24 Training Loss: 0.612526535987854 \n",
      "     Training Step: 25 Training Loss: 0.6156788468360901 \n",
      "     Training Step: 26 Training Loss: 0.6173768639564514 \n",
      "     Training Step: 27 Training Loss: 0.6144556999206543 \n",
      "     Training Step: 28 Training Loss: 0.6140726208686829 \n",
      "     Training Step: 29 Training Loss: 0.6167104244232178 \n",
      "     Training Step: 30 Training Loss: 0.6123891472816467 \n",
      "     Training Step: 31 Training Loss: 0.6136402487754822 \n",
      "     Training Step: 32 Training Loss: 0.6082913875579834 \n",
      "     Training Step: 33 Training Loss: 0.6180237531661987 \n",
      "     Training Step: 34 Training Loss: 0.613501250743866 \n",
      "     Training Step: 35 Training Loss: 0.6153370141983032 \n",
      "     Training Step: 36 Training Loss: 0.6116874814033508 \n",
      "     Training Step: 37 Training Loss: 0.6100597977638245 \n",
      "     Training Step: 38 Training Loss: 0.6107351779937744 \n",
      "     Training Step: 39 Training Loss: 0.6118349432945251 \n",
      "     Training Step: 40 Training Loss: 0.6133542656898499 \n",
      "     Training Step: 41 Training Loss: 0.6127534508705139 \n",
      "     Training Step: 42 Training Loss: 0.6161339282989502 \n",
      "     Training Step: 43 Training Loss: 0.6117768883705139 \n",
      "     Training Step: 44 Training Loss: 0.6145106554031372 \n",
      "     Training Step: 45 Training Loss: 0.6142131090164185 \n",
      "     Training Step: 46 Training Loss: 0.6199186444282532 \n",
      "     Training Step: 47 Training Loss: 0.6169101595878601 \n",
      "     Training Step: 48 Training Loss: 0.6114711165428162 \n",
      "     Training Step: 49 Training Loss: 0.613612949848175 \n",
      "     Training Step: 50 Training Loss: 0.6156818866729736 \n",
      "     Training Step: 51 Training Loss: 0.6121429800987244 \n",
      "     Training Step: 52 Training Loss: 0.6171544790267944 \n",
      "     Training Step: 53 Training Loss: 0.6132743954658508 \n",
      "     Training Step: 54 Training Loss: 0.6144779324531555 \n",
      "     Training Step: 55 Training Loss: 0.6127084493637085 \n",
      "     Training Step: 56 Training Loss: 0.6135943531990051 \n",
      "     Training Step: 57 Training Loss: 0.6166966557502747 \n",
      "     Training Step: 58 Training Loss: 0.6180886030197144 \n",
      "     Training Step: 59 Training Loss: 0.6167102456092834 \n",
      "     Training Step: 60 Training Loss: 0.6115437746047974 \n",
      "     Training Step: 61 Training Loss: 0.6114490628242493 \n",
      "     Training Step: 62 Training Loss: 0.6155928373336792 \n",
      "     Training Step: 63 Training Loss: 0.6147937178611755 \n",
      "     Training Step: 64 Training Loss: 0.6146068572998047 \n",
      "     Training Step: 65 Training Loss: 0.6115248799324036 \n",
      "     Training Step: 66 Training Loss: 0.6128652095794678 \n",
      "     Training Step: 67 Training Loss: 0.6114152669906616 \n",
      "     Training Step: 68 Training Loss: 0.6157469749450684 \n",
      "     Training Step: 69 Training Loss: 0.6166642904281616 \n",
      "     Training Step: 70 Training Loss: 0.6184224486351013 \n",
      "     Training Step: 71 Training Loss: 0.6209457516670227 \n",
      "     Training Step: 72 Training Loss: 0.6120105981826782 \n",
      "     Training Step: 73 Training Loss: 0.6121548414230347 \n",
      "     Training Step: 74 Training Loss: 0.6170881986618042 \n",
      "     Training Step: 75 Training Loss: 0.617681622505188 \n",
      "     Training Step: 76 Training Loss: 0.6188109517097473 \n",
      "     Training Step: 77 Training Loss: 0.6147002577781677 \n",
      "     Training Step: 78 Training Loss: 0.6135310530662537 \n",
      "     Training Step: 79 Training Loss: 0.6092848777770996 \n",
      "     Training Step: 80 Training Loss: 0.6166658997535706 \n",
      "     Training Step: 81 Training Loss: 0.6126608848571777 \n",
      "     Training Step: 82 Training Loss: 0.6114150881767273 \n",
      "     Training Step: 83 Training Loss: 0.6157501339912415 \n",
      "     Training Step: 84 Training Loss: 0.6168012022972107 \n",
      "     Training Step: 85 Training Loss: 0.6122404336929321 \n",
      "     Training Step: 86 Training Loss: 0.6171439290046692 \n",
      "     Training Step: 87 Training Loss: 0.6146902441978455 \n",
      "     Training Step: 88 Training Loss: 0.6160364747047424 \n",
      "     Training Step: 89 Training Loss: 0.6139097809791565 \n",
      "     Training Step: 90 Training Loss: 0.6096941828727722 \n",
      "     Training Step: 91 Training Loss: 0.6141567230224609 \n",
      "     Training Step: 92 Training Loss: 0.6129192113876343 \n",
      "     Training Step: 93 Training Loss: 0.6133110523223877 \n",
      "     Training Step: 94 Training Loss: 0.6105583906173706 \n",
      "     Training Step: 95 Training Loss: 0.6177332997322083 \n",
      "     Training Step: 96 Training Loss: 0.6114270091056824 \n",
      "     Training Step: 97 Training Loss: 0.6116206049919128 \n",
      "     Training Step: 98 Training Loss: 0.6151140928268433 \n",
      "     Training Step: 99 Training Loss: 0.6131821274757385 \n",
      "     Training Step: 100 Training Loss: 0.616252601146698 \n",
      "     Training Step: 101 Training Loss: 0.6104632019996643 \n",
      "     Training Step: 102 Training Loss: 0.6133012175559998 \n",
      "     Training Step: 103 Training Loss: 0.6137341260910034 \n",
      "     Training Step: 104 Training Loss: 0.6177734136581421 \n",
      "     Training Step: 105 Training Loss: 0.6155397295951843 \n",
      "     Training Step: 106 Training Loss: 0.6134622693061829 \n",
      "     Training Step: 107 Training Loss: 0.6152974367141724 \n",
      "     Training Step: 108 Training Loss: 0.6144233345985413 \n",
      "     Training Step: 109 Training Loss: 0.6137627959251404 \n",
      "     Training Step: 110 Training Loss: 0.6128060817718506 \n",
      "     Training Step: 111 Training Loss: 0.6131510138511658 \n",
      "     Training Step: 112 Training Loss: 0.615741491317749 \n",
      "     Training Step: 113 Training Loss: 0.6154756546020508 \n",
      "     Training Step: 114 Training Loss: 0.61253821849823 \n",
      "     Training Step: 115 Training Loss: 0.6146478652954102 \n",
      "     Training Step: 116 Training Loss: 0.6128402352333069 \n",
      "     Training Step: 117 Training Loss: 0.6132053732872009 \n",
      "     Training Step: 118 Training Loss: 0.6146008372306824 \n",
      "     Training Step: 119 Training Loss: 0.61319899559021 \n",
      "     Training Step: 120 Training Loss: 0.6105772256851196 \n",
      "     Training Step: 121 Training Loss: 0.6134679913520813 \n",
      "     Training Step: 122 Training Loss: 0.6118025779724121 \n",
      "     Training Step: 123 Training Loss: 0.6146121025085449 \n",
      "     Training Step: 124 Training Loss: 0.6162190437316895 \n",
      "     Training Step: 125 Training Loss: 0.6166656017303467 \n",
      "     Training Step: 126 Training Loss: 0.615835428237915 \n",
      "     Training Step: 127 Training Loss: 0.6172010898590088 \n",
      "     Training Step: 128 Training Loss: 0.613280713558197 \n",
      "     Training Step: 129 Training Loss: 0.6164008975028992 \n",
      "     Training Step: 130 Training Loss: 0.6152428388595581 \n",
      "     Training Step: 131 Training Loss: 0.6147434115409851 \n",
      "     Training Step: 132 Training Loss: 0.6122948527336121 \n",
      "     Training Step: 133 Training Loss: 0.612306535243988 \n",
      "     Training Step: 134 Training Loss: 0.6182059645652771 \n",
      "     Training Step: 135 Training Loss: 0.6153730154037476 \n",
      "     Training Step: 136 Training Loss: 0.6143450140953064 \n",
      "     Training Step: 137 Training Loss: 0.614336371421814 \n",
      "     Training Step: 138 Training Loss: 0.6118413805961609 \n",
      "     Training Step: 139 Training Loss: 0.6106768846511841 \n",
      "     Training Step: 140 Training Loss: 0.6161972284317017 \n",
      "     Training Step: 141 Training Loss: 0.6152850389480591 \n",
      "     Training Step: 142 Training Loss: 0.6130600571632385 \n",
      "     Training Step: 143 Training Loss: 0.6120678782463074 \n",
      "     Training Step: 144 Training Loss: 0.6141514182090759 \n",
      "     Training Step: 145 Training Loss: 0.6147730946540833 \n",
      "     Training Step: 146 Training Loss: 0.6108844876289368 \n",
      "     Training Step: 147 Training Loss: 0.611818253993988 \n",
      "     Training Step: 148 Training Loss: 0.6140198707580566 \n",
      "     Training Step: 149 Training Loss: 0.6151851415634155 \n",
      "     Training Step: 150 Training Loss: 0.6197380423545837 \n",
      "     Training Step: 151 Training Loss: 0.6143472194671631 \n",
      "     Training Step: 152 Training Loss: 0.6100347638130188 \n",
      "     Training Step: 153 Training Loss: 0.618337094783783 \n",
      "     Training Step: 154 Training Loss: 0.609724223613739 \n",
      "     Training Step: 155 Training Loss: 0.6196221709251404 \n",
      "     Training Step: 156 Training Loss: 0.6180188059806824 \n",
      "     Training Step: 157 Training Loss: 0.6150896549224854 \n",
      "     Training Step: 158 Training Loss: 0.6131407022476196 \n",
      "     Training Step: 159 Training Loss: 0.6159946918487549 \n",
      "     Training Step: 160 Training Loss: 0.6148715019226074 \n",
      "     Training Step: 161 Training Loss: 0.6150717735290527 \n",
      "     Training Step: 162 Training Loss: 0.6169030070304871 \n",
      "     Training Step: 163 Training Loss: 0.6184383630752563 \n",
      "     Training Step: 164 Training Loss: 0.6176708340644836 \n",
      "     Training Step: 165 Training Loss: 0.6155133843421936 \n",
      "     Training Step: 166 Training Loss: 0.6157842874526978 \n",
      "     Training Step: 167 Training Loss: 0.6105521321296692 \n",
      "     Training Step: 168 Training Loss: 0.6154199242591858 \n",
      "     Training Step: 169 Training Loss: 0.614959716796875 \n",
      "     Training Step: 170 Training Loss: 0.6115532517433167 \n",
      "     Training Step: 171 Training Loss: 0.6154375672340393 \n",
      "     Training Step: 172 Training Loss: 0.614932119846344 \n",
      "     Training Step: 173 Training Loss: 0.6134139895439148 \n",
      "     Training Step: 174 Training Loss: 0.6168045997619629 \n",
      "     Training Step: 175 Training Loss: 0.6142446398735046 \n",
      "     Training Step: 176 Training Loss: 0.6182372570037842 \n",
      "     Training Step: 177 Training Loss: 0.6099787354469299 \n",
      "     Training Step: 178 Training Loss: 0.6103939414024353 \n",
      "     Training Step: 179 Training Loss: 0.6175011396408081 \n",
      "     Training Step: 180 Training Loss: 0.6116310358047485 \n",
      "     Training Step: 181 Training Loss: 0.6159582734107971 \n",
      "     Training Step: 182 Training Loss: 0.611403226852417 \n",
      "     Training Step: 183 Training Loss: 0.6194575428962708 \n",
      "     Training Step: 184 Training Loss: 0.6106851100921631 \n",
      "     Training Step: 185 Training Loss: 0.6184511184692383 \n",
      "     Training Step: 186 Training Loss: 0.6188706159591675 \n",
      "     Training Step: 187 Training Loss: 0.6166080832481384 \n",
      "     Training Step: 188 Training Loss: 0.6121315360069275 \n",
      "     Training Step: 189 Training Loss: 0.6129783391952515 \n",
      "     Training Step: 190 Training Loss: 0.614639401435852 \n",
      "     Training Step: 191 Training Loss: 0.609441339969635 \n",
      "     Training Step: 192 Training Loss: 0.6176093816757202 \n",
      "     Training Step: 193 Training Loss: 0.6123985052108765 \n",
      "     Training Step: 194 Training Loss: 0.612531840801239 \n",
      "     Training Step: 195 Training Loss: 0.6168025732040405 \n",
      "     Training Step: 196 Training Loss: 0.6115594506263733 \n",
      "     Training Step: 197 Training Loss: 0.6140215396881104 \n",
      "     Training Step: 198 Training Loss: 0.6116418242454529 \n",
      "     Training Step: 199 Training Loss: 0.6124619245529175 \n",
      "     Training Step: 200 Training Loss: 0.6138876080513 \n",
      "     Training Step: 201 Training Loss: 0.6138171553611755 \n",
      "     Training Step: 202 Training Loss: 0.6144410371780396 \n",
      "     Training Step: 203 Training Loss: 0.6171666979789734 \n",
      "     Training Step: 204 Training Loss: 0.6178328394889832 \n",
      "     Training Step: 205 Training Loss: 0.6149209141731262 \n",
      "     Training Step: 206 Training Loss: 0.6186095476150513 \n",
      "     Training Step: 207 Training Loss: 0.6153943538665771 \n",
      "     Training Step: 208 Training Loss: 0.6166588068008423 \n",
      "     Training Step: 209 Training Loss: 0.6107422113418579 \n",
      "     Training Step: 210 Training Loss: 0.6101661324501038 \n",
      "     Training Step: 211 Training Loss: 0.6146754026412964 \n",
      "     Training Step: 212 Training Loss: 0.6103965044021606 \n",
      "     Training Step: 213 Training Loss: 0.6129136681556702 \n",
      "     Training Step: 214 Training Loss: 0.6164119839668274 \n",
      "     Training Step: 215 Training Loss: 0.6105970144271851 \n",
      "     Training Step: 216 Training Loss: 0.6111388206481934 \n",
      "     Training Step: 217 Training Loss: 0.6147080063819885 \n",
      "     Training Step: 218 Training Loss: 0.6108671426773071 \n",
      "     Training Step: 219 Training Loss: 0.6123700141906738 \n",
      "     Training Step: 220 Training Loss: 0.6155493259429932 \n",
      "     Training Step: 221 Training Loss: 0.6163736581802368 \n",
      "     Training Step: 222 Training Loss: 0.6153885722160339 \n",
      "     Training Step: 223 Training Loss: 0.6115743517875671 \n",
      "     Training Step: 224 Training Loss: 0.6124841570854187 \n",
      "     Training Step: 225 Training Loss: 0.6186110377311707 \n",
      "     Training Step: 226 Training Loss: 0.6139252781867981 \n",
      "     Training Step: 227 Training Loss: 0.6167740821838379 \n",
      "     Training Step: 228 Training Loss: 0.6127648949623108 \n",
      "     Training Step: 229 Training Loss: 0.6177945137023926 \n",
      "     Training Step: 230 Training Loss: 0.61465984582901 \n",
      "     Training Step: 231 Training Loss: 0.6100642085075378 \n",
      "     Training Step: 232 Training Loss: 0.6143727898597717 \n",
      "     Training Step: 233 Training Loss: 0.6142815351486206 \n",
      "     Training Step: 234 Training Loss: 0.6116284728050232 \n",
      "     Training Step: 235 Training Loss: 0.6152880787849426 \n",
      "     Training Step: 236 Training Loss: 0.6180559992790222 \n",
      "     Training Step: 237 Training Loss: 0.6102031469345093 \n",
      "     Training Step: 238 Training Loss: 0.6132970452308655 \n",
      "     Training Step: 239 Training Loss: 0.6122359037399292 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6115590929985046 \n",
      "     Validation Step: 1 Validation Loss: 0.6150315999984741 \n",
      "     Validation Step: 2 Validation Loss: 0.6153139472007751 \n",
      "     Validation Step: 3 Validation Loss: 0.6184629201889038 \n",
      "     Validation Step: 4 Validation Loss: 0.615787148475647 \n",
      "     Validation Step: 5 Validation Loss: 0.6142517924308777 \n",
      "     Validation Step: 6 Validation Loss: 0.6101117134094238 \n",
      "     Validation Step: 7 Validation Loss: 0.6148366332054138 \n",
      "     Validation Step: 8 Validation Loss: 0.6101369857788086 \n",
      "     Validation Step: 9 Validation Loss: 0.6176859736442566 \n",
      "     Validation Step: 10 Validation Loss: 0.6141148805618286 \n",
      "     Validation Step: 11 Validation Loss: 0.6148911118507385 \n",
      "     Validation Step: 12 Validation Loss: 0.6145336031913757 \n",
      "     Validation Step: 13 Validation Loss: 0.6156112551689148 \n",
      "     Validation Step: 14 Validation Loss: 0.6145692467689514 \n",
      "     Validation Step: 15 Validation Loss: 0.6155686974525452 \n",
      "     Validation Step: 16 Validation Loss: 0.6159836649894714 \n",
      "     Validation Step: 17 Validation Loss: 0.6182149052619934 \n",
      "     Validation Step: 18 Validation Loss: 0.6183295249938965 \n",
      "     Validation Step: 19 Validation Loss: 0.6142017841339111 \n",
      "     Validation Step: 20 Validation Loss: 0.6184839010238647 \n",
      "     Validation Step: 21 Validation Loss: 0.6111805438995361 \n",
      "     Validation Step: 22 Validation Loss: 0.6141271591186523 \n",
      "     Validation Step: 23 Validation Loss: 0.6128317713737488 \n",
      "     Validation Step: 24 Validation Loss: 0.6152255535125732 \n",
      "     Validation Step: 25 Validation Loss: 0.6121305823326111 \n",
      "     Validation Step: 26 Validation Loss: 0.6104861497879028 \n",
      "     Validation Step: 27 Validation Loss: 0.6105203628540039 \n",
      "     Validation Step: 28 Validation Loss: 0.6136478781700134 \n",
      "     Validation Step: 29 Validation Loss: 0.6136667728424072 \n",
      "     Validation Step: 30 Validation Loss: 0.6162403225898743 \n",
      "     Validation Step: 31 Validation Loss: 0.6173012256622314 \n",
      "     Validation Step: 32 Validation Loss: 0.6075228452682495 \n",
      "     Validation Step: 33 Validation Loss: 0.6180528402328491 \n",
      "     Validation Step: 34 Validation Loss: 0.6170141696929932 \n",
      "     Validation Step: 35 Validation Loss: 0.6133065819740295 \n",
      "     Validation Step: 36 Validation Loss: 0.6129890084266663 \n",
      "     Validation Step: 37 Validation Loss: 0.6101592779159546 \n",
      "     Validation Step: 38 Validation Loss: 0.6116443276405334 \n",
      "     Validation Step: 39 Validation Loss: 0.6175914406776428 \n",
      "     Validation Step: 40 Validation Loss: 0.6146379113197327 \n",
      "     Validation Step: 41 Validation Loss: 0.6118848919868469 \n",
      "     Validation Step: 42 Validation Loss: 0.6106333136558533 \n",
      "     Validation Step: 43 Validation Loss: 0.6136391758918762 \n",
      "     Validation Step: 44 Validation Loss: 0.6111664772033691 \n",
      "Epoch: 138\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.616412341594696 \n",
      "     Training Step: 1 Training Loss: 0.6134123206138611 \n",
      "     Training Step: 2 Training Loss: 0.6114172339439392 \n",
      "     Training Step: 3 Training Loss: 0.6155344247817993 \n",
      "     Training Step: 4 Training Loss: 0.6122250556945801 \n",
      "     Training Step: 5 Training Loss: 0.6139138340950012 \n",
      "     Training Step: 6 Training Loss: 0.6129111051559448 \n",
      "     Training Step: 7 Training Loss: 0.6146699786186218 \n",
      "     Training Step: 8 Training Loss: 0.6118301153182983 \n",
      "     Training Step: 9 Training Loss: 0.6100230813026428 \n",
      "     Training Step: 10 Training Loss: 0.6136400103569031 \n",
      "     Training Step: 11 Training Loss: 0.6162233352661133 \n",
      "     Training Step: 12 Training Loss: 0.6107250452041626 \n",
      "     Training Step: 13 Training Loss: 0.6172091960906982 \n",
      "     Training Step: 14 Training Loss: 0.6115739345550537 \n",
      "     Training Step: 15 Training Loss: 0.6146827340126038 \n",
      "     Training Step: 16 Training Loss: 0.612917423248291 \n",
      "     Training Step: 17 Training Loss: 0.6157857775688171 \n",
      "     Training Step: 18 Training Loss: 0.6150875091552734 \n",
      "     Training Step: 19 Training Loss: 0.6151533722877502 \n",
      "     Training Step: 20 Training Loss: 0.6134722232818604 \n",
      "     Training Step: 21 Training Loss: 0.614453911781311 \n",
      "     Training Step: 22 Training Loss: 0.6111646890640259 \n",
      "     Training Step: 23 Training Loss: 0.6182252168655396 \n",
      "     Training Step: 24 Training Loss: 0.6157402992248535 \n",
      "     Training Step: 25 Training Loss: 0.6156823635101318 \n",
      "     Training Step: 26 Training Loss: 0.6143428087234497 \n",
      "     Training Step: 27 Training Loss: 0.6152640581130981 \n",
      "     Training Step: 28 Training Loss: 0.6146993637084961 \n",
      "     Training Step: 29 Training Loss: 0.6149529218673706 \n",
      "     Training Step: 30 Training Loss: 0.6188165545463562 \n",
      "     Training Step: 31 Training Loss: 0.6142183542251587 \n",
      "     Training Step: 32 Training Loss: 0.6153967976570129 \n",
      "     Training Step: 33 Training Loss: 0.6123075485229492 \n",
      "     Training Step: 34 Training Loss: 0.6166661381721497 \n",
      "     Training Step: 35 Training Loss: 0.611530601978302 \n",
      "     Training Step: 36 Training Loss: 0.6124677658081055 \n",
      "     Training Step: 37 Training Loss: 0.6124898791313171 \n",
      "     Training Step: 38 Training Loss: 0.6121401190757751 \n",
      "     Training Step: 39 Training Loss: 0.6142070889472961 \n",
      "     Training Step: 40 Training Loss: 0.609688937664032 \n",
      "     Training Step: 41 Training Loss: 0.6131354570388794 \n",
      "     Training Step: 42 Training Loss: 0.6132017970085144 \n",
      "     Training Step: 43 Training Loss: 0.6143503785133362 \n",
      "     Training Step: 44 Training Loss: 0.6117708086967468 \n",
      "     Training Step: 45 Training Loss: 0.6137549877166748 \n",
      "     Training Step: 46 Training Loss: 0.6143484711647034 \n",
      "     Training Step: 47 Training Loss: 0.6135168075561523 \n",
      "     Training Step: 48 Training Loss: 0.6097009778022766 \n",
      "     Training Step: 49 Training Loss: 0.6092013120651245 \n",
      "     Training Step: 50 Training Loss: 0.6120599508285522 \n",
      "     Training Step: 51 Training Loss: 0.6125228404998779 \n",
      "     Training Step: 52 Training Loss: 0.6171796917915344 \n",
      "     Training Step: 53 Training Loss: 0.6115167737007141 \n",
      "     Training Step: 54 Training Loss: 0.6146088242530823 \n",
      "     Training Step: 55 Training Loss: 0.6140214204788208 \n",
      "     Training Step: 56 Training Loss: 0.6107116341590881 \n",
      "     Training Step: 57 Training Loss: 0.6147284507751465 \n",
      "     Training Step: 58 Training Loss: 0.6141496896743774 \n",
      "     Training Step: 59 Training Loss: 0.6099754571914673 \n",
      "     Training Step: 60 Training Loss: 0.6115462183952332 \n",
      "     Training Step: 61 Training Loss: 0.6138178706169128 \n",
      "     Training Step: 62 Training Loss: 0.6158367991447449 \n",
      "     Training Step: 63 Training Loss: 0.6183330416679382 \n",
      "     Training Step: 64 Training Loss: 0.6173883676528931 \n",
      "     Training Step: 65 Training Loss: 0.6167176961898804 \n",
      "     Training Step: 66 Training Loss: 0.6147473454475403 \n",
      "     Training Step: 67 Training Loss: 0.6142465472221375 \n",
      "     Training Step: 68 Training Loss: 0.6157559156417847 \n",
      "     Training Step: 69 Training Loss: 0.6117040514945984 \n",
      "     Training Step: 70 Training Loss: 0.6144238710403442 \n",
      "     Training Step: 71 Training Loss: 0.6167115569114685 \n",
      "     Training Step: 72 Training Loss: 0.6153712272644043 \n",
      "     Training Step: 73 Training Loss: 0.6167958378791809 \n",
      "     Training Step: 74 Training Loss: 0.6176789402961731 \n",
      "     Training Step: 75 Training Loss: 0.611661970615387 \n",
      "     Training Step: 76 Training Loss: 0.6128852367401123 \n",
      "     Training Step: 77 Training Loss: 0.613132655620575 \n",
      "     Training Step: 78 Training Loss: 0.6115411520004272 \n",
      "     Training Step: 79 Training Loss: 0.6185856461524963 \n",
      "     Training Step: 80 Training Loss: 0.6146376132965088 \n",
      "     Training Step: 81 Training Loss: 0.617093563079834 \n",
      "     Training Step: 82 Training Loss: 0.6108778715133667 \n",
      "     Training Step: 83 Training Loss: 0.6188700795173645 \n",
      "     Training Step: 84 Training Loss: 0.6147942543029785 \n",
      "     Training Step: 85 Training Loss: 0.616654634475708 \n",
      "     Training Step: 86 Training Loss: 0.6122313737869263 \n",
      "     Training Step: 87 Training Loss: 0.6132064461708069 \n",
      "     Training Step: 88 Training Loss: 0.6118829846382141 \n",
      "     Training Step: 89 Training Loss: 0.6177549362182617 \n",
      "     Training Step: 90 Training Loss: 0.6118058562278748 \n",
      "     Training Step: 91 Training Loss: 0.6159417033195496 \n",
      "     Training Step: 92 Training Loss: 0.6198517680168152 \n",
      "     Training Step: 93 Training Loss: 0.6140382885932922 \n",
      "     Training Step: 94 Training Loss: 0.6166921854019165 \n",
      "     Training Step: 95 Training Loss: 0.6122542023658752 \n",
      "     Training Step: 96 Training Loss: 0.6126537919044495 \n",
      "     Training Step: 97 Training Loss: 0.6102040410041809 \n",
      "     Training Step: 98 Training Loss: 0.6101553440093994 \n",
      "     Training Step: 99 Training Loss: 0.6174818277359009 \n",
      "     Training Step: 100 Training Loss: 0.6169250011444092 \n",
      "     Training Step: 101 Training Loss: 0.61335289478302 \n",
      "     Training Step: 102 Training Loss: 0.6141058802604675 \n",
      "     Training Step: 103 Training Loss: 0.6137346625328064 \n",
      "     Training Step: 104 Training Loss: 0.6121517419815063 \n",
      "     Training Step: 105 Training Loss: 0.6151774525642395 \n",
      "     Training Step: 106 Training Loss: 0.6123796105384827 \n",
      "     Training Step: 107 Training Loss: 0.6167701482772827 \n",
      "     Training Step: 108 Training Loss: 0.6114274859428406 \n",
      "     Training Step: 109 Training Loss: 0.6114668846130371 \n",
      "     Training Step: 110 Training Loss: 0.6121558547019958 \n",
      "     Training Step: 111 Training Loss: 0.6149353384971619 \n",
      "     Training Step: 112 Training Loss: 0.6108770370483398 \n",
      "     Training Step: 113 Training Loss: 0.6148777008056641 \n",
      "     Training Step: 114 Training Loss: 0.6106475591659546 \n",
      "     Training Step: 115 Training Loss: 0.6166748404502869 \n",
      "     Training Step: 116 Training Loss: 0.6166302561759949 \n",
      "     Training Step: 117 Training Loss: 0.6133041381835938 \n",
      "     Training Step: 118 Training Loss: 0.61671382188797 \n",
      "     Training Step: 119 Training Loss: 0.6171637773513794 \n",
      "     Training Step: 120 Training Loss: 0.6168025732040405 \n",
      "     Training Step: 121 Training Loss: 0.6177999973297119 \n",
      "     Training Step: 122 Training Loss: 0.6111677885055542 \n",
      "     Training Step: 123 Training Loss: 0.6141655445098877 \n",
      "     Training Step: 124 Training Loss: 0.6125441789627075 \n",
      "     Training Step: 125 Training Loss: 0.6123095154762268 \n",
      "     Training Step: 126 Training Loss: 0.6147755980491638 \n",
      "     Training Step: 127 Training Loss: 0.6131975650787354 \n",
      "     Training Step: 128 Training Loss: 0.6153373122215271 \n",
      "     Training Step: 129 Training Loss: 0.609743058681488 \n",
      "     Training Step: 130 Training Loss: 0.6156858205795288 \n",
      "     Training Step: 131 Training Loss: 0.6166675090789795 \n",
      "     Training Step: 132 Training Loss: 0.6105909943580627 \n",
      "     Training Step: 133 Training Loss: 0.6116058230400085 \n",
      "     Training Step: 134 Training Loss: 0.6117852330207825 \n",
      "     Training Step: 135 Training Loss: 0.6140776872634888 \n",
      "     Training Step: 136 Training Loss: 0.6184859275817871 \n",
      "     Training Step: 137 Training Loss: 0.6147105097770691 \n",
      "     Training Step: 138 Training Loss: 0.614484429359436 \n",
      "     Training Step: 139 Training Loss: 0.6136495471000671 \n",
      "     Training Step: 140 Training Loss: 0.6104872226715088 \n",
      "     Training Step: 141 Training Loss: 0.618111252784729 \n",
      "     Training Step: 142 Training Loss: 0.6137617826461792 \n",
      "     Training Step: 143 Training Loss: 0.6152844429016113 \n",
      "     Training Step: 144 Training Loss: 0.6159988045692444 \n",
      "     Training Step: 145 Training Loss: 0.6167972087860107 \n",
      "     Training Step: 146 Training Loss: 0.6123805642127991 \n",
      "     Training Step: 147 Training Loss: 0.6176784038543701 \n",
      "     Training Step: 148 Training Loss: 0.6127588152885437 \n",
      "     Training Step: 149 Training Loss: 0.6105902791023254 \n",
      "     Training Step: 150 Training Loss: 0.6140260696411133 \n",
      "     Training Step: 151 Training Loss: 0.6135941743850708 \n",
      "     Training Step: 152 Training Loss: 0.6154167652130127 \n",
      "     Training Step: 153 Training Loss: 0.6155123114585876 \n",
      "     Training Step: 154 Training Loss: 0.6128036975860596 \n",
      "     Training Step: 155 Training Loss: 0.6155294179916382 \n",
      "     Training Step: 156 Training Loss: 0.6161971092224121 \n",
      "     Training Step: 157 Training Loss: 0.6196048259735107 \n",
      "     Training Step: 158 Training Loss: 0.6162247061729431 \n",
      "     Training Step: 159 Training Loss: 0.6106058359146118 \n",
      "     Training Step: 160 Training Loss: 0.6145012378692627 \n",
      "     Training Step: 161 Training Loss: 0.614286482334137 \n",
      "     Training Step: 162 Training Loss: 0.6163426637649536 \n",
      "     Training Step: 163 Training Loss: 0.6182074546813965 \n",
      "     Training Step: 164 Training Loss: 0.6130682229995728 \n",
      "     Training Step: 165 Training Loss: 0.6146036386489868 \n",
      "     Training Step: 166 Training Loss: 0.6103979349136353 \n",
      "     Training Step: 167 Training Loss: 0.6094815135002136 \n",
      "     Training Step: 168 Training Loss: 0.6116260886192322 \n",
      "     Training Step: 169 Training Loss: 0.6114331483840942 \n",
      "     Training Step: 170 Training Loss: 0.614372968673706 \n",
      "     Training Step: 171 Training Loss: 0.6128386855125427 \n",
      "     Training Step: 172 Training Loss: 0.6184347867965698 \n",
      "     Training Step: 173 Training Loss: 0.6113993525505066 \n",
      "     Training Step: 174 Training Loss: 0.6113899350166321 \n",
      "     Training Step: 175 Training Loss: 0.6118203997612 \n",
      "     Training Step: 176 Training Loss: 0.615108847618103 \n",
      "     Training Step: 177 Training Loss: 0.6177310943603516 \n",
      "     Training Step: 178 Training Loss: 0.6161282658576965 \n",
      "     Training Step: 179 Training Loss: 0.6133121252059937 \n",
      "     Training Step: 180 Training Loss: 0.6111955642700195 \n",
      "     Training Step: 181 Training Loss: 0.6177959442138672 \n",
      "     Training Step: 182 Training Loss: 0.6149150133132935 \n",
      "     Training Step: 183 Training Loss: 0.611639142036438 \n",
      "     Training Step: 184 Training Loss: 0.61007159948349 \n",
      "     Training Step: 185 Training Loss: 0.6154382824897766 \n",
      "     Training Step: 186 Training Loss: 0.6152846813201904 \n",
      "     Training Step: 187 Training Loss: 0.6180287599563599 \n",
      "     Training Step: 188 Training Loss: 0.6104869246482849 \n",
      "     Training Step: 189 Training Loss: 0.6126986742019653 \n",
      "     Training Step: 190 Training Loss: 0.6185769438743591 \n",
      "     Training Step: 191 Training Loss: 0.6196924448013306 \n",
      "     Training Step: 192 Training Loss: 0.6153709888458252 \n",
      "     Training Step: 193 Training Loss: 0.6144434213638306 \n",
      "     Training Step: 194 Training Loss: 0.61805659532547 \n",
      "     Training Step: 195 Training Loss: 0.6107114553451538 \n",
      "     Training Step: 196 Training Loss: 0.6180086731910706 \n",
      "     Training Step: 197 Training Loss: 0.6209039688110352 \n",
      "     Training Step: 198 Training Loss: 0.6082976460456848 \n",
      "     Training Step: 199 Training Loss: 0.6154736280441284 \n",
      "     Training Step: 200 Training Loss: 0.6118436455726624 \n",
      "     Training Step: 201 Training Loss: 0.6127687692642212 \n",
      "     Training Step: 202 Training Loss: 0.6094132661819458 \n",
      "     Training Step: 203 Training Loss: 0.6134592294692993 \n",
      "     Training Step: 204 Training Loss: 0.6156123876571655 \n",
      "     Training Step: 205 Training Loss: 0.615424394607544 \n",
      "     Training Step: 206 Training Loss: 0.6134994029998779 \n",
      "     Training Step: 207 Training Loss: 0.610560953617096 \n",
      "     Training Step: 208 Training Loss: 0.6169114708900452 \n",
      "     Training Step: 209 Training Loss: 0.6132893562316895 \n",
      "     Training Step: 210 Training Loss: 0.6152473092079163 \n",
      "     Training Step: 211 Training Loss: 0.6130552887916565 \n",
      "     Training Step: 212 Training Loss: 0.6146116852760315 \n",
      "     Training Step: 213 Training Loss: 0.6132716536521912 \n",
      "     Training Step: 214 Training Loss: 0.6120094656944275 \n",
      "     Training Step: 215 Training Loss: 0.6164106130599976 \n",
      "     Training Step: 216 Training Loss: 0.6132707595825195 \n",
      "     Training Step: 217 Training Loss: 0.6202307939529419 \n",
      "     Training Step: 218 Training Loss: 0.6164966225624084 \n",
      "     Training Step: 219 Training Loss: 0.6146910190582275 \n",
      "     Training Step: 220 Training Loss: 0.6171351075172424 \n",
      "     Training Step: 221 Training Loss: 0.6150644421577454 \n",
      "     Training Step: 222 Training Loss: 0.610085129737854 \n",
      "     Training Step: 223 Training Loss: 0.6136295795440674 \n",
      "     Training Step: 224 Training Loss: 0.612551748752594 \n",
      "     Training Step: 225 Training Loss: 0.6128755211830139 \n",
      "     Training Step: 226 Training Loss: 0.6104171276092529 \n",
      "     Training Step: 227 Training Loss: 0.6160279512405396 \n",
      "     Training Step: 228 Training Loss: 0.6157427430152893 \n",
      "     Training Step: 229 Training Loss: 0.6123771667480469 \n",
      "     Training Step: 230 Training Loss: 0.612960159778595 \n",
      "     Training Step: 231 Training Loss: 0.6146664619445801 \n",
      "     Training Step: 232 Training Loss: 0.6138877868652344 \n",
      "     Training Step: 233 Training Loss: 0.6139184832572937 \n",
      "     Training Step: 234 Training Loss: 0.6121172904968262 \n",
      "     Training Step: 235 Training Loss: 0.6194828152656555 \n",
      "     Training Step: 236 Training Loss: 0.6146603226661682 \n",
      "     Training Step: 237 Training Loss: 0.6153153777122498 \n",
      "     Training Step: 238 Training Loss: 0.6184574365615845 \n",
      "     Training Step: 239 Training Loss: 0.6176334619522095 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6133074164390564 \n",
      "     Validation Step: 1 Validation Loss: 0.6183290481567383 \n",
      "     Validation Step: 2 Validation Loss: 0.6105250716209412 \n",
      "     Validation Step: 3 Validation Loss: 0.6115649342536926 \n",
      "     Validation Step: 4 Validation Loss: 0.6182142496109009 \n",
      "     Validation Step: 5 Validation Loss: 0.614203929901123 \n",
      "     Validation Step: 6 Validation Loss: 0.6180516481399536 \n",
      "     Validation Step: 7 Validation Loss: 0.6148367524147034 \n",
      "     Validation Step: 8 Validation Loss: 0.6176857948303223 \n",
      "     Validation Step: 9 Validation Loss: 0.6184599995613098 \n",
      "     Validation Step: 10 Validation Loss: 0.6170126795768738 \n",
      "     Validation Step: 11 Validation Loss: 0.6153170466423035 \n",
      "     Validation Step: 12 Validation Loss: 0.6145339012145996 \n",
      "     Validation Step: 13 Validation Loss: 0.6136513352394104 \n",
      "     Validation Step: 14 Validation Loss: 0.6136386394500732 \n",
      "     Validation Step: 15 Validation Loss: 0.6184812188148499 \n",
      "     Validation Step: 16 Validation Loss: 0.6156119108200073 \n",
      "     Validation Step: 17 Validation Loss: 0.612134575843811 \n",
      "     Validation Step: 18 Validation Loss: 0.6101203560829163 \n",
      "     Validation Step: 19 Validation Loss: 0.6116474270820618 \n",
      "     Validation Step: 20 Validation Loss: 0.6162423491477966 \n",
      "     Validation Step: 21 Validation Loss: 0.6157875061035156 \n",
      "     Validation Step: 22 Validation Loss: 0.6141152381896973 \n",
      "     Validation Step: 23 Validation Loss: 0.6111845374107361 \n",
      "     Validation Step: 24 Validation Loss: 0.6159835457801819 \n",
      "     Validation Step: 25 Validation Loss: 0.6142517328262329 \n",
      "     Validation Step: 26 Validation Loss: 0.6172990798950195 \n",
      "     Validation Step: 27 Validation Loss: 0.6118888854980469 \n",
      "     Validation Step: 28 Validation Loss: 0.6155680418014526 \n",
      "     Validation Step: 29 Validation Loss: 0.6150343418121338 \n",
      "     Validation Step: 30 Validation Loss: 0.6129915714263916 \n",
      "     Validation Step: 31 Validation Loss: 0.6136705279350281 \n",
      "     Validation Step: 32 Validation Loss: 0.6175895929336548 \n",
      "     Validation Step: 33 Validation Loss: 0.6145698428153992 \n",
      "     Validation Step: 34 Validation Loss: 0.6128334403038025 \n",
      "     Validation Step: 35 Validation Loss: 0.6111734509468079 \n",
      "     Validation Step: 36 Validation Loss: 0.6104903221130371 \n",
      "     Validation Step: 37 Validation Loss: 0.614640474319458 \n",
      "     Validation Step: 38 Validation Loss: 0.6141304969787598 \n",
      "     Validation Step: 39 Validation Loss: 0.6148926019668579 \n",
      "     Validation Step: 40 Validation Loss: 0.6075323224067688 \n",
      "     Validation Step: 41 Validation Loss: 0.6106380224227905 \n",
      "     Validation Step: 42 Validation Loss: 0.6152260899543762 \n",
      "     Validation Step: 43 Validation Loss: 0.6101627349853516 \n",
      "     Validation Step: 44 Validation Loss: 0.6101452708244324 \n",
      "Epoch: 139\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6106709837913513 \n",
      "     Training Step: 1 Training Loss: 0.6125441193580627 \n",
      "     Training Step: 2 Training Loss: 0.6129320859909058 \n",
      "     Training Step: 3 Training Loss: 0.614078164100647 \n",
      "     Training Step: 4 Training Loss: 0.6152461767196655 \n",
      "     Training Step: 5 Training Loss: 0.6123179197311401 \n",
      "     Training Step: 6 Training Loss: 0.6118927597999573 \n",
      "     Training Step: 7 Training Loss: 0.6115338206291199 \n",
      "     Training Step: 8 Training Loss: 0.6114140748977661 \n",
      "     Training Step: 9 Training Loss: 0.6164970993995667 \n",
      "     Training Step: 10 Training Loss: 0.6133548617362976 \n",
      "     Training Step: 11 Training Loss: 0.6135076880455017 \n",
      "     Training Step: 12 Training Loss: 0.6105588674545288 \n",
      "     Training Step: 13 Training Loss: 0.6160193681716919 \n",
      "     Training Step: 14 Training Loss: 0.6135963797569275 \n",
      "     Training Step: 15 Training Loss: 0.6171732544898987 \n",
      "     Training Step: 16 Training Loss: 0.6108760833740234 \n",
      "     Training Step: 17 Training Loss: 0.6153952479362488 \n",
      "     Training Step: 18 Training Loss: 0.6123701930046082 \n",
      "     Training Step: 19 Training Loss: 0.6115164756774902 \n",
      "     Training Step: 20 Training Loss: 0.6111901998519897 \n",
      "     Training Step: 21 Training Loss: 0.6131141781806946 \n",
      "     Training Step: 22 Training Loss: 0.6136473417282104 \n",
      "     Training Step: 23 Training Loss: 0.6146547794342041 \n",
      "     Training Step: 24 Training Loss: 0.6169005036354065 \n",
      "     Training Step: 25 Training Loss: 0.6096944212913513 \n",
      "     Training Step: 26 Training Loss: 0.61402428150177 \n",
      "     Training Step: 27 Training Loss: 0.6184744834899902 \n",
      "     Training Step: 28 Training Loss: 0.6173833608627319 \n",
      "     Training Step: 29 Training Loss: 0.6130661964416504 \n",
      "     Training Step: 30 Training Loss: 0.6171309351921082 \n",
      "     Training Step: 31 Training Loss: 0.6144994497299194 \n",
      "     Training Step: 32 Training Loss: 0.6155292987823486 \n",
      "     Training Step: 33 Training Loss: 0.6138908267021179 \n",
      "     Training Step: 34 Training Loss: 0.6118571162223816 \n",
      "     Training Step: 35 Training Loss: 0.6150683760643005 \n",
      "     Training Step: 36 Training Loss: 0.6196672320365906 \n",
      "     Training Step: 37 Training Loss: 0.613196849822998 \n",
      "     Training Step: 38 Training Loss: 0.6121791005134583 \n",
      "     Training Step: 39 Training Loss: 0.6135045289993286 \n",
      "     Training Step: 40 Training Loss: 0.6129736304283142 \n",
      "     Training Step: 41 Training Loss: 0.6143397092819214 \n",
      "     Training Step: 42 Training Loss: 0.6143690347671509 \n",
      "     Training Step: 43 Training Loss: 0.6155374646186829 \n",
      "     Training Step: 44 Training Loss: 0.6101373434066772 \n",
      "     Training Step: 45 Training Loss: 0.6116070747375488 \n",
      "     Training Step: 46 Training Loss: 0.6105630993843079 \n",
      "     Training Step: 47 Training Loss: 0.6123724579811096 \n",
      "     Training Step: 48 Training Loss: 0.612696647644043 \n",
      "     Training Step: 49 Training Loss: 0.6154351830482483 \n",
      "     Training Step: 50 Training Loss: 0.6137368679046631 \n",
      "     Training Step: 51 Training Loss: 0.6194897294044495 \n",
      "     Training Step: 52 Training Loss: 0.6136417388916016 \n",
      "     Training Step: 53 Training Loss: 0.6153810620307922 \n",
      "     Training Step: 54 Training Loss: 0.6104848980903625 \n",
      "     Training Step: 55 Training Loss: 0.6156114339828491 \n",
      "     Training Step: 56 Training Loss: 0.6152874231338501 \n",
      "     Training Step: 57 Training Loss: 0.614605724811554 \n",
      "     Training Step: 58 Training Loss: 0.6116454005241394 \n",
      "     Training Step: 59 Training Loss: 0.6168001890182495 \n",
      "     Training Step: 60 Training Loss: 0.6176812052726746 \n",
      "     Training Step: 61 Training Loss: 0.6114503741264343 \n",
      "     Training Step: 62 Training Loss: 0.6094272136688232 \n",
      "     Training Step: 63 Training Loss: 0.6114462614059448 \n",
      "     Training Step: 64 Training Loss: 0.6149152517318726 \n",
      "     Training Step: 65 Training Loss: 0.6097301244735718 \n",
      "     Training Step: 66 Training Loss: 0.6149325370788574 \n",
      "     Training Step: 67 Training Loss: 0.6116173267364502 \n",
      "     Training Step: 68 Training Loss: 0.6134544610977173 \n",
      "     Training Step: 69 Training Loss: 0.6147969365119934 \n",
      "     Training Step: 70 Training Loss: 0.6166903376579285 \n",
      "     Training Step: 71 Training Loss: 0.6202594041824341 \n",
      "     Training Step: 72 Training Loss: 0.6129149198532104 \n",
      "     Training Step: 73 Training Loss: 0.6182416081428528 \n",
      "     Training Step: 74 Training Loss: 0.6185819506645203 \n",
      "     Training Step: 75 Training Loss: 0.6180346012115479 \n",
      "     Training Step: 76 Training Loss: 0.6132829785346985 \n",
      "     Training Step: 77 Training Loss: 0.6140216588973999 \n",
      "     Training Step: 78 Training Loss: 0.6142562627792358 \n",
      "     Training Step: 79 Training Loss: 0.6100961565971375 \n",
      "     Training Step: 80 Training Loss: 0.6117193698883057 \n",
      "     Training Step: 81 Training Loss: 0.610414445400238 \n",
      "     Training Step: 82 Training Loss: 0.6161972284317017 \n",
      "     Training Step: 83 Training Loss: 0.6154016256332397 \n",
      "     Training Step: 84 Training Loss: 0.6146689057350159 \n",
      "     Training Step: 85 Training Loss: 0.6157428622245789 \n",
      "     Training Step: 86 Training Loss: 0.6122332811355591 \n",
      "     Training Step: 87 Training Loss: 0.6198757886886597 \n",
      "     Training Step: 88 Training Loss: 0.6101758480072021 \n",
      "     Training Step: 89 Training Loss: 0.6178156137466431 \n",
      "     Training Step: 90 Training Loss: 0.6163516640663147 \n",
      "     Training Step: 91 Training Loss: 0.6128408312797546 \n",
      "     Training Step: 92 Training Loss: 0.6105939745903015 \n",
      "     Training Step: 93 Training Loss: 0.6146703958511353 \n",
      "     Training Step: 94 Training Loss: 0.6127962470054626 \n",
      "     Training Step: 95 Training Loss: 0.6186034083366394 \n",
      "     Training Step: 96 Training Loss: 0.6107325553894043 \n",
      "     Training Step: 97 Training Loss: 0.6162049174308777 \n",
      "     Training Step: 98 Training Loss: 0.6125303506851196 \n",
      "     Training Step: 99 Training Loss: 0.6147047877311707 \n",
      "     Training Step: 100 Training Loss: 0.6132922172546387 \n",
      "     Training Step: 101 Training Loss: 0.6120136976242065 \n",
      "     Training Step: 102 Training Loss: 0.6180720925331116 \n",
      "     Training Step: 103 Training Loss: 0.61375892162323 \n",
      "     Training Step: 104 Training Loss: 0.6116334199905396 \n",
      "     Training Step: 105 Training Loss: 0.614686906337738 \n",
      "     Training Step: 106 Training Loss: 0.6164013147354126 \n",
      "     Training Step: 107 Training Loss: 0.614452064037323 \n",
      "     Training Step: 108 Training Loss: 0.618014395236969 \n",
      "     Training Step: 109 Training Loss: 0.6111633777618408 \n",
      "     Training Step: 110 Training Loss: 0.6161120533943176 \n",
      "     Training Step: 111 Training Loss: 0.6141060590744019 \n",
      "     Training Step: 112 Training Loss: 0.6176223754882812 \n",
      "     Training Step: 113 Training Loss: 0.6166031956672668 \n",
      "     Training Step: 114 Training Loss: 0.6134174466133118 \n",
      "     Training Step: 115 Training Loss: 0.6104174256324768 \n",
      "     Training Step: 116 Training Loss: 0.6148673295974731 \n",
      "     Training Step: 117 Training Loss: 0.6134721636772156 \n",
      "     Training Step: 118 Training Loss: 0.6124926805496216 \n",
      "     Training Step: 119 Training Loss: 0.6123858094215393 \n",
      "     Training Step: 120 Training Loss: 0.6141513586044312 \n",
      "     Training Step: 121 Training Loss: 0.6125123500823975 \n",
      "     Training Step: 122 Training Loss: 0.6117811799049377 \n",
      "     Training Step: 123 Training Loss: 0.6142055988311768 \n",
      "     Training Step: 124 Training Loss: 0.61444091796875 \n",
      "     Training Step: 125 Training Loss: 0.6157991290092468 \n",
      "     Training Step: 126 Training Loss: 0.6177497506141663 \n",
      "     Training Step: 127 Training Loss: 0.6111321449279785 \n",
      "     Training Step: 128 Training Loss: 0.6177855730056763 \n",
      "     Training Step: 129 Training Loss: 0.6108604669570923 \n",
      "     Training Step: 130 Training Loss: 0.6097055673599243 \n",
      "     Training Step: 131 Training Loss: 0.6157585978507996 \n",
      "     Training Step: 132 Training Loss: 0.6167279481887817 \n",
      "     Training Step: 133 Training Loss: 0.6167148947715759 \n",
      "     Training Step: 134 Training Loss: 0.6147692203521729 \n",
      "     Training Step: 135 Training Loss: 0.6142077445983887 \n",
      "     Training Step: 136 Training Loss: 0.6114853024482727 \n",
      "     Training Step: 137 Training Loss: 0.6170903444290161 \n",
      "     Training Step: 138 Training Loss: 0.6130650639533997 \n",
      "     Training Step: 139 Training Loss: 0.6150878667831421 \n",
      "     Training Step: 140 Training Loss: 0.6155103445053101 \n",
      "     Training Step: 141 Training Loss: 0.6152600049972534 \n",
      "     Training Step: 142 Training Loss: 0.6100780963897705 \n",
      "     Training Step: 143 Training Loss: 0.6152896881103516 \n",
      "     Training Step: 144 Training Loss: 0.6133013963699341 \n",
      "     Training Step: 145 Training Loss: 0.6115594506263733 \n",
      "     Training Step: 146 Training Loss: 0.6182365417480469 \n",
      "     Training Step: 147 Training Loss: 0.6124611496925354 \n",
      "     Training Step: 148 Training Loss: 0.6176896691322327 \n",
      "     Training Step: 149 Training Loss: 0.6121219992637634 \n",
      "     Training Step: 150 Training Loss: 0.6156821250915527 \n",
      "     Training Step: 151 Training Loss: 0.6121538281440735 \n",
      "     Training Step: 152 Training Loss: 0.6106885075569153 \n",
      "     Training Step: 153 Training Loss: 0.6157469749450684 \n",
      "     Training Step: 154 Training Loss: 0.6159511208534241 \n",
      "     Training Step: 155 Training Loss: 0.6145997643470764 \n",
      "     Training Step: 156 Training Loss: 0.6164230108261108 \n",
      "     Training Step: 157 Training Loss: 0.6196181774139404 \n",
      "     Training Step: 158 Training Loss: 0.6166585683822632 \n",
      "     Training Step: 159 Training Loss: 0.6162247061729431 \n",
      "     Training Step: 160 Training Loss: 0.614155650138855 \n",
      "     Training Step: 161 Training Loss: 0.6154425144195557 \n",
      "     Training Step: 162 Training Loss: 0.614486813545227 \n",
      "     Training Step: 163 Training Loss: 0.6147409081459045 \n",
      "     Training Step: 164 Training Loss: 0.6184117794036865 \n",
      "     Training Step: 165 Training Loss: 0.6092967391014099 \n",
      "     Training Step: 166 Training Loss: 0.6137621402740479 \n",
      "     Training Step: 167 Training Loss: 0.6166387796401978 \n",
      "     Training Step: 168 Training Loss: 0.6106269955635071 \n",
      "     Training Step: 169 Training Loss: 0.6153368353843689 \n",
      "     Training Step: 170 Training Loss: 0.6142818331718445 \n",
      "     Training Step: 171 Training Loss: 0.6104828715324402 \n",
      "     Training Step: 172 Training Loss: 0.6149588227272034 \n",
      "     Training Step: 173 Training Loss: 0.6147488355636597 \n",
      "     Training Step: 174 Training Loss: 0.6094392538070679 \n",
      "     Training Step: 175 Training Loss: 0.6140329241752625 \n",
      "     Training Step: 176 Training Loss: 0.6132100820541382 \n",
      "     Training Step: 177 Training Loss: 0.6128743290901184 \n",
      "     Training Step: 178 Training Loss: 0.6132650375366211 \n",
      "     Training Step: 179 Training Loss: 0.6122767925262451 \n",
      "     Training Step: 180 Training Loss: 0.6168115139007568 \n",
      "     Training Step: 181 Training Loss: 0.6114062666893005 \n",
      "     Training Step: 182 Training Loss: 0.6122241020202637 \n",
      "     Training Step: 183 Training Loss: 0.6107030510902405 \n",
      "     Training Step: 184 Training Loss: 0.6183670163154602 \n",
      "     Training Step: 185 Training Loss: 0.6131308078765869 \n",
      "     Training Step: 186 Training Loss: 0.6167231202125549 \n",
      "     Training Step: 187 Training Loss: 0.6147061586380005 \n",
      "     Training Step: 188 Training Loss: 0.6136156320571899 \n",
      "     Training Step: 189 Training Loss: 0.6118385791778564 \n",
      "     Training Step: 190 Training Loss: 0.6143442988395691 \n",
      "     Training Step: 191 Training Loss: 0.6158298850059509 \n",
      "     Training Step: 192 Training Loss: 0.6146078705787659 \n",
      "     Training Step: 193 Training Loss: 0.6209079623222351 \n",
      "     Training Step: 194 Training Loss: 0.6146364212036133 \n",
      "     Training Step: 195 Training Loss: 0.6153993606567383 \n",
      "     Training Step: 196 Training Loss: 0.6154770255088806 \n",
      "     Training Step: 197 Training Loss: 0.6121618747711182 \n",
      "     Training Step: 198 Training Loss: 0.6115561723709106 \n",
      "     Training Step: 199 Training Loss: 0.6144281625747681 \n",
      "     Training Step: 200 Training Loss: 0.6143354773521423 \n",
      "     Training Step: 201 Training Loss: 0.6183899641036987 \n",
      "     Training Step: 202 Training Loss: 0.613929033279419 \n",
      "     Training Step: 203 Training Loss: 0.6115992069244385 \n",
      "     Training Step: 204 Training Loss: 0.615172266960144 \n",
      "     Training Step: 205 Training Loss: 0.615156352519989 \n",
      "     Training Step: 206 Training Loss: 0.6174886226654053 \n",
      "     Training Step: 207 Training Loss: 0.6127554178237915 \n",
      "     Training Step: 208 Training Loss: 0.6082582473754883 \n",
      "     Training Step: 209 Training Loss: 0.6113935708999634 \n",
      "     Training Step: 210 Training Loss: 0.6167265176773071 \n",
      "     Training Step: 211 Training Loss: 0.612637996673584 \n",
      "     Training Step: 212 Training Loss: 0.618891716003418 \n",
      "     Training Step: 213 Training Loss: 0.6160383820533752 \n",
      "     Training Step: 214 Training Loss: 0.6146872043609619 \n",
      "     Training Step: 215 Training Loss: 0.6156879663467407 \n",
      "     Training Step: 216 Training Loss: 0.6120747923851013 \n",
      "     Training Step: 217 Training Loss: 0.6171606183052063 \n",
      "     Training Step: 218 Training Loss: 0.6138259172439575 \n",
      "     Training Step: 219 Training Loss: 0.611841082572937 \n",
      "     Training Step: 220 Training Loss: 0.612768292427063 \n",
      "     Training Step: 221 Training Loss: 0.6167938709259033 \n",
      "     Training Step: 222 Training Loss: 0.617184579372406 \n",
      "     Training Step: 223 Training Loss: 0.6122358441352844 \n",
      "     Training Step: 224 Training Loss: 0.6117932200431824 \n",
      "     Training Step: 225 Training Loss: 0.6168050765991211 \n",
      "     Training Step: 226 Training Loss: 0.6128705143928528 \n",
      "     Training Step: 227 Training Loss: 0.6166357398033142 \n",
      "     Training Step: 228 Training Loss: 0.6139266490936279 \n",
      "     Training Step: 229 Training Loss: 0.6118075251579285 \n",
      "     Training Step: 230 Training Loss: 0.6100654602050781 \n",
      "     Training Step: 231 Training Loss: 0.6132031679153442 \n",
      "     Training Step: 232 Training Loss: 0.61693274974823 \n",
      "     Training Step: 233 Training Loss: 0.609978437423706 \n",
      "     Training Step: 234 Training Loss: 0.6178056001663208 \n",
      "     Training Step: 235 Training Loss: 0.6151018738746643 \n",
      "     Training Step: 236 Training Loss: 0.6153085231781006 \n",
      "     Training Step: 237 Training Loss: 0.6188369393348694 \n",
      "     Training Step: 238 Training Loss: 0.6133130788803101 \n",
      "     Training Step: 239 Training Loss: 0.6180975437164307 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6155654788017273 \n",
      "     Validation Step: 1 Validation Loss: 0.6128345727920532 \n",
      "     Validation Step: 2 Validation Loss: 0.6142556071281433 \n",
      "     Validation Step: 3 Validation Loss: 0.6180480122566223 \n",
      "     Validation Step: 4 Validation Loss: 0.6136467456817627 \n",
      "     Validation Step: 5 Validation Loss: 0.6141151785850525 \n",
      "     Validation Step: 6 Validation Loss: 0.6111697554588318 \n",
      "     Validation Step: 7 Validation Loss: 0.6136673092842102 \n",
      "     Validation Step: 8 Validation Loss: 0.6156080365180969 \n",
      "     Validation Step: 9 Validation Loss: 0.6183260083198547 \n",
      "     Validation Step: 10 Validation Loss: 0.6162380576133728 \n",
      "     Validation Step: 11 Validation Loss: 0.6141308546066284 \n",
      "     Validation Step: 12 Validation Loss: 0.6157816052436829 \n",
      "     Validation Step: 13 Validation Loss: 0.6101647019386292 \n",
      "     Validation Step: 14 Validation Loss: 0.614535927772522 \n",
      "     Validation Step: 15 Validation Loss: 0.6170106530189514 \n",
      "     Validation Step: 16 Validation Loss: 0.6136384606361389 \n",
      "     Validation Step: 17 Validation Loss: 0.6101208329200745 \n",
      "     Validation Step: 18 Validation Loss: 0.6152230501174927 \n",
      "     Validation Step: 19 Validation Loss: 0.617679238319397 \n",
      "     Validation Step: 20 Validation Loss: 0.6148355007171631 \n",
      "     Validation Step: 21 Validation Loss: 0.6182104349136353 \n",
      "     Validation Step: 22 Validation Loss: 0.6184587478637695 \n",
      "     Validation Step: 23 Validation Loss: 0.614640474319458 \n",
      "     Validation Step: 24 Validation Loss: 0.6116474866867065 \n",
      "     Validation Step: 25 Validation Loss: 0.6148921847343445 \n",
      "     Validation Step: 26 Validation Loss: 0.6105251908302307 \n",
      "     Validation Step: 27 Validation Loss: 0.611887514591217 \n",
      "     Validation Step: 28 Validation Loss: 0.6172947287559509 \n",
      "     Validation Step: 29 Validation Loss: 0.6115633249282837 \n",
      "     Validation Step: 30 Validation Loss: 0.614567220211029 \n",
      "     Validation Step: 31 Validation Loss: 0.615314781665802 \n",
      "     Validation Step: 32 Validation Loss: 0.6111836433410645 \n",
      "     Validation Step: 33 Validation Loss: 0.6175885796546936 \n",
      "     Validation Step: 34 Validation Loss: 0.6106373071670532 \n",
      "     Validation Step: 35 Validation Loss: 0.6150311827659607 \n",
      "     Validation Step: 36 Validation Loss: 0.6104910969734192 \n",
      "     Validation Step: 37 Validation Loss: 0.6133105754852295 \n",
      "     Validation Step: 38 Validation Loss: 0.6101460456848145 \n",
      "     Validation Step: 39 Validation Loss: 0.6075350046157837 \n",
      "     Validation Step: 40 Validation Loss: 0.6159778237342834 \n",
      "     Validation Step: 41 Validation Loss: 0.6142039895057678 \n",
      "     Validation Step: 42 Validation Loss: 0.612134575843811 \n",
      "     Validation Step: 43 Validation Loss: 0.6184774041175842 \n",
      "     Validation Step: 44 Validation Loss: 0.6129902005195618 \n",
      "Epoch: 140\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6182253956794739 \n",
      "     Training Step: 1 Training Loss: 0.6167945265769958 \n",
      "     Training Step: 2 Training Loss: 0.6169060468673706 \n",
      "     Training Step: 3 Training Loss: 0.615265429019928 \n",
      "     Training Step: 4 Training Loss: 0.6121799349784851 \n",
      "     Training Step: 5 Training Loss: 0.6127418279647827 \n",
      "     Training Step: 6 Training Loss: 0.6148775815963745 \n",
      "     Training Step: 7 Training Loss: 0.6147538423538208 \n",
      "     Training Step: 8 Training Loss: 0.6149595379829407 \n",
      "     Training Step: 9 Training Loss: 0.6114203929901123 \n",
      "     Training Step: 10 Training Loss: 0.6133245825767517 \n",
      "     Training Step: 11 Training Loss: 0.6144419312477112 \n",
      "     Training Step: 12 Training Loss: 0.6167014241218567 \n",
      "     Training Step: 13 Training Loss: 0.6153736114501953 \n",
      "     Training Step: 14 Training Loss: 0.6136134266853333 \n",
      "     Training Step: 15 Training Loss: 0.613408625125885 \n",
      "     Training Step: 16 Training Loss: 0.611817479133606 \n",
      "     Training Step: 17 Training Loss: 0.6199042201042175 \n",
      "     Training Step: 18 Training Loss: 0.6157615184783936 \n",
      "     Training Step: 19 Training Loss: 0.6116226315498352 \n",
      "     Training Step: 20 Training Loss: 0.614150881767273 \n",
      "     Training Step: 21 Training Loss: 0.610390841960907 \n",
      "     Training Step: 22 Training Loss: 0.6197099685668945 \n",
      "     Training Step: 23 Training Loss: 0.6147730946540833 \n",
      "     Training Step: 24 Training Loss: 0.6160012483596802 \n",
      "     Training Step: 25 Training Loss: 0.6094029545783997 \n",
      "     Training Step: 26 Training Loss: 0.6111612319946289 \n",
      "     Training Step: 27 Training Loss: 0.6171377301216125 \n",
      "     Training Step: 28 Training Loss: 0.6132740378379822 \n",
      "     Training Step: 29 Training Loss: 0.6149159073829651 \n",
      "     Training Step: 30 Training Loss: 0.6171562671661377 \n",
      "     Training Step: 31 Training Loss: 0.6136375665664673 \n",
      "     Training Step: 32 Training Loss: 0.615827739238739 \n",
      "     Training Step: 33 Training Loss: 0.6128044724464417 \n",
      "     Training Step: 34 Training Loss: 0.6144770383834839 \n",
      "     Training Step: 35 Training Loss: 0.6134998202323914 \n",
      "     Training Step: 36 Training Loss: 0.6116164922714233 \n",
      "     Training Step: 37 Training Loss: 0.6122994422912598 \n",
      "     Training Step: 38 Training Loss: 0.6111434102058411 \n",
      "     Training Step: 39 Training Loss: 0.6101741194725037 \n",
      "     Training Step: 40 Training Loss: 0.6125262975692749 \n",
      "     Training Step: 41 Training Loss: 0.6159635186195374 \n",
      "     Training Step: 42 Training Loss: 0.612960696220398 \n",
      "     Training Step: 43 Training Loss: 0.6126395463943481 \n",
      "     Training Step: 44 Training Loss: 0.6140398383140564 \n",
      "     Training Step: 45 Training Loss: 0.6120060086250305 \n",
      "     Training Step: 46 Training Loss: 0.6129189133644104 \n",
      "     Training Step: 47 Training Loss: 0.6181135773658752 \n",
      "     Training Step: 48 Training Loss: 0.6106758713722229 \n",
      "     Training Step: 49 Training Loss: 0.6131302714347839 \n",
      "     Training Step: 50 Training Loss: 0.6156206727027893 \n",
      "     Training Step: 51 Training Loss: 0.6146422028541565 \n",
      "     Training Step: 52 Training Loss: 0.618331789970398 \n",
      "     Training Step: 53 Training Loss: 0.6173797845840454 \n",
      "     Training Step: 54 Training Loss: 0.6171867251396179 \n",
      "     Training Step: 55 Training Loss: 0.6185542345046997 \n",
      "     Training Step: 56 Training Loss: 0.6167945265769958 \n",
      "     Training Step: 57 Training Loss: 0.6184355616569519 \n",
      "     Training Step: 58 Training Loss: 0.6171185970306396 \n",
      "     Training Step: 59 Training Loss: 0.6143916845321655 \n",
      "     Training Step: 60 Training Loss: 0.6153102517127991 \n",
      "     Training Step: 61 Training Loss: 0.617667555809021 \n",
      "     Training Step: 62 Training Loss: 0.6132046580314636 \n",
      "     Training Step: 63 Training Loss: 0.615502655506134 \n",
      "     Training Step: 64 Training Loss: 0.6121881604194641 \n",
      "     Training Step: 65 Training Loss: 0.6176853775978088 \n",
      "     Training Step: 66 Training Loss: 0.6142443418502808 \n",
      "     Training Step: 67 Training Loss: 0.6142610311508179 \n",
      "     Training Step: 68 Training Loss: 0.6115421652793884 \n",
      "     Training Step: 69 Training Loss: 0.6168138980865479 \n",
      "     Training Step: 70 Training Loss: 0.614294707775116 \n",
      "     Training Step: 71 Training Loss: 0.6105765104293823 \n",
      "     Training Step: 72 Training Loss: 0.614619255065918 \n",
      "     Training Step: 73 Training Loss: 0.6169167160987854 \n",
      "     Training Step: 74 Training Loss: 0.6100423336029053 \n",
      "     Training Step: 75 Training Loss: 0.6186423897743225 \n",
      "     Training Step: 76 Training Loss: 0.6139020323753357 \n",
      "     Training Step: 77 Training Loss: 0.6177159547805786 \n",
      "     Training Step: 78 Training Loss: 0.6146671772003174 \n",
      "     Training Step: 79 Training Loss: 0.6152870655059814 \n",
      "     Training Step: 80 Training Loss: 0.6130584478378296 \n",
      "     Training Step: 81 Training Loss: 0.6151014566421509 \n",
      "     Training Step: 82 Training Loss: 0.6160283088684082 \n",
      "     Training Step: 83 Training Loss: 0.613652765750885 \n",
      "     Training Step: 84 Training Loss: 0.616483747959137 \n",
      "     Training Step: 85 Training Loss: 0.616110622882843 \n",
      "     Training Step: 86 Training Loss: 0.6124841570854187 \n",
      "     Training Step: 87 Training Loss: 0.6154016256332397 \n",
      "     Training Step: 88 Training Loss: 0.616667628288269 \n",
      "     Training Step: 89 Training Loss: 0.6133077144622803 \n",
      "     Training Step: 90 Training Loss: 0.6154200434684753 \n",
      "     Training Step: 91 Training Loss: 0.612245500087738 \n",
      "     Training Step: 92 Training Loss: 0.6139197945594788 \n",
      "     Training Step: 93 Training Loss: 0.6177971363067627 \n",
      "     Training Step: 94 Training Loss: 0.6122310757637024 \n",
      "     Training Step: 95 Training Loss: 0.6122385859489441 \n",
      "     Training Step: 96 Training Loss: 0.6153778433799744 \n",
      "     Training Step: 97 Training Loss: 0.6146042943000793 \n",
      "     Training Step: 98 Training Loss: 0.6100336909294128 \n",
      "     Training Step: 99 Training Loss: 0.6150727868080139 \n",
      "     Training Step: 100 Training Loss: 0.6137446165084839 \n",
      "     Training Step: 101 Training Loss: 0.612060546875 \n",
      "     Training Step: 102 Training Loss: 0.6094369888305664 \n",
      "     Training Step: 103 Training Loss: 0.6167364120483398 \n",
      "     Training Step: 104 Training Loss: 0.6125321388244629 \n",
      "     Training Step: 105 Training Loss: 0.6121193170547485 \n",
      "     Training Step: 106 Training Loss: 0.6117739081382751 \n",
      "     Training Step: 107 Training Loss: 0.6135938167572021 \n",
      "     Training Step: 108 Training Loss: 0.6177908778190613 \n",
      "     Training Step: 109 Training Loss: 0.6104804873466492 \n",
      "     Training Step: 110 Training Loss: 0.6142081618309021 \n",
      "     Training Step: 111 Training Loss: 0.6123717427253723 \n",
      "     Training Step: 112 Training Loss: 0.6156867742538452 \n",
      "     Training Step: 113 Training Loss: 0.6128625869750977 \n",
      "     Training Step: 114 Training Loss: 0.615683913230896 \n",
      "     Training Step: 115 Training Loss: 0.6114346981048584 \n",
      "     Training Step: 116 Training Loss: 0.6108964085578918 \n",
      "     Training Step: 117 Training Loss: 0.6092274188995361 \n",
      "     Training Step: 118 Training Loss: 0.6147030591964722 \n",
      "     Training Step: 119 Training Loss: 0.6166571974754333 \n",
      "     Training Step: 120 Training Loss: 0.6140732169151306 \n",
      "     Training Step: 121 Training Loss: 0.615744411945343 \n",
      "     Training Step: 122 Training Loss: 0.6125085353851318 \n",
      "     Training Step: 123 Training Loss: 0.6123759746551514 \n",
      "     Training Step: 124 Training Loss: 0.6147922277450562 \n",
      "     Training Step: 125 Training Loss: 0.6115459203720093 \n",
      "     Training Step: 126 Training Loss: 0.6099748611450195 \n",
      "     Training Step: 127 Training Loss: 0.6166261434555054 \n",
      "     Training Step: 128 Training Loss: 0.6151531934738159 \n",
      "     Training Step: 129 Training Loss: 0.6153364777565002 \n",
      "     Training Step: 130 Training Loss: 0.6146117448806763 \n",
      "     Training Step: 131 Training Loss: 0.6097074151039124 \n",
      "     Training Step: 132 Training Loss: 0.6162163019180298 \n",
      "     Training Step: 133 Training Loss: 0.6141493916511536 \n",
      "     Training Step: 134 Training Loss: 0.6128417253494263 \n",
      "     Training Step: 135 Training Loss: 0.6154180765151978 \n",
      "     Training Step: 136 Training Loss: 0.6105910539627075 \n",
      "     Training Step: 137 Training Loss: 0.6141048669815063 \n",
      "     Training Step: 138 Training Loss: 0.617489755153656 \n",
      "     Training Step: 139 Training Loss: 0.6166526675224304 \n",
      "     Training Step: 140 Training Loss: 0.6164131164550781 \n",
      "     Training Step: 141 Training Loss: 0.6146748065948486 \n",
      "     Training Step: 142 Training Loss: 0.6154431700706482 \n",
      "     Training Step: 143 Training Loss: 0.6103959679603577 \n",
      "     Training Step: 144 Training Loss: 0.6157412528991699 \n",
      "     Training Step: 145 Training Loss: 0.6134705543518066 \n",
      "     Training Step: 146 Training Loss: 0.6115385293960571 \n",
      "     Training Step: 147 Training Loss: 0.6176214814186096 \n",
      "     Training Step: 148 Training Loss: 0.6116370558738708 \n",
      "     Training Step: 149 Training Loss: 0.613187313079834 \n",
      "     Training Step: 150 Training Loss: 0.6143354773521423 \n",
      "     Training Step: 151 Training Loss: 0.6130656003952026 \n",
      "     Training Step: 152 Training Loss: 0.6118248105049133 \n",
      "     Training Step: 153 Training Loss: 0.6138154864311218 \n",
      "     Training Step: 154 Training Loss: 0.6146891713142395 \n",
      "     Training Step: 155 Training Loss: 0.6116827726364136 \n",
      "     Training Step: 156 Training Loss: 0.6140157580375671 \n",
      "     Training Step: 157 Training Loss: 0.6163593530654907 \n",
      "     Training Step: 158 Training Loss: 0.6133509278297424 \n",
      "     Training Step: 159 Training Loss: 0.614730954170227 \n",
      "     Training Step: 160 Training Loss: 0.6137336492538452 \n",
      "     Training Step: 161 Training Loss: 0.6121560335159302 \n",
      "     Training Step: 162 Training Loss: 0.613760232925415 \n",
      "     Training Step: 163 Training Loss: 0.6162055730819702 \n",
      "     Training Step: 164 Training Loss: 0.619625449180603 \n",
      "     Training Step: 165 Training Loss: 0.6144213676452637 \n",
      "     Training Step: 166 Training Loss: 0.616402268409729 \n",
      "     Training Step: 167 Training Loss: 0.6105827689170837 \n",
      "     Training Step: 168 Training Loss: 0.6180874109268188 \n",
      "     Training Step: 169 Training Loss: 0.6105890870094299 \n",
      "     Training Step: 170 Training Loss: 0.6140249371528625 \n",
      "     Training Step: 171 Training Loss: 0.6106817722320557 \n",
      "     Training Step: 172 Training Loss: 0.6201967597007751 \n",
      "     Training Step: 173 Training Loss: 0.6097301244735718 \n",
      "     Training Step: 174 Training Loss: 0.6155129671096802 \n",
      "     Training Step: 175 Training Loss: 0.6183962225914001 \n",
      "     Training Step: 176 Training Loss: 0.613921582698822 \n",
      "     Training Step: 177 Training Loss: 0.6162315607070923 \n",
      "     Training Step: 178 Training Loss: 0.6115239262580872 \n",
      "     Training Step: 179 Training Loss: 0.6133012175559998 \n",
      "     Training Step: 180 Training Loss: 0.6107364892959595 \n",
      "     Training Step: 181 Training Loss: 0.6097250580787659 \n",
      "     Training Step: 182 Training Loss: 0.6167641878128052 \n",
      "     Training Step: 183 Training Loss: 0.6209455132484436 \n",
      "     Training Step: 184 Training Loss: 0.6184505820274353 \n",
      "     Training Step: 185 Training Loss: 0.6144521236419678 \n",
      "     Training Step: 186 Training Loss: 0.6182149052619934 \n",
      "     Training Step: 187 Training Loss: 0.6166883707046509 \n",
      "     Training Step: 188 Training Loss: 0.6143440008163452 \n",
      "     Training Step: 189 Training Loss: 0.6179967522621155 \n",
      "     Training Step: 190 Training Loss: 0.6128928065299988 \n",
      "     Training Step: 191 Training Loss: 0.611915111541748 \n",
      "     Training Step: 192 Training Loss: 0.6194106340408325 \n",
      "     Training Step: 193 Training Loss: 0.6146570444107056 \n",
      "     Training Step: 194 Training Loss: 0.6127865314483643 \n",
      "     Training Step: 195 Training Loss: 0.6145036220550537 \n",
      "     Training Step: 196 Training Loss: 0.614337682723999 \n",
      "     Training Step: 197 Training Loss: 0.6132147908210754 \n",
      "     Training Step: 198 Training Loss: 0.6082911491394043 \n",
      "     Training Step: 199 Training Loss: 0.6114140748977661 \n",
      "     Training Step: 200 Training Loss: 0.6112006902694702 \n",
      "     Training Step: 201 Training Loss: 0.6122797131538391 \n",
      "     Training Step: 202 Training Loss: 0.6155610680580139 \n",
      "     Training Step: 203 Training Loss: 0.613456666469574 \n",
      "     Training Step: 204 Training Loss: 0.6158111691474915 \n",
      "     Training Step: 205 Training Loss: 0.6178483963012695 \n",
      "     Training Step: 206 Training Loss: 0.6123731136322021 \n",
      "     Training Step: 207 Training Loss: 0.6132118105888367 \n",
      "     Training Step: 208 Training Loss: 0.6167432069778442 \n",
      "     Training Step: 209 Training Loss: 0.6100366711616516 \n",
      "     Training Step: 210 Training Loss: 0.6118313074111938 \n",
      "     Training Step: 211 Training Loss: 0.6180473566055298 \n",
      "     Training Step: 212 Training Loss: 0.6116423606872559 \n",
      "     Training Step: 213 Training Loss: 0.611416220664978 \n",
      "     Training Step: 214 Training Loss: 0.612749457359314 \n",
      "     Training Step: 215 Training Loss: 0.6150893568992615 \n",
      "     Training Step: 216 Training Loss: 0.6118066906929016 \n",
      "     Training Step: 217 Training Loss: 0.6152986288070679 \n",
      "     Training Step: 218 Training Loss: 0.6132843494415283 \n",
      "     Training Step: 219 Training Loss: 0.6151736974716187 \n",
      "     Training Step: 220 Training Loss: 0.6104797720909119 \n",
      "     Training Step: 221 Training Loss: 0.6135095357894897 \n",
      "     Training Step: 222 Training Loss: 0.6114318370819092 \n",
      "     Training Step: 223 Training Loss: 0.6124855279922485 \n",
      "     Training Step: 224 Training Loss: 0.6147090792655945 \n",
      "     Training Step: 225 Training Loss: 0.6108628511428833 \n",
      "     Training Step: 226 Training Loss: 0.6115706562995911 \n",
      "     Training Step: 227 Training Loss: 0.6117708683013916 \n",
      "     Training Step: 228 Training Loss: 0.6106989979743958 \n",
      "     Training Step: 229 Training Loss: 0.6149464249610901 \n",
      "     Training Step: 230 Training Loss: 0.6167234182357788 \n",
      "     Training Step: 231 Training Loss: 0.6129295229911804 \n",
      "     Training Step: 232 Training Loss: 0.6171379685401917 \n",
      "     Training Step: 233 Training Loss: 0.6101160049438477 \n",
      "     Training Step: 234 Training Loss: 0.6146905422210693 \n",
      "     Training Step: 235 Training Loss: 0.6152452826499939 \n",
      "     Training Step: 236 Training Loss: 0.6188444495201111 \n",
      "     Training Step: 237 Training Loss: 0.6188737154006958 \n",
      "     Training Step: 238 Training Loss: 0.6155304312705994 \n",
      "     Training Step: 239 Training Loss: 0.6115017533302307 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6075701117515564 \n",
      "     Validation Step: 1 Validation Loss: 0.614544689655304 \n",
      "     Validation Step: 2 Validation Loss: 0.6157846450805664 \n",
      "     Validation Step: 3 Validation Loss: 0.6148433089256287 \n",
      "     Validation Step: 4 Validation Loss: 0.6130057573318481 \n",
      "     Validation Step: 5 Validation Loss: 0.6156088709831238 \n",
      "     Validation Step: 6 Validation Loss: 0.6136513352394104 \n",
      "     Validation Step: 7 Validation Loss: 0.6115822792053223 \n",
      "     Validation Step: 8 Validation Loss: 0.6159735918045044 \n",
      "     Validation Step: 9 Validation Loss: 0.6101853251457214 \n",
      "     Validation Step: 10 Validation Loss: 0.6180425882339478 \n",
      "     Validation Step: 11 Validation Loss: 0.6101453304290771 \n",
      "     Validation Step: 12 Validation Loss: 0.6116636395454407 \n",
      "     Validation Step: 13 Validation Loss: 0.6141422986984253 \n",
      "     Validation Step: 14 Validation Loss: 0.6182035207748413 \n",
      "     Validation Step: 15 Validation Loss: 0.6133240461349487 \n",
      "     Validation Step: 16 Validation Loss: 0.6172900199890137 \n",
      "     Validation Step: 17 Validation Loss: 0.6145705580711365 \n",
      "     Validation Step: 18 Validation Loss: 0.611199140548706 \n",
      "     Validation Step: 19 Validation Loss: 0.6176737546920776 \n",
      "     Validation Step: 20 Validation Loss: 0.6175882816314697 \n",
      "     Validation Step: 21 Validation Loss: 0.6105101108551025 \n",
      "     Validation Step: 22 Validation Loss: 0.6162404417991638 \n",
      "     Validation Step: 23 Validation Loss: 0.6150343418121338 \n",
      "     Validation Step: 24 Validation Loss: 0.6142652630805969 \n",
      "     Validation Step: 25 Validation Loss: 0.6105454564094543 \n",
      "     Validation Step: 26 Validation Loss: 0.6148992776870728 \n",
      "     Validation Step: 27 Validation Loss: 0.6119063496589661 \n",
      "     Validation Step: 28 Validation Loss: 0.6121517419815063 \n",
      "     Validation Step: 29 Validation Loss: 0.6101705431938171 \n",
      "     Validation Step: 30 Validation Loss: 0.6155632734298706 \n",
      "     Validation Step: 31 Validation Loss: 0.6184540390968323 \n",
      "     Validation Step: 32 Validation Loss: 0.6111894845962524 \n",
      "     Validation Step: 33 Validation Loss: 0.6183189749717712 \n",
      "     Validation Step: 34 Validation Loss: 0.6136766672134399 \n",
      "     Validation Step: 35 Validation Loss: 0.6184684634208679 \n",
      "     Validation Step: 36 Validation Loss: 0.6128518581390381 \n",
      "     Validation Step: 37 Validation Loss: 0.6170037388801575 \n",
      "     Validation Step: 38 Validation Loss: 0.6152249574661255 \n",
      "     Validation Step: 39 Validation Loss: 0.6106560826301575 \n",
      "     Validation Step: 40 Validation Loss: 0.6142147779464722 \n",
      "     Validation Step: 41 Validation Loss: 0.6136499643325806 \n",
      "     Validation Step: 42 Validation Loss: 0.6146508455276489 \n",
      "     Validation Step: 43 Validation Loss: 0.6141259670257568 \n",
      "     Validation Step: 44 Validation Loss: 0.615317702293396 \n",
      "Epoch: 141\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6133700013160706 \n",
      "     Training Step: 1 Training Loss: 0.6136592030525208 \n",
      "     Training Step: 2 Training Loss: 0.6149206161499023 \n",
      "     Training Step: 3 Training Loss: 0.6107664108276367 \n",
      "     Training Step: 4 Training Loss: 0.6167961359024048 \n",
      "     Training Step: 5 Training Loss: 0.6127631664276123 \n",
      "     Training Step: 6 Training Loss: 0.614458441734314 \n",
      "     Training Step: 7 Training Loss: 0.6107416152954102 \n",
      "     Training Step: 8 Training Loss: 0.6101901531219482 \n",
      "     Training Step: 9 Training Loss: 0.6094564199447632 \n",
      "     Training Step: 10 Training Loss: 0.6143419146537781 \n",
      "     Training Step: 11 Training Loss: 0.6184951066970825 \n",
      "     Training Step: 12 Training Loss: 0.61644446849823 \n",
      "     Training Step: 13 Training Loss: 0.6122284531593323 \n",
      "     Training Step: 14 Training Loss: 0.6103579998016357 \n",
      "     Training Step: 15 Training Loss: 0.6174108982086182 \n",
      "     Training Step: 16 Training Loss: 0.6157042384147644 \n",
      "     Training Step: 17 Training Loss: 0.6166574954986572 \n",
      "     Training Step: 18 Training Loss: 0.6097195744514465 \n",
      "     Training Step: 19 Training Loss: 0.6150966882705688 \n",
      "     Training Step: 20 Training Loss: 0.6176925897598267 \n",
      "     Training Step: 21 Training Loss: 0.6154175400733948 \n",
      "     Training Step: 22 Training Loss: 0.614165186882019 \n",
      "     Training Step: 23 Training Loss: 0.6125600337982178 \n",
      "     Training Step: 24 Training Loss: 0.6132947206497192 \n",
      "     Training Step: 25 Training Loss: 0.610023021697998 \n",
      "     Training Step: 26 Training Loss: 0.6160313487052917 \n",
      "     Training Step: 27 Training Loss: 0.6149582266807556 \n",
      "     Training Step: 28 Training Loss: 0.6157447695732117 \n",
      "     Training Step: 29 Training Loss: 0.6171354055404663 \n",
      "     Training Step: 30 Training Loss: 0.6158334612846375 \n",
      "     Training Step: 31 Training Loss: 0.6132856607437134 \n",
      "     Training Step: 32 Training Loss: 0.6145943999290466 \n",
      "     Training Step: 33 Training Loss: 0.618436872959137 \n",
      "     Training Step: 34 Training Loss: 0.6122912764549255 \n",
      "     Training Step: 35 Training Loss: 0.6117882132530212 \n",
      "     Training Step: 36 Training Loss: 0.6146383881568909 \n",
      "     Training Step: 37 Training Loss: 0.6162045001983643 \n",
      "     Training Step: 38 Training Loss: 0.6148770451545715 \n",
      "     Training Step: 39 Training Loss: 0.6120728254318237 \n",
      "     Training Step: 40 Training Loss: 0.6152734160423279 \n",
      "     Training Step: 41 Training Loss: 0.6168118715286255 \n",
      "     Training Step: 42 Training Loss: 0.6188768744468689 \n",
      "     Training Step: 43 Training Loss: 0.6155198216438293 \n",
      "     Training Step: 44 Training Loss: 0.610897958278656 \n",
      "     Training Step: 45 Training Loss: 0.6104854345321655 \n",
      "     Training Step: 46 Training Loss: 0.6127643585205078 \n",
      "     Training Step: 47 Training Loss: 0.6115304827690125 \n",
      "     Training Step: 48 Training Loss: 0.6111992597579956 \n",
      "     Training Step: 49 Training Loss: 0.6118325591087341 \n",
      "     Training Step: 50 Training Loss: 0.6163551807403564 \n",
      "     Training Step: 51 Training Loss: 0.6133111119270325 \n",
      "     Training Step: 52 Training Loss: 0.6177011132240295 \n",
      "     Training Step: 53 Training Loss: 0.6169044375419617 \n",
      "     Training Step: 54 Training Loss: 0.612118661403656 \n",
      "     Training Step: 55 Training Loss: 0.6167100667953491 \n",
      "     Training Step: 56 Training Loss: 0.6171009540557861 \n",
      "     Training Step: 57 Training Loss: 0.6157427430152893 \n",
      "     Training Step: 58 Training Loss: 0.6143695116043091 \n",
      "     Training Step: 59 Training Loss: 0.6152933239936829 \n",
      "     Training Step: 60 Training Loss: 0.6116447448730469 \n",
      "     Training Step: 61 Training Loss: 0.6154389381408691 \n",
      "     Training Step: 62 Training Loss: 0.610618531703949 \n",
      "     Training Step: 63 Training Loss: 0.618016242980957 \n",
      "     Training Step: 64 Training Loss: 0.6115480661392212 \n",
      "     Training Step: 65 Training Loss: 0.6194239854812622 \n",
      "     Training Step: 66 Training Loss: 0.6153336763381958 \n",
      "     Training Step: 67 Training Loss: 0.6147044897079468 \n",
      "     Training Step: 68 Training Loss: 0.6094251871109009 \n",
      "     Training Step: 69 Training Loss: 0.6138195991516113 \n",
      "     Training Step: 70 Training Loss: 0.6171384453773499 \n",
      "     Training Step: 71 Training Loss: 0.6146620512008667 \n",
      "     Training Step: 72 Training Loss: 0.6128699779510498 \n",
      "     Training Step: 73 Training Loss: 0.6137641668319702 \n",
      "     Training Step: 74 Training Loss: 0.6177999973297119 \n",
      "     Training Step: 75 Training Loss: 0.6138857007026672 \n",
      "     Training Step: 76 Training Loss: 0.6117809414863586 \n",
      "     Training Step: 77 Training Loss: 0.6115161776542664 \n",
      "     Training Step: 78 Training Loss: 0.616497814655304 \n",
      "     Training Step: 79 Training Loss: 0.6128630042076111 \n",
      "     Training Step: 80 Training Loss: 0.6132884621620178 \n",
      "     Training Step: 81 Training Loss: 0.6116376519203186 \n",
      "     Training Step: 82 Training Loss: 0.6134966015815735 \n",
      "     Training Step: 83 Training Loss: 0.6116059422492981 \n",
      "     Training Step: 84 Training Loss: 0.6131400465965271 \n",
      "     Training Step: 85 Training Loss: 0.6146106123924255 \n",
      "     Training Step: 86 Training Loss: 0.6153790950775146 \n",
      "     Training Step: 87 Training Loss: 0.6121537685394287 \n",
      "     Training Step: 88 Training Loss: 0.6125302314758301 \n",
      "     Training Step: 89 Training Loss: 0.6132035851478577 \n",
      "     Training Step: 90 Training Loss: 0.6126889586448669 \n",
      "     Training Step: 91 Training Loss: 0.6167184710502625 \n",
      "     Training Step: 92 Training Loss: 0.6147717833518982 \n",
      "     Training Step: 93 Training Loss: 0.6082362532615662 \n",
      "     Training Step: 94 Training Loss: 0.6160075068473816 \n",
      "     Training Step: 95 Training Loss: 0.6141493916511536 \n",
      "     Training Step: 96 Training Loss: 0.618422269821167 \n",
      "     Training Step: 97 Training Loss: 0.6140299439430237 \n",
      "     Training Step: 98 Training Loss: 0.6209332346916199 \n",
      "     Training Step: 99 Training Loss: 0.6152808666229248 \n",
      "     Training Step: 100 Training Loss: 0.609250009059906 \n",
      "     Training Step: 101 Training Loss: 0.6111745834350586 \n",
      "     Training Step: 102 Training Loss: 0.6129186749458313 \n",
      "     Training Step: 103 Training Loss: 0.6152849197387695 \n",
      "     Training Step: 104 Training Loss: 0.6151539087295532 \n",
      "     Training Step: 105 Training Loss: 0.6142051219940186 \n",
      "     Training Step: 106 Training Loss: 0.618587851524353 \n",
      "     Training Step: 107 Training Loss: 0.6100525856018066 \n",
      "     Training Step: 108 Training Loss: 0.610405445098877 \n",
      "     Training Step: 109 Training Loss: 0.6167556643486023 \n",
      "     Training Step: 110 Training Loss: 0.6130620241165161 \n",
      "     Training Step: 111 Training Loss: 0.6144416928291321 \n",
      "     Training Step: 112 Training Loss: 0.6118305921554565 \n",
      "     Training Step: 113 Training Loss: 0.6115814447402954 \n",
      "     Training Step: 114 Training Loss: 0.6128365993499756 \n",
      "     Training Step: 115 Training Loss: 0.6124833822250366 \n",
      "     Training Step: 116 Training Loss: 0.6127942204475403 \n",
      "     Training Step: 117 Training Loss: 0.6175124645233154 \n",
      "     Training Step: 118 Training Loss: 0.6143431663513184 \n",
      "     Training Step: 119 Training Loss: 0.6139227151870728 \n",
      "     Training Step: 120 Training Loss: 0.6114662885665894 \n",
      "     Training Step: 121 Training Loss: 0.6172062158584595 \n",
      "     Training Step: 122 Training Loss: 0.614209771156311 \n",
      "     Training Step: 123 Training Loss: 0.6171696782112122 \n",
      "     Training Step: 124 Training Loss: 0.6154164671897888 \n",
      "     Training Step: 125 Training Loss: 0.6151735782623291 \n",
      "     Training Step: 126 Training Loss: 0.614107608795166 \n",
      "     Training Step: 127 Training Loss: 0.6115628480911255 \n",
      "     Training Step: 128 Training Loss: 0.6111553311347961 \n",
      "     Training Step: 129 Training Loss: 0.6146095395088196 \n",
      "     Training Step: 130 Training Loss: 0.6122382879257202 \n",
      "     Training Step: 131 Training Loss: 0.6137444972991943 \n",
      "     Training Step: 132 Training Loss: 0.6131222248077393 \n",
      "     Training Step: 133 Training Loss: 0.6153758764266968 \n",
      "     Training Step: 134 Training Loss: 0.6162320971488953 \n",
      "     Training Step: 135 Training Loss: 0.6118789911270142 \n",
      "     Training Step: 136 Training Loss: 0.6121358275413513 \n",
      "     Training Step: 137 Training Loss: 0.6167206764221191 \n",
      "     Training Step: 138 Training Loss: 0.6122927069664001 \n",
      "     Training Step: 139 Training Loss: 0.615248441696167 \n",
      "     Training Step: 140 Training Loss: 0.6100485920906067 \n",
      "     Training Step: 141 Training Loss: 0.6197130084037781 \n",
      "     Training Step: 142 Training Loss: 0.6097095012664795 \n",
      "     Training Step: 143 Training Loss: 0.6133016347885132 \n",
      "     Training Step: 144 Training Loss: 0.612962007522583 \n",
      "     Training Step: 145 Training Loss: 0.6182392835617065 \n",
      "     Training Step: 146 Training Loss: 0.6144992113113403 \n",
      "     Training Step: 147 Training Loss: 0.6136125326156616 \n",
      "     Training Step: 148 Training Loss: 0.6139083504676819 \n",
      "     Training Step: 149 Training Loss: 0.6147238612174988 \n",
      "     Training Step: 150 Training Loss: 0.6113945245742798 \n",
      "     Training Step: 151 Training Loss: 0.6134740114212036 \n",
      "     Training Step: 152 Training Loss: 0.6144211292266846 \n",
      "     Training Step: 153 Training Loss: 0.6123725175857544 \n",
      "     Training Step: 154 Training Loss: 0.6118268966674805 \n",
      "     Training Step: 155 Training Loss: 0.6188417077064514 \n",
      "     Training Step: 156 Training Loss: 0.6180238723754883 \n",
      "     Training Step: 157 Training Loss: 0.6114299893379211 \n",
      "     Training Step: 158 Training Loss: 0.6135900616645813 \n",
      "     Training Step: 159 Training Loss: 0.6156817078590393 \n",
      "     Training Step: 160 Training Loss: 0.6162024140357971 \n",
      "     Training Step: 161 Training Loss: 0.6105756163597107 \n",
      "     Training Step: 162 Training Loss: 0.6155324578285217 \n",
      "     Training Step: 163 Training Loss: 0.615782618522644 \n",
      "     Training Step: 164 Training Loss: 0.6105749011039734 \n",
      "     Training Step: 165 Training Loss: 0.6166532039642334 \n",
      "     Training Step: 166 Training Loss: 0.611412525177002 \n",
      "     Training Step: 167 Training Loss: 0.611689031124115 \n",
      "     Training Step: 168 Training Loss: 0.611802339553833 \n",
      "     Training Step: 169 Training Loss: 0.6146475076675415 \n",
      "     Training Step: 170 Training Loss: 0.6167262196540833 \n",
      "     Training Step: 171 Training Loss: 0.6124598383903503 \n",
      "     Training Step: 172 Training Loss: 0.6176923513412476 \n",
      "     Training Step: 173 Training Loss: 0.614673376083374 \n",
      "     Training Step: 174 Training Loss: 0.6150668859481812 \n",
      "     Training Step: 175 Training Loss: 0.6100686192512512 \n",
      "     Training Step: 176 Training Loss: 0.6105047464370728 \n",
      "     Training Step: 177 Training Loss: 0.617807924747467 \n",
      "     Training Step: 178 Training Loss: 0.6168018579483032 \n",
      "     Training Step: 179 Training Loss: 0.6136361360549927 \n",
      "     Training Step: 180 Training Loss: 0.61775803565979 \n",
      "     Training Step: 181 Training Loss: 0.6183051466941833 \n",
      "     Training Step: 182 Training Loss: 0.6126471161842346 \n",
      "     Training Step: 183 Training Loss: 0.60973060131073 \n",
      "     Training Step: 184 Training Loss: 0.6106084585189819 \n",
      "     Training Step: 185 Training Loss: 0.6161113381385803 \n",
      "     Training Step: 186 Training Loss: 0.6132071018218994 \n",
      "     Training Step: 187 Training Loss: 0.613460123538971 \n",
      "     Training Step: 188 Training Loss: 0.614702045917511 \n",
      "     Training Step: 189 Training Loss: 0.6166529655456543 \n",
      "     Training Step: 190 Training Loss: 0.6135101914405823 \n",
      "     Training Step: 191 Training Loss: 0.6116213798522949 \n",
      "     Training Step: 192 Training Loss: 0.6125102639198303 \n",
      "     Training Step: 193 Training Loss: 0.6140748262405396 \n",
      "     Training Step: 194 Training Loss: 0.6123761534690857 \n",
      "     Training Step: 195 Training Loss: 0.6130659580230713 \n",
      "     Training Step: 196 Training Loss: 0.6123722791671753 \n",
      "     Training Step: 197 Training Loss: 0.6121534705162048 \n",
      "     Training Step: 198 Training Loss: 0.6114240288734436 \n",
      "     Training Step: 199 Training Loss: 0.618255078792572 \n",
      "     Training Step: 200 Training Loss: 0.6131821274757385 \n",
      "     Training Step: 201 Training Loss: 0.6199033856391907 \n",
      "     Training Step: 202 Training Loss: 0.610863208770752 \n",
      "     Training Step: 203 Training Loss: 0.6106793284416199 \n",
      "     Training Step: 204 Training Loss: 0.6154772639274597 \n",
      "     Training Step: 205 Training Loss: 0.6101288795471191 \n",
      "     Training Step: 206 Training Loss: 0.6185846328735352 \n",
      "     Training Step: 207 Training Loss: 0.6153967380523682 \n",
      "     Training Step: 208 Training Loss: 0.6144774556159973 \n",
      "     Training Step: 209 Training Loss: 0.6142419576644897 \n",
      "     Training Step: 210 Training Loss: 0.6150864958763123 \n",
      "     Training Step: 211 Training Loss: 0.6129201650619507 \n",
      "     Training Step: 212 Training Loss: 0.6137424111366272 \n",
      "     Training Step: 213 Training Loss: 0.6155993342399597 \n",
      "     Training Step: 214 Training Loss: 0.616608738899231 \n",
      "     Training Step: 215 Training Loss: 0.6155301928520203 \n",
      "     Training Step: 216 Training Loss: 0.614017903804779 \n",
      "     Training Step: 217 Training Loss: 0.6180872917175293 \n",
      "     Training Step: 218 Training Loss: 0.6120231747627258 \n",
      "     Training Step: 219 Training Loss: 0.6164008378982544 \n",
      "     Training Step: 220 Training Loss: 0.6201946139335632 \n",
      "     Training Step: 221 Training Loss: 0.618051290512085 \n",
      "     Training Step: 222 Training Loss: 0.6114416718482971 \n",
      "     Training Step: 223 Training Loss: 0.6149342656135559 \n",
      "     Training Step: 224 Training Loss: 0.610688328742981 \n",
      "     Training Step: 225 Training Loss: 0.6146753430366516 \n",
      "     Training Step: 226 Training Loss: 0.6157500743865967 \n",
      "     Training Step: 227 Training Loss: 0.614022433757782 \n",
      "     Training Step: 228 Training Loss: 0.6147443056106567 \n",
      "     Training Step: 229 Training Loss: 0.6169257760047913 \n",
      "     Training Step: 230 Training Loss: 0.6166728138923645 \n",
      "     Training Step: 231 Training Loss: 0.6134095788002014 \n",
      "     Training Step: 232 Training Loss: 0.6176295280456543 \n",
      "     Training Step: 233 Training Loss: 0.6147938370704651 \n",
      "     Training Step: 234 Training Loss: 0.6142855882644653 \n",
      "     Training Step: 235 Training Loss: 0.6146894097328186 \n",
      "     Training Step: 236 Training Loss: 0.6195985674858093 \n",
      "     Training Step: 237 Training Loss: 0.614345371723175 \n",
      "     Training Step: 238 Training Loss: 0.6159397959709167 \n",
      "     Training Step: 239 Training Loss: 0.6122585535049438 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6145693063735962 \n",
      "     Validation Step: 1 Validation Loss: 0.6176765561103821 \n",
      "     Validation Step: 2 Validation Loss: 0.6105074882507324 \n",
      "     Validation Step: 3 Validation Loss: 0.6182012557983398 \n",
      "     Validation Step: 4 Validation Loss: 0.6106537580490112 \n",
      "     Validation Step: 5 Validation Loss: 0.617583692073822 \n",
      "     Validation Step: 6 Validation Loss: 0.6148945689201355 \n",
      "     Validation Step: 7 Validation Loss: 0.6136762499809265 \n",
      "     Validation Step: 8 Validation Loss: 0.6150341629981995 \n",
      "     Validation Step: 9 Validation Loss: 0.6184672713279724 \n",
      "     Validation Step: 10 Validation Loss: 0.6121482849121094 \n",
      "     Validation Step: 11 Validation Loss: 0.6184503436088562 \n",
      "     Validation Step: 12 Validation Loss: 0.6156091690063477 \n",
      "     Validation Step: 13 Validation Loss: 0.6145409941673279 \n",
      "     Validation Step: 14 Validation Loss: 0.617289662361145 \n",
      "     Validation Step: 15 Validation Loss: 0.6142591834068298 \n",
      "     Validation Step: 16 Validation Loss: 0.6157878637313843 \n",
      "     Validation Step: 17 Validation Loss: 0.6155633330345154 \n",
      "     Validation Step: 18 Validation Loss: 0.6183170676231384 \n",
      "     Validation Step: 19 Validation Loss: 0.6115792989730835 \n",
      "     Validation Step: 20 Validation Loss: 0.6142101883888245 \n",
      "     Validation Step: 21 Validation Loss: 0.6153145432472229 \n",
      "     Validation Step: 22 Validation Loss: 0.6101813912391663 \n",
      "     Validation Step: 23 Validation Loss: 0.6119049787521362 \n",
      "     Validation Step: 24 Validation Loss: 0.6146467924118042 \n",
      "     Validation Step: 25 Validation Loss: 0.6159760355949402 \n",
      "     Validation Step: 26 Validation Loss: 0.6180402636528015 \n",
      "     Validation Step: 27 Validation Loss: 0.61016446352005 \n",
      "     Validation Step: 28 Validation Loss: 0.6141237020492554 \n",
      "     Validation Step: 29 Validation Loss: 0.6141395568847656 \n",
      "     Validation Step: 30 Validation Loss: 0.6170016527175903 \n",
      "     Validation Step: 31 Validation Loss: 0.6128482818603516 \n",
      "     Validation Step: 32 Validation Loss: 0.6133182644844055 \n",
      "     Validation Step: 33 Validation Loss: 0.6075646877288818 \n",
      "     Validation Step: 34 Validation Loss: 0.6148431897163391 \n",
      "     Validation Step: 35 Validation Loss: 0.6116613745689392 \n",
      "     Validation Step: 36 Validation Loss: 0.6152253150939941 \n",
      "     Validation Step: 37 Validation Loss: 0.6136497855186462 \n",
      "     Validation Step: 38 Validation Loss: 0.616237223148346 \n",
      "     Validation Step: 39 Validation Loss: 0.6130036115646362 \n",
      "     Validation Step: 40 Validation Loss: 0.6136512756347656 \n",
      "     Validation Step: 41 Validation Loss: 0.6105438470840454 \n",
      "     Validation Step: 42 Validation Loss: 0.6111976504325867 \n",
      "     Validation Step: 43 Validation Loss: 0.6111892461776733 \n",
      "     Validation Step: 44 Validation Loss: 0.6101405620574951 \n",
      "Epoch: 142\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6144792437553406 \n",
      "     Training Step: 1 Training Loss: 0.6118020415306091 \n",
      "     Training Step: 2 Training Loss: 0.6151716709136963 \n",
      "     Training Step: 3 Training Loss: 0.6116332411766052 \n",
      "     Training Step: 4 Training Loss: 0.6157428026199341 \n",
      "     Training Step: 5 Training Loss: 0.6105788350105286 \n",
      "     Training Step: 6 Training Loss: 0.6146786212921143 \n",
      "     Training Step: 7 Training Loss: 0.6137341856956482 \n",
      "     Training Step: 8 Training Loss: 0.6161229610443115 \n",
      "     Training Step: 9 Training Loss: 0.6134589314460754 \n",
      "     Training Step: 10 Training Loss: 0.612459659576416 \n",
      "     Training Step: 11 Training Loss: 0.6104809641838074 \n",
      "     Training Step: 12 Training Loss: 0.614217221736908 \n",
      "     Training Step: 13 Training Loss: 0.6100388765335083 \n",
      "     Training Step: 14 Training Loss: 0.6153867840766907 \n",
      "     Training Step: 15 Training Loss: 0.6151183247566223 \n",
      "     Training Step: 16 Training Loss: 0.6104518175125122 \n",
      "     Training Step: 17 Training Loss: 0.6129629015922546 \n",
      "     Training Step: 18 Training Loss: 0.6152468919754028 \n",
      "     Training Step: 19 Training Loss: 0.6146190762519836 \n",
      "     Training Step: 20 Training Loss: 0.6141549944877625 \n",
      "     Training Step: 21 Training Loss: 0.6151553392410278 \n",
      "     Training Step: 22 Training Loss: 0.6144993901252747 \n",
      "     Training Step: 23 Training Loss: 0.610390841960907 \n",
      "     Training Step: 24 Training Loss: 0.6136104464530945 \n",
      "     Training Step: 25 Training Loss: 0.6178023219108582 \n",
      "     Training Step: 26 Training Loss: 0.6146141886711121 \n",
      "     Training Step: 27 Training Loss: 0.6132046580314636 \n",
      "     Training Step: 28 Training Loss: 0.6130630970001221 \n",
      "     Training Step: 29 Training Loss: 0.614916205406189 \n",
      "     Training Step: 30 Training Loss: 0.6173747181892395 \n",
      "     Training Step: 31 Training Loss: 0.6209116578102112 \n",
      "     Training Step: 32 Training Loss: 0.6115370392799377 \n",
      "     Training Step: 33 Training Loss: 0.6136530041694641 \n",
      "     Training Step: 34 Training Loss: 0.6134234070777893 \n",
      "     Training Step: 35 Training Loss: 0.6114518642425537 \n",
      "     Training Step: 36 Training Loss: 0.610906183719635 \n",
      "     Training Step: 37 Training Loss: 0.6180104613304138 \n",
      "     Training Step: 38 Training Loss: 0.6097311973571777 \n",
      "     Training Step: 39 Training Loss: 0.6176390051841736 \n",
      "     Training Step: 40 Training Loss: 0.6156865358352661 \n",
      "     Training Step: 41 Training Loss: 0.6141075491905212 \n",
      "     Training Step: 42 Training Loss: 0.6132901310920715 \n",
      "     Training Step: 43 Training Loss: 0.6143753528594971 \n",
      "     Training Step: 44 Training Loss: 0.6180719137191772 \n",
      "     Training Step: 45 Training Loss: 0.6122338175773621 \n",
      "     Training Step: 46 Training Loss: 0.6167029142379761 \n",
      "     Training Step: 47 Training Loss: 0.6144440770149231 \n",
      "     Training Step: 48 Training Loss: 0.6167985796928406 \n",
      "     Training Step: 49 Training Loss: 0.6167086958885193 \n",
      "     Training Step: 50 Training Loss: 0.6184260845184326 \n",
      "     Training Step: 51 Training Loss: 0.6114369630813599 \n",
      "     Training Step: 52 Training Loss: 0.6133660078048706 \n",
      "     Training Step: 53 Training Loss: 0.6152880191802979 \n",
      "     Training Step: 54 Training Loss: 0.6142511367797852 \n",
      "     Training Step: 55 Training Loss: 0.6105924844741821 \n",
      "     Training Step: 56 Training Loss: 0.6133227944374084 \n",
      "     Training Step: 57 Training Loss: 0.6147942543029785 \n",
      "     Training Step: 58 Training Loss: 0.6196084022521973 \n",
      "     Training Step: 59 Training Loss: 0.6122978329658508 \n",
      "     Training Step: 60 Training Loss: 0.6101378798484802 \n",
      "     Training Step: 61 Training Loss: 0.6144505739212036 \n",
      "     Training Step: 62 Training Loss: 0.6160386204719543 \n",
      "     Training Step: 63 Training Loss: 0.613181471824646 \n",
      "     Training Step: 64 Training Loss: 0.6177020072937012 \n",
      "     Training Step: 65 Training Loss: 0.6162156462669373 \n",
      "     Training Step: 66 Training Loss: 0.6124839186668396 \n",
      "     Training Step: 67 Training Loss: 0.6103681921958923 \n",
      "     Training Step: 68 Training Loss: 0.6120645403862 \n",
      "     Training Step: 69 Training Loss: 0.6183258295059204 \n",
      "     Training Step: 70 Training Loss: 0.6150684356689453 \n",
      "     Training Step: 71 Training Loss: 0.6116864085197449 \n",
      "     Training Step: 72 Training Loss: 0.6142820119857788 \n",
      "     Training Step: 73 Training Loss: 0.612754225730896 \n",
      "     Training Step: 74 Training Loss: 0.6132850050926208 \n",
      "     Training Step: 75 Training Loss: 0.6128417253494263 \n",
      "     Training Step: 76 Training Loss: 0.6128705143928528 \n",
      "     Training Step: 77 Training Loss: 0.6128664016723633 \n",
      "     Training Step: 78 Training Loss: 0.6116079092025757 \n",
      "     Training Step: 79 Training Loss: 0.6114026308059692 \n",
      "     Training Step: 80 Training Loss: 0.6198847889900208 \n",
      "     Training Step: 81 Training Loss: 0.6117781400680542 \n",
      "     Training Step: 82 Training Loss: 0.6154454946517944 \n",
      "     Training Step: 83 Training Loss: 0.6118226647377014 \n",
      "     Training Step: 84 Training Loss: 0.6115770936012268 \n",
      "     Training Step: 85 Training Loss: 0.6155394315719604 \n",
      "     Training Step: 86 Training Loss: 0.6171050667762756 \n",
      "     Training Step: 87 Training Loss: 0.6166762709617615 \n",
      "     Training Step: 88 Training Loss: 0.6154756546020508 \n",
      "     Training Step: 89 Training Loss: 0.6121221780776978 \n",
      "     Training Step: 90 Training Loss: 0.6171392202377319 \n",
      "     Training Step: 91 Training Loss: 0.612286388874054 \n",
      "     Training Step: 92 Training Loss: 0.6126998066902161 \n",
      "     Training Step: 93 Training Loss: 0.6171528100967407 \n",
      "     Training Step: 94 Training Loss: 0.6146522760391235 \n",
      "     Training Step: 95 Training Loss: 0.6171843409538269 \n",
      "     Training Step: 96 Training Loss: 0.6114482879638672 \n",
      "     Training Step: 97 Training Loss: 0.6154168248176575 \n",
      "     Training Step: 98 Training Loss: 0.6155922412872314 \n",
      "     Training Step: 99 Training Loss: 0.6146731376647949 \n",
      "     Training Step: 100 Training Loss: 0.6111531853675842 \n",
      "     Training Step: 101 Training Loss: 0.614154577255249 \n",
      "     Training Step: 102 Training Loss: 0.6138835549354553 \n",
      "     Training Step: 103 Training Loss: 0.610180675983429 \n",
      "     Training Step: 104 Training Loss: 0.6168026328086853 \n",
      "     Training Step: 105 Training Loss: 0.6127949953079224 \n",
      "     Training Step: 106 Training Loss: 0.6137593388557434 \n",
      "     Training Step: 107 Training Loss: 0.6152679324150085 \n",
      "     Training Step: 108 Training Loss: 0.6132774353027344 \n",
      "     Training Step: 109 Training Loss: 0.6125261783599854 \n",
      "     Training Step: 110 Training Loss: 0.615524172782898 \n",
      "     Training Step: 111 Training Loss: 0.6131361126899719 \n",
      "     Training Step: 112 Training Loss: 0.611517608165741 \n",
      "     Training Step: 113 Training Loss: 0.6082332730293274 \n",
      "     Training Step: 114 Training Loss: 0.6121557950973511 \n",
      "     Training Step: 115 Training Loss: 0.6138174533843994 \n",
      "     Training Step: 116 Training Loss: 0.6106473207473755 \n",
      "     Training Step: 117 Training Loss: 0.612760603427887 \n",
      "     Training Step: 118 Training Loss: 0.6122340559959412 \n",
      "     Training Step: 119 Training Loss: 0.6175205707550049 \n",
      "     Training Step: 120 Training Loss: 0.6100170016288757 \n",
      "     Training Step: 121 Training Loss: 0.6130688190460205 \n",
      "     Training Step: 122 Training Loss: 0.6147512197494507 \n",
      "     Training Step: 123 Training Loss: 0.6143444776535034 \n",
      "     Training Step: 124 Training Loss: 0.6157492995262146 \n",
      "     Training Step: 125 Training Loss: 0.6139205098152161 \n",
      "     Training Step: 126 Training Loss: 0.6186050176620483 \n",
      "     Training Step: 127 Training Loss: 0.6118358969688416 \n",
      "     Training Step: 128 Training Loss: 0.6161990165710449 \n",
      "     Training Step: 129 Training Loss: 0.6120205521583557 \n",
      "     Training Step: 130 Training Loss: 0.6143379807472229 \n",
      "     Training Step: 131 Training Loss: 0.6183871030807495 \n",
      "     Training Step: 132 Training Loss: 0.6107540726661682 \n",
      "     Training Step: 133 Training Loss: 0.6157523989677429 \n",
      "     Training Step: 134 Training Loss: 0.614074170589447 \n",
      "     Training Step: 135 Training Loss: 0.6159929037094116 \n",
      "     Training Step: 136 Training Loss: 0.614708423614502 \n",
      "     Training Step: 137 Training Loss: 0.611846387386322 \n",
      "     Training Step: 138 Training Loss: 0.6092491149902344 \n",
      "     Training Step: 139 Training Loss: 0.6196824312210083 \n",
      "     Training Step: 140 Training Loss: 0.6166964769363403 \n",
      "     Training Step: 141 Training Loss: 0.6118826866149902 \n",
      "     Training Step: 142 Training Loss: 0.6167546510696411 \n",
      "     Training Step: 143 Training Loss: 0.6182191967964172 \n",
      "     Training Step: 144 Training Loss: 0.620211660861969 \n",
      "     Training Step: 145 Training Loss: 0.6129091382026672 \n",
      "     Training Step: 146 Training Loss: 0.6125379800796509 \n",
      "     Training Step: 147 Training Loss: 0.6114983558654785 \n",
      "     Training Step: 148 Training Loss: 0.6107035279273987 \n",
      "     Training Step: 149 Training Loss: 0.616645336151123 \n",
      "     Training Step: 150 Training Loss: 0.6166338324546814 \n",
      "     Training Step: 151 Training Loss: 0.6132104396820068 \n",
      "     Training Step: 152 Training Loss: 0.6108915209770203 \n",
      "     Training Step: 153 Training Loss: 0.6146698594093323 \n",
      "     Training Step: 154 Training Loss: 0.6150848269462585 \n",
      "     Training Step: 155 Training Loss: 0.6188690662384033 \n",
      "     Training Step: 156 Training Loss: 0.6168915033340454 \n",
      "     Training Step: 157 Training Loss: 0.6097260117530823 \n",
      "     Training Step: 158 Training Loss: 0.6139109134674072 \n",
      "     Training Step: 159 Training Loss: 0.6107231974601746 \n",
      "     Training Step: 160 Training Loss: 0.613497257232666 \n",
      "     Training Step: 161 Training Loss: 0.6111577153205872 \n",
      "     Training Step: 162 Training Loss: 0.614030659198761 \n",
      "     Training Step: 163 Training Loss: 0.6177270412445068 \n",
      "     Training Step: 164 Training Loss: 0.6149610877037048 \n",
      "     Training Step: 165 Training Loss: 0.6121513843536377 \n",
      "     Training Step: 166 Training Loss: 0.6134722232818604 \n",
      "     Training Step: 167 Training Loss: 0.6105861067771912 \n",
      "     Training Step: 168 Training Loss: 0.61421138048172 \n",
      "     Training Step: 169 Training Loss: 0.6162403225898743 \n",
      "     Training Step: 170 Training Loss: 0.6169312596321106 \n",
      "     Training Step: 171 Training Loss: 0.6132993698120117 \n",
      "     Training Step: 172 Training Loss: 0.6094620823860168 \n",
      "     Training Step: 173 Training Loss: 0.6115314960479736 \n",
      "     Training Step: 174 Training Loss: 0.6135896444320679 \n",
      "     Training Step: 175 Training Loss: 0.6123817563056946 \n",
      "     Training Step: 176 Training Loss: 0.6176959872245789 \n",
      "     Training Step: 177 Training Loss: 0.6112000942230225 \n",
      "     Training Step: 178 Training Loss: 0.61460942029953 \n",
      "     Training Step: 179 Training Loss: 0.6153433322906494 \n",
      "     Training Step: 180 Training Loss: 0.6163538694381714 \n",
      "     Training Step: 181 Training Loss: 0.6135073900222778 \n",
      "     Training Step: 182 Training Loss: 0.61776202917099 \n",
      "     Training Step: 183 Training Loss: 0.6146364808082581 \n",
      "     Training Step: 184 Training Loss: 0.6116330623626709 \n",
      "     Training Step: 185 Training Loss: 0.6126474142074585 \n",
      "     Training Step: 186 Training Loss: 0.6188356280326843 \n",
      "     Training Step: 187 Training Loss: 0.6194312572479248 \n",
      "     Training Step: 188 Training Loss: 0.6164183020591736 \n",
      "     Training Step: 189 Training Loss: 0.614692747592926 \n",
      "     Training Step: 190 Training Loss: 0.6118224263191223 \n",
      "     Training Step: 191 Training Loss: 0.616793155670166 \n",
      "     Training Step: 192 Training Loss: 0.6164793372154236 \n",
      "     Training Step: 193 Training Loss: 0.6154063940048218 \n",
      "     Training Step: 194 Training Loss: 0.6152905821800232 \n",
      "     Training Step: 195 Training Loss: 0.6156598925590515 \n",
      "     Training Step: 196 Training Loss: 0.6124044060707092 \n",
      "     Training Step: 197 Training Loss: 0.6147328615188599 \n",
      "     Training Step: 198 Training Loss: 0.614777684211731 \n",
      "     Training Step: 199 Training Loss: 0.6184415817260742 \n",
      "     Training Step: 200 Training Loss: 0.6180896162986755 \n",
      "     Training Step: 201 Training Loss: 0.6140193343162537 \n",
      "     Training Step: 202 Training Loss: 0.6114140748977661 \n",
      "     Training Step: 203 Training Loss: 0.6100749373435974 \n",
      "     Training Step: 204 Training Loss: 0.6123895049095154 \n",
      "     Training Step: 205 Training Loss: 0.6125187873840332 \n",
      "     Training Step: 206 Training Loss: 0.6178162693977356 \n",
      "     Training Step: 207 Training Loss: 0.614024817943573 \n",
      "     Training Step: 208 Training Loss: 0.615550696849823 \n",
      "     Training Step: 209 Training Loss: 0.6167280077934265 \n",
      "     Training Step: 210 Training Loss: 0.6171494126319885 \n",
      "     Training Step: 211 Training Loss: 0.6166746616363525 \n",
      "     Training Step: 212 Training Loss: 0.6159468293190002 \n",
      "     Training Step: 213 Training Loss: 0.61822509765625 \n",
      "     Training Step: 214 Training Loss: 0.6148693561553955 \n",
      "     Training Step: 215 Training Loss: 0.6136500239372253 \n",
      "     Training Step: 216 Training Loss: 0.6164060831069946 \n",
      "     Training Step: 217 Training Loss: 0.6152861714363098 \n",
      "     Training Step: 218 Training Loss: 0.6180124878883362 \n",
      "     Training Step: 219 Training Loss: 0.6185453534126282 \n",
      "     Training Step: 220 Training Loss: 0.6100390553474426 \n",
      "     Training Step: 221 Training Loss: 0.6154013872146606 \n",
      "     Training Step: 222 Training Loss: 0.6158294677734375 \n",
      "     Training Step: 223 Training Loss: 0.6147021651268005 \n",
      "     Training Step: 224 Training Loss: 0.6129422187805176 \n",
      "     Training Step: 225 Training Loss: 0.6165992617607117 \n",
      "     Training Step: 226 Training Loss: 0.611655592918396 \n",
      "     Training Step: 227 Training Loss: 0.610603928565979 \n",
      "     Training Step: 228 Training Loss: 0.6143423318862915 \n",
      "     Training Step: 229 Training Loss: 0.6096987724304199 \n",
      "     Training Step: 230 Training Loss: 0.6144272089004517 \n",
      "     Training Step: 231 Training Loss: 0.6153937578201294 \n",
      "     Training Step: 232 Training Loss: 0.6115431785583496 \n",
      "     Training Step: 233 Training Loss: 0.6149439215660095 \n",
      "     Training Step: 234 Training Loss: 0.6093623638153076 \n",
      "     Training Step: 235 Training Loss: 0.6158129572868347 \n",
      "     Training Step: 236 Training Loss: 0.6122273206710815 \n",
      "     Training Step: 237 Training Loss: 0.6121412515640259 \n",
      "     Training Step: 238 Training Loss: 0.6137516498565674 \n",
      "     Training Step: 239 Training Loss: 0.613111674785614 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6153430342674255 \n",
      "     Validation Step: 1 Validation Loss: 0.6104884147644043 \n",
      "     Validation Step: 2 Validation Loss: 0.6141163110733032 \n",
      "     Validation Step: 3 Validation Loss: 0.6145393252372742 \n",
      "     Validation Step: 4 Validation Loss: 0.614129900932312 \n",
      "     Validation Step: 5 Validation Loss: 0.6146419644355774 \n",
      "     Validation Step: 6 Validation Loss: 0.6121131181716919 \n",
      "     Validation Step: 7 Validation Loss: 0.6158080697059631 \n",
      "     Validation Step: 8 Validation Loss: 0.6074581742286682 \n",
      "     Validation Step: 9 Validation Loss: 0.6149084568023682 \n",
      "     Validation Step: 10 Validation Loss: 0.6185089349746704 \n",
      "     Validation Step: 11 Validation Loss: 0.6173424124717712 \n",
      "     Validation Step: 12 Validation Loss: 0.6177313327789307 \n",
      "     Validation Step: 13 Validation Loss: 0.6176252961158752 \n",
      "     Validation Step: 14 Validation Loss: 0.6115393042564392 \n",
      "     Validation Step: 15 Validation Loss: 0.6156023740768433 \n",
      "     Validation Step: 16 Validation Loss: 0.6129758954048157 \n",
      "     Validation Step: 17 Validation Loss: 0.6182692646980286 \n",
      "     Validation Step: 18 Validation Loss: 0.6152470707893372 \n",
      "     Validation Step: 19 Validation Loss: 0.6136691570281982 \n",
      "     Validation Step: 20 Validation Loss: 0.6145867109298706 \n",
      "     Validation Step: 21 Validation Loss: 0.6100779175758362 \n",
      "     Validation Step: 22 Validation Loss: 0.611861526966095 \n",
      "     Validation Step: 23 Validation Loss: 0.6150584816932678 \n",
      "     Validation Step: 24 Validation Loss: 0.6116241812705994 \n",
      "     Validation Step: 25 Validation Loss: 0.6170668005943298 \n",
      "     Validation Step: 26 Validation Loss: 0.6104632019996643 \n",
      "     Validation Step: 27 Validation Loss: 0.6101009845733643 \n",
      "     Validation Step: 28 Validation Loss: 0.6142567992210388 \n",
      "     Validation Step: 29 Validation Loss: 0.6185418963432312 \n",
      "     Validation Step: 30 Validation Loss: 0.6142047643661499 \n",
      "     Validation Step: 31 Validation Loss: 0.6128113269805908 \n",
      "     Validation Step: 32 Validation Loss: 0.6111683249473572 \n",
      "     Validation Step: 33 Validation Loss: 0.6136695742607117 \n",
      "     Validation Step: 34 Validation Loss: 0.6111477017402649 \n",
      "     Validation Step: 35 Validation Loss: 0.6181030869483948 \n",
      "     Validation Step: 36 Validation Loss: 0.6160240769386292 \n",
      "     Validation Step: 37 Validation Loss: 0.6106082797050476 \n",
      "     Validation Step: 38 Validation Loss: 0.6132974028587341 \n",
      "     Validation Step: 39 Validation Loss: 0.6101276874542236 \n",
      "     Validation Step: 40 Validation Loss: 0.6162655353546143 \n",
      "     Validation Step: 41 Validation Loss: 0.6148402690887451 \n",
      "     Validation Step: 42 Validation Loss: 0.618385374546051 \n",
      "     Validation Step: 43 Validation Loss: 0.6156370043754578 \n",
      "     Validation Step: 44 Validation Loss: 0.6136268973350525 \n",
      "Epoch: 143\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6100152730941772 \n",
      "     Training Step: 1 Training Loss: 0.6106444001197815 \n",
      "     Training Step: 2 Training Loss: 0.6129264235496521 \n",
      "     Training Step: 3 Training Loss: 0.612485945224762 \n",
      "     Training Step: 4 Training Loss: 0.6115709543228149 \n",
      "     Training Step: 5 Training Loss: 0.6158444285392761 \n",
      "     Training Step: 6 Training Loss: 0.6147974133491516 \n",
      "     Training Step: 7 Training Loss: 0.6152940988540649 \n",
      "     Training Step: 8 Training Loss: 0.6209515333175659 \n",
      "     Training Step: 9 Training Loss: 0.6166527271270752 \n",
      "     Training Step: 10 Training Loss: 0.6112054586410522 \n",
      "     Training Step: 11 Training Loss: 0.6115679740905762 \n",
      "     Training Step: 12 Training Loss: 0.6184302568435669 \n",
      "     Training Step: 13 Training Loss: 0.6146373152732849 \n",
      "     Training Step: 14 Training Loss: 0.6196661591529846 \n",
      "     Training Step: 15 Training Loss: 0.610436201095581 \n",
      "     Training Step: 16 Training Loss: 0.6100224256515503 \n",
      "     Training Step: 17 Training Loss: 0.6132009625434875 \n",
      "     Training Step: 18 Training Loss: 0.6162316799163818 \n",
      "     Training Step: 19 Training Loss: 0.6168007254600525 \n",
      "     Training Step: 20 Training Loss: 0.6114304065704346 \n",
      "     Training Step: 21 Training Loss: 0.6157418489456177 \n",
      "     Training Step: 22 Training Loss: 0.6184000372886658 \n",
      "     Training Step: 23 Training Loss: 0.6134616136550903 \n",
      "     Training Step: 24 Training Loss: 0.615944504737854 \n",
      "     Training Step: 25 Training Loss: 0.6143722534179688 \n",
      "     Training Step: 26 Training Loss: 0.6140334606170654 \n",
      "     Training Step: 27 Training Loss: 0.6148669123649597 \n",
      "     Training Step: 28 Training Loss: 0.6139236092567444 \n",
      "     Training Step: 29 Training Loss: 0.6140198707580566 \n",
      "     Training Step: 30 Training Loss: 0.6171897649765015 \n",
      "     Training Step: 31 Training Loss: 0.6164013743400574 \n",
      "     Training Step: 32 Training Loss: 0.6145975589752197 \n",
      "     Training Step: 33 Training Loss: 0.6094104051589966 \n",
      "     Training Step: 34 Training Loss: 0.6118383407592773 \n",
      "     Training Step: 35 Training Loss: 0.6156018376350403 \n",
      "     Training Step: 36 Training Loss: 0.6113952994346619 \n",
      "     Training Step: 37 Training Loss: 0.6146631836891174 \n",
      "     Training Step: 38 Training Loss: 0.6173847317695618 \n",
      "     Training Step: 39 Training Loss: 0.6138161420822144 \n",
      "     Training Step: 40 Training Loss: 0.6126938462257385 \n",
      "     Training Step: 41 Training Loss: 0.6154213547706604 \n",
      "     Training Step: 42 Training Loss: 0.617692232131958 \n",
      "     Training Step: 43 Training Loss: 0.6132889986038208 \n",
      "     Training Step: 44 Training Loss: 0.6159995198249817 \n",
      "     Training Step: 45 Training Loss: 0.6150981187820435 \n",
      "     Training Step: 46 Training Loss: 0.6142116189002991 \n",
      "     Training Step: 47 Training Loss: 0.6146990060806274 \n",
      "     Training Step: 48 Training Loss: 0.6117923259735107 \n",
      "     Training Step: 49 Training Loss: 0.611789882183075 \n",
      "     Training Step: 50 Training Loss: 0.6123831272125244 \n",
      "     Training Step: 51 Training Loss: 0.6122943162918091 \n",
      "     Training Step: 52 Training Loss: 0.6104912757873535 \n",
      "     Training Step: 53 Training Loss: 0.6154186129570007 \n",
      "     Training Step: 54 Training Loss: 0.6141076683998108 \n",
      "     Training Step: 55 Training Loss: 0.6146128177642822 \n",
      "     Training Step: 56 Training Loss: 0.6105740070343018 \n",
      "     Training Step: 57 Training Loss: 0.6147552132606506 \n",
      "     Training Step: 58 Training Loss: 0.6171838641166687 \n",
      "     Training Step: 59 Training Loss: 0.6100417971611023 \n",
      "     Training Step: 60 Training Loss: 0.6130688786506653 \n",
      "     Training Step: 61 Training Loss: 0.6188516616821289 \n",
      "     Training Step: 62 Training Loss: 0.6155492067337036 \n",
      "     Training Step: 63 Training Loss: 0.6157890558242798 \n",
      "     Training Step: 64 Training Loss: 0.6167012453079224 \n",
      "     Training Step: 65 Training Loss: 0.6176888346672058 \n",
      "     Training Step: 66 Training Loss: 0.6149559617042542 \n",
      "     Training Step: 67 Training Loss: 0.615687370300293 \n",
      "     Training Step: 68 Training Loss: 0.6133151650428772 \n",
      "     Training Step: 69 Training Loss: 0.614162027835846 \n",
      "     Training Step: 70 Training Loss: 0.619406521320343 \n",
      "     Training Step: 71 Training Loss: 0.615369975566864 \n",
      "     Training Step: 72 Training Loss: 0.6167107224464417 \n",
      "     Training Step: 73 Training Loss: 0.6124273538589478 \n",
      "     Training Step: 74 Training Loss: 0.6149263978004456 \n",
      "     Training Step: 75 Training Loss: 0.6164049506187439 \n",
      "     Training Step: 76 Training Loss: 0.6180791258811951 \n",
      "     Training Step: 77 Training Loss: 0.6116378307342529 \n",
      "     Training Step: 78 Training Loss: 0.6123915910720825 \n",
      "     Training Step: 79 Training Loss: 0.6143357157707214 \n",
      "     Training Step: 80 Training Loss: 0.6131442785263062 \n",
      "     Training Step: 81 Training Loss: 0.614773690700531 \n",
      "     Training Step: 82 Training Loss: 0.6116430163383484 \n",
      "     Training Step: 83 Training Loss: 0.6116863489151001 \n",
      "     Training Step: 84 Training Loss: 0.610462486743927 \n",
      "     Training Step: 85 Training Loss: 0.6144556403160095 \n",
      "     Training Step: 86 Training Loss: 0.6171841621398926 \n",
      "     Training Step: 87 Training Loss: 0.6185133457183838 \n",
      "     Training Step: 88 Training Loss: 0.6186395287513733 \n",
      "     Training Step: 89 Training Loss: 0.6111295819282532 \n",
      "     Training Step: 90 Training Loss: 0.6182414293289185 \n",
      "     Training Step: 91 Training Loss: 0.6180747747421265 \n",
      "     Training Step: 92 Training Loss: 0.6132800579071045 \n",
      "     Training Step: 93 Training Loss: 0.6082857847213745 \n",
      "     Training Step: 94 Training Loss: 0.6154754757881165 \n",
      "     Training Step: 95 Training Loss: 0.6166031956672668 \n",
      "     Training Step: 96 Training Loss: 0.6109133362770081 \n",
      "     Training Step: 97 Training Loss: 0.613749086856842 \n",
      "     Training Step: 98 Training Loss: 0.6107545495033264 \n",
      "     Training Step: 99 Training Loss: 0.6177876591682434 \n",
      "     Training Step: 100 Training Loss: 0.6129222512245178 \n",
      "     Training Step: 101 Training Loss: 0.6130626797676086 \n",
      "     Training Step: 102 Training Loss: 0.6139078736305237 \n",
      "     Training Step: 103 Training Loss: 0.613506019115448 \n",
      "     Training Step: 104 Training Loss: 0.6121553182601929 \n",
      "     Training Step: 105 Training Loss: 0.6157590746879578 \n",
      "     Training Step: 106 Training Loss: 0.6118239760398865 \n",
      "     Training Step: 107 Training Loss: 0.6177920699119568 \n",
      "     Training Step: 108 Training Loss: 0.6122270822525024 \n",
      "     Training Step: 109 Training Loss: 0.6105548739433289 \n",
      "     Training Step: 110 Training Loss: 0.6152451038360596 \n",
      "     Training Step: 111 Training Loss: 0.6150949001312256 \n",
      "     Training Step: 112 Training Loss: 0.6146676540374756 \n",
      "     Training Step: 113 Training Loss: 0.6133538484573364 \n",
      "     Training Step: 114 Training Loss: 0.6169033646583557 \n",
      "     Training Step: 115 Training Loss: 0.6142100691795349 \n",
      "     Training Step: 116 Training Loss: 0.6094506978988647 \n",
      "     Training Step: 117 Training Loss: 0.6167616844177246 \n",
      "     Training Step: 118 Training Loss: 0.612068235874176 \n",
      "     Training Step: 119 Training Loss: 0.6137378215789795 \n",
      "     Training Step: 120 Training Loss: 0.6127989292144775 \n",
      "     Training Step: 121 Training Loss: 0.6118828654289246 \n",
      "     Training Step: 122 Training Loss: 0.6100542545318604 \n",
      "     Training Step: 123 Training Loss: 0.6122277975082397 \n",
      "     Training Step: 124 Training Loss: 0.6134681701660156 \n",
      "     Training Step: 125 Training Loss: 0.6175065636634827 \n",
      "     Training Step: 126 Training Loss: 0.6153110265731812 \n",
      "     Training Step: 127 Training Loss: 0.615073025226593 \n",
      "     Training Step: 128 Training Loss: 0.6092127561569214 \n",
      "     Training Step: 129 Training Loss: 0.616665244102478 \n",
      "     Training Step: 130 Training Loss: 0.6114027500152588 \n",
      "     Training Step: 131 Training Loss: 0.612528920173645 \n",
      "     Training Step: 132 Training Loss: 0.6118271946907043 \n",
      "     Training Step: 133 Training Loss: 0.6101730465888977 \n",
      "     Training Step: 134 Training Loss: 0.6166632771492004 \n",
      "     Training Step: 135 Training Loss: 0.6103666424751282 \n",
      "     Training Step: 136 Training Loss: 0.6114652752876282 \n",
      "     Training Step: 137 Training Loss: 0.6114245057106018 \n",
      "     Training Step: 138 Training Loss: 0.6153792142868042 \n",
      "     Training Step: 139 Training Loss: 0.6157550811767578 \n",
      "     Training Step: 140 Training Loss: 0.615547776222229 \n",
      "     Training Step: 141 Training Loss: 0.6149342656135559 \n",
      "     Training Step: 142 Training Loss: 0.6160425543785095 \n",
      "     Training Step: 143 Training Loss: 0.6097226738929749 \n",
      "     Training Step: 144 Training Loss: 0.6165011525154114 \n",
      "     Training Step: 145 Training Loss: 0.6137663125991821 \n",
      "     Training Step: 146 Training Loss: 0.6128442287445068 \n",
      "     Training Step: 147 Training Loss: 0.6140754222869873 \n",
      "     Training Step: 148 Training Loss: 0.6153927445411682 \n",
      "     Training Step: 149 Training Loss: 0.6144971251487732 \n",
      "     Training Step: 150 Training Loss: 0.6167975068092346 \n",
      "     Training Step: 151 Training Loss: 0.6142447590827942 \n",
      "     Training Step: 152 Training Loss: 0.6163457036018372 \n",
      "     Training Step: 153 Training Loss: 0.6176170110702515 \n",
      "     Training Step: 154 Training Loss: 0.613322913646698 \n",
      "     Training Step: 155 Training Loss: 0.614020824432373 \n",
      "     Training Step: 156 Training Loss: 0.6109045147895813 \n",
      "     Training Step: 157 Training Loss: 0.6115414500236511 \n",
      "     Training Step: 158 Training Loss: 0.6120196580886841 \n",
      "     Training Step: 159 Training Loss: 0.611632764339447 \n",
      "     Training Step: 160 Training Loss: 0.6146489977836609 \n",
      "     Training Step: 161 Training Loss: 0.6162065267562866 \n",
      "     Training Step: 162 Training Loss: 0.6122792363166809 \n",
      "     Training Step: 163 Training Loss: 0.6134986281394958 \n",
      "     Training Step: 164 Training Loss: 0.6171162724494934 \n",
      "     Training Step: 165 Training Loss: 0.6151627898216248 \n",
      "     Training Step: 166 Training Loss: 0.6106783747673035 \n",
      "     Training Step: 167 Training Loss: 0.6128643155097961 \n",
      "     Training Step: 168 Training Loss: 0.6182469129562378 \n",
      "     Training Step: 169 Training Loss: 0.6151788234710693 \n",
      "     Training Step: 170 Training Loss: 0.6124626398086548 \n",
      "     Training Step: 171 Training Loss: 0.6152647137641907 \n",
      "     Training Step: 172 Training Loss: 0.6196163296699524 \n",
      "     Training Step: 173 Training Loss: 0.6132827401161194 \n",
      "     Training Step: 174 Training Loss: 0.611170768737793 \n",
      "     Training Step: 175 Training Loss: 0.611635148525238 \n",
      "     Training Step: 176 Training Loss: 0.6125379800796509 \n",
      "     Training Step: 177 Training Loss: 0.6135912537574768 \n",
      "     Training Step: 178 Training Loss: 0.6155131459236145 \n",
      "     Training Step: 179 Training Loss: 0.6127637028694153 \n",
      "     Training Step: 180 Training Loss: 0.6105875372886658 \n",
      "     Training Step: 181 Training Loss: 0.6115206480026245 \n",
      "     Training Step: 182 Training Loss: 0.6167053580284119 \n",
      "     Training Step: 183 Training Loss: 0.6096896529197693 \n",
      "     Training Step: 184 Training Loss: 0.6138875484466553 \n",
      "     Training Step: 185 Training Loss: 0.6115171909332275 \n",
      "     Training Step: 186 Training Loss: 0.6199092268943787 \n",
      "     Training Step: 187 Training Loss: 0.6177183389663696 \n",
      "     Training Step: 188 Training Loss: 0.6169487833976746 \n",
      "     Training Step: 189 Training Loss: 0.6183264255523682 \n",
      "     Training Step: 190 Training Loss: 0.6180408000946045 \n",
      "     Training Step: 191 Training Loss: 0.61448073387146 \n",
      "     Training Step: 192 Training Loss: 0.6161150932312012 \n",
      "     Training Step: 193 Training Loss: 0.6161934733390808 \n",
      "     Training Step: 194 Training Loss: 0.6141769289970398 \n",
      "     Training Step: 195 Training Loss: 0.6106245517730713 \n",
      "     Training Step: 196 Training Loss: 0.6121662259101868 \n",
      "     Training Step: 197 Training Loss: 0.6156648397445679 \n",
      "     Training Step: 198 Training Loss: 0.6146896481513977 \n",
      "     Training Step: 199 Training Loss: 0.614715039730072 \n",
      "     Training Step: 200 Training Loss: 0.6177893280982971 \n",
      "     Training Step: 201 Training Loss: 0.6132158637046814 \n",
      "     Training Step: 202 Training Loss: 0.612530529499054 \n",
      "     Training Step: 203 Training Loss: 0.6101579666137695 \n",
      "     Training Step: 204 Training Loss: 0.6097288131713867 \n",
      "     Training Step: 205 Training Loss: 0.6122280359268188 \n",
      "     Training Step: 206 Training Loss: 0.6127541065216064 \n",
      "     Training Step: 207 Training Loss: 0.6143556833267212 \n",
      "     Training Step: 208 Training Loss: 0.6121587157249451 \n",
      "     Training Step: 209 Training Loss: 0.6153590083122253 \n",
      "     Training Step: 210 Training Loss: 0.6136171221733093 \n",
      "     Training Step: 211 Training Loss: 0.614745557308197 \n",
      "     Training Step: 212 Training Loss: 0.6121412515640259 \n",
      "     Training Step: 213 Training Loss: 0.6146225333213806 \n",
      "     Training Step: 214 Training Loss: 0.6154599785804749 \n",
      "     Training Step: 215 Training Loss: 0.6168227791786194 \n",
      "     Training Step: 216 Training Loss: 0.616726815700531 \n",
      "     Training Step: 217 Training Loss: 0.6166740655899048 \n",
      "     Training Step: 218 Training Loss: 0.618569552898407 \n",
      "     Training Step: 219 Training Loss: 0.618008017539978 \n",
      "     Training Step: 220 Training Loss: 0.6201861500740051 \n",
      "     Training Step: 221 Training Loss: 0.6114906668663025 \n",
      "     Training Step: 222 Training Loss: 0.6136795878410339 \n",
      "     Training Step: 223 Training Loss: 0.6108018159866333 \n",
      "     Training Step: 224 Training Loss: 0.6171212196350098 \n",
      "     Training Step: 225 Training Loss: 0.6143527030944824 \n",
      "     Training Step: 226 Training Loss: 0.6131647229194641 \n",
      "     Training Step: 227 Training Loss: 0.6134355664253235 \n",
      "     Training Step: 228 Training Loss: 0.6144512891769409 \n",
      "     Training Step: 229 Training Loss: 0.6146919131278992 \n",
      "     Training Step: 230 Training Loss: 0.615290105342865 \n",
      "     Training Step: 231 Training Loss: 0.6144244074821472 \n",
      "     Training Step: 232 Training Loss: 0.6129629611968994 \n",
      "     Training Step: 233 Training Loss: 0.6136418581008911 \n",
      "     Training Step: 234 Training Loss: 0.6128721833229065 \n",
      "     Training Step: 235 Training Loss: 0.6142964363098145 \n",
      "     Training Step: 236 Training Loss: 0.618905782699585 \n",
      "     Training Step: 237 Training Loss: 0.6118043661117554 \n",
      "     Training Step: 238 Training Loss: 0.6126376390457153 \n",
      "     Training Step: 239 Training Loss: 0.6131992340087891 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6115448474884033 \n",
      "     Validation Step: 1 Validation Loss: 0.6141111254692078 \n",
      "     Validation Step: 2 Validation Loss: 0.6150396466255188 \n",
      "     Validation Step: 3 Validation Loss: 0.6133002042770386 \n",
      "     Validation Step: 4 Validation Loss: 0.6141231656074524 \n",
      "     Validation Step: 5 Validation Loss: 0.6145765781402588 \n",
      "     Validation Step: 6 Validation Loss: 0.6177045106887817 \n",
      "     Validation Step: 7 Validation Loss: 0.611867368221283 \n",
      "     Validation Step: 8 Validation Loss: 0.6111695170402527 \n",
      "     Validation Step: 9 Validation Loss: 0.6121184229850769 \n",
      "     Validation Step: 10 Validation Loss: 0.6157909631729126 \n",
      "     Validation Step: 11 Validation Loss: 0.6185111999511719 \n",
      "     Validation Step: 12 Validation Loss: 0.6182389259338379 \n",
      "     Validation Step: 13 Validation Loss: 0.6148347854614258 \n",
      "     Validation Step: 14 Validation Loss: 0.6183539628982544 \n",
      "     Validation Step: 15 Validation Loss: 0.610089898109436 \n",
      "     Validation Step: 16 Validation Loss: 0.6116320490837097 \n",
      "     Validation Step: 17 Validation Loss: 0.6136642694473267 \n",
      "     Validation Step: 18 Validation Loss: 0.6148964762687683 \n",
      "     Validation Step: 19 Validation Loss: 0.6106165647506714 \n",
      "     Validation Step: 20 Validation Loss: 0.6170389652252197 \n",
      "     Validation Step: 21 Validation Loss: 0.617607593536377 \n",
      "     Validation Step: 22 Validation Loss: 0.6104692220687866 \n",
      "     Validation Step: 23 Validation Loss: 0.6155831217765808 \n",
      "     Validation Step: 24 Validation Loss: 0.6156208515167236 \n",
      "     Validation Step: 25 Validation Loss: 0.610116183757782 \n",
      "     Validation Step: 26 Validation Loss: 0.6159977316856384 \n",
      "     Validation Step: 27 Validation Loss: 0.6104996800422668 \n",
      "     Validation Step: 28 Validation Loss: 0.6101405024528503 \n",
      "     Validation Step: 29 Validation Loss: 0.6173207759857178 \n",
      "     Validation Step: 30 Validation Loss: 0.6146372556686401 \n",
      "     Validation Step: 31 Validation Loss: 0.6136531233787537 \n",
      "     Validation Step: 32 Validation Loss: 0.6184849739074707 \n",
      "     Validation Step: 33 Validation Loss: 0.6142012476921082 \n",
      "     Validation Step: 34 Validation Loss: 0.614251971244812 \n",
      "     Validation Step: 35 Validation Loss: 0.618076503276825 \n",
      "     Validation Step: 36 Validation Loss: 0.6153256297111511 \n",
      "     Validation Step: 37 Validation Loss: 0.6136312484741211 \n",
      "     Validation Step: 38 Validation Loss: 0.6145339608192444 \n",
      "     Validation Step: 39 Validation Loss: 0.6074849367141724 \n",
      "     Validation Step: 40 Validation Loss: 0.6111507415771484 \n",
      "     Validation Step: 41 Validation Loss: 0.6152333617210388 \n",
      "     Validation Step: 42 Validation Loss: 0.6162514686584473 \n",
      "     Validation Step: 43 Validation Loss: 0.6129772663116455 \n",
      "     Validation Step: 44 Validation Loss: 0.6128171682357788 \n",
      "Epoch: 144\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6178095936775208 \n",
      "     Training Step: 1 Training Loss: 0.6152867674827576 \n",
      "     Training Step: 2 Training Loss: 0.6167042851448059 \n",
      "     Training Step: 3 Training Loss: 0.6118372082710266 \n",
      "     Training Step: 4 Training Loss: 0.6146757006645203 \n",
      "     Training Step: 5 Training Loss: 0.6125386953353882 \n",
      "     Training Step: 6 Training Loss: 0.6097437143325806 \n",
      "     Training Step: 7 Training Loss: 0.6116395592689514 \n",
      "     Training Step: 8 Training Loss: 0.6168867945671082 \n",
      "     Training Step: 9 Training Loss: 0.6123009920120239 \n",
      "     Training Step: 10 Training Loss: 0.6100607514381409 \n",
      "     Training Step: 11 Training Loss: 0.6118024587631226 \n",
      "     Training Step: 12 Training Loss: 0.6147084832191467 \n",
      "     Training Step: 13 Training Loss: 0.612276017665863 \n",
      "     Training Step: 14 Training Loss: 0.6142468452453613 \n",
      "     Training Step: 15 Training Loss: 0.6142057180404663 \n",
      "     Training Step: 16 Training Loss: 0.6135023236274719 \n",
      "     Training Step: 17 Training Loss: 0.6167464852333069 \n",
      "     Training Step: 18 Training Loss: 0.6156913042068481 \n",
      "     Training Step: 19 Training Loss: 0.6181254982948303 \n",
      "     Training Step: 20 Training Loss: 0.6116226315498352 \n",
      "     Training Step: 21 Training Loss: 0.6117827892303467 \n",
      "     Training Step: 22 Training Loss: 0.6134551167488098 \n",
      "     Training Step: 23 Training Loss: 0.6171942353248596 \n",
      "     Training Step: 24 Training Loss: 0.6144208312034607 \n",
      "     Training Step: 25 Training Loss: 0.6180658340454102 \n",
      "     Training Step: 26 Training Loss: 0.6188631653785706 \n",
      "     Training Step: 27 Training Loss: 0.6147250533103943 \n",
      "     Training Step: 28 Training Loss: 0.6140428185462952 \n",
      "     Training Step: 29 Training Loss: 0.6114564538002014 \n",
      "     Training Step: 30 Training Loss: 0.6097490191459656 \n",
      "     Training Step: 31 Training Loss: 0.6134145259857178 \n",
      "     Training Step: 32 Training Loss: 0.6171283721923828 \n",
      "     Training Step: 33 Training Loss: 0.6132724285125732 \n",
      "     Training Step: 34 Training Loss: 0.6164897680282593 \n",
      "     Training Step: 35 Training Loss: 0.6139116883277893 \n",
      "     Training Step: 36 Training Loss: 0.6128671765327454 \n",
      "     Training Step: 37 Training Loss: 0.6122308969497681 \n",
      "     Training Step: 38 Training Loss: 0.6122251749038696 \n",
      "     Training Step: 39 Training Loss: 0.6100376844406128 \n",
      "     Training Step: 40 Training Loss: 0.6120039820671082 \n",
      "     Training Step: 41 Training Loss: 0.6150960922241211 \n",
      "     Training Step: 42 Training Loss: 0.6151689887046814 \n",
      "     Training Step: 43 Training Loss: 0.6132698059082031 \n",
      "     Training Step: 44 Training Loss: 0.6143559813499451 \n",
      "     Training Step: 45 Training Loss: 0.6146706342697144 \n",
      "     Training Step: 46 Training Loss: 0.6155428886413574 \n",
      "     Training Step: 47 Training Loss: 0.6146056056022644 \n",
      "     Training Step: 48 Training Loss: 0.6103588342666626 \n",
      "     Training Step: 49 Training Loss: 0.6182393431663513 \n",
      "     Training Step: 50 Training Loss: 0.6202316880226135 \n",
      "     Training Step: 51 Training Loss: 0.6126419901847839 \n",
      "     Training Step: 52 Training Loss: 0.6162383556365967 \n",
      "     Training Step: 53 Training Loss: 0.6164153218269348 \n",
      "     Training Step: 54 Training Loss: 0.6121599078178406 \n",
      "     Training Step: 55 Training Loss: 0.6137745380401611 \n",
      "     Training Step: 56 Training Loss: 0.6118515729904175 \n",
      "     Training Step: 57 Training Loss: 0.6136341691017151 \n",
      "     Training Step: 58 Training Loss: 0.6177924275398254 \n",
      "     Training Step: 59 Training Loss: 0.6135953068733215 \n",
      "     Training Step: 60 Training Loss: 0.6111567616462708 \n",
      "     Training Step: 61 Training Loss: 0.6149181127548218 \n",
      "     Training Step: 62 Training Loss: 0.6153765916824341 \n",
      "     Training Step: 63 Training Loss: 0.6153741478919983 \n",
      "     Training Step: 64 Training Loss: 0.6132073402404785 \n",
      "     Training Step: 65 Training Loss: 0.6123836636543274 \n",
      "     Training Step: 66 Training Loss: 0.6124869585037231 \n",
      "     Training Step: 67 Training Loss: 0.617165744304657 \n",
      "     Training Step: 68 Training Loss: 0.6121577024459839 \n",
      "     Training Step: 69 Training Loss: 0.6161225438117981 \n",
      "     Training Step: 70 Training Loss: 0.614774763584137 \n",
      "     Training Step: 71 Training Loss: 0.615475058555603 \n",
      "     Training Step: 72 Training Loss: 0.619450032711029 \n",
      "     Training Step: 73 Training Loss: 0.616207480430603 \n",
      "     Training Step: 74 Training Loss: 0.6147452592849731 \n",
      "     Training Step: 75 Training Loss: 0.6105823516845703 \n",
      "     Training Step: 76 Training Loss: 0.6146985292434692 \n",
      "     Training Step: 77 Training Loss: 0.611496090888977 \n",
      "     Training Step: 78 Training Loss: 0.6183903813362122 \n",
      "     Training Step: 79 Training Loss: 0.6138220429420471 \n",
      "     Training Step: 80 Training Loss: 0.6182956695556641 \n",
      "     Training Step: 81 Training Loss: 0.6152909994125366 \n",
      "     Training Step: 82 Training Loss: 0.6127602458000183 \n",
      "     Training Step: 83 Training Loss: 0.6082990169525146 \n",
      "     Training Step: 84 Training Loss: 0.6159951686859131 \n",
      "     Training Step: 85 Training Loss: 0.6136502027511597 \n",
      "     Training Step: 86 Training Loss: 0.6135160326957703 \n",
      "     Training Step: 87 Training Loss: 0.611163318157196 \n",
      "     Training Step: 88 Training Loss: 0.6167181134223938 \n",
      "     Training Step: 89 Training Loss: 0.6131165027618408 \n",
      "     Training Step: 90 Training Loss: 0.6140730977058411 \n",
      "     Training Step: 91 Training Loss: 0.6168121695518494 \n",
      "     Training Step: 92 Training Loss: 0.614959716796875 \n",
      "     Training Step: 93 Training Loss: 0.615070104598999 \n",
      "     Training Step: 94 Training Loss: 0.6147945523262024 \n",
      "     Training Step: 95 Training Loss: 0.615422785282135 \n",
      "     Training Step: 96 Training Loss: 0.613197922706604 \n",
      "     Training Step: 97 Training Loss: 0.6155322194099426 \n",
      "     Training Step: 98 Training Loss: 0.6167012453079224 \n",
      "     Training Step: 99 Training Loss: 0.6136369109153748 \n",
      "     Training Step: 100 Training Loss: 0.6152430176734924 \n",
      "     Training Step: 101 Training Loss: 0.6132932305335999 \n",
      "     Training Step: 102 Training Loss: 0.6094766855239868 \n",
      "     Training Step: 103 Training Loss: 0.6146027445793152 \n",
      "     Training Step: 104 Training Loss: 0.6133168339729309 \n",
      "     Training Step: 105 Training Loss: 0.6196901798248291 \n",
      "     Training Step: 106 Training Loss: 0.6184608340263367 \n",
      "     Training Step: 107 Training Loss: 0.6209180355072021 \n",
      "     Training Step: 108 Training Loss: 0.6106995940208435 \n",
      "     Training Step: 109 Training Loss: 0.6163987517356873 \n",
      "     Training Step: 110 Training Loss: 0.6160297393798828 \n",
      "     Training Step: 111 Training Loss: 0.6144590973854065 \n",
      "     Training Step: 112 Training Loss: 0.6165984272956848 \n",
      "     Training Step: 113 Training Loss: 0.6140211224555969 \n",
      "     Training Step: 114 Training Loss: 0.6154032945632935 \n",
      "     Training Step: 115 Training Loss: 0.6142054796218872 \n",
      "     Training Step: 116 Training Loss: 0.6131955981254578 \n",
      "     Training Step: 117 Training Loss: 0.6152589917182922 \n",
      "     Training Step: 118 Training Loss: 0.61664879322052 \n",
      "     Training Step: 119 Training Loss: 0.6185803413391113 \n",
      "     Training Step: 120 Training Loss: 0.6130681037902832 \n",
      "     Training Step: 121 Training Loss: 0.6100041270256042 \n",
      "     Training Step: 122 Training Loss: 0.6127114295959473 \n",
      "     Training Step: 123 Training Loss: 0.6146696209907532 \n",
      "     Training Step: 124 Training Loss: 0.6116436719894409 \n",
      "     Training Step: 125 Training Loss: 0.616675615310669 \n",
      "     Training Step: 126 Training Loss: 0.6128386855125427 \n",
      "     Training Step: 127 Training Loss: 0.6145004630088806 \n",
      "     Training Step: 128 Training Loss: 0.6146408319473267 \n",
      "     Training Step: 129 Training Loss: 0.6176955103874207 \n",
      "     Training Step: 130 Training Loss: 0.6174958944320679 \n",
      "     Training Step: 131 Training Loss: 0.61428302526474 \n",
      "     Training Step: 132 Training Loss: 0.6180203557014465 \n",
      "     Training Step: 133 Training Loss: 0.6146083474159241 \n",
      "     Training Step: 134 Training Loss: 0.6094183921813965 \n",
      "     Training Step: 135 Training Loss: 0.6112099885940552 \n",
      "     Training Step: 136 Training Loss: 0.612470269203186 \n",
      "     Training Step: 137 Training Loss: 0.6166449189186096 \n",
      "     Training Step: 138 Training Loss: 0.6167490482330322 \n",
      "     Training Step: 139 Training Loss: 0.6115326285362244 \n",
      "     Training Step: 140 Training Loss: 0.6154385805130005 \n",
      "     Training Step: 141 Training Loss: 0.6143688559532166 \n",
      "     Training Step: 142 Training Loss: 0.6130639314651489 \n",
      "     Training Step: 143 Training Loss: 0.6113963723182678 \n",
      "     Training Step: 144 Training Loss: 0.6118782758712769 \n",
      "     Training Step: 145 Training Loss: 0.6169279217720032 \n",
      "     Training Step: 146 Training Loss: 0.6156837940216064 \n",
      "     Training Step: 147 Training Loss: 0.6115201115608215 \n",
      "     Training Step: 148 Training Loss: 0.6146500110626221 \n",
      "     Training Step: 149 Training Loss: 0.6148704290390015 \n",
      "     Training Step: 150 Training Loss: 0.6171455979347229 \n",
      "     Training Step: 151 Training Loss: 0.6128717064857483 \n",
      "     Training Step: 152 Training Loss: 0.6131432056427002 \n",
      "     Training Step: 153 Training Loss: 0.6177578568458557 \n",
      "     Training Step: 154 Training Loss: 0.6151735782623291 \n",
      "     Training Step: 155 Training Loss: 0.6141538023948669 \n",
      "     Training Step: 156 Training Loss: 0.6114149689674377 \n",
      "     Training Step: 157 Training Loss: 0.6133043169975281 \n",
      "     Training Step: 158 Training Loss: 0.6101870536804199 \n",
      "     Training Step: 159 Training Loss: 0.616348922252655 \n",
      "     Training Step: 160 Training Loss: 0.6141486763954163 \n",
      "     Training Step: 161 Training Loss: 0.611687183380127 \n",
      "     Training Step: 162 Training Loss: 0.6177099347114563 \n",
      "     Training Step: 163 Training Loss: 0.6198714375495911 \n",
      "     Training Step: 164 Training Loss: 0.6168016791343689 \n",
      "     Training Step: 165 Training Loss: 0.6143437027931213 \n",
      "     Training Step: 166 Training Loss: 0.613358736038208 \n",
      "     Training Step: 167 Training Loss: 0.6127687692642212 \n",
      "     Training Step: 168 Training Loss: 0.6115404963493347 \n",
      "     Training Step: 169 Training Loss: 0.6097411513328552 \n",
      "     Training Step: 170 Training Loss: 0.6137380599975586 \n",
      "     Training Step: 171 Training Loss: 0.614442765712738 \n",
      "     Training Step: 172 Training Loss: 0.6107236742973328 \n",
      "     Training Step: 173 Training Loss: 0.6153457760810852 \n",
      "     Training Step: 174 Training Loss: 0.6144848465919495 \n",
      "     Training Step: 175 Training Loss: 0.6108691692352295 \n",
      "     Training Step: 176 Training Loss: 0.6118223667144775 \n",
      "     Training Step: 177 Training Loss: 0.6105589866638184 \n",
      "     Training Step: 178 Training Loss: 0.6114233732223511 \n",
      "     Training Step: 179 Training Loss: 0.6157750487327576 \n",
      "     Training Step: 180 Training Loss: 0.6182762980461121 \n",
      "     Training Step: 181 Training Loss: 0.6120654940605164 \n",
      "     Training Step: 182 Training Loss: 0.6104788184165955 \n",
      "     Training Step: 183 Training Loss: 0.6143532395362854 \n",
      "     Training Step: 184 Training Loss: 0.6168239712715149 \n",
      "     Training Step: 185 Training Loss: 0.6117796301841736 \n",
      "     Training Step: 186 Training Loss: 0.6121533513069153 \n",
      "     Training Step: 187 Training Loss: 0.6158344149589539 \n",
      "     Training Step: 188 Training Loss: 0.6139199137687683 \n",
      "     Training Step: 189 Training Loss: 0.6122336387634277 \n",
      "     Training Step: 190 Training Loss: 0.6146883964538574 \n",
      "     Training Step: 191 Training Loss: 0.6114203333854675 \n",
      "     Training Step: 192 Training Loss: 0.6185711622238159 \n",
      "     Training Step: 193 Training Loss: 0.6157826781272888 \n",
      "     Training Step: 194 Training Loss: 0.6134702563285828 \n",
      "     Training Step: 195 Training Loss: 0.6150922775268555 \n",
      "     Training Step: 196 Training Loss: 0.615528404712677 \n",
      "     Training Step: 197 Training Loss: 0.618808388710022 \n",
      "     Training Step: 198 Training Loss: 0.6149412393569946 \n",
      "     Training Step: 199 Training Loss: 0.6100770235061646 \n",
      "     Training Step: 200 Training Loss: 0.6184231042861938 \n",
      "     Training Step: 201 Training Loss: 0.6115812063217163 \n",
      "     Training Step: 202 Training Loss: 0.6129127144813538 \n",
      "     Training Step: 203 Training Loss: 0.6180204749107361 \n",
      "     Training Step: 204 Training Loss: 0.6129758954048157 \n",
      "     Training Step: 205 Training Loss: 0.6159391403198242 \n",
      "     Training Step: 206 Training Loss: 0.6170868277549744 \n",
      "     Training Step: 207 Training Loss: 0.6155987977981567 \n",
      "     Training Step: 208 Training Loss: 0.6123878955841064 \n",
      "     Training Step: 209 Training Loss: 0.6129268407821655 \n",
      "     Training Step: 210 Training Loss: 0.6106046438217163 \n",
      "     Training Step: 211 Training Loss: 0.6157411336898804 \n",
      "     Training Step: 212 Training Loss: 0.6173766255378723 \n",
      "     Training Step: 213 Training Loss: 0.6106618046760559 \n",
      "     Training Step: 214 Training Loss: 0.6121217608451843 \n",
      "     Training Step: 215 Training Loss: 0.6127954125404358 \n",
      "     Training Step: 216 Training Loss: 0.610468864440918 \n",
      "     Training Step: 217 Training Loss: 0.6196379661560059 \n",
      "     Training Step: 218 Training Loss: 0.6154173016548157 \n",
      "     Training Step: 219 Training Loss: 0.61039137840271 \n",
      "     Training Step: 220 Training Loss: 0.6176980137825012 \n",
      "     Training Step: 221 Training Loss: 0.610889732837677 \n",
      "     Training Step: 222 Training Loss: 0.6152852177619934 \n",
      "     Training Step: 223 Training Loss: 0.6105777621269226 \n",
      "     Training Step: 224 Training Loss: 0.6141046285629272 \n",
      "     Training Step: 225 Training Loss: 0.6107274889945984 \n",
      "     Training Step: 226 Training Loss: 0.6138845086097717 \n",
      "     Training Step: 227 Training Loss: 0.609211802482605 \n",
      "     Training Step: 228 Training Loss: 0.6162101030349731 \n",
      "     Training Step: 229 Training Loss: 0.6115735769271851 \n",
      "     Training Step: 230 Training Loss: 0.6125055551528931 \n",
      "     Training Step: 231 Training Loss: 0.6166720390319824 \n",
      "     Training Step: 232 Training Loss: 0.6137441396713257 \n",
      "     Training Step: 233 Training Loss: 0.6176571249961853 \n",
      "     Training Step: 234 Training Loss: 0.6157466769218445 \n",
      "     Training Step: 235 Training Loss: 0.6123737692832947 \n",
      "     Training Step: 236 Training Loss: 0.6116059422492981 \n",
      "     Training Step: 237 Training Loss: 0.6140211224555969 \n",
      "     Training Step: 238 Training Loss: 0.6125395894050598 \n",
      "     Training Step: 239 Training Loss: 0.610144853591919 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6115566492080688 \n",
      "     Validation Step: 1 Validation Loss: 0.6170212030410767 \n",
      "     Validation Step: 2 Validation Loss: 0.6145673990249634 \n",
      "     Validation Step: 3 Validation Loss: 0.6142496466636658 \n",
      "     Validation Step: 4 Validation Loss: 0.6129851937294006 \n",
      "     Validation Step: 5 Validation Loss: 0.6156125664710999 \n",
      "     Validation Step: 6 Validation Loss: 0.6155756711959839 \n",
      "     Validation Step: 7 Validation Loss: 0.6136670708656311 \n",
      "     Validation Step: 8 Validation Loss: 0.6162444353103638 \n",
      "     Validation Step: 9 Validation Loss: 0.6101349592208862 \n",
      "     Validation Step: 10 Validation Loss: 0.6133021712303162 \n",
      "     Validation Step: 11 Validation Loss: 0.6153183579444885 \n",
      "     Validation Step: 12 Validation Loss: 0.6180581450462341 \n",
      "     Validation Step: 13 Validation Loss: 0.6182212829589844 \n",
      "     Validation Step: 14 Validation Loss: 0.6145322322845459 \n",
      "     Validation Step: 15 Validation Loss: 0.6183383464813232 \n",
      "     Validation Step: 16 Validation Loss: 0.6111641526222229 \n",
      "     Validation Step: 17 Validation Loss: 0.6116392016410828 \n",
      "     Validation Step: 18 Validation Loss: 0.6118776202201843 \n",
      "     Validation Step: 19 Validation Loss: 0.6150373220443726 \n",
      "     Validation Step: 20 Validation Loss: 0.6101093888282776 \n",
      "     Validation Step: 21 Validation Loss: 0.6184656620025635 \n",
      "     Validation Step: 22 Validation Loss: 0.6146389245986938 \n",
      "     Validation Step: 23 Validation Loss: 0.6175929307937622 \n",
      "     Validation Step: 24 Validation Loss: 0.6184898614883423 \n",
      "     Validation Step: 25 Validation Loss: 0.6136302351951599 \n",
      "     Validation Step: 26 Validation Loss: 0.6173016428947449 \n",
      "     Validation Step: 27 Validation Loss: 0.6104834079742432 \n",
      "     Validation Step: 28 Validation Loss: 0.6148334741592407 \n",
      "     Validation Step: 29 Validation Loss: 0.6159888505935669 \n",
      "     Validation Step: 30 Validation Loss: 0.6106281876564026 \n",
      "     Validation Step: 31 Validation Loss: 0.6176915168762207 \n",
      "     Validation Step: 32 Validation Loss: 0.6136527061462402 \n",
      "     Validation Step: 33 Validation Loss: 0.6121253371238708 \n",
      "     Validation Step: 34 Validation Loss: 0.6141104102134705 \n",
      "     Validation Step: 35 Validation Loss: 0.611177384853363 \n",
      "     Validation Step: 36 Validation Loss: 0.6152265667915344 \n",
      "     Validation Step: 37 Validation Loss: 0.6157872080802917 \n",
      "     Validation Step: 38 Validation Loss: 0.6101508736610413 \n",
      "     Validation Step: 39 Validation Loss: 0.6141297817230225 \n",
      "     Validation Step: 40 Validation Loss: 0.607513964176178 \n",
      "     Validation Step: 41 Validation Loss: 0.614890456199646 \n",
      "     Validation Step: 42 Validation Loss: 0.6142013072967529 \n",
      "     Validation Step: 43 Validation Loss: 0.6128236055374146 \n",
      "     Validation Step: 44 Validation Loss: 0.6105129718780518 \n",
      "Epoch: 145\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6103790402412415 \n",
      "     Training Step: 1 Training Loss: 0.6144958138465881 \n",
      "     Training Step: 2 Training Loss: 0.6141494512557983 \n",
      "     Training Step: 3 Training Loss: 0.616357684135437 \n",
      "     Training Step: 4 Training Loss: 0.616814136505127 \n",
      "     Training Step: 5 Training Loss: 0.6151536703109741 \n",
      "     Training Step: 6 Training Loss: 0.6151763796806335 \n",
      "     Training Step: 7 Training Loss: 0.6100537180900574 \n",
      "     Training Step: 8 Training Loss: 0.6183186173439026 \n",
      "     Training Step: 9 Training Loss: 0.6122304797172546 \n",
      "     Training Step: 10 Training Loss: 0.6129094958305359 \n",
      "     Training Step: 11 Training Loss: 0.6159980893135071 \n",
      "     Training Step: 12 Training Loss: 0.6099939346313477 \n",
      "     Training Step: 13 Training Loss: 0.6140706539154053 \n",
      "     Training Step: 14 Training Loss: 0.6152961850166321 \n",
      "     Training Step: 15 Training Loss: 0.6124911308288574 \n",
      "     Training Step: 16 Training Loss: 0.6157419681549072 \n",
      "     Training Step: 17 Training Loss: 0.6127976775169373 \n",
      "     Training Step: 18 Training Loss: 0.6128388047218323 \n",
      "     Training Step: 19 Training Loss: 0.6139266490936279 \n",
      "     Training Step: 20 Training Loss: 0.613498330116272 \n",
      "     Training Step: 21 Training Loss: 0.6182228326797485 \n",
      "     Training Step: 22 Training Loss: 0.6094569563865662 \n",
      "     Training Step: 23 Training Loss: 0.6107309460639954 \n",
      "     Training Step: 24 Training Loss: 0.6113896369934082 \n",
      "     Training Step: 25 Training Loss: 0.6156867742538452 \n",
      "     Training Step: 26 Training Loss: 0.6123736500740051 \n",
      "     Training Step: 27 Training Loss: 0.612760603427887 \n",
      "     Training Step: 28 Training Loss: 0.6116195321083069 \n",
      "     Training Step: 29 Training Loss: 0.6122240424156189 \n",
      "     Training Step: 30 Training Loss: 0.6125326156616211 \n",
      "     Training Step: 31 Training Loss: 0.6149396896362305 \n",
      "     Training Step: 32 Training Loss: 0.6114041209220886 \n",
      "     Training Step: 33 Training Loss: 0.6137689352035522 \n",
      "     Training Step: 34 Training Loss: 0.6120042204856873 \n",
      "     Training Step: 35 Training Loss: 0.6091988682746887 \n",
      "     Training Step: 36 Training Loss: 0.6132916212081909 \n",
      "     Training Step: 37 Training Loss: 0.6106708645820618 \n",
      "     Training Step: 38 Training Loss: 0.6142165660858154 \n",
      "     Training Step: 39 Training Loss: 0.6162592768669128 \n",
      "     Training Step: 40 Training Loss: 0.6096741557121277 \n",
      "     Training Step: 41 Training Loss: 0.618439793586731 \n",
      "     Training Step: 42 Training Loss: 0.6147983074188232 \n",
      "     Training Step: 43 Training Loss: 0.6152664422988892 \n",
      "     Training Step: 44 Training Loss: 0.6147514581680298 \n",
      "     Training Step: 45 Training Loss: 0.6164931654930115 \n",
      "     Training Step: 46 Training Loss: 0.6142204999923706 \n",
      "     Training Step: 47 Training Loss: 0.6129288673400879 \n",
      "     Training Step: 48 Training Loss: 0.6122958064079285 \n",
      "     Training Step: 49 Training Loss: 0.6155112981796265 \n",
      "     Training Step: 50 Training Loss: 0.6171495318412781 \n",
      "     Training Step: 51 Training Loss: 0.6118912696838379 \n",
      "     Training Step: 52 Training Loss: 0.612476646900177 \n",
      "     Training Step: 53 Training Loss: 0.6143394112586975 \n",
      "     Training Step: 54 Training Loss: 0.6146668195724487 \n",
      "     Training Step: 55 Training Loss: 0.6184296011924744 \n",
      "     Training Step: 56 Training Loss: 0.6138870120048523 \n",
      "     Training Step: 57 Training Loss: 0.6112086772918701 \n",
      "     Training Step: 58 Training Loss: 0.61459881067276 \n",
      "     Training Step: 59 Training Loss: 0.6116936206817627 \n",
      "     Training Step: 60 Training Loss: 0.6154413819313049 \n",
      "     Training Step: 61 Training Loss: 0.6167560815811157 \n",
      "     Training Step: 62 Training Loss: 0.6144382953643799 \n",
      "     Training Step: 63 Training Loss: 0.6164166331291199 \n",
      "     Training Step: 64 Training Loss: 0.6134660243988037 \n",
      "     Training Step: 65 Training Loss: 0.6132057309150696 \n",
      "     Training Step: 66 Training Loss: 0.6104817390441895 \n",
      "     Training Step: 67 Training Loss: 0.6144214868545532 \n",
      "     Training Step: 68 Training Loss: 0.6141536235809326 \n",
      "     Training Step: 69 Training Loss: 0.6166101098060608 \n",
      "     Training Step: 70 Training Loss: 0.6118243932723999 \n",
      "     Training Step: 71 Training Loss: 0.6137337684631348 \n",
      "     Training Step: 72 Training Loss: 0.612373411655426 \n",
      "     Training Step: 73 Training Loss: 0.6159496903419495 \n",
      "     Training Step: 74 Training Loss: 0.6136361360549927 \n",
      "     Training Step: 75 Training Loss: 0.6209372282028198 \n",
      "     Training Step: 76 Training Loss: 0.6154736280441284 \n",
      "     Training Step: 77 Training Loss: 0.6148672103881836 \n",
      "     Training Step: 78 Training Loss: 0.6123836636543274 \n",
      "     Training Step: 79 Training Loss: 0.6174789667129517 \n",
      "     Training Step: 80 Training Loss: 0.6155958771705627 \n",
      "     Training Step: 81 Training Loss: 0.6133196353912354 \n",
      "     Training Step: 82 Training Loss: 0.6194202899932861 \n",
      "     Training Step: 83 Training Loss: 0.6167957782745361 \n",
      "     Training Step: 84 Training Loss: 0.6150898337364197 \n",
      "     Training Step: 85 Training Loss: 0.6115469336509705 \n",
      "     Training Step: 86 Training Loss: 0.611850380897522 \n",
      "     Training Step: 87 Training Loss: 0.6131960153579712 \n",
      "     Training Step: 88 Training Loss: 0.613467276096344 \n",
      "     Training Step: 89 Training Loss: 0.610887348651886 \n",
      "     Training Step: 90 Training Loss: 0.6103993654251099 \n",
      "     Training Step: 91 Training Loss: 0.6130692958831787 \n",
      "     Training Step: 92 Training Loss: 0.6125229597091675 \n",
      "     Training Step: 93 Training Loss: 0.6154399514198303 \n",
      "     Training Step: 94 Training Loss: 0.6082130074501038 \n",
      "     Training Step: 95 Training Loss: 0.6125082969665527 \n",
      "     Training Step: 96 Training Loss: 0.6105700135231018 \n",
      "     Training Step: 97 Training Loss: 0.6154595017433167 \n",
      "     Training Step: 98 Training Loss: 0.614039421081543 \n",
      "     Training Step: 99 Training Loss: 0.6167700886726379 \n",
      "     Training Step: 100 Training Loss: 0.611774206161499 \n",
      "     Training Step: 101 Training Loss: 0.6121561527252197 \n",
      "     Training Step: 102 Training Loss: 0.6127579808235168 \n",
      "     Training Step: 103 Training Loss: 0.6168263554573059 \n",
      "     Training Step: 104 Training Loss: 0.6118030548095703 \n",
      "     Training Step: 105 Training Loss: 0.6115502119064331 \n",
      "     Training Step: 106 Training Loss: 0.6169248819351196 \n",
      "     Training Step: 107 Training Loss: 0.6146512627601624 \n",
      "     Training Step: 108 Training Loss: 0.6180964708328247 \n",
      "     Training Step: 109 Training Loss: 0.6166541576385498 \n",
      "     Training Step: 110 Training Loss: 0.6131421327590942 \n",
      "     Training Step: 111 Training Loss: 0.6101765036582947 \n",
      "     Training Step: 112 Training Loss: 0.6100848317146301 \n",
      "     Training Step: 113 Training Loss: 0.6118544340133667 \n",
      "     Training Step: 114 Training Loss: 0.6142464280128479 \n",
      "     Training Step: 115 Training Loss: 0.6161984801292419 \n",
      "     Training Step: 116 Training Loss: 0.6116250157356262 \n",
      "     Training Step: 117 Training Loss: 0.6143714189529419 \n",
      "     Training Step: 118 Training Loss: 0.6107140183448792 \n",
      "     Training Step: 119 Training Loss: 0.6182504296302795 \n",
      "     Training Step: 120 Training Loss: 0.6152893304824829 \n",
      "     Training Step: 121 Training Loss: 0.614026665687561 \n",
      "     Training Step: 122 Training Loss: 0.6185966730117798 \n",
      "     Training Step: 123 Training Loss: 0.6176462173461914 \n",
      "     Training Step: 124 Training Loss: 0.6150970458984375 \n",
      "     Training Step: 125 Training Loss: 0.6196087002754211 \n",
      "     Training Step: 126 Training Loss: 0.6157490611076355 \n",
      "     Training Step: 127 Training Loss: 0.6153652667999268 \n",
      "     Training Step: 128 Training Loss: 0.6106405854225159 \n",
      "     Training Step: 129 Training Loss: 0.6179918050765991 \n",
      "     Training Step: 130 Training Loss: 0.613091766834259 \n",
      "     Training Step: 131 Training Loss: 0.6129986047744751 \n",
      "     Training Step: 132 Training Loss: 0.6147050857543945 \n",
      "     Training Step: 133 Training Loss: 0.6196622252464294 \n",
      "     Training Step: 134 Training Loss: 0.6156666278839111 \n",
      "     Training Step: 135 Training Loss: 0.617673933506012 \n",
      "     Training Step: 136 Training Loss: 0.614696741104126 \n",
      "     Training Step: 137 Training Loss: 0.6123246550559998 \n",
      "     Training Step: 138 Training Loss: 0.6115512251853943 \n",
      "     Training Step: 139 Training Loss: 0.6155267953872681 \n",
      "     Training Step: 140 Training Loss: 0.6097428798675537 \n",
      "     Training Step: 141 Training Loss: 0.6121658086776733 \n",
      "     Training Step: 142 Training Loss: 0.6116085648536682 \n",
      "     Training Step: 143 Training Loss: 0.6162106394767761 \n",
      "     Training Step: 144 Training Loss: 0.6180924773216248 \n",
      "     Training Step: 145 Training Loss: 0.6147119998931885 \n",
      "     Training Step: 146 Training Loss: 0.6150761842727661 \n",
      "     Training Step: 147 Training Loss: 0.6153996586799622 \n",
      "     Training Step: 148 Training Loss: 0.6115731596946716 \n",
      "     Training Step: 149 Training Loss: 0.613267719745636 \n",
      "     Training Step: 150 Training Loss: 0.6152881979942322 \n",
      "     Training Step: 151 Training Loss: 0.6122288703918457 \n",
      "     Training Step: 152 Training Loss: 0.6133070588111877 \n",
      "     Training Step: 153 Training Loss: 0.6132006049156189 \n",
      "     Training Step: 154 Training Loss: 0.6114743947982788 \n",
      "     Training Step: 155 Training Loss: 0.6186133027076721 \n",
      "     Training Step: 156 Training Loss: 0.6167154312133789 \n",
      "     Training Step: 157 Training Loss: 0.6136116981506348 \n",
      "     Training Step: 158 Training Loss: 0.6142830848693848 \n",
      "     Training Step: 159 Training Loss: 0.6120714545249939 \n",
      "     Training Step: 160 Training Loss: 0.6108930706977844 \n",
      "     Training Step: 161 Training Loss: 0.6144521236419678 \n",
      "     Training Step: 162 Training Loss: 0.6166489124298096 \n",
      "     Training Step: 163 Training Loss: 0.6141064763069153 \n",
      "     Training Step: 164 Training Loss: 0.6155354976654053 \n",
      "     Training Step: 165 Training Loss: 0.6178064346313477 \n",
      "     Training Step: 166 Training Loss: 0.6111466884613037 \n",
      "     Training Step: 167 Training Loss: 0.6157830357551575 \n",
      "     Training Step: 168 Training Loss: 0.6133555769920349 \n",
      "     Training Step: 169 Training Loss: 0.6105765700340271 \n",
      "     Training Step: 170 Training Loss: 0.6147730946540833 \n",
      "     Training Step: 171 Training Loss: 0.6171333193778992 \n",
      "     Training Step: 172 Training Loss: 0.6106634736061096 \n",
      "     Training Step: 173 Training Loss: 0.6166716814041138 \n",
      "     Training Step: 174 Training Loss: 0.6134124994277954 \n",
      "     Training Step: 175 Training Loss: 0.6132749319076538 \n",
      "     Training Step: 176 Training Loss: 0.6167187690734863 \n",
      "     Training Step: 177 Training Loss: 0.6137421131134033 \n",
      "     Training Step: 178 Training Loss: 0.6144769191741943 \n",
      "     Training Step: 179 Training Loss: 0.6116470098495483 \n",
      "     Training Step: 180 Training Loss: 0.6104965806007385 \n",
      "     Training Step: 181 Training Loss: 0.6115213632583618 \n",
      "     Training Step: 182 Training Loss: 0.6114344596862793 \n",
      "     Training Step: 183 Training Loss: 0.6136487722396851 \n",
      "     Training Step: 184 Training Loss: 0.6126381754875183 \n",
      "     Training Step: 185 Training Loss: 0.6189009547233582 \n",
      "     Training Step: 186 Training Loss: 0.6146448254585266 \n",
      "     Training Step: 187 Training Loss: 0.6128631234169006 \n",
      "     Training Step: 188 Training Loss: 0.6131319999694824 \n",
      "     Training Step: 189 Training Loss: 0.6153806447982788 \n",
      "     Training Step: 190 Training Loss: 0.614341676235199 \n",
      "     Training Step: 191 Training Loss: 0.6166579723358154 \n",
      "     Training Step: 192 Training Loss: 0.6101744771003723 \n",
      "     Training Step: 193 Training Loss: 0.6117867231369019 \n",
      "     Training Step: 194 Training Loss: 0.6177617907524109 \n",
      "     Training Step: 195 Training Loss: 0.6146804094314575 \n",
      "     Training Step: 196 Training Loss: 0.6121249198913574 \n",
      "     Training Step: 197 Training Loss: 0.616028904914856 \n",
      "     Training Step: 198 Training Loss: 0.6176851391792297 \n",
      "     Training Step: 199 Training Loss: 0.6146656274795532 \n",
      "     Training Step: 200 Training Loss: 0.6135132908821106 \n",
      "     Training Step: 201 Training Loss: 0.6158297657966614 \n",
      "     Training Step: 202 Training Loss: 0.6168840527534485 \n",
      "     Training Step: 203 Training Loss: 0.6149541735649109 \n",
      "     Training Step: 204 Training Loss: 0.609736442565918 \n",
      "     Training Step: 205 Training Loss: 0.6114174723625183 \n",
      "     Training Step: 206 Training Loss: 0.6167003512382507 \n",
      "     Training Step: 207 Training Loss: 0.6140323281288147 \n",
      "     Training Step: 208 Training Loss: 0.6164048910140991 \n",
      "     Training Step: 209 Training Loss: 0.6114261746406555 \n",
      "     Training Step: 210 Training Loss: 0.6149184703826904 \n",
      "     Training Step: 211 Training Loss: 0.6202298402786255 \n",
      "     Training Step: 212 Training Loss: 0.6152415871620178 \n",
      "     Training Step: 213 Training Loss: 0.6188291907310486 \n",
      "     Training Step: 214 Training Loss: 0.6121410131454468 \n",
      "     Training Step: 215 Training Loss: 0.6139134168624878 \n",
      "     Training Step: 216 Training Loss: 0.614344596862793 \n",
      "     Training Step: 217 Training Loss: 0.6170885562896729 \n",
      "     Training Step: 218 Training Loss: 0.618449866771698 \n",
      "     Training Step: 219 Training Loss: 0.6157522797584534 \n",
      "     Training Step: 220 Training Loss: 0.6094313859939575 \n",
      "     Training Step: 221 Training Loss: 0.6127133965492249 \n",
      "     Training Step: 222 Training Loss: 0.6138206720352173 \n",
      "     Training Step: 223 Training Loss: 0.6105820536613464 \n",
      "     Training Step: 224 Training Loss: 0.6153325438499451 \n",
      "     Training Step: 225 Training Loss: 0.613592803478241 \n",
      "     Training Step: 226 Training Loss: 0.6128697991371155 \n",
      "     Training Step: 227 Training Loss: 0.6161172986030579 \n",
      "     Training Step: 228 Training Loss: 0.6172008514404297 \n",
      "     Training Step: 229 Training Loss: 0.6177150011062622 \n",
      "     Training Step: 230 Training Loss: 0.6100457310676575 \n",
      "     Training Step: 231 Training Loss: 0.6146083474159241 \n",
      "     Training Step: 232 Training Loss: 0.6198725700378418 \n",
      "     Training Step: 233 Training Loss: 0.6147258877754211 \n",
      "     Training Step: 234 Training Loss: 0.6177863478660583 \n",
      "     Training Step: 235 Training Loss: 0.6171310544013977 \n",
      "     Training Step: 236 Training Loss: 0.6173591613769531 \n",
      "     Training Step: 237 Training Loss: 0.6180159449577332 \n",
      "     Training Step: 238 Training Loss: 0.6111958026885986 \n",
      "     Training Step: 239 Training Loss: 0.6146173477172852 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6180347204208374 \n",
      "     Validation Step: 1 Validation Loss: 0.6076120138168335 \n",
      "     Validation Step: 2 Validation Loss: 0.6136587858200073 \n",
      "     Validation Step: 3 Validation Loss: 0.6116873025894165 \n",
      "     Validation Step: 4 Validation Loss: 0.6183108687400818 \n",
      "     Validation Step: 5 Validation Loss: 0.6153205633163452 \n",
      "     Validation Step: 6 Validation Loss: 0.6148596405982971 \n",
      "     Validation Step: 7 Validation Loss: 0.618458092212677 \n",
      "     Validation Step: 8 Validation Loss: 0.6141597032546997 \n",
      "     Validation Step: 9 Validation Loss: 0.6156134605407715 \n",
      "     Validation Step: 10 Validation Loss: 0.6184478998184204 \n",
      "     Validation Step: 11 Validation Loss: 0.6119333505630493 \n",
      "     Validation Step: 12 Validation Loss: 0.6130277514457703 \n",
      "     Validation Step: 13 Validation Loss: 0.6159742474555969 \n",
      "     Validation Step: 14 Validation Loss: 0.610679566860199 \n",
      "     Validation Step: 15 Validation Loss: 0.6157965660095215 \n",
      "     Validation Step: 16 Validation Loss: 0.612173318862915 \n",
      "     Validation Step: 17 Validation Loss: 0.6149049401283264 \n",
      "     Validation Step: 18 Validation Loss: 0.6142764687538147 \n",
      "     Validation Step: 19 Validation Loss: 0.6176736950874329 \n",
      "     Validation Step: 20 Validation Loss: 0.6181946396827698 \n",
      "     Validation Step: 21 Validation Loss: 0.6141420602798462 \n",
      "     Validation Step: 22 Validation Loss: 0.6112208962440491 \n",
      "     Validation Step: 23 Validation Loss: 0.6133410334587097 \n",
      "     Validation Step: 24 Validation Loss: 0.6105362772941589 \n",
      "     Validation Step: 25 Validation Loss: 0.6145772933959961 \n",
      "     Validation Step: 26 Validation Loss: 0.6101723313331604 \n",
      "     Validation Step: 27 Validation Loss: 0.6101952195167542 \n",
      "     Validation Step: 28 Validation Loss: 0.6152316927909851 \n",
      "     Validation Step: 29 Validation Loss: 0.6175870895385742 \n",
      "     Validation Step: 30 Validation Loss: 0.6136924028396606 \n",
      "     Validation Step: 31 Validation Loss: 0.6116042137145996 \n",
      "     Validation Step: 32 Validation Loss: 0.6102110743522644 \n",
      "     Validation Step: 33 Validation Loss: 0.6162407994270325 \n",
      "     Validation Step: 34 Validation Loss: 0.6172866821289062 \n",
      "     Validation Step: 35 Validation Loss: 0.6169965863227844 \n",
      "     Validation Step: 36 Validation Loss: 0.6136733293533325 \n",
      "     Validation Step: 37 Validation Loss: 0.6105747222900391 \n",
      "     Validation Step: 38 Validation Loss: 0.614225447177887 \n",
      "     Validation Step: 39 Validation Loss: 0.615041196346283 \n",
      "     Validation Step: 40 Validation Loss: 0.6155645847320557 \n",
      "     Validation Step: 41 Validation Loss: 0.6112151145935059 \n",
      "     Validation Step: 42 Validation Loss: 0.6146644353866577 \n",
      "     Validation Step: 43 Validation Loss: 0.6128760576248169 \n",
      "     Validation Step: 44 Validation Loss: 0.6145581603050232 \n",
      "Epoch: 146\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6185404062271118 \n",
      "     Training Step: 1 Training Loss: 0.610111653804779 \n",
      "     Training Step: 2 Training Loss: 0.6161931753158569 \n",
      "     Training Step: 3 Training Loss: 0.612923800945282 \n",
      "     Training Step: 4 Training Loss: 0.6165977120399475 \n",
      "     Training Step: 5 Training Loss: 0.6118640899658203 \n",
      "     Training Step: 6 Training Loss: 0.6125021576881409 \n",
      "     Training Step: 7 Training Loss: 0.614636242389679 \n",
      "     Training Step: 8 Training Loss: 0.6162288784980774 \n",
      "     Training Step: 9 Training Loss: 0.6181038022041321 \n",
      "     Training Step: 10 Training Loss: 0.6168015599250793 \n",
      "     Training Step: 11 Training Loss: 0.6147456169128418 \n",
      "     Training Step: 12 Training Loss: 0.6202164888381958 \n",
      "     Training Step: 13 Training Loss: 0.6133559346199036 \n",
      "     Training Step: 14 Training Loss: 0.6116489171981812 \n",
      "     Training Step: 15 Training Loss: 0.6115606427192688 \n",
      "     Training Step: 16 Training Loss: 0.6117960214614868 \n",
      "     Training Step: 17 Training Loss: 0.6180310249328613 \n",
      "     Training Step: 18 Training Loss: 0.612282931804657 \n",
      "     Training Step: 19 Training Loss: 0.6113942265510559 \n",
      "     Training Step: 20 Training Loss: 0.6141107082366943 \n",
      "     Training Step: 21 Training Loss: 0.6118394732475281 \n",
      "     Training Step: 22 Training Loss: 0.6099735498428345 \n",
      "     Training Step: 23 Training Loss: 0.6126885414123535 \n",
      "     Training Step: 24 Training Loss: 0.6143452525138855 \n",
      "     Training Step: 25 Training Loss: 0.6101585030555725 \n",
      "     Training Step: 26 Training Loss: 0.6158450841903687 \n",
      "     Training Step: 27 Training Loss: 0.6121516227722168 \n",
      "     Training Step: 28 Training Loss: 0.6125233769416809 \n",
      "     Training Step: 29 Training Loss: 0.6155695915222168 \n",
      "     Training Step: 30 Training Loss: 0.6161341071128845 \n",
      "     Training Step: 31 Training Loss: 0.6171219348907471 \n",
      "     Training Step: 32 Training Loss: 0.61444091796875 \n",
      "     Training Step: 33 Training Loss: 0.6153876781463623 \n",
      "     Training Step: 34 Training Loss: 0.6154212355613708 \n",
      "     Training Step: 35 Training Loss: 0.616415798664093 \n",
      "     Training Step: 36 Training Loss: 0.6154776215553284 \n",
      "     Training Step: 37 Training Loss: 0.6141599416732788 \n",
      "     Training Step: 38 Training Loss: 0.6132169365882874 \n",
      "     Training Step: 39 Training Loss: 0.6115531325340271 \n",
      "     Training Step: 40 Training Loss: 0.6149566173553467 \n",
      "     Training Step: 41 Training Loss: 0.6100853681564331 \n",
      "     Training Step: 42 Training Loss: 0.6182219386100769 \n",
      "     Training Step: 43 Training Loss: 0.6184487342834473 \n",
      "     Training Step: 44 Training Loss: 0.6114181876182556 \n",
      "     Training Step: 45 Training Loss: 0.613638699054718 \n",
      "     Training Step: 46 Training Loss: 0.612237274646759 \n",
      "     Training Step: 47 Training Loss: 0.6139186024665833 \n",
      "     Training Step: 48 Training Loss: 0.6151748299598694 \n",
      "     Training Step: 49 Training Loss: 0.6147928237915039 \n",
      "     Training Step: 50 Training Loss: 0.6114310026168823 \n",
      "     Training Step: 51 Training Loss: 0.6115144491195679 \n",
      "     Training Step: 52 Training Loss: 0.6105641722679138 \n",
      "     Training Step: 53 Training Loss: 0.6169622540473938 \n",
      "     Training Step: 54 Training Loss: 0.6126390099525452 \n",
      "     Training Step: 55 Training Loss: 0.6143479347229004 \n",
      "     Training Step: 56 Training Loss: 0.6186287999153137 \n",
      "     Training Step: 57 Training Loss: 0.6167191863059998 \n",
      "     Training Step: 58 Training Loss: 0.6137334704399109 \n",
      "     Training Step: 59 Training Loss: 0.61574786901474 \n",
      "     Training Step: 60 Training Loss: 0.6123918294906616 \n",
      "     Training Step: 61 Training Loss: 0.6120181083679199 \n",
      "     Training Step: 62 Training Loss: 0.6142183542251587 \n",
      "     Training Step: 63 Training Loss: 0.6118072271347046 \n",
      "     Training Step: 64 Training Loss: 0.618062436580658 \n",
      "     Training Step: 65 Training Loss: 0.6082730889320374 \n",
      "     Training Step: 66 Training Loss: 0.6092390418052673 \n",
      "     Training Step: 67 Training Loss: 0.6115298271179199 \n",
      "     Training Step: 68 Training Loss: 0.6116092205047607 \n",
      "     Training Step: 69 Training Loss: 0.6132073998451233 \n",
      "     Training Step: 70 Training Loss: 0.610457181930542 \n",
      "     Training Step: 71 Training Loss: 0.6128695011138916 \n",
      "     Training Step: 72 Training Loss: 0.6128696799278259 \n",
      "     Training Step: 73 Training Loss: 0.6122350692749023 \n",
      "     Training Step: 74 Training Loss: 0.6144436001777649 \n",
      "     Training Step: 75 Training Loss: 0.6147927045822144 \n",
      "     Training Step: 76 Training Loss: 0.6147469878196716 \n",
      "     Training Step: 77 Training Loss: 0.611686646938324 \n",
      "     Training Step: 78 Training Loss: 0.6184533834457397 \n",
      "     Training Step: 79 Training Loss: 0.6156893372535706 \n",
      "     Training Step: 80 Training Loss: 0.6136497259140015 \n",
      "     Training Step: 81 Training Loss: 0.616499662399292 \n",
      "     Training Step: 82 Training Loss: 0.6168074607849121 \n",
      "     Training Step: 83 Training Loss: 0.6133174300193787 \n",
      "     Training Step: 84 Training Loss: 0.6140779852867126 \n",
      "     Training Step: 85 Training Loss: 0.6143712401390076 \n",
      "     Training Step: 86 Training Loss: 0.6104373931884766 \n",
      "     Training Step: 87 Training Loss: 0.609451949596405 \n",
      "     Training Step: 88 Training Loss: 0.6154390573501587 \n",
      "     Training Step: 89 Training Loss: 0.6094889044761658 \n",
      "     Training Step: 90 Training Loss: 0.6148691773414612 \n",
      "     Training Step: 91 Training Loss: 0.6147062182426453 \n",
      "     Training Step: 92 Training Loss: 0.6144511103630066 \n",
      "     Training Step: 93 Training Loss: 0.6101269721984863 \n",
      "     Training Step: 94 Training Loss: 0.6114632487297058 \n",
      "     Training Step: 95 Training Loss: 0.6183559894561768 \n",
      "     Training Step: 96 Training Loss: 0.6178328394889832 \n",
      "     Training Step: 97 Training Loss: 0.6151695251464844 \n",
      "     Training Step: 98 Training Loss: 0.6117739677429199 \n",
      "     Training Step: 99 Training Loss: 0.6138186454772949 \n",
      "     Training Step: 100 Training Loss: 0.6137645840644836 \n",
      "     Training Step: 101 Training Loss: 0.613287627696991 \n",
      "     Training Step: 102 Training Loss: 0.6153415441513062 \n",
      "     Training Step: 103 Training Loss: 0.6124605536460876 \n",
      "     Training Step: 104 Training Loss: 0.6127965450286865 \n",
      "     Training Step: 105 Training Loss: 0.6130567193031311 \n",
      "     Training Step: 106 Training Loss: 0.6142839193344116 \n",
      "     Training Step: 107 Training Loss: 0.6104999780654907 \n",
      "     Training Step: 108 Training Loss: 0.6184405088424683 \n",
      "     Training Step: 109 Training Loss: 0.6154088377952576 \n",
      "     Training Step: 110 Training Loss: 0.6105733513832092 \n",
      "     Training Step: 111 Training Loss: 0.6127526164054871 \n",
      "     Training Step: 112 Training Loss: 0.6106643676757812 \n",
      "     Training Step: 113 Training Loss: 0.6152399182319641 \n",
      "     Training Step: 114 Training Loss: 0.6152626276016235 \n",
      "     Training Step: 115 Training Loss: 0.6103705167770386 \n",
      "     Training Step: 116 Training Loss: 0.6182253360748291 \n",
      "     Training Step: 117 Training Loss: 0.6134564876556396 \n",
      "     Training Step: 118 Training Loss: 0.6157458424568176 \n",
      "     Training Step: 119 Training Loss: 0.6127634048461914 \n",
      "     Training Step: 120 Training Loss: 0.6150683164596558 \n",
      "     Training Step: 121 Training Loss: 0.6121384501457214 \n",
      "     Training Step: 122 Training Loss: 0.61944979429245 \n",
      "     Training Step: 123 Training Loss: 0.6129621267318726 \n",
      "     Training Step: 124 Training Loss: 0.6111570000648499 \n",
      "     Training Step: 125 Training Loss: 0.6146606206893921 \n",
      "     Training Step: 126 Training Loss: 0.6140215992927551 \n",
      "     Training Step: 127 Training Loss: 0.6153008341789246 \n",
      "     Training Step: 128 Training Loss: 0.610723614692688 \n",
      "     Training Step: 129 Training Loss: 0.6152830123901367 \n",
      "     Training Step: 130 Training Loss: 0.6132795214653015 \n",
      "     Training Step: 131 Training Loss: 0.6123790144920349 \n",
      "     Training Step: 132 Training Loss: 0.6176350712776184 \n",
      "     Training Step: 133 Training Loss: 0.6125332117080688 \n",
      "     Training Step: 134 Training Loss: 0.6149287223815918 \n",
      "     Training Step: 135 Training Loss: 0.6137420535087585 \n",
      "     Training Step: 136 Training Loss: 0.6125102639198303 \n",
      "     Training Step: 137 Training Loss: 0.6145989298820496 \n",
      "     Training Step: 138 Training Loss: 0.6168059706687927 \n",
      "     Training Step: 139 Training Loss: 0.6107351779937744 \n",
      "     Training Step: 140 Training Loss: 0.6209340691566467 \n",
      "     Training Step: 141 Training Loss: 0.6171913146972656 \n",
      "     Training Step: 142 Training Loss: 0.6131390929222107 \n",
      "     Training Step: 143 Training Loss: 0.615088164806366 \n",
      "     Training Step: 144 Training Loss: 0.6167114973068237 \n",
      "     Training Step: 145 Training Loss: 0.616642415523529 \n",
      "     Training Step: 146 Training Loss: 0.6146088242530823 \n",
      "     Training Step: 147 Training Loss: 0.6129338145256042 \n",
      "     Training Step: 148 Training Loss: 0.6166877746582031 \n",
      "     Training Step: 149 Training Loss: 0.6114426255226135 \n",
      "     Training Step: 150 Training Loss: 0.6131983399391174 \n",
      "     Training Step: 151 Training Loss: 0.6195869445800781 \n",
      "     Training Step: 152 Training Loss: 0.6146793365478516 \n",
      "     Training Step: 153 Training Loss: 0.6133043766021729 \n",
      "     Training Step: 154 Training Loss: 0.6188567280769348 \n",
      "     Training Step: 155 Training Loss: 0.616644024848938 \n",
      "     Training Step: 156 Training Loss: 0.614607572555542 \n",
      "     Training Step: 157 Training Loss: 0.6140158176422119 \n",
      "     Training Step: 158 Training Loss: 0.617359459400177 \n",
      "     Training Step: 159 Training Loss: 0.6109209060668945 \n",
      "     Training Step: 160 Training Loss: 0.6142013072967529 \n",
      "     Training Step: 161 Training Loss: 0.6159400939941406 \n",
      "     Training Step: 162 Training Loss: 0.6188045144081116 \n",
      "     Training Step: 163 Training Loss: 0.6116020679473877 \n",
      "     Training Step: 164 Training Loss: 0.6155262589454651 \n",
      "     Training Step: 165 Training Loss: 0.6155943274497986 \n",
      "     Training Step: 166 Training Loss: 0.6140394806861877 \n",
      "     Training Step: 167 Training Loss: 0.6135182976722717 \n",
      "     Training Step: 168 Training Loss: 0.6121294498443604 \n",
      "     Training Step: 169 Training Loss: 0.615779459476471 \n",
      "     Training Step: 170 Training Loss: 0.6171348690986633 \n",
      "     Training Step: 171 Training Loss: 0.6141490936279297 \n",
      "     Training Step: 172 Training Loss: 0.6164023876190186 \n",
      "     Training Step: 173 Training Loss: 0.6167525053024292 \n",
      "     Training Step: 174 Training Loss: 0.6134975552558899 \n",
      "     Training Step: 175 Training Loss: 0.611832857131958 \n",
      "     Training Step: 176 Training Loss: 0.6136155724525452 \n",
      "     Training Step: 177 Training Loss: 0.6147001385688782 \n",
      "     Training Step: 178 Training Loss: 0.6144775152206421 \n",
      "     Training Step: 179 Training Loss: 0.6153929233551025 \n",
      "     Training Step: 180 Training Loss: 0.6161962747573853 \n",
      "     Training Step: 181 Training Loss: 0.613120973110199 \n",
      "     Training Step: 182 Training Loss: 0.6134052276611328 \n",
      "     Training Step: 183 Training Loss: 0.6116296648979187 \n",
      "     Training Step: 184 Training Loss: 0.6146821975708008 \n",
      "     Training Step: 185 Training Loss: 0.6176899671554565 \n",
      "     Training Step: 186 Training Loss: 0.6132705211639404 \n",
      "     Training Step: 187 Training Loss: 0.610588550567627 \n",
      "     Training Step: 188 Training Loss: 0.610046923160553 \n",
      "     Training Step: 189 Training Loss: 0.6116215586662292 \n",
      "     Training Step: 190 Training Loss: 0.6146923303604126 \n",
      "     Training Step: 191 Training Loss: 0.6111880540847778 \n",
      "     Training Step: 192 Training Loss: 0.6153842210769653 \n",
      "     Training Step: 193 Training Loss: 0.6149283647537231 \n",
      "     Training Step: 194 Training Loss: 0.6155363321304321 \n",
      "     Training Step: 195 Training Loss: 0.6135941743850708 \n",
      "     Training Step: 196 Training Loss: 0.6134691834449768 \n",
      "     Training Step: 197 Training Loss: 0.6160439848899841 \n",
      "     Training Step: 198 Training Loss: 0.6157602667808533 \n",
      "     Training Step: 199 Training Loss: 0.6139109134674072 \n",
      "     Training Step: 200 Training Loss: 0.6123750805854797 \n",
      "     Training Step: 201 Training Loss: 0.612153172492981 \n",
      "     Training Step: 202 Training Loss: 0.6171566843986511 \n",
      "     Training Step: 203 Training Loss: 0.6097280383110046 \n",
      "     Training Step: 204 Training Loss: 0.6152852177619934 \n",
      "     Training Step: 205 Training Loss: 0.6178037524223328 \n",
      "     Training Step: 206 Training Loss: 0.6111503839492798 \n",
      "     Training Step: 207 Training Loss: 0.61284339427948 \n",
      "     Training Step: 208 Training Loss: 0.6142433881759644 \n",
      "     Training Step: 209 Training Loss: 0.61069256067276 \n",
      "     Training Step: 210 Training Loss: 0.6120677590370178 \n",
      "     Training Step: 211 Training Loss: 0.6163507103919983 \n",
      "     Training Step: 212 Training Loss: 0.6122903227806091 \n",
      "     Training Step: 213 Training Loss: 0.611874520778656 \n",
      "     Training Step: 214 Training Loss: 0.6166858077049255 \n",
      "     Training Step: 215 Training Loss: 0.6146523952484131 \n",
      "     Training Step: 216 Training Loss: 0.6143496036529541 \n",
      "     Training Step: 217 Training Loss: 0.6177042126655579 \n",
      "     Training Step: 218 Training Loss: 0.611423134803772 \n",
      "     Training Step: 219 Training Loss: 0.6169010996818542 \n",
      "     Training Step: 220 Training Loss: 0.6138855814933777 \n",
      "     Training Step: 221 Training Loss: 0.6167182922363281 \n",
      "     Training Step: 222 Training Loss: 0.6144963502883911 \n",
      "     Training Step: 223 Training Loss: 0.6177515387535095 \n",
      "     Training Step: 224 Training Loss: 0.6159932017326355 \n",
      "     Training Step: 225 Training Loss: 0.6097526550292969 \n",
      "     Training Step: 226 Training Loss: 0.6109054684638977 \n",
      "     Training Step: 227 Training Loss: 0.6122474670410156 \n",
      "     Training Step: 228 Training Loss: 0.6097371578216553 \n",
      "     Training Step: 229 Training Loss: 0.6156738996505737 \n",
      "     Training Step: 230 Training Loss: 0.6174826622009277 \n",
      "     Training Step: 231 Training Loss: 0.6166493892669678 \n",
      "     Training Step: 232 Training Loss: 0.6130654215812683 \n",
      "     Training Step: 233 Training Loss: 0.6198568940162659 \n",
      "     Training Step: 234 Training Loss: 0.6180127263069153 \n",
      "     Training Step: 235 Training Loss: 0.6176843643188477 \n",
      "     Training Step: 236 Training Loss: 0.6150915026664734 \n",
      "     Training Step: 237 Training Loss: 0.6196674108505249 \n",
      "     Training Step: 238 Training Loss: 0.6171290874481201 \n",
      "     Training Step: 239 Training Loss: 0.6106225252151489 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6136520504951477 \n",
      "     Validation Step: 1 Validation Loss: 0.6180349588394165 \n",
      "     Validation Step: 2 Validation Loss: 0.6112186908721924 \n",
      "     Validation Step: 3 Validation Loss: 0.6116053462028503 \n",
      "     Validation Step: 4 Validation Loss: 0.6159673929214478 \n",
      "     Validation Step: 5 Validation Loss: 0.6146644949913025 \n",
      "     Validation Step: 6 Validation Loss: 0.6106820106506348 \n",
      "     Validation Step: 7 Validation Loss: 0.611690104007721 \n",
      "     Validation Step: 8 Validation Loss: 0.6121748089790344 \n",
      "     Validation Step: 9 Validation Loss: 0.6181911826133728 \n",
      "     Validation Step: 10 Validation Loss: 0.6145598888397217 \n",
      "     Validation Step: 11 Validation Loss: 0.614155113697052 \n",
      "     Validation Step: 12 Validation Loss: 0.6145811676979065 \n",
      "     Validation Step: 13 Validation Loss: 0.6142807602882385 \n",
      "     Validation Step: 14 Validation Loss: 0.6162423491477966 \n",
      "     Validation Step: 15 Validation Loss: 0.6119304895401001 \n",
      "     Validation Step: 16 Validation Loss: 0.6130253672599792 \n",
      "     Validation Step: 17 Validation Loss: 0.6101715564727783 \n",
      "     Validation Step: 18 Validation Loss: 0.6149061918258667 \n",
      "     Validation Step: 19 Validation Loss: 0.6142286062240601 \n",
      "     Validation Step: 20 Validation Loss: 0.6076118350028992 \n",
      "     Validation Step: 21 Validation Loss: 0.6136894822120667 \n",
      "     Validation Step: 22 Validation Loss: 0.6150354146957397 \n",
      "     Validation Step: 23 Validation Loss: 0.6133453845977783 \n",
      "     Validation Step: 24 Validation Loss: 0.6152323484420776 \n",
      "     Validation Step: 25 Validation Loss: 0.6184582710266113 \n",
      "     Validation Step: 26 Validation Loss: 0.6183062195777893 \n",
      "     Validation Step: 27 Validation Loss: 0.6155647039413452 \n",
      "     Validation Step: 28 Validation Loss: 0.6169965863227844 \n",
      "     Validation Step: 29 Validation Loss: 0.6101993322372437 \n",
      "     Validation Step: 30 Validation Loss: 0.613676905632019 \n",
      "     Validation Step: 31 Validation Loss: 0.6148589849472046 \n",
      "     Validation Step: 32 Validation Loss: 0.6102157831192017 \n",
      "     Validation Step: 33 Validation Loss: 0.6176684498786926 \n",
      "     Validation Step: 34 Validation Loss: 0.6105746626853943 \n",
      "     Validation Step: 35 Validation Loss: 0.6175899505615234 \n",
      "     Validation Step: 36 Validation Loss: 0.6112117171287537 \n",
      "     Validation Step: 37 Validation Loss: 0.6172891855239868 \n",
      "     Validation Step: 38 Validation Loss: 0.6184514760971069 \n",
      "     Validation Step: 39 Validation Loss: 0.6157894134521484 \n",
      "     Validation Step: 40 Validation Loss: 0.6141406297683716 \n",
      "     Validation Step: 41 Validation Loss: 0.6153168678283691 \n",
      "     Validation Step: 42 Validation Loss: 0.6105349659919739 \n",
      "     Validation Step: 43 Validation Loss: 0.6128772497177124 \n",
      "     Validation Step: 44 Validation Loss: 0.6156124472618103 \n",
      "Epoch: 147\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6157639622688293 \n",
      "     Training Step: 1 Training Loss: 0.6155908703804016 \n",
      "     Training Step: 2 Training Loss: 0.6147594451904297 \n",
      "     Training Step: 3 Training Loss: 0.6152885556221008 \n",
      "     Training Step: 4 Training Loss: 0.6107337474822998 \n",
      "     Training Step: 5 Training Loss: 0.6144856214523315 \n",
      "     Training Step: 6 Training Loss: 0.6167125701904297 \n",
      "     Training Step: 7 Training Loss: 0.6149371862411499 \n",
      "     Training Step: 8 Training Loss: 0.6102010607719421 \n",
      "     Training Step: 9 Training Loss: 0.6133180856704712 \n",
      "     Training Step: 10 Training Loss: 0.6152858138084412 \n",
      "     Training Step: 11 Training Loss: 0.6103960275650024 \n",
      "     Training Step: 12 Training Loss: 0.6152706146240234 \n",
      "     Training Step: 13 Training Loss: 0.6162229180335999 \n",
      "     Training Step: 14 Training Loss: 0.608227014541626 \n",
      "     Training Step: 15 Training Loss: 0.6120633482933044 \n",
      "     Training Step: 16 Training Loss: 0.6150808334350586 \n",
      "     Training Step: 17 Training Loss: 0.6106986403465271 \n",
      "     Training Step: 18 Training Loss: 0.6107262372970581 \n",
      "     Training Step: 19 Training Loss: 0.6138252019882202 \n",
      "     Training Step: 20 Training Loss: 0.616273045539856 \n",
      "     Training Step: 21 Training Loss: 0.6181471347808838 \n",
      "     Training Step: 22 Training Loss: 0.616788923740387 \n",
      "     Training Step: 23 Training Loss: 0.6133527159690857 \n",
      "     Training Step: 24 Training Loss: 0.6166552901268005 \n",
      "     Training Step: 25 Training Loss: 0.6176919341087341 \n",
      "     Training Step: 26 Training Loss: 0.6146684288978577 \n",
      "     Training Step: 27 Training Loss: 0.615666925907135 \n",
      "     Training Step: 28 Training Loss: 0.6142628788948059 \n",
      "     Training Step: 29 Training Loss: 0.6142918467521667 \n",
      "     Training Step: 30 Training Loss: 0.6154861450195312 \n",
      "     Training Step: 31 Training Loss: 0.6115343570709229 \n",
      "     Training Step: 32 Training Loss: 0.6127896904945374 \n",
      "     Training Step: 33 Training Loss: 0.6181952953338623 \n",
      "     Training Step: 34 Training Loss: 0.6125391721725464 \n",
      "     Training Step: 35 Training Loss: 0.6146734952926636 \n",
      "     Training Step: 36 Training Loss: 0.6114478707313538 \n",
      "     Training Step: 37 Training Loss: 0.6094701290130615 \n",
      "     Training Step: 38 Training Loss: 0.6123836636543274 \n",
      "     Training Step: 39 Training Loss: 0.6124879121780396 \n",
      "     Training Step: 40 Training Loss: 0.6115415096282959 \n",
      "     Training Step: 41 Training Loss: 0.6100329756736755 \n",
      "     Training Step: 42 Training Loss: 0.614179253578186 \n",
      "     Training Step: 43 Training Loss: 0.6132729649543762 \n",
      "     Training Step: 44 Training Loss: 0.6172400116920471 \n",
      "     Training Step: 45 Training Loss: 0.6108707189559937 \n",
      "     Training Step: 46 Training Loss: 0.6177604794502258 \n",
      "     Training Step: 47 Training Loss: 0.6181358098983765 \n",
      "     Training Step: 48 Training Loss: 0.6152520179748535 \n",
      "     Training Step: 49 Training Loss: 0.609961986541748 \n",
      "     Training Step: 50 Training Loss: 0.6127997040748596 \n",
      "     Training Step: 51 Training Loss: 0.6096938848495483 \n",
      "     Training Step: 52 Training Loss: 0.6167111396789551 \n",
      "     Training Step: 53 Training Loss: 0.6133055686950684 \n",
      "     Training Step: 54 Training Loss: 0.6120117902755737 \n",
      "     Training Step: 55 Training Loss: 0.6164906620979309 \n",
      "     Training Step: 56 Training Loss: 0.6131447553634644 \n",
      "     Training Step: 57 Training Loss: 0.6149177551269531 \n",
      "     Training Step: 58 Training Loss: 0.611167311668396 \n",
      "     Training Step: 59 Training Loss: 0.6150955557823181 \n",
      "     Training Step: 60 Training Loss: 0.6147727370262146 \n",
      "     Training Step: 61 Training Loss: 0.6117866635322571 \n",
      "     Training Step: 62 Training Loss: 0.6153756380081177 \n",
      "     Training Step: 63 Training Loss: 0.6144235134124756 \n",
      "     Training Step: 64 Training Loss: 0.6129083633422852 \n",
      "     Training Step: 65 Training Loss: 0.6106566786766052 \n",
      "     Training Step: 66 Training Loss: 0.6147043108940125 \n",
      "     Training Step: 67 Training Loss: 0.6160413026809692 \n",
      "     Training Step: 68 Training Loss: 0.6144391298294067 \n",
      "     Training Step: 69 Training Loss: 0.618607223033905 \n",
      "     Training Step: 70 Training Loss: 0.613467276096344 \n",
      "     Training Step: 71 Training Loss: 0.6113924384117126 \n",
      "     Training Step: 72 Training Loss: 0.6142069101333618 \n",
      "     Training Step: 73 Training Loss: 0.6128389239311218 \n",
      "     Training Step: 74 Training Loss: 0.6146351099014282 \n",
      "     Training Step: 75 Training Loss: 0.6132055521011353 \n",
      "     Training Step: 76 Training Loss: 0.6118375062942505 \n",
      "     Training Step: 77 Training Loss: 0.6150835156440735 \n",
      "     Training Step: 78 Training Loss: 0.6143321394920349 \n",
      "     Training Step: 79 Training Loss: 0.6111432909965515 \n",
      "     Training Step: 80 Training Loss: 0.6170938014984131 \n",
      "     Training Step: 81 Training Loss: 0.6159996390342712 \n",
      "     Training Step: 82 Training Loss: 0.6161981225013733 \n",
      "     Training Step: 83 Training Loss: 0.6136170625686646 \n",
      "     Training Step: 84 Training Loss: 0.610501766204834 \n",
      "     Training Step: 85 Training Loss: 0.6169219017028809 \n",
      "     Training Step: 86 Training Loss: 0.6138837933540344 \n",
      "     Training Step: 87 Training Loss: 0.6131222248077393 \n",
      "     Training Step: 88 Training Loss: 0.610880434513092 \n",
      "     Training Step: 89 Training Loss: 0.617375910282135 \n",
      "     Training Step: 90 Training Loss: 0.6147235035896301 \n",
      "     Training Step: 91 Training Loss: 0.6159446835517883 \n",
      "     Training Step: 92 Training Loss: 0.6115249991416931 \n",
      "     Training Step: 93 Training Loss: 0.6116284728050232 \n",
      "     Training Step: 94 Training Loss: 0.6123731732368469 \n",
      "     Training Step: 95 Training Loss: 0.6101303696632385 \n",
      "     Training Step: 96 Training Loss: 0.6137353181838989 \n",
      "     Training Step: 97 Training Loss: 0.6144517660140991 \n",
      "     Training Step: 98 Training Loss: 0.6100316047668457 \n",
      "     Training Step: 99 Training Loss: 0.612687885761261 \n",
      "     Training Step: 100 Training Loss: 0.6178324222564697 \n",
      "     Training Step: 101 Training Loss: 0.609691858291626 \n",
      "     Training Step: 102 Training Loss: 0.6105539202690125 \n",
      "     Training Step: 103 Training Loss: 0.6121185421943665 \n",
      "     Training Step: 104 Training Loss: 0.618514358997345 \n",
      "     Training Step: 105 Training Loss: 0.6140861511230469 \n",
      "     Training Step: 106 Training Loss: 0.6130698323249817 \n",
      "     Training Step: 107 Training Loss: 0.6148761510848999 \n",
      "     Training Step: 108 Training Loss: 0.6153448820114136 \n",
      "     Training Step: 109 Training Loss: 0.6142101287841797 \n",
      "     Training Step: 110 Training Loss: 0.6166545152664185 \n",
      "     Training Step: 111 Training Loss: 0.6156813502311707 \n",
      "     Training Step: 112 Training Loss: 0.6146070957183838 \n",
      "     Training Step: 113 Training Loss: 0.6143388748168945 \n",
      "     Training Step: 114 Training Loss: 0.6151561737060547 \n",
      "     Training Step: 115 Training Loss: 0.6167938113212585 \n",
      "     Training Step: 116 Training Loss: 0.6201890110969543 \n",
      "     Training Step: 117 Training Loss: 0.6179967522621155 \n",
      "     Training Step: 118 Training Loss: 0.6132261753082275 \n",
      "     Training Step: 119 Training Loss: 0.6127714514732361 \n",
      "     Training Step: 120 Training Loss: 0.6184107661247253 \n",
      "     Training Step: 121 Training Loss: 0.6167938113212585 \n",
      "     Training Step: 122 Training Loss: 0.6134375333786011 \n",
      "     Training Step: 123 Training Loss: 0.6141232252120972 \n",
      "     Training Step: 124 Training Loss: 0.6129927635192871 \n",
      "     Training Step: 125 Training Loss: 0.6157392859458923 \n",
      "     Training Step: 126 Training Loss: 0.6157780289649963 \n",
      "     Training Step: 127 Training Loss: 0.6104940176010132 \n",
      "     Training Step: 128 Training Loss: 0.6115272045135498 \n",
      "     Training Step: 129 Training Loss: 0.6188321709632874 \n",
      "     Training Step: 130 Training Loss: 0.6122296452522278 \n",
      "     Training Step: 131 Training Loss: 0.6146513819694519 \n",
      "     Training Step: 132 Training Loss: 0.6092105507850647 \n",
      "     Training Step: 133 Training Loss: 0.6132899522781372 \n",
      "     Training Step: 134 Training Loss: 0.6125283241271973 \n",
      "     Training Step: 135 Training Loss: 0.6182708144187927 \n",
      "     Training Step: 136 Training Loss: 0.6199095845222473 \n",
      "     Training Step: 137 Training Loss: 0.6118221282958984 \n",
      "     Training Step: 138 Training Loss: 0.6137439608573914 \n",
      "     Training Step: 139 Training Loss: 0.6093819737434387 \n",
      "     Training Step: 140 Training Loss: 0.612228274345398 \n",
      "     Training Step: 141 Training Loss: 0.6177690625190735 \n",
      "     Training Step: 142 Training Loss: 0.6166145205497742 \n",
      "     Training Step: 143 Training Loss: 0.6135085225105286 \n",
      "     Training Step: 144 Training Loss: 0.6183103919029236 \n",
      "     Training Step: 145 Training Loss: 0.6176905632019043 \n",
      "     Training Step: 146 Training Loss: 0.6171258091926575 \n",
      "     Training Step: 147 Training Loss: 0.6196631789207458 \n",
      "     Training Step: 148 Training Loss: 0.6147004961967468 \n",
      "     Training Step: 149 Training Loss: 0.6141819953918457 \n",
      "     Training Step: 150 Training Loss: 0.6129012107849121 \n",
      "     Training Step: 151 Training Loss: 0.6132246851921082 \n",
      "     Training Step: 152 Training Loss: 0.6122828125953674 \n",
      "     Training Step: 153 Training Loss: 0.610639214515686 \n",
      "     Training Step: 154 Training Loss: 0.6154386401176453 \n",
      "     Training Step: 155 Training Loss: 0.6135037541389465 \n",
      "     Training Step: 156 Training Loss: 0.6152867078781128 \n",
      "     Training Step: 157 Training Loss: 0.6146858334541321 \n",
      "     Training Step: 158 Training Loss: 0.6194637417793274 \n",
      "     Training Step: 159 Training Loss: 0.6121577024459839 \n",
      "     Training Step: 160 Training Loss: 0.6116390824317932 \n",
      "     Training Step: 161 Training Loss: 0.6163594722747803 \n",
      "     Training Step: 162 Training Loss: 0.6116881370544434 \n",
      "     Training Step: 163 Training Loss: 0.6161274909973145 \n",
      "     Training Step: 164 Training Loss: 0.618053674697876 \n",
      "     Training Step: 165 Training Loss: 0.6118037700653076 \n",
      "     Training Step: 166 Training Loss: 0.6171474456787109 \n",
      "     Training Step: 167 Training Loss: 0.614956259727478 \n",
      "     Training Step: 168 Training Loss: 0.6154053211212158 \n",
      "     Training Step: 169 Training Loss: 0.6164002418518066 \n",
      "     Training Step: 170 Training Loss: 0.6114560961723328 \n",
      "     Training Step: 171 Training Loss: 0.6164057850837708 \n",
      "     Training Step: 172 Training Loss: 0.6140264272689819 \n",
      "     Training Step: 173 Training Loss: 0.6136565208435059 \n",
      "     Training Step: 174 Training Loss: 0.6167958974838257 \n",
      "     Training Step: 175 Training Loss: 0.6168825626373291 \n",
      "     Training Step: 176 Training Loss: 0.6143471598625183 \n",
      "     Training Step: 177 Training Loss: 0.6158256530761719 \n",
      "     Training Step: 178 Training Loss: 0.6118988394737244 \n",
      "     Training Step: 179 Training Loss: 0.6174735426902771 \n",
      "     Training Step: 180 Training Loss: 0.6097447276115417 \n",
      "     Training Step: 181 Training Loss: 0.6183909177780151 \n",
      "     Training Step: 182 Training Loss: 0.6155303716659546 \n",
      "     Training Step: 183 Training Loss: 0.6143680810928345 \n",
      "     Training Step: 184 Training Loss: 0.611413300037384 \n",
      "     Training Step: 185 Training Loss: 0.6128734946250916 \n",
      "     Training Step: 186 Training Loss: 0.6123849153518677 \n",
      "     Training Step: 187 Training Loss: 0.610373854637146 \n",
      "     Training Step: 188 Training Loss: 0.615401029586792 \n",
      "     Training Step: 189 Training Loss: 0.6185925602912903 \n",
      "     Training Step: 190 Training Loss: 0.6136362552642822 \n",
      "     Training Step: 191 Training Loss: 0.6115769147872925 \n",
      "     Training Step: 192 Training Loss: 0.6146023273468018 \n",
      "     Training Step: 193 Training Loss: 0.6166794300079346 \n",
      "     Training Step: 194 Training Loss: 0.6188822388648987 \n",
      "     Training Step: 195 Training Loss: 0.6147914528846741 \n",
      "     Training Step: 196 Training Loss: 0.6129195094108582 \n",
      "     Training Step: 197 Training Loss: 0.6121464967727661 \n",
      "     Training Step: 198 Training Loss: 0.6112025380134583 \n",
      "     Training Step: 199 Training Loss: 0.610047459602356 \n",
      "     Training Step: 200 Training Loss: 0.6154196262359619 \n",
      "     Training Step: 201 Training Loss: 0.6140176057815552 \n",
      "     Training Step: 202 Training Loss: 0.6155354976654053 \n",
      "     Training Step: 203 Training Loss: 0.6116043925285339 \n",
      "     Training Step: 204 Training Loss: 0.6122915148735046 \n",
      "     Training Step: 205 Training Loss: 0.6167235970497131 \n",
      "     Training Step: 206 Training Loss: 0.611777663230896 \n",
      "     Training Step: 207 Training Loss: 0.6132718324661255 \n",
      "     Training Step: 208 Training Loss: 0.6116189956665039 \n",
      "     Training Step: 209 Training Loss: 0.6140310168266296 \n",
      "     Training Step: 210 Training Loss: 0.6145012974739075 \n",
      "     Training Step: 211 Training Loss: 0.6124619245529175 \n",
      "     Training Step: 212 Training Loss: 0.6147103309631348 \n",
      "     Training Step: 213 Training Loss: 0.6157516241073608 \n",
      "     Training Step: 214 Training Loss: 0.6176541447639465 \n",
      "     Training Step: 215 Training Loss: 0.6130563616752625 \n",
      "     Training Step: 216 Training Loss: 0.610569179058075 \n",
      "     Training Step: 217 Training Loss: 0.6122845411300659 \n",
      "     Training Step: 218 Training Loss: 0.6137626767158508 \n",
      "     Training Step: 219 Training Loss: 0.611528754234314 \n",
      "     Training Step: 220 Training Loss: 0.6153756976127625 \n",
      "     Training Step: 221 Training Loss: 0.6209280490875244 \n",
      "     Training Step: 222 Training Loss: 0.615173876285553 \n",
      "     Training Step: 223 Training Loss: 0.6114227175712585 \n",
      "     Training Step: 224 Training Loss: 0.6139276027679443 \n",
      "     Training Step: 225 Training Loss: 0.6105809211730957 \n",
      "     Training Step: 226 Training Loss: 0.6139158010482788 \n",
      "     Training Step: 227 Training Loss: 0.6146057844161987 \n",
      "     Training Step: 228 Training Loss: 0.6166964173316956 \n",
      "     Training Step: 229 Training Loss: 0.612160861492157 \n",
      "     Training Step: 230 Training Loss: 0.6178085803985596 \n",
      "     Training Step: 231 Training Loss: 0.6118270754814148 \n",
      "     Training Step: 232 Training Loss: 0.6126402020454407 \n",
      "     Training Step: 233 Training Loss: 0.6125312447547913 \n",
      "     Training Step: 234 Training Loss: 0.6134552955627441 \n",
      "     Training Step: 235 Training Loss: 0.6155204176902771 \n",
      "     Training Step: 236 Training Loss: 0.617197573184967 \n",
      "     Training Step: 237 Training Loss: 0.6166658401489258 \n",
      "     Training Step: 238 Training Loss: 0.6135873794555664 \n",
      "     Training Step: 239 Training Loss: 0.6196139454841614 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.610162079334259 \n",
      "     Validation Step: 1 Validation Loss: 0.6115594506263733 \n",
      "     Validation Step: 2 Validation Loss: 0.6118847727775574 \n",
      "     Validation Step: 3 Validation Loss: 0.6176806092262268 \n",
      "     Validation Step: 4 Validation Loss: 0.6104851961135864 \n",
      "     Validation Step: 5 Validation Loss: 0.6116458773612976 \n",
      "     Validation Step: 6 Validation Loss: 0.6136409640312195 \n",
      "     Validation Step: 7 Validation Loss: 0.6146400570869446 \n",
      "     Validation Step: 8 Validation Loss: 0.6133114099502563 \n",
      "     Validation Step: 9 Validation Loss: 0.6150285005569458 \n",
      "     Validation Step: 10 Validation Loss: 0.6105202436447144 \n",
      "     Validation Step: 11 Validation Loss: 0.6156083941459656 \n",
      "     Validation Step: 12 Validation Loss: 0.6106336712837219 \n",
      "     Validation Step: 13 Validation Loss: 0.618466854095459 \n",
      "     Validation Step: 14 Validation Loss: 0.6157816648483276 \n",
      "     Validation Step: 15 Validation Loss: 0.6182112693786621 \n",
      "     Validation Step: 16 Validation Loss: 0.614116370677948 \n",
      "     Validation Step: 17 Validation Loss: 0.6184829473495483 \n",
      "     Validation Step: 18 Validation Loss: 0.6170141696929932 \n",
      "     Validation Step: 19 Validation Loss: 0.6145384907722473 \n",
      "     Validation Step: 20 Validation Loss: 0.6173021793365479 \n",
      "     Validation Step: 21 Validation Loss: 0.6159748435020447 \n",
      "     Validation Step: 22 Validation Loss: 0.6148949265480042 \n",
      "     Validation Step: 23 Validation Loss: 0.6183266043663025 \n",
      "     Validation Step: 24 Validation Loss: 0.6136633157730103 \n",
      "     Validation Step: 25 Validation Loss: 0.6075252294540405 \n",
      "     Validation Step: 26 Validation Loss: 0.6101105213165283 \n",
      "     Validation Step: 27 Validation Loss: 0.6142053008079529 \n",
      "     Validation Step: 28 Validation Loss: 0.6129866242408752 \n",
      "     Validation Step: 29 Validation Loss: 0.6175947785377502 \n",
      "     Validation Step: 30 Validation Loss: 0.6142582297325134 \n",
      "     Validation Step: 31 Validation Loss: 0.6141277551651001 \n",
      "     Validation Step: 32 Validation Loss: 0.6153132319450378 \n",
      "     Validation Step: 33 Validation Loss: 0.611179769039154 \n",
      "     Validation Step: 34 Validation Loss: 0.6111666560173035 \n",
      "     Validation Step: 35 Validation Loss: 0.6145713329315186 \n",
      "     Validation Step: 36 Validation Loss: 0.6136437058448792 \n",
      "     Validation Step: 37 Validation Loss: 0.614837110042572 \n",
      "     Validation Step: 38 Validation Loss: 0.615565836429596 \n",
      "     Validation Step: 39 Validation Loss: 0.6152255535125732 \n",
      "     Validation Step: 40 Validation Loss: 0.6101372241973877 \n",
      "     Validation Step: 41 Validation Loss: 0.6180521249771118 \n",
      "     Validation Step: 42 Validation Loss: 0.616237998008728 \n",
      "     Validation Step: 43 Validation Loss: 0.6121324896812439 \n",
      "     Validation Step: 44 Validation Loss: 0.6128346920013428 \n",
      "Epoch: 148\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6144992709159851 \n",
      "     Training Step: 1 Training Loss: 0.6124660968780518 \n",
      "     Training Step: 2 Training Loss: 0.6180903315544128 \n",
      "     Training Step: 3 Training Loss: 0.6181999444961548 \n",
      "     Training Step: 4 Training Loss: 0.6157816052436829 \n",
      "     Training Step: 5 Training Loss: 0.6167364120483398 \n",
      "     Training Step: 6 Training Loss: 0.6153450608253479 \n",
      "     Training Step: 7 Training Loss: 0.6116575002670288 \n",
      "     Training Step: 8 Training Loss: 0.6131579279899597 \n",
      "     Training Step: 9 Training Loss: 0.610233724117279 \n",
      "     Training Step: 10 Training Loss: 0.614777684211731 \n",
      "     Training Step: 11 Training Loss: 0.6180506944656372 \n",
      "     Training Step: 12 Training Loss: 0.6146830916404724 \n",
      "     Training Step: 13 Training Loss: 0.615265429019928 \n",
      "     Training Step: 14 Training Loss: 0.6136370301246643 \n",
      "     Training Step: 15 Training Loss: 0.6138876080513 \n",
      "     Training Step: 16 Training Loss: 0.6163479089736938 \n",
      "     Training Step: 17 Training Loss: 0.6127971410751343 \n",
      "     Training Step: 18 Training Loss: 0.6142459511756897 \n",
      "     Training Step: 19 Training Loss: 0.6171664595603943 \n",
      "     Training Step: 20 Training Loss: 0.6146059036254883 \n",
      "     Training Step: 21 Training Loss: 0.6121376752853394 \n",
      "     Training Step: 22 Training Loss: 0.6161211133003235 \n",
      "     Training Step: 23 Training Loss: 0.6128642559051514 \n",
      "     Training Step: 24 Training Loss: 0.6184462904930115 \n",
      "     Training Step: 25 Training Loss: 0.6106642484664917 \n",
      "     Training Step: 26 Training Loss: 0.6166510581970215 \n",
      "     Training Step: 27 Training Loss: 0.6142169833183289 \n",
      "     Training Step: 28 Training Loss: 0.613499641418457 \n",
      "     Training Step: 29 Training Loss: 0.616485595703125 \n",
      "     Training Step: 30 Training Loss: 0.6177965998649597 \n",
      "     Training Step: 31 Training Loss: 0.614456295967102 \n",
      "     Training Step: 32 Training Loss: 0.6161906719207764 \n",
      "     Training Step: 33 Training Loss: 0.6164054274559021 \n",
      "     Training Step: 34 Training Loss: 0.6123037934303284 \n",
      "     Training Step: 35 Training Loss: 0.6154374480247498 \n",
      "     Training Step: 36 Training Loss: 0.6145997047424316 \n",
      "     Training Step: 37 Training Loss: 0.6115464568138123 \n",
      "     Training Step: 38 Training Loss: 0.6166936159133911 \n",
      "     Training Step: 39 Training Loss: 0.6182976961135864 \n",
      "     Training Step: 40 Training Loss: 0.6124973297119141 \n",
      "     Training Step: 41 Training Loss: 0.6125208735466003 \n",
      "     Training Step: 42 Training Loss: 0.6141536831855774 \n",
      "     Training Step: 43 Training Loss: 0.6094066500663757 \n",
      "     Training Step: 44 Training Loss: 0.6103739142417908 \n",
      "     Training Step: 45 Training Loss: 0.6100279092788696 \n",
      "     Training Step: 46 Training Loss: 0.6100339293479919 \n",
      "     Training Step: 47 Training Loss: 0.6196761131286621 \n",
      "     Training Step: 48 Training Loss: 0.6108531951904297 \n",
      "     Training Step: 49 Training Loss: 0.6171979308128357 \n",
      "     Training Step: 50 Training Loss: 0.6138267517089844 \n",
      "     Training Step: 51 Training Loss: 0.6189311742782593 \n",
      "     Training Step: 52 Training Loss: 0.6131335496902466 \n",
      "     Training Step: 53 Training Loss: 0.614879310131073 \n",
      "     Training Step: 54 Training Loss: 0.6121559739112854 \n",
      "     Training Step: 55 Training Loss: 0.6173873543739319 \n",
      "     Training Step: 56 Training Loss: 0.6141057014465332 \n",
      "     Training Step: 57 Training Loss: 0.6182256937026978 \n",
      "     Training Step: 58 Training Loss: 0.6209059357643127 \n",
      "     Training Step: 59 Training Loss: 0.6136338114738464 \n",
      "     Training Step: 60 Training Loss: 0.6146699786186218 \n",
      "     Training Step: 61 Training Loss: 0.6115632653236389 \n",
      "     Training Step: 62 Training Loss: 0.6153785586357117 \n",
      "     Training Step: 63 Training Loss: 0.6188055872917175 \n",
      "     Training Step: 64 Training Loss: 0.6149243116378784 \n",
      "     Training Step: 65 Training Loss: 0.6155924201011658 \n",
      "     Training Step: 66 Training Loss: 0.6133145093917847 \n",
      "     Training Step: 67 Training Loss: 0.6114492416381836 \n",
      "     Training Step: 68 Training Loss: 0.6107125282287598 \n",
      "     Training Step: 69 Training Loss: 0.6130697727203369 \n",
      "     Training Step: 70 Training Loss: 0.6115304827690125 \n",
      "     Training Step: 71 Training Loss: 0.6168120503425598 \n",
      "     Training Step: 72 Training Loss: 0.6105798482894897 \n",
      "     Training Step: 73 Training Loss: 0.6105639934539795 \n",
      "     Training Step: 74 Training Loss: 0.6146199107170105 \n",
      "     Training Step: 75 Training Loss: 0.6115449070930481 \n",
      "     Training Step: 76 Training Loss: 0.6139387488365173 \n",
      "     Training Step: 77 Training Loss: 0.6181046366691589 \n",
      "     Training Step: 78 Training Loss: 0.6164353489875793 \n",
      "     Training Step: 79 Training Loss: 0.6147462725639343 \n",
      "     Training Step: 80 Training Loss: 0.6162617802619934 \n",
      "     Training Step: 81 Training Loss: 0.6137461066246033 \n",
      "     Training Step: 82 Training Loss: 0.6147053241729736 \n",
      "     Training Step: 83 Training Loss: 0.611405074596405 \n",
      "     Training Step: 84 Training Loss: 0.6111627221107483 \n",
      "     Training Step: 85 Training Loss: 0.6153678297996521 \n",
      "     Training Step: 86 Training Loss: 0.6196825504302979 \n",
      "     Training Step: 87 Training Loss: 0.6120235919952393 \n",
      "     Training Step: 88 Training Loss: 0.6201955676078796 \n",
      "     Training Step: 89 Training Loss: 0.6132037043571472 \n",
      "     Training Step: 90 Training Loss: 0.6150946617126465 \n",
      "     Training Step: 91 Training Loss: 0.6146761775016785 \n",
      "     Training Step: 92 Training Loss: 0.6167963147163391 \n",
      "     Training Step: 93 Training Loss: 0.6136016249656677 \n",
      "     Training Step: 94 Training Loss: 0.6143461465835571 \n",
      "     Training Step: 95 Training Loss: 0.6166400909423828 \n",
      "     Training Step: 96 Training Loss: 0.6167062520980835 \n",
      "     Training Step: 97 Training Loss: 0.6129140853881836 \n",
      "     Training Step: 98 Training Loss: 0.6118055582046509 \n",
      "     Training Step: 99 Training Loss: 0.6167932152748108 \n",
      "     Training Step: 100 Training Loss: 0.6154178977012634 \n",
      "     Training Step: 101 Training Loss: 0.6151713728904724 \n",
      "     Training Step: 102 Training Loss: 0.6126458048820496 \n",
      "     Training Step: 103 Training Loss: 0.6125351190567017 \n",
      "     Training Step: 104 Training Loss: 0.6156805157661438 \n",
      "     Training Step: 105 Training Loss: 0.6133111119270325 \n",
      "     Training Step: 106 Training Loss: 0.6118768453598022 \n",
      "     Training Step: 107 Training Loss: 0.6157458424568176 \n",
      "     Training Step: 108 Training Loss: 0.6174954175949097 \n",
      "     Training Step: 109 Training Loss: 0.61858069896698 \n",
      "     Training Step: 110 Training Loss: 0.6094580888748169 \n",
      "     Training Step: 111 Training Loss: 0.6107227802276611 \n",
      "     Training Step: 112 Training Loss: 0.6117811799049377 \n",
      "     Training Step: 113 Training Loss: 0.614478588104248 \n",
      "     Training Step: 114 Training Loss: 0.612230122089386 \n",
      "     Training Step: 115 Training Loss: 0.6159517765045166 \n",
      "     Training Step: 116 Training Loss: 0.6127593517303467 \n",
      "     Training Step: 117 Training Loss: 0.6097094416618347 \n",
      "     Training Step: 118 Training Loss: 0.618421733379364 \n",
      "     Training Step: 119 Training Loss: 0.6176478862762451 \n",
      "     Training Step: 120 Training Loss: 0.6136484146118164 \n",
      "     Training Step: 121 Training Loss: 0.6140189170837402 \n",
      "     Training Step: 122 Training Loss: 0.6114360094070435 \n",
      "     Training Step: 123 Training Loss: 0.6129227876663208 \n",
      "     Training Step: 124 Training Loss: 0.6171359419822693 \n",
      "     Training Step: 125 Training Loss: 0.6143338084220886 \n",
      "     Training Step: 126 Training Loss: 0.6146361231803894 \n",
      "     Training Step: 127 Training Loss: 0.6132946014404297 \n",
      "     Training Step: 128 Training Loss: 0.61184161901474 \n",
      "     Training Step: 129 Training Loss: 0.6112073659896851 \n",
      "     Training Step: 130 Training Loss: 0.6139150261878967 \n",
      "     Training Step: 131 Training Loss: 0.6142827272415161 \n",
      "     Training Step: 132 Training Loss: 0.6146474480628967 \n",
      "     Training Step: 133 Training Loss: 0.6118210554122925 \n",
      "     Training Step: 134 Training Loss: 0.6111352443695068 \n",
      "     Training Step: 135 Training Loss: 0.6114190816879272 \n",
      "     Training Step: 136 Training Loss: 0.6166300177574158 \n",
      "     Training Step: 137 Training Loss: 0.6097196936607361 \n",
      "     Training Step: 138 Training Loss: 0.6162275075912476 \n",
      "     Training Step: 139 Training Loss: 0.6143479347229004 \n",
      "     Training Step: 140 Training Loss: 0.6115710735321045 \n",
      "     Training Step: 141 Training Loss: 0.6152937412261963 \n",
      "     Training Step: 142 Training Loss: 0.6184831261634827 \n",
      "     Training Step: 143 Training Loss: 0.6144267916679382 \n",
      "     Training Step: 144 Training Loss: 0.6177012324333191 \n",
      "     Training Step: 145 Training Loss: 0.6109021306037903 \n",
      "     Training Step: 146 Training Loss: 0.6143707036972046 \n",
      "     Training Step: 147 Training Loss: 0.6128454208374023 \n",
      "     Training Step: 148 Training Loss: 0.6092479825019836 \n",
      "     Training Step: 149 Training Loss: 0.6141527891159058 \n",
      "     Training Step: 150 Training Loss: 0.6146907806396484 \n",
      "     Training Step: 151 Training Loss: 0.6169164776802063 \n",
      "     Training Step: 152 Training Loss: 0.61709064245224 \n",
      "     Training Step: 153 Training Loss: 0.6097270250320435 \n",
      "     Training Step: 154 Training Loss: 0.6176842451095581 \n",
      "     Training Step: 155 Training Loss: 0.6157518029212952 \n",
      "     Training Step: 156 Training Loss: 0.615283727645874 \n",
      "     Training Step: 157 Training Loss: 0.6159959435462952 \n",
      "     Training Step: 158 Training Loss: 0.614208459854126 \n",
      "     Training Step: 159 Training Loss: 0.6144427061080933 \n",
      "     Training Step: 160 Training Loss: 0.6101505756378174 \n",
      "     Training Step: 161 Training Loss: 0.6104037761688232 \n",
      "     Training Step: 162 Training Loss: 0.6185987591743469 \n",
      "     Training Step: 163 Training Loss: 0.6099779605865479 \n",
      "     Training Step: 164 Training Loss: 0.6140326857566833 \n",
      "     Training Step: 165 Training Loss: 0.6137641668319702 \n",
      "     Training Step: 166 Training Loss: 0.6166824102401733 \n",
      "     Training Step: 167 Training Loss: 0.6116200089454651 \n",
      "     Training Step: 168 Training Loss: 0.6178125143051147 \n",
      "     Training Step: 169 Training Loss: 0.6104617714881897 \n",
      "     Training Step: 170 Training Loss: 0.6118199825286865 \n",
      "     Training Step: 171 Training Loss: 0.6135073900222778 \n",
      "     Training Step: 172 Training Loss: 0.6121518611907959 \n",
      "     Training Step: 173 Training Loss: 0.6125290989875793 \n",
      "     Training Step: 174 Training Loss: 0.6128677129745483 \n",
      "     Training Step: 175 Training Loss: 0.6198974251747131 \n",
      "     Training Step: 176 Training Loss: 0.6105597019195557 \n",
      "     Training Step: 177 Training Loss: 0.6149569153785706 \n",
      "     Training Step: 178 Training Loss: 0.6167241930961609 \n",
      "     Training Step: 179 Training Loss: 0.6122316718101501 \n",
      "     Training Step: 180 Training Loss: 0.6137357950210571 \n",
      "     Training Step: 181 Training Loss: 0.6132031679153442 \n",
      "     Training Step: 182 Training Loss: 0.6120686531066895 \n",
      "     Training Step: 183 Training Loss: 0.6150973439216614 \n",
      "     Training Step: 184 Training Loss: 0.6151564717292786 \n",
      "     Training Step: 185 Training Loss: 0.6153953671455383 \n",
      "     Training Step: 186 Training Loss: 0.6127505302429199 \n",
      "     Training Step: 187 Training Loss: 0.6116887927055359 \n",
      "     Training Step: 188 Training Loss: 0.612372636795044 \n",
      "     Training Step: 189 Training Loss: 0.6132034659385681 \n",
      "     Training Step: 190 Training Loss: 0.6140754222869873 \n",
      "     Training Step: 191 Training Loss: 0.6116431355476379 \n",
      "     Training Step: 192 Training Loss: 0.6154763102531433 \n",
      "     Training Step: 193 Training Loss: 0.6154200434684753 \n",
      "     Training Step: 194 Training Loss: 0.6152393221855164 \n",
      "     Training Step: 195 Training Loss: 0.6147928833961487 \n",
      "     Training Step: 196 Training Loss: 0.6140156984329224 \n",
      "     Training Step: 197 Training Loss: 0.611802875995636 \n",
      "     Training Step: 198 Training Loss: 0.619445264339447 \n",
      "     Training Step: 199 Training Loss: 0.6158269047737122 \n",
      "     Training Step: 200 Training Loss: 0.6134135723114014 \n",
      "     Training Step: 201 Training Loss: 0.6160303354263306 \n",
      "     Training Step: 202 Training Loss: 0.610578179359436 \n",
      "     Training Step: 203 Training Loss: 0.6130664348602295 \n",
      "     Training Step: 204 Training Loss: 0.617679238319397 \n",
      "     Training Step: 205 Training Loss: 0.6149319410324097 \n",
      "     Training Step: 206 Training Loss: 0.6171836853027344 \n",
      "     Training Step: 207 Training Loss: 0.6133599877357483 \n",
      "     Training Step: 208 Training Loss: 0.6180061101913452 \n",
      "     Training Step: 209 Training Loss: 0.6155288815498352 \n",
      "     Training Step: 210 Training Loss: 0.6127076148986816 \n",
      "     Training Step: 211 Training Loss: 0.6122352480888367 \n",
      "     Training Step: 212 Training Loss: 0.6105142831802368 \n",
      "     Training Step: 213 Training Loss: 0.6168853044509888 \n",
      "     Training Step: 214 Training Loss: 0.6134724617004395 \n",
      "     Training Step: 215 Training Loss: 0.6150652170181274 \n",
      "     Training Step: 216 Training Loss: 0.6155145764350891 \n",
      "     Training Step: 217 Training Loss: 0.6107350587844849 \n",
      "     Training Step: 218 Training Loss: 0.6134570837020874 \n",
      "     Training Step: 219 Training Loss: 0.6123785972595215 \n",
      "     Training Step: 220 Training Loss: 0.6113897562026978 \n",
      "     Training Step: 221 Training Loss: 0.612118661403656 \n",
      "     Training Step: 222 Training Loss: 0.6123719811439514 \n",
      "     Training Step: 223 Training Loss: 0.6122828722000122 \n",
      "     Training Step: 224 Training Loss: 0.6133068799972534 \n",
      "     Training Step: 225 Training Loss: 0.6157602667808533 \n",
      "     Training Step: 226 Training Loss: 0.6166782379150391 \n",
      "     Training Step: 227 Training Loss: 0.6156913042068481 \n",
      "     Training Step: 228 Training Loss: 0.6153204441070557 \n",
      "     Training Step: 229 Training Loss: 0.6129639148712158 \n",
      "     Training Step: 230 Training Loss: 0.611620306968689 \n",
      "     Training Step: 231 Training Loss: 0.6082508563995361 \n",
      "     Training Step: 232 Training Loss: 0.6100534796714783 \n",
      "     Training Step: 233 Training Loss: 0.6167028546333313 \n",
      "     Training Step: 234 Training Loss: 0.6147047877311707 \n",
      "     Training Step: 235 Training Loss: 0.615536630153656 \n",
      "     Training Step: 236 Training Loss: 0.6132729053497314 \n",
      "     Training Step: 237 Training Loss: 0.6177636384963989 \n",
      "     Training Step: 238 Training Loss: 0.6114793419837952 \n",
      "     Training Step: 239 Training Loss: 0.6147463321685791 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6145658493041992 \n",
      "     Validation Step: 1 Validation Loss: 0.6184870600700378 \n",
      "     Validation Step: 2 Validation Loss: 0.6155695915222168 \n",
      "     Validation Step: 3 Validation Loss: 0.6170191764831543 \n",
      "     Validation Step: 4 Validation Loss: 0.6128301024436951 \n",
      "     Validation Step: 5 Validation Loss: 0.611555278301239 \n",
      "     Validation Step: 6 Validation Loss: 0.6180564165115356 \n",
      "     Validation Step: 7 Validation Loss: 0.612128496170044 \n",
      "     Validation Step: 8 Validation Loss: 0.6148934364318848 \n",
      "     Validation Step: 9 Validation Loss: 0.6152238845825195 \n",
      "     Validation Step: 10 Validation Loss: 0.611881673336029 \n",
      "     Validation Step: 11 Validation Loss: 0.6129884719848633 \n",
      "     Validation Step: 12 Validation Loss: 0.6183359622955322 \n",
      "     Validation Step: 13 Validation Loss: 0.6101517677307129 \n",
      "     Validation Step: 14 Validation Loss: 0.6162386536598206 \n",
      "     Validation Step: 15 Validation Loss: 0.6116387248039246 \n",
      "     Validation Step: 16 Validation Loss: 0.618466854095459 \n",
      "     Validation Step: 17 Validation Loss: 0.614254891872406 \n",
      "     Validation Step: 18 Validation Loss: 0.6136664152145386 \n",
      "     Validation Step: 19 Validation Loss: 0.6156092286109924 \n",
      "     Validation Step: 20 Validation Loss: 0.610515832901001 \n",
      "     Validation Step: 21 Validation Loss: 0.6101073026657104 \n",
      "     Validation Step: 22 Validation Loss: 0.6111785173416138 \n",
      "     Validation Step: 23 Validation Loss: 0.6150379180908203 \n",
      "     Validation Step: 24 Validation Loss: 0.6157886385917664 \n",
      "     Validation Step: 25 Validation Loss: 0.6136354804039001 \n",
      "     Validation Step: 26 Validation Loss: 0.6159854531288147 \n",
      "     Validation Step: 27 Validation Loss: 0.6133058667182922 \n",
      "     Validation Step: 28 Validation Loss: 0.6106255650520325 \n",
      "     Validation Step: 29 Validation Loss: 0.6182201504707336 \n",
      "     Validation Step: 30 Validation Loss: 0.6101290583610535 \n",
      "     Validation Step: 31 Validation Loss: 0.6142030954360962 \n",
      "     Validation Step: 32 Validation Loss: 0.6146403551101685 \n",
      "     Validation Step: 33 Validation Loss: 0.6145366430282593 \n",
      "     Validation Step: 34 Validation Loss: 0.6153194308280945 \n",
      "     Validation Step: 35 Validation Loss: 0.6104854941368103 \n",
      "     Validation Step: 36 Validation Loss: 0.6141151785850525 \n",
      "     Validation Step: 37 Validation Loss: 0.6175937652587891 \n",
      "     Validation Step: 38 Validation Loss: 0.6141343116760254 \n",
      "     Validation Step: 39 Validation Loss: 0.6176876425743103 \n",
      "     Validation Step: 40 Validation Loss: 0.6111660003662109 \n",
      "     Validation Step: 41 Validation Loss: 0.6136494278907776 \n",
      "     Validation Step: 42 Validation Loss: 0.614835798740387 \n",
      "     Validation Step: 43 Validation Loss: 0.6173006296157837 \n",
      "     Validation Step: 44 Validation Loss: 0.6075184345245361 \n",
      "Epoch: 149\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.6171568036079407 \n",
      "     Training Step: 1 Training Loss: 0.6123821139335632 \n",
      "     Training Step: 2 Training Loss: 0.6123786568641663 \n",
      "     Training Step: 3 Training Loss: 0.6115259528160095 \n",
      "     Training Step: 4 Training Loss: 0.6141491532325745 \n",
      "     Training Step: 5 Training Loss: 0.6128687262535095 \n",
      "     Training Step: 6 Training Loss: 0.6155166625976562 \n",
      "     Training Step: 7 Training Loss: 0.6152827739715576 \n",
      "     Training Step: 8 Training Loss: 0.6149179935455322 \n",
      "     Training Step: 9 Training Loss: 0.6140745878219604 \n",
      "     Training Step: 10 Training Loss: 0.6124637722969055 \n",
      "     Training Step: 11 Training Loss: 0.6117826700210571 \n",
      "     Training Step: 12 Training Loss: 0.6176897883415222 \n",
      "     Training Step: 13 Training Loss: 0.6147270798683167 \n",
      "     Training Step: 14 Training Loss: 0.6105759143829346 \n",
      "     Training Step: 15 Training Loss: 0.6177114248275757 \n",
      "     Training Step: 16 Training Loss: 0.6104915738105774 \n",
      "     Training Step: 17 Training Loss: 0.6108682751655579 \n",
      "     Training Step: 18 Training Loss: 0.6125075817108154 \n",
      "     Training Step: 19 Training Loss: 0.6132715344429016 \n",
      "     Training Step: 20 Training Loss: 0.6178309917449951 \n",
      "     Training Step: 21 Training Loss: 0.6127497553825378 \n",
      "     Training Step: 22 Training Loss: 0.6145011186599731 \n",
      "     Training Step: 23 Training Loss: 0.6116388440132141 \n",
      "     Training Step: 24 Training Loss: 0.610649585723877 \n",
      "     Training Step: 25 Training Loss: 0.6151798367500305 \n",
      "     Training Step: 26 Training Loss: 0.6146637201309204 \n",
      "     Training Step: 27 Training Loss: 0.6137416958808899 \n",
      "     Training Step: 28 Training Loss: 0.6139064431190491 \n",
      "     Training Step: 29 Training Loss: 0.6165004968643188 \n",
      "     Training Step: 30 Training Loss: 0.6184709072113037 \n",
      "     Training Step: 31 Training Loss: 0.6097045540809631 \n",
      "     Training Step: 32 Training Loss: 0.6152385473251343 \n",
      "     Training Step: 33 Training Loss: 0.6153903603553772 \n",
      "     Training Step: 34 Training Loss: 0.6115872859954834 \n",
      "     Training Step: 35 Training Loss: 0.61803138256073 \n",
      "     Training Step: 36 Training Loss: 0.6104041934013367 \n",
      "     Training Step: 37 Training Loss: 0.6167553663253784 \n",
      "     Training Step: 38 Training Loss: 0.61286860704422 \n",
      "     Training Step: 39 Training Loss: 0.6167039275169373 \n",
      "     Training Step: 40 Training Loss: 0.6159952878952026 \n",
      "     Training Step: 41 Training Loss: 0.6135903596878052 \n",
      "     Training Step: 42 Training Loss: 0.6153350472450256 \n",
      "     Training Step: 43 Training Loss: 0.6151564717292786 \n",
      "     Training Step: 44 Training Loss: 0.6118393540382385 \n",
      "     Training Step: 45 Training Loss: 0.6144221425056458 \n",
      "     Training Step: 46 Training Loss: 0.6166930198669434 \n",
      "     Training Step: 47 Training Loss: 0.6185622811317444 \n",
      "     Training Step: 48 Training Loss: 0.6131476759910583 \n",
      "     Training Step: 49 Training Loss: 0.6137630939483643 \n",
      "     Training Step: 50 Training Loss: 0.6154741048812866 \n",
      "     Training Step: 51 Training Loss: 0.6097385287284851 \n",
      "     Training Step: 52 Training Loss: 0.610732913017273 \n",
      "     Training Step: 53 Training Loss: 0.6161138415336609 \n",
      "     Training Step: 54 Training Loss: 0.6157443523406982 \n",
      "     Training Step: 55 Training Loss: 0.6123752593994141 \n",
      "     Training Step: 56 Training Loss: 0.6094456911087036 \n",
      "     Training Step: 57 Training Loss: 0.6131991744041443 \n",
      "     Training Step: 58 Training Loss: 0.6178196668624878 \n",
      "     Training Step: 59 Training Loss: 0.6163650155067444 \n",
      "     Training Step: 60 Training Loss: 0.6146121621131897 \n",
      "     Training Step: 61 Training Loss: 0.6157534122467041 \n",
      "     Training Step: 62 Training Loss: 0.6116185784339905 \n",
      "     Training Step: 63 Training Loss: 0.6144396662712097 \n",
      "     Training Step: 64 Training Loss: 0.6130550503730774 \n",
      "     Training Step: 65 Training Loss: 0.6133543848991394 \n",
      "     Training Step: 66 Training Loss: 0.6125311851501465 \n",
      "     Training Step: 67 Training Loss: 0.6114329099655151 \n",
      "     Training Step: 68 Training Loss: 0.6186014413833618 \n",
      "     Training Step: 69 Training Loss: 0.6147956848144531 \n",
      "     Training Step: 70 Training Loss: 0.6134627461433411 \n",
      "     Training Step: 71 Training Loss: 0.6129659414291382 \n",
      "     Training Step: 72 Training Loss: 0.612229585647583 \n",
      "     Training Step: 73 Training Loss: 0.614674985408783 \n",
      "     Training Step: 74 Training Loss: 0.6136230230331421 \n",
      "     Training Step: 75 Training Loss: 0.6168947815895081 \n",
      "     Training Step: 76 Training Loss: 0.6198546290397644 \n",
      "     Training Step: 77 Training Loss: 0.6099911332130432 \n",
      "     Training Step: 78 Training Loss: 0.6155962944030762 \n",
      "     Training Step: 79 Training Loss: 0.6161937713623047 \n",
      "     Training Step: 80 Training Loss: 0.6144554615020752 \n",
      "     Training Step: 81 Training Loss: 0.617085874080658 \n",
      "     Training Step: 82 Training Loss: 0.6137436628341675 \n",
      "     Training Step: 83 Training Loss: 0.6121622323989868 \n",
      "     Training Step: 84 Training Loss: 0.6111598610877991 \n",
      "     Training Step: 85 Training Loss: 0.6166658401489258 \n",
      "     Training Step: 86 Training Loss: 0.6143690943717957 \n",
      "     Training Step: 87 Training Loss: 0.6120737195014954 \n",
      "     Training Step: 88 Training Loss: 0.6168002486228943 \n",
      "     Training Step: 89 Training Loss: 0.6147443652153015 \n",
      "     Training Step: 90 Training Loss: 0.6118272542953491 \n",
      "     Training Step: 91 Training Loss: 0.6142446994781494 \n",
      "     Training Step: 92 Training Loss: 0.6169244050979614 \n",
      "     Training Step: 93 Training Loss: 0.6147026419639587 \n",
      "     Training Step: 94 Training Loss: 0.6138167977333069 \n",
      "     Training Step: 95 Training Loss: 0.6147031188011169 \n",
      "     Training Step: 96 Training Loss: 0.6156837344169617 \n",
      "     Training Step: 97 Training Loss: 0.6107354760169983 \n",
      "     Training Step: 98 Training Loss: 0.614342212677002 \n",
      "     Training Step: 99 Training Loss: 0.6146484613418579 \n",
      "     Training Step: 100 Training Loss: 0.6150950789451599 \n",
      "     Training Step: 101 Training Loss: 0.6130649447441101 \n",
      "     Training Step: 102 Training Loss: 0.6121603846549988 \n",
      "     Training Step: 103 Training Loss: 0.6168005466461182 \n",
      "     Training Step: 104 Training Loss: 0.6105908751487732 \n",
      "     Training Step: 105 Training Loss: 0.6196159720420837 \n",
      "     Training Step: 106 Training Loss: 0.6141044497489929 \n",
      "     Training Step: 107 Training Loss: 0.6129084825515747 \n",
      "     Training Step: 108 Training Loss: 0.6108959913253784 \n",
      "     Training Step: 109 Training Loss: 0.6116113066673279 \n",
      "     Training Step: 110 Training Loss: 0.6168028116226196 \n",
      "     Training Step: 111 Training Loss: 0.6150657534599304 \n",
      "     Training Step: 112 Training Loss: 0.613633930683136 \n",
      "     Training Step: 113 Training Loss: 0.6149552464485168 \n",
      "     Training Step: 114 Training Loss: 0.6143345832824707 \n",
      "     Training Step: 115 Training Loss: 0.6125264763832092 \n",
      "     Training Step: 116 Training Loss: 0.6180198192596436 \n",
      "     Training Step: 117 Training Loss: 0.6148697733879089 \n",
      "     Training Step: 118 Training Loss: 0.6115306615829468 \n",
      "     Training Step: 119 Training Loss: 0.6140148639678955 \n",
      "     Training Step: 120 Training Loss: 0.6143343448638916 \n",
      "     Training Step: 121 Training Loss: 0.6183035969734192 \n",
      "     Training Step: 122 Training Loss: 0.6132915616035461 \n",
      "     Training Step: 123 Training Loss: 0.6180582046508789 \n",
      "     Training Step: 124 Training Loss: 0.6136511564254761 \n",
      "     Training Step: 125 Training Loss: 0.6196714639663696 \n",
      "     Training Step: 126 Training Loss: 0.6161937713623047 \n",
      "     Training Step: 127 Training Loss: 0.6135303974151611 \n",
      "     Training Step: 128 Training Loss: 0.6117101907730103 \n",
      "     Training Step: 129 Training Loss: 0.6122634410858154 \n",
      "     Training Step: 130 Training Loss: 0.6115021109580994 \n",
      "     Training Step: 131 Training Loss: 0.616645097732544 \n",
      "     Training Step: 132 Training Loss: 0.6106989979743958 \n",
      "     Training Step: 133 Training Loss: 0.6160370707511902 \n",
      "     Training Step: 134 Training Loss: 0.6128420829772949 \n",
      "     Training Step: 135 Training Loss: 0.6097278594970703 \n",
      "     Training Step: 136 Training Loss: 0.6155479550361633 \n",
      "     Training Step: 137 Training Loss: 0.6126387119293213 \n",
      "     Training Step: 138 Training Loss: 0.6175021529197693 \n",
      "     Training Step: 139 Training Loss: 0.6105632185935974 \n",
      "     Training Step: 140 Training Loss: 0.6127954125404358 \n",
      "     Training Step: 141 Training Loss: 0.6162441372871399 \n",
      "     Training Step: 142 Training Loss: 0.614936351776123 \n",
      "     Training Step: 143 Training Loss: 0.6172008514404297 \n",
      "     Training Step: 144 Training Loss: 0.614643931388855 \n",
      "     Training Step: 145 Training Loss: 0.6113954186439514 \n",
      "     Training Step: 146 Training Loss: 0.615828275680542 \n",
      "     Training Step: 147 Training Loss: 0.616718053817749 \n",
      "     Training Step: 148 Training Loss: 0.6146673560142517 \n",
      "     Training Step: 149 Training Loss: 0.6145963668823242 \n",
      "     Training Step: 150 Training Loss: 0.6166481375694275 \n",
      "     Training Step: 151 Training Loss: 0.6140397191047668 \n",
      "     Training Step: 152 Training Loss: 0.6102015972137451 \n",
      "     Training Step: 153 Training Loss: 0.6154362559318542 \n",
      "     Training Step: 154 Training Loss: 0.6209053993225098 \n",
      "     Training Step: 155 Training Loss: 0.6122456192970276 \n",
      "     Training Step: 156 Training Loss: 0.6147720217704773 \n",
      "     Training Step: 157 Training Loss: 0.6153745651245117 \n",
      "     Training Step: 158 Training Loss: 0.615749716758728 \n",
      "     Training Step: 159 Training Loss: 0.612129807472229 \n",
      "     Training Step: 160 Training Loss: 0.6100660562515259 \n",
      "     Training Step: 161 Training Loss: 0.6094061136245728 \n",
      "     Training Step: 162 Training Loss: 0.6117798686027527 \n",
      "     Training Step: 163 Training Loss: 0.6184216141700745 \n",
      "     Training Step: 164 Training Loss: 0.6132780909538269 \n",
      "     Training Step: 165 Training Loss: 0.6154241561889648 \n",
      "     Training Step: 166 Training Loss: 0.6118214130401611 \n",
      "     Training Step: 167 Training Loss: 0.6122784614562988 \n",
      "     Training Step: 168 Training Loss: 0.6124826669692993 \n",
      "     Training Step: 169 Training Loss: 0.6101185083389282 \n",
      "     Training Step: 170 Training Loss: 0.6139204502105713 \n",
      "     Training Step: 171 Training Loss: 0.6166365146636963 \n",
      "     Training Step: 172 Training Loss: 0.6164424419403076 \n",
      "     Training Step: 173 Training Loss: 0.6167330741882324 \n",
      "     Training Step: 174 Training Loss: 0.6082357168197632 \n",
      "     Training Step: 175 Training Loss: 0.615087628364563 \n",
      "     Training Step: 176 Training Loss: 0.6152883172035217 \n",
      "     Training Step: 177 Training Loss: 0.6115479469299316 \n",
      "     Training Step: 178 Training Loss: 0.6142804026603699 \n",
      "     Training Step: 179 Training Loss: 0.6182268261909485 \n",
      "     Training Step: 180 Training Loss: 0.6157795786857605 \n",
      "     Training Step: 181 Training Loss: 0.6159434914588928 \n",
      "     Training Step: 182 Training Loss: 0.6129228472709656 \n",
      "     Training Step: 183 Training Loss: 0.6112115383148193 \n",
      "     Training Step: 184 Training Loss: 0.6104916334152222 \n",
      "     Training Step: 185 Training Loss: 0.6131251454353333 \n",
      "     Training Step: 186 Training Loss: 0.6180941462516785 \n",
      "     Training Step: 187 Training Loss: 0.6176337003707886 \n",
      "     Training Step: 188 Training Loss: 0.6154201030731201 \n",
      "     Training Step: 189 Training Loss: 0.613203763961792 \n",
      "     Training Step: 190 Training Loss: 0.614021897315979 \n",
      "     Training Step: 191 Training Loss: 0.614151656627655 \n",
      "     Training Step: 192 Training Loss: 0.6105733513832092 \n",
      "     Training Step: 193 Training Loss: 0.6114085912704468 \n",
      "     Training Step: 194 Training Loss: 0.6134953498840332 \n",
      "     Training Step: 195 Training Loss: 0.6126933693885803 \n",
      "     Training Step: 196 Training Loss: 0.6100279688835144 \n",
      "     Training Step: 197 Training Loss: 0.6100389361381531 \n",
      "     Training Step: 198 Training Loss: 0.6155490875244141 \n",
      "     Training Step: 199 Training Loss: 0.6120055913925171 \n",
      "     Training Step: 200 Training Loss: 0.614497184753418 \n",
      "     Training Step: 201 Training Loss: 0.6152872443199158 \n",
      "     Training Step: 202 Training Loss: 0.6184897422790527 \n",
      "     Training Step: 203 Training Loss: 0.6133096814155579 \n",
      "     Training Step: 204 Training Loss: 0.61213618516922 \n",
      "     Training Step: 205 Training Loss: 0.617148220539093 \n",
      "     Training Step: 206 Training Loss: 0.6092196106910706 \n",
      "     Training Step: 207 Training Loss: 0.6114161610603333 \n",
      "     Training Step: 208 Training Loss: 0.6194422841072083 \n",
      "     Training Step: 209 Training Loss: 0.6115259528160095 \n",
      "     Training Step: 210 Training Loss: 0.6142030358314514 \n",
      "     Training Step: 211 Training Loss: 0.6118847131729126 \n",
      "     Training Step: 212 Training Loss: 0.6111637949943542 \n",
      "     Training Step: 213 Training Loss: 0.6156790256500244 \n",
      "     Training Step: 214 Training Loss: 0.6146883964538574 \n",
      "     Training Step: 215 Training Loss: 0.6131855249404907 \n",
      "     Training Step: 216 Training Loss: 0.6188263893127441 \n",
      "     Training Step: 217 Training Loss: 0.614606499671936 \n",
      "     Training Step: 218 Training Loss: 0.6118046045303345 \n",
      "     Training Step: 219 Training Loss: 0.6103799343109131 \n",
      "     Training Step: 220 Training Loss: 0.6142176985740662 \n",
      "     Training Step: 221 Training Loss: 0.6173725724220276 \n",
      "     Training Step: 222 Training Loss: 0.6166569590568542 \n",
      "     Training Step: 223 Training Loss: 0.6164025068283081 \n",
      "     Training Step: 224 Training Loss: 0.6182176470756531 \n",
      "     Training Step: 225 Training Loss: 0.617135763168335 \n",
      "     Training Step: 226 Training Loss: 0.6123077273368835 \n",
      "     Training Step: 227 Training Loss: 0.6127678751945496 \n",
      "     Training Step: 228 Training Loss: 0.6114495396614075 \n",
      "     Training Step: 229 Training Loss: 0.6188515424728394 \n",
      "     Training Step: 230 Training Loss: 0.6134688258171082 \n",
      "     Training Step: 231 Training Loss: 0.6134172677993774 \n",
      "     Training Step: 232 Training Loss: 0.6201990246772766 \n",
      "     Training Step: 233 Training Loss: 0.6152939796447754 \n",
      "     Training Step: 234 Training Loss: 0.611639678478241 \n",
      "     Training Step: 235 Training Loss: 0.6176791787147522 \n",
      "     Training Step: 236 Training Loss: 0.6153669357299805 \n",
      "     Training Step: 237 Training Loss: 0.6177470088005066 \n",
      "     Training Step: 238 Training Loss: 0.6138908267021179 \n",
      "     Training Step: 239 Training Loss: 0.6133082509040833 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 0.6142565608024597 \n",
      "     Validation Step: 1 Validation Loss: 0.610168993473053 \n",
      "     Validation Step: 2 Validation Loss: 0.6153178811073303 \n",
      "     Validation Step: 3 Validation Loss: 0.6136370897293091 \n",
      "     Validation Step: 4 Validation Loss: 0.6146438717842102 \n",
      "     Validation Step: 5 Validation Loss: 0.6115701198577881 \n",
      "     Validation Step: 6 Validation Loss: 0.6156087517738342 \n",
      "     Validation Step: 7 Validation Loss: 0.6141160130500793 \n",
      "     Validation Step: 8 Validation Loss: 0.6148915886878967 \n",
      "     Validation Step: 9 Validation Loss: 0.6145662665367126 \n",
      "     Validation Step: 10 Validation Loss: 0.6148354411125183 \n",
      "     Validation Step: 11 Validation Loss: 0.6157810091972351 \n",
      "     Validation Step: 12 Validation Loss: 0.6075466871261597 \n",
      "     Validation Step: 13 Validation Loss: 0.6118919253349304 \n",
      "     Validation Step: 14 Validation Loss: 0.6170080900192261 \n",
      "     Validation Step: 15 Validation Loss: 0.614137589931488 \n",
      "     Validation Step: 16 Validation Loss: 0.6176762580871582 \n",
      "     Validation Step: 17 Validation Loss: 0.6180441379547119 \n",
      "     Validation Step: 18 Validation Loss: 0.6172884702682495 \n",
      "     Validation Step: 19 Validation Loss: 0.6111782789230347 \n",
      "     Validation Step: 20 Validation Loss: 0.6128363609313965 \n",
      "     Validation Step: 21 Validation Loss: 0.6121391654014587 \n",
      "     Validation Step: 22 Validation Loss: 0.6175832748413086 \n",
      "     Validation Step: 23 Validation Loss: 0.6150366067886353 \n",
      "     Validation Step: 24 Validation Loss: 0.6129953861236572 \n",
      "     Validation Step: 25 Validation Loss: 0.6136727929115295 \n",
      "     Validation Step: 26 Validation Loss: 0.6184720993041992 \n",
      "     Validation Step: 27 Validation Loss: 0.6101555228233337 \n",
      "     Validation Step: 28 Validation Loss: 0.6136530041694641 \n",
      "     Validation Step: 29 Validation Loss: 0.6183241009712219 \n",
      "     Validation Step: 30 Validation Loss: 0.6152235269546509 \n",
      "     Validation Step: 31 Validation Loss: 0.6105008125305176 \n",
      "     Validation Step: 32 Validation Loss: 0.6133129596710205 \n",
      "     Validation Step: 33 Validation Loss: 0.6105331182479858 \n",
      "     Validation Step: 34 Validation Loss: 0.6159782409667969 \n",
      "     Validation Step: 35 Validation Loss: 0.6142060160636902 \n",
      "     Validation Step: 36 Validation Loss: 0.6111908555030823 \n",
      "     Validation Step: 37 Validation Loss: 0.6155678033828735 \n",
      "     Validation Step: 38 Validation Loss: 0.6182063221931458 \n",
      "     Validation Step: 39 Validation Loss: 0.6184508800506592 \n",
      "     Validation Step: 40 Validation Loss: 0.6106421947479248 \n",
      "     Validation Step: 41 Validation Loss: 0.6162376999855042 \n",
      "     Validation Step: 42 Validation Loss: 0.6101323366165161 \n",
      "     Validation Step: 43 Validation Loss: 0.6145373582839966 \n",
      "     Validation Step: 44 Validation Loss: 0.6116518378257751 \n"
     ]
    }
   ],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 76\n",
    "num_epochs = 150\n",
    "loss_weights = (1.0 ,1.0, 1.0)\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF_chained(None,None, embedder_model,ACOPF_optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{'SB': tensor([[ 6.0855, -1.1536,  6.4379, -4.8384]]),\n 'PQ': tensor([[-0.2824,  1.3603,  0.1809,  0.1686],\n         [-0.1250,  0.1585,  0.1808,  0.1706],\n         [-0.1259,  0.1522,  0.2675,  0.2229],\n         [-0.1347,  0.1003,  0.8072,  0.5386],\n         [ 0.0017,  0.9427,  0.1777,  0.1698],\n         [-0.2852,  1.0777,  0.1790,  0.1704],\n         [-0.0672,  0.5336,  0.7829,  0.5605],\n         [-0.1090,  1.2983,  0.1582,  0.1465],\n         [-0.1837,  0.5072,  0.1779,  0.1705],\n         [-0.1263,  0.1656,  0.1798,  0.1688],\n         [-0.2874,  1.0945,  0.1818,  0.1696],\n         [-0.1101,  0.5598,  0.3081,  0.2222],\n         [-0.3022,  1.3244,  0.3070,  0.2200],\n         [-0.2846,  1.0818,  0.1790,  0.1714],\n         [-0.1251,  0.1578,  0.1781,  0.1715],\n         [-0.1140,  1.3667,  0.1820,  0.1702]]),\n 'PV': tensor([[ 4.5760e+00, -1.8209e-01,  5.3566e+00,  4.0813e+00],\n         [ 1.5967e+00,  6.8131e-01,  3.4700e+00,  6.6232e+00],\n         [-3.2277e-01,  1.2543e+00,  8.1089e-02,  1.1234e-01],\n         [-3.0086e-01,  1.3178e+00, -1.8759e-01, -4.3205e-02],\n         [-2.6189e-01,  1.4150e+00,  1.2465e-02,  7.1887e-02],\n         [-3.1611e-01,  1.2356e+00, -2.5183e+00, -1.4000e+00],\n         [-1.5332e-01,  4.3070e-01, -1.0926e-01, -1.2705e-04],\n         [ 3.6277e-03,  9.5690e-01, -2.0944e+00, -1.1669e+00],\n         [-5.8869e-02,  5.8428e-01, -4.6212e-01, -2.0310e-01],\n         [-8.8145e-02,  1.5568e+00, -1.6314e-01, -2.9545e-02],\n         [-2.1968e-01,  8.2148e-01, -2.5475e-01, -8.0533e-02],\n         [-2.7909e-01,  1.1051e+00, -2.6458e-01, -8.8273e-02],\n         [-2.7170e-01,  1.5355e+00, -2.0541e+00, -1.1300e+00],\n         [-2.3369e-01,  1.0595e+00, -1.3267e-01, -3.8659e-03],\n         [-2.4051e-01,  1.0142e+00,  7.7394e-03,  6.9893e-02],\n         [-1.4984e-02,  8.4820e-01, -6.6040e-01, -3.3222e-01],\n         [-3.1083e-01,  1.2662e+00,  4.1089e-02,  8.7559e-02],\n         [-1.0966e-01,  5.6269e-01, -1.2325e-01, -5.7864e-03],\n         [-9.7389e-02,  9.2813e-01, -2.0001e-01, -5.0996e-02],\n         [-2.8139e-01,  1.5369e+00, -7.6335e-01, -3.7457e-01],\n         [-3.1854e-01,  1.2825e+00, -4.3700e-01, -1.8769e-01],\n         [-1.5563e-01,  4.1641e-01, -2.0977e-02,  5.3182e-02],\n         [-2.8172e-01,  1.3650e+00,  2.1658e-04,  6.6407e-02],\n         [-1.3296e-01,  1.0451e-01, -1.0052e-01,  4.1680e-03],\n         [-1.2493e-01,  1.5851e-01,  8.7588e-02,  1.1635e-01],\n         [-3.1687e-01,  1.2928e+00,  7.6920e-02,  1.1133e-01],\n         [-2.6274e-01,  1.4095e+00,  3.3414e-02,  8.3426e-02],\n         [-2.9786e-01,  1.3594e+00, -4.8350e-01, -2.1657e-01],\n         [-6.0051e-02,  1.7287e+00, -8.0927e-01, -4.0552e-01],\n         [-1.1529e-01,  1.3878e+00,  7.0184e-02,  1.0348e-01],\n         [-9.8535e-02,  1.4924e+00,  6.4351e-02,  1.0301e-01],\n         [-9.7065e-02,  9.5684e-01, -8.8611e-02,  1.4018e-02],\n         [-9.7943e-02,  1.1538e+00, -2.4139e-01, -6.8684e-02],\n         [-3.2260e-01,  1.2555e+00,  1.8408e-02,  7.8160e-02],\n         [-2.4702e-01,  1.4397e+00, -7.8002e-01, -3.8842e-01],\n         [-1.2456e-01,  1.6082e-01, -1.8670e-02,  5.4901e-02],\n         [-2.5707e-01,  1.4210e+00,  2.9690e-02,  8.2576e-02],\n         [-9.7706e-02,  1.4970e+00,  6.8150e-02,  1.0380e-01],\n         [-9.7567e-02,  1.1561e+00, -1.2043e-01, -4.0874e-03],\n         [-1.0221e-01,  7.6760e-01, -3.1353e-02,  4.5576e-02],\n         [-3.0026e-01,  1.3411e+00, -2.2942e-01, -6.6746e-02],\n         [-3.0325e-01,  1.3889e+00, -5.8324e-02,  3.0973e-02]]),\n 'NB': tensor([[-0.2494,  0.9562,  0.1240,  0.1376],\n         [-0.1844,  0.5117,  0.1240,  0.1376],\n         [-0.2873,  1.0942,  0.1240,  0.1376],\n         [-0.2064,  0.6600,  0.1240,  0.1376],\n         [-0.0150,  0.8481,  0.1240,  0.1376]])}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
