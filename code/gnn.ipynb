{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from helper import *\n",
    "from heterognn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The State of the nth node is expressed by 4 real scalars:\n",
    "\n",
    "v_n -> the voltage at the node\n",
    "delta_n -> the voltage angle at the node (relative to the slack bus)\n",
    "p_n -> the net active power flowing into the node\n",
    "q_n -> the net reactive power flowing into the node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical characteristics of the network are described by the power flow equations:\n",
    "\n",
    "p = P(v, delta, W)\n",
    "q = Q(v, delta, W)\n",
    "\n",
    "-> Relate local net power generation with the global state\n",
    "-> Depends on the topology W of the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electrical grid => Weighted Graph\n",
    "\n",
    "Nodes in the graph produce/consume power\n",
    "\n",
    "Edges represent electrical connections between nodes\n",
    "\n",
    "State Matrix X element of  R(N x 4) => graph signal with 4 features\n",
    "    => Each row is the state of the corresponding Node\n",
    "\n",
    "Adjacency Matrix A => sparse matrix to represent the connections of each node, element of R(N x N), Aij = 1 if node i is connected to node j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the GNN as a mode phi(X, A, H)\n",
    "\n",
    "We want to imitate the OPF solution p*\n",
    "-> We want to minimize a loss L over a dataset T = {{X, p*}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Function:min arg H of sum over T L(p*,phi(X, A, H)) and we use L = Mean Squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once GNN model phi is trained, we do not need the costly p* from pandapower to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input Data X - R^(Nx4): Uniformly sample p_ref and q_ref of each load L with P_L ~ Uniform(0.9 * p_ref, 1.1 * p_ref) and Q_L ~ Uniform(0.9 * q_ref, 1.1 * q_ref)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pseudocode for X and y in supervised learning:\n",
    "for each P_L and Q_L:\n",
    "    Create X with sub-optimal DCOPF results\n",
    "    Create y with Pandapower calculating p* ACOPF with IPOPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatio-Temporal GNN -> superposition of a gnn with spatial info and a temporal layer (Temporal Conv,LSTM etc.) ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bus => Node in GNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['1-complete_data-mixed-all-0-sw',\n '1-complete_data-mixed-all-1-sw',\n '1-complete_data-mixed-all-2-sw',\n '1-EHVHVMVLV-mixed-all-0-sw',\n '1-EHVHVMVLV-mixed-all-1-sw',\n '1-EHVHVMVLV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-0-sw',\n '1-EHVHV-mixed-all-0-no_sw',\n '1-EHVHV-mixed-all-1-sw',\n '1-EHVHV-mixed-all-1-no_sw',\n '1-EHVHV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-2-no_sw',\n '1-EHVHV-mixed-1-0-sw',\n '1-EHVHV-mixed-1-0-no_sw',\n '1-EHVHV-mixed-1-1-sw',\n '1-EHVHV-mixed-1-1-no_sw',\n '1-EHVHV-mixed-1-2-sw',\n '1-EHVHV-mixed-1-2-no_sw',\n '1-EHVHV-mixed-2-0-sw',\n '1-EHVHV-mixed-2-0-no_sw',\n '1-EHVHV-mixed-2-1-sw',\n '1-EHVHV-mixed-2-1-no_sw',\n '1-EHVHV-mixed-2-2-sw',\n '1-EHVHV-mixed-2-2-no_sw',\n '1-EHV-mixed--0-sw',\n '1-EHV-mixed--0-no_sw',\n '1-EHV-mixed--1-sw',\n '1-EHV-mixed--1-no_sw',\n '1-EHV-mixed--2-sw',\n '1-EHV-mixed--2-no_sw',\n '1-HVMV-mixed-all-0-sw',\n '1-HVMV-mixed-all-0-no_sw',\n '1-HVMV-mixed-all-1-sw',\n '1-HVMV-mixed-all-1-no_sw',\n '1-HVMV-mixed-all-2-sw',\n '1-HVMV-mixed-all-2-no_sw',\n '1-HVMV-mixed-1.105-0-sw',\n '1-HVMV-mixed-1.105-0-no_sw',\n '1-HVMV-mixed-1.105-1-sw',\n '1-HVMV-mixed-1.105-1-no_sw',\n '1-HVMV-mixed-1.105-2-sw',\n '1-HVMV-mixed-1.105-2-no_sw',\n '1-HVMV-mixed-2.102-0-sw',\n '1-HVMV-mixed-2.102-0-no_sw',\n '1-HVMV-mixed-2.102-1-sw',\n '1-HVMV-mixed-2.102-1-no_sw',\n '1-HVMV-mixed-2.102-2-sw',\n '1-HVMV-mixed-2.102-2-no_sw',\n '1-HVMV-mixed-4.101-0-sw',\n '1-HVMV-mixed-4.101-0-no_sw',\n '1-HVMV-mixed-4.101-1-sw',\n '1-HVMV-mixed-4.101-1-no_sw',\n '1-HVMV-mixed-4.101-2-sw',\n '1-HVMV-mixed-4.101-2-no_sw',\n '1-HVMV-urban-all-0-sw',\n '1-HVMV-urban-all-0-no_sw',\n '1-HVMV-urban-all-1-sw',\n '1-HVMV-urban-all-1-no_sw',\n '1-HVMV-urban-all-2-sw',\n '1-HVMV-urban-all-2-no_sw',\n '1-HVMV-urban-2.203-0-sw',\n '1-HVMV-urban-2.203-0-no_sw',\n '1-HVMV-urban-2.203-1-sw',\n '1-HVMV-urban-2.203-1-no_sw',\n '1-HVMV-urban-2.203-2-sw',\n '1-HVMV-urban-2.203-2-no_sw',\n '1-HVMV-urban-3.201-0-sw',\n '1-HVMV-urban-3.201-0-no_sw',\n '1-HVMV-urban-3.201-1-sw',\n '1-HVMV-urban-3.201-1-no_sw',\n '1-HVMV-urban-3.201-2-sw',\n '1-HVMV-urban-3.201-2-no_sw',\n '1-HVMV-urban-4.201-0-sw',\n '1-HVMV-urban-4.201-0-no_sw',\n '1-HVMV-urban-4.201-1-sw',\n '1-HVMV-urban-4.201-1-no_sw',\n '1-HVMV-urban-4.201-2-sw',\n '1-HVMV-urban-4.201-2-no_sw',\n '1-HV-mixed--0-sw',\n '1-HV-mixed--0-no_sw',\n '1-HV-mixed--1-sw',\n '1-HV-mixed--1-no_sw',\n '1-HV-mixed--2-sw',\n '1-HV-mixed--2-no_sw',\n '1-HV-urban--0-sw',\n '1-HV-urban--0-no_sw',\n '1-HV-urban--1-sw',\n '1-HV-urban--1-no_sw',\n '1-HV-urban--2-sw',\n '1-HV-urban--2-no_sw',\n '1-MVLV-rural-all-0-sw',\n '1-MVLV-rural-all-0-no_sw',\n '1-MVLV-rural-all-1-sw',\n '1-MVLV-rural-all-1-no_sw',\n '1-MVLV-rural-all-2-sw',\n '1-MVLV-rural-all-2-no_sw',\n '1-MVLV-rural-1.108-0-sw',\n '1-MVLV-rural-1.108-0-no_sw',\n '1-MVLV-rural-1.108-1-sw',\n '1-MVLV-rural-1.108-1-no_sw',\n '1-MVLV-rural-1.108-2-sw',\n '1-MVLV-rural-1.108-2-no_sw',\n '1-MVLV-rural-2.107-0-sw',\n '1-MVLV-rural-2.107-0-no_sw',\n '1-MVLV-rural-2.107-1-sw',\n '1-MVLV-rural-2.107-1-no_sw',\n '1-MVLV-rural-2.107-2-sw',\n '1-MVLV-rural-2.107-2-no_sw',\n '1-MVLV-rural-4.101-0-sw',\n '1-MVLV-rural-4.101-0-no_sw',\n '1-MVLV-rural-4.101-1-sw',\n '1-MVLV-rural-4.101-1-no_sw',\n '1-MVLV-rural-4.101-2-sw',\n '1-MVLV-rural-4.101-2-no_sw',\n '1-MVLV-semiurb-all-0-sw',\n '1-MVLV-semiurb-all-0-no_sw',\n '1-MVLV-semiurb-all-1-sw',\n '1-MVLV-semiurb-all-1-no_sw',\n '1-MVLV-semiurb-all-2-sw',\n '1-MVLV-semiurb-all-2-no_sw',\n '1-MVLV-semiurb-3.202-0-sw',\n '1-MVLV-semiurb-3.202-0-no_sw',\n '1-MVLV-semiurb-3.202-1-sw',\n '1-MVLV-semiurb-3.202-1-no_sw',\n '1-MVLV-semiurb-3.202-2-sw',\n '1-MVLV-semiurb-3.202-2-no_sw',\n '1-MVLV-semiurb-4.201-0-sw',\n '1-MVLV-semiurb-4.201-0-no_sw',\n '1-MVLV-semiurb-4.201-1-sw',\n '1-MVLV-semiurb-4.201-1-no_sw',\n '1-MVLV-semiurb-4.201-2-sw',\n '1-MVLV-semiurb-4.201-2-no_sw',\n '1-MVLV-semiurb-5.220-0-sw',\n '1-MVLV-semiurb-5.220-0-no_sw',\n '1-MVLV-semiurb-5.220-1-sw',\n '1-MVLV-semiurb-5.220-1-no_sw',\n '1-MVLV-semiurb-5.220-2-sw',\n '1-MVLV-semiurb-5.220-2-no_sw',\n '1-MVLV-urban-all-0-sw',\n '1-MVLV-urban-all-0-no_sw',\n '1-MVLV-urban-all-1-sw',\n '1-MVLV-urban-all-1-no_sw',\n '1-MVLV-urban-all-2-sw',\n '1-MVLV-urban-all-2-no_sw',\n '1-MVLV-urban-5.303-0-sw',\n '1-MVLV-urban-5.303-0-no_sw',\n '1-MVLV-urban-5.303-1-sw',\n '1-MVLV-urban-5.303-1-no_sw',\n '1-MVLV-urban-5.303-2-sw',\n '1-MVLV-urban-5.303-2-no_sw',\n '1-MVLV-urban-6.305-0-sw',\n '1-MVLV-urban-6.305-0-no_sw',\n '1-MVLV-urban-6.305-1-sw',\n '1-MVLV-urban-6.305-1-no_sw',\n '1-MVLV-urban-6.305-2-sw',\n '1-MVLV-urban-6.305-2-no_sw',\n '1-MVLV-urban-6.309-0-sw',\n '1-MVLV-urban-6.309-0-no_sw',\n '1-MVLV-urban-6.309-1-sw',\n '1-MVLV-urban-6.309-1-no_sw',\n '1-MVLV-urban-6.309-2-sw',\n '1-MVLV-urban-6.309-2-no_sw',\n '1-MVLV-comm-all-0-sw',\n '1-MVLV-comm-all-0-no_sw',\n '1-MVLV-comm-all-1-sw',\n '1-MVLV-comm-all-1-no_sw',\n '1-MVLV-comm-all-2-sw',\n '1-MVLV-comm-all-2-no_sw',\n '1-MVLV-comm-3.403-0-sw',\n '1-MVLV-comm-3.403-0-no_sw',\n '1-MVLV-comm-3.403-1-sw',\n '1-MVLV-comm-3.403-1-no_sw',\n '1-MVLV-comm-3.403-2-sw',\n '1-MVLV-comm-3.403-2-no_sw',\n '1-MVLV-comm-4.416-0-sw',\n '1-MVLV-comm-4.416-0-no_sw',\n '1-MVLV-comm-4.416-1-sw',\n '1-MVLV-comm-4.416-1-no_sw',\n '1-MVLV-comm-4.416-2-sw',\n '1-MVLV-comm-4.416-2-no_sw',\n '1-MVLV-comm-5.401-0-sw',\n '1-MVLV-comm-5.401-0-no_sw',\n '1-MVLV-comm-5.401-1-sw',\n '1-MVLV-comm-5.401-1-no_sw',\n '1-MVLV-comm-5.401-2-sw',\n '1-MVLV-comm-5.401-2-no_sw',\n '1-MV-rural--0-sw',\n '1-MV-rural--0-no_sw',\n '1-MV-rural--1-sw',\n '1-MV-rural--1-no_sw',\n '1-MV-rural--2-sw',\n '1-MV-rural--2-no_sw',\n '1-MV-semiurb--0-sw',\n '1-MV-semiurb--0-no_sw',\n '1-MV-semiurb--1-sw',\n '1-MV-semiurb--1-no_sw',\n '1-MV-semiurb--2-sw',\n '1-MV-semiurb--2-no_sw',\n '1-MV-urban--0-sw',\n '1-MV-urban--0-no_sw',\n '1-MV-urban--1-sw',\n '1-MV-urban--1-no_sw',\n '1-MV-urban--2-sw',\n '1-MV-urban--2-no_sw',\n '1-MV-comm--0-sw',\n '1-MV-comm--0-no_sw',\n '1-MV-comm--1-sw',\n '1-MV-comm--1-no_sw',\n '1-MV-comm--2-sw',\n '1-MV-comm--2-no_sw',\n '1-LV-rural1--0-sw',\n '1-LV-rural1--0-no_sw',\n '1-LV-rural1--1-sw',\n '1-LV-rural1--1-no_sw',\n '1-LV-rural1--2-sw',\n '1-LV-rural1--2-no_sw',\n '1-LV-rural2--0-sw',\n '1-LV-rural2--0-no_sw',\n '1-LV-rural2--1-sw',\n '1-LV-rural2--1-no_sw',\n '1-LV-rural2--2-sw',\n '1-LV-rural2--2-no_sw',\n '1-LV-rural3--0-sw',\n '1-LV-rural3--0-no_sw',\n '1-LV-rural3--1-sw',\n '1-LV-rural3--1-no_sw',\n '1-LV-rural3--2-sw',\n '1-LV-rural3--2-no_sw',\n '1-LV-semiurb4--0-sw',\n '1-LV-semiurb4--0-no_sw',\n '1-LV-semiurb4--1-sw',\n '1-LV-semiurb4--1-no_sw',\n '1-LV-semiurb4--2-sw',\n '1-LV-semiurb4--2-no_sw',\n '1-LV-semiurb5--0-sw',\n '1-LV-semiurb5--0-no_sw',\n '1-LV-semiurb5--1-sw',\n '1-LV-semiurb5--1-no_sw',\n '1-LV-semiurb5--2-sw',\n '1-LV-semiurb5--2-no_sw',\n '1-LV-urban6--0-sw',\n '1-LV-urban6--0-no_sw',\n '1-LV-urban6--1-sw',\n '1-LV-urban6--1-no_sw',\n '1-LV-urban6--2-sw',\n '1-LV-urban6--2-no_sw']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get lists of simbench codes\n",
    "all_simbench_codes = sb.collect_all_simbench_codes()\n",
    "all_simbench_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'/'.join(os.path.dirname(os.path.abspath(__file__).split(\"\\\\\"))) + r\"/Models/SelfSupervised/base_model.pt\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net.bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_json('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'\n",
    "#Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "#TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "#TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "#NETWORK CONSTRAINTS\n",
    "\n",
    "#Maximize the branch limits\n",
    "\n",
    "#max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "#for i in range(len(max_i_ka)):\n",
    "# max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "#Maximize line loading percents\n",
    "max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo loading percent\n",
    "max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo3w loading percent\n",
    "max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Cost assignment\n",
    "\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "pp.runopp(net,verbose=True)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = read_unsupervised_dataset('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_data[0].has_isolated_nodes()\n",
    "#train_data[0].has_self_loops()\n",
    "#train_data[0].is_undirected()\n",
    "x_dict = train_data[0].to_dict()\n",
    "#to_json(train_dict)\n",
    "ln = len(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"][0])\n",
    "print(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"])#[:, :int(ln/2)]\n",
    "x_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')\n",
    "idx_mapper, node_types_as_dict = extract_node_types_as_dict(net)\n",
    "for key in node_types_as_dict:\n",
    "    print(f\"Bus Type: {key}\")\n",
    "    for i in range(len(node_types_as_dict[key])):\n",
    "        #node_types_as_dict[key][i] = idx_mapper[node_types_as_dict[key][i]]\n",
    "        print(str(node_types_as_dict[key][i]))\n",
    "    print(\"-------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_dict[\"PQ\"]['x'][2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_available = []\n",
    "for nw_name in all_simbench_codes:\n",
    "        net = sb.get_simbench_net(nw_name)\n",
    "        print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "        #dict_probs = pp.diagnostic(net,report_style='None')\n",
    "        #for bus_num in dict_probs['multiple_voltage_controlling_elements_per_bus']['buses_with_gens_and_ext_grids']:\n",
    "        #    net.gen = net.gen.drop(net.gen[net.gen.bus == bus_num].index)\n",
    "\n",
    "        #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "        #Set upper and lower limits of active-reactive powers of loads\n",
    "        min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "        p_mw = list(net.load.p_mw.values)\n",
    "        q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "        for i in range(len(p_mw)):\n",
    "            min_p_mw_val.append(p_mw[i])\n",
    "            max_p_mw_val.append(p_mw[i])\n",
    "            min_q_mvar_val.append(q_mvar[i])\n",
    "            max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "        net.load.min_p_mw = min_p_mw_val\n",
    "        net.load.max_p_mw = max_p_mw_val\n",
    "        net.load.min_q_mvar = min_q_mvar_val\n",
    "        net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "        #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "        ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "        pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "        #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "        #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "        #NETWORK CONSTRAINTS\n",
    "\n",
    "        #Maximize the branch limits\n",
    "\n",
    "        #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "        #for i in range(len(max_i_ka)):\n",
    "        # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "        #Maximize line loading percents\n",
    "        max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo loading percent\n",
    "        max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo3w loading percent\n",
    "        max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Cost assignment\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "        try:\n",
    "            pp.runpm_dc_opf(net) # Run DCOPP\n",
    "        except pp.OPFNotConverged:\n",
    "            text = \"DC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "            print(text)\n",
    "            continue\n",
    "        print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR DCOPF\")\n",
    "        grids_available.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_available:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) # dcopp on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_ready = []\n",
    "for nw_name in all_simbench_codes[6:]:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "    #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "    #Set upper and lower limits of active-reactive powers of loads\n",
    "    min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "    p_mw = list(net.load.p_mw.values)\n",
    "    q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "    for i in range(len(p_mw)):\n",
    "        min_p_mw_val.append(p_mw[i])\n",
    "        max_p_mw_val.append(p_mw[i])\n",
    "        min_q_mvar_val.append(q_mvar[i])\n",
    "        max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "    net.load.min_p_mw = min_p_mw_val\n",
    "    net.load.max_p_mw = max_p_mw_val\n",
    "    net.load.min_q_mvar = min_q_mvar_val\n",
    "    net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "    #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "    ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "    pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "    #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "    #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "    #NETWORK CONSTRAINTS\n",
    "\n",
    "    #Maximize the branch limits\n",
    "\n",
    "    #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "    #for i in range(len(max_i_ka)):\n",
    "    # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "    #Maximize line loading percents\n",
    "    max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo loading percent\n",
    "    max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo3w loading percent\n",
    "    max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Cost assignment\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "    #ac_converged = True\n",
    "\n",
    "    #start_vec_name = \"\"\n",
    "    #for init in [\"pf\", \"flat\", \"results\"]:\n",
    "    #    try:\n",
    "    #        pp.runopp(net, init=init)  # Calculate ACOPF with IPFOPT\n",
    "    #    except pp.OPFNotConverged:\n",
    "    #        if init == \"results\":\n",
    "    #            text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \". SKIPPING THIS GRID.\"\n",
    "    #            print(text)\n",
    "    #            break\n",
    "    #        continue\n",
    "    #    start_vec_name = init\n",
    "    #    ac_converged = True\n",
    "    #    break\n",
    "    #if ac_converged:\n",
    "    #    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + start_vec_name + \".\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        pp.runpm_ac_opf(net) # Run DCOPP\n",
    "    except pp.OPFNotConverged:\n",
    "        text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "        print(text)\n",
    "        continue\n",
    "    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + \".\")\n",
    "    grids_ready.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_acopf_available_grid_names = [\"1-HV-mixed--0-no_sw\",\"1-HV-urban--0-no_sw\", \"1-MV-comm--0-no_sw\", \"1-MV-semiurb--0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_acopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf_and_acopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_available_grid_names = [\"1-HVMV-mixed-all-0-no_sw\", \"1-HVMV-mixed-1.105-0-no_sw\", \"1-HVMV-mixed-2.102-0-no_sw\",\"1-HVMV-mixed-4.101-0-no_sw\", \"1-HVMV-urban-all-0-no_sw\", \"1-HVMV-urban-2.203-0-no_sw\", \"1-HVMV-urban-3.201-0-no_sw\", \"1-HVMV-urban-4.201-0-no_sw\", \"1-HV-mixed--0-no_sw\", \"1-HV-urban--0-no_sw\", \"1-MVLV-rural-all-0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Unsupervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this revised version of the compute_node_embeddings function, the node features are weighted by the reverse admittance values in the adjacency matrix before they are summed to compute the node embeddings. The resulting node embeddings will reflect the strength of the connections between the nodes.\n",
    "\n",
    "To use the reverse admittance values as the edge weights, you would need to pass the Ybus matrix as the adjacency matrix when calling the compute_node_embeddings function. The Ybus matrix should be converted to a PyTorch tensor before passing it to the function.\n",
    "\n",
    "use the reverse admittance values as edge weights, you can modify the computation of the node embeddings to weight the node features by the reverse admittance values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the node types\n",
    "node_types = ['Slack Node', 'Generator Node', 'Load Node']\n",
    "\n",
    "# Define the number of nodes of each type in the graph\n",
    "num_nodes = {\n",
    "    'Slack Node': 1,\n",
    "    'Generator Node': 20,\n",
    "    'Load Node': 99\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_names = [_ for _ in os.listdir(os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\\")]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphdata_lst = read_multiple_supervised_datasets(grid_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(4, 256, num_layers, 4, dropout=0.0, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=\"last\",layer_type=\"TransConv\", activation=\"elu\")#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_datasets_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "    run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"OPF-GNN-Supervised-Homo\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Homo-GNN\",\n",
    "    \"num_layers\": num_layers,\n",
    "    \"dataset\": grid_names[i],\n",
    "    \"epochs\": 1000,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"output_channels\": out_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"channel type\": layer_type,\n",
    "    \"norm\": \"BatchNorm\",\n",
    "    \"scaler\": scaler\n",
    "\n",
    "    }\n",
    "    )\n",
    "    num_epochs = wandb.run.config.epochs\n",
    "    run.watch(model)\n",
    "    grid_name = graphdata.grid_name\n",
    "    run.config.dataset = grid_name\n",
    "    train_data = graphdata.train_data\n",
    "    run.config[\"number of busses\"] = np.shape(train_data[0].x)[0]\n",
    "    val_data = graphdata.val_data\n",
    "    test_data = graphdata.test_data\n",
    "    test_datasets_lst.append(test_data)\n",
    "    training_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    validation_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "    #test_loader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "    for _ in range(num_epochs):\n",
    "        #train_one_epoch(i, optimizer, training_loader, model, nn.MSELoss(), edge_index, edge_weights)\n",
    "        train_validate_one_epoch(_, grid_name, optimizer, training_loader, validation_loader, model, nn.MSELoss(), scaler)\n",
    "    print(\"Training and Validation finished \" + \"for GraphData \" + str(i) + \".\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader_lst = []\n",
    "val_loader_lst = []\n",
    "test_loader_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "\n",
    "    # Divide training data into chunks, load into Dataloaders and append to the list of training loaders\n",
    "    for train_data in divide_chunks(graphdata.train_data, 5):\n",
    "        train_loader_lst.append(DataLoader(train_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Divide validation data into chunks, load into Dataloaders and append to the list of validation loaders\n",
    "    for val_data in divide_chunks(graphdata.val_data, 5):\n",
    "        val_loader_lst.append(DataLoader(val_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Append the test data to the list\n",
    "    for test_data in divide_chunks(graphdata.test_data, 5):\n",
    "        test_loader_lst.append(DataLoader(test_data, batch_size=1, shuffle=True))\n",
    "\n",
    "print(\"Data Preparation finished.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr/100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"OPF-GNN-Supervised-Homo\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Homo-GNN\",\n",
    "    \"num_layers\": num_layers,\n",
    "    \"epochs\": 2000,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"output_channels\": out_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"channel type\": layer_type,\n",
    "    \"norm\": \"BatchNorm\",\n",
    "    \"scaler\": scaler\n",
    "    }\n",
    "    )\n",
    "\"\"\"\n",
    "for _ in range(wandb.run.config.epochs):\n",
    "    # Training\n",
    "    random.shuffle(train_loader_lst)\n",
    "    print(\"Training the model for epoch \" + str(_))\n",
    "    train_all_one_epoch(_, optimizer, train_loader_lst, model, nn.MSELoss())\n",
    "\n",
    "    # Validation\n",
    "    random.shuffle(val_loader_lst)\n",
    "    print(\"Validating the model for epoch \" + str(_))\n",
    "    validate_all_one_epoch(_, val_loader_lst, model, nn.MSELoss())\n",
    "\n",
    "    outputs = test_all_one_epoch(test_loader_lst, model, nn.MSELoss())\n",
    "print(\"Training and Validation finished.\")\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = test_all_one_epoch(test_loader_lst, model, nn.MSELoss())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output,target = outputs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\" #os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\Models\\\\Supervised\\\\\" + \"basemodel.pt\" #r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\"\n",
    "torch.save(model.state_dict(), \"supervisedmodel.pt\")\n",
    "print(\"done\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'/'.join(os.path.dirname(os.path.abspath(\"gnn.ipynb\")).split(\"\\\\\"))+ r\"/Models/SelfSupervised/base_model.pt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandapower.plotting.simple_plot import simple_plot\n",
    "from pandapower.plotting.plotly.simple_plotly import simple_plotly\n",
    "#simple_plot(net, plot_gens=True, plot_loads=True, plot_sgens=True, library=\"igraph\")\n",
    "simple_plotly(net, map_style=\"satellite\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runopp(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx_mapper, node_types_as_dict = extract_node_types_as_dict(net)\n",
    "node_types_as_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Module.parameters of GNN(4, 4, num_layers=5)>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "net = process_network(grid_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " HETEROGENEOUS GNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "index_mappers,net,data = generate_unsupervised_input('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    }
   ],
   "source": [
    "x_dict, constraint_dict, edge_idx_dict, edge_attr_dict, bus_idx_neighbors_dict, scaler, angle_params, res_bus_dict = extract_unsupervised_inputs(data, net, index_mappers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "for from_bus, val in bus_idx_neighbors_dict.items():\n",
    "    # Sorting the dictionary by its keys\n",
    "    sorted_data = {k: val[k] for k in sorted(val.keys())}\n",
    "    bus_idx_neighbors_dict[from_bus] = sorted_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: [('PV', 5, tensor([7.3469e-06, 1.7959e+00]))],\n 1: [('PV', 2, tensor([4.2778e-06, 1.3333e+00]))],\n 2: [('PV', 33, tensor([0.0719, 0.1944])),\n  ('PV', 20, tensor([0.2999, 0.8108])),\n  ('PV', 3, tensor([0.5696, 1.5398])),\n  ('PV', 41, tensor([0.8170, 2.2086])),\n  ('PV', 11, tensor([1.3666, 3.6942])),\n  ('PV', 1, tensor([4.2778e-06, 1.3333e+00]))],\n 3: [('PV', 22, tensor([0.5483, 1.4821])),\n  ('PV', 2, tensor([0.5696, 1.5398]))],\n 4: [('PV', 26, tensor([0.3375, 0.9123]))],\n 5: [('PQ', 10, tensor([ 3.7512, 10.1401])),\n  ('PV', 27, tensor([0.6237, 1.6859])),\n  ('PV', 16, tensor([2.4927, 6.7384])),\n  ('PV', 40, tensor([1.9532, 5.2799])),\n  ('PV', 0, tensor([7.3469e-06, 1.7959e+00]))],\n 6: [('PV', 21, tensor([0.4586, 1.2397]))],\n 7: [('PQ', 4, tensor([2.1370, 5.7768])),\n  ('PV', 15, tensor([0.8511, 2.3007]))],\n 8: [('PQ', 6, tensor([0.6969, 1.8839]))],\n 9: [('PV', 28, tensor([1.4714, 3.9775])),\n  ('PV', 30, tensor([0.4157, 1.1238]))],\n 10: [('PV', 21, tensor([2.6916, 7.2760])),\n  ('PV', 11, tensor([2.1478, 5.8060]))],\n 11: [('PV', 10, tensor([2.1478, 5.8060])),\n  ('PV', 2, tensor([1.3666, 3.6942]))],\n 12: [('PV', 27, tensor([1.0758, 2.9081]))],\n 13: [('PV', 14, tensor([1.2198, 3.2974]))],\n 14: [('NB', 0, tensor([1.0761, 2.9089])),\n  ('PV', 13, tensor([1.2198, 3.2974]))],\n 15: [('PQ', 6, tensor([1.7621, 4.7632])),\n  ('NB', 4, tensor([0.5852, 1.5820])),\n  ('PV', 7, tensor([0.8511, 2.3007]))],\n 16: [('PV', 5, tensor([2.4927, 6.7384]))],\n 17: [('PQ', 11, tensor([0.1192, 0.3221])),\n  ('PQ', 1, tensor([2.2874, 6.1833])),\n  ('PV', 39, tensor([1.1915, 3.2209]))],\n 18: [('PV', 31, tensor([0.2041, 0.5518])),\n  ('PV', 39, tensor([0.9938, 2.6863]))],\n 19: [('PV', 41, tensor([1.1007, 2.9754]))],\n 20: [('PV', 2, tensor([0.2999, 0.8108])),\n  ('PV', 25, tensor([1.4386, 3.8887]))],\n 21: [('PQ', 1, tensor([1.5287, 4.1324])),\n  ('PV', 10, tensor([2.6916, 7.2760])),\n  ('PV', 6, tensor([0.4586, 1.2397]))],\n 22: [('PQ', 0, tensor([0.5434, 1.4688])),\n  ('PV', 26, tensor([0.5569, 1.5055])),\n  ('PV', 3, tensor([0.5483, 1.4821]))],\n 23: [('PQ', 1, tensor([0.8772, 2.3713])),\n  ('PQ', 3, tensor([0.1755, 0.1763]))],\n 24: [('PQ', 14, tensor([0.1643, 0.4440])),\n  ('PQ', 2, tensor([0.3369, 0.9108])),\n  ('PV', 35, tensor([0.1239, 0.3348]))],\n 25: [('PV', 20, tensor([1.4386, 3.8887]))],\n 26: [('PV', 22, tensor([0.5569, 1.5055])),\n  ('PV', 4, tensor([0.3375, 0.9123])),\n  ('PV', 36, tensor([0.1787, 0.4829]))],\n 27: [('PQ', 12, tensor([1.1997, 3.2430])),\n  ('PV', 12, tensor([1.0758, 2.9081])),\n  ('PV', 5, tensor([0.6237, 1.6859]))],\n 28: [('PV', 9, tensor([1.4714, 3.9775]))],\n 29: [('PQ', 15, tensor([0.2171, 0.5869])),\n  ('PV', 30, tensor([0.6109, 1.6514])),\n  ('PV', 34, tensor([3.2143, 8.6889]))],\n 30: [('PV', 29, tensor([0.6109, 1.6514])),\n  ('PV', 9, tensor([0.4157, 1.1238])),\n  ('PV', 37, tensor([0.6679, 1.8055]))],\n 31: [('PV', 18, tensor([0.2041, 0.5518])),\n  ('PV', 32, tensor([1.5459, 4.1788]))],\n 32: [('PQ', 7, tensor([1.6194, 4.3776])),\n  ('PV', 38, tensor([0.0741, 0.2003])),\n  ('PV', 31, tensor([1.5459, 4.1788]))],\n 33: [('PV', 2, tensor([0.0719, 0.1944]))],\n 34: [('PV', 36, tensor([0.3279, 0.8865])),\n  ('PV', 29, tensor([3.2143, 8.6889]))],\n 35: [('PV', 24, tensor([0.1239, 0.3348]))],\n 36: [('PV', 34, tensor([0.3279, 0.8865])),\n  ('PV', 26, tensor([0.1787, 0.4829]))],\n 37: [('PV', 30, tensor([0.6679, 1.8055]))],\n 38: [('PV', 32, tensor([0.0741, 0.2003]))],\n 39: [('PV', 17, tensor([1.1915, 3.2209])),\n  ('PV', 18, tensor([0.9938, 2.6863]))],\n 40: [('PV', 5, tensor([1.9532, 5.2799]))],\n 41: [('PV', 19, tensor([1.1007, 2.9754])),\n  ('PV', 2, tensor([0.8170, 2.2086]))]}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_idx_neighbors_dict[\"PV\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SB', 'isConnected', 'PQ') : 1\n",
      "('PV', 'isConnected', 'PQ') : 15\n",
      "('PV', 'isConnected', 'NB') : 2\n",
      "('PQ', 'isConnected', 'NB') : 3\n",
      "('PQ', 'isConnected', 'SB') : 1\n",
      "('PQ', 'isConnected', 'PV') : 15\n",
      "('NB', 'isConnected', 'PV') : 2\n",
      "('NB', 'isConnected', 'PQ') : 3\n",
      "('PV', 'isConnected', 'PV') : 36\n",
      "('PQ', 'isConnected', 'PQ') : 7\n",
      "('NB', 'isConnected', 'NB') : 2\n"
     ]
    }
   ],
   "source": [
    "for edge_type in edge_idx_dict:\n",
    "    print(f\"{edge_type} : {len(edge_idx_dict[edge_type][0])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SB', 'isConnected', 'PQ'):tensor([[0],\n",
      "        [0]])\n",
      "('PV', 'isConnected', 'PQ'):tensor([[ 0,  1,  2,  3,  4,  5,  6,  6,  8,  9, 10, 11,  5,  1, 14],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  5,  9, 10, 11, 12,  5,  0]])\n",
      "('PV', 'isConnected', 'NB'):tensor([[0, 1],\n",
      "        [0, 1]])\n",
      "('PQ', 'isConnected', 'NB'):tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n",
      "('PQ', 'isConnected', 'SB'):tensor([[0],\n",
      "        [0]])\n",
      "('PQ', 'isConnected', 'PV'):tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  5,  9, 10, 11, 12,  5,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  6,  8,  9, 10, 11,  5,  1, 14]])\n",
      "('NB', 'isConnected', 'PV'):tensor([[0, 1],\n",
      "        [0, 1]])\n",
      "('NB', 'isConnected', 'PQ'):tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n",
      "('PV', 'isConnected', 'PV'):tensor([[ 0,  1,  2,  3,  2,  5,  6,  5,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 17,  0, 23, 23,  5, 26,  6, 28,  5, 18, 31, 32,  5, 34, 23],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  0,  9, 10, 11, 12, 13,  9, 10, 16, 17,\n",
      "         18, 19, 20, 18, 22, 16, 24, 25, 26, 27, 28, 22, 30, 31, 27, 33, 28, 35]])\n",
      "('PQ', 'isConnected', 'PQ'):tensor([[0, 1, 2, 3, 1, 1, 6],\n",
      "        [0, 1, 2, 2, 0, 5, 6]])\n",
      "('NB', 'isConnected', 'NB'):tensor([[0, 1],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for edge_type, edge_index in edge_idx_dict.items():\n",
    "    src, rel, dst = edge_type\n",
    "    edge_attr = edge_attr_dict[edge_type]\n",
    "\n",
    "    mapper_from = dict()\n",
    "    mapper_to = dict()\n",
    "\n",
    "    for j in range(len(edge_index[0])):\n",
    "        from_idx = edge_index[0][j].item()\n",
    "\n",
    "        if from_idx not in mapper_from:\n",
    "            mapper_from[from_idx] = j\n",
    "\n",
    "\n",
    "    for k in range(len(edge_index[1])):\n",
    "        to_idx = edge_index[1][k].item()\n",
    "\n",
    "        if to_idx not in mapper_to:\n",
    "            mapper_to[to_idx] = k\n",
    "\n",
    "    from_idx = [mapper_from[key.item()] for key in edge_index[0]]\n",
    "    to_idx = [mapper_to[key.item()] for key in edge_index[1]]\n",
    "\n",
    "    print(f\"{edge_type}:{torch.tensor([from_idx, to_idx])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  2,  2, 31],\n        [ 1,  2, 25, 31]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1,2,2,31],[1,2,25,31],[11,2,32,531],[1,22,2,311]]\n",
    "#y = list(reversed(x))\n",
    "t = torch.tensor(x)\n",
    "t[torch.tensor([0,1])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "{4: [('PV', 15, tensor([0.5852, 1.5820]))],\n 0: [('PV', 14, tensor([1.0761, 2.9089])),\n  ('PQ', 12, tensor([2.1526, 5.8188])),\n  ('NB', 3, tensor([3.6198, 9.7850]))],\n 2: [('PQ', 9, tensor([1.3074, 3.5341]))],\n 1: [('PQ', 7, tensor([0.0587, 0.1587])), ('NB', 3, tensor([1.8857, 5.0973]))],\n 3: [('NB', 1, tensor([1.8857, 5.0973])), ('NB', 0, tensor([3.6198, 9.7850]))]}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_idx_neighbors_dict[\"NB\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(torch.Tensor,\n            {'PQ': tensor([[-0.9727,  0.4975,  0.1855,  0.1735],\n                     [-0.9881, -0.3441,  0.1876,  0.1740],\n                     [-0.9433, -0.1555,  0.5085,  0.3566],\n                     [-0.8736, -0.6474,  0.8787,  0.5507],\n                     [-0.9996,  2.1459,  0.1823,  0.1737],\n                     [-0.7947, -0.3202,  0.1851,  0.1765],\n                     [-1.0000,  6.4361,  0.7988,  0.5809],\n                     [-0.8129, -0.4006,  0.1863,  0.1734],\n                     [-0.9954,  4.7195,  0.1867,  0.1750],\n                     [-1.0000,  3.5074,  0.1859,  0.1759],\n                     [-0.9077, -0.3457,  0.5017,  0.3234],\n                     [-0.9938,  0.8982,  0.4428,  0.3478],\n                     [ 0.1649,  0.0599,  0.1850,  0.1719],\n                     [-0.9517,  1.1979,  0.1877,  0.1745],\n                     [-0.9716,  0.3346,  0.1855,  0.1764]], grad_fn=<CatBackward0>),\n             'NB': tensor([[-0.5042, -0.9679,  0.1318,  0.1443],\n                     [-0.4228, -0.5739,  0.1318,  0.1443],\n                     [-0.0776, -0.9174,  0.1318,  0.1443],\n                     [-0.9629, -0.8821,  0.1318,  0.1443],\n                     [-0.7104, -0.5152,  0.1318,  0.1443]], grad_fn=<CatBackward0>),\n             'SB': tensor([[-0.7722,  0.6120,  0.1318,  0.1443]], grad_fn=<CatBackward0>),\n             'PV': tensor([[-9.7774e-01, -8.5157e-01, -2.8817e+00, -4.3336e+00],\n                     [-4.5090e-01, -7.3275e-01, -2.4512e+00, -3.6939e+00],\n                     [-8.8442e-01,  2.3983e+00,  5.7386e-02,  1.0016e-01],\n                     [-2.7672e-01,  9.2331e-01, -7.4906e-02,  2.2673e-02],\n                     [-1.6263e-01,  8.0254e-01,  2.2714e-02,  8.2273e-02],\n                     [-9.9844e-01,  1.2012e+01, -1.1516e+00, -6.0946e-01],\n                     [-4.7029e-02,  7.0152e-01, -2.9466e-01, -1.0793e-01],\n                     [-8.0817e-01,  5.4861e+00, -1.4915e+00, -8.1052e-01],\n                     [-6.1014e-01,  2.7211e+00, -2.0738e-01, -5.5428e-02],\n                     [-1.5199e-01,  7.9273e-01, -7.6784e-03,  6.2356e-02],\n                     [-2.5229e-02,  6.8396e-01, -3.4897e-01, -1.4393e-01],\n                     [-8.9097e-01,  2.1765e+00, -1.1272e-01,  1.8092e-03],\n                     [ 2.3027e-01,  4.8371e-01, -9.8136e-01, -5.0980e-01],\n                     [-4.4762e-02,  6.9927e-01, -3.0475e-01, -1.0172e-01],\n                     [-4.4939e-01, -9.5437e-02,  7.5322e-02,  1.1110e-01],\n                     [-9.5586e-01,  5.3520e+00, -5.7378e-01, -2.6986e-01],\n                     [-9.7663e-01,  3.5803e+00,  3.7141e-02,  8.9300e-02],\n                     [-8.7474e-01,  4.7665e+00, -4.2391e-02,  4.2921e-02],\n                     [-4.3252e-01,  2.7053e+00, -6.9198e-01, -3.4512e-01],\n                     [-6.1359e-01,  1.3603e+00, -2.5587e-02,  5.1840e-02],\n                     [-5.7900e-01,  1.2790e+00, -6.1336e-01, -2.8357e-01],\n                     [ 1.8788e-01,  5.6472e-01, -4.4179e-01, -1.9509e-01],\n                     [-9.8367e-01,  6.0517e+00,  6.1374e-02,  1.0291e-01],\n                     [-5.9596e-01,  2.7398e+00,  2.2123e-02,  7.7446e-02],\n                     [-4.3141e-01,  2.3136e+00, -2.9866e-01, -9.7611e-02],\n                     [-3.5326e-01,  1.8666e+00,  6.1353e-02,  1.0200e-01],\n                     [-8.4697e-01,  2.1156e+00,  5.5587e-02,  9.9328e-02],\n                     [-2.0836e-01,  8.7470e-01,  3.4563e-02,  8.8339e-02],\n                     [-9.2869e-01,  5.0721e+00, -2.1648e-01, -6.0706e-02],\n                     [-7.7225e-01,  1.7022e+00, -3.2151e-01, -1.2195e-01],\n                     [-2.9127e-01,  1.8119e+00,  5.1375e-02,  9.8303e-02],\n                     [-2.7762e-01,  9.3781e-01,  5.2180e-02,  9.6424e-02],\n                     [ 9.2224e-02,  6.7261e-01, -2.4821e-02,  5.3573e-02],\n                     [-8.4763e-01,  2.0518e+00, -3.4189e-01, -1.3095e-01],\n                     [ 3.0258e-01,  5.1016e-01,  3.0583e-02,  8.4228e-02],\n                     [-9.7691e-01,  3.2098e+00, -3.5932e-01, -1.4345e-01],\n                     [ 2.4285e-01,  5.4805e-01,  1.1315e-02,  7.3480e-02],\n                     [-1.0202e-01,  8.3838e-01,  3.2260e-02,  8.5170e-02],\n                     [-4.0031e-01,  1.0560e+00,  5.2399e-02,  9.6106e-02],\n                     [ 6.5637e-02,  7.8332e-01, -4.1604e-02,  4.3883e-02],\n                     [-1.5517e-01,  7.9563e-01,  1.1319e-03,  6.8834e-02],\n                     [-1.2226e-01,  7.6581e-01, -9.2208e-02,  1.1550e-02],\n                     [-1.5127e-01,  7.9204e-01, -1.0091e-02,  6.1869e-02]],\n                    grad_fn=<CatBackward0>)})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder_model(x_dict, constraint_dict, edge_idx_dict, edge_attr_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0.23723669614964715"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.167**2 + 0.1685**2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 342.       418.       350.      -350.       350.      -350.\n",
      "   350.00003]]\n",
      "[[ 99.        121.          3.1069336   2.8781586   2.8781586   1.1700897\n",
      "    1.1700897]\n",
      " [ 99.        121.          3.257019    3.0235138   3.0235138   1.2110443\n",
      "    1.2110443]\n",
      " [ 99.        121.         37.16533     8.023071   34.423065    3.5772247\n",
      "   14.011185 ]\n",
      " [ 99.        121.         51.020508   33.0876     46.6476     15.307114\n",
      "   20.66623  ]\n",
      " [ 99.        121.          3.0006409   2.704483    2.704483    1.2998047\n",
      "    1.2998047]\n",
      " [ 99.        121.          3.2549133   3.059372    3.059372    1.1112213\n",
      "    1.1112213]\n",
      " [ 99.        121.         49.814194   31.622696   45.182693   15.616501\n",
      "   20.975632 ]\n",
      " [ 99.        121.          3.423462    3.208374    3.208374    1.1942902\n",
      "    1.1942902]\n",
      " [ 99.        121.          3.0209045   2.8138428   2.8138428   1.0991058\n",
      "    1.0991058]\n",
      " [ 99.        121.          3.4769897   3.2351685   3.2351685   1.2740631\n",
      "    1.2740631]\n",
      " [ 99.        121.         31.884735    5.5887756  29.388763    2.9603271\n",
      "   12.366684 ]\n",
      " [ 99.        121.         32.33777     6.388916   30.18892     2.185028\n",
      "   11.5914   ]\n",
      " [ 99.        121.          3.4465942   3.223648    3.223648    1.2194519\n",
      "    1.2194519]\n",
      " [ 99.        121.          2.9874878   2.776535    2.776535    1.1026306\n",
      "    1.1026306]\n",
      " [ 99.        121.          2.9378662   2.7017365   2.7017365   1.1539612\n",
      "    1.1539612]]\n",
      "[[ 3.42000000e+02  4.18000000e+02  3.50000000e+02 -3.50000000e+02\n",
      "  -1.52587891e-05 -3.50000000e+02  1.52587891e-05]\n",
      " [ 1.98000000e+02  2.42000000e+02  3.00000000e+02 -3.00000000e+02\n",
      "  -1.52587891e-05 -3.00000000e+02  1.52587891e-05]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.10754395e+00 -5.67999268e+00\n",
      "  -2.70428467e+00 -1.25527954e+00 -2.24487305e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  2.23441010e+01 -2.07800140e+01\n",
      "  -2.90278625e+00 -1.30058289e+00 -8.21281433e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.02581177e+01 -9.54000854e+00\n",
      "  -3.05505371e+00 -1.07026672e+00 -3.77052307e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.60280014e+02 -1.49059998e+02\n",
      "  -1.52587891e-05  1.52587891e-05 -5.89134369e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.39892426e+01 -3.16100006e+01\n",
      "  -1.89137115e+01 -6.66284180e+00 -1.24929810e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.70806427e+02 -1.58850006e+02\n",
      "  -3.03179932e+01 -1.35118561e+01 -6.27813721e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.90108032e+01 -3.62799988e+01\n",
      "  -3.11334229e+00 -1.28262329e+00 -1.43388977e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.74194183e+01 -1.61999969e+01\n",
      "  -1.52587891e-05  1.52587891e-05 -6.40275574e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  4.15376129e+01 -3.86300049e+01\n",
      "  -1.68571930e+01 -7.40226746e+00 -1.52674561e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  2.70538025e+01 -2.51600037e+01\n",
      "  -3.01489258e+00 -1.22120667e+00 -9.94395447e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.35645020e+02 -1.26150009e+02\n",
      "  -3.19332886e+00 -1.18431091e+00 -4.98572235e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.43118134e+01 -3.19100037e+01\n",
      "  -1.65105896e+01 -6.15379333e+00 -1.26115570e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  7.05377197e+00 -6.56001282e+00\n",
      "  -1.52587891e-05  1.52587891e-05 -2.59266663e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.78279114e+01 -6.30800095e+01\n",
      "  -1.76498260e+01 -6.88507080e+00 -2.49306335e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  8.46238708e+00 -7.87001038e+00\n",
      "  -3.05123901e+00 -1.14131165e+00 -3.11041260e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.84516144e+01 -1.71600037e+01\n",
      "  -3.27821350e+00 -1.17807007e+00 -6.78202820e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.36668549e+01 -4.92400055e+01\n",
      "  -4.84744797e+01 -2.13436737e+01 -1.94607849e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.96558990e+01 -1.82799988e+01\n",
      "  -1.52587891e-05  1.52587891e-05 -7.22467041e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  7.37203979e+01 -6.85599976e+01\n",
      "  -1.61134033e+01 -7.38481140e+00 -2.70965576e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.32795105e+01 -4.95500031e+01\n",
      "  -1.73746796e+01 -6.95779419e+00 -1.95832214e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  8.79570007e+00 -8.18000793e+00\n",
      "  -1.52587891e-05  1.52587891e-05 -3.23294067e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.07419128e+01 -9.99002075e+00\n",
      "  -2.93855286e+00 -1.27900696e+00 -3.94818115e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  3.36128845e+01 -3.12600098e+01\n",
      "  -1.58906250e+01 -6.95913696e+00 -1.23546753e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.44085693e+00 -5.05999756e+00\n",
      "  -2.73703003e+00 -1.09313965e+00 -1.99983215e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.18280029e+00 -5.75000000e+00\n",
      "  -3.27565002e+00 -1.08125305e+00 -2.27253723e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  8.92474365e+00 -8.30001831e+00\n",
      "  -3.24577332e+00 -1.19587708e+00 -3.28036499e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  4.01828156e+01 -3.73700104e+01\n",
      "  -2.94935608e+00 -1.19841003e+00 -1.47695618e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.66129150e+01 -5.26500092e+01\n",
      "  -1.52587891e-05  1.52587891e-05 -2.08085938e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.80645752e+00 -6.33000183e+00\n",
      "  -2.77929688e+00 -1.22323608e+00 -2.50175476e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.93548584e+00 -6.44999695e+00\n",
      "  -2.98042297e+00 -1.17948914e+00 -2.54917908e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.62688141e+01 -1.51300049e+01\n",
      "  -3.23254395e+00 -1.09085083e+00 -5.97970581e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  4.12580109e+01 -3.83700104e+01\n",
      "  -1.73695374e+01 -6.78463745e+00 -1.51645966e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  9.63442993e+00 -8.96000671e+00\n",
      "  -3.09129333e+00 -1.14685059e+00 -3.54119873e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  5.80753021e+01 -5.40100021e+01\n",
      "  -3.20039368e+00 -1.29483032e+00 -2.13461914e+01]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.20108032e+01 -1.11700134e+01\n",
      "  -3.07546997e+00 -1.19583130e+00 -4.41477966e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  9.04301453e+00 -8.41000366e+00\n",
      "  -2.99108887e+00 -1.26058960e+00 -3.32382202e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  6.84945679e+00 -6.36999512e+00\n",
      "  -3.14657593e+00 -1.17970276e+00 -2.51757812e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.82687988e+01 -1.69900055e+01\n",
      "  -3.12025452e+00 -1.22277832e+00 -6.71482849e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.29140167e+01 -1.20100098e+01\n",
      "  -2.92842102e+00 -1.26052856e+00 -4.74670410e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  2.47419128e+01 -2.30100098e+01\n",
      "  -2.88661194e+00 -1.24166870e+00 -9.09402466e+00]\n",
      " [ 9.90000000e+01  1.21000000e+02  1.44839020e+01 -1.34700165e+01\n",
      "  -3.08052063e+00 -1.30276489e+00 -5.32376099e+00]]\n",
      "[[ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]\n",
      " [ 9.9000000e+01  1.2100000e+02  0.0000000e+00 -1.5258789e-05\n",
      "  -1.5258789e-05  1.5258789e-05  1.5258789e-05]]\n"
     ]
    }
   ],
   "source": [
    "for node_type in constraint_dict:\n",
    "    print(custom_standard_inverse_transform(scaler, constraint_dict[node_type].detach().numpy()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ACOPFOutput(x_dict, scalers_dict, None, index_mappers).output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_names = sb.collect_all_simbench_codes()[25:]\n",
    "acopf_ok_grid_names = []\n",
    "\n",
    "for grid_name in grid_names:\n",
    "    print(f\"Trying grid {grid_name}\")\n",
    "    net = process_network(grid_name)\n",
    "\n",
    "    # try:\n",
    "    #     acopf_ok_grid_names.append(grid_name)\n",
    "    #     pp.runpm_ac_opf(net)\n",
    "    #     #print(net.res_bus)\n",
    "    # except:\n",
    "    #     print(f\"Julia didnt converge for {grid_name}\")\n",
    "    #     acopf_ok_grid_names.remove(grid_name)\n",
    "    try:\n",
    "        acopf_ok_grid_names.append(grid_name)\n",
    "        pp.runopp(net)\n",
    "        #print(net.res_bus)\n",
    "    except:\n",
    "        print(f\"OPP didnt converge for {grid_name}\")\n",
    "        acopf_ok_grid_names.remove(grid_name)\n",
    "acopf_ok_grid_names\n",
    "#save_multiple_unsupervised_inputs(grid_names, 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "data": {
      "text/plain": "        vm_pu   va_degree      p_mw     q_mvar     lam_p         lam_q\n0    1.025000    0.000000 -8.057173 -11.835759  1.000000  2.259515e-21\n2    1.001425  209.119600 -1.176327   0.135443  1.000714  1.887024e-03\n4    0.999942  209.185313  0.397800   0.183689  1.002495  4.950700e-03\n5    0.999154  209.227646  0.293150   0.148678  1.003357  6.811706e-03\n6    0.998564  209.265084  0.239906   0.171257  1.003935  8.382988e-03\n..        ...         ...       ...        ...       ...           ...\n112  0.987459  209.245271  0.237492   0.096853  1.020719  1.774877e-02\n113  0.987333  209.251486  0.036984   0.085200  1.020904  1.802879e-02\n114  0.987261  209.254787 -0.074431   0.034476  1.021013  1.818089e-02\n115  0.987149  209.258063  0.118649   0.137547  1.021198  1.835478e-02\n116  0.987060  209.260287  0.159486   0.137561  1.021350  1.848019e-02\n\n[115 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vm_pu</th>\n      <th>va_degree</th>\n      <th>p_mw</th>\n      <th>q_mvar</th>\n      <th>lam_p</th>\n      <th>lam_q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.025000</td>\n      <td>0.000000</td>\n      <td>-8.057173</td>\n      <td>-11.835759</td>\n      <td>1.000000</td>\n      <td>2.259515e-21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.001425</td>\n      <td>209.119600</td>\n      <td>-1.176327</td>\n      <td>0.135443</td>\n      <td>1.000714</td>\n      <td>1.887024e-03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.999942</td>\n      <td>209.185313</td>\n      <td>0.397800</td>\n      <td>0.183689</td>\n      <td>1.002495</td>\n      <td>4.950700e-03</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.999154</td>\n      <td>209.227646</td>\n      <td>0.293150</td>\n      <td>0.148678</td>\n      <td>1.003357</td>\n      <td>6.811706e-03</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.998564</td>\n      <td>209.265084</td>\n      <td>0.239906</td>\n      <td>0.171257</td>\n      <td>1.003935</td>\n      <td>8.382988e-03</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>0.987459</td>\n      <td>209.245271</td>\n      <td>0.237492</td>\n      <td>0.096853</td>\n      <td>1.020719</td>\n      <td>1.774877e-02</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>0.987333</td>\n      <td>209.251486</td>\n      <td>0.036984</td>\n      <td>0.085200</td>\n      <td>1.020904</td>\n      <td>1.802879e-02</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>0.987261</td>\n      <td>209.254787</td>\n      <td>-0.074431</td>\n      <td>0.034476</td>\n      <td>1.021013</td>\n      <td>1.818089e-02</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>0.987149</td>\n      <td>209.258063</td>\n      <td>0.118649</td>\n      <td>0.137547</td>\n      <td>1.021198</td>\n      <td>1.835478e-02</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>0.987060</td>\n      <td>209.260287</td>\n      <td>0.159486</td>\n      <td>0.137561</td>\n      <td>1.021350</td>\n      <td>1.848019e-02</td>\n    </tr>\n  </tbody>\n</table>\n<p>115 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acopf_ok_grid_names = ['1-HV-mixed--1-sw','1-MV-semiurb--0-no_sw','1-MV-comm--0-sw','1-MV-comm--0-no_sw']\n",
    "net = process_network(acopf_ok_grid_names[1])\n",
    "pp.runopp(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acopf_grid_names = ['1-HV-mixed--0-no_sw','1-MV-semiurb--0-no_sw', '1-MV-comm--0-no_sw']\n",
    "save_multiple_unsupervised_inputs(acopf_grid_names, num_samples=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "inputs = load_multiple_unsupervised_inputs()\n",
    "for i,input in enumerate(inputs):\n",
    "    if input.res_bus is None:\n",
    "        print(f\"None at {i}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_unsupervised_inputs('1-HV-mixed--0-no_sw', 300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = load_unsupervised_inputs('1-HV-mixed--0-no_sw')\n",
    "inputs[0].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs[57].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display Plot of Customized Sigmoid Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as p\n",
    "import torch\n",
    "\n",
    "def custom_tanh(x: torch.Tensor, lower_bound: float, upper_bound: float) -> torch.Tensor:\n",
    "    width = upper_bound - lower_bound\n",
    "    return 0.5 * width * torch.tanh(x) + 0.5 * (upper_bound + lower_bound)\n",
    "\n",
    "\n",
    "lst = []\n",
    "min_val = 0\n",
    "max_val = 0.1\n",
    "range_ = []\n",
    "i = -2\n",
    "while i < 2:\n",
    "    range_.append(i)\n",
    "    i += 0.01\n",
    "\n",
    "for i in range_:\n",
    "    val = custom_tanh(torch.tensor(i), min_val, max_val)\n",
    "    #val = torch.tanh(torch.tensor(i))\n",
    "    lst.append(val)\n",
    "\n",
    "# Now, plot the values in lst\n",
    "p.plot(range_, lst)\n",
    "p.xlabel('Input')\n",
    "p.ylabel('Value')\n",
    "p.title('Plot of Enforcing Activation Function')\n",
    "p.grid(True)\n",
    "p.show()\n",
    "print(range_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Heterogeneous Self Supervised Model and Process Inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "embedder_model = load_ACOPFGNN_model(grid_name, \"embedder_model.pt\", hidden_channels=4, num_layers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create ACOPFGNN model and Optimizer\n",
    "index_mappers,net,data = generate_unsupervised_input(grid_name)\n",
    "model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "learning_rate = 2.5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "2321"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mcanbay96\u001B[0m (\u001B[33mcbml\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\wandb\\run-20231019_023541-i8m08wvo</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/i8m08wvo' target=\"_blank\">generous-salad-227</a></strong> to <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/i8m08wvo' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/i8m08wvo</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_NOTEBOOK_NAME = \"msi\"\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"ACOPF-GNN-SelfSupervised-Hetero\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            #\"learning_rate\": optimizer.,\n",
    "            \"architecture\": \"Hetero-SelfSupervised-GNN\",\n",
    "            \"num_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "            \"num_heads\": model.heads,\n",
    "            \"num_layers\": model.num_layers,\n",
    "            \"grid_name\": grid_name,\n",
    "            \"activation\": model.act_fn,\n",
    "            \"dropout\": model.dropout,\n",
    "            \"in_channels\": model.in_channels,\n",
    "            \"output_channels\": model.out_channels,\n",
    "            \"hidden_channels\": model.hidden_channels,\n",
    "            \"channel type\": \"TransformerConv\",\n",
    "            \"scaler\": \"MinMax\",\n",
    "            \"model\": \"base_model_wired\"\n",
    "        }\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load Inputs\n",
    "inputs = load_unsupervised_inputs(grid_name)\n",
    "#inputs = load_multiple_unsupervised_inputs()\n",
    "n = len(inputs)\n",
    "train_inputs = inputs[:int(n*0.8)]\n",
    "val_inputs = inputs[int(n*0.8): int(n*0.95)]\n",
    "test_inputs = inputs[int(n*0.95):]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'SB': tensor([[ 6.0855, -1.1532,  6.4307, -4.8642]]),\n 'PQ': tensor([[-0.2825,  1.3591,  0.1810,  0.1701],\n         [-0.1253,  0.1567,  0.1793,  0.1708],\n         [-0.1267,  0.1450,  0.3101,  0.2293],\n         [-0.1348,  0.1020,  0.7808,  0.5394],\n         [ 0.0020,  0.9540,  0.1793,  0.1688],\n         [-0.2854,  1.0763,  0.1809,  0.1702],\n         [-0.0670,  0.5362,  0.8061,  0.5207],\n         [-0.1101,  1.3001,  0.1545,  0.1648],\n         [-0.1840,  0.5053,  0.1789,  0.1704],\n         [-0.1266,  0.1638,  0.1796,  0.1689],\n         [-0.2876,  1.0932,  0.1815,  0.1698],\n         [-0.1107,  0.5591,  0.3072,  0.2274],\n         [-0.3032,  1.3241,  0.3103,  0.2452],\n         [-0.2848,  1.0805,  0.1771,  0.1708],\n         [-0.1255,  0.1539,  0.1796,  0.1717],\n         [-0.1149,  1.3686,  0.1814,  0.1689]]),\n 'PV': tensor([[ 4.5760e+00, -1.8228e-01,  5.3563e+00,  4.0652e+00],\n         [ 1.5969e+00,  6.8250e-01,  3.4548e+00,  6.6139e+00],\n         [-3.2272e-01,  1.2525e+00,  7.9409e-02,  1.1184e-01],\n         [-3.0089e-01,  1.3164e+00, -1.8874e-01, -4.3694e-02],\n         [-2.6203e-01,  1.4144e+00,  8.1061e-03,  7.0635e-02],\n         [-3.1635e-01,  1.2346e+00, -2.5190e+00, -1.4007e+00],\n         [-1.5385e-01,  4.2675e-01, -1.0354e-01,  4.7541e-03],\n         [ 3.9337e-03,  9.6895e-01, -2.1284e+00, -1.1557e+00],\n         [-5.8643e-02,  5.8700e-01, -4.6419e-01, -2.0291e-01],\n         [-8.9029e-02,  1.5592e+00, -1.6361e-01, -2.9483e-02],\n         [-2.2024e-01,  8.1573e-01, -2.3017e-01, -7.1563e-02],\n         [-2.7928e-01,  1.1016e+00, -2.6452e-01, -9.0405e-02],\n         [-2.7206e-01,  1.5350e+00, -2.0580e+00, -1.1318e+00],\n         [-2.3360e-01,  1.0583e+00, -1.3468e-01, -7.6614e-03],\n         [-2.4053e-01,  1.0129e+00,  7.2963e-03,  7.0013e-02],\n         [-1.4721e-02,  8.5832e-01, -6.8502e-01, -3.1884e-01],\n         [-3.1111e-01,  1.2670e+00,  3.7510e-02,  9.0090e-02],\n         [-1.1028e-01,  5.6204e-01, -1.2300e-01, -7.2779e-03],\n         [-9.8230e-02,  9.2833e-01, -2.0048e-01, -5.0946e-02],\n         [-2.8116e-01,  1.5333e+00, -7.6083e-01, -3.8149e-01],\n         [-3.1849e-01,  1.2810e+00, -4.4203e-01, -1.8570e-01],\n         [-1.5609e-01,  4.1282e-01, -2.1424e-02,  5.3293e-02],\n         [-2.8183e-01,  1.3638e+00,  1.7864e-03,  6.7814e-02],\n         [-1.3315e-01,  1.0595e-01, -9.7209e-02,  1.0052e-02],\n         [-1.2550e-01,  1.5349e-01,  8.8421e-02,  1.1652e-01],\n         [-3.1680e-01,  1.2909e+00,  7.7380e-02,  1.1036e-01],\n         [-2.6291e-01,  1.4087e+00,  3.0625e-02,  8.3932e-02],\n         [-2.9826e-01,  1.3586e+00, -4.8129e-01, -2.1678e-01],\n         [-6.0923e-02,  1.7311e+00, -8.0982e-01, -4.0567e-01],\n         [-1.1620e-01,  1.3897e+00,  6.8115e-02,  1.0496e-01],\n         [-9.9423e-02,  1.4947e+00,  6.2627e-02,  1.0222e-01],\n         [-9.7926e-02,  9.5710e-01, -8.9095e-02,  1.4997e-02],\n         [-9.8945e-02,  1.1545e+00, -2.3634e-01, -6.3489e-02],\n         [-3.2255e-01,  1.2537e+00,  2.1368e-02,  7.7986e-02],\n         [-2.4725e-01,  1.4391e+00, -7.7966e-01, -3.8715e-01],\n         [-1.2513e-01,  1.5574e-01, -1.5959e-02,  5.5402e-02],\n         [-2.5727e-01,  1.4202e+00,  3.2090e-02,  8.3587e-02],\n         [-9.8596e-02,  1.4997e+00,  6.5025e-02,  1.0559e-01],\n         [-9.8568e-02,  1.1568e+00, -1.1960e-01, -5.1343e-03],\n         [-1.0295e-01,  7.6752e-01, -3.4208e-02,  4.7358e-02],\n         [-3.0041e-01,  1.3403e+00, -2.3056e-01, -6.8620e-02],\n         [-3.0313e-01,  1.3864e+00, -5.8311e-02,  3.1730e-02]]),\n 'NB': tensor([[-0.2496,  0.9548,  0.1236,  0.1377],\n         [-0.1846,  0.5098,  0.1236,  0.1377],\n         [-0.2875,  1.0929,  0.1236,  0.1377],\n         [-0.2066,  0.6582,  0.1236,  0.1377],\n         [-0.0147,  0.8583,  0.1236,  0.1377]])}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0].res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------   CURRENT LEARNING RATE: 0.001   ---------------\n",
      "Epoch: 0\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 0.0 \n",
      "     Training Step: 1 Training Loss: 0.0 \n",
      "     Training Step: 2 Training Loss: 0.0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17588\\836682791.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mloss_weights\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mmodel_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"embedder_model_specialized.pt\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mtrain_input\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mval_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_validate_ACOPF\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mtrain_validate_ACOPF\u001B[1;34m(model, model_name, optimizer, train_inputs, val_inputs, loss_weights, start_epoch, num_epochs, return_outputs, save_model)\u001B[0m\n\u001B[0;32m   2287\u001B[0m             \u001B[1;31m#physics_loss, mre_loss, loss, penalty_loss, unsupervised_loss = self_supervised_hetero_obj_fn(out_dict,bus_idx_neighbors_dict,constraint_dict,scaler, alpha,beta, gamma)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2288\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2289\u001B[1;33m             \u001B[0mphysics_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmre_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpenalty_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munsupervised_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2290\u001B[0m             \u001B[0mphysics_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2291\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mself_supervised_embedder_obj_fn\u001B[1;34m(out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   2182\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2183\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2184\u001B[1;33m     \u001B[0mphysics_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_physics_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2185\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2186\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mphysics_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mcalc_physics_loss\u001B[1;34m(out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   3058\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3059\u001B[0m                 \u001B[1;31m# ACOPF Equation for P_i\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3060\u001B[1;33m                 \u001B[0mP_ij\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1.0\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_i\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0msoftabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mV_j\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mG_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mB_ij\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdelta_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3061\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3062\u001B[0m                 \u001B[0mP\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_ij\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mformat_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mformat_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mextract_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    198\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m     \u001B[0mstack\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mStackSummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwalk_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m     \u001B[0mstack\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreverse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mstack\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    357\u001B[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001B[0;32m    358\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mfilename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfnames\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 359\u001B[1;33m             \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckcache\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    360\u001B[0m         \u001B[1;31m# If immediate lookup was desired, trigger lookups now.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlookup_lines\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\IPython\\core\\compilerop.py\u001B[0m in \u001B[0;36mcheck_linecache_ipython\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \"\"\"\n\u001B[0;32m    184\u001B[0m     \u001B[1;31m# First call the original checkcache as intended\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 185\u001B[1;33m     \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_checkcache_ori\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    186\u001B[0m     \u001B[1;31m# Then, update back the cache with our data, so that tracebacks related\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m     \u001B[1;31m# to our compiled codes can be produced.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\linecache.py\u001B[0m in \u001B[0;36mcheckcache\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mcontinue\u001B[0m   \u001B[1;31m# no-op for files loaded via a __loader__\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m             \u001B[0mstat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfullname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mcache\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 1\n",
    "num_epochs = 100\n",
    "loss_weights = (0.0, 1.0, 0.0)\n",
    "model_name = \"embedder_model_specialized.pt\"\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF(model, model_name, optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------   CURRENT LEARNING RATE: 4e-05   ---------------\n",
      "Epoch: 60\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 6.849725723266602 \n",
      "     Training Step: 1 Training Loss: 7.994863033294678 \n",
      "     Training Step: 2 Training Loss: 7.418996334075928 \n",
      "     Training Step: 3 Training Loss: 4.887266635894775 \n",
      "     Training Step: 4 Training Loss: 5.012649059295654 \n",
      "     Training Step: 5 Training Loss: 3.0778064727783203 \n",
      "     Training Step: 6 Training Loss: 11.675814628601074 \n",
      "     Training Step: 7 Training Loss: 10.785894393920898 \n",
      "     Training Step: 8 Training Loss: 8.542866706848145 \n",
      "     Training Step: 9 Training Loss: 9.035882949829102 \n",
      "     Training Step: 10 Training Loss: 9.545356750488281 \n",
      "     Training Step: 11 Training Loss: 8.43463134765625 \n",
      "     Training Step: 12 Training Loss: 12.55498218536377 \n",
      "     Training Step: 13 Training Loss: 5.320827484130859 \n",
      "     Training Step: 14 Training Loss: 6.62136173248291 \n",
      "     Training Step: 15 Training Loss: 5.096195220947266 \n",
      "     Training Step: 16 Training Loss: 9.50904655456543 \n",
      "     Training Step: 17 Training Loss: 7.444781303405762 \n",
      "     Training Step: 18 Training Loss: 6.758533477783203 \n",
      "     Training Step: 19 Training Loss: 6.758355617523193 \n",
      "     Training Step: 20 Training Loss: 7.661477565765381 \n",
      "     Training Step: 21 Training Loss: 6.79921293258667 \n",
      "     Training Step: 22 Training Loss: 10.037893295288086 \n",
      "     Training Step: 23 Training Loss: 9.075201034545898 \n",
      "     Training Step: 24 Training Loss: 4.894169807434082 \n",
      "     Training Step: 25 Training Loss: 7.776320934295654 \n",
      "     Training Step: 26 Training Loss: 3.6629114151000977 \n",
      "     Training Step: 27 Training Loss: 7.650761127471924 \n",
      "     Training Step: 28 Training Loss: 7.431985378265381 \n",
      "     Training Step: 29 Training Loss: 7.586144924163818 \n",
      "     Training Step: 30 Training Loss: 12.974316596984863 \n",
      "     Training Step: 31 Training Loss: 12.894868850708008 \n",
      "     Training Step: 32 Training Loss: 6.032436370849609 \n",
      "     Training Step: 33 Training Loss: 4.2507405281066895 \n",
      "     Training Step: 34 Training Loss: 5.452780723571777 \n",
      "     Training Step: 35 Training Loss: 13.459784507751465 \n",
      "     Training Step: 36 Training Loss: 5.302128314971924 \n",
      "     Training Step: 37 Training Loss: 13.050894737243652 \n",
      "     Training Step: 38 Training Loss: 13.304935455322266 \n",
      "     Training Step: 39 Training Loss: 3.919915199279785 \n",
      "     Training Step: 40 Training Loss: 5.36727237701416 \n",
      "     Training Step: 41 Training Loss: 5.129926681518555 \n",
      "     Training Step: 42 Training Loss: 5.51865291595459 \n",
      "     Training Step: 43 Training Loss: 7.853332042694092 \n",
      "     Training Step: 44 Training Loss: 5.049275875091553 \n",
      "     Training Step: 45 Training Loss: 5.308444499969482 \n",
      "     Training Step: 46 Training Loss: 6.933781623840332 \n",
      "     Training Step: 47 Training Loss: 10.68270206451416 \n",
      "     Training Step: 48 Training Loss: 3.287910223007202 \n",
      "     Training Step: 49 Training Loss: 6.718997955322266 \n",
      "     Training Step: 50 Training Loss: 7.86416482925415 \n",
      "     Training Step: 51 Training Loss: 4.65550422668457 \n",
      "     Training Step: 52 Training Loss: 7.46273136138916 \n",
      "     Training Step: 53 Training Loss: 4.859919548034668 \n",
      "     Training Step: 54 Training Loss: 4.085713863372803 \n",
      "     Training Step: 55 Training Loss: 7.330260753631592 \n",
      "     Training Step: 56 Training Loss: 5.473433494567871 \n",
      "     Training Step: 57 Training Loss: 4.964493274688721 \n",
      "     Training Step: 58 Training Loss: 11.09634780883789 \n",
      "     Training Step: 59 Training Loss: 5.558187484741211 \n",
      "     Training Step: 60 Training Loss: 10.589357376098633 \n",
      "     Training Step: 61 Training Loss: 5.367259502410889 \n",
      "     Training Step: 62 Training Loss: 14.799457550048828 \n",
      "     Training Step: 63 Training Loss: 13.711862564086914 \n",
      "     Training Step: 64 Training Loss: 3.6749696731567383 \n",
      "     Training Step: 65 Training Loss: 9.222043991088867 \n",
      "     Training Step: 66 Training Loss: 6.734905242919922 \n",
      "     Training Step: 67 Training Loss: 8.710442543029785 \n",
      "     Training Step: 68 Training Loss: 7.278345584869385 \n",
      "     Training Step: 69 Training Loss: 8.809966087341309 \n",
      "     Training Step: 70 Training Loss: 8.195501327514648 \n",
      "     Training Step: 71 Training Loss: 8.988158226013184 \n",
      "     Training Step: 72 Training Loss: 9.117876052856445 \n",
      "     Training Step: 73 Training Loss: 5.26828670501709 \n",
      "     Training Step: 74 Training Loss: 5.45643424987793 \n",
      "     Training Step: 75 Training Loss: 3.749128818511963 \n",
      "     Training Step: 76 Training Loss: 10.256917953491211 \n",
      "     Training Step: 77 Training Loss: 12.184932708740234 \n",
      "     Training Step: 78 Training Loss: 12.044828414916992 \n",
      "     Training Step: 79 Training Loss: 8.22641372680664 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 6.2922682762146 \n",
      "     Validation Step: 1 Validation Loss: 7.970943450927734 \n",
      "     Validation Step: 2 Validation Loss: 9.62479305267334 \n",
      "     Validation Step: 3 Validation Loss: 8.7261381149292 \n",
      "     Validation Step: 4 Validation Loss: 12.534525871276855 \n",
      "     Validation Step: 5 Validation Loss: 11.159050941467285 \n",
      "     Validation Step: 6 Validation Loss: 3.3455405235290527 \n",
      "     Validation Step: 7 Validation Loss: 4.4155097007751465 \n",
      "     Validation Step: 8 Validation Loss: 5.8844828605651855 \n",
      "     Validation Step: 9 Validation Loss: 4.783188819885254 \n",
      "     Validation Step: 10 Validation Loss: 9.90269947052002 \n",
      "     Validation Step: 11 Validation Loss: 6.927859783172607 \n",
      "     Validation Step: 12 Validation Loss: 13.698420524597168 \n",
      "     Validation Step: 13 Validation Loss: 12.103291511535645 \n",
      "     Validation Step: 14 Validation Loss: 5.034355640411377 \n",
      "Epoch: 61\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.6355698108673096 \n",
      "     Training Step: 1 Training Loss: 3.2790489196777344 \n",
      "     Training Step: 2 Training Loss: 2.0579960346221924 \n",
      "     Training Step: 3 Training Loss: 3.4389989376068115 \n",
      "     Training Step: 4 Training Loss: 2.5847768783569336 \n",
      "     Training Step: 5 Training Loss: 2.554276704788208 \n",
      "     Training Step: 6 Training Loss: 2.485879898071289 \n",
      "     Training Step: 7 Training Loss: 2.446885585784912 \n",
      "     Training Step: 8 Training Loss: 2.324148654937744 \n",
      "     Training Step: 9 Training Loss: 2.551927089691162 \n",
      "     Training Step: 10 Training Loss: 2.7412290573120117 \n",
      "     Training Step: 11 Training Loss: 2.4219207763671875 \n",
      "     Training Step: 12 Training Loss: 2.387845516204834 \n",
      "     Training Step: 13 Training Loss: 2.9680519104003906 \n",
      "     Training Step: 14 Training Loss: 2.352520227432251 \n",
      "     Training Step: 15 Training Loss: 2.8653669357299805 \n",
      "     Training Step: 16 Training Loss: 3.28800106048584 \n",
      "     Training Step: 17 Training Loss: 2.507077693939209 \n",
      "     Training Step: 18 Training Loss: 3.054689884185791 \n",
      "     Training Step: 19 Training Loss: 2.3753955364227295 \n",
      "     Training Step: 20 Training Loss: 3.495718479156494 \n",
      "     Training Step: 21 Training Loss: 2.7977917194366455 \n",
      "     Training Step: 22 Training Loss: 2.6439530849456787 \n",
      "     Training Step: 23 Training Loss: 3.3722167015075684 \n",
      "     Training Step: 24 Training Loss: 2.768357276916504 \n",
      "     Training Step: 25 Training Loss: 2.9859795570373535 \n",
      "     Training Step: 26 Training Loss: 3.1551802158355713 \n",
      "     Training Step: 27 Training Loss: 2.31691837310791 \n",
      "     Training Step: 28 Training Loss: 3.2671611309051514 \n",
      "     Training Step: 29 Training Loss: 2.217538833618164 \n",
      "     Training Step: 30 Training Loss: 2.7339463233947754 \n",
      "     Training Step: 31 Training Loss: 2.6649608612060547 \n",
      "     Training Step: 32 Training Loss: 2.751533031463623 \n",
      "     Training Step: 33 Training Loss: 2.944531202316284 \n",
      "     Training Step: 34 Training Loss: 2.8072686195373535 \n",
      "     Training Step: 35 Training Loss: 2.9901790618896484 \n",
      "     Training Step: 36 Training Loss: 2.7854702472686768 \n",
      "     Training Step: 37 Training Loss: 2.4883780479431152 \n",
      "     Training Step: 38 Training Loss: 2.213292121887207 \n",
      "     Training Step: 39 Training Loss: 2.548321008682251 \n",
      "     Training Step: 40 Training Loss: 3.0088748931884766 \n",
      "     Training Step: 41 Training Loss: 3.368178129196167 \n",
      "     Training Step: 42 Training Loss: 2.822603225708008 \n",
      "     Training Step: 43 Training Loss: 3.030764102935791 \n",
      "     Training Step: 44 Training Loss: 2.229597568511963 \n",
      "     Training Step: 45 Training Loss: 2.8533804416656494 \n",
      "     Training Step: 46 Training Loss: 2.0107736587524414 \n",
      "     Training Step: 47 Training Loss: 2.209465980529785 \n",
      "     Training Step: 48 Training Loss: 2.3026845455169678 \n",
      "     Training Step: 49 Training Loss: 2.301964044570923 \n",
      "     Training Step: 50 Training Loss: 3.3419573307037354 \n",
      "     Training Step: 51 Training Loss: 3.6319408416748047 \n",
      "     Training Step: 52 Training Loss: 3.3818068504333496 \n",
      "     Training Step: 53 Training Loss: 2.5126912593841553 \n",
      "     Training Step: 54 Training Loss: 3.7466955184936523 \n",
      "     Training Step: 55 Training Loss: 2.8762738704681396 \n",
      "     Training Step: 56 Training Loss: 3.5802860260009766 \n",
      "     Training Step: 57 Training Loss: 3.4996771812438965 \n",
      "     Training Step: 58 Training Loss: 2.536621570587158 \n",
      "     Training Step: 59 Training Loss: 2.3883039951324463 \n",
      "     Training Step: 60 Training Loss: 3.328050136566162 \n",
      "     Training Step: 61 Training Loss: 2.908033609390259 \n",
      "     Training Step: 62 Training Loss: 2.9240691661834717 \n",
      "     Training Step: 63 Training Loss: 2.533116102218628 \n",
      "     Training Step: 64 Training Loss: 2.4196269512176514 \n",
      "     Training Step: 65 Training Loss: 2.5194225311279297 \n",
      "     Training Step: 66 Training Loss: 2.468540668487549 \n",
      "     Training Step: 67 Training Loss: 2.6171860694885254 \n",
      "     Training Step: 68 Training Loss: 2.3061156272888184 \n",
      "     Training Step: 69 Training Loss: 2.3685994148254395 \n",
      "     Training Step: 70 Training Loss: 2.282291889190674 \n",
      "     Training Step: 71 Training Loss: 3.3871171474456787 \n",
      "     Training Step: 72 Training Loss: 3.9170775413513184 \n",
      "     Training Step: 73 Training Loss: 3.755721092224121 \n",
      "     Training Step: 74 Training Loss: 2.3278894424438477 \n",
      "     Training Step: 75 Training Loss: 3.6431431770324707 \n",
      "     Training Step: 76 Training Loss: 2.7319881916046143 \n",
      "     Training Step: 77 Training Loss: 2.9789109230041504 \n",
      "     Training Step: 78 Training Loss: 2.6027445793151855 \n",
      "     Training Step: 79 Training Loss: 3.4189162254333496 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.321836471557617 \n",
      "     Validation Step: 1 Validation Loss: 2.7259020805358887 \n",
      "     Validation Step: 2 Validation Loss: 3.4820737838745117 \n",
      "     Validation Step: 3 Validation Loss: 2.551522731781006 \n",
      "     Validation Step: 4 Validation Loss: 3.259505271911621 \n",
      "     Validation Step: 5 Validation Loss: 2.785046100616455 \n",
      "     Validation Step: 6 Validation Loss: 2.381053924560547 \n",
      "     Validation Step: 7 Validation Loss: 3.0370004177093506 \n",
      "     Validation Step: 8 Validation Loss: 3.1503448486328125 \n",
      "     Validation Step: 9 Validation Loss: 3.0699212551116943 \n",
      "     Validation Step: 10 Validation Loss: 2.853329658508301 \n",
      "     Validation Step: 11 Validation Loss: 2.3067870140075684 \n",
      "     Validation Step: 12 Validation Loss: 2.680511951446533 \n",
      "     Validation Step: 13 Validation Loss: 3.0116653442382812 \n",
      "     Validation Step: 14 Validation Loss: 2.2327804565429688 \n",
      "Epoch: 62\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.0609235763549805 \n",
      "     Training Step: 1 Training Loss: 3.17659592628479 \n",
      "     Training Step: 2 Training Loss: 2.1783833503723145 \n",
      "     Training Step: 3 Training Loss: 2.174607515335083 \n",
      "     Training Step: 4 Training Loss: 2.0741963386535645 \n",
      "     Training Step: 5 Training Loss: 3.1043784618377686 \n",
      "     Training Step: 6 Training Loss: 3.13986873626709 \n",
      "     Training Step: 7 Training Loss: 2.9091439247131348 \n",
      "     Training Step: 8 Training Loss: 3.4113197326660156 \n",
      "     Training Step: 9 Training Loss: 2.7322945594787598 \n",
      "     Training Step: 10 Training Loss: 2.6169681549072266 \n",
      "     Training Step: 11 Training Loss: 2.9376585483551025 \n",
      "     Training Step: 12 Training Loss: 3.0303378105163574 \n",
      "     Training Step: 13 Training Loss: 2.6589856147766113 \n",
      "     Training Step: 14 Training Loss: 2.191561698913574 \n",
      "     Training Step: 15 Training Loss: 2.44677472114563 \n",
      "     Training Step: 16 Training Loss: 2.3249590396881104 \n",
      "     Training Step: 17 Training Loss: 2.220390796661377 \n",
      "     Training Step: 18 Training Loss: 3.325751781463623 \n",
      "     Training Step: 19 Training Loss: 2.368943929672241 \n",
      "     Training Step: 20 Training Loss: 3.3433055877685547 \n",
      "     Training Step: 21 Training Loss: 2.7937192916870117 \n",
      "     Training Step: 22 Training Loss: 3.1934733390808105 \n",
      "     Training Step: 23 Training Loss: 3.539168357849121 \n",
      "     Training Step: 24 Training Loss: 4.160991191864014 \n",
      "     Training Step: 25 Training Loss: 2.595266819000244 \n",
      "     Training Step: 26 Training Loss: 2.7317471504211426 \n",
      "     Training Step: 27 Training Loss: 2.6427972316741943 \n",
      "     Training Step: 28 Training Loss: 2.339905261993408 \n",
      "     Training Step: 29 Training Loss: 2.443847894668579 \n",
      "     Training Step: 30 Training Loss: 2.3639633655548096 \n",
      "     Training Step: 31 Training Loss: 2.397139549255371 \n",
      "     Training Step: 32 Training Loss: 3.443136692047119 \n",
      "     Training Step: 33 Training Loss: 2.488542079925537 \n",
      "     Training Step: 34 Training Loss: 3.13734769821167 \n",
      "     Training Step: 35 Training Loss: 3.054051160812378 \n",
      "     Training Step: 36 Training Loss: 2.646345615386963 \n",
      "     Training Step: 37 Training Loss: 2.6646361351013184 \n",
      "     Training Step: 38 Training Loss: 2.7199504375457764 \n",
      "     Training Step: 39 Training Loss: 2.992504119873047 \n",
      "     Training Step: 40 Training Loss: 3.421041488647461 \n",
      "     Training Step: 41 Training Loss: 3.175034284591675 \n",
      "     Training Step: 42 Training Loss: 2.222857713699341 \n",
      "     Training Step: 43 Training Loss: 3.186616897583008 \n",
      "     Training Step: 44 Training Loss: 3.084123134613037 \n",
      "     Training Step: 45 Training Loss: 3.214345693588257 \n",
      "     Training Step: 46 Training Loss: 3.729248523712158 \n",
      "     Training Step: 47 Training Loss: 2.296299695968628 \n",
      "     Training Step: 48 Training Loss: 2.6865251064300537 \n",
      "     Training Step: 49 Training Loss: 2.6053147315979004 \n",
      "     Training Step: 50 Training Loss: 2.660512924194336 \n",
      "     Training Step: 51 Training Loss: 2.4678382873535156 \n",
      "     Training Step: 52 Training Loss: 2.597283363342285 \n",
      "     Training Step: 53 Training Loss: 2.315915822982788 \n",
      "     Training Step: 54 Training Loss: 3.7120602130889893 \n",
      "     Training Step: 55 Training Loss: 4.433568954467773 \n",
      "     Training Step: 56 Training Loss: 4.211912155151367 \n",
      "     Training Step: 57 Training Loss: 4.363589286804199 \n",
      "     Training Step: 58 Training Loss: 2.1148648262023926 \n",
      "     Training Step: 59 Training Loss: 2.2037737369537354 \n",
      "     Training Step: 60 Training Loss: 2.6702089309692383 \n",
      "     Training Step: 61 Training Loss: 2.6055774688720703 \n",
      "     Training Step: 62 Training Loss: 3.20627498626709 \n",
      "     Training Step: 63 Training Loss: 3.055093765258789 \n",
      "     Training Step: 64 Training Loss: 3.664633274078369 \n",
      "     Training Step: 65 Training Loss: 3.743229866027832 \n",
      "     Training Step: 66 Training Loss: 3.6341564655303955 \n",
      "     Training Step: 67 Training Loss: 2.6190099716186523 \n",
      "     Training Step: 68 Training Loss: 3.978950023651123 \n",
      "     Training Step: 69 Training Loss: 3.9750943183898926 \n",
      "     Training Step: 70 Training Loss: 2.541748046875 \n",
      "     Training Step: 71 Training Loss: 3.103971481323242 \n",
      "     Training Step: 72 Training Loss: 3.233975410461426 \n",
      "     Training Step: 73 Training Loss: 2.6943447589874268 \n",
      "     Training Step: 74 Training Loss: 3.0929341316223145 \n",
      "     Training Step: 75 Training Loss: 3.601835250854492 \n",
      "     Training Step: 76 Training Loss: 3.197295665740967 \n",
      "     Training Step: 77 Training Loss: 3.3342061042785645 \n",
      "     Training Step: 78 Training Loss: 2.585078477859497 \n",
      "     Training Step: 79 Training Loss: 2.988157033920288 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.6997427940368652 \n",
      "     Validation Step: 1 Validation Loss: 2.272503137588501 \n",
      "     Validation Step: 2 Validation Loss: 2.965146064758301 \n",
      "     Validation Step: 3 Validation Loss: 3.2630581855773926 \n",
      "     Validation Step: 4 Validation Loss: 2.7251014709472656 \n",
      "     Validation Step: 5 Validation Loss: 3.580699920654297 \n",
      "     Validation Step: 6 Validation Loss: 3.2723846435546875 \n",
      "     Validation Step: 7 Validation Loss: 2.634354591369629 \n",
      "     Validation Step: 8 Validation Loss: 3.1506710052490234 \n",
      "     Validation Step: 9 Validation Loss: 2.446561813354492 \n",
      "     Validation Step: 10 Validation Loss: 3.5477006435394287 \n",
      "     Validation Step: 11 Validation Loss: 3.0839805603027344 \n",
      "     Validation Step: 12 Validation Loss: 2.4819352626800537 \n",
      "     Validation Step: 13 Validation Loss: 2.7969632148742676 \n",
      "     Validation Step: 14 Validation Loss: 3.4660515785217285 \n",
      "Epoch: 63\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.350492477416992 \n",
      "     Training Step: 1 Training Loss: 2.7761542797088623 \n",
      "     Training Step: 2 Training Loss: 3.0742547512054443 \n",
      "     Training Step: 3 Training Loss: 3.4398179054260254 \n",
      "     Training Step: 4 Training Loss: 3.5629544258117676 \n",
      "     Training Step: 5 Training Loss: 3.8351540565490723 \n",
      "     Training Step: 6 Training Loss: 2.3371591567993164 \n",
      "     Training Step: 7 Training Loss: 2.1466307640075684 \n",
      "     Training Step: 8 Training Loss: 3.4183337688446045 \n",
      "     Training Step: 9 Training Loss: 3.439323663711548 \n",
      "     Training Step: 10 Training Loss: 3.4735283851623535 \n",
      "     Training Step: 11 Training Loss: 2.482759714126587 \n",
      "     Training Step: 12 Training Loss: 2.5249476432800293 \n",
      "     Training Step: 13 Training Loss: 3.8131227493286133 \n",
      "     Training Step: 14 Training Loss: 2.285106658935547 \n",
      "     Training Step: 15 Training Loss: 2.5044875144958496 \n",
      "     Training Step: 16 Training Loss: 2.2803661823272705 \n",
      "     Training Step: 17 Training Loss: 2.891028881072998 \n",
      "     Training Step: 18 Training Loss: 2.4770584106445312 \n",
      "     Training Step: 19 Training Loss: 2.6894140243530273 \n",
      "     Training Step: 20 Training Loss: 2.416494369506836 \n",
      "     Training Step: 21 Training Loss: 2.9659314155578613 \n",
      "     Training Step: 22 Training Loss: 2.8433446884155273 \n",
      "     Training Step: 23 Training Loss: 2.8131070137023926 \n",
      "     Training Step: 24 Training Loss: 2.9125990867614746 \n",
      "     Training Step: 25 Training Loss: 2.824246644973755 \n",
      "     Training Step: 26 Training Loss: 3.9514894485473633 \n",
      "     Training Step: 27 Training Loss: 4.339582920074463 \n",
      "     Training Step: 28 Training Loss: 2.129723072052002 \n",
      "     Training Step: 29 Training Loss: 3.8834619522094727 \n",
      "     Training Step: 30 Training Loss: 3.5999560356140137 \n",
      "     Training Step: 31 Training Loss: 2.211543083190918 \n",
      "     Training Step: 32 Training Loss: 3.1559441089630127 \n",
      "     Training Step: 33 Training Loss: 3.321014642715454 \n",
      "     Training Step: 34 Training Loss: 2.520155668258667 \n",
      "     Training Step: 35 Training Loss: 2.9687461853027344 \n",
      "     Training Step: 36 Training Loss: 2.6559042930603027 \n",
      "     Training Step: 37 Training Loss: 3.444615602493286 \n",
      "     Training Step: 38 Training Loss: 2.644380807876587 \n",
      "     Training Step: 39 Training Loss: 3.680511474609375 \n",
      "     Training Step: 40 Training Loss: 2.8726015090942383 \n",
      "     Training Step: 41 Training Loss: 2.729069709777832 \n",
      "     Training Step: 42 Training Loss: 2.273205041885376 \n",
      "     Training Step: 43 Training Loss: 2.4821088314056396 \n",
      "     Training Step: 44 Training Loss: 2.5093166828155518 \n",
      "     Training Step: 45 Training Loss: 3.199690580368042 \n",
      "     Training Step: 46 Training Loss: 2.885711669921875 \n",
      "     Training Step: 47 Training Loss: 2.489046812057495 \n",
      "     Training Step: 48 Training Loss: 2.793769598007202 \n",
      "     Training Step: 49 Training Loss: 3.263688802719116 \n",
      "     Training Step: 50 Training Loss: 2.9064230918884277 \n",
      "     Training Step: 51 Training Loss: 2.7272562980651855 \n",
      "     Training Step: 52 Training Loss: 4.3142218589782715 \n",
      "     Training Step: 53 Training Loss: 3.3402628898620605 \n",
      "     Training Step: 54 Training Loss: 2.689541816711426 \n",
      "     Training Step: 55 Training Loss: 3.015148878097534 \n",
      "     Training Step: 56 Training Loss: 2.851558208465576 \n",
      "     Training Step: 57 Training Loss: 2.5671162605285645 \n",
      "     Training Step: 58 Training Loss: 2.8845326900482178 \n",
      "     Training Step: 59 Training Loss: 3.1151041984558105 \n",
      "     Training Step: 60 Training Loss: 3.166029214859009 \n",
      "     Training Step: 61 Training Loss: 2.046379804611206 \n",
      "     Training Step: 62 Training Loss: 3.0691781044006348 \n",
      "     Training Step: 63 Training Loss: 3.0757110118865967 \n",
      "     Training Step: 64 Training Loss: 2.6397342681884766 \n",
      "     Training Step: 65 Training Loss: 2.235316276550293 \n",
      "     Training Step: 66 Training Loss: 2.7301552295684814 \n",
      "     Training Step: 67 Training Loss: 2.3605520725250244 \n",
      "     Training Step: 68 Training Loss: 4.518928527832031 \n",
      "     Training Step: 69 Training Loss: 2.4143919944763184 \n",
      "     Training Step: 70 Training Loss: 2.604386806488037 \n",
      "     Training Step: 71 Training Loss: 4.575492858886719 \n",
      "     Training Step: 72 Training Loss: 2.312904119491577 \n",
      "     Training Step: 73 Training Loss: 2.4002461433410645 \n",
      "     Training Step: 74 Training Loss: 3.2159948348999023 \n",
      "     Training Step: 75 Training Loss: 2.4122750759124756 \n",
      "     Training Step: 76 Training Loss: 2.3115389347076416 \n",
      "     Training Step: 77 Training Loss: 2.249664068222046 \n",
      "     Training Step: 78 Training Loss: 2.3040928840637207 \n",
      "     Training Step: 79 Training Loss: 2.7971134185791016 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.239210844039917 \n",
      "     Validation Step: 1 Validation Loss: 2.3554508686065674 \n",
      "     Validation Step: 2 Validation Loss: 3.5278046131134033 \n",
      "     Validation Step: 3 Validation Loss: 3.0248594284057617 \n",
      "     Validation Step: 4 Validation Loss: 2.554131507873535 \n",
      "     Validation Step: 5 Validation Loss: 2.6708338260650635 \n",
      "     Validation Step: 6 Validation Loss: 3.3792877197265625 \n",
      "     Validation Step: 7 Validation Loss: 3.4421427249908447 \n",
      "     Validation Step: 8 Validation Loss: 3.602975368499756 \n",
      "     Validation Step: 9 Validation Loss: 3.724350690841675 \n",
      "     Validation Step: 10 Validation Loss: 2.8725876808166504 \n",
      "     Validation Step: 11 Validation Loss: 2.9635519981384277 \n",
      "     Validation Step: 12 Validation Loss: 2.778569221496582 \n",
      "     Validation Step: 13 Validation Loss: 2.422628879547119 \n",
      "     Validation Step: 14 Validation Loss: 3.1461822986602783 \n",
      "Epoch: 64\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.8166370391845703 \n",
      "     Training Step: 1 Training Loss: 3.3043031692504883 \n",
      "     Training Step: 2 Training Loss: 2.995069742202759 \n",
      "     Training Step: 3 Training Loss: 2.9556267261505127 \n",
      "     Training Step: 4 Training Loss: 4.579167366027832 \n",
      "     Training Step: 5 Training Loss: 2.63482666015625 \n",
      "     Training Step: 6 Training Loss: 3.885169506072998 \n",
      "     Training Step: 7 Training Loss: 3.189358711242676 \n",
      "     Training Step: 8 Training Loss: 2.163419485092163 \n",
      "     Training Step: 9 Training Loss: 2.4347879886627197 \n",
      "     Training Step: 10 Training Loss: 3.018167018890381 \n",
      "     Training Step: 11 Training Loss: 3.377235174179077 \n",
      "     Training Step: 12 Training Loss: 2.4053196907043457 \n",
      "     Training Step: 13 Training Loss: 2.1560871601104736 \n",
      "     Training Step: 14 Training Loss: 2.700906753540039 \n",
      "     Training Step: 15 Training Loss: 3.9529075622558594 \n",
      "     Training Step: 16 Training Loss: 3.056607246398926 \n",
      "     Training Step: 17 Training Loss: 3.8545994758605957 \n",
      "     Training Step: 18 Training Loss: 3.3856749534606934 \n",
      "     Training Step: 19 Training Loss: 3.9476754665374756 \n",
      "     Training Step: 20 Training Loss: 2.929025411605835 \n",
      "     Training Step: 21 Training Loss: 2.7837371826171875 \n",
      "     Training Step: 22 Training Loss: 2.6997005939483643 \n",
      "     Training Step: 23 Training Loss: 2.4101946353912354 \n",
      "     Training Step: 24 Training Loss: 4.0705037117004395 \n",
      "     Training Step: 25 Training Loss: 2.2036521434783936 \n",
      "     Training Step: 26 Training Loss: 4.069825172424316 \n",
      "     Training Step: 27 Training Loss: 2.2287724018096924 \n",
      "     Training Step: 28 Training Loss: 3.7236714363098145 \n",
      "     Training Step: 29 Training Loss: 3.2921931743621826 \n",
      "     Training Step: 30 Training Loss: 4.292503356933594 \n",
      "     Training Step: 31 Training Loss: 3.0457520484924316 \n",
      "     Training Step: 32 Training Loss: 3.6483469009399414 \n",
      "     Training Step: 33 Training Loss: 2.474405288696289 \n",
      "     Training Step: 34 Training Loss: 3.161085605621338 \n",
      "     Training Step: 35 Training Loss: 3.968397617340088 \n",
      "     Training Step: 36 Training Loss: 2.4151289463043213 \n",
      "     Training Step: 37 Training Loss: 3.075357437133789 \n",
      "     Training Step: 38 Training Loss: 2.1406006813049316 \n",
      "     Training Step: 39 Training Loss: 3.006072521209717 \n",
      "     Training Step: 40 Training Loss: 3.510308027267456 \n",
      "     Training Step: 41 Training Loss: 3.576425075531006 \n",
      "     Training Step: 42 Training Loss: 2.5647549629211426 \n",
      "     Training Step: 43 Training Loss: 2.3023178577423096 \n",
      "     Training Step: 44 Training Loss: 2.391667127609253 \n",
      "     Training Step: 45 Training Loss: 2.377190113067627 \n",
      "     Training Step: 46 Training Loss: 2.3370378017425537 \n",
      "     Training Step: 47 Training Loss: 2.341648817062378 \n",
      "     Training Step: 48 Training Loss: 2.265803337097168 \n",
      "     Training Step: 49 Training Loss: 2.798945665359497 \n",
      "     Training Step: 50 Training Loss: 2.6837716102600098 \n",
      "     Training Step: 51 Training Loss: 3.2978055477142334 \n",
      "     Training Step: 52 Training Loss: 2.975473165512085 \n",
      "     Training Step: 53 Training Loss: 3.124541997909546 \n",
      "     Training Step: 54 Training Loss: 3.680816173553467 \n",
      "     Training Step: 55 Training Loss: 4.100606918334961 \n",
      "     Training Step: 56 Training Loss: 2.8304805755615234 \n",
      "     Training Step: 57 Training Loss: 3.435638427734375 \n",
      "     Training Step: 58 Training Loss: 3.1693568229675293 \n",
      "     Training Step: 59 Training Loss: 3.089113235473633 \n",
      "     Training Step: 60 Training Loss: 3.567826509475708 \n",
      "     Training Step: 61 Training Loss: 3.1940441131591797 \n",
      "     Training Step: 62 Training Loss: 3.0819849967956543 \n",
      "     Training Step: 63 Training Loss: 2.978015661239624 \n",
      "     Training Step: 64 Training Loss: 2.742379665374756 \n",
      "     Training Step: 65 Training Loss: 2.5247392654418945 \n",
      "     Training Step: 66 Training Loss: 3.7250044345855713 \n",
      "     Training Step: 67 Training Loss: 2.448546886444092 \n",
      "     Training Step: 68 Training Loss: 3.9751317501068115 \n",
      "     Training Step: 69 Training Loss: 2.4564361572265625 \n",
      "     Training Step: 70 Training Loss: 2.963017463684082 \n",
      "     Training Step: 71 Training Loss: 3.46236515045166 \n",
      "     Training Step: 72 Training Loss: 2.551771879196167 \n",
      "     Training Step: 73 Training Loss: 2.571284294128418 \n",
      "     Training Step: 74 Training Loss: 2.9263148307800293 \n",
      "     Training Step: 75 Training Loss: 3.1761281490325928 \n",
      "     Training Step: 76 Training Loss: 2.353303909301758 \n",
      "     Training Step: 77 Training Loss: 3.092446804046631 \n",
      "     Training Step: 78 Training Loss: 2.3967509269714355 \n",
      "     Training Step: 79 Training Loss: 2.622293472290039 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.6334776878356934 \n",
      "     Validation Step: 1 Validation Loss: 3.3547286987304688 \n",
      "     Validation Step: 2 Validation Loss: 2.700427770614624 \n",
      "     Validation Step: 3 Validation Loss: 2.5452282428741455 \n",
      "     Validation Step: 4 Validation Loss: 2.4194741249084473 \n",
      "     Validation Step: 5 Validation Loss: 3.811025619506836 \n",
      "     Validation Step: 6 Validation Loss: 3.8318610191345215 \n",
      "     Validation Step: 7 Validation Loss: 2.7649917602539062 \n",
      "     Validation Step: 8 Validation Loss: 3.5420398712158203 \n",
      "     Validation Step: 9 Validation Loss: 2.287759304046631 \n",
      "     Validation Step: 10 Validation Loss: 3.3077077865600586 \n",
      "     Validation Step: 11 Validation Loss: 3.264392375946045 \n",
      "     Validation Step: 12 Validation Loss: 3.200045108795166 \n",
      "     Validation Step: 13 Validation Loss: 2.9103503227233887 \n",
      "     Validation Step: 14 Validation Loss: 3.4357707500457764 \n",
      "Epoch: 65\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.3412466049194336 \n",
      "     Training Step: 1 Training Loss: 2.4244332313537598 \n",
      "     Training Step: 2 Training Loss: 3.6601033210754395 \n",
      "     Training Step: 3 Training Loss: 2.4363794326782227 \n",
      "     Training Step: 4 Training Loss: 3.0832571983337402 \n",
      "     Training Step: 5 Training Loss: 2.882855176925659 \n",
      "     Training Step: 6 Training Loss: 2.520778179168701 \n",
      "     Training Step: 7 Training Loss: 3.821288585662842 \n",
      "     Training Step: 8 Training Loss: 2.736907482147217 \n",
      "     Training Step: 9 Training Loss: 3.9853460788726807 \n",
      "     Training Step: 10 Training Loss: 3.300044059753418 \n",
      "     Training Step: 11 Training Loss: 2.325242757797241 \n",
      "     Training Step: 12 Training Loss: 3.136544704437256 \n",
      "     Training Step: 13 Training Loss: 2.4410109519958496 \n",
      "     Training Step: 14 Training Loss: 2.2346086502075195 \n",
      "     Training Step: 15 Training Loss: 3.0691933631896973 \n",
      "     Training Step: 16 Training Loss: 3.402808904647827 \n",
      "     Training Step: 17 Training Loss: 2.297043561935425 \n",
      "     Training Step: 18 Training Loss: 3.5698938369750977 \n",
      "     Training Step: 19 Training Loss: 2.8692615032196045 \n",
      "     Training Step: 20 Training Loss: 3.5164237022399902 \n",
      "     Training Step: 21 Training Loss: 3.423877239227295 \n",
      "     Training Step: 22 Training Loss: 2.6200037002563477 \n",
      "     Training Step: 23 Training Loss: 2.2629904747009277 \n",
      "     Training Step: 24 Training Loss: 2.506802558898926 \n",
      "     Training Step: 25 Training Loss: 3.1912684440612793 \n",
      "     Training Step: 26 Training Loss: 3.4551334381103516 \n",
      "     Training Step: 27 Training Loss: 2.1556410789489746 \n",
      "     Training Step: 28 Training Loss: 3.0390005111694336 \n",
      "     Training Step: 29 Training Loss: 3.740813970565796 \n",
      "     Training Step: 30 Training Loss: 2.935110092163086 \n",
      "     Training Step: 31 Training Loss: 3.4247875213623047 \n",
      "     Training Step: 32 Training Loss: 3.8621678352355957 \n",
      "     Training Step: 33 Training Loss: 3.6848859786987305 \n",
      "     Training Step: 34 Training Loss: 2.7361297607421875 \n",
      "     Training Step: 35 Training Loss: 2.7050461769104004 \n",
      "     Training Step: 36 Training Loss: 2.411414384841919 \n",
      "     Training Step: 37 Training Loss: 3.1775684356689453 \n",
      "     Training Step: 38 Training Loss: 2.3705430030822754 \n",
      "     Training Step: 39 Training Loss: 2.130967140197754 \n",
      "     Training Step: 40 Training Loss: 2.2590174674987793 \n",
      "     Training Step: 41 Training Loss: 2.489447593688965 \n",
      "     Training Step: 42 Training Loss: 2.657982587814331 \n",
      "     Training Step: 43 Training Loss: 2.8750996589660645 \n",
      "     Training Step: 44 Training Loss: 3.3974289894104004 \n",
      "     Training Step: 45 Training Loss: 3.55523419380188 \n",
      "     Training Step: 46 Training Loss: 2.7299013137817383 \n",
      "     Training Step: 47 Training Loss: 2.7185142040252686 \n",
      "     Training Step: 48 Training Loss: 3.033860683441162 \n",
      "     Training Step: 49 Training Loss: 3.5523123741149902 \n",
      "     Training Step: 50 Training Loss: 2.528726100921631 \n",
      "     Training Step: 51 Training Loss: 2.5876331329345703 \n",
      "     Training Step: 52 Training Loss: 3.132133960723877 \n",
      "     Training Step: 53 Training Loss: 2.201655149459839 \n",
      "     Training Step: 54 Training Loss: 2.3190412521362305 \n",
      "     Training Step: 55 Training Loss: 3.4573025703430176 \n",
      "     Training Step: 56 Training Loss: 2.556990623474121 \n",
      "     Training Step: 57 Training Loss: 2.734179735183716 \n",
      "     Training Step: 58 Training Loss: 2.9428014755249023 \n",
      "     Training Step: 59 Training Loss: 2.816711902618408 \n",
      "     Training Step: 60 Training Loss: 2.7931809425354004 \n",
      "     Training Step: 61 Training Loss: 3.556525707244873 \n",
      "     Training Step: 62 Training Loss: 5.194555282592773 \n",
      "     Training Step: 63 Training Loss: 3.178161144256592 \n",
      "     Training Step: 64 Training Loss: 2.440124988555908 \n",
      "     Training Step: 65 Training Loss: 3.2478036880493164 \n",
      "     Training Step: 66 Training Loss: 2.6442463397979736 \n",
      "     Training Step: 67 Training Loss: 2.8246521949768066 \n",
      "     Training Step: 68 Training Loss: 4.507791519165039 \n",
      "     Training Step: 69 Training Loss: 2.335753917694092 \n",
      "     Training Step: 70 Training Loss: 3.1959481239318848 \n",
      "     Training Step: 71 Training Loss: 3.152040958404541 \n",
      "     Training Step: 72 Training Loss: 2.980825185775757 \n",
      "     Training Step: 73 Training Loss: 2.6741528511047363 \n",
      "     Training Step: 74 Training Loss: 3.1745309829711914 \n",
      "     Training Step: 75 Training Loss: 2.3682074546813965 \n",
      "     Training Step: 76 Training Loss: 4.582268714904785 \n",
      "     Training Step: 77 Training Loss: 3.6945347785949707 \n",
      "     Training Step: 78 Training Loss: 3.4372501373291016 \n",
      "     Training Step: 79 Training Loss: 2.620643138885498 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.034815549850464 \n",
      "     Validation Step: 1 Validation Loss: 3.5666956901550293 \n",
      "     Validation Step: 2 Validation Loss: 2.7859580516815186 \n",
      "     Validation Step: 3 Validation Loss: 3.471325635910034 \n",
      "     Validation Step: 4 Validation Loss: 3.576883316040039 \n",
      "     Validation Step: 5 Validation Loss: 4.05910587310791 \n",
      "     Validation Step: 6 Validation Loss: 3.9909300804138184 \n",
      "     Validation Step: 7 Validation Loss: 4.253793716430664 \n",
      "     Validation Step: 8 Validation Loss: 2.90686297416687 \n",
      "     Validation Step: 9 Validation Loss: 2.7579565048217773 \n",
      "     Validation Step: 10 Validation Loss: 2.430593967437744 \n",
      "     Validation Step: 11 Validation Loss: 3.007106304168701 \n",
      "     Validation Step: 12 Validation Loss: 2.9753942489624023 \n",
      "     Validation Step: 13 Validation Loss: 3.0002613067626953 \n",
      "     Validation Step: 14 Validation Loss: 3.536858081817627 \n",
      "Epoch: 66\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.757416248321533 \n",
      "     Training Step: 1 Training Loss: 2.7919888496398926 \n",
      "     Training Step: 2 Training Loss: 2.768770217895508 \n",
      "     Training Step: 3 Training Loss: 3.4729301929473877 \n",
      "     Training Step: 4 Training Loss: 3.3766136169433594 \n",
      "     Training Step: 5 Training Loss: 2.8275256156921387 \n",
      "     Training Step: 6 Training Loss: 2.5979864597320557 \n",
      "     Training Step: 7 Training Loss: 2.731060743331909 \n",
      "     Training Step: 8 Training Loss: 4.051044464111328 \n",
      "     Training Step: 9 Training Loss: 3.178060293197632 \n",
      "     Training Step: 10 Training Loss: 3.6726932525634766 \n",
      "     Training Step: 11 Training Loss: 4.170749187469482 \n",
      "     Training Step: 12 Training Loss: 3.206249713897705 \n",
      "     Training Step: 13 Training Loss: 2.413889169692993 \n",
      "     Training Step: 14 Training Loss: 3.694615125656128 \n",
      "     Training Step: 15 Training Loss: 3.8220558166503906 \n",
      "     Training Step: 16 Training Loss: 2.7899537086486816 \n",
      "     Training Step: 17 Training Loss: 2.8846139907836914 \n",
      "     Training Step: 18 Training Loss: 2.8885769844055176 \n",
      "     Training Step: 19 Training Loss: 3.6845200061798096 \n",
      "     Training Step: 20 Training Loss: 2.6363768577575684 \n",
      "     Training Step: 21 Training Loss: 2.3099935054779053 \n",
      "     Training Step: 22 Training Loss: 2.7882025241851807 \n",
      "     Training Step: 23 Training Loss: 3.1490566730499268 \n",
      "     Training Step: 24 Training Loss: 2.84049654006958 \n",
      "     Training Step: 25 Training Loss: 3.1084470748901367 \n",
      "     Training Step: 26 Training Loss: 2.650998115539551 \n",
      "     Training Step: 27 Training Loss: 2.5145320892333984 \n",
      "     Training Step: 28 Training Loss: 3.225149154663086 \n",
      "     Training Step: 29 Training Loss: 2.3865747451782227 \n",
      "     Training Step: 30 Training Loss: 3.2863335609436035 \n",
      "     Training Step: 31 Training Loss: 2.550858497619629 \n",
      "     Training Step: 32 Training Loss: 4.010443210601807 \n",
      "     Training Step: 33 Training Loss: 2.6733767986297607 \n",
      "     Training Step: 34 Training Loss: 2.793221950531006 \n",
      "     Training Step: 35 Training Loss: 3.3221027851104736 \n",
      "     Training Step: 36 Training Loss: 2.415220260620117 \n",
      "     Training Step: 37 Training Loss: 2.185811996459961 \n",
      "     Training Step: 38 Training Loss: 2.963890552520752 \n",
      "     Training Step: 39 Training Loss: 2.699791431427002 \n",
      "     Training Step: 40 Training Loss: 2.819584369659424 \n",
      "     Training Step: 41 Training Loss: 4.393510818481445 \n",
      "     Training Step: 42 Training Loss: 2.8043837547302246 \n",
      "     Training Step: 43 Training Loss: 2.8925158977508545 \n",
      "     Training Step: 44 Training Loss: 2.8223044872283936 \n",
      "     Training Step: 45 Training Loss: 2.4004061222076416 \n",
      "     Training Step: 46 Training Loss: 2.777791976928711 \n",
      "     Training Step: 47 Training Loss: 2.7563300132751465 \n",
      "     Training Step: 48 Training Loss: 3.068240165710449 \n",
      "     Training Step: 49 Training Loss: 2.8629727363586426 \n",
      "     Training Step: 50 Training Loss: 4.65223503112793 \n",
      "     Training Step: 51 Training Loss: 3.260021686553955 \n",
      "     Training Step: 52 Training Loss: 2.828136444091797 \n",
      "     Training Step: 53 Training Loss: 3.045400619506836 \n",
      "     Training Step: 54 Training Loss: 2.4707388877868652 \n",
      "     Training Step: 55 Training Loss: 3.355945110321045 \n",
      "     Training Step: 56 Training Loss: 3.379423141479492 \n",
      "     Training Step: 57 Training Loss: 3.264540195465088 \n",
      "     Training Step: 58 Training Loss: 3.2019405364990234 \n",
      "     Training Step: 59 Training Loss: 3.252058506011963 \n",
      "     Training Step: 60 Training Loss: 3.6671204566955566 \n",
      "     Training Step: 61 Training Loss: 2.8773813247680664 \n",
      "     Training Step: 62 Training Loss: 3.275329828262329 \n",
      "     Training Step: 63 Training Loss: 2.8429057598114014 \n",
      "     Training Step: 64 Training Loss: 4.12801456451416 \n",
      "     Training Step: 65 Training Loss: 2.37164568901062 \n",
      "     Training Step: 66 Training Loss: 4.199367046356201 \n",
      "     Training Step: 67 Training Loss: 2.6401526927948 \n",
      "     Training Step: 68 Training Loss: 2.4857144355773926 \n",
      "     Training Step: 69 Training Loss: 2.565760612487793 \n",
      "     Training Step: 70 Training Loss: 2.721193552017212 \n",
      "     Training Step: 71 Training Loss: 3.8316667079925537 \n",
      "     Training Step: 72 Training Loss: 3.205169439315796 \n",
      "     Training Step: 73 Training Loss: 3.3253824710845947 \n",
      "     Training Step: 74 Training Loss: 2.7015182971954346 \n",
      "     Training Step: 75 Training Loss: 2.681657552719116 \n",
      "     Training Step: 76 Training Loss: 2.801240921020508 \n",
      "     Training Step: 77 Training Loss: 3.0749926567077637 \n",
      "     Training Step: 78 Training Loss: 2.207120895385742 \n",
      "     Training Step: 79 Training Loss: 4.0178303718566895 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.828429698944092 \n",
      "     Validation Step: 1 Validation Loss: 4.197119235992432 \n",
      "     Validation Step: 2 Validation Loss: 3.5465245246887207 \n",
      "     Validation Step: 3 Validation Loss: 3.6235432624816895 \n",
      "     Validation Step: 4 Validation Loss: 2.683095693588257 \n",
      "     Validation Step: 5 Validation Loss: 4.505313873291016 \n",
      "     Validation Step: 6 Validation Loss: 3.656599283218384 \n",
      "     Validation Step: 7 Validation Loss: 4.333347320556641 \n",
      "     Validation Step: 8 Validation Loss: 3.793977737426758 \n",
      "     Validation Step: 9 Validation Loss: 3.3418149948120117 \n",
      "     Validation Step: 10 Validation Loss: 2.5750203132629395 \n",
      "     Validation Step: 11 Validation Loss: 3.6512160301208496 \n",
      "     Validation Step: 12 Validation Loss: 2.595144510269165 \n",
      "     Validation Step: 13 Validation Loss: 4.450430870056152 \n",
      "     Validation Step: 14 Validation Loss: 3.0166468620300293 \n",
      "Epoch: 67\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.9800796508789062 \n",
      "     Training Step: 1 Training Loss: 2.404939651489258 \n",
      "     Training Step: 2 Training Loss: 2.968441963195801 \n",
      "     Training Step: 3 Training Loss: 3.6953983306884766 \n",
      "     Training Step: 4 Training Loss: 3.8599987030029297 \n",
      "     Training Step: 5 Training Loss: 3.1228983402252197 \n",
      "     Training Step: 6 Training Loss: 2.333810329437256 \n",
      "     Training Step: 7 Training Loss: 3.110743522644043 \n",
      "     Training Step: 8 Training Loss: 3.337332248687744 \n",
      "     Training Step: 9 Training Loss: 2.82924222946167 \n",
      "     Training Step: 10 Training Loss: 2.4106948375701904 \n",
      "     Training Step: 11 Training Loss: 3.095079183578491 \n",
      "     Training Step: 12 Training Loss: 2.444843053817749 \n",
      "     Training Step: 13 Training Loss: 4.1123199462890625 \n",
      "     Training Step: 14 Training Loss: 3.220590829849243 \n",
      "     Training Step: 15 Training Loss: 2.131993532180786 \n",
      "     Training Step: 16 Training Loss: 2.8694229125976562 \n",
      "     Training Step: 17 Training Loss: 4.137569904327393 \n",
      "     Training Step: 18 Training Loss: 2.2587108612060547 \n",
      "     Training Step: 19 Training Loss: 3.3002805709838867 \n",
      "     Training Step: 20 Training Loss: 3.308314561843872 \n",
      "     Training Step: 21 Training Loss: 2.6860976219177246 \n",
      "     Training Step: 22 Training Loss: 2.951270341873169 \n",
      "     Training Step: 23 Training Loss: 3.1124143600463867 \n",
      "     Training Step: 24 Training Loss: 3.300198554992676 \n",
      "     Training Step: 25 Training Loss: 3.1401734352111816 \n",
      "     Training Step: 26 Training Loss: 3.372130870819092 \n",
      "     Training Step: 27 Training Loss: 2.4699552059173584 \n",
      "     Training Step: 28 Training Loss: 3.412726879119873 \n",
      "     Training Step: 29 Training Loss: 3.546781539916992 \n",
      "     Training Step: 30 Training Loss: 2.3688292503356934 \n",
      "     Training Step: 31 Training Loss: 3.266605854034424 \n",
      "     Training Step: 32 Training Loss: 2.29292893409729 \n",
      "     Training Step: 33 Training Loss: 2.540274143218994 \n",
      "     Training Step: 34 Training Loss: 3.1423349380493164 \n",
      "     Training Step: 35 Training Loss: 2.8153107166290283 \n",
      "     Training Step: 36 Training Loss: 2.2511684894561768 \n",
      "     Training Step: 37 Training Loss: 2.931042194366455 \n",
      "     Training Step: 38 Training Loss: 3.5365376472473145 \n",
      "     Training Step: 39 Training Loss: 2.0464277267456055 \n",
      "     Training Step: 40 Training Loss: 2.648054599761963 \n",
      "     Training Step: 41 Training Loss: 3.1463382244110107 \n",
      "     Training Step: 42 Training Loss: 3.9189534187316895 \n",
      "     Training Step: 43 Training Loss: 2.12688946723938 \n",
      "     Training Step: 44 Training Loss: 2.584662675857544 \n",
      "     Training Step: 45 Training Loss: 2.915128231048584 \n",
      "     Training Step: 46 Training Loss: 3.7320687770843506 \n",
      "     Training Step: 47 Training Loss: 3.1199893951416016 \n",
      "     Training Step: 48 Training Loss: 2.626598834991455 \n",
      "     Training Step: 49 Training Loss: 3.555039882659912 \n",
      "     Training Step: 50 Training Loss: 3.595332622528076 \n",
      "     Training Step: 51 Training Loss: 3.6844005584716797 \n",
      "     Training Step: 52 Training Loss: 3.7816548347473145 \n",
      "     Training Step: 53 Training Loss: 2.503875255584717 \n",
      "     Training Step: 54 Training Loss: 2.49776029586792 \n",
      "     Training Step: 55 Training Loss: 4.026512622833252 \n",
      "     Training Step: 56 Training Loss: 4.050997257232666 \n",
      "     Training Step: 57 Training Loss: 2.4530670642852783 \n",
      "     Training Step: 58 Training Loss: 3.1619176864624023 \n",
      "     Training Step: 59 Training Loss: 2.446878671646118 \n",
      "     Training Step: 60 Training Loss: 3.659313201904297 \n",
      "     Training Step: 61 Training Loss: 3.094566822052002 \n",
      "     Training Step: 62 Training Loss: 2.3307206630706787 \n",
      "     Training Step: 63 Training Loss: 2.230520248413086 \n",
      "     Training Step: 64 Training Loss: 2.4143435955047607 \n",
      "     Training Step: 65 Training Loss: 2.4176065921783447 \n",
      "     Training Step: 66 Training Loss: 2.1693615913391113 \n",
      "     Training Step: 67 Training Loss: 2.4591567516326904 \n",
      "     Training Step: 68 Training Loss: 2.457047939300537 \n",
      "     Training Step: 69 Training Loss: 3.645663261413574 \n",
      "     Training Step: 70 Training Loss: 2.6148295402526855 \n",
      "     Training Step: 71 Training Loss: 3.6586437225341797 \n",
      "     Training Step: 72 Training Loss: 3.2987351417541504 \n",
      "     Training Step: 73 Training Loss: 2.7996246814727783 \n",
      "     Training Step: 74 Training Loss: 3.1399145126342773 \n",
      "     Training Step: 75 Training Loss: 3.702789306640625 \n",
      "     Training Step: 76 Training Loss: 2.5563578605651855 \n",
      "     Training Step: 77 Training Loss: 3.049520492553711 \n",
      "     Training Step: 78 Training Loss: 3.159393310546875 \n",
      "     Training Step: 79 Training Loss: 2.6528210639953613 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.245046854019165 \n",
      "     Validation Step: 1 Validation Loss: 2.7663326263427734 \n",
      "     Validation Step: 2 Validation Loss: 2.2890074253082275 \n",
      "     Validation Step: 3 Validation Loss: 3.5367319583892822 \n",
      "     Validation Step: 4 Validation Loss: 3.752258777618408 \n",
      "     Validation Step: 5 Validation Loss: 3.3443093299865723 \n",
      "     Validation Step: 6 Validation Loss: 2.5215110778808594 \n",
      "     Validation Step: 7 Validation Loss: 3.1630852222442627 \n",
      "     Validation Step: 8 Validation Loss: 2.254012107849121 \n",
      "     Validation Step: 9 Validation Loss: 2.578977584838867 \n",
      "     Validation Step: 10 Validation Loss: 2.926823139190674 \n",
      "     Validation Step: 11 Validation Loss: 2.5314865112304688 \n",
      "     Validation Step: 12 Validation Loss: 2.9408857822418213 \n",
      "     Validation Step: 13 Validation Loss: 2.9075193405151367 \n",
      "     Validation Step: 14 Validation Loss: 3.76043701171875 \n",
      "Epoch: 68\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.92633056640625 \n",
      "     Training Step: 1 Training Loss: 3.888920783996582 \n",
      "     Training Step: 2 Training Loss: 4.255284786224365 \n",
      "     Training Step: 3 Training Loss: 2.32720685005188 \n",
      "     Training Step: 4 Training Loss: 3.0711750984191895 \n",
      "     Training Step: 5 Training Loss: 3.0128839015960693 \n",
      "     Training Step: 6 Training Loss: 3.2994914054870605 \n",
      "     Training Step: 7 Training Loss: 4.189970970153809 \n",
      "     Training Step: 8 Training Loss: 3.446687698364258 \n",
      "     Training Step: 9 Training Loss: 3.4511466026306152 \n",
      "     Training Step: 10 Training Loss: 2.2584075927734375 \n",
      "     Training Step: 11 Training Loss: 2.229454278945923 \n",
      "     Training Step: 12 Training Loss: 3.8915820121765137 \n",
      "     Training Step: 13 Training Loss: 2.9185805320739746 \n",
      "     Training Step: 14 Training Loss: 3.120506763458252 \n",
      "     Training Step: 15 Training Loss: 3.2052130699157715 \n",
      "     Training Step: 16 Training Loss: 2.2704856395721436 \n",
      "     Training Step: 17 Training Loss: 2.7452425956726074 \n",
      "     Training Step: 18 Training Loss: 2.514890432357788 \n",
      "     Training Step: 19 Training Loss: 2.288527727127075 \n",
      "     Training Step: 20 Training Loss: 2.964277744293213 \n",
      "     Training Step: 21 Training Loss: 2.6581850051879883 \n",
      "     Training Step: 22 Training Loss: 2.586643695831299 \n",
      "     Training Step: 23 Training Loss: 3.4925711154937744 \n",
      "     Training Step: 24 Training Loss: 3.0198493003845215 \n",
      "     Training Step: 25 Training Loss: 3.110100269317627 \n",
      "     Training Step: 26 Training Loss: 3.5676517486572266 \n",
      "     Training Step: 27 Training Loss: 3.1490564346313477 \n",
      "     Training Step: 28 Training Loss: 2.0429420471191406 \n",
      "     Training Step: 29 Training Loss: 2.8243188858032227 \n",
      "     Training Step: 30 Training Loss: 2.632249355316162 \n",
      "     Training Step: 31 Training Loss: 2.1323814392089844 \n",
      "     Training Step: 32 Training Loss: 3.132200241088867 \n",
      "     Training Step: 33 Training Loss: 3.4895029067993164 \n",
      "     Training Step: 34 Training Loss: 3.348206043243408 \n",
      "     Training Step: 35 Training Loss: 2.472032308578491 \n",
      "     Training Step: 36 Training Loss: 2.5806922912597656 \n",
      "     Training Step: 37 Training Loss: 2.9500184059143066 \n",
      "     Training Step: 38 Training Loss: 3.2351291179656982 \n",
      "     Training Step: 39 Training Loss: 2.5871729850769043 \n",
      "     Training Step: 40 Training Loss: 3.031161308288574 \n",
      "     Training Step: 41 Training Loss: 2.394306182861328 \n",
      "     Training Step: 42 Training Loss: 3.5740151405334473 \n",
      "     Training Step: 43 Training Loss: 3.485480785369873 \n",
      "     Training Step: 44 Training Loss: 2.946516513824463 \n",
      "     Training Step: 45 Training Loss: 3.057931423187256 \n",
      "     Training Step: 46 Training Loss: 2.6497881412506104 \n",
      "     Training Step: 47 Training Loss: 3.3137545585632324 \n",
      "     Training Step: 48 Training Loss: 2.2678942680358887 \n",
      "     Training Step: 49 Training Loss: 3.773848533630371 \n",
      "     Training Step: 50 Training Loss: 2.6342127323150635 \n",
      "     Training Step: 51 Training Loss: 4.194113731384277 \n",
      "     Training Step: 52 Training Loss: 3.086387872695923 \n",
      "     Training Step: 53 Training Loss: 2.8747386932373047 \n",
      "     Training Step: 54 Training Loss: 2.9177238941192627 \n",
      "     Training Step: 55 Training Loss: 3.993530750274658 \n",
      "     Training Step: 56 Training Loss: 3.256727695465088 \n",
      "     Training Step: 57 Training Loss: 2.7596187591552734 \n",
      "     Training Step: 58 Training Loss: 2.404909610748291 \n",
      "     Training Step: 59 Training Loss: 2.506186008453369 \n",
      "     Training Step: 60 Training Loss: 3.189793586730957 \n",
      "     Training Step: 61 Training Loss: 2.401590585708618 \n",
      "     Training Step: 62 Training Loss: 2.659193992614746 \n",
      "     Training Step: 63 Training Loss: 3.0411224365234375 \n",
      "     Training Step: 64 Training Loss: 2.4407236576080322 \n",
      "     Training Step: 65 Training Loss: 2.955921173095703 \n",
      "     Training Step: 66 Training Loss: 2.8083810806274414 \n",
      "     Training Step: 67 Training Loss: 3.055178165435791 \n",
      "     Training Step: 68 Training Loss: 3.1392412185668945 \n",
      "     Training Step: 69 Training Loss: 2.0809638500213623 \n",
      "     Training Step: 70 Training Loss: 2.4605884552001953 \n",
      "     Training Step: 71 Training Loss: 3.4012644290924072 \n",
      "     Training Step: 72 Training Loss: 2.8914644718170166 \n",
      "     Training Step: 73 Training Loss: 3.0658347606658936 \n",
      "     Training Step: 74 Training Loss: 3.0409350395202637 \n",
      "     Training Step: 75 Training Loss: 3.206784248352051 \n",
      "     Training Step: 76 Training Loss: 3.6537787914276123 \n",
      "     Training Step: 77 Training Loss: 3.303849697113037 \n",
      "     Training Step: 78 Training Loss: 2.143547534942627 \n",
      "     Training Step: 79 Training Loss: 2.591796398162842 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1738250255584717 \n",
      "     Validation Step: 1 Validation Loss: 2.976388692855835 \n",
      "     Validation Step: 2 Validation Loss: 2.900782823562622 \n",
      "     Validation Step: 3 Validation Loss: 2.64689040184021 \n",
      "     Validation Step: 4 Validation Loss: 3.5986595153808594 \n",
      "     Validation Step: 5 Validation Loss: 2.8018083572387695 \n",
      "     Validation Step: 6 Validation Loss: 3.9189863204956055 \n",
      "     Validation Step: 7 Validation Loss: 2.3344802856445312 \n",
      "     Validation Step: 8 Validation Loss: 3.2001137733459473 \n",
      "     Validation Step: 9 Validation Loss: 3.630046844482422 \n",
      "     Validation Step: 10 Validation Loss: 2.641007423400879 \n",
      "     Validation Step: 11 Validation Loss: 3.10603666305542 \n",
      "     Validation Step: 12 Validation Loss: 3.5892157554626465 \n",
      "     Validation Step: 13 Validation Loss: 3.2980189323425293 \n",
      "     Validation Step: 14 Validation Loss: 2.805398464202881 \n",
      "Epoch: 69\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.4569618701934814 \n",
      "     Training Step: 1 Training Loss: 3.118788480758667 \n",
      "     Training Step: 2 Training Loss: 2.4244837760925293 \n",
      "     Training Step: 3 Training Loss: 3.1173019409179688 \n",
      "     Training Step: 4 Training Loss: 2.9197497367858887 \n",
      "     Training Step: 5 Training Loss: 3.452955722808838 \n",
      "     Training Step: 6 Training Loss: 2.28100323677063 \n",
      "     Training Step: 7 Training Loss: 3.7038564682006836 \n",
      "     Training Step: 8 Training Loss: 3.1633975505828857 \n",
      "     Training Step: 9 Training Loss: 2.5192508697509766 \n",
      "     Training Step: 10 Training Loss: 3.100217819213867 \n",
      "     Training Step: 11 Training Loss: 2.558281421661377 \n",
      "     Training Step: 12 Training Loss: 2.336671829223633 \n",
      "     Training Step: 13 Training Loss: 2.6309447288513184 \n",
      "     Training Step: 14 Training Loss: 3.7643752098083496 \n",
      "     Training Step: 15 Training Loss: 3.329288959503174 \n",
      "     Training Step: 16 Training Loss: 2.8244190216064453 \n",
      "     Training Step: 17 Training Loss: 3.1597585678100586 \n",
      "     Training Step: 18 Training Loss: 2.4332666397094727 \n",
      "     Training Step: 19 Training Loss: 3.504819393157959 \n",
      "     Training Step: 20 Training Loss: 2.9182848930358887 \n",
      "     Training Step: 21 Training Loss: 3.1757771968841553 \n",
      "     Training Step: 22 Training Loss: 3.2260148525238037 \n",
      "     Training Step: 23 Training Loss: 2.7132151126861572 \n",
      "     Training Step: 24 Training Loss: 2.0834109783172607 \n",
      "     Training Step: 25 Training Loss: 2.6420178413391113 \n",
      "     Training Step: 26 Training Loss: 3.084001302719116 \n",
      "     Training Step: 27 Training Loss: 2.719355583190918 \n",
      "     Training Step: 28 Training Loss: 2.2316341400146484 \n",
      "     Training Step: 29 Training Loss: 2.5876529216766357 \n",
      "     Training Step: 30 Training Loss: 2.870124101638794 \n",
      "     Training Step: 31 Training Loss: 4.296316146850586 \n",
      "     Training Step: 32 Training Loss: 3.0355005264282227 \n",
      "     Training Step: 33 Training Loss: 3.12387752532959 \n",
      "     Training Step: 34 Training Loss: 2.9071879386901855 \n",
      "     Training Step: 35 Training Loss: 3.0135061740875244 \n",
      "     Training Step: 36 Training Loss: 3.276231288909912 \n",
      "     Training Step: 37 Training Loss: 3.941862106323242 \n",
      "     Training Step: 38 Training Loss: 3.645176410675049 \n",
      "     Training Step: 39 Training Loss: 2.52834415435791 \n",
      "     Training Step: 40 Training Loss: 2.8216867446899414 \n",
      "     Training Step: 41 Training Loss: 2.282526969909668 \n",
      "     Training Step: 42 Training Loss: 2.809553623199463 \n",
      "     Training Step: 43 Training Loss: 3.769899368286133 \n",
      "     Training Step: 44 Training Loss: 2.608783721923828 \n",
      "     Training Step: 45 Training Loss: 2.8472492694854736 \n",
      "     Training Step: 46 Training Loss: 3.091465950012207 \n",
      "     Training Step: 47 Training Loss: 3.4579672813415527 \n",
      "     Training Step: 48 Training Loss: 3.446979284286499 \n",
      "     Training Step: 49 Training Loss: 2.7618069648742676 \n",
      "     Training Step: 50 Training Loss: 4.327534198760986 \n",
      "     Training Step: 51 Training Loss: 2.286712169647217 \n",
      "     Training Step: 52 Training Loss: 3.675461530685425 \n",
      "     Training Step: 53 Training Loss: 2.493455410003662 \n",
      "     Training Step: 54 Training Loss: 2.844625949859619 \n",
      "     Training Step: 55 Training Loss: 2.324678421020508 \n",
      "     Training Step: 56 Training Loss: 2.46878719329834 \n",
      "     Training Step: 57 Training Loss: 3.6717286109924316 \n",
      "     Training Step: 58 Training Loss: 2.870272159576416 \n",
      "     Training Step: 59 Training Loss: 2.6702914237976074 \n",
      "     Training Step: 60 Training Loss: 3.5796926021575928 \n",
      "     Training Step: 61 Training Loss: 3.891183376312256 \n",
      "     Training Step: 62 Training Loss: 3.489076614379883 \n",
      "     Training Step: 63 Training Loss: 2.6481752395629883 \n",
      "     Training Step: 64 Training Loss: 4.025807857513428 \n",
      "     Training Step: 65 Training Loss: 3.1932716369628906 \n",
      "     Training Step: 66 Training Loss: 2.8548648357391357 \n",
      "     Training Step: 67 Training Loss: 2.895651340484619 \n",
      "     Training Step: 68 Training Loss: 3.1967365741729736 \n",
      "     Training Step: 69 Training Loss: 3.0581817626953125 \n",
      "     Training Step: 70 Training Loss: 3.4135994911193848 \n",
      "     Training Step: 71 Training Loss: 3.4416608810424805 \n",
      "     Training Step: 72 Training Loss: 3.0653493404388428 \n",
      "     Training Step: 73 Training Loss: 2.87869930267334 \n",
      "     Training Step: 74 Training Loss: 2.4079794883728027 \n",
      "     Training Step: 75 Training Loss: 3.728273868560791 \n",
      "     Training Step: 76 Training Loss: 2.7098488807678223 \n",
      "     Training Step: 77 Training Loss: 2.8159189224243164 \n",
      "     Training Step: 78 Training Loss: 2.86495304107666 \n",
      "     Training Step: 79 Training Loss: 2.741276264190674 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.4629292488098145 \n",
      "     Validation Step: 1 Validation Loss: 3.186652660369873 \n",
      "     Validation Step: 2 Validation Loss: 3.0839099884033203 \n",
      "     Validation Step: 3 Validation Loss: 3.952679395675659 \n",
      "     Validation Step: 4 Validation Loss: 2.7806310653686523 \n",
      "     Validation Step: 5 Validation Loss: 3.3821873664855957 \n",
      "     Validation Step: 6 Validation Loss: 3.904107093811035 \n",
      "     Validation Step: 7 Validation Loss: 4.131795883178711 \n",
      "     Validation Step: 8 Validation Loss: 3.0460362434387207 \n",
      "     Validation Step: 9 Validation Loss: 3.387998104095459 \n",
      "     Validation Step: 10 Validation Loss: 3.5851681232452393 \n",
      "     Validation Step: 11 Validation Loss: 3.3068745136260986 \n",
      "     Validation Step: 12 Validation Loss: 2.8595941066741943 \n",
      "     Validation Step: 13 Validation Loss: 2.758230686187744 \n",
      "     Validation Step: 14 Validation Loss: 3.1515703201293945 \n",
      "Epoch: 70\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.092508554458618 \n",
      "     Training Step: 1 Training Loss: 2.3504714965820312 \n",
      "     Training Step: 2 Training Loss: 3.7740707397460938 \n",
      "     Training Step: 3 Training Loss: 3.581231117248535 \n",
      "     Training Step: 4 Training Loss: 2.3271303176879883 \n",
      "     Training Step: 5 Training Loss: 2.433692216873169 \n",
      "     Training Step: 6 Training Loss: 2.3223042488098145 \n",
      "     Training Step: 7 Training Loss: 3.330448865890503 \n",
      "     Training Step: 8 Training Loss: 2.610761880874634 \n",
      "     Training Step: 9 Training Loss: 3.693258047103882 \n",
      "     Training Step: 10 Training Loss: 3.0851614475250244 \n",
      "     Training Step: 11 Training Loss: 3.4503250122070312 \n",
      "     Training Step: 12 Training Loss: 2.4077529907226562 \n",
      "     Training Step: 13 Training Loss: 2.4707589149475098 \n",
      "     Training Step: 14 Training Loss: 2.511570692062378 \n",
      "     Training Step: 15 Training Loss: 2.7364187240600586 \n",
      "     Training Step: 16 Training Loss: 2.5472476482391357 \n",
      "     Training Step: 17 Training Loss: 2.656698703765869 \n",
      "     Training Step: 18 Training Loss: 2.339851140975952 \n",
      "     Training Step: 19 Training Loss: 3.4255573749542236 \n",
      "     Training Step: 20 Training Loss: 3.1218056678771973 \n",
      "     Training Step: 21 Training Loss: 2.902228832244873 \n",
      "     Training Step: 22 Training Loss: 2.4731690883636475 \n",
      "     Training Step: 23 Training Loss: 2.4494121074676514 \n",
      "     Training Step: 24 Training Loss: 2.1316781044006348 \n",
      "     Training Step: 25 Training Loss: 2.2452292442321777 \n",
      "     Training Step: 26 Training Loss: 3.619837522506714 \n",
      "     Training Step: 27 Training Loss: 2.962334632873535 \n",
      "     Training Step: 28 Training Loss: 2.156996488571167 \n",
      "     Training Step: 29 Training Loss: 2.778700113296509 \n",
      "     Training Step: 30 Training Loss: 2.9550037384033203 \n",
      "     Training Step: 31 Training Loss: 2.5129175186157227 \n",
      "     Training Step: 32 Training Loss: 4.55842399597168 \n",
      "     Training Step: 33 Training Loss: 3.26843523979187 \n",
      "     Training Step: 34 Training Loss: 3.4783873558044434 \n",
      "     Training Step: 35 Training Loss: 2.663784980773926 \n",
      "     Training Step: 36 Training Loss: 4.338017463684082 \n",
      "     Training Step: 37 Training Loss: 2.3144054412841797 \n",
      "     Training Step: 38 Training Loss: 2.8360068798065186 \n",
      "     Training Step: 39 Training Loss: 2.4673337936401367 \n",
      "     Training Step: 40 Training Loss: 2.619072914123535 \n",
      "     Training Step: 41 Training Loss: 2.926816940307617 \n",
      "     Training Step: 42 Training Loss: 3.1883339881896973 \n",
      "     Training Step: 43 Training Loss: 2.591238498687744 \n",
      "     Training Step: 44 Training Loss: 4.169231414794922 \n",
      "     Training Step: 45 Training Loss: 3.551642894744873 \n",
      "     Training Step: 46 Training Loss: 2.6757893562316895 \n",
      "     Training Step: 47 Training Loss: 2.7308573722839355 \n",
      "     Training Step: 48 Training Loss: 3.8725790977478027 \n",
      "     Training Step: 49 Training Loss: 2.725267171859741 \n",
      "     Training Step: 50 Training Loss: 2.5984106063842773 \n",
      "     Training Step: 51 Training Loss: 4.013891220092773 \n",
      "     Training Step: 52 Training Loss: 2.3238179683685303 \n",
      "     Training Step: 53 Training Loss: 2.9592602252960205 \n",
      "     Training Step: 54 Training Loss: 3.466717004776001 \n",
      "     Training Step: 55 Training Loss: 2.2259469032287598 \n",
      "     Training Step: 56 Training Loss: 2.4811506271362305 \n",
      "     Training Step: 57 Training Loss: 2.326690673828125 \n",
      "     Training Step: 58 Training Loss: 3.901884078979492 \n",
      "     Training Step: 59 Training Loss: 2.9461183547973633 \n",
      "     Training Step: 60 Training Loss: 3.6660494804382324 \n",
      "     Training Step: 61 Training Loss: 2.8513550758361816 \n",
      "     Training Step: 62 Training Loss: 3.0426859855651855 \n",
      "     Training Step: 63 Training Loss: 3.4102654457092285 \n",
      "     Training Step: 64 Training Loss: 3.62593936920166 \n",
      "     Training Step: 65 Training Loss: 2.2639670372009277 \n",
      "     Training Step: 66 Training Loss: 2.8863027095794678 \n",
      "     Training Step: 67 Training Loss: 2.6659774780273438 \n",
      "     Training Step: 68 Training Loss: 3.024656295776367 \n",
      "     Training Step: 69 Training Loss: 2.8956503868103027 \n",
      "     Training Step: 70 Training Loss: 2.6805779933929443 \n",
      "     Training Step: 71 Training Loss: 3.0748329162597656 \n",
      "     Training Step: 72 Training Loss: 3.0382089614868164 \n",
      "     Training Step: 73 Training Loss: 3.0359930992126465 \n",
      "     Training Step: 74 Training Loss: 4.26733922958374 \n",
      "     Training Step: 75 Training Loss: 3.740424156188965 \n",
      "     Training Step: 76 Training Loss: 3.260685443878174 \n",
      "     Training Step: 77 Training Loss: 2.789181709289551 \n",
      "     Training Step: 78 Training Loss: 2.4530529975891113 \n",
      "     Training Step: 79 Training Loss: 2.1625802516937256 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.839892864227295 \n",
      "     Validation Step: 1 Validation Loss: 3.652275800704956 \n",
      "     Validation Step: 2 Validation Loss: 3.140718460083008 \n",
      "     Validation Step: 3 Validation Loss: 3.0058722496032715 \n",
      "     Validation Step: 4 Validation Loss: 3.616770029067993 \n",
      "     Validation Step: 5 Validation Loss: 3.241367816925049 \n",
      "     Validation Step: 6 Validation Loss: 2.8264873027801514 \n",
      "     Validation Step: 7 Validation Loss: 3.220012903213501 \n",
      "     Validation Step: 8 Validation Loss: 3.7282748222351074 \n",
      "     Validation Step: 9 Validation Loss: 3.033780813217163 \n",
      "     Validation Step: 10 Validation Loss: 2.0371928215026855 \n",
      "     Validation Step: 11 Validation Loss: 3.1720359325408936 \n",
      "     Validation Step: 12 Validation Loss: 3.2643210887908936 \n",
      "     Validation Step: 13 Validation Loss: 3.8247547149658203 \n",
      "     Validation Step: 14 Validation Loss: 2.994004487991333 \n",
      "Epoch: 71\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.2144975662231445 \n",
      "     Training Step: 1 Training Loss: 3.11665678024292 \n",
      "     Training Step: 2 Training Loss: 3.1273820400238037 \n",
      "     Training Step: 3 Training Loss: 2.870293140411377 \n",
      "     Training Step: 4 Training Loss: 2.6075549125671387 \n",
      "     Training Step: 5 Training Loss: 3.7189786434173584 \n",
      "     Training Step: 6 Training Loss: 3.311227321624756 \n",
      "     Training Step: 7 Training Loss: 3.038839817047119 \n",
      "     Training Step: 8 Training Loss: 3.7889065742492676 \n",
      "     Training Step: 9 Training Loss: 2.5586297512054443 \n",
      "     Training Step: 10 Training Loss: 2.867814064025879 \n",
      "     Training Step: 11 Training Loss: 2.6700611114501953 \n",
      "     Training Step: 12 Training Loss: 2.5887975692749023 \n",
      "     Training Step: 13 Training Loss: 2.653841972351074 \n",
      "     Training Step: 14 Training Loss: 3.4666523933410645 \n",
      "     Training Step: 15 Training Loss: 2.835458517074585 \n",
      "     Training Step: 16 Training Loss: 2.85123348236084 \n",
      "     Training Step: 17 Training Loss: 2.133788585662842 \n",
      "     Training Step: 18 Training Loss: 3.039170026779175 \n",
      "     Training Step: 19 Training Loss: 2.2482900619506836 \n",
      "     Training Step: 20 Training Loss: 2.7020068168640137 \n",
      "     Training Step: 21 Training Loss: 2.0196237564086914 \n",
      "     Training Step: 22 Training Loss: 3.631206750869751 \n",
      "     Training Step: 23 Training Loss: 2.964311122894287 \n",
      "     Training Step: 24 Training Loss: 2.4331789016723633 \n",
      "     Training Step: 25 Training Loss: 3.993518352508545 \n",
      "     Training Step: 26 Training Loss: 3.9214391708374023 \n",
      "     Training Step: 27 Training Loss: 4.110648155212402 \n",
      "     Training Step: 28 Training Loss: 2.516742706298828 \n",
      "     Training Step: 29 Training Loss: 4.3122029304504395 \n",
      "     Training Step: 30 Training Loss: 3.9425315856933594 \n",
      "     Training Step: 31 Training Loss: 3.1601760387420654 \n",
      "     Training Step: 32 Training Loss: 3.585658073425293 \n",
      "     Training Step: 33 Training Loss: 3.7493906021118164 \n",
      "     Training Step: 34 Training Loss: 3.2103147506713867 \n",
      "     Training Step: 35 Training Loss: 2.6787304878234863 \n",
      "     Training Step: 36 Training Loss: 2.920753240585327 \n",
      "     Training Step: 37 Training Loss: 3.4049129486083984 \n",
      "     Training Step: 38 Training Loss: 3.2271852493286133 \n",
      "     Training Step: 39 Training Loss: 3.935506820678711 \n",
      "     Training Step: 40 Training Loss: 3.004455089569092 \n",
      "     Training Step: 41 Training Loss: 2.689412832260132 \n",
      "     Training Step: 42 Training Loss: 2.643517255783081 \n",
      "     Training Step: 43 Training Loss: 3.3351805210113525 \n",
      "     Training Step: 44 Training Loss: 2.72222900390625 \n",
      "     Training Step: 45 Training Loss: 3.2947134971618652 \n",
      "     Training Step: 46 Training Loss: 2.178083896636963 \n",
      "     Training Step: 47 Training Loss: 3.057300090789795 \n",
      "     Training Step: 48 Training Loss: 3.946108341217041 \n",
      "     Training Step: 49 Training Loss: 2.2175238132476807 \n",
      "     Training Step: 50 Training Loss: 2.203476905822754 \n",
      "     Training Step: 51 Training Loss: 2.7790305614471436 \n",
      "     Training Step: 52 Training Loss: 2.2491555213928223 \n",
      "     Training Step: 53 Training Loss: 3.3149826526641846 \n",
      "     Training Step: 54 Training Loss: 3.2599215507507324 \n",
      "     Training Step: 55 Training Loss: 4.5201568603515625 \n",
      "     Training Step: 56 Training Loss: 2.9347825050354004 \n",
      "     Training Step: 57 Training Loss: 3.4656643867492676 \n",
      "     Training Step: 58 Training Loss: 3.0578501224517822 \n",
      "     Training Step: 59 Training Loss: 3.6030285358428955 \n",
      "     Training Step: 60 Training Loss: 2.422071933746338 \n",
      "     Training Step: 61 Training Loss: 3.132904529571533 \n",
      "     Training Step: 62 Training Loss: 3.4488561153411865 \n",
      "     Training Step: 63 Training Loss: 2.3296589851379395 \n",
      "     Training Step: 64 Training Loss: 2.7239973545074463 \n",
      "     Training Step: 65 Training Loss: 2.173391819000244 \n",
      "     Training Step: 66 Training Loss: 3.9351296424865723 \n",
      "     Training Step: 67 Training Loss: 3.5744895935058594 \n",
      "     Training Step: 68 Training Loss: 2.530240058898926 \n",
      "     Training Step: 69 Training Loss: 3.099202871322632 \n",
      "     Training Step: 70 Training Loss: 2.7750301361083984 \n",
      "     Training Step: 71 Training Loss: 3.0274832248687744 \n",
      "     Training Step: 72 Training Loss: 2.688926935195923 \n",
      "     Training Step: 73 Training Loss: 2.340763568878174 \n",
      "     Training Step: 74 Training Loss: 2.9146933555603027 \n",
      "     Training Step: 75 Training Loss: 3.010559558868408 \n",
      "     Training Step: 76 Training Loss: 3.3687686920166016 \n",
      "     Training Step: 77 Training Loss: 2.8069818019866943 \n",
      "     Training Step: 78 Training Loss: 3.9904985427856445 \n",
      "     Training Step: 79 Training Loss: 3.0871338844299316 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9581360816955566 \n",
      "     Validation Step: 1 Validation Loss: 3.515228271484375 \n",
      "     Validation Step: 2 Validation Loss: 3.146388530731201 \n",
      "     Validation Step: 3 Validation Loss: 3.9771599769592285 \n",
      "     Validation Step: 4 Validation Loss: 2.77168869972229 \n",
      "     Validation Step: 5 Validation Loss: 3.3409769535064697 \n",
      "     Validation Step: 6 Validation Loss: 4.107226371765137 \n",
      "     Validation Step: 7 Validation Loss: 2.9078633785247803 \n",
      "     Validation Step: 8 Validation Loss: 3.814162015914917 \n",
      "     Validation Step: 9 Validation Loss: 3.182302474975586 \n",
      "     Validation Step: 10 Validation Loss: 2.985215663909912 \n",
      "     Validation Step: 11 Validation Loss: 2.8925960063934326 \n",
      "     Validation Step: 12 Validation Loss: 3.254363536834717 \n",
      "     Validation Step: 13 Validation Loss: 3.6353023052215576 \n",
      "     Validation Step: 14 Validation Loss: 2.3038971424102783 \n",
      "Epoch: 72\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.2357876300811768 \n",
      "     Training Step: 1 Training Loss: 3.831292152404785 \n",
      "     Training Step: 2 Training Loss: 2.3062307834625244 \n",
      "     Training Step: 3 Training Loss: 3.4777581691741943 \n",
      "     Training Step: 4 Training Loss: 3.375150203704834 \n",
      "     Training Step: 5 Training Loss: 2.6379878520965576 \n",
      "     Training Step: 6 Training Loss: 2.910569190979004 \n",
      "     Training Step: 7 Training Loss: 3.5169525146484375 \n",
      "     Training Step: 8 Training Loss: 3.017564296722412 \n",
      "     Training Step: 9 Training Loss: 2.569835662841797 \n",
      "     Training Step: 10 Training Loss: 3.069490432739258 \n",
      "     Training Step: 11 Training Loss: 2.8517491817474365 \n",
      "     Training Step: 12 Training Loss: 2.9531140327453613 \n",
      "     Training Step: 13 Training Loss: 2.6418185234069824 \n",
      "     Training Step: 14 Training Loss: 4.1732025146484375 \n",
      "     Training Step: 15 Training Loss: 2.8207523822784424 \n",
      "     Training Step: 16 Training Loss: 2.351916790008545 \n",
      "     Training Step: 17 Training Loss: 2.1709036827087402 \n",
      "     Training Step: 18 Training Loss: 3.387037992477417 \n",
      "     Training Step: 19 Training Loss: 2.5357494354248047 \n",
      "     Training Step: 20 Training Loss: 3.3668394088745117 \n",
      "     Training Step: 21 Training Loss: 2.658308744430542 \n",
      "     Training Step: 22 Training Loss: 3.1958506107330322 \n",
      "     Training Step: 23 Training Loss: 3.3137245178222656 \n",
      "     Training Step: 24 Training Loss: 2.7852232456207275 \n",
      "     Training Step: 25 Training Loss: 3.22206449508667 \n",
      "     Training Step: 26 Training Loss: 2.6629831790924072 \n",
      "     Training Step: 27 Training Loss: 2.731424331665039 \n",
      "     Training Step: 28 Training Loss: 3.108336925506592 \n",
      "     Training Step: 29 Training Loss: 2.4564194679260254 \n",
      "     Training Step: 30 Training Loss: 3.2378435134887695 \n",
      "     Training Step: 31 Training Loss: 3.816606044769287 \n",
      "     Training Step: 32 Training Loss: 3.4593095779418945 \n",
      "     Training Step: 33 Training Loss: 3.0671849250793457 \n",
      "     Training Step: 34 Training Loss: 3.0405023097991943 \n",
      "     Training Step: 35 Training Loss: 2.400937557220459 \n",
      "     Training Step: 36 Training Loss: 3.151981830596924 \n",
      "     Training Step: 37 Training Loss: 2.869431257247925 \n",
      "     Training Step: 38 Training Loss: 3.344102621078491 \n",
      "     Training Step: 39 Training Loss: 3.0962142944335938 \n",
      "     Training Step: 40 Training Loss: 3.543107032775879 \n",
      "     Training Step: 41 Training Loss: 2.6534597873687744 \n",
      "     Training Step: 42 Training Loss: 2.943896770477295 \n",
      "     Training Step: 43 Training Loss: 3.830364465713501 \n",
      "     Training Step: 44 Training Loss: 2.929915189743042 \n",
      "     Training Step: 45 Training Loss: 3.6600940227508545 \n",
      "     Training Step: 46 Training Loss: 3.2512903213500977 \n",
      "     Training Step: 47 Training Loss: 2.7292771339416504 \n",
      "     Training Step: 48 Training Loss: 2.690009832382202 \n",
      "     Training Step: 49 Training Loss: 3.6105761528015137 \n",
      "     Training Step: 50 Training Loss: 2.111645221710205 \n",
      "     Training Step: 51 Training Loss: 2.672157049179077 \n",
      "     Training Step: 52 Training Loss: 2.1913962364196777 \n",
      "     Training Step: 53 Training Loss: 3.032698154449463 \n",
      "     Training Step: 54 Training Loss: 3.598752498626709 \n",
      "     Training Step: 55 Training Loss: 2.919260025024414 \n",
      "     Training Step: 56 Training Loss: 2.6615428924560547 \n",
      "     Training Step: 57 Training Loss: 3.5123515129089355 \n",
      "     Training Step: 58 Training Loss: 2.9950942993164062 \n",
      "     Training Step: 59 Training Loss: 3.5167832374572754 \n",
      "     Training Step: 60 Training Loss: 2.7507050037384033 \n",
      "     Training Step: 61 Training Loss: 3.177415370941162 \n",
      "     Training Step: 62 Training Loss: 3.277305841445923 \n",
      "     Training Step: 63 Training Loss: 2.6854164600372314 \n",
      "     Training Step: 64 Training Loss: 2.4980900287628174 \n",
      "     Training Step: 65 Training Loss: 3.2005231380462646 \n",
      "     Training Step: 66 Training Loss: 3.3334035873413086 \n",
      "     Training Step: 67 Training Loss: 2.148317337036133 \n",
      "     Training Step: 68 Training Loss: 2.682422161102295 \n",
      "     Training Step: 69 Training Loss: 2.5581765174865723 \n",
      "     Training Step: 70 Training Loss: 4.279880523681641 \n",
      "     Training Step: 71 Training Loss: 2.411449909210205 \n",
      "     Training Step: 72 Training Loss: 3.9151601791381836 \n",
      "     Training Step: 73 Training Loss: 3.0481910705566406 \n",
      "     Training Step: 74 Training Loss: 2.3934521675109863 \n",
      "     Training Step: 75 Training Loss: 3.141209363937378 \n",
      "     Training Step: 76 Training Loss: 3.080069065093994 \n",
      "     Training Step: 77 Training Loss: 2.8056023120880127 \n",
      "     Training Step: 78 Training Loss: 2.5628015995025635 \n",
      "     Training Step: 79 Training Loss: 2.6715610027313232 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.650590419769287 \n",
      "     Validation Step: 1 Validation Loss: 3.399604320526123 \n",
      "     Validation Step: 2 Validation Loss: 3.1098127365112305 \n",
      "     Validation Step: 3 Validation Loss: 2.6424341201782227 \n",
      "     Validation Step: 4 Validation Loss: 3.693294048309326 \n",
      "     Validation Step: 5 Validation Loss: 2.7313904762268066 \n",
      "     Validation Step: 6 Validation Loss: 3.2968146800994873 \n",
      "     Validation Step: 7 Validation Loss: 3.377579689025879 \n",
      "     Validation Step: 8 Validation Loss: 3.16683292388916 \n",
      "     Validation Step: 9 Validation Loss: 2.9194869995117188 \n",
      "     Validation Step: 10 Validation Loss: 2.903313398361206 \n",
      "     Validation Step: 11 Validation Loss: 3.196204900741577 \n",
      "     Validation Step: 12 Validation Loss: 3.4085075855255127 \n",
      "     Validation Step: 13 Validation Loss: 2.658904790878296 \n",
      "     Validation Step: 14 Validation Loss: 2.350834846496582 \n",
      "Epoch: 73\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.6072278022766113 \n",
      "     Training Step: 1 Training Loss: 2.474151134490967 \n",
      "     Training Step: 2 Training Loss: 2.723017692565918 \n",
      "     Training Step: 3 Training Loss: 3.2742176055908203 \n",
      "     Training Step: 4 Training Loss: 3.094348192214966 \n",
      "     Training Step: 5 Training Loss: 3.2585525512695312 \n",
      "     Training Step: 6 Training Loss: 3.94815993309021 \n",
      "     Training Step: 7 Training Loss: 3.2425780296325684 \n",
      "     Training Step: 8 Training Loss: 3.633665084838867 \n",
      "     Training Step: 9 Training Loss: 2.936885356903076 \n",
      "     Training Step: 10 Training Loss: 2.7462894916534424 \n",
      "     Training Step: 11 Training Loss: 2.3317325115203857 \n",
      "     Training Step: 12 Training Loss: 3.3927252292633057 \n",
      "     Training Step: 13 Training Loss: 2.7892704010009766 \n",
      "     Training Step: 14 Training Loss: 3.242896795272827 \n",
      "     Training Step: 15 Training Loss: 3.456852436065674 \n",
      "     Training Step: 16 Training Loss: 2.1038448810577393 \n",
      "     Training Step: 17 Training Loss: 2.8094582557678223 \n",
      "     Training Step: 18 Training Loss: 2.6828455924987793 \n",
      "     Training Step: 19 Training Loss: 2.973184823989868 \n",
      "     Training Step: 20 Training Loss: 2.243375301361084 \n",
      "     Training Step: 21 Training Loss: 4.42046594619751 \n",
      "     Training Step: 22 Training Loss: 3.538667917251587 \n",
      "     Training Step: 23 Training Loss: 3.1004042625427246 \n",
      "     Training Step: 24 Training Loss: 2.643651247024536 \n",
      "     Training Step: 25 Training Loss: 3.6242311000823975 \n",
      "     Training Step: 26 Training Loss: 2.1702218055725098 \n",
      "     Training Step: 27 Training Loss: 2.2070789337158203 \n",
      "     Training Step: 28 Training Loss: 2.1011810302734375 \n",
      "     Training Step: 29 Training Loss: 2.9630744457244873 \n",
      "     Training Step: 30 Training Loss: 2.9432194232940674 \n",
      "     Training Step: 31 Training Loss: 2.7028679847717285 \n",
      "     Training Step: 32 Training Loss: 2.56927227973938 \n",
      "     Training Step: 33 Training Loss: 2.7826180458068848 \n",
      "     Training Step: 34 Training Loss: 3.1068992614746094 \n",
      "     Training Step: 35 Training Loss: 3.715306043624878 \n",
      "     Training Step: 36 Training Loss: 3.334745407104492 \n",
      "     Training Step: 37 Training Loss: 3.111304521560669 \n",
      "     Training Step: 38 Training Loss: 2.5773258209228516 \n",
      "     Training Step: 39 Training Loss: 2.8552913665771484 \n",
      "     Training Step: 40 Training Loss: 2.808370590209961 \n",
      "     Training Step: 41 Training Loss: 3.2276172637939453 \n",
      "     Training Step: 42 Training Loss: 2.372655153274536 \n",
      "     Training Step: 43 Training Loss: 3.2118000984191895 \n",
      "     Training Step: 44 Training Loss: 3.524588108062744 \n",
      "     Training Step: 45 Training Loss: 3.010941743850708 \n",
      "     Training Step: 46 Training Loss: 3.201352596282959 \n",
      "     Training Step: 47 Training Loss: 2.309192180633545 \n",
      "     Training Step: 48 Training Loss: 4.142763137817383 \n",
      "     Training Step: 49 Training Loss: 2.677260398864746 \n",
      "     Training Step: 50 Training Loss: 2.428391456604004 \n",
      "     Training Step: 51 Training Loss: 2.9517111778259277 \n",
      "     Training Step: 52 Training Loss: 3.3177807331085205 \n",
      "     Training Step: 53 Training Loss: 3.4611868858337402 \n",
      "     Training Step: 54 Training Loss: 2.7352242469787598 \n",
      "     Training Step: 55 Training Loss: 2.6941545009613037 \n",
      "     Training Step: 56 Training Loss: 2.6906094551086426 \n",
      "     Training Step: 57 Training Loss: 2.672316551208496 \n",
      "     Training Step: 58 Training Loss: 3.070032835006714 \n",
      "     Training Step: 59 Training Loss: 2.4976673126220703 \n",
      "     Training Step: 60 Training Loss: 2.725794792175293 \n",
      "     Training Step: 61 Training Loss: 3.648561477661133 \n",
      "     Training Step: 62 Training Loss: 4.511281967163086 \n",
      "     Training Step: 63 Training Loss: 2.9737601280212402 \n",
      "     Training Step: 64 Training Loss: 2.4902820587158203 \n",
      "     Training Step: 65 Training Loss: 3.118831157684326 \n",
      "     Training Step: 66 Training Loss: 2.783702850341797 \n",
      "     Training Step: 67 Training Loss: 2.789210796356201 \n",
      "     Training Step: 68 Training Loss: 3.61007022857666 \n",
      "     Training Step: 69 Training Loss: 4.237696647644043 \n",
      "     Training Step: 70 Training Loss: 2.967020034790039 \n",
      "     Training Step: 71 Training Loss: 2.2867612838745117 \n",
      "     Training Step: 72 Training Loss: 3.621962547302246 \n",
      "     Training Step: 73 Training Loss: 3.668031692504883 \n",
      "     Training Step: 74 Training Loss: 2.463397979736328 \n",
      "     Training Step: 75 Training Loss: 2.4368886947631836 \n",
      "     Training Step: 76 Training Loss: 3.248748540878296 \n",
      "     Training Step: 77 Training Loss: 2.149308681488037 \n",
      "     Training Step: 78 Training Loss: 2.6298398971557617 \n",
      "     Training Step: 79 Training Loss: 2.9218690395355225 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.9335880279541016 \n",
      "     Validation Step: 1 Validation Loss: 2.983700752258301 \n",
      "     Validation Step: 2 Validation Loss: 3.4396328926086426 \n",
      "     Validation Step: 3 Validation Loss: 2.907719612121582 \n",
      "     Validation Step: 4 Validation Loss: 3.122973680496216 \n",
      "     Validation Step: 5 Validation Loss: 2.828704595565796 \n",
      "     Validation Step: 6 Validation Loss: 3.3585364818573 \n",
      "     Validation Step: 7 Validation Loss: 3.3923885822296143 \n",
      "     Validation Step: 8 Validation Loss: 2.3764214515686035 \n",
      "     Validation Step: 9 Validation Loss: 3.5788984298706055 \n",
      "     Validation Step: 10 Validation Loss: 3.9011011123657227 \n",
      "     Validation Step: 11 Validation Loss: 2.6351089477539062 \n",
      "     Validation Step: 12 Validation Loss: 2.8502767086029053 \n",
      "     Validation Step: 13 Validation Loss: 4.010265827178955 \n",
      "     Validation Step: 14 Validation Loss: 2.8790955543518066 \n",
      "Epoch: 74\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.36830997467041 \n",
      "     Training Step: 1 Training Loss: 2.213787794113159 \n",
      "     Training Step: 2 Training Loss: 2.9758782386779785 \n",
      "     Training Step: 3 Training Loss: 2.676769733428955 \n",
      "     Training Step: 4 Training Loss: 3.296267509460449 \n",
      "     Training Step: 5 Training Loss: 3.3955254554748535 \n",
      "     Training Step: 6 Training Loss: 2.9434192180633545 \n",
      "     Training Step: 7 Training Loss: 2.2294187545776367 \n",
      "     Training Step: 8 Training Loss: 2.2586917877197266 \n",
      "     Training Step: 9 Training Loss: 3.054830551147461 \n",
      "     Training Step: 10 Training Loss: 3.0235915184020996 \n",
      "     Training Step: 11 Training Loss: 2.8315305709838867 \n",
      "     Training Step: 12 Training Loss: 3.228639841079712 \n",
      "     Training Step: 13 Training Loss: 4.023061752319336 \n",
      "     Training Step: 14 Training Loss: 2.2308502197265625 \n",
      "     Training Step: 15 Training Loss: 3.9324965476989746 \n",
      "     Training Step: 16 Training Loss: 2.5270309448242188 \n",
      "     Training Step: 17 Training Loss: 3.0235469341278076 \n",
      "     Training Step: 18 Training Loss: 3.983541488647461 \n",
      "     Training Step: 19 Training Loss: 3.286546468734741 \n",
      "     Training Step: 20 Training Loss: 3.3651742935180664 \n",
      "     Training Step: 21 Training Loss: 4.387045860290527 \n",
      "     Training Step: 22 Training Loss: 2.789139747619629 \n",
      "     Training Step: 23 Training Loss: 3.3941264152526855 \n",
      "     Training Step: 24 Training Loss: 2.4398069381713867 \n",
      "     Training Step: 25 Training Loss: 3.480649471282959 \n",
      "     Training Step: 26 Training Loss: 2.7214252948760986 \n",
      "     Training Step: 27 Training Loss: 4.423488140106201 \n",
      "     Training Step: 28 Training Loss: 3.700007438659668 \n",
      "     Training Step: 29 Training Loss: 3.362921953201294 \n",
      "     Training Step: 30 Training Loss: 2.658388376235962 \n",
      "     Training Step: 31 Training Loss: 2.8448829650878906 \n",
      "     Training Step: 32 Training Loss: 2.5693845748901367 \n",
      "     Training Step: 33 Training Loss: 2.2262120246887207 \n",
      "     Training Step: 34 Training Loss: 3.0490190982818604 \n",
      "     Training Step: 35 Training Loss: 2.8120126724243164 \n",
      "     Training Step: 36 Training Loss: 2.8118057250976562 \n",
      "     Training Step: 37 Training Loss: 2.747690200805664 \n",
      "     Training Step: 38 Training Loss: 2.432555675506592 \n",
      "     Training Step: 39 Training Loss: 3.4359848499298096 \n",
      "     Training Step: 40 Training Loss: 2.967259407043457 \n",
      "     Training Step: 41 Training Loss: 3.0997462272644043 \n",
      "     Training Step: 42 Training Loss: 3.1353912353515625 \n",
      "     Training Step: 43 Training Loss: 2.6590702533721924 \n",
      "     Training Step: 44 Training Loss: 2.7202694416046143 \n",
      "     Training Step: 45 Training Loss: 3.5267374515533447 \n",
      "     Training Step: 46 Training Loss: 3.0902862548828125 \n",
      "     Training Step: 47 Training Loss: 4.272139072418213 \n",
      "     Training Step: 48 Training Loss: 4.03371524810791 \n",
      "     Training Step: 49 Training Loss: 3.0557732582092285 \n",
      "     Training Step: 50 Training Loss: 3.224226474761963 \n",
      "     Training Step: 51 Training Loss: 3.438476085662842 \n",
      "     Training Step: 52 Training Loss: 3.2036120891571045 \n",
      "     Training Step: 53 Training Loss: 2.09706449508667 \n",
      "     Training Step: 54 Training Loss: 3.5103085041046143 \n",
      "     Training Step: 55 Training Loss: 2.3200836181640625 \n",
      "     Training Step: 56 Training Loss: 2.0889973640441895 \n",
      "     Training Step: 57 Training Loss: 3.0868663787841797 \n",
      "     Training Step: 58 Training Loss: 3.371553659439087 \n",
      "     Training Step: 59 Training Loss: 2.5582587718963623 \n",
      "     Training Step: 60 Training Loss: 3.028298854827881 \n",
      "     Training Step: 61 Training Loss: 2.476532220840454 \n",
      "     Training Step: 62 Training Loss: 3.2174906730651855 \n",
      "     Training Step: 63 Training Loss: 3.177546739578247 \n",
      "     Training Step: 64 Training Loss: 4.2251386642456055 \n",
      "     Training Step: 65 Training Loss: 2.56494402885437 \n",
      "     Training Step: 66 Training Loss: 2.633150577545166 \n",
      "     Training Step: 67 Training Loss: 3.1078600883483887 \n",
      "     Training Step: 68 Training Loss: 2.27175235748291 \n",
      "     Training Step: 69 Training Loss: 2.3254826068878174 \n",
      "     Training Step: 70 Training Loss: 2.4935810565948486 \n",
      "     Training Step: 71 Training Loss: 3.3230721950531006 \n",
      "     Training Step: 72 Training Loss: 3.488147258758545 \n",
      "     Training Step: 73 Training Loss: 3.819162368774414 \n",
      "     Training Step: 74 Training Loss: 3.361384391784668 \n",
      "     Training Step: 75 Training Loss: 2.508281707763672 \n",
      "     Training Step: 76 Training Loss: 2.9857022762298584 \n",
      "     Training Step: 77 Training Loss: 3.1332077980041504 \n",
      "     Training Step: 78 Training Loss: 2.670274257659912 \n",
      "     Training Step: 79 Training Loss: 2.7195205688476562 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1401309967041016 \n",
      "     Validation Step: 1 Validation Loss: 3.1364247798919678 \n",
      "     Validation Step: 2 Validation Loss: 2.894442558288574 \n",
      "     Validation Step: 3 Validation Loss: 2.9268288612365723 \n",
      "     Validation Step: 4 Validation Loss: 2.9046664237976074 \n",
      "     Validation Step: 5 Validation Loss: 3.997066020965576 \n",
      "     Validation Step: 6 Validation Loss: 3.7985336780548096 \n",
      "     Validation Step: 7 Validation Loss: 3.666475772857666 \n",
      "     Validation Step: 8 Validation Loss: 3.213355541229248 \n",
      "     Validation Step: 9 Validation Loss: 3.0626416206359863 \n",
      "     Validation Step: 10 Validation Loss: 3.2968697547912598 \n",
      "     Validation Step: 11 Validation Loss: 3.0547640323638916 \n",
      "     Validation Step: 12 Validation Loss: 3.48704195022583 \n",
      "     Validation Step: 13 Validation Loss: 3.177858352661133 \n",
      "     Validation Step: 14 Validation Loss: 2.029367446899414 \n",
      "Epoch: 75\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.180116653442383 \n",
      "     Training Step: 1 Training Loss: 3.9012348651885986 \n",
      "     Training Step: 2 Training Loss: 2.827202081680298 \n",
      "     Training Step: 3 Training Loss: 2.695744037628174 \n",
      "     Training Step: 4 Training Loss: 2.5722551345825195 \n",
      "     Training Step: 5 Training Loss: 2.210800886154175 \n",
      "     Training Step: 6 Training Loss: 3.189393997192383 \n",
      "     Training Step: 7 Training Loss: 2.2418363094329834 \n",
      "     Training Step: 8 Training Loss: 2.4260473251342773 \n",
      "     Training Step: 9 Training Loss: 3.291635274887085 \n",
      "     Training Step: 10 Training Loss: 2.347437858581543 \n",
      "     Training Step: 11 Training Loss: 3.3452117443084717 \n",
      "     Training Step: 12 Training Loss: 3.9449121952056885 \n",
      "     Training Step: 13 Training Loss: 3.6541993618011475 \n",
      "     Training Step: 14 Training Loss: 3.1219892501831055 \n",
      "     Training Step: 15 Training Loss: 3.824615478515625 \n",
      "     Training Step: 16 Training Loss: 2.2590880393981934 \n",
      "     Training Step: 17 Training Loss: 2.7911434173583984 \n",
      "     Training Step: 18 Training Loss: 3.167377471923828 \n",
      "     Training Step: 19 Training Loss: 2.387650966644287 \n",
      "     Training Step: 20 Training Loss: 2.8902769088745117 \n",
      "     Training Step: 21 Training Loss: 2.8234500885009766 \n",
      "     Training Step: 22 Training Loss: 3.236628770828247 \n",
      "     Training Step: 23 Training Loss: 3.565706491470337 \n",
      "     Training Step: 24 Training Loss: 2.7676615715026855 \n",
      "     Training Step: 25 Training Loss: 2.7174487113952637 \n",
      "     Training Step: 26 Training Loss: 3.7677183151245117 \n",
      "     Training Step: 27 Training Loss: 3.4326610565185547 \n",
      "     Training Step: 28 Training Loss: 4.003658294677734 \n",
      "     Training Step: 29 Training Loss: 2.820348024368286 \n",
      "     Training Step: 30 Training Loss: 2.9434118270874023 \n",
      "     Training Step: 31 Training Loss: 2.2808871269226074 \n",
      "     Training Step: 32 Training Loss: 2.4419260025024414 \n",
      "     Training Step: 33 Training Loss: 3.484832286834717 \n",
      "     Training Step: 34 Training Loss: 2.172816038131714 \n",
      "     Training Step: 35 Training Loss: 2.866917848587036 \n",
      "     Training Step: 36 Training Loss: 3.321173906326294 \n",
      "     Training Step: 37 Training Loss: 3.5436339378356934 \n",
      "     Training Step: 38 Training Loss: 2.7170791625976562 \n",
      "     Training Step: 39 Training Loss: 3.1505866050720215 \n",
      "     Training Step: 40 Training Loss: 3.0826239585876465 \n",
      "     Training Step: 41 Training Loss: 2.9940643310546875 \n",
      "     Training Step: 42 Training Loss: 3.561765432357788 \n",
      "     Training Step: 43 Training Loss: 3.8895013332366943 \n",
      "     Training Step: 44 Training Loss: 3.0317587852478027 \n",
      "     Training Step: 45 Training Loss: 3.3417458534240723 \n",
      "     Training Step: 46 Training Loss: 3.0824801921844482 \n",
      "     Training Step: 47 Training Loss: 3.825640916824341 \n",
      "     Training Step: 48 Training Loss: 2.5748307704925537 \n",
      "     Training Step: 49 Training Loss: 3.140195369720459 \n",
      "     Training Step: 50 Training Loss: 3.0125348567962646 \n",
      "     Training Step: 51 Training Loss: 2.219028949737549 \n",
      "     Training Step: 52 Training Loss: 2.611403226852417 \n",
      "     Training Step: 53 Training Loss: 3.430675745010376 \n",
      "     Training Step: 54 Training Loss: 2.69344425201416 \n",
      "     Training Step: 55 Training Loss: 2.807689666748047 \n",
      "     Training Step: 56 Training Loss: 3.147580862045288 \n",
      "     Training Step: 57 Training Loss: 2.386720895767212 \n",
      "     Training Step: 58 Training Loss: 3.8042032718658447 \n",
      "     Training Step: 59 Training Loss: 2.2433595657348633 \n",
      "     Training Step: 60 Training Loss: 2.6262779235839844 \n",
      "     Training Step: 61 Training Loss: 3.601726531982422 \n",
      "     Training Step: 62 Training Loss: 2.287811040878296 \n",
      "     Training Step: 63 Training Loss: 3.4251346588134766 \n",
      "     Training Step: 64 Training Loss: 2.7931101322174072 \n",
      "     Training Step: 65 Training Loss: 2.4542887210845947 \n",
      "     Training Step: 66 Training Loss: 2.7031123638153076 \n",
      "     Training Step: 67 Training Loss: 2.9929275512695312 \n",
      "     Training Step: 68 Training Loss: 2.906451463699341 \n",
      "     Training Step: 69 Training Loss: 3.181394100189209 \n",
      "     Training Step: 70 Training Loss: 3.754263162612915 \n",
      "     Training Step: 71 Training Loss: 2.765831232070923 \n",
      "     Training Step: 72 Training Loss: 2.624915838241577 \n",
      "     Training Step: 73 Training Loss: 2.8048057556152344 \n",
      "     Training Step: 74 Training Loss: 2.336503028869629 \n",
      "     Training Step: 75 Training Loss: 3.0114965438842773 \n",
      "     Training Step: 76 Training Loss: 2.679286241531372 \n",
      "     Training Step: 77 Training Loss: 2.847494602203369 \n",
      "     Training Step: 78 Training Loss: 2.7680935859680176 \n",
      "     Training Step: 79 Training Loss: 3.508425235748291 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.106451988220215 \n",
      "     Validation Step: 1 Validation Loss: 3.509291410446167 \n",
      "     Validation Step: 2 Validation Loss: 3.256592035293579 \n",
      "     Validation Step: 3 Validation Loss: 2.9855923652648926 \n",
      "     Validation Step: 4 Validation Loss: 3.0498852729797363 \n",
      "     Validation Step: 5 Validation Loss: 3.3161821365356445 \n",
      "     Validation Step: 6 Validation Loss: 2.307995080947876 \n",
      "     Validation Step: 7 Validation Loss: 3.9152331352233887 \n",
      "     Validation Step: 8 Validation Loss: 2.6748526096343994 \n",
      "     Validation Step: 9 Validation Loss: 3.4143712520599365 \n",
      "     Validation Step: 10 Validation Loss: 3.756159782409668 \n",
      "     Validation Step: 11 Validation Loss: 3.707510232925415 \n",
      "     Validation Step: 12 Validation Loss: 2.9449567794799805 \n",
      "     Validation Step: 13 Validation Loss: 3.2289469242095947 \n",
      "     Validation Step: 14 Validation Loss: 3.0439116954803467 \n",
      "Epoch: 76\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.6906557083129883 \n",
      "     Training Step: 1 Training Loss: 3.1448099613189697 \n",
      "     Training Step: 2 Training Loss: 4.483408451080322 \n",
      "     Training Step: 3 Training Loss: 2.3854570388793945 \n",
      "     Training Step: 4 Training Loss: 2.222639560699463 \n",
      "     Training Step: 5 Training Loss: 2.705263614654541 \n",
      "     Training Step: 6 Training Loss: 3.184998035430908 \n",
      "     Training Step: 7 Training Loss: 2.9107205867767334 \n",
      "     Training Step: 8 Training Loss: 2.396313190460205 \n",
      "     Training Step: 9 Training Loss: 3.1088781356811523 \n",
      "     Training Step: 10 Training Loss: 3.1708521842956543 \n",
      "     Training Step: 11 Training Loss: 3.082336664199829 \n",
      "     Training Step: 12 Training Loss: 2.517308235168457 \n",
      "     Training Step: 13 Training Loss: 2.4434762001037598 \n",
      "     Training Step: 14 Training Loss: 2.692354917526245 \n",
      "     Training Step: 15 Training Loss: 3.6212515830993652 \n",
      "     Training Step: 16 Training Loss: 2.3145158290863037 \n",
      "     Training Step: 17 Training Loss: 3.366025447845459 \n",
      "     Training Step: 18 Training Loss: 2.7297699451446533 \n",
      "     Training Step: 19 Training Loss: 2.904444456100464 \n",
      "     Training Step: 20 Training Loss: 2.6992673873901367 \n",
      "     Training Step: 21 Training Loss: 2.8550591468811035 \n",
      "     Training Step: 22 Training Loss: 2.536966323852539 \n",
      "     Training Step: 23 Training Loss: 3.3366246223449707 \n",
      "     Training Step: 24 Training Loss: 2.860541343688965 \n",
      "     Training Step: 25 Training Loss: 3.010401725769043 \n",
      "     Training Step: 26 Training Loss: 2.7450568675994873 \n",
      "     Training Step: 27 Training Loss: 3.541822671890259 \n",
      "     Training Step: 28 Training Loss: 4.118041038513184 \n",
      "     Training Step: 29 Training Loss: 3.4197750091552734 \n",
      "     Training Step: 30 Training Loss: 2.663890838623047 \n",
      "     Training Step: 31 Training Loss: 2.3978466987609863 \n",
      "     Training Step: 32 Training Loss: 2.707815647125244 \n",
      "     Training Step: 33 Training Loss: 2.744950771331787 \n",
      "     Training Step: 34 Training Loss: 3.0873847007751465 \n",
      "     Training Step: 35 Training Loss: 2.700430393218994 \n",
      "     Training Step: 36 Training Loss: 2.7610743045806885 \n",
      "     Training Step: 37 Training Loss: 2.987617015838623 \n",
      "     Training Step: 38 Training Loss: 2.1622142791748047 \n",
      "     Training Step: 39 Training Loss: 2.994565010070801 \n",
      "     Training Step: 40 Training Loss: 2.267965316772461 \n",
      "     Training Step: 41 Training Loss: 2.2554855346679688 \n",
      "     Training Step: 42 Training Loss: 4.490623474121094 \n",
      "     Training Step: 43 Training Loss: 2.6334242820739746 \n",
      "     Training Step: 44 Training Loss: 2.922849178314209 \n",
      "     Training Step: 45 Training Loss: 3.262566566467285 \n",
      "     Training Step: 46 Training Loss: 2.6787071228027344 \n",
      "     Training Step: 47 Training Loss: 3.0300092697143555 \n",
      "     Training Step: 48 Training Loss: 2.250009536743164 \n",
      "     Training Step: 49 Training Loss: 3.321133613586426 \n",
      "     Training Step: 50 Training Loss: 3.0122904777526855 \n",
      "     Training Step: 51 Training Loss: 2.62545108795166 \n",
      "     Training Step: 52 Training Loss: 4.007596492767334 \n",
      "     Training Step: 53 Training Loss: 2.9905593395233154 \n",
      "     Training Step: 54 Training Loss: 2.5315494537353516 \n",
      "     Training Step: 55 Training Loss: 2.7027535438537598 \n",
      "     Training Step: 56 Training Loss: 2.3913321495056152 \n",
      "     Training Step: 57 Training Loss: 3.249868631362915 \n",
      "     Training Step: 58 Training Loss: 3.0146424770355225 \n",
      "     Training Step: 59 Training Loss: 2.7205405235290527 \n",
      "     Training Step: 60 Training Loss: 2.3377246856689453 \n",
      "     Training Step: 61 Training Loss: 3.7590861320495605 \n",
      "     Training Step: 62 Training Loss: 3.201151132583618 \n",
      "     Training Step: 63 Training Loss: 2.7590160369873047 \n",
      "     Training Step: 64 Training Loss: 2.578536033630371 \n",
      "     Training Step: 65 Training Loss: 2.600534677505493 \n",
      "     Training Step: 66 Training Loss: 2.4302966594696045 \n",
      "     Training Step: 67 Training Loss: 3.155036211013794 \n",
      "     Training Step: 68 Training Loss: 3.7380762100219727 \n",
      "     Training Step: 69 Training Loss: 2.666973352432251 \n",
      "     Training Step: 70 Training Loss: 3.13130784034729 \n",
      "     Training Step: 71 Training Loss: 2.881897449493408 \n",
      "     Training Step: 72 Training Loss: 2.5852389335632324 \n",
      "     Training Step: 73 Training Loss: 2.80853271484375 \n",
      "     Training Step: 74 Training Loss: 2.5448126792907715 \n",
      "     Training Step: 75 Training Loss: 2.7864789962768555 \n",
      "     Training Step: 76 Training Loss: 3.2834455966949463 \n",
      "     Training Step: 77 Training Loss: 3.495901107788086 \n",
      "     Training Step: 78 Training Loss: 2.8513426780700684 \n",
      "     Training Step: 79 Training Loss: 3.3656275272369385 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9917943477630615 \n",
      "     Validation Step: 1 Validation Loss: 3.208130359649658 \n",
      "     Validation Step: 2 Validation Loss: 3.4326629638671875 \n",
      "     Validation Step: 3 Validation Loss: 3.3852386474609375 \n",
      "     Validation Step: 4 Validation Loss: 2.5970935821533203 \n",
      "     Validation Step: 5 Validation Loss: 2.7846333980560303 \n",
      "     Validation Step: 6 Validation Loss: 2.938565731048584 \n",
      "     Validation Step: 7 Validation Loss: 2.823119878768921 \n",
      "     Validation Step: 8 Validation Loss: 2.510255813598633 \n",
      "     Validation Step: 9 Validation Loss: 2.8588528633117676 \n",
      "     Validation Step: 10 Validation Loss: 2.566067695617676 \n",
      "     Validation Step: 11 Validation Loss: 3.6342761516571045 \n",
      "     Validation Step: 12 Validation Loss: 2.4093048572540283 \n",
      "     Validation Step: 13 Validation Loss: 3.237705707550049 \n",
      "     Validation Step: 14 Validation Loss: 3.5911431312561035 \n",
      "Epoch: 77\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.253333806991577 \n",
      "     Training Step: 1 Training Loss: 3.611982822418213 \n",
      "     Training Step: 2 Training Loss: 2.967888355255127 \n",
      "     Training Step: 3 Training Loss: 2.2910187244415283 \n",
      "     Training Step: 4 Training Loss: 4.393502235412598 \n",
      "     Training Step: 5 Training Loss: 2.5497779846191406 \n",
      "     Training Step: 6 Training Loss: 3.0745229721069336 \n",
      "     Training Step: 7 Training Loss: 2.463163375854492 \n",
      "     Training Step: 8 Training Loss: 2.4339473247528076 \n",
      "     Training Step: 9 Training Loss: 2.9367291927337646 \n",
      "     Training Step: 10 Training Loss: 3.0262460708618164 \n",
      "     Training Step: 11 Training Loss: 3.447087049484253 \n",
      "     Training Step: 12 Training Loss: 2.23892879486084 \n",
      "     Training Step: 13 Training Loss: 2.5888073444366455 \n",
      "     Training Step: 14 Training Loss: 3.396327257156372 \n",
      "     Training Step: 15 Training Loss: 2.33026385307312 \n",
      "     Training Step: 16 Training Loss: 3.006371021270752 \n",
      "     Training Step: 17 Training Loss: 2.3793210983276367 \n",
      "     Training Step: 18 Training Loss: 2.9919979572296143 \n",
      "     Training Step: 19 Training Loss: 3.6892409324645996 \n",
      "     Training Step: 20 Training Loss: 3.182088613510132 \n",
      "     Training Step: 21 Training Loss: 3.919311285018921 \n",
      "     Training Step: 22 Training Loss: 2.602883815765381 \n",
      "     Training Step: 23 Training Loss: 2.816218852996826 \n",
      "     Training Step: 24 Training Loss: 3.097594738006592 \n",
      "     Training Step: 25 Training Loss: 3.2296059131622314 \n",
      "     Training Step: 26 Training Loss: 3.267519235610962 \n",
      "     Training Step: 27 Training Loss: 3.6589126586914062 \n",
      "     Training Step: 28 Training Loss: 3.335725784301758 \n",
      "     Training Step: 29 Training Loss: 2.7696285247802734 \n",
      "     Training Step: 30 Training Loss: 2.4296469688415527 \n",
      "     Training Step: 31 Training Loss: 2.2349541187286377 \n",
      "     Training Step: 32 Training Loss: 2.3866941928863525 \n",
      "     Training Step: 33 Training Loss: 2.6908345222473145 \n",
      "     Training Step: 34 Training Loss: 3.1795053482055664 \n",
      "     Training Step: 35 Training Loss: 3.519381284713745 \n",
      "     Training Step: 36 Training Loss: 2.1940736770629883 \n",
      "     Training Step: 37 Training Loss: 4.428057670593262 \n",
      "     Training Step: 38 Training Loss: 2.3858790397644043 \n",
      "     Training Step: 39 Training Loss: 3.17104172706604 \n",
      "     Training Step: 40 Training Loss: 3.304830312728882 \n",
      "     Training Step: 41 Training Loss: 2.8035504817962646 \n",
      "     Training Step: 42 Training Loss: 2.7132768630981445 \n",
      "     Training Step: 43 Training Loss: 3.212916374206543 \n",
      "     Training Step: 44 Training Loss: 3.0459532737731934 \n",
      "     Training Step: 45 Training Loss: 2.6921491622924805 \n",
      "     Training Step: 46 Training Loss: 3.2373132705688477 \n",
      "     Training Step: 47 Training Loss: 3.177863597869873 \n",
      "     Training Step: 48 Training Loss: 3.143580675125122 \n",
      "     Training Step: 49 Training Loss: 2.246386766433716 \n",
      "     Training Step: 50 Training Loss: 3.373112916946411 \n",
      "     Training Step: 51 Training Loss: 2.733898162841797 \n",
      "     Training Step: 52 Training Loss: 2.5535707473754883 \n",
      "     Training Step: 53 Training Loss: 3.745854139328003 \n",
      "     Training Step: 54 Training Loss: 2.762432098388672 \n",
      "     Training Step: 55 Training Loss: 3.077760696411133 \n",
      "     Training Step: 56 Training Loss: 2.1482934951782227 \n",
      "     Training Step: 57 Training Loss: 3.751736640930176 \n",
      "     Training Step: 58 Training Loss: 2.700411558151245 \n",
      "     Training Step: 59 Training Loss: 2.872659683227539 \n",
      "     Training Step: 60 Training Loss: 2.5108916759490967 \n",
      "     Training Step: 61 Training Loss: 2.9822235107421875 \n",
      "     Training Step: 62 Training Loss: 4.022724151611328 \n",
      "     Training Step: 63 Training Loss: 2.493865966796875 \n",
      "     Training Step: 64 Training Loss: 3.7419655323028564 \n",
      "     Training Step: 65 Training Loss: 2.351465940475464 \n",
      "     Training Step: 66 Training Loss: 3.917543888092041 \n",
      "     Training Step: 67 Training Loss: 3.3788070678710938 \n",
      "     Training Step: 68 Training Loss: 3.3501710891723633 \n",
      "     Training Step: 69 Training Loss: 2.137472629547119 \n",
      "     Training Step: 70 Training Loss: 3.3238914012908936 \n",
      "     Training Step: 71 Training Loss: 3.053617000579834 \n",
      "     Training Step: 72 Training Loss: 2.247985601425171 \n",
      "     Training Step: 73 Training Loss: 2.5122203826904297 \n",
      "     Training Step: 74 Training Loss: 3.496245861053467 \n",
      "     Training Step: 75 Training Loss: 2.6178674697875977 \n",
      "     Training Step: 76 Training Loss: 2.603571891784668 \n",
      "     Training Step: 77 Training Loss: 2.5116868019104004 \n",
      "     Training Step: 78 Training Loss: 2.4991352558135986 \n",
      "     Training Step: 79 Training Loss: 3.6932373046875 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.588927984237671 \n",
      "     Validation Step: 1 Validation Loss: 2.319812297821045 \n",
      "     Validation Step: 2 Validation Loss: 2.3365395069122314 \n",
      "     Validation Step: 3 Validation Loss: 2.6731886863708496 \n",
      "     Validation Step: 4 Validation Loss: 2.900085926055908 \n",
      "     Validation Step: 5 Validation Loss: 4.268548488616943 \n",
      "     Validation Step: 6 Validation Loss: 3.07485032081604 \n",
      "     Validation Step: 7 Validation Loss: 3.9524197578430176 \n",
      "     Validation Step: 8 Validation Loss: 3.2198009490966797 \n",
      "     Validation Step: 9 Validation Loss: 2.6315245628356934 \n",
      "     Validation Step: 10 Validation Loss: 3.6886258125305176 \n",
      "     Validation Step: 11 Validation Loss: 4.037630081176758 \n",
      "     Validation Step: 12 Validation Loss: 3.0970702171325684 \n",
      "     Validation Step: 13 Validation Loss: 2.4332892894744873 \n",
      "     Validation Step: 14 Validation Loss: 2.7937655448913574 \n",
      "---------------   LEARNING RATE HALVING DOWN TO: 8.000000000000001e-06   ---------------\n",
      "---------------   CURRENT LEARNING RATE: 8.000000000000001e-06   ---------------\n",
      "Epoch: 78\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.8465168476104736 \n",
      "     Training Step: 1 Training Loss: 3.2983169555664062 \n",
      "     Training Step: 2 Training Loss: 3.8158044815063477 \n",
      "     Training Step: 3 Training Loss: 3.83827543258667 \n",
      "     Training Step: 4 Training Loss: 3.498547077178955 \n",
      "     Training Step: 5 Training Loss: 2.681875228881836 \n",
      "     Training Step: 6 Training Loss: 2.158905506134033 \n",
      "     Training Step: 7 Training Loss: 3.473330020904541 \n",
      "     Training Step: 8 Training Loss: 2.7305426597595215 \n",
      "     Training Step: 9 Training Loss: 3.1231517791748047 \n",
      "     Training Step: 10 Training Loss: 3.3123342990875244 \n",
      "     Training Step: 11 Training Loss: 2.800264596939087 \n",
      "     Training Step: 12 Training Loss: 2.0819578170776367 \n",
      "     Training Step: 13 Training Loss: 3.3878846168518066 \n",
      "     Training Step: 14 Training Loss: 3.1154916286468506 \n",
      "     Training Step: 15 Training Loss: 3.700608253479004 \n",
      "     Training Step: 16 Training Loss: 2.6553831100463867 \n",
      "     Training Step: 17 Training Loss: 2.804182529449463 \n",
      "     Training Step: 18 Training Loss: 3.0499918460845947 \n",
      "     Training Step: 19 Training Loss: 2.495487689971924 \n",
      "     Training Step: 20 Training Loss: 3.2302772998809814 \n",
      "     Training Step: 21 Training Loss: 3.631469488143921 \n",
      "     Training Step: 22 Training Loss: 2.1254262924194336 \n",
      "     Training Step: 23 Training Loss: 2.456265687942505 \n",
      "     Training Step: 24 Training Loss: 3.616889476776123 \n",
      "     Training Step: 25 Training Loss: 3.616501569747925 \n",
      "     Training Step: 26 Training Loss: 2.5304174423217773 \n",
      "     Training Step: 27 Training Loss: 2.351551055908203 \n",
      "     Training Step: 28 Training Loss: 2.712475299835205 \n",
      "     Training Step: 29 Training Loss: 2.6418566703796387 \n",
      "     Training Step: 30 Training Loss: 2.1195802688598633 \n",
      "     Training Step: 31 Training Loss: 3.531841278076172 \n",
      "     Training Step: 32 Training Loss: 3.546875476837158 \n",
      "     Training Step: 33 Training Loss: 2.978020668029785 \n",
      "     Training Step: 34 Training Loss: 2.9578938484191895 \n",
      "     Training Step: 35 Training Loss: 2.893446922302246 \n",
      "     Training Step: 36 Training Loss: 3.117391347885132 \n",
      "     Training Step: 37 Training Loss: 3.1512279510498047 \n",
      "     Training Step: 38 Training Loss: 2.407409906387329 \n",
      "     Training Step: 39 Training Loss: 3.326674461364746 \n",
      "     Training Step: 40 Training Loss: 3.1475136280059814 \n",
      "     Training Step: 41 Training Loss: 2.3428897857666016 \n",
      "     Training Step: 42 Training Loss: 3.4315273761749268 \n",
      "     Training Step: 43 Training Loss: 2.613539218902588 \n",
      "     Training Step: 44 Training Loss: 3.9765663146972656 \n",
      "     Training Step: 45 Training Loss: 3.6021087169647217 \n",
      "     Training Step: 46 Training Loss: 3.109848976135254 \n",
      "     Training Step: 47 Training Loss: 3.1973462104797363 \n",
      "     Training Step: 48 Training Loss: 3.425299644470215 \n",
      "     Training Step: 49 Training Loss: 3.7161810398101807 \n",
      "     Training Step: 50 Training Loss: 2.304710865020752 \n",
      "     Training Step: 51 Training Loss: 3.9968349933624268 \n",
      "     Training Step: 52 Training Loss: 3.053882122039795 \n",
      "     Training Step: 53 Training Loss: 3.0132970809936523 \n",
      "     Training Step: 54 Training Loss: 2.3319778442382812 \n",
      "     Training Step: 55 Training Loss: 2.3858938217163086 \n",
      "     Training Step: 56 Training Loss: 3.19389009475708 \n",
      "     Training Step: 57 Training Loss: 2.9416627883911133 \n",
      "     Training Step: 58 Training Loss: 3.274895668029785 \n",
      "     Training Step: 59 Training Loss: 2.556326150894165 \n",
      "     Training Step: 60 Training Loss: 2.37503981590271 \n",
      "     Training Step: 61 Training Loss: 2.6359591484069824 \n",
      "     Training Step: 62 Training Loss: 2.3616862297058105 \n",
      "     Training Step: 63 Training Loss: 2.412137508392334 \n",
      "     Training Step: 64 Training Loss: 4.094822406768799 \n",
      "     Training Step: 65 Training Loss: 3.3870444297790527 \n",
      "     Training Step: 66 Training Loss: 2.8077621459960938 \n",
      "     Training Step: 67 Training Loss: 3.5541934967041016 \n",
      "     Training Step: 68 Training Loss: 3.5502703189849854 \n",
      "     Training Step: 69 Training Loss: 3.598839521408081 \n",
      "     Training Step: 70 Training Loss: 2.941814422607422 \n",
      "     Training Step: 71 Training Loss: 2.563458204269409 \n",
      "     Training Step: 72 Training Loss: 3.486198902130127 \n",
      "     Training Step: 73 Training Loss: 2.807600975036621 \n",
      "     Training Step: 74 Training Loss: 2.393235445022583 \n",
      "     Training Step: 75 Training Loss: 2.8345518112182617 \n",
      "     Training Step: 76 Training Loss: 4.4990234375 \n",
      "     Training Step: 77 Training Loss: 2.2257485389709473 \n",
      "     Training Step: 78 Training Loss: 3.9385738372802734 \n",
      "     Training Step: 79 Training Loss: 3.047114372253418 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.505666971206665 \n",
      "     Validation Step: 1 Validation Loss: 3.086466073989868 \n",
      "     Validation Step: 2 Validation Loss: 3.5051286220550537 \n",
      "     Validation Step: 3 Validation Loss: 3.6440563201904297 \n",
      "     Validation Step: 4 Validation Loss: 3.3745131492614746 \n",
      "     Validation Step: 5 Validation Loss: 2.872417449951172 \n",
      "     Validation Step: 6 Validation Loss: 3.328947067260742 \n",
      "     Validation Step: 7 Validation Loss: 3.7055747509002686 \n",
      "     Validation Step: 8 Validation Loss: 2.758960723876953 \n",
      "     Validation Step: 9 Validation Loss: 2.4656853675842285 \n",
      "     Validation Step: 10 Validation Loss: 3.3248109817504883 \n",
      "     Validation Step: 11 Validation Loss: 2.360898971557617 \n",
      "     Validation Step: 12 Validation Loss: 3.001498222351074 \n",
      "     Validation Step: 13 Validation Loss: 3.562925338745117 \n",
      "     Validation Step: 14 Validation Loss: 2.7631828784942627 \n",
      "Epoch: 79\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.4428610801696777 \n",
      "     Training Step: 1 Training Loss: 2.43595552444458 \n",
      "     Training Step: 2 Training Loss: 2.5482499599456787 \n",
      "     Training Step: 3 Training Loss: 4.053919315338135 \n",
      "     Training Step: 4 Training Loss: 3.187623977661133 \n",
      "     Training Step: 5 Training Loss: 2.5454044342041016 \n",
      "     Training Step: 6 Training Loss: 2.9727704524993896 \n",
      "     Training Step: 7 Training Loss: 3.0815649032592773 \n",
      "     Training Step: 8 Training Loss: 3.0299458503723145 \n",
      "     Training Step: 9 Training Loss: 2.915590286254883 \n",
      "     Training Step: 10 Training Loss: 2.139655590057373 \n",
      "     Training Step: 11 Training Loss: 2.743821144104004 \n",
      "     Training Step: 12 Training Loss: 2.2150959968566895 \n",
      "     Training Step: 13 Training Loss: 2.410475730895996 \n",
      "     Training Step: 14 Training Loss: 3.6081955432891846 \n",
      "     Training Step: 15 Training Loss: 2.9336228370666504 \n",
      "     Training Step: 16 Training Loss: 3.3451614379882812 \n",
      "     Training Step: 17 Training Loss: 2.590536594390869 \n",
      "     Training Step: 18 Training Loss: 2.382948637008667 \n",
      "     Training Step: 19 Training Loss: 2.5863213539123535 \n",
      "     Training Step: 20 Training Loss: 2.5216660499572754 \n",
      "     Training Step: 21 Training Loss: 3.9700989723205566 \n",
      "     Training Step: 22 Training Loss: 3.1373391151428223 \n",
      "     Training Step: 23 Training Loss: 2.0943644046783447 \n",
      "     Training Step: 24 Training Loss: 4.555177211761475 \n",
      "     Training Step: 25 Training Loss: 3.3272223472595215 \n",
      "     Training Step: 26 Training Loss: 2.48223876953125 \n",
      "     Training Step: 27 Training Loss: 2.878681182861328 \n",
      "     Training Step: 28 Training Loss: 2.507866859436035 \n",
      "     Training Step: 29 Training Loss: 3.5600907802581787 \n",
      "     Training Step: 30 Training Loss: 2.94711971282959 \n",
      "     Training Step: 31 Training Loss: 2.5485939979553223 \n",
      "     Training Step: 32 Training Loss: 3.174128532409668 \n",
      "     Training Step: 33 Training Loss: 3.1950631141662598 \n",
      "     Training Step: 34 Training Loss: 2.705336570739746 \n",
      "     Training Step: 35 Training Loss: 2.4566173553466797 \n",
      "     Training Step: 36 Training Loss: 2.8718881607055664 \n",
      "     Training Step: 37 Training Loss: 2.763195514678955 \n",
      "     Training Step: 38 Training Loss: 3.1134090423583984 \n",
      "     Training Step: 39 Training Loss: 3.159371852874756 \n",
      "     Training Step: 40 Training Loss: 2.9173154830932617 \n",
      "     Training Step: 41 Training Loss: 4.041712284088135 \n",
      "     Training Step: 42 Training Loss: 3.2261650562286377 \n",
      "     Training Step: 43 Training Loss: 3.422537326812744 \n",
      "     Training Step: 44 Training Loss: 3.5806033611297607 \n",
      "     Training Step: 45 Training Loss: 2.5699801445007324 \n",
      "     Training Step: 46 Training Loss: 2.1046414375305176 \n",
      "     Training Step: 47 Training Loss: 3.0386922359466553 \n",
      "     Training Step: 48 Training Loss: 4.135958671569824 \n",
      "     Training Step: 49 Training Loss: 2.477879047393799 \n",
      "     Training Step: 50 Training Loss: 2.6239142417907715 \n",
      "     Training Step: 51 Training Loss: 2.4046874046325684 \n",
      "     Training Step: 52 Training Loss: 3.0605971813201904 \n",
      "     Training Step: 53 Training Loss: 3.300178289413452 \n",
      "     Training Step: 54 Training Loss: 2.539167642593384 \n",
      "     Training Step: 55 Training Loss: 3.7343649864196777 \n",
      "     Training Step: 56 Training Loss: 3.382639169692993 \n",
      "     Training Step: 57 Training Loss: 3.126925230026245 \n",
      "     Training Step: 58 Training Loss: 2.9336581230163574 \n",
      "     Training Step: 59 Training Loss: 3.126133680343628 \n",
      "     Training Step: 60 Training Loss: 2.9480576515197754 \n",
      "     Training Step: 61 Training Loss: 3.146925449371338 \n",
      "     Training Step: 62 Training Loss: 2.449462890625 \n",
      "     Training Step: 63 Training Loss: 2.6353559494018555 \n",
      "     Training Step: 64 Training Loss: 2.500396251678467 \n",
      "     Training Step: 65 Training Loss: 3.419274091720581 \n",
      "     Training Step: 66 Training Loss: 3.7351455688476562 \n",
      "     Training Step: 67 Training Loss: 3.1139512062072754 \n",
      "     Training Step: 68 Training Loss: 2.442211151123047 \n",
      "     Training Step: 69 Training Loss: 3.2096290588378906 \n",
      "     Training Step: 70 Training Loss: 2.0993080139160156 \n",
      "     Training Step: 71 Training Loss: 3.1943435668945312 \n",
      "     Training Step: 72 Training Loss: 3.221226215362549 \n",
      "     Training Step: 73 Training Loss: 2.7548716068267822 \n",
      "     Training Step: 74 Training Loss: 2.7282285690307617 \n",
      "     Training Step: 75 Training Loss: 2.9285836219787598 \n",
      "     Training Step: 76 Training Loss: 2.36222767829895 \n",
      "     Training Step: 77 Training Loss: 2.4731523990631104 \n",
      "     Training Step: 78 Training Loss: 2.9850025177001953 \n",
      "     Training Step: 79 Training Loss: 3.2627859115600586 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.5505378246307373 \n",
      "     Validation Step: 1 Validation Loss: 2.200185775756836 \n",
      "     Validation Step: 2 Validation Loss: 3.482311725616455 \n",
      "     Validation Step: 3 Validation Loss: 3.1316211223602295 \n",
      "     Validation Step: 4 Validation Loss: 2.8587546348571777 \n",
      "     Validation Step: 5 Validation Loss: 3.5185394287109375 \n",
      "     Validation Step: 6 Validation Loss: 3.3684372901916504 \n",
      "     Validation Step: 7 Validation Loss: 2.9385013580322266 \n",
      "     Validation Step: 8 Validation Loss: 2.9692955017089844 \n",
      "     Validation Step: 9 Validation Loss: 2.6158688068389893 \n",
      "     Validation Step: 10 Validation Loss: 3.061396360397339 \n",
      "     Validation Step: 11 Validation Loss: 3.5351338386535645 \n",
      "     Validation Step: 12 Validation Loss: 3.074495315551758 \n",
      "     Validation Step: 13 Validation Loss: 2.7067956924438477 \n",
      "     Validation Step: 14 Validation Loss: 3.8193655014038086 \n",
      "Epoch: 80\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.1053011417388916 \n",
      "     Training Step: 1 Training Loss: 2.047422170639038 \n",
      "     Training Step: 2 Training Loss: 3.2456345558166504 \n",
      "     Training Step: 3 Training Loss: 4.762136459350586 \n",
      "     Training Step: 4 Training Loss: 2.3856003284454346 \n",
      "     Training Step: 5 Training Loss: 2.501915216445923 \n",
      "     Training Step: 6 Training Loss: 3.045243263244629 \n",
      "     Training Step: 7 Training Loss: 2.082569122314453 \n",
      "     Training Step: 8 Training Loss: 2.701554775238037 \n",
      "     Training Step: 9 Training Loss: 3.3373026847839355 \n",
      "     Training Step: 10 Training Loss: 3.5481371879577637 \n",
      "     Training Step: 11 Training Loss: 3.0162253379821777 \n",
      "     Training Step: 12 Training Loss: 2.960237503051758 \n",
      "     Training Step: 13 Training Loss: 3.632028341293335 \n",
      "     Training Step: 14 Training Loss: 3.027132034301758 \n",
      "     Training Step: 15 Training Loss: 3.0979084968566895 \n",
      "     Training Step: 16 Training Loss: 3.8975019454956055 \n",
      "     Training Step: 17 Training Loss: 2.5876898765563965 \n",
      "     Training Step: 18 Training Loss: 2.4830422401428223 \n",
      "     Training Step: 19 Training Loss: 3.1082205772399902 \n",
      "     Training Step: 20 Training Loss: 3.3334035873413086 \n",
      "     Training Step: 21 Training Loss: 3.507380247116089 \n",
      "     Training Step: 22 Training Loss: 2.863346576690674 \n",
      "     Training Step: 23 Training Loss: 2.218003749847412 \n",
      "     Training Step: 24 Training Loss: 2.9287567138671875 \n",
      "     Training Step: 25 Training Loss: 2.5128684043884277 \n",
      "     Training Step: 26 Training Loss: 3.710806369781494 \n",
      "     Training Step: 27 Training Loss: 3.721517562866211 \n",
      "     Training Step: 28 Training Loss: 2.617638111114502 \n",
      "     Training Step: 29 Training Loss: 2.2387800216674805 \n",
      "     Training Step: 30 Training Loss: 2.4110424518585205 \n",
      "     Training Step: 31 Training Loss: 3.185530185699463 \n",
      "     Training Step: 32 Training Loss: 2.1952123641967773 \n",
      "     Training Step: 33 Training Loss: 2.400541305541992 \n",
      "     Training Step: 34 Training Loss: 4.086997985839844 \n",
      "     Training Step: 35 Training Loss: 3.673971176147461 \n",
      "     Training Step: 36 Training Loss: 2.960618734359741 \n",
      "     Training Step: 37 Training Loss: 3.8210291862487793 \n",
      "     Training Step: 38 Training Loss: 3.9791665077209473 \n",
      "     Training Step: 39 Training Loss: 2.6183812618255615 \n",
      "     Training Step: 40 Training Loss: 2.4447174072265625 \n",
      "     Training Step: 41 Training Loss: 3.168586492538452 \n",
      "     Training Step: 42 Training Loss: 3.612621307373047 \n",
      "     Training Step: 43 Training Loss: 3.4541077613830566 \n",
      "     Training Step: 44 Training Loss: 2.4829816818237305 \n",
      "     Training Step: 45 Training Loss: 2.6817357540130615 \n",
      "     Training Step: 46 Training Loss: 3.0894904136657715 \n",
      "     Training Step: 47 Training Loss: 3.358582019805908 \n",
      "     Training Step: 48 Training Loss: 2.8841171264648438 \n",
      "     Training Step: 49 Training Loss: 3.586322784423828 \n",
      "     Training Step: 50 Training Loss: 2.7681946754455566 \n",
      "     Training Step: 51 Training Loss: 3.0360727310180664 \n",
      "     Training Step: 52 Training Loss: 4.32019567489624 \n",
      "     Training Step: 53 Training Loss: 3.21964168548584 \n",
      "     Training Step: 54 Training Loss: 2.850090742111206 \n",
      "     Training Step: 55 Training Loss: 2.6580073833465576 \n",
      "     Training Step: 56 Training Loss: 3.265004873275757 \n",
      "     Training Step: 57 Training Loss: 2.2537033557891846 \n",
      "     Training Step: 58 Training Loss: 2.4816579818725586 \n",
      "     Training Step: 59 Training Loss: 2.8058362007141113 \n",
      "     Training Step: 60 Training Loss: 2.818178653717041 \n",
      "     Training Step: 61 Training Loss: 3.720198631286621 \n",
      "     Training Step: 62 Training Loss: 3.2439093589782715 \n",
      "     Training Step: 63 Training Loss: 4.0763983726501465 \n",
      "     Training Step: 64 Training Loss: 2.6007275581359863 \n",
      "     Training Step: 65 Training Loss: 2.949584484100342 \n",
      "     Training Step: 66 Training Loss: 2.248359441757202 \n",
      "     Training Step: 67 Training Loss: 3.49746036529541 \n",
      "     Training Step: 68 Training Loss: 3.3443603515625 \n",
      "     Training Step: 69 Training Loss: 2.975252151489258 \n",
      "     Training Step: 70 Training Loss: 3.161510467529297 \n",
      "     Training Step: 71 Training Loss: 2.9075334072113037 \n",
      "     Training Step: 72 Training Loss: 2.583110809326172 \n",
      "     Training Step: 73 Training Loss: 2.39263916015625 \n",
      "     Training Step: 74 Training Loss: 3.6690282821655273 \n",
      "     Training Step: 75 Training Loss: 3.076624870300293 \n",
      "     Training Step: 76 Training Loss: 2.8659088611602783 \n",
      "     Training Step: 77 Training Loss: 2.7498583793640137 \n",
      "     Training Step: 78 Training Loss: 2.7592711448669434 \n",
      "     Training Step: 79 Training Loss: 3.0359745025634766 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.005242347717285 \n",
      "     Validation Step: 1 Validation Loss: 3.5008604526519775 \n",
      "     Validation Step: 2 Validation Loss: 3.0489578247070312 \n",
      "     Validation Step: 3 Validation Loss: 3.2635321617126465 \n",
      "     Validation Step: 4 Validation Loss: 2.842008113861084 \n",
      "     Validation Step: 5 Validation Loss: 3.3503973484039307 \n",
      "     Validation Step: 6 Validation Loss: 3.3997604846954346 \n",
      "     Validation Step: 7 Validation Loss: 3.541621685028076 \n",
      "     Validation Step: 8 Validation Loss: 3.517094612121582 \n",
      "     Validation Step: 9 Validation Loss: 2.663109302520752 \n",
      "     Validation Step: 10 Validation Loss: 3.084322214126587 \n",
      "     Validation Step: 11 Validation Loss: 3.2294657230377197 \n",
      "     Validation Step: 12 Validation Loss: 2.1533703804016113 \n",
      "     Validation Step: 13 Validation Loss: 3.8317036628723145 \n",
      "     Validation Step: 14 Validation Loss: 3.818826913833618 \n",
      "---------------   CURRENT LEARNING RATE: 8.000000000000001e-06   ---------------\n",
      "Epoch: 80\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 5.708128452301025 \n",
      "     Training Step: 1 Training Loss: 5.0616044998168945 \n",
      "     Training Step: 2 Training Loss: 6.351646423339844 \n",
      "     Training Step: 3 Training Loss: 7.297306060791016 \n",
      "     Training Step: 4 Training Loss: 4.279818534851074 \n",
      "     Training Step: 5 Training Loss: 3.6887454986572266 \n",
      "     Training Step: 6 Training Loss: 6.1683030128479 \n",
      "     Training Step: 7 Training Loss: 2.831423282623291 \n",
      "     Training Step: 8 Training Loss: 5.634777545928955 \n",
      "     Training Step: 9 Training Loss: 5.386972427368164 \n",
      "     Training Step: 10 Training Loss: 7.844235897064209 \n",
      "     Training Step: 11 Training Loss: 3.690275192260742 \n",
      "     Training Step: 12 Training Loss: 7.334329605102539 \n",
      "     Training Step: 13 Training Loss: 6.757225036621094 \n",
      "     Training Step: 14 Training Loss: 5.898207187652588 \n",
      "     Training Step: 15 Training Loss: 6.673759937286377 \n",
      "     Training Step: 16 Training Loss: 7.907372951507568 \n",
      "     Training Step: 17 Training Loss: 4.030007362365723 \n",
      "     Training Step: 18 Training Loss: 6.641307830810547 \n",
      "     Training Step: 19 Training Loss: 7.4419074058532715 \n",
      "     Training Step: 20 Training Loss: 4.1867475509643555 \n",
      "     Training Step: 21 Training Loss: 5.800421714782715 \n",
      "     Training Step: 22 Training Loss: 9.041643142700195 \n",
      "     Training Step: 23 Training Loss: 8.33658504486084 \n",
      "     Training Step: 24 Training Loss: 7.482402324676514 \n",
      "     Training Step: 25 Training Loss: 6.579489707946777 \n",
      "     Training Step: 26 Training Loss: 5.233977317810059 \n",
      "     Training Step: 27 Training Loss: 5.413455963134766 \n",
      "     Training Step: 28 Training Loss: 6.951939105987549 \n",
      "     Training Step: 29 Training Loss: 8.413948059082031 \n",
      "     Training Step: 30 Training Loss: 4.569578170776367 \n",
      "     Training Step: 31 Training Loss: 6.206502437591553 \n",
      "     Training Step: 32 Training Loss: 5.134590148925781 \n",
      "     Training Step: 33 Training Loss: 3.3431272506713867 \n",
      "     Training Step: 34 Training Loss: 5.097894668579102 \n",
      "     Training Step: 35 Training Loss: 2.6871330738067627 \n",
      "     Training Step: 36 Training Loss: 6.048884391784668 \n",
      "     Training Step: 37 Training Loss: 6.670421600341797 \n",
      "     Training Step: 38 Training Loss: 3.05116605758667 \n",
      "     Training Step: 39 Training Loss: 9.6887845993042 \n",
      "     Training Step: 40 Training Loss: 5.908835411071777 \n",
      "     Training Step: 41 Training Loss: 10.079747200012207 \n",
      "     Training Step: 42 Training Loss: 3.647167682647705 \n",
      "     Training Step: 43 Training Loss: 6.850049018859863 \n",
      "     Training Step: 44 Training Loss: 11.411752700805664 \n",
      "     Training Step: 45 Training Loss: 11.921300888061523 \n",
      "     Training Step: 46 Training Loss: 13.086312294006348 \n",
      "     Training Step: 47 Training Loss: 4.610090255737305 \n",
      "     Training Step: 48 Training Loss: 4.974365711212158 \n",
      "     Training Step: 49 Training Loss: 5.103796482086182 \n",
      "     Training Step: 50 Training Loss: 5.729461193084717 \n",
      "     Training Step: 51 Training Loss: 6.642345905303955 \n",
      "     Training Step: 52 Training Loss: 3.602734327316284 \n",
      "     Training Step: 53 Training Loss: 8.085168838500977 \n",
      "     Training Step: 54 Training Loss: 5.44284725189209 \n",
      "     Training Step: 55 Training Loss: 5.706419944763184 \n",
      "     Training Step: 56 Training Loss: 8.380205154418945 \n",
      "     Training Step: 57 Training Loss: 6.340789794921875 \n",
      "     Training Step: 58 Training Loss: 3.7628843784332275 \n",
      "     Training Step: 59 Training Loss: 9.587419509887695 \n",
      "     Training Step: 60 Training Loss: 5.441897392272949 \n",
      "     Training Step: 61 Training Loss: 4.853376388549805 \n",
      "     Training Step: 62 Training Loss: 5.480179786682129 \n",
      "     Training Step: 63 Training Loss: 4.741135597229004 \n",
      "     Training Step: 64 Training Loss: 11.849407196044922 \n",
      "     Training Step: 65 Training Loss: 6.118941307067871 \n",
      "     Training Step: 66 Training Loss: 5.6376800537109375 \n",
      "     Training Step: 67 Training Loss: 6.668901443481445 \n",
      "     Training Step: 68 Training Loss: 5.393822193145752 \n",
      "     Training Step: 69 Training Loss: 7.164230823516846 \n",
      "     Training Step: 70 Training Loss: 4.940639972686768 \n",
      "     Training Step: 71 Training Loss: 5.406057834625244 \n",
      "     Training Step: 72 Training Loss: 6.0453972816467285 \n",
      "     Training Step: 73 Training Loss: 5.252697944641113 \n",
      "     Training Step: 74 Training Loss: 6.14147424697876 \n",
      "     Training Step: 75 Training Loss: 8.027912139892578 \n",
      "     Training Step: 76 Training Loss: 6.9921464920043945 \n",
      "     Training Step: 77 Training Loss: 4.940079212188721 \n",
      "     Training Step: 78 Training Loss: 7.112635135650635 \n",
      "     Training Step: 79 Training Loss: 5.864377021789551 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 8.939371109008789 \n",
      "     Validation Step: 1 Validation Loss: 6.873490333557129 \n",
      "     Validation Step: 2 Validation Loss: 3.310596466064453 \n",
      "     Validation Step: 3 Validation Loss: 5.768277168273926 \n",
      "     Validation Step: 4 Validation Loss: 5.6228928565979 \n",
      "     Validation Step: 5 Validation Loss: 6.3052778244018555 \n",
      "     Validation Step: 6 Validation Loss: 10.116703033447266 \n",
      "     Validation Step: 7 Validation Loss: 8.134273529052734 \n",
      "     Validation Step: 8 Validation Loss: 6.651108741760254 \n",
      "     Validation Step: 9 Validation Loss: 5.687870025634766 \n",
      "     Validation Step: 10 Validation Loss: 6.628545761108398 \n",
      "     Validation Step: 11 Validation Loss: 8.442007064819336 \n",
      "     Validation Step: 12 Validation Loss: 8.13235092163086 \n",
      "     Validation Step: 13 Validation Loss: 6.307669639587402 \n",
      "     Validation Step: 14 Validation Loss: 6.4690728187561035 \n",
      "Epoch: 81\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.086559772491455 \n",
      "     Training Step: 1 Training Loss: 2.6032207012176514 \n",
      "     Training Step: 2 Training Loss: 2.8844566345214844 \n",
      "     Training Step: 3 Training Loss: 2.698699951171875 \n",
      "     Training Step: 4 Training Loss: 2.7250454425811768 \n",
      "     Training Step: 5 Training Loss: 2.3071517944335938 \n",
      "     Training Step: 6 Training Loss: 3.4518611431121826 \n",
      "     Training Step: 7 Training Loss: 2.3750720024108887 \n",
      "     Training Step: 8 Training Loss: 2.770284414291382 \n",
      "     Training Step: 9 Training Loss: 3.2188706398010254 \n",
      "     Training Step: 10 Training Loss: 2.6442368030548096 \n",
      "     Training Step: 11 Training Loss: 3.5284266471862793 \n",
      "     Training Step: 12 Training Loss: 2.63737154006958 \n",
      "     Training Step: 13 Training Loss: 2.839735269546509 \n",
      "     Training Step: 14 Training Loss: 3.8272385597229004 \n",
      "     Training Step: 15 Training Loss: 3.2846412658691406 \n",
      "     Training Step: 16 Training Loss: 4.690441131591797 \n",
      "     Training Step: 17 Training Loss: 2.752335548400879 \n",
      "     Training Step: 18 Training Loss: 2.7858242988586426 \n",
      "     Training Step: 19 Training Loss: 2.4743103981018066 \n",
      "     Training Step: 20 Training Loss: 3.0812809467315674 \n",
      "     Training Step: 21 Training Loss: 2.8036718368530273 \n",
      "     Training Step: 22 Training Loss: 3.13495135307312 \n",
      "     Training Step: 23 Training Loss: 2.781329870223999 \n",
      "     Training Step: 24 Training Loss: 2.109100580215454 \n",
      "     Training Step: 25 Training Loss: 2.607881784439087 \n",
      "     Training Step: 26 Training Loss: 2.3975229263305664 \n",
      "     Training Step: 27 Training Loss: 3.647634506225586 \n",
      "     Training Step: 28 Training Loss: 2.5144546031951904 \n",
      "     Training Step: 29 Training Loss: 2.6480438709259033 \n",
      "     Training Step: 30 Training Loss: 3.8623335361480713 \n",
      "     Training Step: 31 Training Loss: 2.264103412628174 \n",
      "     Training Step: 32 Training Loss: 2.8324875831604004 \n",
      "     Training Step: 33 Training Loss: 2.837465286254883 \n",
      "     Training Step: 34 Training Loss: 3.1614983081817627 \n",
      "     Training Step: 35 Training Loss: 2.7472715377807617 \n",
      "     Training Step: 36 Training Loss: 2.4263548851013184 \n",
      "     Training Step: 37 Training Loss: 2.1645007133483887 \n",
      "     Training Step: 38 Training Loss: 3.49346923828125 \n",
      "     Training Step: 39 Training Loss: 3.5307531356811523 \n",
      "     Training Step: 40 Training Loss: 3.1451799869537354 \n",
      "     Training Step: 41 Training Loss: 2.6909878253936768 \n",
      "     Training Step: 42 Training Loss: 2.428027868270874 \n",
      "     Training Step: 43 Training Loss: 2.4473180770874023 \n",
      "     Training Step: 44 Training Loss: 3.4027352333068848 \n",
      "     Training Step: 45 Training Loss: 2.6311416625976562 \n",
      "     Training Step: 46 Training Loss: 2.7158641815185547 \n",
      "     Training Step: 47 Training Loss: 2.9303901195526123 \n",
      "     Training Step: 48 Training Loss: 3.2809700965881348 \n",
      "     Training Step: 49 Training Loss: 2.973729372024536 \n",
      "     Training Step: 50 Training Loss: 3.5920233726501465 \n",
      "     Training Step: 51 Training Loss: 2.9725379943847656 \n",
      "     Training Step: 52 Training Loss: 3.703645706176758 \n",
      "     Training Step: 53 Training Loss: 3.2726693153381348 \n",
      "     Training Step: 54 Training Loss: 3.8442864418029785 \n",
      "     Training Step: 55 Training Loss: 3.673194169998169 \n",
      "     Training Step: 56 Training Loss: 2.7781097888946533 \n",
      "     Training Step: 57 Training Loss: 2.578400135040283 \n",
      "     Training Step: 58 Training Loss: 3.0522279739379883 \n",
      "     Training Step: 59 Training Loss: 3.805231809616089 \n",
      "     Training Step: 60 Training Loss: 3.233771562576294 \n",
      "     Training Step: 61 Training Loss: 2.675901174545288 \n",
      "     Training Step: 62 Training Loss: 2.3373446464538574 \n",
      "     Training Step: 63 Training Loss: 3.0575108528137207 \n",
      "     Training Step: 64 Training Loss: 3.3774733543395996 \n",
      "     Training Step: 65 Training Loss: 3.04549503326416 \n",
      "     Training Step: 66 Training Loss: 2.2598183155059814 \n",
      "     Training Step: 67 Training Loss: 2.3789844512939453 \n",
      "     Training Step: 68 Training Loss: 3.669931411743164 \n",
      "     Training Step: 69 Training Loss: 2.9435977935791016 \n",
      "     Training Step: 70 Training Loss: 3.9075326919555664 \n",
      "     Training Step: 71 Training Loss: 2.544290781021118 \n",
      "     Training Step: 72 Training Loss: 3.448119878768921 \n",
      "     Training Step: 73 Training Loss: 2.0231308937072754 \n",
      "     Training Step: 74 Training Loss: 3.095055103302002 \n",
      "     Training Step: 75 Training Loss: 3.3098113536834717 \n",
      "     Training Step: 76 Training Loss: 3.5548038482666016 \n",
      "     Training Step: 77 Training Loss: 3.0559029579162598 \n",
      "     Training Step: 78 Training Loss: 2.4447126388549805 \n",
      "     Training Step: 79 Training Loss: 2.5189476013183594 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9866862297058105 \n",
      "     Validation Step: 1 Validation Loss: 3.010317325592041 \n",
      "     Validation Step: 2 Validation Loss: 2.7734875679016113 \n",
      "     Validation Step: 3 Validation Loss: 2.998879909515381 \n",
      "     Validation Step: 4 Validation Loss: 2.9110989570617676 \n",
      "     Validation Step: 5 Validation Loss: 3.375516653060913 \n",
      "     Validation Step: 6 Validation Loss: 2.2739477157592773 \n",
      "     Validation Step: 7 Validation Loss: 3.079072952270508 \n",
      "     Validation Step: 8 Validation Loss: 3.5197818279266357 \n",
      "     Validation Step: 9 Validation Loss: 2.6084208488464355 \n",
      "     Validation Step: 10 Validation Loss: 3.7785234451293945 \n",
      "     Validation Step: 11 Validation Loss: 2.6990315914154053 \n",
      "     Validation Step: 12 Validation Loss: 3.5027804374694824 \n",
      "     Validation Step: 13 Validation Loss: 3.5302255153656006 \n",
      "     Validation Step: 14 Validation Loss: 3.5408215522766113 \n",
      "Epoch: 82\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.217957019805908 \n",
      "     Training Step: 1 Training Loss: 2.6543519496917725 \n",
      "     Training Step: 2 Training Loss: 3.4251625537872314 \n",
      "     Training Step: 3 Training Loss: 2.886777877807617 \n",
      "     Training Step: 4 Training Loss: 2.600367546081543 \n",
      "     Training Step: 5 Training Loss: 3.57438325881958 \n",
      "     Training Step: 6 Training Loss: 2.376802444458008 \n",
      "     Training Step: 7 Training Loss: 2.7816805839538574 \n",
      "     Training Step: 8 Training Loss: 2.3087077140808105 \n",
      "     Training Step: 9 Training Loss: 3.134568452835083 \n",
      "     Training Step: 10 Training Loss: 3.2983570098876953 \n",
      "     Training Step: 11 Training Loss: 2.8590731620788574 \n",
      "     Training Step: 12 Training Loss: 3.1525025367736816 \n",
      "     Training Step: 13 Training Loss: 3.601762294769287 \n",
      "     Training Step: 14 Training Loss: 2.2896034717559814 \n",
      "     Training Step: 15 Training Loss: 3.1149544715881348 \n",
      "     Training Step: 16 Training Loss: 2.7201621532440186 \n",
      "     Training Step: 17 Training Loss: 2.424678087234497 \n",
      "     Training Step: 18 Training Loss: 2.9927988052368164 \n",
      "     Training Step: 19 Training Loss: 4.754209518432617 \n",
      "     Training Step: 20 Training Loss: 2.5750064849853516 \n",
      "     Training Step: 21 Training Loss: 3.1400856971740723 \n",
      "     Training Step: 22 Training Loss: 4.397074222564697 \n",
      "     Training Step: 23 Training Loss: 2.547865390777588 \n",
      "     Training Step: 24 Training Loss: 2.745936632156372 \n",
      "     Training Step: 25 Training Loss: 2.810767650604248 \n",
      "     Training Step: 26 Training Loss: 2.640425682067871 \n",
      "     Training Step: 27 Training Loss: 3.2430970668792725 \n",
      "     Training Step: 28 Training Loss: 3.1366002559661865 \n",
      "     Training Step: 29 Training Loss: 2.4214515686035156 \n",
      "     Training Step: 30 Training Loss: 4.032630443572998 \n",
      "     Training Step: 31 Training Loss: 2.418978691101074 \n",
      "     Training Step: 32 Training Loss: 3.517320156097412 \n",
      "     Training Step: 33 Training Loss: 2.9707932472229004 \n",
      "     Training Step: 34 Training Loss: 2.8194613456726074 \n",
      "     Training Step: 35 Training Loss: 2.6662447452545166 \n",
      "     Training Step: 36 Training Loss: 2.8752946853637695 \n",
      "     Training Step: 37 Training Loss: 2.8364453315734863 \n",
      "     Training Step: 38 Training Loss: 3.0270256996154785 \n",
      "     Training Step: 39 Training Loss: 2.765998601913452 \n",
      "     Training Step: 40 Training Loss: 2.834052324295044 \n",
      "     Training Step: 41 Training Loss: 3.04887056350708 \n",
      "     Training Step: 42 Training Loss: 2.912754774093628 \n",
      "     Training Step: 43 Training Loss: 3.108745813369751 \n",
      "     Training Step: 44 Training Loss: 2.7887613773345947 \n",
      "     Training Step: 45 Training Loss: 3.0303235054016113 \n",
      "     Training Step: 46 Training Loss: 2.2843830585479736 \n",
      "     Training Step: 47 Training Loss: 3.3314671516418457 \n",
      "     Training Step: 48 Training Loss: 2.990368366241455 \n",
      "     Training Step: 49 Training Loss: 3.1822383403778076 \n",
      "     Training Step: 50 Training Loss: 2.045358657836914 \n",
      "     Training Step: 51 Training Loss: 4.184443950653076 \n",
      "     Training Step: 52 Training Loss: 3.2367660999298096 \n",
      "     Training Step: 53 Training Loss: 3.330759286880493 \n",
      "     Training Step: 54 Training Loss: 2.103499412536621 \n",
      "     Training Step: 55 Training Loss: 2.2948684692382812 \n",
      "     Training Step: 56 Training Loss: 3.757002115249634 \n",
      "     Training Step: 57 Training Loss: 3.0203380584716797 \n",
      "     Training Step: 58 Training Loss: 2.047933578491211 \n",
      "     Training Step: 59 Training Loss: 3.7297606468200684 \n",
      "     Training Step: 60 Training Loss: 2.7907397747039795 \n",
      "     Training Step: 61 Training Loss: 2.712275266647339 \n",
      "     Training Step: 62 Training Loss: 2.7345776557922363 \n",
      "     Training Step: 63 Training Loss: 2.125119209289551 \n",
      "     Training Step: 64 Training Loss: 2.245864152908325 \n",
      "     Training Step: 65 Training Loss: 3.395695686340332 \n",
      "     Training Step: 66 Training Loss: 2.6921215057373047 \n",
      "     Training Step: 67 Training Loss: 2.9338831901550293 \n",
      "     Training Step: 68 Training Loss: 2.6114554405212402 \n",
      "     Training Step: 69 Training Loss: 2.3860597610473633 \n",
      "     Training Step: 70 Training Loss: 2.211223602294922 \n",
      "     Training Step: 71 Training Loss: 3.20210862159729 \n",
      "     Training Step: 72 Training Loss: 3.1447858810424805 \n",
      "     Training Step: 73 Training Loss: 2.5607175827026367 \n",
      "     Training Step: 74 Training Loss: 2.629347562789917 \n",
      "     Training Step: 75 Training Loss: 3.0870532989501953 \n",
      "     Training Step: 76 Training Loss: 4.2584733963012695 \n",
      "     Training Step: 77 Training Loss: 3.7181615829467773 \n",
      "     Training Step: 78 Training Loss: 3.140690326690674 \n",
      "     Training Step: 79 Training Loss: 2.3366973400115967 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.4476161003112793 \n",
      "     Validation Step: 1 Validation Loss: 3.675938129425049 \n",
      "     Validation Step: 2 Validation Loss: 2.745270252227783 \n",
      "     Validation Step: 3 Validation Loss: 2.347513198852539 \n",
      "     Validation Step: 4 Validation Loss: 2.925234794616699 \n",
      "     Validation Step: 5 Validation Loss: 3.4181244373321533 \n",
      "     Validation Step: 6 Validation Loss: 2.5165815353393555 \n",
      "     Validation Step: 7 Validation Loss: 2.8816349506378174 \n",
      "     Validation Step: 8 Validation Loss: 3.4058310985565186 \n",
      "     Validation Step: 9 Validation Loss: 3.538769006729126 \n",
      "     Validation Step: 10 Validation Loss: 3.2505245208740234 \n",
      "     Validation Step: 11 Validation Loss: 2.6454224586486816 \n",
      "     Validation Step: 12 Validation Loss: 3.7935171127319336 \n",
      "     Validation Step: 13 Validation Loss: 3.0302605628967285 \n",
      "     Validation Step: 14 Validation Loss: 2.9759395122528076 \n",
      "Epoch: 83\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.518609046936035 \n",
      "     Training Step: 1 Training Loss: 2.9087748527526855 \n",
      "     Training Step: 2 Training Loss: 3.41182804107666 \n",
      "     Training Step: 3 Training Loss: 4.675167560577393 \n",
      "     Training Step: 4 Training Loss: 3.3233213424682617 \n",
      "     Training Step: 5 Training Loss: 3.438235282897949 \n",
      "     Training Step: 6 Training Loss: 2.9404735565185547 \n",
      "     Training Step: 7 Training Loss: 2.5120959281921387 \n",
      "     Training Step: 8 Training Loss: 2.9856019020080566 \n",
      "     Training Step: 9 Training Loss: 2.370168924331665 \n",
      "     Training Step: 10 Training Loss: 2.218284845352173 \n",
      "     Training Step: 11 Training Loss: 2.7955238819122314 \n",
      "     Training Step: 12 Training Loss: 3.5405614376068115 \n",
      "     Training Step: 13 Training Loss: 2.8733768463134766 \n",
      "     Training Step: 14 Training Loss: 2.5358452796936035 \n",
      "     Training Step: 15 Training Loss: 2.9243569374084473 \n",
      "     Training Step: 16 Training Loss: 3.2161359786987305 \n",
      "     Training Step: 17 Training Loss: 3.103351354598999 \n",
      "     Training Step: 18 Training Loss: 2.2321441173553467 \n",
      "     Training Step: 19 Training Loss: 2.4087255001068115 \n",
      "     Training Step: 20 Training Loss: 3.2018496990203857 \n",
      "     Training Step: 21 Training Loss: 3.39274263381958 \n",
      "     Training Step: 22 Training Loss: 2.2617015838623047 \n",
      "     Training Step: 23 Training Loss: 2.1513142585754395 \n",
      "     Training Step: 24 Training Loss: 3.0081608295440674 \n",
      "     Training Step: 25 Training Loss: 2.720475673675537 \n",
      "     Training Step: 26 Training Loss: 3.517817258834839 \n",
      "     Training Step: 27 Training Loss: 2.9935760498046875 \n",
      "     Training Step: 28 Training Loss: 3.7686927318573 \n",
      "     Training Step: 29 Training Loss: 3.5994808673858643 \n",
      "     Training Step: 30 Training Loss: 3.3750782012939453 \n",
      "     Training Step: 31 Training Loss: 2.12869930267334 \n",
      "     Training Step: 32 Training Loss: 3.1339540481567383 \n",
      "     Training Step: 33 Training Loss: 3.135340690612793 \n",
      "     Training Step: 34 Training Loss: 2.5547561645507812 \n",
      "     Training Step: 35 Training Loss: 2.2397708892822266 \n",
      "     Training Step: 36 Training Loss: 3.053311347961426 \n",
      "     Training Step: 37 Training Loss: 2.3429341316223145 \n",
      "     Training Step: 38 Training Loss: 3.7658936977386475 \n",
      "     Training Step: 39 Training Loss: 4.331502437591553 \n",
      "     Training Step: 40 Training Loss: 2.254896402359009 \n",
      "     Training Step: 41 Training Loss: 2.709838390350342 \n",
      "     Training Step: 42 Training Loss: 2.5059914588928223 \n",
      "     Training Step: 43 Training Loss: 3.2229342460632324 \n",
      "     Training Step: 44 Training Loss: 3.745121955871582 \n",
      "     Training Step: 45 Training Loss: 2.1831231117248535 \n",
      "     Training Step: 46 Training Loss: 2.887815475463867 \n",
      "     Training Step: 47 Training Loss: 4.226601600646973 \n",
      "     Training Step: 48 Training Loss: 3.3141632080078125 \n",
      "     Training Step: 49 Training Loss: 3.6704258918762207 \n",
      "     Training Step: 50 Training Loss: 2.963486671447754 \n",
      "     Training Step: 51 Training Loss: 2.350727081298828 \n",
      "     Training Step: 52 Training Loss: 3.695646286010742 \n",
      "     Training Step: 53 Training Loss: 4.081585884094238 \n",
      "     Training Step: 54 Training Loss: 3.0897555351257324 \n",
      "     Training Step: 55 Training Loss: 3.2786262035369873 \n",
      "     Training Step: 56 Training Loss: 3.239119052886963 \n",
      "     Training Step: 57 Training Loss: 3.447690486907959 \n",
      "     Training Step: 58 Training Loss: 2.864189624786377 \n",
      "     Training Step: 59 Training Loss: 2.933520555496216 \n",
      "     Training Step: 60 Training Loss: 3.6089649200439453 \n",
      "     Training Step: 61 Training Loss: 3.22349214553833 \n",
      "     Training Step: 62 Training Loss: 2.2455716133117676 \n",
      "     Training Step: 63 Training Loss: 2.6375536918640137 \n",
      "     Training Step: 64 Training Loss: 2.5401315689086914 \n",
      "     Training Step: 65 Training Loss: 2.789238929748535 \n",
      "     Training Step: 66 Training Loss: 2.5678601264953613 \n",
      "     Training Step: 67 Training Loss: 2.467031478881836 \n",
      "     Training Step: 68 Training Loss: 2.539113998413086 \n",
      "     Training Step: 69 Training Loss: 3.1953444480895996 \n",
      "     Training Step: 70 Training Loss: 2.7810111045837402 \n",
      "     Training Step: 71 Training Loss: 2.4906094074249268 \n",
      "     Training Step: 72 Training Loss: 3.5963706970214844 \n",
      "     Training Step: 73 Training Loss: 2.8969216346740723 \n",
      "     Training Step: 74 Training Loss: 2.895810127258301 \n",
      "     Training Step: 75 Training Loss: 3.012211322784424 \n",
      "     Training Step: 76 Training Loss: 4.113494873046875 \n",
      "     Training Step: 77 Training Loss: 3.611851692199707 \n",
      "     Training Step: 78 Training Loss: 3.2607884407043457 \n",
      "     Training Step: 79 Training Loss: 3.1315221786499023 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.439946174621582 \n",
      "     Validation Step: 1 Validation Loss: 2.936877727508545 \n",
      "     Validation Step: 2 Validation Loss: 2.6656341552734375 \n",
      "     Validation Step: 3 Validation Loss: 3.1389780044555664 \n",
      "     Validation Step: 4 Validation Loss: 2.536752462387085 \n",
      "     Validation Step: 5 Validation Loss: 3.8935165405273438 \n",
      "     Validation Step: 6 Validation Loss: 3.6038947105407715 \n",
      "     Validation Step: 7 Validation Loss: 3.316214084625244 \n",
      "     Validation Step: 8 Validation Loss: 3.517888307571411 \n",
      "     Validation Step: 9 Validation Loss: 2.9546091556549072 \n",
      "     Validation Step: 10 Validation Loss: 2.4665098190307617 \n",
      "     Validation Step: 11 Validation Loss: 3.813391923904419 \n",
      "     Validation Step: 12 Validation Loss: 2.8785815238952637 \n",
      "     Validation Step: 13 Validation Loss: 2.4916083812713623 \n",
      "     Validation Step: 14 Validation Loss: 3.4663760662078857 \n",
      "Epoch: 84\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.38728404045105 \n",
      "     Training Step: 1 Training Loss: 3.1741981506347656 \n",
      "     Training Step: 2 Training Loss: 3.525338888168335 \n",
      "     Training Step: 3 Training Loss: 2.6523959636688232 \n",
      "     Training Step: 4 Training Loss: 3.225336790084839 \n",
      "     Training Step: 5 Training Loss: 2.076201915740967 \n",
      "     Training Step: 6 Training Loss: 3.128523826599121 \n",
      "     Training Step: 7 Training Loss: 3.2845053672790527 \n",
      "     Training Step: 8 Training Loss: 3.1131067276000977 \n",
      "     Training Step: 9 Training Loss: 2.1884140968322754 \n",
      "     Training Step: 10 Training Loss: 2.7116241455078125 \n",
      "     Training Step: 11 Training Loss: 3.16505765914917 \n",
      "     Training Step: 12 Training Loss: 3.5473155975341797 \n",
      "     Training Step: 13 Training Loss: 2.7740306854248047 \n",
      "     Training Step: 14 Training Loss: 3.502828598022461 \n",
      "     Training Step: 15 Training Loss: 3.6237149238586426 \n",
      "     Training Step: 16 Training Loss: 4.013299465179443 \n",
      "     Training Step: 17 Training Loss: 3.3610262870788574 \n",
      "     Training Step: 18 Training Loss: 3.0156168937683105 \n",
      "     Training Step: 19 Training Loss: 4.172622203826904 \n",
      "     Training Step: 20 Training Loss: 2.9073867797851562 \n",
      "     Training Step: 21 Training Loss: 3.851027011871338 \n",
      "     Training Step: 22 Training Loss: 3.321139097213745 \n",
      "     Training Step: 23 Training Loss: 2.4062411785125732 \n",
      "     Training Step: 24 Training Loss: 3.21982479095459 \n",
      "     Training Step: 25 Training Loss: 2.771190881729126 \n",
      "     Training Step: 26 Training Loss: 2.4972565174102783 \n",
      "     Training Step: 27 Training Loss: 3.5654966831207275 \n",
      "     Training Step: 28 Training Loss: 3.160788059234619 \n",
      "     Training Step: 29 Training Loss: 3.2022571563720703 \n",
      "     Training Step: 30 Training Loss: 3.5427823066711426 \n",
      "     Training Step: 31 Training Loss: 2.2561752796173096 \n",
      "     Training Step: 32 Training Loss: 2.3752987384796143 \n",
      "     Training Step: 33 Training Loss: 3.6131303310394287 \n",
      "     Training Step: 34 Training Loss: 2.9422290325164795 \n",
      "     Training Step: 35 Training Loss: 2.941422462463379 \n",
      "     Training Step: 36 Training Loss: 3.2885079383850098 \n",
      "     Training Step: 37 Training Loss: 2.4878501892089844 \n",
      "     Training Step: 38 Training Loss: 3.153085231781006 \n",
      "     Training Step: 39 Training Loss: 2.8327066898345947 \n",
      "     Training Step: 40 Training Loss: 2.9410758018493652 \n",
      "     Training Step: 41 Training Loss: 2.4702653884887695 \n",
      "     Training Step: 42 Training Loss: 2.417506694793701 \n",
      "     Training Step: 43 Training Loss: 3.3215763568878174 \n",
      "     Training Step: 44 Training Loss: 2.4295270442962646 \n",
      "     Training Step: 45 Training Loss: 2.853627920150757 \n",
      "     Training Step: 46 Training Loss: 2.917360305786133 \n",
      "     Training Step: 47 Training Loss: 3.3668341636657715 \n",
      "     Training Step: 48 Training Loss: 2.6600341796875 \n",
      "     Training Step: 49 Training Loss: 3.193892240524292 \n",
      "     Training Step: 50 Training Loss: 2.3059463500976562 \n",
      "     Training Step: 51 Training Loss: 4.245398998260498 \n",
      "     Training Step: 52 Training Loss: 2.4777026176452637 \n",
      "     Training Step: 53 Training Loss: 2.1574440002441406 \n",
      "     Training Step: 54 Training Loss: 2.2494468688964844 \n",
      "     Training Step: 55 Training Loss: 2.0846996307373047 \n",
      "     Training Step: 56 Training Loss: 3.187422752380371 \n",
      "     Training Step: 57 Training Loss: 2.8199336528778076 \n",
      "     Training Step: 58 Training Loss: 3.23756742477417 \n",
      "     Training Step: 59 Training Loss: 2.454979419708252 \n",
      "     Training Step: 60 Training Loss: 2.125231981277466 \n",
      "     Training Step: 61 Training Loss: 2.5891623497009277 \n",
      "     Training Step: 62 Training Loss: 3.268108367919922 \n",
      "     Training Step: 63 Training Loss: 2.814229726791382 \n",
      "     Training Step: 64 Training Loss: 2.5525217056274414 \n",
      "     Training Step: 65 Training Loss: 3.0107879638671875 \n",
      "     Training Step: 66 Training Loss: 3.839345932006836 \n",
      "     Training Step: 67 Training Loss: 3.194847583770752 \n",
      "     Training Step: 68 Training Loss: 2.6320526599884033 \n",
      "     Training Step: 69 Training Loss: 4.349871635437012 \n",
      "     Training Step: 70 Training Loss: 2.3838508129119873 \n",
      "     Training Step: 71 Training Loss: 2.572817802429199 \n",
      "     Training Step: 72 Training Loss: 2.3561010360717773 \n",
      "     Training Step: 73 Training Loss: 2.1318366527557373 \n",
      "     Training Step: 74 Training Loss: 2.886932373046875 \n",
      "     Training Step: 75 Training Loss: 4.665594100952148 \n",
      "     Training Step: 76 Training Loss: 3.535118341445923 \n",
      "     Training Step: 77 Training Loss: 3.0864248275756836 \n",
      "     Training Step: 78 Training Loss: 3.1318140029907227 \n",
      "     Training Step: 79 Training Loss: 2.2319788932800293 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9864466190338135 \n",
      "     Validation Step: 1 Validation Loss: 3.5210254192352295 \n",
      "     Validation Step: 2 Validation Loss: 2.8180460929870605 \n",
      "     Validation Step: 3 Validation Loss: 3.3701045513153076 \n",
      "     Validation Step: 4 Validation Loss: 3.0676605701446533 \n",
      "     Validation Step: 5 Validation Loss: 3.3449041843414307 \n",
      "     Validation Step: 6 Validation Loss: 3.5838003158569336 \n",
      "     Validation Step: 7 Validation Loss: 2.294213056564331 \n",
      "     Validation Step: 8 Validation Loss: 3.8158645629882812 \n",
      "     Validation Step: 9 Validation Loss: 3.532930612564087 \n",
      "     Validation Step: 10 Validation Loss: 2.6032323837280273 \n",
      "     Validation Step: 11 Validation Loss: 2.7069320678710938 \n",
      "     Validation Step: 12 Validation Loss: 3.0803539752960205 \n",
      "     Validation Step: 13 Validation Loss: 3.1353132724761963 \n",
      "     Validation Step: 14 Validation Loss: 3.649554967880249 \n",
      "Epoch: 85\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.091872215270996 \n",
      "     Training Step: 1 Training Loss: 3.25300931930542 \n",
      "     Training Step: 2 Training Loss: 2.8835887908935547 \n",
      "     Training Step: 3 Training Loss: 3.021296977996826 \n",
      "     Training Step: 4 Training Loss: 2.9260478019714355 \n",
      "     Training Step: 5 Training Loss: 2.6895155906677246 \n",
      "     Training Step: 6 Training Loss: 2.974738597869873 \n",
      "     Training Step: 7 Training Loss: 2.4053540229797363 \n",
      "     Training Step: 8 Training Loss: 2.801892042160034 \n",
      "     Training Step: 9 Training Loss: 3.390512466430664 \n",
      "     Training Step: 10 Training Loss: 3.3344790935516357 \n",
      "     Training Step: 11 Training Loss: 3.656019926071167 \n",
      "     Training Step: 12 Training Loss: 2.8586156368255615 \n",
      "     Training Step: 13 Training Loss: 3.5995054244995117 \n",
      "     Training Step: 14 Training Loss: 2.303649425506592 \n",
      "     Training Step: 15 Training Loss: 2.5525312423706055 \n",
      "     Training Step: 16 Training Loss: 3.1913931369781494 \n",
      "     Training Step: 17 Training Loss: 3.205334186553955 \n",
      "     Training Step: 18 Training Loss: 3.1050515174865723 \n",
      "     Training Step: 19 Training Loss: 2.176088809967041 \n",
      "     Training Step: 20 Training Loss: 3.4001386165618896 \n",
      "     Training Step: 21 Training Loss: 2.8457069396972656 \n",
      "     Training Step: 22 Training Loss: 3.272106409072876 \n",
      "     Training Step: 23 Training Loss: 3.3476076126098633 \n",
      "     Training Step: 24 Training Loss: 2.138439655303955 \n",
      "     Training Step: 25 Training Loss: 2.7616934776306152 \n",
      "     Training Step: 26 Training Loss: 3.775212526321411 \n",
      "     Training Step: 27 Training Loss: 3.5504112243652344 \n",
      "     Training Step: 28 Training Loss: 2.7775216102600098 \n",
      "     Training Step: 29 Training Loss: 3.544908046722412 \n",
      "     Training Step: 30 Training Loss: 2.067155599594116 \n",
      "     Training Step: 31 Training Loss: 3.7705116271972656 \n",
      "     Training Step: 32 Training Loss: 2.546492576599121 \n",
      "     Training Step: 33 Training Loss: 3.827967882156372 \n",
      "     Training Step: 34 Training Loss: 3.3220973014831543 \n",
      "     Training Step: 35 Training Loss: 2.781067371368408 \n",
      "     Training Step: 36 Training Loss: 2.9459569454193115 \n",
      "     Training Step: 37 Training Loss: 2.547894239425659 \n",
      "     Training Step: 38 Training Loss: 2.3619585037231445 \n",
      "     Training Step: 39 Training Loss: 4.388194561004639 \n",
      "     Training Step: 40 Training Loss: 2.278374671936035 \n",
      "     Training Step: 41 Training Loss: 3.351933479309082 \n",
      "     Training Step: 42 Training Loss: 3.153779983520508 \n",
      "     Training Step: 43 Training Loss: 3.189206600189209 \n",
      "     Training Step: 44 Training Loss: 3.263881206512451 \n",
      "     Training Step: 45 Training Loss: 4.306209564208984 \n",
      "     Training Step: 46 Training Loss: 3.147155284881592 \n",
      "     Training Step: 47 Training Loss: 3.7056961059570312 \n",
      "     Training Step: 48 Training Loss: 2.654726505279541 \n",
      "     Training Step: 49 Training Loss: 2.4235119819641113 \n",
      "     Training Step: 50 Training Loss: 3.083383560180664 \n",
      "     Training Step: 51 Training Loss: 3.286625385284424 \n",
      "     Training Step: 52 Training Loss: 3.0780386924743652 \n",
      "     Training Step: 53 Training Loss: 2.5783023834228516 \n",
      "     Training Step: 54 Training Loss: 2.5459556579589844 \n",
      "     Training Step: 55 Training Loss: 3.735978603363037 \n",
      "     Training Step: 56 Training Loss: 2.315192699432373 \n",
      "     Training Step: 57 Training Loss: 2.8051376342773438 \n",
      "     Training Step: 58 Training Loss: 2.888334274291992 \n",
      "     Training Step: 59 Training Loss: 2.1620235443115234 \n",
      "     Training Step: 60 Training Loss: 3.591176748275757 \n",
      "     Training Step: 61 Training Loss: 2.6902503967285156 \n",
      "     Training Step: 62 Training Loss: 2.4199085235595703 \n",
      "     Training Step: 63 Training Loss: 2.9160618782043457 \n",
      "     Training Step: 64 Training Loss: 2.8181943893432617 \n",
      "     Training Step: 65 Training Loss: 3.022254228591919 \n",
      "     Training Step: 66 Training Loss: 2.708348512649536 \n",
      "     Training Step: 67 Training Loss: 4.193424701690674 \n",
      "     Training Step: 68 Training Loss: 3.676685333251953 \n",
      "     Training Step: 69 Training Loss: 4.676959991455078 \n",
      "     Training Step: 70 Training Loss: 2.546271562576294 \n",
      "     Training Step: 71 Training Loss: 3.9400644302368164 \n",
      "     Training Step: 72 Training Loss: 2.181351661682129 \n",
      "     Training Step: 73 Training Loss: 2.2509055137634277 \n",
      "     Training Step: 74 Training Loss: 3.3700947761535645 \n",
      "     Training Step: 75 Training Loss: 2.7915310859680176 \n",
      "     Training Step: 76 Training Loss: 2.5491061210632324 \n",
      "     Training Step: 77 Training Loss: 2.2379684448242188 \n",
      "     Training Step: 78 Training Loss: 2.5590994358062744 \n",
      "     Training Step: 79 Training Loss: 2.6066455841064453 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.612302303314209 \n",
      "     Validation Step: 1 Validation Loss: 2.795193672180176 \n",
      "     Validation Step: 2 Validation Loss: 2.9524128437042236 \n",
      "     Validation Step: 3 Validation Loss: 3.5017666816711426 \n",
      "     Validation Step: 4 Validation Loss: 2.337484836578369 \n",
      "     Validation Step: 5 Validation Loss: 3.173386573791504 \n",
      "     Validation Step: 6 Validation Loss: 3.136164903640747 \n",
      "     Validation Step: 7 Validation Loss: 3.7089147567749023 \n",
      "     Validation Step: 8 Validation Loss: 3.6887660026550293 \n",
      "     Validation Step: 9 Validation Loss: 2.7029404640197754 \n",
      "     Validation Step: 10 Validation Loss: 3.101780891418457 \n",
      "     Validation Step: 11 Validation Loss: 3.046189785003662 \n",
      "     Validation Step: 12 Validation Loss: 2.7491936683654785 \n",
      "     Validation Step: 13 Validation Loss: 3.95540189743042 \n",
      "     Validation Step: 14 Validation Loss: 3.5894627571105957 \n",
      "Epoch: 86\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.3255105018615723 \n",
      "     Training Step: 1 Training Loss: 2.8626692295074463 \n",
      "     Training Step: 2 Training Loss: 2.449577808380127 \n",
      "     Training Step: 3 Training Loss: 3.263604164123535 \n",
      "     Training Step: 4 Training Loss: 3.3550057411193848 \n",
      "     Training Step: 5 Training Loss: 4.2283501625061035 \n",
      "     Training Step: 6 Training Loss: 3.643430233001709 \n",
      "     Training Step: 7 Training Loss: 2.447567939758301 \n",
      "     Training Step: 8 Training Loss: 2.883234977722168 \n",
      "     Training Step: 9 Training Loss: 3.8454086780548096 \n",
      "     Training Step: 10 Training Loss: 3.17038631439209 \n",
      "     Training Step: 11 Training Loss: 2.3422203063964844 \n",
      "     Training Step: 12 Training Loss: 2.924044609069824 \n",
      "     Training Step: 13 Training Loss: 3.588984966278076 \n",
      "     Training Step: 14 Training Loss: 2.858341693878174 \n",
      "     Training Step: 15 Training Loss: 3.3558571338653564 \n",
      "     Training Step: 16 Training Loss: 2.449436664581299 \n",
      "     Training Step: 17 Training Loss: 4.433424949645996 \n",
      "     Training Step: 18 Training Loss: 3.5092239379882812 \n",
      "     Training Step: 19 Training Loss: 4.032950401306152 \n",
      "     Training Step: 20 Training Loss: 3.505547046661377 \n",
      "     Training Step: 21 Training Loss: 2.643428325653076 \n",
      "     Training Step: 22 Training Loss: 2.913130283355713 \n",
      "     Training Step: 23 Training Loss: 3.322554588317871 \n",
      "     Training Step: 24 Training Loss: 2.884974241256714 \n",
      "     Training Step: 25 Training Loss: 2.7687127590179443 \n",
      "     Training Step: 26 Training Loss: 4.0425944328308105 \n",
      "     Training Step: 27 Training Loss: 2.4328341484069824 \n",
      "     Training Step: 28 Training Loss: 3.041455030441284 \n",
      "     Training Step: 29 Training Loss: 3.113090991973877 \n",
      "     Training Step: 30 Training Loss: 2.858217239379883 \n",
      "     Training Step: 31 Training Loss: 3.334868907928467 \n",
      "     Training Step: 32 Training Loss: 2.5075924396514893 \n",
      "     Training Step: 33 Training Loss: 3.145880699157715 \n",
      "     Training Step: 34 Training Loss: 3.5430705547332764 \n",
      "     Training Step: 35 Training Loss: 3.5137081146240234 \n",
      "     Training Step: 36 Training Loss: 3.4810707569122314 \n",
      "     Training Step: 37 Training Loss: 2.511800527572632 \n",
      "     Training Step: 38 Training Loss: 3.595231056213379 \n",
      "     Training Step: 39 Training Loss: 2.719109058380127 \n",
      "     Training Step: 40 Training Loss: 3.149237632751465 \n",
      "     Training Step: 41 Training Loss: 3.068948268890381 \n",
      "     Training Step: 42 Training Loss: 2.5792593955993652 \n",
      "     Training Step: 43 Training Loss: 2.3914949893951416 \n",
      "     Training Step: 44 Training Loss: 2.5089831352233887 \n",
      "     Training Step: 45 Training Loss: 2.2132856845855713 \n",
      "     Training Step: 46 Training Loss: 3.3819360733032227 \n",
      "     Training Step: 47 Training Loss: 2.8015241622924805 \n",
      "     Training Step: 48 Training Loss: 2.283967971801758 \n",
      "     Training Step: 49 Training Loss: 3.3370397090911865 \n",
      "     Training Step: 50 Training Loss: 2.9781360626220703 \n",
      "     Training Step: 51 Training Loss: 2.3080296516418457 \n",
      "     Training Step: 52 Training Loss: 2.988764762878418 \n",
      "     Training Step: 53 Training Loss: 2.5692691802978516 \n",
      "     Training Step: 54 Training Loss: 3.040128469467163 \n",
      "     Training Step: 55 Training Loss: 2.251573085784912 \n",
      "     Training Step: 56 Training Loss: 2.6372179985046387 \n",
      "     Training Step: 57 Training Loss: 3.376347541809082 \n",
      "     Training Step: 58 Training Loss: 3.4681694507598877 \n",
      "     Training Step: 59 Training Loss: 2.470069646835327 \n",
      "     Training Step: 60 Training Loss: 3.342589855194092 \n",
      "     Training Step: 61 Training Loss: 3.219027042388916 \n",
      "     Training Step: 62 Training Loss: 3.6005196571350098 \n",
      "     Training Step: 63 Training Loss: 2.2954635620117188 \n",
      "     Training Step: 64 Training Loss: 3.387645721435547 \n",
      "     Training Step: 65 Training Loss: 2.3325748443603516 \n",
      "     Training Step: 66 Training Loss: 3.5368988513946533 \n",
      "     Training Step: 67 Training Loss: 2.144184112548828 \n",
      "     Training Step: 68 Training Loss: 2.263021945953369 \n",
      "     Training Step: 69 Training Loss: 2.449587345123291 \n",
      "     Training Step: 70 Training Loss: 2.385305166244507 \n",
      "     Training Step: 71 Training Loss: 2.727949619293213 \n",
      "     Training Step: 72 Training Loss: 4.239418029785156 \n",
      "     Training Step: 73 Training Loss: 2.1363537311553955 \n",
      "     Training Step: 74 Training Loss: 2.7418413162231445 \n",
      "     Training Step: 75 Training Loss: 2.5991499423980713 \n",
      "     Training Step: 76 Training Loss: 2.7097535133361816 \n",
      "     Training Step: 77 Training Loss: 2.7397823333740234 \n",
      "     Training Step: 78 Training Loss: 2.7124691009521484 \n",
      "     Training Step: 79 Training Loss: 2.2739005088806152 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.793488025665283 \n",
      "     Validation Step: 1 Validation Loss: 3.17032790184021 \n",
      "     Validation Step: 2 Validation Loss: 3.0180840492248535 \n",
      "     Validation Step: 3 Validation Loss: 3.6177215576171875 \n",
      "     Validation Step: 4 Validation Loss: 2.95013689994812 \n",
      "     Validation Step: 5 Validation Loss: 3.2886240482330322 \n",
      "     Validation Step: 6 Validation Loss: 2.2323057651519775 \n",
      "     Validation Step: 7 Validation Loss: 3.7893505096435547 \n",
      "     Validation Step: 8 Validation Loss: 4.018948554992676 \n",
      "     Validation Step: 9 Validation Loss: 3.5942862033843994 \n",
      "     Validation Step: 10 Validation Loss: 2.841087579727173 \n",
      "     Validation Step: 11 Validation Loss: 2.726402521133423 \n",
      "     Validation Step: 12 Validation Loss: 2.8361544609069824 \n",
      "     Validation Step: 13 Validation Loss: 3.2694146633148193 \n",
      "     Validation Step: 14 Validation Loss: 3.1145646572113037 \n",
      "Epoch: 87\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.7150373458862305 \n",
      "     Training Step: 1 Training Loss: 2.2198479175567627 \n",
      "     Training Step: 2 Training Loss: 3.1893959045410156 \n",
      "     Training Step: 3 Training Loss: 2.4008288383483887 \n",
      "     Training Step: 4 Training Loss: 3.3093252182006836 \n",
      "     Training Step: 5 Training Loss: 2.6786646842956543 \n",
      "     Training Step: 6 Training Loss: 2.8016884326934814 \n",
      "     Training Step: 7 Training Loss: 3.3465306758880615 \n",
      "     Training Step: 8 Training Loss: 3.2154245376586914 \n",
      "     Training Step: 9 Training Loss: 2.4015047550201416 \n",
      "     Training Step: 10 Training Loss: 3.138758420944214 \n",
      "     Training Step: 11 Training Loss: 3.151937484741211 \n",
      "     Training Step: 12 Training Loss: 2.5708117485046387 \n",
      "     Training Step: 13 Training Loss: 2.4368667602539062 \n",
      "     Training Step: 14 Training Loss: 3.526869773864746 \n",
      "     Training Step: 15 Training Loss: 2.6356186866760254 \n",
      "     Training Step: 16 Training Loss: 2.6402788162231445 \n",
      "     Training Step: 17 Training Loss: 2.166137933731079 \n",
      "     Training Step: 18 Training Loss: 4.646235466003418 \n",
      "     Training Step: 19 Training Loss: 4.101316452026367 \n",
      "     Training Step: 20 Training Loss: 3.443274974822998 \n",
      "     Training Step: 21 Training Loss: 2.361635684967041 \n",
      "     Training Step: 22 Training Loss: 3.302861213684082 \n",
      "     Training Step: 23 Training Loss: 2.951455593109131 \n",
      "     Training Step: 24 Training Loss: 2.9583005905151367 \n",
      "     Training Step: 25 Training Loss: 2.4004931449890137 \n",
      "     Training Step: 26 Training Loss: 3.132235527038574 \n",
      "     Training Step: 27 Training Loss: 2.4752798080444336 \n",
      "     Training Step: 28 Training Loss: 3.0251784324645996 \n",
      "     Training Step: 29 Training Loss: 2.3990237712860107 \n",
      "     Training Step: 30 Training Loss: 3.563828468322754 \n",
      "     Training Step: 31 Training Loss: 3.133082866668701 \n",
      "     Training Step: 32 Training Loss: 3.597074508666992 \n",
      "     Training Step: 33 Training Loss: 2.32191801071167 \n",
      "     Training Step: 34 Training Loss: 2.6684587001800537 \n",
      "     Training Step: 35 Training Loss: 4.005645275115967 \n",
      "     Training Step: 36 Training Loss: 3.2065558433532715 \n",
      "     Training Step: 37 Training Loss: 2.5772836208343506 \n",
      "     Training Step: 38 Training Loss: 3.590231418609619 \n",
      "     Training Step: 39 Training Loss: 2.895346164703369 \n",
      "     Training Step: 40 Training Loss: 3.1115076541900635 \n",
      "     Training Step: 41 Training Loss: 2.585206985473633 \n",
      "     Training Step: 42 Training Loss: 3.577181339263916 \n",
      "     Training Step: 43 Training Loss: 2.540447473526001 \n",
      "     Training Step: 44 Training Loss: 3.20686936378479 \n",
      "     Training Step: 45 Training Loss: 2.390407085418701 \n",
      "     Training Step: 46 Training Loss: 2.9117813110351562 \n",
      "     Training Step: 47 Training Loss: 3.1079843044281006 \n",
      "     Training Step: 48 Training Loss: 2.8766837120056152 \n",
      "     Training Step: 49 Training Loss: 3.68471097946167 \n",
      "     Training Step: 50 Training Loss: 2.2864437103271484 \n",
      "     Training Step: 51 Training Loss: 4.252653121948242 \n",
      "     Training Step: 52 Training Loss: 2.265568733215332 \n",
      "     Training Step: 53 Training Loss: 2.8175692558288574 \n",
      "     Training Step: 54 Training Loss: 3.4350688457489014 \n",
      "     Training Step: 55 Training Loss: 3.40364146232605 \n",
      "     Training Step: 56 Training Loss: 2.910163402557373 \n",
      "     Training Step: 57 Training Loss: 3.0154523849487305 \n",
      "     Training Step: 58 Training Loss: 3.8376710414886475 \n",
      "     Training Step: 59 Training Loss: 2.9092564582824707 \n",
      "     Training Step: 60 Training Loss: 2.101503849029541 \n",
      "     Training Step: 61 Training Loss: 3.8880653381347656 \n",
      "     Training Step: 62 Training Loss: 2.0911107063293457 \n",
      "     Training Step: 63 Training Loss: 3.2845423221588135 \n",
      "     Training Step: 64 Training Loss: 2.63049054145813 \n",
      "     Training Step: 65 Training Loss: 3.1731033325195312 \n",
      "     Training Step: 66 Training Loss: 3.3332858085632324 \n",
      "     Training Step: 67 Training Loss: 2.537311553955078 \n",
      "     Training Step: 68 Training Loss: 2.9684982299804688 \n",
      "     Training Step: 69 Training Loss: 2.8253397941589355 \n",
      "     Training Step: 70 Training Loss: 2.614457130432129 \n",
      "     Training Step: 71 Training Loss: 4.5815582275390625 \n",
      "     Training Step: 72 Training Loss: 3.290616512298584 \n",
      "     Training Step: 73 Training Loss: 3.8493595123291016 \n",
      "     Training Step: 74 Training Loss: 2.494840383529663 \n",
      "     Training Step: 75 Training Loss: 3.143540859222412 \n",
      "     Training Step: 76 Training Loss: 2.805475950241089 \n",
      "     Training Step: 77 Training Loss: 2.3246989250183105 \n",
      "     Training Step: 78 Training Loss: 3.698075294494629 \n",
      "     Training Step: 79 Training Loss: 2.8909494876861572 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.2078216075897217 \n",
      "     Validation Step: 1 Validation Loss: 3.688000440597534 \n",
      "     Validation Step: 2 Validation Loss: 3.3806185722351074 \n",
      "     Validation Step: 3 Validation Loss: 3.784027099609375 \n",
      "     Validation Step: 4 Validation Loss: 3.018496036529541 \n",
      "     Validation Step: 5 Validation Loss: 3.89376163482666 \n",
      "     Validation Step: 6 Validation Loss: 2.74837589263916 \n",
      "     Validation Step: 7 Validation Loss: 3.5197973251342773 \n",
      "     Validation Step: 8 Validation Loss: 3.6542398929595947 \n",
      "     Validation Step: 9 Validation Loss: 2.7580032348632812 \n",
      "     Validation Step: 10 Validation Loss: 3.0644168853759766 \n",
      "     Validation Step: 11 Validation Loss: 3.2672390937805176 \n",
      "     Validation Step: 12 Validation Loss: 2.136946678161621 \n",
      "     Validation Step: 13 Validation Loss: 3.1080169677734375 \n",
      "     Validation Step: 14 Validation Loss: 3.1324942111968994 \n",
      "Epoch: 88\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.7150704860687256 \n",
      "     Training Step: 1 Training Loss: 3.4704365730285645 \n",
      "     Training Step: 2 Training Loss: 2.576143980026245 \n",
      "     Training Step: 3 Training Loss: 2.330226421356201 \n",
      "     Training Step: 4 Training Loss: 3.001173496246338 \n",
      "     Training Step: 5 Training Loss: 3.1167964935302734 \n",
      "     Training Step: 6 Training Loss: 2.726775646209717 \n",
      "     Training Step: 7 Training Loss: 3.069571018218994 \n",
      "     Training Step: 8 Training Loss: 2.7892210483551025 \n",
      "     Training Step: 9 Training Loss: 2.971072196960449 \n",
      "     Training Step: 10 Training Loss: 2.3231260776519775 \n",
      "     Training Step: 11 Training Loss: 2.1011996269226074 \n",
      "     Training Step: 12 Training Loss: 2.891474723815918 \n",
      "     Training Step: 13 Training Loss: 4.694244861602783 \n",
      "     Training Step: 14 Training Loss: 3.0829498767852783 \n",
      "     Training Step: 15 Training Loss: 2.213449001312256 \n",
      "     Training Step: 16 Training Loss: 3.487527370452881 \n",
      "     Training Step: 17 Training Loss: 2.4228453636169434 \n",
      "     Training Step: 18 Training Loss: 2.593773365020752 \n",
      "     Training Step: 19 Training Loss: 2.912029266357422 \n",
      "     Training Step: 20 Training Loss: 3.3726792335510254 \n",
      "     Training Step: 21 Training Loss: 2.7822117805480957 \n",
      "     Training Step: 22 Training Loss: 2.379072427749634 \n",
      "     Training Step: 23 Training Loss: 3.4669113159179688 \n",
      "     Training Step: 24 Training Loss: 2.086198329925537 \n",
      "     Training Step: 25 Training Loss: 3.8421053886413574 \n",
      "     Training Step: 26 Training Loss: 3.4474070072174072 \n",
      "     Training Step: 27 Training Loss: 3.111692428588867 \n",
      "     Training Step: 28 Training Loss: 3.1604197025299072 \n",
      "     Training Step: 29 Training Loss: 3.7488393783569336 \n",
      "     Training Step: 30 Training Loss: 3.819148063659668 \n",
      "     Training Step: 31 Training Loss: 2.741431713104248 \n",
      "     Training Step: 32 Training Loss: 2.752406597137451 \n",
      "     Training Step: 33 Training Loss: 2.6067559719085693 \n",
      "     Training Step: 34 Training Loss: 3.4501752853393555 \n",
      "     Training Step: 35 Training Loss: 2.2457194328308105 \n",
      "     Training Step: 36 Training Loss: 2.016894817352295 \n",
      "     Training Step: 37 Training Loss: 2.756019115447998 \n",
      "     Training Step: 38 Training Loss: 3.419651985168457 \n",
      "     Training Step: 39 Training Loss: 4.317755222320557 \n",
      "     Training Step: 40 Training Loss: 3.2624218463897705 \n",
      "     Training Step: 41 Training Loss: 2.2272658348083496 \n",
      "     Training Step: 42 Training Loss: 2.3267390727996826 \n",
      "     Training Step: 43 Training Loss: 2.5512993335723877 \n",
      "     Training Step: 44 Training Loss: 4.274260997772217 \n",
      "     Training Step: 45 Training Loss: 3.1310625076293945 \n",
      "     Training Step: 46 Training Loss: 2.2463128566741943 \n",
      "     Training Step: 47 Training Loss: 2.3484554290771484 \n",
      "     Training Step: 48 Training Loss: 2.91024112701416 \n",
      "     Training Step: 49 Training Loss: 2.6797356605529785 \n",
      "     Training Step: 50 Training Loss: 3.3492093086242676 \n",
      "     Training Step: 51 Training Loss: 3.415419578552246 \n",
      "     Training Step: 52 Training Loss: 3.3697328567504883 \n",
      "     Training Step: 53 Training Loss: 3.7331180572509766 \n",
      "     Training Step: 54 Training Loss: 4.126865863800049 \n",
      "     Training Step: 55 Training Loss: 3.0637869834899902 \n",
      "     Training Step: 56 Training Loss: 3.061859130859375 \n",
      "     Training Step: 57 Training Loss: 3.016322612762451 \n",
      "     Training Step: 58 Training Loss: 2.108281373977661 \n",
      "     Training Step: 59 Training Loss: 2.480276107788086 \n",
      "     Training Step: 60 Training Loss: 3.2515711784362793 \n",
      "     Training Step: 61 Training Loss: 3.14566969871521 \n",
      "     Training Step: 62 Training Loss: 3.2411980628967285 \n",
      "     Training Step: 63 Training Loss: 2.517873764038086 \n",
      "     Training Step: 64 Training Loss: 2.40559983253479 \n",
      "     Training Step: 65 Training Loss: 2.626087188720703 \n",
      "     Training Step: 66 Training Loss: 2.9322268962860107 \n",
      "     Training Step: 67 Training Loss: 3.6437370777130127 \n",
      "     Training Step: 68 Training Loss: 2.42643404006958 \n",
      "     Training Step: 69 Training Loss: 3.318863868713379 \n",
      "     Training Step: 70 Training Loss: 3.086365222930908 \n",
      "     Training Step: 71 Training Loss: 2.748363971710205 \n",
      "     Training Step: 72 Training Loss: 2.247922420501709 \n",
      "     Training Step: 73 Training Loss: 3.2798662185668945 \n",
      "     Training Step: 74 Training Loss: 2.886857271194458 \n",
      "     Training Step: 75 Training Loss: 4.321902275085449 \n",
      "     Training Step: 76 Training Loss: 2.7416975498199463 \n",
      "     Training Step: 77 Training Loss: 3.0019726753234863 \n",
      "     Training Step: 78 Training Loss: 3.4357175827026367 \n",
      "     Training Step: 79 Training Loss: 2.4774856567382812 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.545858860015869 \n",
      "     Validation Step: 1 Validation Loss: 3.2608470916748047 \n",
      "     Validation Step: 2 Validation Loss: 2.922305107116699 \n",
      "     Validation Step: 3 Validation Loss: 2.629533290863037 \n",
      "     Validation Step: 4 Validation Loss: 2.6337528228759766 \n",
      "     Validation Step: 5 Validation Loss: 2.236945152282715 \n",
      "     Validation Step: 6 Validation Loss: 3.531165838241577 \n",
      "     Validation Step: 7 Validation Loss: 3.4877564907073975 \n",
      "     Validation Step: 8 Validation Loss: 3.1007323265075684 \n",
      "     Validation Step: 9 Validation Loss: 3.6592249870300293 \n",
      "     Validation Step: 10 Validation Loss: 2.9570930004119873 \n",
      "     Validation Step: 11 Validation Loss: 3.7629802227020264 \n",
      "     Validation Step: 12 Validation Loss: 3.597275733947754 \n",
      "     Validation Step: 13 Validation Loss: 3.0210633277893066 \n",
      "     Validation Step: 14 Validation Loss: 2.8538804054260254 \n",
      "Epoch: 89\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.4896774291992188 \n",
      "     Training Step: 1 Training Loss: 3.4474825859069824 \n",
      "     Training Step: 2 Training Loss: 2.7035231590270996 \n",
      "     Training Step: 3 Training Loss: 2.2351791858673096 \n",
      "     Training Step: 4 Training Loss: 2.9535393714904785 \n",
      "     Training Step: 5 Training Loss: 4.231462001800537 \n",
      "     Training Step: 6 Training Loss: 2.5712900161743164 \n",
      "     Training Step: 7 Training Loss: 2.700444221496582 \n",
      "     Training Step: 8 Training Loss: 3.4919145107269287 \n",
      "     Training Step: 9 Training Loss: 2.5210938453674316 \n",
      "     Training Step: 10 Training Loss: 2.7468056678771973 \n",
      "     Training Step: 11 Training Loss: 3.7284929752349854 \n",
      "     Training Step: 12 Training Loss: 3.133463144302368 \n",
      "     Training Step: 13 Training Loss: 2.4511566162109375 \n",
      "     Training Step: 14 Training Loss: 3.9244422912597656 \n",
      "     Training Step: 15 Training Loss: 3.5796947479248047 \n",
      "     Training Step: 16 Training Loss: 3.3360953330993652 \n",
      "     Training Step: 17 Training Loss: 2.3276681900024414 \n",
      "     Training Step: 18 Training Loss: 3.580852508544922 \n",
      "     Training Step: 19 Training Loss: 2.879276752471924 \n",
      "     Training Step: 20 Training Loss: 2.268873929977417 \n",
      "     Training Step: 21 Training Loss: 2.5897746086120605 \n",
      "     Training Step: 22 Training Loss: 3.54268741607666 \n",
      "     Training Step: 23 Training Loss: 3.0259134769439697 \n",
      "     Training Step: 24 Training Loss: 2.922410011291504 \n",
      "     Training Step: 25 Training Loss: 2.986604690551758 \n",
      "     Training Step: 26 Training Loss: 2.295498847961426 \n",
      "     Training Step: 27 Training Loss: 2.399272918701172 \n",
      "     Training Step: 28 Training Loss: 2.8694820404052734 \n",
      "     Training Step: 29 Training Loss: 3.2559471130371094 \n",
      "     Training Step: 30 Training Loss: 3.050182580947876 \n",
      "     Training Step: 31 Training Loss: 3.246593952178955 \n",
      "     Training Step: 32 Training Loss: 3.2903506755828857 \n",
      "     Training Step: 33 Training Loss: 2.8741092681884766 \n",
      "     Training Step: 34 Training Loss: 2.3991923332214355 \n",
      "     Training Step: 35 Training Loss: 3.6318554878234863 \n",
      "     Training Step: 36 Training Loss: 3.4287867546081543 \n",
      "     Training Step: 37 Training Loss: 2.7999684810638428 \n",
      "     Training Step: 38 Training Loss: 2.8091044425964355 \n",
      "     Training Step: 39 Training Loss: 2.8794472217559814 \n",
      "     Training Step: 40 Training Loss: 2.022498607635498 \n",
      "     Training Step: 41 Training Loss: 2.6528162956237793 \n",
      "     Training Step: 42 Training Loss: 2.387523651123047 \n",
      "     Training Step: 43 Training Loss: 3.087002992630005 \n",
      "     Training Step: 44 Training Loss: 2.2946383953094482 \n",
      "     Training Step: 45 Training Loss: 3.6076064109802246 \n",
      "     Training Step: 46 Training Loss: 3.094944715499878 \n",
      "     Training Step: 47 Training Loss: 2.084029197692871 \n",
      "     Training Step: 48 Training Loss: 3.957095146179199 \n",
      "     Training Step: 49 Training Loss: 2.837617874145508 \n",
      "     Training Step: 50 Training Loss: 2.1626181602478027 \n",
      "     Training Step: 51 Training Loss: 2.6228318214416504 \n",
      "     Training Step: 52 Training Loss: 3.639329433441162 \n",
      "     Training Step: 53 Training Loss: 2.461021661758423 \n",
      "     Training Step: 54 Training Loss: 2.4162817001342773 \n",
      "     Training Step: 55 Training Loss: 2.9307010173797607 \n",
      "     Training Step: 56 Training Loss: 2.982326030731201 \n",
      "     Training Step: 57 Training Loss: 3.4354066848754883 \n",
      "     Training Step: 58 Training Loss: 2.604248046875 \n",
      "     Training Step: 59 Training Loss: 3.8915185928344727 \n",
      "     Training Step: 60 Training Loss: 2.7444143295288086 \n",
      "     Training Step: 61 Training Loss: 4.150150299072266 \n",
      "     Training Step: 62 Training Loss: 3.0524516105651855 \n",
      "     Training Step: 63 Training Loss: 3.0738461017608643 \n",
      "     Training Step: 64 Training Loss: 3.728804349899292 \n",
      "     Training Step: 65 Training Loss: 4.301446437835693 \n",
      "     Training Step: 66 Training Loss: 2.3892955780029297 \n",
      "     Training Step: 67 Training Loss: 4.970714569091797 \n",
      "     Training Step: 68 Training Loss: 2.3421578407287598 \n",
      "     Training Step: 69 Training Loss: 3.1208295822143555 \n",
      "     Training Step: 70 Training Loss: 2.8445889949798584 \n",
      "     Training Step: 71 Training Loss: 2.710920572280884 \n",
      "     Training Step: 72 Training Loss: 2.30183744430542 \n",
      "     Training Step: 73 Training Loss: 2.4997925758361816 \n",
      "     Training Step: 74 Training Loss: 2.20389986038208 \n",
      "     Training Step: 75 Training Loss: 4.370721340179443 \n",
      "     Training Step: 76 Training Loss: 2.613894462585449 \n",
      "     Training Step: 77 Training Loss: 2.4739809036254883 \n",
      "     Training Step: 78 Training Loss: 3.8901612758636475 \n",
      "     Training Step: 79 Training Loss: 3.2167181968688965 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.3656461238861084 \n",
      "     Validation Step: 1 Validation Loss: 3.8279683589935303 \n",
      "     Validation Step: 2 Validation Loss: 3.5350427627563477 \n",
      "     Validation Step: 3 Validation Loss: 3.570497512817383 \n",
      "     Validation Step: 4 Validation Loss: 3.2054190635681152 \n",
      "     Validation Step: 5 Validation Loss: 3.0311803817749023 \n",
      "     Validation Step: 6 Validation Loss: 3.067458391189575 \n",
      "     Validation Step: 7 Validation Loss: 3.0877323150634766 \n",
      "     Validation Step: 8 Validation Loss: 2.734236240386963 \n",
      "     Validation Step: 9 Validation Loss: 3.636996269226074 \n",
      "     Validation Step: 10 Validation Loss: 3.7235381603240967 \n",
      "     Validation Step: 11 Validation Loss: 2.725142478942871 \n",
      "     Validation Step: 12 Validation Loss: 3.648669719696045 \n",
      "     Validation Step: 13 Validation Loss: 2.8229589462280273 \n",
      "     Validation Step: 14 Validation Loss: 2.9804320335388184 \n",
      "Epoch: 90\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.4094128608703613 \n",
      "     Training Step: 1 Training Loss: 3.048114776611328 \n",
      "     Training Step: 2 Training Loss: 2.137676477432251 \n",
      "     Training Step: 3 Training Loss: 2.775991678237915 \n",
      "     Training Step: 4 Training Loss: 2.2191545963287354 \n",
      "     Training Step: 5 Training Loss: 3.197131872177124 \n",
      "     Training Step: 6 Training Loss: 2.8422505855560303 \n",
      "     Training Step: 7 Training Loss: 2.3635597229003906 \n",
      "     Training Step: 8 Training Loss: 3.0276365280151367 \n",
      "     Training Step: 9 Training Loss: 2.2365708351135254 \n",
      "     Training Step: 10 Training Loss: 4.224048614501953 \n",
      "     Training Step: 11 Training Loss: 3.2518157958984375 \n",
      "     Training Step: 12 Training Loss: 2.1617298126220703 \n",
      "     Training Step: 13 Training Loss: 3.3616716861724854 \n",
      "     Training Step: 14 Training Loss: 3.402005910873413 \n",
      "     Training Step: 15 Training Loss: 2.6649386882781982 \n",
      "     Training Step: 16 Training Loss: 2.4457342624664307 \n",
      "     Training Step: 17 Training Loss: 2.5285842418670654 \n",
      "     Training Step: 18 Training Loss: 2.776412010192871 \n",
      "     Training Step: 19 Training Loss: 3.419966697692871 \n",
      "     Training Step: 20 Training Loss: 4.165882587432861 \n",
      "     Training Step: 21 Training Loss: 2.365170955657959 \n",
      "     Training Step: 22 Training Loss: 4.261285781860352 \n",
      "     Training Step: 23 Training Loss: 2.1185760498046875 \n",
      "     Training Step: 24 Training Loss: 3.342085838317871 \n",
      "     Training Step: 25 Training Loss: 2.168840169906616 \n",
      "     Training Step: 26 Training Loss: 4.664874076843262 \n",
      "     Training Step: 27 Training Loss: 3.133126735687256 \n",
      "     Training Step: 28 Training Loss: 2.880192756652832 \n",
      "     Training Step: 29 Training Loss: 2.8660597801208496 \n",
      "     Training Step: 30 Training Loss: 2.5050578117370605 \n",
      "     Training Step: 31 Training Loss: 2.2820606231689453 \n",
      "     Training Step: 32 Training Loss: 3.5859932899475098 \n",
      "     Training Step: 33 Training Loss: 3.575594425201416 \n",
      "     Training Step: 34 Training Loss: 2.984313488006592 \n",
      "     Training Step: 35 Training Loss: 2.9088973999023438 \n",
      "     Training Step: 36 Training Loss: 2.6866683959960938 \n",
      "     Training Step: 37 Training Loss: 2.994209051132202 \n",
      "     Training Step: 38 Training Loss: 2.844332456588745 \n",
      "     Training Step: 39 Training Loss: 3.6513352394104004 \n",
      "     Training Step: 40 Training Loss: 3.6788296699523926 \n",
      "     Training Step: 41 Training Loss: 3.586456775665283 \n",
      "     Training Step: 42 Training Loss: 3.3184475898742676 \n",
      "     Training Step: 43 Training Loss: 3.035761833190918 \n",
      "     Training Step: 44 Training Loss: 2.777871608734131 \n",
      "     Training Step: 45 Training Loss: 3.0831613540649414 \n",
      "     Training Step: 46 Training Loss: 2.8974204063415527 \n",
      "     Training Step: 47 Training Loss: 4.020031929016113 \n",
      "     Training Step: 48 Training Loss: 2.2223153114318848 \n",
      "     Training Step: 49 Training Loss: 3.4182662963867188 \n",
      "     Training Step: 50 Training Loss: 3.397616147994995 \n",
      "     Training Step: 51 Training Loss: 3.2123894691467285 \n",
      "     Training Step: 52 Training Loss: 3.9827446937561035 \n",
      "     Training Step: 53 Training Loss: 2.9330763816833496 \n",
      "     Training Step: 54 Training Loss: 2.8129820823669434 \n",
      "     Training Step: 55 Training Loss: 3.117760419845581 \n",
      "     Training Step: 56 Training Loss: 2.3970704078674316 \n",
      "     Training Step: 57 Training Loss: 2.1628010272979736 \n",
      "     Training Step: 58 Training Loss: 2.5652554035186768 \n",
      "     Training Step: 59 Training Loss: 3.4399328231811523 \n",
      "     Training Step: 60 Training Loss: 2.856962203979492 \n",
      "     Training Step: 61 Training Loss: 2.69626522064209 \n",
      "     Training Step: 62 Training Loss: 3.5806102752685547 \n",
      "     Training Step: 63 Training Loss: 3.666116952896118 \n",
      "     Training Step: 64 Training Loss: 2.7009310722351074 \n",
      "     Training Step: 65 Training Loss: 2.6141676902770996 \n",
      "     Training Step: 66 Training Loss: 2.8449771404266357 \n",
      "     Training Step: 67 Training Loss: 2.692082166671753 \n",
      "     Training Step: 68 Training Loss: 3.338902711868286 \n",
      "     Training Step: 69 Training Loss: 2.452305793762207 \n",
      "     Training Step: 70 Training Loss: 2.737590789794922 \n",
      "     Training Step: 71 Training Loss: 2.5259454250335693 \n",
      "     Training Step: 72 Training Loss: 3.2756340503692627 \n",
      "     Training Step: 73 Training Loss: 2.9825754165649414 \n",
      "     Training Step: 74 Training Loss: 2.6073455810546875 \n",
      "     Training Step: 75 Training Loss: 3.455657958984375 \n",
      "     Training Step: 76 Training Loss: 3.249573230743408 \n",
      "     Training Step: 77 Training Loss: 2.7145633697509766 \n",
      "     Training Step: 78 Training Loss: 3.8696792125701904 \n",
      "     Training Step: 79 Training Loss: 2.660336494445801 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1312599182128906 \n",
      "     Validation Step: 1 Validation Loss: 2.910292625427246 \n",
      "     Validation Step: 2 Validation Loss: 3.200751304626465 \n",
      "     Validation Step: 3 Validation Loss: 3.698988914489746 \n",
      "     Validation Step: 4 Validation Loss: 3.6525702476501465 \n",
      "     Validation Step: 5 Validation Loss: 3.613636016845703 \n",
      "     Validation Step: 6 Validation Loss: 2.7382125854492188 \n",
      "     Validation Step: 7 Validation Loss: 3.3476197719573975 \n",
      "     Validation Step: 8 Validation Loss: 3.201974868774414 \n",
      "     Validation Step: 9 Validation Loss: 3.9522387981414795 \n",
      "     Validation Step: 10 Validation Loss: 2.908823013305664 \n",
      "     Validation Step: 11 Validation Loss: 3.649005174636841 \n",
      "     Validation Step: 12 Validation Loss: 3.0539817810058594 \n",
      "     Validation Step: 13 Validation Loss: 2.235006809234619 \n",
      "     Validation Step: 14 Validation Loss: 3.140752077102661 \n",
      "Epoch: 91\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.029672145843506 \n",
      "     Training Step: 1 Training Loss: 3.2876136302948 \n",
      "     Training Step: 2 Training Loss: 3.7960824966430664 \n",
      "     Training Step: 3 Training Loss: 2.4352777004241943 \n",
      "     Training Step: 4 Training Loss: 3.1337027549743652 \n",
      "     Training Step: 5 Training Loss: 3.245396375656128 \n",
      "     Training Step: 6 Training Loss: 2.377049207687378 \n",
      "     Training Step: 7 Training Loss: 2.7101686000823975 \n",
      "     Training Step: 8 Training Loss: 2.738079071044922 \n",
      "     Training Step: 9 Training Loss: 2.7537598609924316 \n",
      "     Training Step: 10 Training Loss: 2.070563316345215 \n",
      "     Training Step: 11 Training Loss: 2.1815764904022217 \n",
      "     Training Step: 12 Training Loss: 3.5093390941619873 \n",
      "     Training Step: 13 Training Loss: 3.07993221282959 \n",
      "     Training Step: 14 Training Loss: 3.0622448921203613 \n",
      "     Training Step: 15 Training Loss: 3.5635266304016113 \n",
      "     Training Step: 16 Training Loss: 3.5513885021209717 \n",
      "     Training Step: 17 Training Loss: 3.462193489074707 \n",
      "     Training Step: 18 Training Loss: 3.8501667976379395 \n",
      "     Training Step: 19 Training Loss: 2.8139073848724365 \n",
      "     Training Step: 20 Training Loss: 3.805854320526123 \n",
      "     Training Step: 21 Training Loss: 4.01821231842041 \n",
      "     Training Step: 22 Training Loss: 3.0212039947509766 \n",
      "     Training Step: 23 Training Loss: 3.470486640930176 \n",
      "     Training Step: 24 Training Loss: 2.297631025314331 \n",
      "     Training Step: 25 Training Loss: 3.3223819732666016 \n",
      "     Training Step: 26 Training Loss: 3.2508084774017334 \n",
      "     Training Step: 27 Training Loss: 2.197683811187744 \n",
      "     Training Step: 28 Training Loss: 2.5467026233673096 \n",
      "     Training Step: 29 Training Loss: 3.2455873489379883 \n",
      "     Training Step: 30 Training Loss: 3.101336717605591 \n",
      "     Training Step: 31 Training Loss: 3.7264842987060547 \n",
      "     Training Step: 32 Training Loss: 2.6748363971710205 \n",
      "     Training Step: 33 Training Loss: 2.9894895553588867 \n",
      "     Training Step: 34 Training Loss: 2.205472469329834 \n",
      "     Training Step: 35 Training Loss: 4.200348854064941 \n",
      "     Training Step: 36 Training Loss: 2.4944310188293457 \n",
      "     Training Step: 37 Training Loss: 2.734151601791382 \n",
      "     Training Step: 38 Training Loss: 3.216517448425293 \n",
      "     Training Step: 39 Training Loss: 2.5204596519470215 \n",
      "     Training Step: 40 Training Loss: 3.1186957359313965 \n",
      "     Training Step: 41 Training Loss: 3.953965187072754 \n",
      "     Training Step: 42 Training Loss: 2.342527151107788 \n",
      "     Training Step: 43 Training Loss: 2.475182056427002 \n",
      "     Training Step: 44 Training Loss: 2.5645642280578613 \n",
      "     Training Step: 45 Training Loss: 2.6347603797912598 \n",
      "     Training Step: 46 Training Loss: 3.364511013031006 \n",
      "     Training Step: 47 Training Loss: 3.3551902770996094 \n",
      "     Training Step: 48 Training Loss: 3.19913911819458 \n",
      "     Training Step: 49 Training Loss: 2.5226945877075195 \n",
      "     Training Step: 50 Training Loss: 3.4496641159057617 \n",
      "     Training Step: 51 Training Loss: 2.2533090114593506 \n",
      "     Training Step: 52 Training Loss: 2.3915600776672363 \n",
      "     Training Step: 53 Training Loss: 2.857853889465332 \n",
      "     Training Step: 54 Training Loss: 2.515749931335449 \n",
      "     Training Step: 55 Training Loss: 3.073017120361328 \n",
      "     Training Step: 56 Training Loss: 2.8472301959991455 \n",
      "     Training Step: 57 Training Loss: 2.9923501014709473 \n",
      "     Training Step: 58 Training Loss: 3.4097211360931396 \n",
      "     Training Step: 59 Training Loss: 2.4780004024505615 \n",
      "     Training Step: 60 Training Loss: 3.204059362411499 \n",
      "     Training Step: 61 Training Loss: 2.2219786643981934 \n",
      "     Training Step: 62 Training Loss: 2.7445623874664307 \n",
      "     Training Step: 63 Training Loss: 2.7366490364074707 \n",
      "     Training Step: 64 Training Loss: 3.1149532794952393 \n",
      "     Training Step: 65 Training Loss: 4.407847881317139 \n",
      "     Training Step: 66 Training Loss: 3.253920078277588 \n",
      "     Training Step: 67 Training Loss: 2.9683828353881836 \n",
      "     Training Step: 68 Training Loss: 2.7156312465667725 \n",
      "     Training Step: 69 Training Loss: 4.861326217651367 \n",
      "     Training Step: 70 Training Loss: 2.391031503677368 \n",
      "     Training Step: 71 Training Loss: 2.294001579284668 \n",
      "     Training Step: 72 Training Loss: 3.4922871589660645 \n",
      "     Training Step: 73 Training Loss: 3.081552028656006 \n",
      "     Training Step: 74 Training Loss: 2.644550323486328 \n",
      "     Training Step: 75 Training Loss: 2.4502274990081787 \n",
      "     Training Step: 76 Training Loss: 2.5879578590393066 \n",
      "     Training Step: 77 Training Loss: 3.1472907066345215 \n",
      "     Training Step: 78 Training Loss: 3.388946533203125 \n",
      "     Training Step: 79 Training Loss: 2.050861358642578 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1239171028137207 \n",
      "     Validation Step: 1 Validation Loss: 3.730982780456543 \n",
      "     Validation Step: 2 Validation Loss: 2.1557955741882324 \n",
      "     Validation Step: 3 Validation Loss: 3.0776944160461426 \n",
      "     Validation Step: 4 Validation Loss: 3.6106858253479004 \n",
      "     Validation Step: 5 Validation Loss: 2.7023298740386963 \n",
      "     Validation Step: 6 Validation Loss: 3.895204782485962 \n",
      "     Validation Step: 7 Validation Loss: 3.1668126583099365 \n",
      "     Validation Step: 8 Validation Loss: 3.1383001804351807 \n",
      "     Validation Step: 9 Validation Loss: 2.969607353210449 \n",
      "     Validation Step: 10 Validation Loss: 3.2103636264801025 \n",
      "     Validation Step: 11 Validation Loss: 3.5771279335021973 \n",
      "     Validation Step: 12 Validation Loss: 2.763946771621704 \n",
      "     Validation Step: 13 Validation Loss: 3.36226749420166 \n",
      "     Validation Step: 14 Validation Loss: 3.627530574798584 \n",
      "Epoch: 92\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.144289255142212 \n",
      "     Training Step: 1 Training Loss: 3.51198673248291 \n",
      "     Training Step: 2 Training Loss: 2.522665023803711 \n",
      "     Training Step: 3 Training Loss: 2.771652936935425 \n",
      "     Training Step: 4 Training Loss: 2.151346206665039 \n",
      "     Training Step: 5 Training Loss: 2.820138454437256 \n",
      "     Training Step: 6 Training Loss: 2.7578189373016357 \n",
      "     Training Step: 7 Training Loss: 2.7575597763061523 \n",
      "     Training Step: 8 Training Loss: 4.801661491394043 \n",
      "     Training Step: 9 Training Loss: 2.7499523162841797 \n",
      "     Training Step: 10 Training Loss: 2.295717716217041 \n",
      "     Training Step: 11 Training Loss: 2.4068727493286133 \n",
      "     Training Step: 12 Training Loss: 2.478588581085205 \n",
      "     Training Step: 13 Training Loss: 3.0789248943328857 \n",
      "     Training Step: 14 Training Loss: 2.97831392288208 \n",
      "     Training Step: 15 Training Loss: 2.8641278743743896 \n",
      "     Training Step: 16 Training Loss: 2.543696403503418 \n",
      "     Training Step: 17 Training Loss: 4.2708258628845215 \n",
      "     Training Step: 18 Training Loss: 3.6369104385375977 \n",
      "     Training Step: 19 Training Loss: 2.6657519340515137 \n",
      "     Training Step: 20 Training Loss: 2.6176087856292725 \n",
      "     Training Step: 21 Training Loss: 3.137373208999634 \n",
      "     Training Step: 22 Training Loss: 3.1721138954162598 \n",
      "     Training Step: 23 Training Loss: 2.784409999847412 \n",
      "     Training Step: 24 Training Loss: 3.0956602096557617 \n",
      "     Training Step: 25 Training Loss: 3.507070541381836 \n",
      "     Training Step: 26 Training Loss: 3.442574977874756 \n",
      "     Training Step: 27 Training Loss: 2.5795862674713135 \n",
      "     Training Step: 28 Training Loss: 3.419154167175293 \n",
      "     Training Step: 29 Training Loss: 2.391674041748047 \n",
      "     Training Step: 30 Training Loss: 2.373014450073242 \n",
      "     Training Step: 31 Training Loss: 3.870110511779785 \n",
      "     Training Step: 32 Training Loss: 2.9875361919403076 \n",
      "     Training Step: 33 Training Loss: 3.8568296432495117 \n",
      "     Training Step: 34 Training Loss: 2.5918331146240234 \n",
      "     Training Step: 35 Training Loss: 3.5127930641174316 \n",
      "     Training Step: 36 Training Loss: 3.2652487754821777 \n",
      "     Training Step: 37 Training Loss: 3.1832923889160156 \n",
      "     Training Step: 38 Training Loss: 2.9729530811309814 \n",
      "     Training Step: 39 Training Loss: 2.0757017135620117 \n",
      "     Training Step: 40 Training Loss: 2.3455281257629395 \n",
      "     Training Step: 41 Training Loss: 2.76314115524292 \n",
      "     Training Step: 42 Training Loss: 3.0752780437469482 \n",
      "     Training Step: 43 Training Loss: 3.5174241065979004 \n",
      "     Training Step: 44 Training Loss: 2.2107551097869873 \n",
      "     Training Step: 45 Training Loss: 3.7393908500671387 \n",
      "     Training Step: 46 Training Loss: 2.9216079711914062 \n",
      "     Training Step: 47 Training Loss: 3.423491954803467 \n",
      "     Training Step: 48 Training Loss: 3.7606401443481445 \n",
      "     Training Step: 49 Training Loss: 2.1467418670654297 \n",
      "     Training Step: 50 Training Loss: 3.2660720348358154 \n",
      "     Training Step: 51 Training Loss: 3.4552149772644043 \n",
      "     Training Step: 52 Training Loss: 3.383545398712158 \n",
      "     Training Step: 53 Training Loss: 2.856365203857422 \n",
      "     Training Step: 54 Training Loss: 2.0971035957336426 \n",
      "     Training Step: 55 Training Loss: 4.255249500274658 \n",
      "     Training Step: 56 Training Loss: 3.6452908515930176 \n",
      "     Training Step: 57 Training Loss: 2.912602424621582 \n",
      "     Training Step: 58 Training Loss: 3.4586377143859863 \n",
      "     Training Step: 59 Training Loss: 3.035710573196411 \n",
      "     Training Step: 60 Training Loss: 3.03918194770813 \n",
      "     Training Step: 61 Training Loss: 2.956879138946533 \n",
      "     Training Step: 62 Training Loss: 2.9084155559539795 \n",
      "     Training Step: 63 Training Loss: 2.5603485107421875 \n",
      "     Training Step: 64 Training Loss: 2.386723518371582 \n",
      "     Training Step: 65 Training Loss: 3.0747532844543457 \n",
      "     Training Step: 66 Training Loss: 3.7679994106292725 \n",
      "     Training Step: 67 Training Loss: 3.1243739128112793 \n",
      "     Training Step: 68 Training Loss: 2.5502262115478516 \n",
      "     Training Step: 69 Training Loss: 2.484179735183716 \n",
      "     Training Step: 70 Training Loss: 3.082000255584717 \n",
      "     Training Step: 71 Training Loss: 4.414720058441162 \n",
      "     Training Step: 72 Training Loss: 3.4877452850341797 \n",
      "     Training Step: 73 Training Loss: 3.921194553375244 \n",
      "     Training Step: 74 Training Loss: 3.0089588165283203 \n",
      "     Training Step: 75 Training Loss: 3.074615478515625 \n",
      "     Training Step: 76 Training Loss: 2.9552552700042725 \n",
      "     Training Step: 77 Training Loss: 3.334071159362793 \n",
      "     Training Step: 78 Training Loss: 2.0345211029052734 \n",
      "     Training Step: 79 Training Loss: 2.5237512588500977 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.2679333686828613 \n",
      "     Validation Step: 1 Validation Loss: 2.8829739093780518 \n",
      "     Validation Step: 2 Validation Loss: 2.9843480587005615 \n",
      "     Validation Step: 3 Validation Loss: 3.3269217014312744 \n",
      "     Validation Step: 4 Validation Loss: 3.7905521392822266 \n",
      "     Validation Step: 5 Validation Loss: 3.7847371101379395 \n",
      "     Validation Step: 6 Validation Loss: 3.117025136947632 \n",
      "     Validation Step: 7 Validation Loss: 3.5869083404541016 \n",
      "     Validation Step: 8 Validation Loss: 2.8452723026275635 \n",
      "     Validation Step: 9 Validation Loss: 3.0112905502319336 \n",
      "     Validation Step: 10 Validation Loss: 3.1604223251342773 \n",
      "     Validation Step: 11 Validation Loss: 3.6508336067199707 \n",
      "     Validation Step: 12 Validation Loss: 2.188167095184326 \n",
      "     Validation Step: 13 Validation Loss: 3.1145284175872803 \n",
      "     Validation Step: 14 Validation Loss: 3.9410974979400635 \n",
      "Epoch: 93\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.0471529960632324 \n",
      "     Training Step: 1 Training Loss: 2.983989715576172 \n",
      "     Training Step: 2 Training Loss: 2.4976611137390137 \n",
      "     Training Step: 3 Training Loss: 2.798600196838379 \n",
      "     Training Step: 4 Training Loss: 2.63261079788208 \n",
      "     Training Step: 5 Training Loss: 3.7388672828674316 \n",
      "     Training Step: 6 Training Loss: 3.4831905364990234 \n",
      "     Training Step: 7 Training Loss: 3.9031267166137695 \n",
      "     Training Step: 8 Training Loss: 3.754131317138672 \n",
      "     Training Step: 9 Training Loss: 2.219414710998535 \n",
      "     Training Step: 10 Training Loss: 2.3000073432922363 \n",
      "     Training Step: 11 Training Loss: 3.392242908477783 \n",
      "     Training Step: 12 Training Loss: 3.468581199645996 \n",
      "     Training Step: 13 Training Loss: 2.7719650268554688 \n",
      "     Training Step: 14 Training Loss: 2.6266613006591797 \n",
      "     Training Step: 15 Training Loss: 2.3738622665405273 \n",
      "     Training Step: 16 Training Loss: 3.091411590576172 \n",
      "     Training Step: 17 Training Loss: 2.7690205574035645 \n",
      "     Training Step: 18 Training Loss: 4.012923240661621 \n",
      "     Training Step: 19 Training Loss: 2.941812515258789 \n",
      "     Training Step: 20 Training Loss: 2.6749765872955322 \n",
      "     Training Step: 21 Training Loss: 2.8623995780944824 \n",
      "     Training Step: 22 Training Loss: 2.5986106395721436 \n",
      "     Training Step: 23 Training Loss: 2.503997802734375 \n",
      "     Training Step: 24 Training Loss: 2.599747657775879 \n",
      "     Training Step: 25 Training Loss: 2.4378957748413086 \n",
      "     Training Step: 26 Training Loss: 2.158644199371338 \n",
      "     Training Step: 27 Training Loss: 3.543285369873047 \n",
      "     Training Step: 28 Training Loss: 4.0711894035339355 \n",
      "     Training Step: 29 Training Loss: 3.3700294494628906 \n",
      "     Training Step: 30 Training Loss: 4.6336259841918945 \n",
      "     Training Step: 31 Training Loss: 3.3354527950286865 \n",
      "     Training Step: 32 Training Loss: 2.8491289615631104 \n",
      "     Training Step: 33 Training Loss: 3.518646240234375 \n",
      "     Training Step: 34 Training Loss: 3.1786370277404785 \n",
      "     Training Step: 35 Training Loss: 3.2034850120544434 \n",
      "     Training Step: 36 Training Loss: 3.330155372619629 \n",
      "     Training Step: 37 Training Loss: 2.236557960510254 \n",
      "     Training Step: 38 Training Loss: 3.6649527549743652 \n",
      "     Training Step: 39 Training Loss: 2.8270535469055176 \n",
      "     Training Step: 40 Training Loss: 2.935959815979004 \n",
      "     Training Step: 41 Training Loss: 2.1568174362182617 \n",
      "     Training Step: 42 Training Loss: 3.53836989402771 \n",
      "     Training Step: 43 Training Loss: 3.117283821105957 \n",
      "     Training Step: 44 Training Loss: 3.2995429039001465 \n",
      "     Training Step: 45 Training Loss: 2.6591944694519043 \n",
      "     Training Step: 46 Training Loss: 4.455589294433594 \n",
      "     Training Step: 47 Training Loss: 2.591536045074463 \n",
      "     Training Step: 48 Training Loss: 2.675752639770508 \n",
      "     Training Step: 49 Training Loss: 2.8767037391662598 \n",
      "     Training Step: 50 Training Loss: 2.9782843589782715 \n",
      "     Training Step: 51 Training Loss: 2.739121913909912 \n",
      "     Training Step: 52 Training Loss: 3.267369031906128 \n",
      "     Training Step: 53 Training Loss: 3.2163238525390625 \n",
      "     Training Step: 54 Training Loss: 2.3039259910583496 \n",
      "     Training Step: 55 Training Loss: 2.3013124465942383 \n",
      "     Training Step: 56 Training Loss: 2.3767664432525635 \n",
      "     Training Step: 57 Training Loss: 3.0753636360168457 \n",
      "     Training Step: 58 Training Loss: 4.376291751861572 \n",
      "     Training Step: 59 Training Loss: 2.3059329986572266 \n",
      "     Training Step: 60 Training Loss: 3.3048148155212402 \n",
      "     Training Step: 61 Training Loss: 2.602348804473877 \n",
      "     Training Step: 62 Training Loss: 2.8361477851867676 \n",
      "     Training Step: 63 Training Loss: 2.3813648223876953 \n",
      "     Training Step: 64 Training Loss: 2.870296001434326 \n",
      "     Training Step: 65 Training Loss: 2.9352962970733643 \n",
      "     Training Step: 66 Training Loss: 3.288163185119629 \n",
      "     Training Step: 67 Training Loss: 2.855195999145508 \n",
      "     Training Step: 68 Training Loss: 2.8288474082946777 \n",
      "     Training Step: 69 Training Loss: 2.258194923400879 \n",
      "     Training Step: 70 Training Loss: 2.8165688514709473 \n",
      "     Training Step: 71 Training Loss: 2.094914436340332 \n",
      "     Training Step: 72 Training Loss: 2.457568407058716 \n",
      "     Training Step: 73 Training Loss: 3.874701976776123 \n",
      "     Training Step: 74 Training Loss: 3.496281623840332 \n",
      "     Training Step: 75 Training Loss: 3.001044511795044 \n",
      "     Training Step: 76 Training Loss: 2.809234142303467 \n",
      "     Training Step: 77 Training Loss: 3.2202353477478027 \n",
      "     Training Step: 78 Training Loss: 3.0168557167053223 \n",
      "     Training Step: 79 Training Loss: 3.1127381324768066 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.077273368835449 \n",
      "     Validation Step: 1 Validation Loss: 3.0265166759490967 \n",
      "     Validation Step: 2 Validation Loss: 3.0707626342773438 \n",
      "     Validation Step: 3 Validation Loss: 3.3024797439575195 \n",
      "     Validation Step: 4 Validation Loss: 4.126578330993652 \n",
      "     Validation Step: 5 Validation Loss: 2.953603744506836 \n",
      "     Validation Step: 6 Validation Loss: 3.4864113330841064 \n",
      "     Validation Step: 7 Validation Loss: 2.2104203701019287 \n",
      "     Validation Step: 8 Validation Loss: 3.8009068965911865 \n",
      "     Validation Step: 9 Validation Loss: 3.213028907775879 \n",
      "     Validation Step: 10 Validation Loss: 3.8550689220428467 \n",
      "     Validation Step: 11 Validation Loss: 3.293156623840332 \n",
      "     Validation Step: 12 Validation Loss: 3.7870893478393555 \n",
      "     Validation Step: 13 Validation Loss: 3.138817310333252 \n",
      "     Validation Step: 14 Validation Loss: 3.0465493202209473 \n",
      "Epoch: 94\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.9670426845550537 \n",
      "     Training Step: 1 Training Loss: 2.7002851963043213 \n",
      "     Training Step: 2 Training Loss: 2.884587526321411 \n",
      "     Training Step: 3 Training Loss: 2.969695568084717 \n",
      "     Training Step: 4 Training Loss: 3.8109147548675537 \n",
      "     Training Step: 5 Training Loss: 3.292884349822998 \n",
      "     Training Step: 6 Training Loss: 2.957505464553833 \n",
      "     Training Step: 7 Training Loss: 2.7427570819854736 \n",
      "     Training Step: 8 Training Loss: 3.714500665664673 \n",
      "     Training Step: 9 Training Loss: 2.713592052459717 \n",
      "     Training Step: 10 Training Loss: 2.569615364074707 \n",
      "     Training Step: 11 Training Loss: 2.6112117767333984 \n",
      "     Training Step: 12 Training Loss: 2.598848581314087 \n",
      "     Training Step: 13 Training Loss: 3.5122106075286865 \n",
      "     Training Step: 14 Training Loss: 3.274158477783203 \n",
      "     Training Step: 15 Training Loss: 3.510654926300049 \n",
      "     Training Step: 16 Training Loss: 2.197030544281006 \n",
      "     Training Step: 17 Training Loss: 2.8900578022003174 \n",
      "     Training Step: 18 Training Loss: 2.5531387329101562 \n",
      "     Training Step: 19 Training Loss: 2.671408176422119 \n",
      "     Training Step: 20 Training Loss: 2.9169771671295166 \n",
      "     Training Step: 21 Training Loss: 4.254810810089111 \n",
      "     Training Step: 22 Training Loss: 3.578528881072998 \n",
      "     Training Step: 23 Training Loss: 3.5517826080322266 \n",
      "     Training Step: 24 Training Loss: 3.4034316539764404 \n",
      "     Training Step: 25 Training Loss: 2.518969774246216 \n",
      "     Training Step: 26 Training Loss: 3.301757574081421 \n",
      "     Training Step: 27 Training Loss: 3.4059624671936035 \n",
      "     Training Step: 28 Training Loss: 2.1404895782470703 \n",
      "     Training Step: 29 Training Loss: 3.3725879192352295 \n",
      "     Training Step: 30 Training Loss: 3.10347843170166 \n",
      "     Training Step: 31 Training Loss: 2.863105297088623 \n",
      "     Training Step: 32 Training Loss: 2.561826467514038 \n",
      "     Training Step: 33 Training Loss: 2.511902332305908 \n",
      "     Training Step: 34 Training Loss: 2.8780243396759033 \n",
      "     Training Step: 35 Training Loss: 2.853890895843506 \n",
      "     Training Step: 36 Training Loss: 2.9024460315704346 \n",
      "     Training Step: 37 Training Loss: 3.3248353004455566 \n",
      "     Training Step: 38 Training Loss: 2.1493310928344727 \n",
      "     Training Step: 39 Training Loss: 2.411881446838379 \n",
      "     Training Step: 40 Training Loss: 2.8805348873138428 \n",
      "     Training Step: 41 Training Loss: 3.3364267349243164 \n",
      "     Training Step: 42 Training Loss: 2.1958730220794678 \n",
      "     Training Step: 43 Training Loss: 2.4041285514831543 \n",
      "     Training Step: 44 Training Loss: 2.3913965225219727 \n",
      "     Training Step: 45 Training Loss: 4.313570976257324 \n",
      "     Training Step: 46 Training Loss: 2.2303309440612793 \n",
      "     Training Step: 47 Training Loss: 2.1565632820129395 \n",
      "     Training Step: 48 Training Loss: 3.3235881328582764 \n",
      "     Training Step: 49 Training Loss: 2.734705924987793 \n",
      "     Training Step: 50 Training Loss: 2.4405150413513184 \n",
      "     Training Step: 51 Training Loss: 3.6935620307922363 \n",
      "     Training Step: 52 Training Loss: 3.1352906227111816 \n",
      "     Training Step: 53 Training Loss: 2.5078606605529785 \n",
      "     Training Step: 54 Training Loss: 2.1333327293395996 \n",
      "     Training Step: 55 Training Loss: 2.227288246154785 \n",
      "     Training Step: 56 Training Loss: 2.7495532035827637 \n",
      "     Training Step: 57 Training Loss: 3.7949774265289307 \n",
      "     Training Step: 58 Training Loss: 3.154949426651001 \n",
      "     Training Step: 59 Training Loss: 4.333450794219971 \n",
      "     Training Step: 60 Training Loss: 4.659541130065918 \n",
      "     Training Step: 61 Training Loss: 3.658644676208496 \n",
      "     Training Step: 62 Training Loss: 2.850323438644409 \n",
      "     Training Step: 63 Training Loss: 3.443662643432617 \n",
      "     Training Step: 64 Training Loss: 2.9152631759643555 \n",
      "     Training Step: 65 Training Loss: 2.350405216217041 \n",
      "     Training Step: 66 Training Loss: 2.4379186630249023 \n",
      "     Training Step: 67 Training Loss: 3.865067481994629 \n",
      "     Training Step: 68 Training Loss: 3.799654960632324 \n",
      "     Training Step: 69 Training Loss: 3.01965069770813 \n",
      "     Training Step: 70 Training Loss: 2.79600191116333 \n",
      "     Training Step: 71 Training Loss: 3.5627098083496094 \n",
      "     Training Step: 72 Training Loss: 2.9297661781311035 \n",
      "     Training Step: 73 Training Loss: 2.900482177734375 \n",
      "     Training Step: 74 Training Loss: 3.0525569915771484 \n",
      "     Training Step: 75 Training Loss: 3.1590216159820557 \n",
      "     Training Step: 76 Training Loss: 3.76373291015625 \n",
      "     Training Step: 77 Training Loss: 2.2688355445861816 \n",
      "     Training Step: 78 Training Loss: 2.96988844871521 \n",
      "     Training Step: 79 Training Loss: 2.9551801681518555 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6528444290161133 \n",
      "     Validation Step: 1 Validation Loss: 3.154562473297119 \n",
      "     Validation Step: 2 Validation Loss: 3.2980234622955322 \n",
      "     Validation Step: 3 Validation Loss: 3.0490777492523193 \n",
      "     Validation Step: 4 Validation Loss: 3.705639123916626 \n",
      "     Validation Step: 5 Validation Loss: 3.0770559310913086 \n",
      "     Validation Step: 6 Validation Loss: 2.2623448371887207 \n",
      "     Validation Step: 7 Validation Loss: 2.868234634399414 \n",
      "     Validation Step: 8 Validation Loss: 3.675847291946411 \n",
      "     Validation Step: 9 Validation Loss: 2.775637626647949 \n",
      "     Validation Step: 10 Validation Loss: 4.001750946044922 \n",
      "     Validation Step: 11 Validation Loss: 3.079986333847046 \n",
      "     Validation Step: 12 Validation Loss: 3.1032891273498535 \n",
      "     Validation Step: 13 Validation Loss: 3.6073131561279297 \n",
      "     Validation Step: 14 Validation Loss: 2.7147130966186523 \n",
      "Epoch: 95\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.8583121299743652 \n",
      "     Training Step: 1 Training Loss: 3.295095205307007 \n",
      "     Training Step: 2 Training Loss: 2.862745761871338 \n",
      "     Training Step: 3 Training Loss: 3.5557150840759277 \n",
      "     Training Step: 4 Training Loss: 2.464712142944336 \n",
      "     Training Step: 5 Training Loss: 2.177156686782837 \n",
      "     Training Step: 6 Training Loss: 4.315672397613525 \n",
      "     Training Step: 7 Training Loss: 3.3713631629943848 \n",
      "     Training Step: 8 Training Loss: 2.8379945755004883 \n",
      "     Training Step: 9 Training Loss: 3.0145883560180664 \n",
      "     Training Step: 10 Training Loss: 3.7620458602905273 \n",
      "     Training Step: 11 Training Loss: 2.5046887397766113 \n",
      "     Training Step: 12 Training Loss: 3.012058973312378 \n",
      "     Training Step: 13 Training Loss: 2.4222989082336426 \n",
      "     Training Step: 14 Training Loss: 2.977379322052002 \n",
      "     Training Step: 15 Training Loss: 2.372685432434082 \n",
      "     Training Step: 16 Training Loss: 2.609159469604492 \n",
      "     Training Step: 17 Training Loss: 3.6116795539855957 \n",
      "     Training Step: 18 Training Loss: 2.8499772548675537 \n",
      "     Training Step: 19 Training Loss: 3.299973726272583 \n",
      "     Training Step: 20 Training Loss: 3.6393661499023438 \n",
      "     Training Step: 21 Training Loss: 3.791935682296753 \n",
      "     Training Step: 22 Training Loss: 2.1753053665161133 \n",
      "     Training Step: 23 Training Loss: 2.6070032119750977 \n",
      "     Training Step: 24 Training Loss: 3.263322114944458 \n",
      "     Training Step: 25 Training Loss: 2.4951846599578857 \n",
      "     Training Step: 26 Training Loss: 3.0768537521362305 \n",
      "     Training Step: 27 Training Loss: 3.182544708251953 \n",
      "     Training Step: 28 Training Loss: 2.1218910217285156 \n",
      "     Training Step: 29 Training Loss: 2.952756643295288 \n",
      "     Training Step: 30 Training Loss: 3.2013494968414307 \n",
      "     Training Step: 31 Training Loss: 2.2699193954467773 \n",
      "     Training Step: 32 Training Loss: 4.03354549407959 \n",
      "     Training Step: 33 Training Loss: 3.279359817504883 \n",
      "     Training Step: 34 Training Loss: 2.605349540710449 \n",
      "     Training Step: 35 Training Loss: 3.933833122253418 \n",
      "     Training Step: 36 Training Loss: 2.554924964904785 \n",
      "     Training Step: 37 Training Loss: 3.597170829772949 \n",
      "     Training Step: 38 Training Loss: 4.542297840118408 \n",
      "     Training Step: 39 Training Loss: 2.385375738143921 \n",
      "     Training Step: 40 Training Loss: 3.1412456035614014 \n",
      "     Training Step: 41 Training Loss: 2.257488250732422 \n",
      "     Training Step: 42 Training Loss: 2.5134968757629395 \n",
      "     Training Step: 43 Training Loss: 3.362363815307617 \n",
      "     Training Step: 44 Training Loss: 3.353365421295166 \n",
      "     Training Step: 45 Training Loss: 2.870673418045044 \n",
      "     Training Step: 46 Training Loss: 2.9096415042877197 \n",
      "     Training Step: 47 Training Loss: 3.3750338554382324 \n",
      "     Training Step: 48 Training Loss: 2.389126777648926 \n",
      "     Training Step: 49 Training Loss: 3.2287817001342773 \n",
      "     Training Step: 50 Training Loss: 2.484628677368164 \n",
      "     Training Step: 51 Training Loss: 3.0429000854492188 \n",
      "     Training Step: 52 Training Loss: 2.546217441558838 \n",
      "     Training Step: 53 Training Loss: 3.392007827758789 \n",
      "     Training Step: 54 Training Loss: 2.397698402404785 \n",
      "     Training Step: 55 Training Loss: 3.2518975734710693 \n",
      "     Training Step: 56 Training Loss: 2.4193530082702637 \n",
      "     Training Step: 57 Training Loss: 3.1101319789886475 \n",
      "     Training Step: 58 Training Loss: 2.635831832885742 \n",
      "     Training Step: 59 Training Loss: 3.7047510147094727 \n",
      "     Training Step: 60 Training Loss: 2.4561104774475098 \n",
      "     Training Step: 61 Training Loss: 4.1928300857543945 \n",
      "     Training Step: 62 Training Loss: 3.446777820587158 \n",
      "     Training Step: 63 Training Loss: 2.701181411743164 \n",
      "     Training Step: 64 Training Loss: 3.0227956771850586 \n",
      "     Training Step: 65 Training Loss: 2.505579948425293 \n",
      "     Training Step: 66 Training Loss: 3.0744972229003906 \n",
      "     Training Step: 67 Training Loss: 2.162123680114746 \n",
      "     Training Step: 68 Training Loss: 3.296544075012207 \n",
      "     Training Step: 69 Training Loss: 2.768488883972168 \n",
      "     Training Step: 70 Training Loss: 2.4275803565979004 \n",
      "     Training Step: 71 Training Loss: 2.835064172744751 \n",
      "     Training Step: 72 Training Loss: 3.3759846687316895 \n",
      "     Training Step: 73 Training Loss: 2.5910439491271973 \n",
      "     Training Step: 74 Training Loss: 2.733579158782959 \n",
      "     Training Step: 75 Training Loss: 2.718569755554199 \n",
      "     Training Step: 76 Training Loss: 2.3161282539367676 \n",
      "     Training Step: 77 Training Loss: 3.5690832138061523 \n",
      "     Training Step: 78 Training Loss: 2.9154417514801025 \n",
      "     Training Step: 79 Training Loss: 3.151583671569824 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.257290363311768 \n",
      "     Validation Step: 1 Validation Loss: 3.7372124195098877 \n",
      "     Validation Step: 2 Validation Loss: 3.2424964904785156 \n",
      "     Validation Step: 3 Validation Loss: 3.282809019088745 \n",
      "     Validation Step: 4 Validation Loss: 2.975864887237549 \n",
      "     Validation Step: 5 Validation Loss: 3.821629524230957 \n",
      "     Validation Step: 6 Validation Loss: 3.208620071411133 \n",
      "     Validation Step: 7 Validation Loss: 2.931774377822876 \n",
      "     Validation Step: 8 Validation Loss: 3.5545334815979004 \n",
      "     Validation Step: 9 Validation Loss: 3.849673271179199 \n",
      "     Validation Step: 10 Validation Loss: 2.924466371536255 \n",
      "     Validation Step: 11 Validation Loss: 2.817079544067383 \n",
      "     Validation Step: 12 Validation Loss: 3.086731433868408 \n",
      "     Validation Step: 13 Validation Loss: 2.1978683471679688 \n",
      "     Validation Step: 14 Validation Loss: 3.27976655960083 \n",
      "Epoch: 96\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.1758575439453125 \n",
      "     Training Step: 1 Training Loss: 4.284124851226807 \n",
      "     Training Step: 2 Training Loss: 2.6659374237060547 \n",
      "     Training Step: 3 Training Loss: 2.3084375858306885 \n",
      "     Training Step: 4 Training Loss: 3.096527576446533 \n",
      "     Training Step: 5 Training Loss: 2.6934657096862793 \n",
      "     Training Step: 6 Training Loss: 2.4071359634399414 \n",
      "     Training Step: 7 Training Loss: 2.6044068336486816 \n",
      "     Training Step: 8 Training Loss: 2.2021291255950928 \n",
      "     Training Step: 9 Training Loss: 3.578951358795166 \n",
      "     Training Step: 10 Training Loss: 2.1252593994140625 \n",
      "     Training Step: 11 Training Loss: 2.7547709941864014 \n",
      "     Training Step: 12 Training Loss: 2.135944366455078 \n",
      "     Training Step: 13 Training Loss: 3.1926052570343018 \n",
      "     Training Step: 14 Training Loss: 3.679718017578125 \n",
      "     Training Step: 15 Training Loss: 3.030738353729248 \n",
      "     Training Step: 16 Training Loss: 3.414719581604004 \n",
      "     Training Step: 17 Training Loss: 2.3570032119750977 \n",
      "     Training Step: 18 Training Loss: 4.139590740203857 \n",
      "     Training Step: 19 Training Loss: 4.075984477996826 \n",
      "     Training Step: 20 Training Loss: 2.4463725090026855 \n",
      "     Training Step: 21 Training Loss: 4.15069055557251 \n",
      "     Training Step: 22 Training Loss: 3.198460817337036 \n",
      "     Training Step: 23 Training Loss: 4.526805400848389 \n",
      "     Training Step: 24 Training Loss: 3.0606436729431152 \n",
      "     Training Step: 25 Training Loss: 3.30251407623291 \n",
      "     Training Step: 26 Training Loss: 2.9501514434814453 \n",
      "     Training Step: 27 Training Loss: 3.453403949737549 \n",
      "     Training Step: 28 Training Loss: 2.9376566410064697 \n",
      "     Training Step: 29 Training Loss: 2.5296339988708496 \n",
      "     Training Step: 30 Training Loss: 2.867239236831665 \n",
      "     Training Step: 31 Training Loss: 3.2762131690979004 \n",
      "     Training Step: 32 Training Loss: 3.6220033168792725 \n",
      "     Training Step: 33 Training Loss: 2.9234824180603027 \n",
      "     Training Step: 34 Training Loss: 2.952936887741089 \n",
      "     Training Step: 35 Training Loss: 3.33876895904541 \n",
      "     Training Step: 36 Training Loss: 2.399492025375366 \n",
      "     Training Step: 37 Training Loss: 3.0928232669830322 \n",
      "     Training Step: 38 Training Loss: 2.5671205520629883 \n",
      "     Training Step: 39 Training Loss: 2.584242582321167 \n",
      "     Training Step: 40 Training Loss: 3.219764232635498 \n",
      "     Training Step: 41 Training Loss: 2.3149404525756836 \n",
      "     Training Step: 42 Training Loss: 2.4791226387023926 \n",
      "     Training Step: 43 Training Loss: 3.499415397644043 \n",
      "     Training Step: 44 Training Loss: 2.822922706604004 \n",
      "     Training Step: 45 Training Loss: 3.3246803283691406 \n",
      "     Training Step: 46 Training Loss: 3.484299659729004 \n",
      "     Training Step: 47 Training Loss: 3.0602645874023438 \n",
      "     Training Step: 48 Training Loss: 3.5649166107177734 \n",
      "     Training Step: 49 Training Loss: 2.500967264175415 \n",
      "     Training Step: 50 Training Loss: 3.0460660457611084 \n",
      "     Training Step: 51 Training Loss: 2.835379123687744 \n",
      "     Training Step: 52 Training Loss: 2.872720241546631 \n",
      "     Training Step: 53 Training Loss: 2.34902024269104 \n",
      "     Training Step: 54 Training Loss: 3.209944248199463 \n",
      "     Training Step: 55 Training Loss: 3.4260239601135254 \n",
      "     Training Step: 56 Training Loss: 3.202538013458252 \n",
      "     Training Step: 57 Training Loss: 4.595612525939941 \n",
      "     Training Step: 58 Training Loss: 2.737382411956787 \n",
      "     Training Step: 59 Training Loss: 3.121201515197754 \n",
      "     Training Step: 60 Training Loss: 2.3748767375946045 \n",
      "     Training Step: 61 Training Loss: 2.2261834144592285 \n",
      "     Training Step: 62 Training Loss: 2.1634671688079834 \n",
      "     Training Step: 63 Training Loss: 3.514111280441284 \n",
      "     Training Step: 64 Training Loss: 2.892747402191162 \n",
      "     Training Step: 65 Training Loss: 2.9569931030273438 \n",
      "     Training Step: 66 Training Loss: 2.743119955062866 \n",
      "     Training Step: 67 Training Loss: 3.946836471557617 \n",
      "     Training Step: 68 Training Loss: 3.710261106491089 \n",
      "     Training Step: 69 Training Loss: 2.8060967922210693 \n",
      "     Training Step: 70 Training Loss: 3.2899818420410156 \n",
      "     Training Step: 71 Training Loss: 3.5331950187683105 \n",
      "     Training Step: 72 Training Loss: 2.735991954803467 \n",
      "     Training Step: 73 Training Loss: 2.991211414337158 \n",
      "     Training Step: 74 Training Loss: 3.905003309249878 \n",
      "     Training Step: 75 Training Loss: 3.156360626220703 \n",
      "     Training Step: 76 Training Loss: 2.2835655212402344 \n",
      "     Training Step: 77 Training Loss: 3.0408923625946045 \n",
      "     Training Step: 78 Training Loss: 2.094468116760254 \n",
      "     Training Step: 79 Training Loss: 3.3355913162231445 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.629755973815918 \n",
      "     Validation Step: 1 Validation Loss: 3.0910205841064453 \n",
      "     Validation Step: 2 Validation Loss: 2.762714385986328 \n",
      "     Validation Step: 3 Validation Loss: 3.6323137283325195 \n",
      "     Validation Step: 4 Validation Loss: 2.925386428833008 \n",
      "     Validation Step: 5 Validation Loss: 3.521636962890625 \n",
      "     Validation Step: 6 Validation Loss: 3.553925037384033 \n",
      "     Validation Step: 7 Validation Loss: 3.8088316917419434 \n",
      "     Validation Step: 8 Validation Loss: 3.6608424186706543 \n",
      "     Validation Step: 9 Validation Loss: 2.9250786304473877 \n",
      "     Validation Step: 10 Validation Loss: 3.266768455505371 \n",
      "     Validation Step: 11 Validation Loss: 2.671116828918457 \n",
      "     Validation Step: 12 Validation Loss: 2.2161459922790527 \n",
      "     Validation Step: 13 Validation Loss: 3.2083332538604736 \n",
      "     Validation Step: 14 Validation Loss: 3.1086018085479736 \n",
      "Epoch: 97\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.246814727783203 \n",
      "     Training Step: 1 Training Loss: 3.272238254547119 \n",
      "     Training Step: 2 Training Loss: 2.279426097869873 \n",
      "     Training Step: 3 Training Loss: 2.3315892219543457 \n",
      "     Training Step: 4 Training Loss: 2.2442901134490967 \n",
      "     Training Step: 5 Training Loss: 3.149986505508423 \n",
      "     Training Step: 6 Training Loss: 3.669562339782715 \n",
      "     Training Step: 7 Training Loss: 2.352847099304199 \n",
      "     Training Step: 8 Training Loss: 2.9220893383026123 \n",
      "     Training Step: 9 Training Loss: 3.1691155433654785 \n",
      "     Training Step: 10 Training Loss: 2.5120580196380615 \n",
      "     Training Step: 11 Training Loss: 2.969611644744873 \n",
      "     Training Step: 12 Training Loss: 2.253098964691162 \n",
      "     Training Step: 13 Training Loss: 3.134154796600342 \n",
      "     Training Step: 14 Training Loss: 3.185368061065674 \n",
      "     Training Step: 15 Training Loss: 2.365828275680542 \n",
      "     Training Step: 16 Training Loss: 3.4464943408966064 \n",
      "     Training Step: 17 Training Loss: 3.1904854774475098 \n",
      "     Training Step: 18 Training Loss: 2.4177446365356445 \n",
      "     Training Step: 19 Training Loss: 3.1759872436523438 \n",
      "     Training Step: 20 Training Loss: 2.0460774898529053 \n",
      "     Training Step: 21 Training Loss: 4.080564975738525 \n",
      "     Training Step: 22 Training Loss: 2.9302821159362793 \n",
      "     Training Step: 23 Training Loss: 3.3286757469177246 \n",
      "     Training Step: 24 Training Loss: 4.2854509353637695 \n",
      "     Training Step: 25 Training Loss: 2.4850125312805176 \n",
      "     Training Step: 26 Training Loss: 4.031179428100586 \n",
      "     Training Step: 27 Training Loss: 2.484126329421997 \n",
      "     Training Step: 28 Training Loss: 3.1442651748657227 \n",
      "     Training Step: 29 Training Loss: 3.930412530899048 \n",
      "     Training Step: 30 Training Loss: 2.5568180084228516 \n",
      "     Training Step: 31 Training Loss: 2.172818660736084 \n",
      "     Training Step: 32 Training Loss: 2.973607063293457 \n",
      "     Training Step: 33 Training Loss: 2.8013994693756104 \n",
      "     Training Step: 34 Training Loss: 2.985851287841797 \n",
      "     Training Step: 35 Training Loss: 2.5748181343078613 \n",
      "     Training Step: 36 Training Loss: 2.8826608657836914 \n",
      "     Training Step: 37 Training Loss: 3.0155258178710938 \n",
      "     Training Step: 38 Training Loss: 2.9275832176208496 \n",
      "     Training Step: 39 Training Loss: 3.397418737411499 \n",
      "     Training Step: 40 Training Loss: 4.59384298324585 \n",
      "     Training Step: 41 Training Loss: 2.7990071773529053 \n",
      "     Training Step: 42 Training Loss: 2.6060588359832764 \n",
      "     Training Step: 43 Training Loss: 3.3015689849853516 \n",
      "     Training Step: 44 Training Loss: 2.562263250350952 \n",
      "     Training Step: 45 Training Loss: 2.3964319229125977 \n",
      "     Training Step: 46 Training Loss: 2.7565078735351562 \n",
      "     Training Step: 47 Training Loss: 2.721717357635498 \n",
      "     Training Step: 48 Training Loss: 3.4330811500549316 \n",
      "     Training Step: 49 Training Loss: 3.721797227859497 \n",
      "     Training Step: 50 Training Loss: 2.7389798164367676 \n",
      "     Training Step: 51 Training Loss: 2.178673028945923 \n",
      "     Training Step: 52 Training Loss: 3.309818744659424 \n",
      "     Training Step: 53 Training Loss: 3.249119997024536 \n",
      "     Training Step: 54 Training Loss: 3.312403678894043 \n",
      "     Training Step: 55 Training Loss: 3.12839412689209 \n",
      "     Training Step: 56 Training Loss: 2.519084930419922 \n",
      "     Training Step: 57 Training Loss: 3.585444450378418 \n",
      "     Training Step: 58 Training Loss: 2.7213943004608154 \n",
      "     Training Step: 59 Training Loss: 3.7406017780303955 \n",
      "     Training Step: 60 Training Loss: 2.793750762939453 \n",
      "     Training Step: 61 Training Loss: 2.390522003173828 \n",
      "     Training Step: 62 Training Loss: 3.254948377609253 \n",
      "     Training Step: 63 Training Loss: 3.0974299907684326 \n",
      "     Training Step: 64 Training Loss: 3.4352593421936035 \n",
      "     Training Step: 65 Training Loss: 2.6905336380004883 \n",
      "     Training Step: 66 Training Loss: 3.144760847091675 \n",
      "     Training Step: 67 Training Loss: 4.368508815765381 \n",
      "     Training Step: 68 Training Loss: 3.0772337913513184 \n",
      "     Training Step: 69 Training Loss: 3.524649143218994 \n",
      "     Training Step: 70 Training Loss: 3.1486244201660156 \n",
      "     Training Step: 71 Training Loss: 2.5672764778137207 \n",
      "     Training Step: 72 Training Loss: 3.635768413543701 \n",
      "     Training Step: 73 Training Loss: 2.6763389110565186 \n",
      "     Training Step: 74 Training Loss: 3.046825885772705 \n",
      "     Training Step: 75 Training Loss: 3.4799134731292725 \n",
      "     Training Step: 76 Training Loss: 2.784715175628662 \n",
      "     Training Step: 77 Training Loss: 2.073082447052002 \n",
      "     Training Step: 78 Training Loss: 2.4835476875305176 \n",
      "     Training Step: 79 Training Loss: 2.2348101139068604 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.5835700035095215 \n",
      "     Validation Step: 1 Validation Loss: 2.942688465118408 \n",
      "     Validation Step: 2 Validation Loss: 3.0685393810272217 \n",
      "     Validation Step: 3 Validation Loss: 3.654419183731079 \n",
      "     Validation Step: 4 Validation Loss: 2.8900294303894043 \n",
      "     Validation Step: 5 Validation Loss: 2.6563587188720703 \n",
      "     Validation Step: 6 Validation Loss: 2.893819570541382 \n",
      "     Validation Step: 7 Validation Loss: 3.128994941711426 \n",
      "     Validation Step: 8 Validation Loss: 3.3399057388305664 \n",
      "     Validation Step: 9 Validation Loss: 3.5879364013671875 \n",
      "     Validation Step: 10 Validation Loss: 2.354341983795166 \n",
      "     Validation Step: 11 Validation Loss: 3.2836475372314453 \n",
      "     Validation Step: 12 Validation Loss: 3.4041690826416016 \n",
      "     Validation Step: 13 Validation Loss: 3.621255397796631 \n",
      "     Validation Step: 14 Validation Loss: 2.5139718055725098 \n",
      "---------------   LEARNING RATE HALVING DOWN TO: 1.6000000000000004e-06   ---------------\n",
      "---------------   CURRENT LEARNING RATE: 1.6000000000000004e-06   ---------------\n",
      "Epoch: 98\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.4236338138580322 \n",
      "     Training Step: 1 Training Loss: 3.143789291381836 \n",
      "     Training Step: 2 Training Loss: 3.2711963653564453 \n",
      "     Training Step: 3 Training Loss: 2.865234851837158 \n",
      "     Training Step: 4 Training Loss: 2.8679311275482178 \n",
      "     Training Step: 5 Training Loss: 3.8365113735198975 \n",
      "     Training Step: 6 Training Loss: 2.526322841644287 \n",
      "     Training Step: 7 Training Loss: 4.090980529785156 \n",
      "     Training Step: 8 Training Loss: 2.4858005046844482 \n",
      "     Training Step: 9 Training Loss: 2.515143871307373 \n",
      "     Training Step: 10 Training Loss: 4.2565178871154785 \n",
      "     Training Step: 11 Training Loss: 3.291633129119873 \n",
      "     Training Step: 12 Training Loss: 3.1797170639038086 \n",
      "     Training Step: 13 Training Loss: 3.6083452701568604 \n",
      "     Training Step: 14 Training Loss: 2.137549638748169 \n",
      "     Training Step: 15 Training Loss: 2.765690803527832 \n",
      "     Training Step: 16 Training Loss: 2.3060436248779297 \n",
      "     Training Step: 17 Training Loss: 3.195580005645752 \n",
      "     Training Step: 18 Training Loss: 3.8886706829071045 \n",
      "     Training Step: 19 Training Loss: 2.854132890701294 \n",
      "     Training Step: 20 Training Loss: 2.619267463684082 \n",
      "     Training Step: 21 Training Loss: 3.2590270042419434 \n",
      "     Training Step: 22 Training Loss: 3.7726383209228516 \n",
      "     Training Step: 23 Training Loss: 3.7494616508483887 \n",
      "     Training Step: 24 Training Loss: 2.5857319831848145 \n",
      "     Training Step: 25 Training Loss: 2.6635653972625732 \n",
      "     Training Step: 26 Training Loss: 2.768526792526245 \n",
      "     Training Step: 27 Training Loss: 3.5535919666290283 \n",
      "     Training Step: 28 Training Loss: 2.425311326980591 \n",
      "     Training Step: 29 Training Loss: 3.7307281494140625 \n",
      "     Training Step: 30 Training Loss: 3.315551280975342 \n",
      "     Training Step: 31 Training Loss: 3.739380359649658 \n",
      "     Training Step: 32 Training Loss: 3.48099422454834 \n",
      "     Training Step: 33 Training Loss: 3.389674425125122 \n",
      "     Training Step: 34 Training Loss: 3.250063180923462 \n",
      "     Training Step: 35 Training Loss: 3.0135576725006104 \n",
      "     Training Step: 36 Training Loss: 2.794208526611328 \n",
      "     Training Step: 37 Training Loss: 3.0352742671966553 \n",
      "     Training Step: 38 Training Loss: 2.8439931869506836 \n",
      "     Training Step: 39 Training Loss: 4.754212379455566 \n",
      "     Training Step: 40 Training Loss: 2.970381736755371 \n",
      "     Training Step: 41 Training Loss: 3.3939337730407715 \n",
      "     Training Step: 42 Training Loss: 3.043278217315674 \n",
      "     Training Step: 43 Training Loss: 2.589749813079834 \n",
      "     Training Step: 44 Training Loss: 3.233120918273926 \n",
      "     Training Step: 45 Training Loss: 2.5819966793060303 \n",
      "     Training Step: 46 Training Loss: 2.9306414127349854 \n",
      "     Training Step: 47 Training Loss: 3.1567816734313965 \n",
      "     Training Step: 48 Training Loss: 2.552032947540283 \n",
      "     Training Step: 49 Training Loss: 2.9822137355804443 \n",
      "     Training Step: 50 Training Loss: 2.300781488418579 \n",
      "     Training Step: 51 Training Loss: 2.9807422161102295 \n",
      "     Training Step: 52 Training Loss: 2.898364305496216 \n",
      "     Training Step: 53 Training Loss: 3.0361528396606445 \n",
      "     Training Step: 54 Training Loss: 2.1364307403564453 \n",
      "     Training Step: 55 Training Loss: 3.1080803871154785 \n",
      "     Training Step: 56 Training Loss: 2.3438358306884766 \n",
      "     Training Step: 57 Training Loss: 2.6514174938201904 \n",
      "     Training Step: 58 Training Loss: 3.7840828895568848 \n",
      "     Training Step: 59 Training Loss: 2.4267897605895996 \n",
      "     Training Step: 60 Training Loss: 4.292131423950195 \n",
      "     Training Step: 61 Training Loss: 2.384554624557495 \n",
      "     Training Step: 62 Training Loss: 2.2435684204101562 \n",
      "     Training Step: 63 Training Loss: 2.2246885299682617 \n",
      "     Training Step: 64 Training Loss: 2.8096959590911865 \n",
      "     Training Step: 65 Training Loss: 2.7126312255859375 \n",
      "     Training Step: 66 Training Loss: 2.7508060932159424 \n",
      "     Training Step: 67 Training Loss: 3.2378430366516113 \n",
      "     Training Step: 68 Training Loss: 3.159191608428955 \n",
      "     Training Step: 69 Training Loss: 2.686997175216675 \n",
      "     Training Step: 70 Training Loss: 2.3977935314178467 \n",
      "     Training Step: 71 Training Loss: 2.8787424564361572 \n",
      "     Training Step: 72 Training Loss: 2.1733646392822266 \n",
      "     Training Step: 73 Training Loss: 3.550889492034912 \n",
      "     Training Step: 74 Training Loss: 2.269408702850342 \n",
      "     Training Step: 75 Training Loss: 2.028555154800415 \n",
      "     Training Step: 76 Training Loss: 2.482964038848877 \n",
      "     Training Step: 77 Training Loss: 2.5568182468414307 \n",
      "     Training Step: 78 Training Loss: 2.6702656745910645 \n",
      "     Training Step: 79 Training Loss: 2.9927186965942383 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.7072651386260986 \n",
      "     Validation Step: 1 Validation Loss: 2.7829554080963135 \n",
      "     Validation Step: 2 Validation Loss: 3.031236410140991 \n",
      "     Validation Step: 3 Validation Loss: 3.136042594909668 \n",
      "     Validation Step: 4 Validation Loss: 3.332934617996216 \n",
      "     Validation Step: 5 Validation Loss: 3.149477005004883 \n",
      "     Validation Step: 6 Validation Loss: 3.9500021934509277 \n",
      "     Validation Step: 7 Validation Loss: 3.1915624141693115 \n",
      "     Validation Step: 8 Validation Loss: 2.754042863845825 \n",
      "     Validation Step: 9 Validation Loss: 2.194363594055176 \n",
      "     Validation Step: 10 Validation Loss: 3.6274030208587646 \n",
      "     Validation Step: 11 Validation Loss: 3.5822763442993164 \n",
      "     Validation Step: 12 Validation Loss: 2.8847689628601074 \n",
      "     Validation Step: 13 Validation Loss: 3.648491621017456 \n",
      "     Validation Step: 14 Validation Loss: 3.059950351715088 \n",
      "Epoch: 99\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.74811053276062 \n",
      "     Training Step: 1 Training Loss: 3.464482545852661 \n",
      "     Training Step: 2 Training Loss: 2.6394898891448975 \n",
      "     Training Step: 3 Training Loss: 2.4765892028808594 \n",
      "     Training Step: 4 Training Loss: 2.7711377143859863 \n",
      "     Training Step: 5 Training Loss: 2.3915605545043945 \n",
      "     Training Step: 6 Training Loss: 3.5166666507720947 \n",
      "     Training Step: 7 Training Loss: 2.692929983139038 \n",
      "     Training Step: 8 Training Loss: 2.3649678230285645 \n",
      "     Training Step: 9 Training Loss: 2.687767267227173 \n",
      "     Training Step: 10 Training Loss: 2.2096519470214844 \n",
      "     Training Step: 11 Training Loss: 3.8083622455596924 \n",
      "     Training Step: 12 Training Loss: 2.2440192699432373 \n",
      "     Training Step: 13 Training Loss: 3.5663790702819824 \n",
      "     Training Step: 14 Training Loss: 2.4548587799072266 \n",
      "     Training Step: 15 Training Loss: 3.014925956726074 \n",
      "     Training Step: 16 Training Loss: 2.557511806488037 \n",
      "     Training Step: 17 Training Loss: 3.859846591949463 \n",
      "     Training Step: 18 Training Loss: 2.975245237350464 \n",
      "     Training Step: 19 Training Loss: 3.4168589115142822 \n",
      "     Training Step: 20 Training Loss: 3.096932888031006 \n",
      "     Training Step: 21 Training Loss: 3.819176435470581 \n",
      "     Training Step: 22 Training Loss: 2.5503759384155273 \n",
      "     Training Step: 23 Training Loss: 3.082725763320923 \n",
      "     Training Step: 24 Training Loss: 3.0812859535217285 \n",
      "     Training Step: 25 Training Loss: 3.037140130996704 \n",
      "     Training Step: 26 Training Loss: 3.5017402172088623 \n",
      "     Training Step: 27 Training Loss: 2.822924852371216 \n",
      "     Training Step: 28 Training Loss: 3.1222071647644043 \n",
      "     Training Step: 29 Training Loss: 2.9808571338653564 \n",
      "     Training Step: 30 Training Loss: 2.8266427516937256 \n",
      "     Training Step: 31 Training Loss: 3.5660252571105957 \n",
      "     Training Step: 32 Training Loss: 2.6878480911254883 \n",
      "     Training Step: 33 Training Loss: 2.621269941329956 \n",
      "     Training Step: 34 Training Loss: 2.330484390258789 \n",
      "     Training Step: 35 Training Loss: 4.1772780418396 \n",
      "     Training Step: 36 Training Loss: 3.850109577178955 \n",
      "     Training Step: 37 Training Loss: 3.1611084938049316 \n",
      "     Training Step: 38 Training Loss: 2.82487154006958 \n",
      "     Training Step: 39 Training Loss: 4.378697395324707 \n",
      "     Training Step: 40 Training Loss: 4.199602127075195 \n",
      "     Training Step: 41 Training Loss: 2.8836746215820312 \n",
      "     Training Step: 42 Training Loss: 2.1583542823791504 \n",
      "     Training Step: 43 Training Loss: 4.730981826782227 \n",
      "     Training Step: 44 Training Loss: 3.318988800048828 \n",
      "     Training Step: 45 Training Loss: 3.0501627922058105 \n",
      "     Training Step: 46 Training Loss: 3.1153178215026855 \n",
      "     Training Step: 47 Training Loss: 2.9472055435180664 \n",
      "     Training Step: 48 Training Loss: 3.5440025329589844 \n",
      "     Training Step: 49 Training Loss: 2.589965343475342 \n",
      "     Training Step: 50 Training Loss: 3.4472007751464844 \n",
      "     Training Step: 51 Training Loss: 2.555180788040161 \n",
      "     Training Step: 52 Training Loss: 2.227257013320923 \n",
      "     Training Step: 53 Training Loss: 2.159507989883423 \n",
      "     Training Step: 54 Training Loss: 2.742863178253174 \n",
      "     Training Step: 55 Training Loss: 3.0904226303100586 \n",
      "     Training Step: 56 Training Loss: 4.344768524169922 \n",
      "     Training Step: 57 Training Loss: 3.049825668334961 \n",
      "     Training Step: 58 Training Loss: 2.750709056854248 \n",
      "     Training Step: 59 Training Loss: 2.8026392459869385 \n",
      "     Training Step: 60 Training Loss: 2.751774787902832 \n",
      "     Training Step: 61 Training Loss: 2.3469491004943848 \n",
      "     Training Step: 62 Training Loss: 3.125153064727783 \n",
      "     Training Step: 63 Training Loss: 2.4209868907928467 \n",
      "     Training Step: 64 Training Loss: 2.8027830123901367 \n",
      "     Training Step: 65 Training Loss: 2.5554232597351074 \n",
      "     Training Step: 66 Training Loss: 3.2398557662963867 \n",
      "     Training Step: 67 Training Loss: 2.3904590606689453 \n",
      "     Training Step: 68 Training Loss: 3.4842991828918457 \n",
      "     Training Step: 69 Training Loss: 2.0853757858276367 \n",
      "     Training Step: 70 Training Loss: 3.2486419677734375 \n",
      "     Training Step: 71 Training Loss: 3.5110580921173096 \n",
      "     Training Step: 72 Training Loss: 2.618227005004883 \n",
      "     Training Step: 73 Training Loss: 2.0181989669799805 \n",
      "     Training Step: 74 Training Loss: 2.867245674133301 \n",
      "     Training Step: 75 Training Loss: 3.526066303253174 \n",
      "     Training Step: 76 Training Loss: 3.46686053276062 \n",
      "     Training Step: 77 Training Loss: 3.3358659744262695 \n",
      "     Training Step: 78 Training Loss: 3.338503360748291 \n",
      "     Training Step: 79 Training Loss: 2.461160898208618 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.8833868503570557 \n",
      "     Validation Step: 1 Validation Loss: 3.6867835521698 \n",
      "     Validation Step: 2 Validation Loss: 3.15159010887146 \n",
      "     Validation Step: 3 Validation Loss: 2.647331714630127 \n",
      "     Validation Step: 4 Validation Loss: 3.550321578979492 \n",
      "     Validation Step: 5 Validation Loss: 3.5120882987976074 \n",
      "     Validation Step: 6 Validation Loss: 3.020254611968994 \n",
      "     Validation Step: 7 Validation Loss: 3.0539143085479736 \n",
      "     Validation Step: 8 Validation Loss: 2.986816167831421 \n",
      "     Validation Step: 9 Validation Loss: 3.5336341857910156 \n",
      "     Validation Step: 10 Validation Loss: 2.6395719051361084 \n",
      "     Validation Step: 11 Validation Loss: 3.207979679107666 \n",
      "     Validation Step: 12 Validation Loss: 2.247368812561035 \n",
      "     Validation Step: 13 Validation Loss: 3.680840492248535 \n",
      "     Validation Step: 14 Validation Loss: 3.941417932510376 \n",
      "Epoch: 100\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.5835375785827637 \n",
      "     Training Step: 1 Training Loss: 3.4471583366394043 \n",
      "     Training Step: 2 Training Loss: 4.800356864929199 \n",
      "     Training Step: 3 Training Loss: 2.6118311882019043 \n",
      "     Training Step: 4 Training Loss: 2.4562840461730957 \n",
      "     Training Step: 5 Training Loss: 4.344081401824951 \n",
      "     Training Step: 6 Training Loss: 2.9438095092773438 \n",
      "     Training Step: 7 Training Loss: 2.305685520172119 \n",
      "     Training Step: 8 Training Loss: 2.974033832550049 \n",
      "     Training Step: 9 Training Loss: 2.698215961456299 \n",
      "     Training Step: 10 Training Loss: 3.2736847400665283 \n",
      "     Training Step: 11 Training Loss: 2.7612359523773193 \n",
      "     Training Step: 12 Training Loss: 2.732560634613037 \n",
      "     Training Step: 13 Training Loss: 2.8962814807891846 \n",
      "     Training Step: 14 Training Loss: 2.04600191116333 \n",
      "     Training Step: 15 Training Loss: 2.744941473007202 \n",
      "     Training Step: 16 Training Loss: 3.775704860687256 \n",
      "     Training Step: 17 Training Loss: 3.086117744445801 \n",
      "     Training Step: 18 Training Loss: 2.756448984146118 \n",
      "     Training Step: 19 Training Loss: 2.919142007827759 \n",
      "     Training Step: 20 Training Loss: 3.4886584281921387 \n",
      "     Training Step: 21 Training Loss: 3.1544992923736572 \n",
      "     Training Step: 22 Training Loss: 2.9154212474823 \n",
      "     Training Step: 23 Training Loss: 2.8137714862823486 \n",
      "     Training Step: 24 Training Loss: 3.3432974815368652 \n",
      "     Training Step: 25 Training Loss: 2.1451807022094727 \n",
      "     Training Step: 26 Training Loss: 4.025269985198975 \n",
      "     Training Step: 27 Training Loss: 3.0415704250335693 \n",
      "     Training Step: 28 Training Loss: 3.3463568687438965 \n",
      "     Training Step: 29 Training Loss: 2.221501111984253 \n",
      "     Training Step: 30 Training Loss: 3.203817844390869 \n",
      "     Training Step: 31 Training Loss: 2.884207248687744 \n",
      "     Training Step: 32 Training Loss: 3.028228759765625 \n",
      "     Training Step: 33 Training Loss: 2.579409122467041 \n",
      "     Training Step: 34 Training Loss: 3.6385560035705566 \n",
      "     Training Step: 35 Training Loss: 3.2481393814086914 \n",
      "     Training Step: 36 Training Loss: 2.0631613731384277 \n",
      "     Training Step: 37 Training Loss: 2.2290329933166504 \n",
      "     Training Step: 38 Training Loss: 2.1401662826538086 \n",
      "     Training Step: 39 Training Loss: 3.3932137489318848 \n",
      "     Training Step: 40 Training Loss: 4.361369609832764 \n",
      "     Training Step: 41 Training Loss: 2.191401958465576 \n",
      "     Training Step: 42 Training Loss: 2.3766977787017822 \n",
      "     Training Step: 43 Training Loss: 2.5316851139068604 \n",
      "     Training Step: 44 Training Loss: 2.6997766494750977 \n",
      "     Training Step: 45 Training Loss: 2.3866257667541504 \n",
      "     Training Step: 46 Training Loss: 2.251783847808838 \n",
      "     Training Step: 47 Training Loss: 2.995832681655884 \n",
      "     Training Step: 48 Training Loss: 3.476628303527832 \n",
      "     Training Step: 49 Training Loss: 2.2925732135772705 \n",
      "     Training Step: 50 Training Loss: 3.12672758102417 \n",
      "     Training Step: 51 Training Loss: 3.740662097930908 \n",
      "     Training Step: 52 Training Loss: 3.3709635734558105 \n",
      "     Training Step: 53 Training Loss: 3.3591904640197754 \n",
      "     Training Step: 54 Training Loss: 2.7097959518432617 \n",
      "     Training Step: 55 Training Loss: 3.897789478302002 \n",
      "     Training Step: 56 Training Loss: 2.451742172241211 \n",
      "     Training Step: 57 Training Loss: 2.4043540954589844 \n",
      "     Training Step: 58 Training Loss: 3.7503743171691895 \n",
      "     Training Step: 59 Training Loss: 3.273486375808716 \n",
      "     Training Step: 60 Training Loss: 3.3819451332092285 \n",
      "     Training Step: 61 Training Loss: 2.8759663105010986 \n",
      "     Training Step: 62 Training Loss: 3.375065326690674 \n",
      "     Training Step: 63 Training Loss: 3.0590999126434326 \n",
      "     Training Step: 64 Training Loss: 2.711000442504883 \n",
      "     Training Step: 65 Training Loss: 2.7533254623413086 \n",
      "     Training Step: 66 Training Loss: 3.105516195297241 \n",
      "     Training Step: 67 Training Loss: 3.3076038360595703 \n",
      "     Training Step: 68 Training Loss: 2.39894437789917 \n",
      "     Training Step: 69 Training Loss: 2.455385684967041 \n",
      "     Training Step: 70 Training Loss: 2.697624683380127 \n",
      "     Training Step: 71 Training Loss: 4.334470272064209 \n",
      "     Training Step: 72 Training Loss: 2.6160073280334473 \n",
      "     Training Step: 73 Training Loss: 2.3698480129241943 \n",
      "     Training Step: 74 Training Loss: 3.324705123901367 \n",
      "     Training Step: 75 Training Loss: 3.2095189094543457 \n",
      "     Training Step: 76 Training Loss: 3.1453146934509277 \n",
      "     Training Step: 77 Training Loss: 3.06174635887146 \n",
      "     Training Step: 78 Training Loss: 2.726444959640503 \n",
      "     Training Step: 79 Training Loss: 2.2966089248657227 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.108457088470459 \n",
      "     Validation Step: 1 Validation Loss: 2.976858377456665 \n",
      "     Validation Step: 2 Validation Loss: 3.2247462272644043 \n",
      "     Validation Step: 3 Validation Loss: 2.9186785221099854 \n",
      "     Validation Step: 4 Validation Loss: 3.6619794368743896 \n",
      "     Validation Step: 5 Validation Loss: 3.6651554107666016 \n",
      "     Validation Step: 6 Validation Loss: 2.776916980743408 \n",
      "     Validation Step: 7 Validation Loss: 3.1787269115448 \n",
      "     Validation Step: 8 Validation Loss: 2.842000961303711 \n",
      "     Validation Step: 9 Validation Loss: 3.6679060459136963 \n",
      "     Validation Step: 10 Validation Loss: 2.2333202362060547 \n",
      "     Validation Step: 11 Validation Loss: 3.0850014686584473 \n",
      "     Validation Step: 12 Validation Loss: 3.1831634044647217 \n",
      "     Validation Step: 13 Validation Loss: 3.7202131748199463 \n",
      "     Validation Step: 14 Validation Loss: 3.0660383701324463 \n",
      "---------------   CURRENT LEARNING RATE: 1.6000000000000004e-06   ---------------\n",
      "Epoch: 100\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 6.315705299377441 \n",
      "     Training Step: 1 Training Loss: 7.244068145751953 \n",
      "     Training Step: 2 Training Loss: 4.454128265380859 \n",
      "     Training Step: 3 Training Loss: 10.347792625427246 \n",
      "     Training Step: 4 Training Loss: 6.638555526733398 \n",
      "     Training Step: 5 Training Loss: 3.349789619445801 \n",
      "     Training Step: 6 Training Loss: 6.631289958953857 \n",
      "     Training Step: 7 Training Loss: 6.665380477905273 \n",
      "     Training Step: 8 Training Loss: 5.485795497894287 \n",
      "     Training Step: 9 Training Loss: 8.762036323547363 \n",
      "     Training Step: 10 Training Loss: 6.621796131134033 \n",
      "     Training Step: 11 Training Loss: 6.96030330657959 \n",
      "     Training Step: 12 Training Loss: 7.20860481262207 \n",
      "     Training Step: 13 Training Loss: 3.2659196853637695 \n",
      "     Training Step: 14 Training Loss: 4.065299034118652 \n",
      "     Training Step: 15 Training Loss: 5.064201354980469 \n",
      "     Training Step: 16 Training Loss: 5.191128253936768 \n",
      "     Training Step: 17 Training Loss: 8.967781066894531 \n",
      "     Training Step: 18 Training Loss: 3.3221254348754883 \n",
      "     Training Step: 19 Training Loss: 4.9340410232543945 \n",
      "     Training Step: 20 Training Loss: 4.848502159118652 \n",
      "     Training Step: 21 Training Loss: 8.221604347229004 \n",
      "     Training Step: 22 Training Loss: 5.564946174621582 \n",
      "     Training Step: 23 Training Loss: 4.55942440032959 \n",
      "     Training Step: 24 Training Loss: 12.346108436584473 \n",
      "     Training Step: 25 Training Loss: 4.906598091125488 \n",
      "     Training Step: 26 Training Loss: 10.074295997619629 \n",
      "     Training Step: 27 Training Loss: 8.8071870803833 \n",
      "     Training Step: 28 Training Loss: 4.3252458572387695 \n",
      "     Training Step: 29 Training Loss: 3.5359907150268555 \n",
      "     Training Step: 30 Training Loss: 7.300327301025391 \n",
      "     Training Step: 31 Training Loss: 2.978018045425415 \n",
      "     Training Step: 32 Training Loss: 5.47663688659668 \n",
      "     Training Step: 33 Training Loss: 6.132676124572754 \n",
      "     Training Step: 34 Training Loss: 6.874701023101807 \n",
      "     Training Step: 35 Training Loss: 4.960198879241943 \n",
      "     Training Step: 36 Training Loss: 6.147006511688232 \n",
      "     Training Step: 37 Training Loss: 5.99641752243042 \n",
      "     Training Step: 38 Training Loss: 4.395364284515381 \n",
      "     Training Step: 39 Training Loss: 7.417538642883301 \n",
      "     Training Step: 40 Training Loss: 10.200090408325195 \n",
      "     Training Step: 41 Training Loss: 8.067131996154785 \n",
      "     Training Step: 42 Training Loss: 3.6617894172668457 \n",
      "     Training Step: 43 Training Loss: 4.162550926208496 \n",
      "     Training Step: 44 Training Loss: 3.2720437049865723 \n",
      "     Training Step: 45 Training Loss: 6.642420768737793 \n",
      "     Training Step: 46 Training Loss: 8.452583312988281 \n",
      "     Training Step: 47 Training Loss: 6.987141132354736 \n",
      "     Training Step: 48 Training Loss: 4.296846866607666 \n",
      "     Training Step: 49 Training Loss: 6.07908821105957 \n",
      "     Training Step: 50 Training Loss: 10.636251449584961 \n",
      "     Training Step: 51 Training Loss: 5.307145118713379 \n",
      "     Training Step: 52 Training Loss: 3.6311068534851074 \n",
      "     Training Step: 53 Training Loss: 5.267079830169678 \n",
      "     Training Step: 54 Training Loss: 5.431302070617676 \n",
      "     Training Step: 55 Training Loss: 5.9543137550354 \n",
      "     Training Step: 56 Training Loss: 7.426760673522949 \n",
      "     Training Step: 57 Training Loss: 2.9321086406707764 \n",
      "     Training Step: 58 Training Loss: 2.755427360534668 \n",
      "     Training Step: 59 Training Loss: 3.95790433883667 \n",
      "     Training Step: 60 Training Loss: 6.133782863616943 \n",
      "     Training Step: 61 Training Loss: 7.361504554748535 \n",
      "     Training Step: 62 Training Loss: 6.340631484985352 \n",
      "     Training Step: 63 Training Loss: 5.9662699699401855 \n",
      "     Training Step: 64 Training Loss: 6.799368381500244 \n",
      "     Training Step: 65 Training Loss: 4.095131874084473 \n",
      "     Training Step: 66 Training Loss: 7.424806118011475 \n",
      "     Training Step: 67 Training Loss: 5.375712871551514 \n",
      "     Training Step: 68 Training Loss: 5.1369524002075195 \n",
      "     Training Step: 69 Training Loss: 6.192532539367676 \n",
      "     Training Step: 70 Training Loss: 7.568655967712402 \n",
      "     Training Step: 71 Training Loss: 3.5338616371154785 \n",
      "     Training Step: 72 Training Loss: 4.455545425415039 \n",
      "     Training Step: 73 Training Loss: 8.642576217651367 \n",
      "     Training Step: 74 Training Loss: 3.289175033569336 \n",
      "     Training Step: 75 Training Loss: 7.617958068847656 \n",
      "     Training Step: 76 Training Loss: 4.900948524475098 \n",
      "     Training Step: 77 Training Loss: 4.854245185852051 \n",
      "     Training Step: 78 Training Loss: 6.955386161804199 \n",
      "     Training Step: 79 Training Loss: 4.712984561920166 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 9.501187324523926 \n",
      "     Validation Step: 1 Validation Loss: 5.192089080810547 \n",
      "     Validation Step: 2 Validation Loss: 8.034553527832031 \n",
      "     Validation Step: 3 Validation Loss: 8.444432258605957 \n",
      "     Validation Step: 4 Validation Loss: 6.351536750793457 \n",
      "     Validation Step: 5 Validation Loss: 6.719906330108643 \n",
      "     Validation Step: 6 Validation Loss: 7.245396614074707 \n",
      "     Validation Step: 7 Validation Loss: 6.216196537017822 \n",
      "     Validation Step: 8 Validation Loss: 6.304421424865723 \n",
      "     Validation Step: 9 Validation Loss: 3.230677843093872 \n",
      "     Validation Step: 10 Validation Loss: 6.328394889831543 \n",
      "     Validation Step: 11 Validation Loss: 5.379700660705566 \n",
      "     Validation Step: 12 Validation Loss: 5.8099799156188965 \n",
      "     Validation Step: 13 Validation Loss: 8.407064437866211 \n",
      "     Validation Step: 14 Validation Loss: 8.230209350585938 \n",
      "Epoch: 101\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.534174919128418 \n",
      "     Training Step: 1 Training Loss: 3.339362144470215 \n",
      "     Training Step: 2 Training Loss: 2.730725049972534 \n",
      "     Training Step: 3 Training Loss: 2.190870523452759 \n",
      "     Training Step: 4 Training Loss: 3.310800552368164 \n",
      "     Training Step: 5 Training Loss: 3.065293788909912 \n",
      "     Training Step: 6 Training Loss: 2.996281623840332 \n",
      "     Training Step: 7 Training Loss: 3.3730907440185547 \n",
      "     Training Step: 8 Training Loss: 3.1550793647766113 \n",
      "     Training Step: 9 Training Loss: 2.75626802444458 \n",
      "     Training Step: 10 Training Loss: 2.6282081604003906 \n",
      "     Training Step: 11 Training Loss: 3.7943506240844727 \n",
      "     Training Step: 12 Training Loss: 4.2643914222717285 \n",
      "     Training Step: 13 Training Loss: 2.139247417449951 \n",
      "     Training Step: 14 Training Loss: 2.940213918685913 \n",
      "     Training Step: 15 Training Loss: 4.006777286529541 \n",
      "     Training Step: 16 Training Loss: 3.7020339965820312 \n",
      "     Training Step: 17 Training Loss: 3.127699375152588 \n",
      "     Training Step: 18 Training Loss: 3.300632953643799 \n",
      "     Training Step: 19 Training Loss: 2.923668622970581 \n",
      "     Training Step: 20 Training Loss: 2.470276117324829 \n",
      "     Training Step: 21 Training Loss: 3.3048155307769775 \n",
      "     Training Step: 22 Training Loss: 2.3307511806488037 \n",
      "     Training Step: 23 Training Loss: 2.912848472595215 \n",
      "     Training Step: 24 Training Loss: 3.153958797454834 \n",
      "     Training Step: 25 Training Loss: 2.685420036315918 \n",
      "     Training Step: 26 Training Loss: 3.3972012996673584 \n",
      "     Training Step: 27 Training Loss: 3.7390170097351074 \n",
      "     Training Step: 28 Training Loss: 3.2946677207946777 \n",
      "     Training Step: 29 Training Loss: 2.318368434906006 \n",
      "     Training Step: 30 Training Loss: 2.444420337677002 \n",
      "     Training Step: 31 Training Loss: 4.2265520095825195 \n",
      "     Training Step: 32 Training Loss: 2.6179676055908203 \n",
      "     Training Step: 33 Training Loss: 2.1637020111083984 \n",
      "     Training Step: 34 Training Loss: 3.727903366088867 \n",
      "     Training Step: 35 Training Loss: 2.9115748405456543 \n",
      "     Training Step: 36 Training Loss: 3.537853717803955 \n",
      "     Training Step: 37 Training Loss: 2.753843069076538 \n",
      "     Training Step: 38 Training Loss: 2.579254627227783 \n",
      "     Training Step: 39 Training Loss: 2.9746971130371094 \n",
      "     Training Step: 40 Training Loss: 2.471539258956909 \n",
      "     Training Step: 41 Training Loss: 3.6275429725646973 \n",
      "     Training Step: 42 Training Loss: 2.1791162490844727 \n",
      "     Training Step: 43 Training Loss: 2.740347146987915 \n",
      "     Training Step: 44 Training Loss: 3.341982841491699 \n",
      "     Training Step: 45 Training Loss: 2.8703856468200684 \n",
      "     Training Step: 46 Training Loss: 2.5244555473327637 \n",
      "     Training Step: 47 Training Loss: 2.670755386352539 \n",
      "     Training Step: 48 Training Loss: 3.126176118850708 \n",
      "     Training Step: 49 Training Loss: 2.3964037895202637 \n",
      "     Training Step: 50 Training Loss: 2.8114373683929443 \n",
      "     Training Step: 51 Training Loss: 2.6529159545898438 \n",
      "     Training Step: 52 Training Loss: 3.077800750732422 \n",
      "     Training Step: 53 Training Loss: 3.076714038848877 \n",
      "     Training Step: 54 Training Loss: 2.1965322494506836 \n",
      "     Training Step: 55 Training Loss: 3.7502732276916504 \n",
      "     Training Step: 56 Training Loss: 2.8922383785247803 \n",
      "     Training Step: 57 Training Loss: 2.3608968257904053 \n",
      "     Training Step: 58 Training Loss: 3.252342700958252 \n",
      "     Training Step: 59 Training Loss: 2.5100765228271484 \n",
      "     Training Step: 60 Training Loss: 3.207132339477539 \n",
      "     Training Step: 61 Training Loss: 2.1232049465179443 \n",
      "     Training Step: 62 Training Loss: 2.47808837890625 \n",
      "     Training Step: 63 Training Loss: 2.8137013912200928 \n",
      "     Training Step: 64 Training Loss: 3.195737838745117 \n",
      "     Training Step: 65 Training Loss: 2.6408843994140625 \n",
      "     Training Step: 66 Training Loss: 3.10884428024292 \n",
      "     Training Step: 67 Training Loss: 3.5987586975097656 \n",
      "     Training Step: 68 Training Loss: 3.174217700958252 \n",
      "     Training Step: 69 Training Loss: 2.611884355545044 \n",
      "     Training Step: 70 Training Loss: 2.613624095916748 \n",
      "     Training Step: 71 Training Loss: 2.8439886569976807 \n",
      "     Training Step: 72 Training Loss: 3.2982940673828125 \n",
      "     Training Step: 73 Training Loss: 2.520148754119873 \n",
      "     Training Step: 74 Training Loss: 4.813880443572998 \n",
      "     Training Step: 75 Training Loss: 4.412202835083008 \n",
      "     Training Step: 76 Training Loss: 2.324972629547119 \n",
      "     Training Step: 77 Training Loss: 3.3207430839538574 \n",
      "     Training Step: 78 Training Loss: 2.287454128265381 \n",
      "     Training Step: 79 Training Loss: 2.5182623863220215 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9079763889312744 \n",
      "     Validation Step: 1 Validation Loss: 3.043985366821289 \n",
      "     Validation Step: 2 Validation Loss: 3.864452838897705 \n",
      "     Validation Step: 3 Validation Loss: 3.0553600788116455 \n",
      "     Validation Step: 4 Validation Loss: 3.4146182537078857 \n",
      "     Validation Step: 5 Validation Loss: 3.568972587585449 \n",
      "     Validation Step: 6 Validation Loss: 2.6605892181396484 \n",
      "     Validation Step: 7 Validation Loss: 3.6585352420806885 \n",
      "     Validation Step: 8 Validation Loss: 2.2314600944519043 \n",
      "     Validation Step: 9 Validation Loss: 3.093435525894165 \n",
      "     Validation Step: 10 Validation Loss: 2.8367409706115723 \n",
      "     Validation Step: 11 Validation Loss: 3.7165117263793945 \n",
      "     Validation Step: 12 Validation Loss: 3.0662384033203125 \n",
      "     Validation Step: 13 Validation Loss: 3.170640468597412 \n",
      "     Validation Step: 14 Validation Loss: 3.6239943504333496 \n",
      "Epoch: 102\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.410940408706665 \n",
      "     Training Step: 1 Training Loss: 3.532611608505249 \n",
      "     Training Step: 2 Training Loss: 3.311034679412842 \n",
      "     Training Step: 3 Training Loss: 3.047023296356201 \n",
      "     Training Step: 4 Training Loss: 3.065577983856201 \n",
      "     Training Step: 5 Training Loss: 3.336151599884033 \n",
      "     Training Step: 6 Training Loss: 2.8117899894714355 \n",
      "     Training Step: 7 Training Loss: 2.755643129348755 \n",
      "     Training Step: 8 Training Loss: 3.401139497756958 \n",
      "     Training Step: 9 Training Loss: 2.760625123977661 \n",
      "     Training Step: 10 Training Loss: 2.675493001937866 \n",
      "     Training Step: 11 Training Loss: 2.5800113677978516 \n",
      "     Training Step: 12 Training Loss: 3.247086524963379 \n",
      "     Training Step: 13 Training Loss: 2.4009439945220947 \n",
      "     Training Step: 14 Training Loss: 2.4703874588012695 \n",
      "     Training Step: 15 Training Loss: 2.548342227935791 \n",
      "     Training Step: 16 Training Loss: 2.68371844291687 \n",
      "     Training Step: 17 Training Loss: 2.3128602504730225 \n",
      "     Training Step: 18 Training Loss: 3.1249144077301025 \n",
      "     Training Step: 19 Training Loss: 4.88879919052124 \n",
      "     Training Step: 20 Training Loss: 2.279970169067383 \n",
      "     Training Step: 21 Training Loss: 2.11140775680542 \n",
      "     Training Step: 22 Training Loss: 3.869757652282715 \n",
      "     Training Step: 23 Training Loss: 3.8850865364074707 \n",
      "     Training Step: 24 Training Loss: 3.128032684326172 \n",
      "     Training Step: 25 Training Loss: 2.760316848754883 \n",
      "     Training Step: 26 Training Loss: 2.686063528060913 \n",
      "     Training Step: 27 Training Loss: 3.2947463989257812 \n",
      "     Training Step: 28 Training Loss: 2.019613742828369 \n",
      "     Training Step: 29 Training Loss: 3.0977962017059326 \n",
      "     Training Step: 30 Training Loss: 2.797518253326416 \n",
      "     Training Step: 31 Training Loss: 2.8776278495788574 \n",
      "     Training Step: 32 Training Loss: 3.264803409576416 \n",
      "     Training Step: 33 Training Loss: 4.304278373718262 \n",
      "     Training Step: 34 Training Loss: 3.322181224822998 \n",
      "     Training Step: 35 Training Loss: 3.5724940299987793 \n",
      "     Training Step: 36 Training Loss: 2.603191375732422 \n",
      "     Training Step: 37 Training Loss: 3.1316518783569336 \n",
      "     Training Step: 38 Training Loss: 2.8618762493133545 \n",
      "     Training Step: 39 Training Loss: 4.385018825531006 \n",
      "     Training Step: 40 Training Loss: 3.1509640216827393 \n",
      "     Training Step: 41 Training Loss: 3.4047012329101562 \n",
      "     Training Step: 42 Training Loss: 4.123673915863037 \n",
      "     Training Step: 43 Training Loss: 2.2597222328186035 \n",
      "     Training Step: 44 Training Loss: 2.168398380279541 \n",
      "     Training Step: 45 Training Loss: 2.7288177013397217 \n",
      "     Training Step: 46 Training Loss: 2.353954315185547 \n",
      "     Training Step: 47 Training Loss: 3.8118669986724854 \n",
      "     Training Step: 48 Training Loss: 2.590787887573242 \n",
      "     Training Step: 49 Training Loss: 2.9156930446624756 \n",
      "     Training Step: 50 Training Loss: 3.4370412826538086 \n",
      "     Training Step: 51 Training Loss: 3.1468663215637207 \n",
      "     Training Step: 52 Training Loss: 2.4026145935058594 \n",
      "     Training Step: 53 Training Loss: 2.386634349822998 \n",
      "     Training Step: 54 Training Loss: 3.3301401138305664 \n",
      "     Training Step: 55 Training Loss: 2.6038742065429688 \n",
      "     Training Step: 56 Training Loss: 2.6732845306396484 \n",
      "     Training Step: 57 Training Loss: 2.912163257598877 \n",
      "     Training Step: 58 Training Loss: 3.314389228820801 \n",
      "     Training Step: 59 Training Loss: 2.6630613803863525 \n",
      "     Training Step: 60 Training Loss: 4.229770660400391 \n",
      "     Training Step: 61 Training Loss: 3.732722520828247 \n",
      "     Training Step: 62 Training Loss: 4.008252143859863 \n",
      "     Training Step: 63 Training Loss: 3.2810702323913574 \n",
      "     Training Step: 64 Training Loss: 2.9044346809387207 \n",
      "     Training Step: 65 Training Loss: 2.511885643005371 \n",
      "     Training Step: 66 Training Loss: 2.227639675140381 \n",
      "     Training Step: 67 Training Loss: 2.861402988433838 \n",
      "     Training Step: 68 Training Loss: 3.497581958770752 \n",
      "     Training Step: 69 Training Loss: 3.1221320629119873 \n",
      "     Training Step: 70 Training Loss: 3.264446258544922 \n",
      "     Training Step: 71 Training Loss: 2.9446182250976562 \n",
      "     Training Step: 72 Training Loss: 3.601438522338867 \n",
      "     Training Step: 73 Training Loss: 2.5485610961914062 \n",
      "     Training Step: 74 Training Loss: 2.1035518646240234 \n",
      "     Training Step: 75 Training Loss: 2.3459856510162354 \n",
      "     Training Step: 76 Training Loss: 2.505093574523926 \n",
      "     Training Step: 77 Training Loss: 2.9928526878356934 \n",
      "     Training Step: 78 Training Loss: 3.43483304977417 \n",
      "     Training Step: 79 Training Loss: 3.210428237915039 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6165685653686523 \n",
      "     Validation Step: 1 Validation Loss: 3.5164248943328857 \n",
      "     Validation Step: 2 Validation Loss: 3.2221481800079346 \n",
      "     Validation Step: 3 Validation Loss: 2.7527668476104736 \n",
      "     Validation Step: 4 Validation Loss: 3.156486988067627 \n",
      "     Validation Step: 5 Validation Loss: 3.1023974418640137 \n",
      "     Validation Step: 6 Validation Loss: 3.6144471168518066 \n",
      "     Validation Step: 7 Validation Loss: 2.652003288269043 \n",
      "     Validation Step: 8 Validation Loss: 2.8518929481506348 \n",
      "     Validation Step: 9 Validation Loss: 3.0081958770751953 \n",
      "     Validation Step: 10 Validation Loss: 2.3142411708831787 \n",
      "     Validation Step: 11 Validation Loss: 3.893293619155884 \n",
      "     Validation Step: 12 Validation Loss: 2.9595751762390137 \n",
      "     Validation Step: 13 Validation Loss: 3.746692180633545 \n",
      "     Validation Step: 14 Validation Loss: 3.5386345386505127 \n",
      "Epoch: 103\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.648138999938965 \n",
      "     Training Step: 1 Training Loss: 2.9892947673797607 \n",
      "     Training Step: 2 Training Loss: 2.2768237590789795 \n",
      "     Training Step: 3 Training Loss: 2.799866199493408 \n",
      "     Training Step: 4 Training Loss: 2.4189629554748535 \n",
      "     Training Step: 5 Training Loss: 2.8882827758789062 \n",
      "     Training Step: 6 Training Loss: 3.9585225582122803 \n",
      "     Training Step: 7 Training Loss: 4.269007682800293 \n",
      "     Training Step: 8 Training Loss: 2.4144952297210693 \n",
      "     Training Step: 9 Training Loss: 4.6563310623168945 \n",
      "     Training Step: 10 Training Loss: 3.4763317108154297 \n",
      "     Training Step: 11 Training Loss: 3.3468780517578125 \n",
      "     Training Step: 12 Training Loss: 3.3446762561798096 \n",
      "     Training Step: 13 Training Loss: 3.1695046424865723 \n",
      "     Training Step: 14 Training Loss: 3.0171847343444824 \n",
      "     Training Step: 15 Training Loss: 3.352200508117676 \n",
      "     Training Step: 16 Training Loss: 2.998873472213745 \n",
      "     Training Step: 17 Training Loss: 3.386735439300537 \n",
      "     Training Step: 18 Training Loss: 2.8948752880096436 \n",
      "     Training Step: 19 Training Loss: 3.4816155433654785 \n",
      "     Training Step: 20 Training Loss: 2.2031679153442383 \n",
      "     Training Step: 21 Training Loss: 2.8191657066345215 \n",
      "     Training Step: 22 Training Loss: 3.3855624198913574 \n",
      "     Training Step: 23 Training Loss: 2.6728579998016357 \n",
      "     Training Step: 24 Training Loss: 2.7884268760681152 \n",
      "     Training Step: 25 Training Loss: 2.5616390705108643 \n",
      "     Training Step: 26 Training Loss: 2.2602274417877197 \n",
      "     Training Step: 27 Training Loss: 3.323695182800293 \n",
      "     Training Step: 28 Training Loss: 3.188131332397461 \n",
      "     Training Step: 29 Training Loss: 2.478773355484009 \n",
      "     Training Step: 30 Training Loss: 3.789468288421631 \n",
      "     Training Step: 31 Training Loss: 2.4470529556274414 \n",
      "     Training Step: 32 Training Loss: 4.3180952072143555 \n",
      "     Training Step: 33 Training Loss: 2.6905622482299805 \n",
      "     Training Step: 34 Training Loss: 2.7983803749084473 \n",
      "     Training Step: 35 Training Loss: 3.360830783843994 \n",
      "     Training Step: 36 Training Loss: 4.083382606506348 \n",
      "     Training Step: 37 Training Loss: 3.2307586669921875 \n",
      "     Training Step: 38 Training Loss: 2.388859272003174 \n",
      "     Training Step: 39 Training Loss: 2.8826651573181152 \n",
      "     Training Step: 40 Training Loss: 2.946993350982666 \n",
      "     Training Step: 41 Training Loss: 2.1932783126831055 \n",
      "     Training Step: 42 Training Loss: 3.3830959796905518 \n",
      "     Training Step: 43 Training Loss: 2.4144978523254395 \n",
      "     Training Step: 44 Training Loss: 2.3941166400909424 \n",
      "     Training Step: 45 Training Loss: 2.5171072483062744 \n",
      "     Training Step: 46 Training Loss: 3.056046485900879 \n",
      "     Training Step: 47 Training Loss: 3.1376700401306152 \n",
      "     Training Step: 48 Training Loss: 2.766237735748291 \n",
      "     Training Step: 49 Training Loss: 2.592337131500244 \n",
      "     Training Step: 50 Training Loss: 3.278775691986084 \n",
      "     Training Step: 51 Training Loss: 3.86395263671875 \n",
      "     Training Step: 52 Training Loss: 2.16896653175354 \n",
      "     Training Step: 53 Training Loss: 3.320070266723633 \n",
      "     Training Step: 54 Training Loss: 2.5186586380004883 \n",
      "     Training Step: 55 Training Loss: 2.8593835830688477 \n",
      "     Training Step: 56 Training Loss: 3.698887348175049 \n",
      "     Training Step: 57 Training Loss: 2.9580163955688477 \n",
      "     Training Step: 58 Training Loss: 3.2136096954345703 \n",
      "     Training Step: 59 Training Loss: 2.8642892837524414 \n",
      "     Training Step: 60 Training Loss: 2.330922842025757 \n",
      "     Training Step: 61 Training Loss: 3.437296152114868 \n",
      "     Training Step: 62 Training Loss: 2.1598846912384033 \n",
      "     Training Step: 63 Training Loss: 2.2632458209991455 \n",
      "     Training Step: 64 Training Loss: 2.8880136013031006 \n",
      "     Training Step: 65 Training Loss: 2.495447874069214 \n",
      "     Training Step: 66 Training Loss: 2.151644229888916 \n",
      "     Training Step: 67 Training Loss: 2.451550006866455 \n",
      "     Training Step: 68 Training Loss: 2.062305450439453 \n",
      "     Training Step: 69 Training Loss: 2.991849184036255 \n",
      "     Training Step: 70 Training Loss: 3.783809185028076 \n",
      "     Training Step: 71 Training Loss: 3.2997899055480957 \n",
      "     Training Step: 72 Training Loss: 3.0370612144470215 \n",
      "     Training Step: 73 Training Loss: 2.220616340637207 \n",
      "     Training Step: 74 Training Loss: 3.090393543243408 \n",
      "     Training Step: 75 Training Loss: 3.166991710662842 \n",
      "     Training Step: 76 Training Loss: 3.572793483734131 \n",
      "     Training Step: 77 Training Loss: 2.8608930110931396 \n",
      "     Training Step: 78 Training Loss: 3.4956488609313965 \n",
      "     Training Step: 79 Training Loss: 3.1344923973083496 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.193514823913574 \n",
      "     Validation Step: 1 Validation Loss: 3.089625835418701 \n",
      "     Validation Step: 2 Validation Loss: 4.0391621589660645 \n",
      "     Validation Step: 3 Validation Loss: 3.1712398529052734 \n",
      "     Validation Step: 4 Validation Loss: 3.6554532051086426 \n",
      "     Validation Step: 5 Validation Loss: 3.148606061935425 \n",
      "     Validation Step: 6 Validation Loss: 3.6471035480499268 \n",
      "     Validation Step: 7 Validation Loss: 3.608064889907837 \n",
      "     Validation Step: 8 Validation Loss: 2.848747491836548 \n",
      "     Validation Step: 9 Validation Loss: 3.7084648609161377 \n",
      "     Validation Step: 10 Validation Loss: 2.706368923187256 \n",
      "     Validation Step: 11 Validation Loss: 2.272247791290283 \n",
      "     Validation Step: 12 Validation Loss: 3.391892671585083 \n",
      "     Validation Step: 13 Validation Loss: 3.1126391887664795 \n",
      "     Validation Step: 14 Validation Loss: 2.73085880279541 \n",
      "Epoch: 104\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.844653844833374 \n",
      "     Training Step: 1 Training Loss: 3.009634017944336 \n",
      "     Training Step: 2 Training Loss: 2.414318561553955 \n",
      "     Training Step: 3 Training Loss: 3.1627614498138428 \n",
      "     Training Step: 4 Training Loss: 3.34763765335083 \n",
      "     Training Step: 5 Training Loss: 2.1049671173095703 \n",
      "     Training Step: 6 Training Loss: 3.845095634460449 \n",
      "     Training Step: 7 Training Loss: 2.515392303466797 \n",
      "     Training Step: 8 Training Loss: 2.5980277061462402 \n",
      "     Training Step: 9 Training Loss: 3.2495455741882324 \n",
      "     Training Step: 10 Training Loss: 3.368612289428711 \n",
      "     Training Step: 11 Training Loss: 2.777829170227051 \n",
      "     Training Step: 12 Training Loss: 2.704043388366699 \n",
      "     Training Step: 13 Training Loss: 3.4856467247009277 \n",
      "     Training Step: 14 Training Loss: 2.0925285816192627 \n",
      "     Training Step: 15 Training Loss: 2.775172710418701 \n",
      "     Training Step: 16 Training Loss: 3.0750508308410645 \n",
      "     Training Step: 17 Training Loss: 3.0514683723449707 \n",
      "     Training Step: 18 Training Loss: 2.873255729675293 \n",
      "     Training Step: 19 Training Loss: 2.8823435306549072 \n",
      "     Training Step: 20 Training Loss: 2.298971176147461 \n",
      "     Training Step: 21 Training Loss: 3.0446696281433105 \n",
      "     Training Step: 22 Training Loss: 3.2583224773406982 \n",
      "     Training Step: 23 Training Loss: 2.528074026107788 \n",
      "     Training Step: 24 Training Loss: 3.8751895427703857 \n",
      "     Training Step: 25 Training Loss: 3.1331465244293213 \n",
      "     Training Step: 26 Training Loss: 2.909620761871338 \n",
      "     Training Step: 27 Training Loss: 3.203841209411621 \n",
      "     Training Step: 28 Training Loss: 2.6844305992126465 \n",
      "     Training Step: 29 Training Loss: 2.968883514404297 \n",
      "     Training Step: 30 Training Loss: 2.4032535552978516 \n",
      "     Training Step: 31 Training Loss: 3.3355050086975098 \n",
      "     Training Step: 32 Training Loss: 4.416654109954834 \n",
      "     Training Step: 33 Training Loss: 2.5274627208709717 \n",
      "     Training Step: 34 Training Loss: 2.3615946769714355 \n",
      "     Training Step: 35 Training Loss: 2.1793270111083984 \n",
      "     Training Step: 36 Training Loss: 2.805704355239868 \n",
      "     Training Step: 37 Training Loss: 2.7237532138824463 \n",
      "     Training Step: 38 Training Loss: 2.571726083755493 \n",
      "     Training Step: 39 Training Loss: 2.8222599029541016 \n",
      "     Training Step: 40 Training Loss: 2.8553051948547363 \n",
      "     Training Step: 41 Training Loss: 2.4203662872314453 \n",
      "     Training Step: 42 Training Loss: 4.214142322540283 \n",
      "     Training Step: 43 Training Loss: 2.7900562286376953 \n",
      "     Training Step: 44 Training Loss: 3.3696999549865723 \n",
      "     Training Step: 45 Training Loss: 3.5196433067321777 \n",
      "     Training Step: 46 Training Loss: 2.031001567840576 \n",
      "     Training Step: 47 Training Loss: 3.597087860107422 \n",
      "     Training Step: 48 Training Loss: 3.6659700870513916 \n",
      "     Training Step: 49 Training Loss: 3.092038154602051 \n",
      "     Training Step: 50 Training Loss: 2.248385190963745 \n",
      "     Training Step: 51 Training Loss: 2.3184616565704346 \n",
      "     Training Step: 52 Training Loss: 2.28808331489563 \n",
      "     Training Step: 53 Training Loss: 3.879870891571045 \n",
      "     Training Step: 54 Training Loss: 2.235431432723999 \n",
      "     Training Step: 55 Training Loss: 3.321155071258545 \n",
      "     Training Step: 56 Training Loss: 3.280120611190796 \n",
      "     Training Step: 57 Training Loss: 2.6782727241516113 \n",
      "     Training Step: 58 Training Loss: 3.1032140254974365 \n",
      "     Training Step: 59 Training Loss: 3.1449475288391113 \n",
      "     Training Step: 60 Training Loss: 4.144785404205322 \n",
      "     Training Step: 61 Training Loss: 2.972330093383789 \n",
      "     Training Step: 62 Training Loss: 2.8669803142547607 \n",
      "     Training Step: 63 Training Loss: 3.3711276054382324 \n",
      "     Training Step: 64 Training Loss: 2.785025119781494 \n",
      "     Training Step: 65 Training Loss: 2.0766806602478027 \n",
      "     Training Step: 66 Training Loss: 2.748809576034546 \n",
      "     Training Step: 67 Training Loss: 4.331437110900879 \n",
      "     Training Step: 68 Training Loss: 2.708777666091919 \n",
      "     Training Step: 69 Training Loss: 3.1863598823547363 \n",
      "     Training Step: 70 Training Loss: 3.4005520343780518 \n",
      "     Training Step: 71 Training Loss: 3.8390581607818604 \n",
      "     Training Step: 72 Training Loss: 3.0232276916503906 \n",
      "     Training Step: 73 Training Loss: 2.821971893310547 \n",
      "     Training Step: 74 Training Loss: 3.3777456283569336 \n",
      "     Training Step: 75 Training Loss: 4.699954032897949 \n",
      "     Training Step: 76 Training Loss: 2.55487060546875 \n",
      "     Training Step: 77 Training Loss: 2.446608543395996 \n",
      "     Training Step: 78 Training Loss: 2.98185396194458 \n",
      "     Training Step: 79 Training Loss: 2.957113265991211 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6135497093200684 \n",
      "     Validation Step: 1 Validation Loss: 3.0272293090820312 \n",
      "     Validation Step: 2 Validation Loss: 2.298826217651367 \n",
      "     Validation Step: 3 Validation Loss: 3.5106866359710693 \n",
      "     Validation Step: 4 Validation Loss: 3.310067653656006 \n",
      "     Validation Step: 5 Validation Loss: 3.522737979888916 \n",
      "     Validation Step: 6 Validation Loss: 3.7382748126983643 \n",
      "     Validation Step: 7 Validation Loss: 3.647136926651001 \n",
      "     Validation Step: 8 Validation Loss: 2.6746137142181396 \n",
      "     Validation Step: 9 Validation Loss: 2.9829044342041016 \n",
      "     Validation Step: 10 Validation Loss: 3.007905960083008 \n",
      "     Validation Step: 11 Validation Loss: 3.840679168701172 \n",
      "     Validation Step: 12 Validation Loss: 3.131558895111084 \n",
      "     Validation Step: 13 Validation Loss: 2.845050811767578 \n",
      "     Validation Step: 14 Validation Loss: 2.5810890197753906 \n",
      "Epoch: 105\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.0312955379486084 \n",
      "     Training Step: 1 Training Loss: 2.497365951538086 \n",
      "     Training Step: 2 Training Loss: 2.679490804672241 \n",
      "     Training Step: 3 Training Loss: 3.7252962589263916 \n",
      "     Training Step: 4 Training Loss: 3.9309134483337402 \n",
      "     Training Step: 5 Training Loss: 3.2401576042175293 \n",
      "     Training Step: 6 Training Loss: 2.6963818073272705 \n",
      "     Training Step: 7 Training Loss: 2.8472487926483154 \n",
      "     Training Step: 8 Training Loss: 2.269838333129883 \n",
      "     Training Step: 9 Training Loss: 2.986513614654541 \n",
      "     Training Step: 10 Training Loss: 2.203761577606201 \n",
      "     Training Step: 11 Training Loss: 2.7110445499420166 \n",
      "     Training Step: 12 Training Loss: 2.5165436267852783 \n",
      "     Training Step: 13 Training Loss: 2.468012571334839 \n",
      "     Training Step: 14 Training Loss: 2.813955783843994 \n",
      "     Training Step: 15 Training Loss: 2.8784570693969727 \n",
      "     Training Step: 16 Training Loss: 2.2854442596435547 \n",
      "     Training Step: 17 Training Loss: 3.273512363433838 \n",
      "     Training Step: 18 Training Loss: 2.286195755004883 \n",
      "     Training Step: 19 Training Loss: 3.2067158222198486 \n",
      "     Training Step: 20 Training Loss: 3.2978005409240723 \n",
      "     Training Step: 21 Training Loss: 3.0785107612609863 \n",
      "     Training Step: 22 Training Loss: 2.6303937435150146 \n",
      "     Training Step: 23 Training Loss: 3.264955520629883 \n",
      "     Training Step: 24 Training Loss: 3.341789960861206 \n",
      "     Training Step: 25 Training Loss: 4.298022747039795 \n",
      "     Training Step: 26 Training Loss: 4.33636474609375 \n",
      "     Training Step: 27 Training Loss: 3.5382556915283203 \n",
      "     Training Step: 28 Training Loss: 3.4535224437713623 \n",
      "     Training Step: 29 Training Loss: 2.1637771129608154 \n",
      "     Training Step: 30 Training Loss: 3.3866970539093018 \n",
      "     Training Step: 31 Training Loss: 3.6486358642578125 \n",
      "     Training Step: 32 Training Loss: 3.785438060760498 \n",
      "     Training Step: 33 Training Loss: 2.5676021575927734 \n",
      "     Training Step: 34 Training Loss: 3.1130213737487793 \n",
      "     Training Step: 35 Training Loss: 3.200063705444336 \n",
      "     Training Step: 36 Training Loss: 2.9048914909362793 \n",
      "     Training Step: 37 Training Loss: 2.8692502975463867 \n",
      "     Training Step: 38 Training Loss: 2.407701015472412 \n",
      "     Training Step: 39 Training Loss: 2.237729072570801 \n",
      "     Training Step: 40 Training Loss: 3.5480313301086426 \n",
      "     Training Step: 41 Training Loss: 3.2945170402526855 \n",
      "     Training Step: 42 Training Loss: 2.0863332748413086 \n",
      "     Training Step: 43 Training Loss: 2.570420742034912 \n",
      "     Training Step: 44 Training Loss: 3.5298399925231934 \n",
      "     Training Step: 45 Training Loss: 2.929485321044922 \n",
      "     Training Step: 46 Training Loss: 3.3268165588378906 \n",
      "     Training Step: 47 Training Loss: 3.9487195014953613 \n",
      "     Training Step: 48 Training Loss: 2.5739755630493164 \n",
      "     Training Step: 49 Training Loss: 2.9825024604797363 \n",
      "     Training Step: 50 Training Loss: 2.4128270149230957 \n",
      "     Training Step: 51 Training Loss: 3.378995180130005 \n",
      "     Training Step: 52 Training Loss: 3.158094882965088 \n",
      "     Training Step: 53 Training Loss: 3.360520124435425 \n",
      "     Training Step: 54 Training Loss: 4.735495090484619 \n",
      "     Training Step: 55 Training Loss: 3.4151740074157715 \n",
      "     Training Step: 56 Training Loss: 2.693044424057007 \n",
      "     Training Step: 57 Training Loss: 2.2233047485351562 \n",
      "     Training Step: 58 Training Loss: 2.1618711948394775 \n",
      "     Training Step: 59 Training Loss: 3.3031818866729736 \n",
      "     Training Step: 60 Training Loss: 3.3919615745544434 \n",
      "     Training Step: 61 Training Loss: 2.103034734725952 \n",
      "     Training Step: 62 Training Loss: 2.553053855895996 \n",
      "     Training Step: 63 Training Loss: 3.812894821166992 \n",
      "     Training Step: 64 Training Loss: 3.188171863555908 \n",
      "     Training Step: 65 Training Loss: 4.333383560180664 \n",
      "     Training Step: 66 Training Loss: 3.260923385620117 \n",
      "     Training Step: 67 Training Loss: 2.8927149772644043 \n",
      "     Training Step: 68 Training Loss: 2.790147304534912 \n",
      "     Training Step: 69 Training Loss: 2.8329718112945557 \n",
      "     Training Step: 70 Training Loss: 3.00767183303833 \n",
      "     Training Step: 71 Training Loss: 2.379845380783081 \n",
      "     Training Step: 72 Training Loss: 2.495676279067993 \n",
      "     Training Step: 73 Training Loss: 2.992323875427246 \n",
      "     Training Step: 74 Training Loss: 2.86972975730896 \n",
      "     Training Step: 75 Training Loss: 2.3685765266418457 \n",
      "     Training Step: 76 Training Loss: 2.969572067260742 \n",
      "     Training Step: 77 Training Loss: 2.8950681686401367 \n",
      "     Training Step: 78 Training Loss: 2.401794195175171 \n",
      "     Training Step: 79 Training Loss: 2.98016095161438 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.257733106613159 \n",
      "     Validation Step: 1 Validation Loss: 3.873067855834961 \n",
      "     Validation Step: 2 Validation Loss: 3.514416456222534 \n",
      "     Validation Step: 3 Validation Loss: 2.595580816268921 \n",
      "     Validation Step: 4 Validation Loss: 2.684910297393799 \n",
      "     Validation Step: 5 Validation Loss: 2.998843193054199 \n",
      "     Validation Step: 6 Validation Loss: 3.1151976585388184 \n",
      "     Validation Step: 7 Validation Loss: 2.831960678100586 \n",
      "     Validation Step: 8 Validation Loss: 3.5890872478485107 \n",
      "     Validation Step: 9 Validation Loss: 3.4902586936950684 \n",
      "     Validation Step: 10 Validation Loss: 2.3489413261413574 \n",
      "     Validation Step: 11 Validation Loss: 3.5567867755889893 \n",
      "     Validation Step: 12 Validation Loss: 3.735220432281494 \n",
      "     Validation Step: 13 Validation Loss: 3.045626163482666 \n",
      "     Validation Step: 14 Validation Loss: 3.0297341346740723 \n",
      "Epoch: 106\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.31824254989624 \n",
      "     Training Step: 1 Training Loss: 2.409783363342285 \n",
      "     Training Step: 2 Training Loss: 2.790457010269165 \n",
      "     Training Step: 3 Training Loss: 2.3644440174102783 \n",
      "     Training Step: 4 Training Loss: 2.268383502960205 \n",
      "     Training Step: 5 Training Loss: 2.3838210105895996 \n",
      "     Training Step: 6 Training Loss: 2.6898765563964844 \n",
      "     Training Step: 7 Training Loss: 2.7594826221466064 \n",
      "     Training Step: 8 Training Loss: 2.9485418796539307 \n",
      "     Training Step: 9 Training Loss: 3.862377643585205 \n",
      "     Training Step: 10 Training Loss: 3.0865273475646973 \n",
      "     Training Step: 11 Training Loss: 2.678732395172119 \n",
      "     Training Step: 12 Training Loss: 2.820394515991211 \n",
      "     Training Step: 13 Training Loss: 4.811887741088867 \n",
      "     Training Step: 14 Training Loss: 2.6212878227233887 \n",
      "     Training Step: 15 Training Loss: 3.4266703128814697 \n",
      "     Training Step: 16 Training Loss: 3.413390636444092 \n",
      "     Training Step: 17 Training Loss: 3.191603183746338 \n",
      "     Training Step: 18 Training Loss: 2.5185773372650146 \n",
      "     Training Step: 19 Training Loss: 2.1261868476867676 \n",
      "     Training Step: 20 Training Loss: 3.009683609008789 \n",
      "     Training Step: 21 Training Loss: 3.4427762031555176 \n",
      "     Training Step: 22 Training Loss: 3.1757278442382812 \n",
      "     Training Step: 23 Training Loss: 3.4771459102630615 \n",
      "     Training Step: 24 Training Loss: 2.8645074367523193 \n",
      "     Training Step: 25 Training Loss: 2.3143441677093506 \n",
      "     Training Step: 26 Training Loss: 2.4620461463928223 \n",
      "     Training Step: 27 Training Loss: 3.0199713706970215 \n",
      "     Training Step: 28 Training Loss: 3.200773239135742 \n",
      "     Training Step: 29 Training Loss: 3.2289175987243652 \n",
      "     Training Step: 30 Training Loss: 3.3099913597106934 \n",
      "     Training Step: 31 Training Loss: 3.090909957885742 \n",
      "     Training Step: 32 Training Loss: 3.1345882415771484 \n",
      "     Training Step: 33 Training Loss: 3.240922451019287 \n",
      "     Training Step: 34 Training Loss: 3.11160945892334 \n",
      "     Training Step: 35 Training Loss: 3.5591511726379395 \n",
      "     Training Step: 36 Training Loss: 4.267612934112549 \n",
      "     Training Step: 37 Training Loss: 2.684040069580078 \n",
      "     Training Step: 38 Training Loss: 3.120546579360962 \n",
      "     Training Step: 39 Training Loss: 2.528066873550415 \n",
      "     Training Step: 40 Training Loss: 2.82912540435791 \n",
      "     Training Step: 41 Training Loss: 2.700023651123047 \n",
      "     Training Step: 42 Training Loss: 3.5006422996520996 \n",
      "     Training Step: 43 Training Loss: 3.056896924972534 \n",
      "     Training Step: 44 Training Loss: 4.298721790313721 \n",
      "     Training Step: 45 Training Loss: 2.881610870361328 \n",
      "     Training Step: 46 Training Loss: 3.0488476753234863 \n",
      "     Training Step: 47 Training Loss: 2.1143126487731934 \n",
      "     Training Step: 48 Training Loss: 3.5300188064575195 \n",
      "     Training Step: 49 Training Loss: 2.2932186126708984 \n",
      "     Training Step: 50 Training Loss: 4.368923187255859 \n",
      "     Training Step: 51 Training Loss: 2.216409206390381 \n",
      "     Training Step: 52 Training Loss: 3.005960464477539 \n",
      "     Training Step: 53 Training Loss: 2.69663405418396 \n",
      "     Training Step: 54 Training Loss: 2.479884624481201 \n",
      "     Training Step: 55 Training Loss: 2.7888529300689697 \n",
      "     Training Step: 56 Training Loss: 3.5655012130737305 \n",
      "     Training Step: 57 Training Loss: 3.754546642303467 \n",
      "     Training Step: 58 Training Loss: 3.131117820739746 \n",
      "     Training Step: 59 Training Loss: 2.1603293418884277 \n",
      "     Training Step: 60 Training Loss: 2.490668773651123 \n",
      "     Training Step: 61 Training Loss: 3.086738109588623 \n",
      "     Training Step: 62 Training Loss: 2.373063087463379 \n",
      "     Training Step: 63 Training Loss: 2.409191608428955 \n",
      "     Training Step: 64 Training Loss: 2.93424916267395 \n",
      "     Training Step: 65 Training Loss: 2.505892038345337 \n",
      "     Training Step: 66 Training Loss: 3.9475674629211426 \n",
      "     Training Step: 67 Training Loss: 2.484174966812134 \n",
      "     Training Step: 68 Training Loss: 2.8043198585510254 \n",
      "     Training Step: 69 Training Loss: 2.475515842437744 \n",
      "     Training Step: 70 Training Loss: 2.91776704788208 \n",
      "     Training Step: 71 Training Loss: 3.6150436401367188 \n",
      "     Training Step: 72 Training Loss: 2.527533769607544 \n",
      "     Training Step: 73 Training Loss: 2.130436420440674 \n",
      "     Training Step: 74 Training Loss: 2.4207816123962402 \n",
      "     Training Step: 75 Training Loss: 2.946000576019287 \n",
      "     Training Step: 76 Training Loss: 3.307222604751587 \n",
      "     Training Step: 77 Training Loss: 3.7562432289123535 \n",
      "     Training Step: 78 Training Loss: 2.8162498474121094 \n",
      "     Training Step: 79 Training Loss: 2.2845311164855957 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.3142292499542236 \n",
      "     Validation Step: 1 Validation Loss: 2.7094669342041016 \n",
      "     Validation Step: 2 Validation Loss: 2.763568878173828 \n",
      "     Validation Step: 3 Validation Loss: 3.6142690181732178 \n",
      "     Validation Step: 4 Validation Loss: 2.7732033729553223 \n",
      "     Validation Step: 5 Validation Loss: 3.169400691986084 \n",
      "     Validation Step: 6 Validation Loss: 3.5586092472076416 \n",
      "     Validation Step: 7 Validation Loss: 3.759159803390503 \n",
      "     Validation Step: 8 Validation Loss: 3.6247003078460693 \n",
      "     Validation Step: 9 Validation Loss: 3.1004533767700195 \n",
      "     Validation Step: 10 Validation Loss: 2.5867154598236084 \n",
      "     Validation Step: 11 Validation Loss: 3.4638915061950684 \n",
      "     Validation Step: 12 Validation Loss: 3.3420181274414062 \n",
      "     Validation Step: 13 Validation Loss: 2.30340576171875 \n",
      "     Validation Step: 14 Validation Loss: 2.6589322090148926 \n",
      "Epoch: 107\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.424025297164917 \n",
      "     Training Step: 1 Training Loss: 2.7562594413757324 \n",
      "     Training Step: 2 Training Loss: 3.493189811706543 \n",
      "     Training Step: 3 Training Loss: 3.412118434906006 \n",
      "     Training Step: 4 Training Loss: 3.2106804847717285 \n",
      "     Training Step: 5 Training Loss: 3.666934013366699 \n",
      "     Training Step: 6 Training Loss: 2.4150004386901855 \n",
      "     Training Step: 7 Training Loss: 3.034579277038574 \n",
      "     Training Step: 8 Training Loss: 4.137019634246826 \n",
      "     Training Step: 9 Training Loss: 2.619919776916504 \n",
      "     Training Step: 10 Training Loss: 3.086203098297119 \n",
      "     Training Step: 11 Training Loss: 2.092261552810669 \n",
      "     Training Step: 12 Training Loss: 2.4787418842315674 \n",
      "     Training Step: 13 Training Loss: 2.1239819526672363 \n",
      "     Training Step: 14 Training Loss: 2.255584239959717 \n",
      "     Training Step: 15 Training Loss: 2.5380754470825195 \n",
      "     Training Step: 16 Training Loss: 2.894542694091797 \n",
      "     Training Step: 17 Training Loss: 2.1491575241088867 \n",
      "     Training Step: 18 Training Loss: 2.290374517440796 \n",
      "     Training Step: 19 Training Loss: 2.4125819206237793 \n",
      "     Training Step: 20 Training Loss: 2.7900922298431396 \n",
      "     Training Step: 21 Training Loss: 2.896637439727783 \n",
      "     Training Step: 22 Training Loss: 2.3840999603271484 \n",
      "     Training Step: 23 Training Loss: 3.0309219360351562 \n",
      "     Training Step: 24 Training Loss: 4.0980730056762695 \n",
      "     Training Step: 25 Training Loss: 3.8061108589172363 \n",
      "     Training Step: 26 Training Loss: 2.4452321529388428 \n",
      "     Training Step: 27 Training Loss: 2.9433093070983887 \n",
      "     Training Step: 28 Training Loss: 2.2557263374328613 \n",
      "     Training Step: 29 Training Loss: 3.151327610015869 \n",
      "     Training Step: 30 Training Loss: 2.753171920776367 \n",
      "     Training Step: 31 Training Loss: 2.7458553314208984 \n",
      "     Training Step: 32 Training Loss: 2.2492928504943848 \n",
      "     Training Step: 33 Training Loss: 4.4817352294921875 \n",
      "     Training Step: 34 Training Loss: 2.808375120162964 \n",
      "     Training Step: 35 Training Loss: 2.623173236846924 \n",
      "     Training Step: 36 Training Loss: 2.474823474884033 \n",
      "     Training Step: 37 Training Loss: 3.142068862915039 \n",
      "     Training Step: 38 Training Loss: 2.5516536235809326 \n",
      "     Training Step: 39 Training Loss: 3.1639366149902344 \n",
      "     Training Step: 40 Training Loss: 3.7822532653808594 \n",
      "     Training Step: 41 Training Loss: 2.9346373081207275 \n",
      "     Training Step: 42 Training Loss: 3.410726547241211 \n",
      "     Training Step: 43 Training Loss: 3.7653019428253174 \n",
      "     Training Step: 44 Training Loss: 3.4967150688171387 \n",
      "     Training Step: 45 Training Loss: 2.320300817489624 \n",
      "     Training Step: 46 Training Loss: 2.930216073989868 \n",
      "     Training Step: 47 Training Loss: 3.0022315979003906 \n",
      "     Training Step: 48 Training Loss: 2.66713809967041 \n",
      "     Training Step: 49 Training Loss: 4.732925891876221 \n",
      "     Training Step: 50 Training Loss: 3.9920742511749268 \n",
      "     Training Step: 51 Training Loss: 3.305591344833374 \n",
      "     Training Step: 52 Training Loss: 3.427945613861084 \n",
      "     Training Step: 53 Training Loss: 2.1032299995422363 \n",
      "     Training Step: 54 Training Loss: 3.238807201385498 \n",
      "     Training Step: 55 Training Loss: 3.389700412750244 \n",
      "     Training Step: 56 Training Loss: 3.5064258575439453 \n",
      "     Training Step: 57 Training Loss: 3.4420008659362793 \n",
      "     Training Step: 58 Training Loss: 2.609666347503662 \n",
      "     Training Step: 59 Training Loss: 3.3873565196990967 \n",
      "     Training Step: 60 Training Loss: 2.7717337608337402 \n",
      "     Training Step: 61 Training Loss: 3.490050792694092 \n",
      "     Training Step: 62 Training Loss: 2.5313568115234375 \n",
      "     Training Step: 63 Training Loss: 3.517199754714966 \n",
      "     Training Step: 64 Training Loss: 2.4571945667266846 \n",
      "     Training Step: 65 Training Loss: 2.182992458343506 \n",
      "     Training Step: 66 Training Loss: 3.2815749645233154 \n",
      "     Training Step: 67 Training Loss: 3.0247600078582764 \n",
      "     Training Step: 68 Training Loss: 3.0492115020751953 \n",
      "     Training Step: 69 Training Loss: 2.960871458053589 \n",
      "     Training Step: 70 Training Loss: 3.4074394702911377 \n",
      "     Training Step: 71 Training Loss: 3.180267333984375 \n",
      "     Training Step: 72 Training Loss: 2.6749467849731445 \n",
      "     Training Step: 73 Training Loss: 2.3006656169891357 \n",
      "     Training Step: 74 Training Loss: 2.945425510406494 \n",
      "     Training Step: 75 Training Loss: 2.895139217376709 \n",
      "     Training Step: 76 Training Loss: 4.3114013671875 \n",
      "     Training Step: 77 Training Loss: 3.6234328746795654 \n",
      "     Training Step: 78 Training Loss: 3.4240994453430176 \n",
      "     Training Step: 79 Training Loss: 2.5422420501708984 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.741196632385254 \n",
      "     Validation Step: 1 Validation Loss: 3.123640298843384 \n",
      "     Validation Step: 2 Validation Loss: 3.0390892028808594 \n",
      "     Validation Step: 3 Validation Loss: 3.2222228050231934 \n",
      "     Validation Step: 4 Validation Loss: 2.7278084754943848 \n",
      "     Validation Step: 5 Validation Loss: 3.435080051422119 \n",
      "     Validation Step: 6 Validation Loss: 3.1684210300445557 \n",
      "     Validation Step: 7 Validation Loss: 3.063100576400757 \n",
      "     Validation Step: 8 Validation Loss: 3.6655781269073486 \n",
      "     Validation Step: 9 Validation Loss: 2.7345476150512695 \n",
      "     Validation Step: 10 Validation Loss: 3.622384786605835 \n",
      "     Validation Step: 11 Validation Loss: 2.851771593093872 \n",
      "     Validation Step: 12 Validation Loss: 2.343575954437256 \n",
      "     Validation Step: 13 Validation Loss: 3.8603036403656006 \n",
      "     Validation Step: 14 Validation Loss: 3.626389980316162 \n",
      "Epoch: 108\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.459083080291748 \n",
      "     Training Step: 1 Training Loss: 2.6555593013763428 \n",
      "     Training Step: 2 Training Loss: 4.31363582611084 \n",
      "     Training Step: 3 Training Loss: 2.657334566116333 \n",
      "     Training Step: 4 Training Loss: 2.5623817443847656 \n",
      "     Training Step: 5 Training Loss: 2.443913698196411 \n",
      "     Training Step: 6 Training Loss: 3.393543004989624 \n",
      "     Training Step: 7 Training Loss: 2.8438405990600586 \n",
      "     Training Step: 8 Training Loss: 2.5134503841400146 \n",
      "     Training Step: 9 Training Loss: 3.2452404499053955 \n",
      "     Training Step: 10 Training Loss: 3.235895872116089 \n",
      "     Training Step: 11 Training Loss: 2.8303067684173584 \n",
      "     Training Step: 12 Training Loss: 3.4745185375213623 \n",
      "     Training Step: 13 Training Loss: 3.045832633972168 \n",
      "     Training Step: 14 Training Loss: 2.248875141143799 \n",
      "     Training Step: 15 Training Loss: 2.9846818447113037 \n",
      "     Training Step: 16 Training Loss: 2.1920695304870605 \n",
      "     Training Step: 17 Training Loss: 2.5135934352874756 \n",
      "     Training Step: 18 Training Loss: 2.9668848514556885 \n",
      "     Training Step: 19 Training Loss: 2.1077401638031006 \n",
      "     Training Step: 20 Training Loss: 3.7303061485290527 \n",
      "     Training Step: 21 Training Loss: 3.459404230117798 \n",
      "     Training Step: 22 Training Loss: 4.28342866897583 \n",
      "     Training Step: 23 Training Loss: 2.1615958213806152 \n",
      "     Training Step: 24 Training Loss: 2.763652801513672 \n",
      "     Training Step: 25 Training Loss: 2.2779576778411865 \n",
      "     Training Step: 26 Training Loss: 3.295576333999634 \n",
      "     Training Step: 27 Training Loss: 2.6028342247009277 \n",
      "     Training Step: 28 Training Loss: 3.373624324798584 \n",
      "     Training Step: 29 Training Loss: 3.013061285018921 \n",
      "     Training Step: 30 Training Loss: 2.5748610496520996 \n",
      "     Training Step: 31 Training Loss: 3.8558707237243652 \n",
      "     Training Step: 32 Training Loss: 2.562105178833008 \n",
      "     Training Step: 33 Training Loss: 4.074610710144043 \n",
      "     Training Step: 34 Training Loss: 4.56192684173584 \n",
      "     Training Step: 35 Training Loss: 2.748823642730713 \n",
      "     Training Step: 36 Training Loss: 3.506138801574707 \n",
      "     Training Step: 37 Training Loss: 2.9686920642852783 \n",
      "     Training Step: 38 Training Loss: 2.4404327869415283 \n",
      "     Training Step: 39 Training Loss: 3.6092405319213867 \n",
      "     Training Step: 40 Training Loss: 2.209695816040039 \n",
      "     Training Step: 41 Training Loss: 2.6291098594665527 \n",
      "     Training Step: 42 Training Loss: 3.5798075199127197 \n",
      "     Training Step: 43 Training Loss: 3.003260374069214 \n",
      "     Training Step: 44 Training Loss: 3.001732349395752 \n",
      "     Training Step: 45 Training Loss: 2.7560057640075684 \n",
      "     Training Step: 46 Training Loss: 2.7118496894836426 \n",
      "     Training Step: 47 Training Loss: 2.0886893272399902 \n",
      "     Training Step: 48 Training Loss: 2.135237216949463 \n",
      "     Training Step: 49 Training Loss: 2.9269373416900635 \n",
      "     Training Step: 50 Training Loss: 2.5668702125549316 \n",
      "     Training Step: 51 Training Loss: 3.0190634727478027 \n",
      "     Training Step: 52 Training Loss: 3.465615749359131 \n",
      "     Training Step: 53 Training Loss: 3.3109121322631836 \n",
      "     Training Step: 54 Training Loss: 3.015867233276367 \n",
      "     Training Step: 55 Training Loss: 3.3831324577331543 \n",
      "     Training Step: 56 Training Loss: 3.1532490253448486 \n",
      "     Training Step: 57 Training Loss: 3.4087750911712646 \n",
      "     Training Step: 58 Training Loss: 2.9238994121551514 \n",
      "     Training Step: 59 Training Loss: 2.9334518909454346 \n",
      "     Training Step: 60 Training Loss: 3.139880418777466 \n",
      "     Training Step: 61 Training Loss: 4.31405782699585 \n",
      "     Training Step: 62 Training Loss: 3.396536350250244 \n",
      "     Training Step: 63 Training Loss: 3.3391523361206055 \n",
      "     Training Step: 64 Training Loss: 3.7819247245788574 \n",
      "     Training Step: 65 Training Loss: 3.522488594055176 \n",
      "     Training Step: 66 Training Loss: 3.300093173980713 \n",
      "     Training Step: 67 Training Loss: 2.422318458557129 \n",
      "     Training Step: 68 Training Loss: 2.9614624977111816 \n",
      "     Training Step: 69 Training Loss: 2.6072921752929688 \n",
      "     Training Step: 70 Training Loss: 2.798790216445923 \n",
      "     Training Step: 71 Training Loss: 3.0411462783813477 \n",
      "     Training Step: 72 Training Loss: 2.6312098503112793 \n",
      "     Training Step: 73 Training Loss: 2.3651645183563232 \n",
      "     Training Step: 74 Training Loss: 3.4521560668945312 \n",
      "     Training Step: 75 Training Loss: 2.2630844116210938 \n",
      "     Training Step: 76 Training Loss: 2.6578078269958496 \n",
      "     Training Step: 77 Training Loss: 2.600497007369995 \n",
      "     Training Step: 78 Training Loss: 3.365212917327881 \n",
      "     Training Step: 79 Training Loss: 2.0741825103759766 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.7399401664733887 \n",
      "     Validation Step: 1 Validation Loss: 2.8345093727111816 \n",
      "     Validation Step: 2 Validation Loss: 3.203923225402832 \n",
      "     Validation Step: 3 Validation Loss: 3.7034592628479004 \n",
      "     Validation Step: 4 Validation Loss: 3.6741764545440674 \n",
      "     Validation Step: 5 Validation Loss: 3.1805953979492188 \n",
      "     Validation Step: 6 Validation Loss: 3.0757710933685303 \n",
      "     Validation Step: 7 Validation Loss: 3.973179340362549 \n",
      "     Validation Step: 8 Validation Loss: 3.1432290077209473 \n",
      "     Validation Step: 9 Validation Loss: 3.3866546154022217 \n",
      "     Validation Step: 10 Validation Loss: 3.071683406829834 \n",
      "     Validation Step: 11 Validation Loss: 2.7176461219787598 \n",
      "     Validation Step: 12 Validation Loss: 3.6762404441833496 \n",
      "     Validation Step: 13 Validation Loss: 2.8941471576690674 \n",
      "     Validation Step: 14 Validation Loss: 2.2865121364593506 \n",
      "Epoch: 109\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.079787254333496 \n",
      "     Training Step: 1 Training Loss: 3.1119656562805176 \n",
      "     Training Step: 2 Training Loss: 4.0956268310546875 \n",
      "     Training Step: 3 Training Loss: 2.9665985107421875 \n",
      "     Training Step: 4 Training Loss: 3.3276288509368896 \n",
      "     Training Step: 5 Training Loss: 2.4051616191864014 \n",
      "     Training Step: 6 Training Loss: 3.405362606048584 \n",
      "     Training Step: 7 Training Loss: 2.834078311920166 \n",
      "     Training Step: 8 Training Loss: 3.4448001384735107 \n",
      "     Training Step: 9 Training Loss: 2.7862660884857178 \n",
      "     Training Step: 10 Training Loss: 2.686722755432129 \n",
      "     Training Step: 11 Training Loss: 3.1942567825317383 \n",
      "     Training Step: 12 Training Loss: 2.103095769882202 \n",
      "     Training Step: 13 Training Loss: 4.759999752044678 \n",
      "     Training Step: 14 Training Loss: 2.902834415435791 \n",
      "     Training Step: 15 Training Loss: 3.3925538063049316 \n",
      "     Training Step: 16 Training Loss: 3.0105278491973877 \n",
      "     Training Step: 17 Training Loss: 3.3037490844726562 \n",
      "     Training Step: 18 Training Loss: 3.920823097229004 \n",
      "     Training Step: 19 Training Loss: 2.5497050285339355 \n",
      "     Training Step: 20 Training Loss: 2.547410488128662 \n",
      "     Training Step: 21 Training Loss: 2.539055585861206 \n",
      "     Training Step: 22 Training Loss: 2.141726493835449 \n",
      "     Training Step: 23 Training Loss: 3.226670265197754 \n",
      "     Training Step: 24 Training Loss: 2.8509764671325684 \n",
      "     Training Step: 25 Training Loss: 2.385498285293579 \n",
      "     Training Step: 26 Training Loss: 3.376948118209839 \n",
      "     Training Step: 27 Training Loss: 3.7921910285949707 \n",
      "     Training Step: 28 Training Loss: 3.895643711090088 \n",
      "     Training Step: 29 Training Loss: 2.3968167304992676 \n",
      "     Training Step: 30 Training Loss: 3.047213554382324 \n",
      "     Training Step: 31 Training Loss: 2.9769704341888428 \n",
      "     Training Step: 32 Training Loss: 2.1642532348632812 \n",
      "     Training Step: 33 Training Loss: 3.0964841842651367 \n",
      "     Training Step: 34 Training Loss: 2.7595152854919434 \n",
      "     Training Step: 35 Training Loss: 3.484549045562744 \n",
      "     Training Step: 36 Training Loss: 2.2871885299682617 \n",
      "     Training Step: 37 Training Loss: 3.065572738647461 \n",
      "     Training Step: 38 Training Loss: 3.51769757270813 \n",
      "     Training Step: 39 Training Loss: 3.8672986030578613 \n",
      "     Training Step: 40 Training Loss: 4.351761817932129 \n",
      "     Training Step: 41 Training Loss: 3.3749732971191406 \n",
      "     Training Step: 42 Training Loss: 3.324178695678711 \n",
      "     Training Step: 43 Training Loss: 3.341029167175293 \n",
      "     Training Step: 44 Training Loss: 2.6652605533599854 \n",
      "     Training Step: 45 Training Loss: 3.233234167098999 \n",
      "     Training Step: 46 Training Loss: 2.787309169769287 \n",
      "     Training Step: 47 Training Loss: 2.419916868209839 \n",
      "     Training Step: 48 Training Loss: 3.1889641284942627 \n",
      "     Training Step: 49 Training Loss: 2.7618415355682373 \n",
      "     Training Step: 50 Training Loss: 2.330004930496216 \n",
      "     Training Step: 51 Training Loss: 3.241288661956787 \n",
      "     Training Step: 52 Training Loss: 2.874824047088623 \n",
      "     Training Step: 53 Training Loss: 2.0948312282562256 \n",
      "     Training Step: 54 Training Loss: 3.2085461616516113 \n",
      "     Training Step: 55 Training Loss: 2.3370118141174316 \n",
      "     Training Step: 56 Training Loss: 2.9780025482177734 \n",
      "     Training Step: 57 Training Loss: 3.6598877906799316 \n",
      "     Training Step: 58 Training Loss: 2.555966854095459 \n",
      "     Training Step: 59 Training Loss: 2.6220862865448 \n",
      "     Training Step: 60 Training Loss: 3.481340169906616 \n",
      "     Training Step: 61 Training Loss: 2.9394214153289795 \n",
      "     Training Step: 62 Training Loss: 2.9462599754333496 \n",
      "     Training Step: 63 Training Loss: 2.2635750770568848 \n",
      "     Training Step: 64 Training Loss: 3.415780782699585 \n",
      "     Training Step: 65 Training Loss: 4.424476146697998 \n",
      "     Training Step: 66 Training Loss: 2.574235200881958 \n",
      "     Training Step: 67 Training Loss: 2.3873178958892822 \n",
      "     Training Step: 68 Training Loss: 2.766618251800537 \n",
      "     Training Step: 69 Training Loss: 3.884575366973877 \n",
      "     Training Step: 70 Training Loss: 3.497945547103882 \n",
      "     Training Step: 71 Training Loss: 2.7288691997528076 \n",
      "     Training Step: 72 Training Loss: 2.229881525039673 \n",
      "     Training Step: 73 Training Loss: 2.717273712158203 \n",
      "     Training Step: 74 Training Loss: 2.4164674282073975 \n",
      "     Training Step: 75 Training Loss: 3.1237943172454834 \n",
      "     Training Step: 76 Training Loss: 2.565497875213623 \n",
      "     Training Step: 77 Training Loss: 4.470588684082031 \n",
      "     Training Step: 78 Training Loss: 2.1827125549316406 \n",
      "     Training Step: 79 Training Loss: 3.2634263038635254 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.00185489654541 \n",
      "     Validation Step: 1 Validation Loss: 3.8162174224853516 \n",
      "     Validation Step: 2 Validation Loss: 3.6503336429595947 \n",
      "     Validation Step: 3 Validation Loss: 2.201084613800049 \n",
      "     Validation Step: 4 Validation Loss: 4.129164695739746 \n",
      "     Validation Step: 5 Validation Loss: 3.7719781398773193 \n",
      "     Validation Step: 6 Validation Loss: 3.208860397338867 \n",
      "     Validation Step: 7 Validation Loss: 3.104787826538086 \n",
      "     Validation Step: 8 Validation Loss: 2.797346591949463 \n",
      "     Validation Step: 9 Validation Loss: 3.7361557483673096 \n",
      "     Validation Step: 10 Validation Loss: 2.942852020263672 \n",
      "     Validation Step: 11 Validation Loss: 3.440669059753418 \n",
      "     Validation Step: 12 Validation Loss: 2.80312180519104 \n",
      "     Validation Step: 13 Validation Loss: 3.189363479614258 \n",
      "     Validation Step: 14 Validation Loss: 3.105189800262451 \n",
      "Epoch: 110\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.1510448455810547 \n",
      "     Training Step: 1 Training Loss: 3.1445374488830566 \n",
      "     Training Step: 2 Training Loss: 3.5305018424987793 \n",
      "     Training Step: 3 Training Loss: 3.473217487335205 \n",
      "     Training Step: 4 Training Loss: 4.209092140197754 \n",
      "     Training Step: 5 Training Loss: 4.350616455078125 \n",
      "     Training Step: 6 Training Loss: 2.563981533050537 \n",
      "     Training Step: 7 Training Loss: 2.9574809074401855 \n",
      "     Training Step: 8 Training Loss: 2.142652988433838 \n",
      "     Training Step: 9 Training Loss: 3.0962724685668945 \n",
      "     Training Step: 10 Training Loss: 3.1926589012145996 \n",
      "     Training Step: 11 Training Loss: 3.407097816467285 \n",
      "     Training Step: 12 Training Loss: 2.923154830932617 \n",
      "     Training Step: 13 Training Loss: 2.8869385719299316 \n",
      "     Training Step: 14 Training Loss: 2.328702449798584 \n",
      "     Training Step: 15 Training Loss: 3.0900115966796875 \n",
      "     Training Step: 16 Training Loss: 3.375457763671875 \n",
      "     Training Step: 17 Training Loss: 3.1065354347229004 \n",
      "     Training Step: 18 Training Loss: 3.3445146083831787 \n",
      "     Training Step: 19 Training Loss: 2.4533660411834717 \n",
      "     Training Step: 20 Training Loss: 3.27490234375 \n",
      "     Training Step: 21 Training Loss: 3.848959445953369 \n",
      "     Training Step: 22 Training Loss: 2.723942279815674 \n",
      "     Training Step: 23 Training Loss: 2.5760748386383057 \n",
      "     Training Step: 24 Training Loss: 2.243556499481201 \n",
      "     Training Step: 25 Training Loss: 2.5916080474853516 \n",
      "     Training Step: 26 Training Loss: 2.772091865539551 \n",
      "     Training Step: 27 Training Loss: 2.8855624198913574 \n",
      "     Training Step: 28 Training Loss: 3.727484941482544 \n",
      "     Training Step: 29 Training Loss: 3.3332791328430176 \n",
      "     Training Step: 30 Training Loss: 3.2501258850097656 \n",
      "     Training Step: 31 Training Loss: 2.446312189102173 \n",
      "     Training Step: 32 Training Loss: 2.7836480140686035 \n",
      "     Training Step: 33 Training Loss: 2.0727453231811523 \n",
      "     Training Step: 34 Training Loss: 3.92360258102417 \n",
      "     Training Step: 35 Training Loss: 2.849064588546753 \n",
      "     Training Step: 36 Training Loss: 3.128248453140259 \n",
      "     Training Step: 37 Training Loss: 3.3969101905822754 \n",
      "     Training Step: 38 Training Loss: 2.6780261993408203 \n",
      "     Training Step: 39 Training Loss: 3.0001673698425293 \n",
      "     Training Step: 40 Training Loss: 4.076138019561768 \n",
      "     Training Step: 41 Training Loss: 2.3188915252685547 \n",
      "     Training Step: 42 Training Loss: 2.7212202548980713 \n",
      "     Training Step: 43 Training Loss: 2.88891863822937 \n",
      "     Training Step: 44 Training Loss: 3.4612135887145996 \n",
      "     Training Step: 45 Training Loss: 2.882582187652588 \n",
      "     Training Step: 46 Training Loss: 2.1472010612487793 \n",
      "     Training Step: 47 Training Loss: 2.9986207485198975 \n",
      "     Training Step: 48 Training Loss: 3.665184497833252 \n",
      "     Training Step: 49 Training Loss: 2.0457606315612793 \n",
      "     Training Step: 50 Training Loss: 3.4171085357666016 \n",
      "     Training Step: 51 Training Loss: 3.0101101398468018 \n",
      "     Training Step: 52 Training Loss: 2.4000563621520996 \n",
      "     Training Step: 53 Training Loss: 2.3877921104431152 \n",
      "     Training Step: 54 Training Loss: 2.1267359256744385 \n",
      "     Training Step: 55 Training Loss: 2.237610340118408 \n",
      "     Training Step: 56 Training Loss: 3.3491225242614746 \n",
      "     Training Step: 57 Training Loss: 3.0887439250946045 \n",
      "     Training Step: 58 Training Loss: 4.738469123840332 \n",
      "     Training Step: 59 Training Loss: 3.522228717803955 \n",
      "     Training Step: 60 Training Loss: 4.324684143066406 \n",
      "     Training Step: 61 Training Loss: 3.2718253135681152 \n",
      "     Training Step: 62 Training Loss: 2.417093276977539 \n",
      "     Training Step: 63 Training Loss: 3.2108938694000244 \n",
      "     Training Step: 64 Training Loss: 2.8897547721862793 \n",
      "     Training Step: 65 Training Loss: 2.8295822143554688 \n",
      "     Training Step: 66 Training Loss: 2.4506664276123047 \n",
      "     Training Step: 67 Training Loss: 2.6036477088928223 \n",
      "     Training Step: 68 Training Loss: 2.234166145324707 \n",
      "     Training Step: 69 Training Loss: 2.5448946952819824 \n",
      "     Training Step: 70 Training Loss: 2.1156511306762695 \n",
      "     Training Step: 71 Training Loss: 3.741257667541504 \n",
      "     Training Step: 72 Training Loss: 2.7875213623046875 \n",
      "     Training Step: 73 Training Loss: 3.0126500129699707 \n",
      "     Training Step: 74 Training Loss: 2.763843059539795 \n",
      "     Training Step: 75 Training Loss: 2.472385883331299 \n",
      "     Training Step: 76 Training Loss: 3.112238645553589 \n",
      "     Training Step: 77 Training Loss: 3.2918872833251953 \n",
      "     Training Step: 78 Training Loss: 2.7269387245178223 \n",
      "     Training Step: 79 Training Loss: 2.5746524333953857 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.648650646209717 \n",
      "     Validation Step: 1 Validation Loss: 3.6295855045318604 \n",
      "     Validation Step: 2 Validation Loss: 3.141963005065918 \n",
      "     Validation Step: 3 Validation Loss: 3.0516672134399414 \n",
      "     Validation Step: 4 Validation Loss: 2.879690408706665 \n",
      "     Validation Step: 5 Validation Loss: 3.887348175048828 \n",
      "     Validation Step: 6 Validation Loss: 3.112386703491211 \n",
      "     Validation Step: 7 Validation Loss: 2.3002097606658936 \n",
      "     Validation Step: 8 Validation Loss: 2.6918630599975586 \n",
      "     Validation Step: 9 Validation Loss: 3.668266773223877 \n",
      "     Validation Step: 10 Validation Loss: 3.649857997894287 \n",
      "     Validation Step: 11 Validation Loss: 2.7480380535125732 \n",
      "     Validation Step: 12 Validation Loss: 3.371460199356079 \n",
      "     Validation Step: 13 Validation Loss: 3.0574426651000977 \n",
      "     Validation Step: 14 Validation Loss: 3.082287311553955 \n",
      "Epoch: 111\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.1047158241271973 \n",
      "     Training Step: 1 Training Loss: 3.3015518188476562 \n",
      "     Training Step: 2 Training Loss: 3.445871591567993 \n",
      "     Training Step: 3 Training Loss: 3.2075629234313965 \n",
      "     Training Step: 4 Training Loss: 2.4035837650299072 \n",
      "     Training Step: 5 Training Loss: 2.489659547805786 \n",
      "     Training Step: 6 Training Loss: 3.3953943252563477 \n",
      "     Training Step: 7 Training Loss: 2.533334493637085 \n",
      "     Training Step: 8 Training Loss: 3.122311592102051 \n",
      "     Training Step: 9 Training Loss: 2.4695522785186768 \n",
      "     Training Step: 10 Training Loss: 3.3872780799865723 \n",
      "     Training Step: 11 Training Loss: 3.468740224838257 \n",
      "     Training Step: 12 Training Loss: 2.477377414703369 \n",
      "     Training Step: 13 Training Loss: 2.7105019092559814 \n",
      "     Training Step: 14 Training Loss: 2.1687569618225098 \n",
      "     Training Step: 15 Training Loss: 2.184558153152466 \n",
      "     Training Step: 16 Training Loss: 2.0680172443389893 \n",
      "     Training Step: 17 Training Loss: 2.734732151031494 \n",
      "     Training Step: 18 Training Loss: 2.796990394592285 \n",
      "     Training Step: 19 Training Loss: 2.5751264095306396 \n",
      "     Training Step: 20 Training Loss: 2.772801399230957 \n",
      "     Training Step: 21 Training Loss: 4.756649971008301 \n",
      "     Training Step: 22 Training Loss: 3.8758459091186523 \n",
      "     Training Step: 23 Training Loss: 3.0899910926818848 \n",
      "     Training Step: 24 Training Loss: 3.1374008655548096 \n",
      "     Training Step: 25 Training Loss: 2.0958638191223145 \n",
      "     Training Step: 26 Training Loss: 2.6290833950042725 \n",
      "     Training Step: 27 Training Loss: 2.3250632286071777 \n",
      "     Training Step: 28 Training Loss: 2.992805004119873 \n",
      "     Training Step: 29 Training Loss: 2.407620429992676 \n",
      "     Training Step: 30 Training Loss: 3.0381100177764893 \n",
      "     Training Step: 31 Training Loss: 3.3450698852539062 \n",
      "     Training Step: 32 Training Loss: 2.311412811279297 \n",
      "     Training Step: 33 Training Loss: 2.2504916191101074 \n",
      "     Training Step: 34 Training Loss: 3.4324350357055664 \n",
      "     Training Step: 35 Training Loss: 3.110595226287842 \n",
      "     Training Step: 36 Training Loss: 2.8672525882720947 \n",
      "     Training Step: 37 Training Loss: 3.117765426635742 \n",
      "     Training Step: 38 Training Loss: 2.5917603969573975 \n",
      "     Training Step: 39 Training Loss: 3.3400113582611084 \n",
      "     Training Step: 40 Training Loss: 3.948350191116333 \n",
      "     Training Step: 41 Training Loss: 2.310126304626465 \n",
      "     Training Step: 42 Training Loss: 4.275699138641357 \n",
      "     Training Step: 43 Training Loss: 3.3055179119110107 \n",
      "     Training Step: 44 Training Loss: 3.080674886703491 \n",
      "     Training Step: 45 Training Loss: 3.2850584983825684 \n",
      "     Training Step: 46 Training Loss: 2.7203242778778076 \n",
      "     Training Step: 47 Training Loss: 3.285067558288574 \n",
      "     Training Step: 48 Training Loss: 2.8358161449432373 \n",
      "     Training Step: 49 Training Loss: 3.3046154975891113 \n",
      "     Training Step: 50 Training Loss: 3.463855028152466 \n",
      "     Training Step: 51 Training Loss: 2.941788673400879 \n",
      "     Training Step: 52 Training Loss: 2.613506317138672 \n",
      "     Training Step: 53 Training Loss: 2.9018216133117676 \n",
      "     Training Step: 54 Training Loss: 2.5107169151306152 \n",
      "     Training Step: 55 Training Loss: 3.5726070404052734 \n",
      "     Training Step: 56 Training Loss: 2.7613189220428467 \n",
      "     Training Step: 57 Training Loss: 3.7889339923858643 \n",
      "     Training Step: 58 Training Loss: 3.0914883613586426 \n",
      "     Training Step: 59 Training Loss: 2.239499568939209 \n",
      "     Training Step: 60 Training Loss: 4.374825477600098 \n",
      "     Training Step: 61 Training Loss: 2.087308168411255 \n",
      "     Training Step: 62 Training Loss: 3.10452938079834 \n",
      "     Training Step: 63 Training Loss: 2.2544941902160645 \n",
      "     Training Step: 64 Training Loss: 3.103834867477417 \n",
      "     Training Step: 65 Training Loss: 3.2754039764404297 \n",
      "     Training Step: 66 Training Loss: 3.4061195850372314 \n",
      "     Training Step: 67 Training Loss: 3.5716724395751953 \n",
      "     Training Step: 68 Training Loss: 3.006561756134033 \n",
      "     Training Step: 69 Training Loss: 2.860118865966797 \n",
      "     Training Step: 70 Training Loss: 2.7697699069976807 \n",
      "     Training Step: 71 Training Loss: 4.097087860107422 \n",
      "     Training Step: 72 Training Loss: 2.550084352493286 \n",
      "     Training Step: 73 Training Loss: 3.0745046138763428 \n",
      "     Training Step: 74 Training Loss: 2.721214532852173 \n",
      "     Training Step: 75 Training Loss: 4.308312892913818 \n",
      "     Training Step: 76 Training Loss: 2.496438503265381 \n",
      "     Training Step: 77 Training Loss: 2.109386682510376 \n",
      "     Training Step: 78 Training Loss: 2.90602970123291 \n",
      "     Training Step: 79 Training Loss: 3.8058314323425293 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6163930892944336 \n",
      "     Validation Step: 1 Validation Loss: 2.2915563583374023 \n",
      "     Validation Step: 2 Validation Loss: 3.0500638484954834 \n",
      "     Validation Step: 3 Validation Loss: 3.043020248413086 \n",
      "     Validation Step: 4 Validation Loss: 3.5857324600219727 \n",
      "     Validation Step: 5 Validation Loss: 2.63749623298645 \n",
      "     Validation Step: 6 Validation Loss: 3.474949359893799 \n",
      "     Validation Step: 7 Validation Loss: 3.636955976486206 \n",
      "     Validation Step: 8 Validation Loss: 2.743943214416504 \n",
      "     Validation Step: 9 Validation Loss: 3.1562652587890625 \n",
      "     Validation Step: 10 Validation Loss: 2.868917465209961 \n",
      "     Validation Step: 11 Validation Loss: 3.0181829929351807 \n",
      "     Validation Step: 12 Validation Loss: 3.222360849380493 \n",
      "     Validation Step: 13 Validation Loss: 3.697878837585449 \n",
      "     Validation Step: 14 Validation Loss: 3.9439549446105957 \n",
      "Epoch: 112\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.3264689445495605 \n",
      "     Training Step: 1 Training Loss: 3.130963087081909 \n",
      "     Training Step: 2 Training Loss: 2.2299811840057373 \n",
      "     Training Step: 3 Training Loss: 2.9974477291107178 \n",
      "     Training Step: 4 Training Loss: 3.8461546897888184 \n",
      "     Training Step: 5 Training Loss: 2.1565423011779785 \n",
      "     Training Step: 6 Training Loss: 3.114469528198242 \n",
      "     Training Step: 7 Training Loss: 2.9367523193359375 \n",
      "     Training Step: 8 Training Loss: 4.310655117034912 \n",
      "     Training Step: 9 Training Loss: 2.4639716148376465 \n",
      "     Training Step: 10 Training Loss: 2.6031334400177 \n",
      "     Training Step: 11 Training Loss: 3.1662240028381348 \n",
      "     Training Step: 12 Training Loss: 2.84735107421875 \n",
      "     Training Step: 13 Training Loss: 4.670844078063965 \n",
      "     Training Step: 14 Training Loss: 3.437558174133301 \n",
      "     Training Step: 15 Training Loss: 3.550942897796631 \n",
      "     Training Step: 16 Training Loss: 2.1452622413635254 \n",
      "     Training Step: 17 Training Loss: 3.3977365493774414 \n",
      "     Training Step: 18 Training Loss: 3.410094738006592 \n",
      "     Training Step: 19 Training Loss: 3.1916303634643555 \n",
      "     Training Step: 20 Training Loss: 2.696341037750244 \n",
      "     Training Step: 21 Training Loss: 2.5112617015838623 \n",
      "     Training Step: 22 Training Loss: 2.654033660888672 \n",
      "     Training Step: 23 Training Loss: 2.274489402770996 \n",
      "     Training Step: 24 Training Loss: 2.672341823577881 \n",
      "     Training Step: 25 Training Loss: 2.4598989486694336 \n",
      "     Training Step: 26 Training Loss: 2.9995248317718506 \n",
      "     Training Step: 27 Training Loss: 3.2445592880249023 \n",
      "     Training Step: 28 Training Loss: 3.657407760620117 \n",
      "     Training Step: 29 Training Loss: 3.7406511306762695 \n",
      "     Training Step: 30 Training Loss: 3.345615863800049 \n",
      "     Training Step: 31 Training Loss: 2.6492886543273926 \n",
      "     Training Step: 32 Training Loss: 2.8685848712921143 \n",
      "     Training Step: 33 Training Loss: 3.354419469833374 \n",
      "     Training Step: 34 Training Loss: 2.3562653064727783 \n",
      "     Training Step: 35 Training Loss: 3.4480514526367188 \n",
      "     Training Step: 36 Training Loss: 2.375253438949585 \n",
      "     Training Step: 37 Training Loss: 3.5826826095581055 \n",
      "     Training Step: 38 Training Loss: 3.145839214324951 \n",
      "     Training Step: 39 Training Loss: 2.122319221496582 \n",
      "     Training Step: 40 Training Loss: 2.979358673095703 \n",
      "     Training Step: 41 Training Loss: 4.055714130401611 \n",
      "     Training Step: 42 Training Loss: 2.995694637298584 \n",
      "     Training Step: 43 Training Loss: 3.174454927444458 \n",
      "     Training Step: 44 Training Loss: 2.5196785926818848 \n",
      "     Training Step: 45 Training Loss: 2.9857935905456543 \n",
      "     Training Step: 46 Training Loss: 2.919612169265747 \n",
      "     Training Step: 47 Training Loss: 2.507105827331543 \n",
      "     Training Step: 48 Training Loss: 2.873063802719116 \n",
      "     Training Step: 49 Training Loss: 3.3374927043914795 \n",
      "     Training Step: 50 Training Loss: 3.720635414123535 \n",
      "     Training Step: 51 Training Loss: 2.6385176181793213 \n",
      "     Training Step: 52 Training Loss: 3.156782627105713 \n",
      "     Training Step: 53 Training Loss: 2.3394222259521484 \n",
      "     Training Step: 54 Training Loss: 4.069370269775391 \n",
      "     Training Step: 55 Training Loss: 3.03141450881958 \n",
      "     Training Step: 56 Training Loss: 2.969257354736328 \n",
      "     Training Step: 57 Training Loss: 2.42669677734375 \n",
      "     Training Step: 58 Training Loss: 2.3938770294189453 \n",
      "     Training Step: 59 Training Loss: 3.448431968688965 \n",
      "     Training Step: 60 Training Loss: 3.372572898864746 \n",
      "     Training Step: 61 Training Loss: 3.28513765335083 \n",
      "     Training Step: 62 Training Loss: 2.769017219543457 \n",
      "     Training Step: 63 Training Loss: 3.6084845066070557 \n",
      "     Training Step: 64 Training Loss: 2.829066753387451 \n",
      "     Training Step: 65 Training Loss: 2.854928970336914 \n",
      "     Training Step: 66 Training Loss: 2.265648603439331 \n",
      "     Training Step: 67 Training Loss: 2.213322162628174 \n",
      "     Training Step: 68 Training Loss: 2.8626253604888916 \n",
      "     Training Step: 69 Training Loss: 2.99483060836792 \n",
      "     Training Step: 70 Training Loss: 2.892106533050537 \n",
      "     Training Step: 71 Training Loss: 3.867335557937622 \n",
      "     Training Step: 72 Training Loss: 4.403916358947754 \n",
      "     Training Step: 73 Training Loss: 2.367412567138672 \n",
      "     Training Step: 74 Training Loss: 2.5594184398651123 \n",
      "     Training Step: 75 Training Loss: 2.0382401943206787 \n",
      "     Training Step: 76 Training Loss: 3.145310878753662 \n",
      "     Training Step: 77 Training Loss: 2.6940717697143555 \n",
      "     Training Step: 78 Training Loss: 2.801596164703369 \n",
      "     Training Step: 79 Training Loss: 2.2802882194519043 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.891550302505493 \n",
      "     Validation Step: 1 Validation Loss: 2.684490442276001 \n",
      "     Validation Step: 2 Validation Loss: 3.694857597351074 \n",
      "     Validation Step: 3 Validation Loss: 2.803952217102051 \n",
      "     Validation Step: 4 Validation Loss: 2.2443554401397705 \n",
      "     Validation Step: 5 Validation Loss: 3.1220762729644775 \n",
      "     Validation Step: 6 Validation Loss: 3.6337342262268066 \n",
      "     Validation Step: 7 Validation Loss: 3.1820998191833496 \n",
      "     Validation Step: 8 Validation Loss: 3.0666632652282715 \n",
      "     Validation Step: 9 Validation Loss: 3.677046060562134 \n",
      "     Validation Step: 10 Validation Loss: 3.171067476272583 \n",
      "     Validation Step: 11 Validation Loss: 3.6345126628875732 \n",
      "     Validation Step: 12 Validation Loss: 3.4840357303619385 \n",
      "     Validation Step: 13 Validation Loss: 3.8943402767181396 \n",
      "     Validation Step: 14 Validation Loss: 3.0364060401916504 \n",
      "Epoch: 113\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.2503738403320312 \n",
      "     Training Step: 1 Training Loss: 2.494466781616211 \n",
      "     Training Step: 2 Training Loss: 3.3912272453308105 \n",
      "     Training Step: 3 Training Loss: 3.099205493927002 \n",
      "     Training Step: 4 Training Loss: 2.3039939403533936 \n",
      "     Training Step: 5 Training Loss: 2.7962610721588135 \n",
      "     Training Step: 6 Training Loss: 2.7973132133483887 \n",
      "     Training Step: 7 Training Loss: 3.3790712356567383 \n",
      "     Training Step: 8 Training Loss: 2.5514960289001465 \n",
      "     Training Step: 9 Training Loss: 2.093538761138916 \n",
      "     Training Step: 10 Training Loss: 2.9634029865264893 \n",
      "     Training Step: 11 Training Loss: 2.804880142211914 \n",
      "     Training Step: 12 Training Loss: 3.0570926666259766 \n",
      "     Training Step: 13 Training Loss: 2.367706060409546 \n",
      "     Training Step: 14 Training Loss: 3.1950740814208984 \n",
      "     Training Step: 15 Training Loss: 3.490361213684082 \n",
      "     Training Step: 16 Training Loss: 3.0279195308685303 \n",
      "     Training Step: 17 Training Loss: 3.108956813812256 \n",
      "     Training Step: 18 Training Loss: 3.0070199966430664 \n",
      "     Training Step: 19 Training Loss: 2.165013551712036 \n",
      "     Training Step: 20 Training Loss: 3.162808418273926 \n",
      "     Training Step: 21 Training Loss: 3.250082015991211 \n",
      "     Training Step: 22 Training Loss: 2.7554426193237305 \n",
      "     Training Step: 23 Training Loss: 3.883319854736328 \n",
      "     Training Step: 24 Training Loss: 4.405563831329346 \n",
      "     Training Step: 25 Training Loss: 2.561674118041992 \n",
      "     Training Step: 26 Training Loss: 2.5849246978759766 \n",
      "     Training Step: 27 Training Loss: 2.774076223373413 \n",
      "     Training Step: 28 Training Loss: 2.6111838817596436 \n",
      "     Training Step: 29 Training Loss: 2.326347827911377 \n",
      "     Training Step: 30 Training Loss: 3.4396307468414307 \n",
      "     Training Step: 31 Training Loss: 2.769031047821045 \n",
      "     Training Step: 32 Training Loss: 3.2974305152893066 \n",
      "     Training Step: 33 Training Loss: 2.7073161602020264 \n",
      "     Training Step: 34 Training Loss: 2.8805150985717773 \n",
      "     Training Step: 35 Training Loss: 2.1181201934814453 \n",
      "     Training Step: 36 Training Loss: 3.7889504432678223 \n",
      "     Training Step: 37 Training Loss: 2.392364978790283 \n",
      "     Training Step: 38 Training Loss: 3.392045021057129 \n",
      "     Training Step: 39 Training Loss: 3.249453067779541 \n",
      "     Training Step: 40 Training Loss: 2.3532700538635254 \n",
      "     Training Step: 41 Training Loss: 2.4866251945495605 \n",
      "     Training Step: 42 Training Loss: 3.8668088912963867 \n",
      "     Training Step: 43 Training Loss: 4.255780220031738 \n",
      "     Training Step: 44 Training Loss: 2.2841579914093018 \n",
      "     Training Step: 45 Training Loss: 2.7350075244903564 \n",
      "     Training Step: 46 Training Loss: 2.309896469116211 \n",
      "     Training Step: 47 Training Loss: 3.84513521194458 \n",
      "     Training Step: 48 Training Loss: 3.161237955093384 \n",
      "     Training Step: 49 Training Loss: 3.3517773151397705 \n",
      "     Training Step: 50 Training Loss: 3.7390308380126953 \n",
      "     Training Step: 51 Training Loss: 3.4957549571990967 \n",
      "     Training Step: 52 Training Loss: 2.139380931854248 \n",
      "     Training Step: 53 Training Loss: 4.306427478790283 \n",
      "     Training Step: 54 Training Loss: 2.9732680320739746 \n",
      "     Training Step: 55 Training Loss: 2.441354274749756 \n",
      "     Training Step: 56 Training Loss: 2.4852423667907715 \n",
      "     Training Step: 57 Training Loss: 2.373833179473877 \n",
      "     Training Step: 58 Training Loss: 4.697381019592285 \n",
      "     Training Step: 59 Training Loss: 2.8831276893615723 \n",
      "     Training Step: 60 Training Loss: 2.873347759246826 \n",
      "     Training Step: 61 Training Loss: 3.112499713897705 \n",
      "     Training Step: 62 Training Loss: 3.141979694366455 \n",
      "     Training Step: 63 Training Loss: 2.4474639892578125 \n",
      "     Training Step: 64 Training Loss: 3.877056121826172 \n",
      "     Training Step: 65 Training Loss: 2.5950376987457275 \n",
      "     Training Step: 66 Training Loss: 3.28928279876709 \n",
      "     Training Step: 67 Training Loss: 2.231175422668457 \n",
      "     Training Step: 68 Training Loss: 2.7950525283813477 \n",
      "     Training Step: 69 Training Loss: 3.4534130096435547 \n",
      "     Training Step: 70 Training Loss: 2.4614267349243164 \n",
      "     Training Step: 71 Training Loss: 4.274367809295654 \n",
      "     Training Step: 72 Training Loss: 2.952341079711914 \n",
      "     Training Step: 73 Training Loss: 2.9835634231567383 \n",
      "     Training Step: 74 Training Loss: 3.301823139190674 \n",
      "     Training Step: 75 Training Loss: 2.146746873855591 \n",
      "     Training Step: 76 Training Loss: 2.932392120361328 \n",
      "     Training Step: 77 Training Loss: 2.9341273307800293 \n",
      "     Training Step: 78 Training Loss: 3.40109920501709 \n",
      "     Training Step: 79 Training Loss: 3.588216543197632 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.9956798553466797 \n",
      "     Validation Step: 1 Validation Loss: 2.688666343688965 \n",
      "     Validation Step: 2 Validation Loss: 2.6941707134246826 \n",
      "     Validation Step: 3 Validation Loss: 3.0538763999938965 \n",
      "     Validation Step: 4 Validation Loss: 3.154961585998535 \n",
      "     Validation Step: 5 Validation Loss: 3.6513609886169434 \n",
      "     Validation Step: 6 Validation Loss: 3.60962176322937 \n",
      "     Validation Step: 7 Validation Loss: 3.8553333282470703 \n",
      "     Validation Step: 8 Validation Loss: 3.548154354095459 \n",
      "     Validation Step: 9 Validation Loss: 3.2554802894592285 \n",
      "     Validation Step: 10 Validation Loss: 2.305352210998535 \n",
      "     Validation Step: 11 Validation Loss: 3.0710694789886475 \n",
      "     Validation Step: 12 Validation Loss: 3.721625328063965 \n",
      "     Validation Step: 13 Validation Loss: 2.86912202835083 \n",
      "     Validation Step: 14 Validation Loss: 3.5098674297332764 \n",
      "Epoch: 114\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.3065454959869385 \n",
      "     Training Step: 1 Training Loss: 2.955394744873047 \n",
      "     Training Step: 2 Training Loss: 3.416224956512451 \n",
      "     Training Step: 3 Training Loss: 3.708047389984131 \n",
      "     Training Step: 4 Training Loss: 2.983177661895752 \n",
      "     Training Step: 5 Training Loss: 3.540666103363037 \n",
      "     Training Step: 6 Training Loss: 3.387596607208252 \n",
      "     Training Step: 7 Training Loss: 2.179960250854492 \n",
      "     Training Step: 8 Training Loss: 2.885744333267212 \n",
      "     Training Step: 9 Training Loss: 3.1978421211242676 \n",
      "     Training Step: 10 Training Loss: 2.3311703205108643 \n",
      "     Training Step: 11 Training Loss: 2.9769744873046875 \n",
      "     Training Step: 12 Training Loss: 2.450533628463745 \n",
      "     Training Step: 13 Training Loss: 3.184807300567627 \n",
      "     Training Step: 14 Training Loss: 2.2255728244781494 \n",
      "     Training Step: 15 Training Loss: 2.572993755340576 \n",
      "     Training Step: 16 Training Loss: 2.8218255043029785 \n",
      "     Training Step: 17 Training Loss: 3.4658751487731934 \n",
      "     Training Step: 18 Training Loss: 3.3447155952453613 \n",
      "     Training Step: 19 Training Loss: 3.466202735900879 \n",
      "     Training Step: 20 Training Loss: 2.5347695350646973 \n",
      "     Training Step: 21 Training Loss: 3.2982521057128906 \n",
      "     Training Step: 22 Training Loss: 2.3298864364624023 \n",
      "     Training Step: 23 Training Loss: 3.5873003005981445 \n",
      "     Training Step: 24 Training Loss: 3.2966487407684326 \n",
      "     Training Step: 25 Training Loss: 2.650439739227295 \n",
      "     Training Step: 26 Training Loss: 2.518375873565674 \n",
      "     Training Step: 27 Training Loss: 2.1170248985290527 \n",
      "     Training Step: 28 Training Loss: 3.766205310821533 \n",
      "     Training Step: 29 Training Loss: 4.005674839019775 \n",
      "     Training Step: 30 Training Loss: 2.5045459270477295 \n",
      "     Training Step: 31 Training Loss: 3.1507182121276855 \n",
      "     Training Step: 32 Training Loss: 2.8264050483703613 \n",
      "     Training Step: 33 Training Loss: 2.9128506183624268 \n",
      "     Training Step: 34 Training Loss: 2.8491177558898926 \n",
      "     Training Step: 35 Training Loss: 4.297513961791992 \n",
      "     Training Step: 36 Training Loss: 2.423643112182617 \n",
      "     Training Step: 37 Training Loss: 4.760954856872559 \n",
      "     Training Step: 38 Training Loss: 2.5731475353240967 \n",
      "     Training Step: 39 Training Loss: 2.0738978385925293 \n",
      "     Training Step: 40 Training Loss: 3.040048837661743 \n",
      "     Training Step: 41 Training Loss: 3.432401180267334 \n",
      "     Training Step: 42 Training Loss: 3.2826826572418213 \n",
      "     Training Step: 43 Training Loss: 2.7440946102142334 \n",
      "     Training Step: 44 Training Loss: 2.398923873901367 \n",
      "     Training Step: 45 Training Loss: 2.795858144760132 \n",
      "     Training Step: 46 Training Loss: 3.445783853530884 \n",
      "     Training Step: 47 Training Loss: 3.4711053371429443 \n",
      "     Training Step: 48 Training Loss: 2.919178009033203 \n",
      "     Training Step: 49 Training Loss: 3.3619470596313477 \n",
      "     Training Step: 50 Training Loss: 3.192587375640869 \n",
      "     Training Step: 51 Training Loss: 4.361391067504883 \n",
      "     Training Step: 52 Training Loss: 3.4168896675109863 \n",
      "     Training Step: 53 Training Loss: 2.2292139530181885 \n",
      "     Training Step: 54 Training Loss: 2.5138206481933594 \n",
      "     Training Step: 55 Training Loss: 2.101846694946289 \n",
      "     Training Step: 56 Training Loss: 2.9777376651763916 \n",
      "     Training Step: 57 Training Loss: 3.759885787963867 \n",
      "     Training Step: 58 Training Loss: 3.2836050987243652 \n",
      "     Training Step: 59 Training Loss: 3.2758402824401855 \n",
      "     Training Step: 60 Training Loss: 2.5729281902313232 \n",
      "     Training Step: 61 Training Loss: 2.905344009399414 \n",
      "     Training Step: 62 Training Loss: 2.786004066467285 \n",
      "     Training Step: 63 Training Loss: 2.6979727745056152 \n",
      "     Training Step: 64 Training Loss: 2.0707082748413086 \n",
      "     Training Step: 65 Training Loss: 2.7559542655944824 \n",
      "     Training Step: 66 Training Loss: 3.1014404296875 \n",
      "     Training Step: 67 Training Loss: 3.289780378341675 \n",
      "     Training Step: 68 Training Loss: 3.169903039932251 \n",
      "     Training Step: 69 Training Loss: 2.922558546066284 \n",
      "     Training Step: 70 Training Loss: 3.2024569511413574 \n",
      "     Training Step: 71 Training Loss: 2.7703986167907715 \n",
      "     Training Step: 72 Training Loss: 3.161646604537964 \n",
      "     Training Step: 73 Training Loss: 4.3853631019592285 \n",
      "     Training Step: 74 Training Loss: 2.354405403137207 \n",
      "     Training Step: 75 Training Loss: 3.411539077758789 \n",
      "     Training Step: 76 Training Loss: 3.877626895904541 \n",
      "     Training Step: 77 Training Loss: 2.2726073265075684 \n",
      "     Training Step: 78 Training Loss: 2.901883125305176 \n",
      "     Training Step: 79 Training Loss: 2.4998981952667236 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6410274505615234 \n",
      "     Validation Step: 1 Validation Loss: 3.049943685531616 \n",
      "     Validation Step: 2 Validation Loss: 3.964813709259033 \n",
      "     Validation Step: 3 Validation Loss: 3.600968837738037 \n",
      "     Validation Step: 4 Validation Loss: 3.7096595764160156 \n",
      "     Validation Step: 5 Validation Loss: 2.864617109298706 \n",
      "     Validation Step: 6 Validation Loss: 3.111988067626953 \n",
      "     Validation Step: 7 Validation Loss: 3.223184585571289 \n",
      "     Validation Step: 8 Validation Loss: 2.7409610748291016 \n",
      "     Validation Step: 9 Validation Loss: 3.0518834590911865 \n",
      "     Validation Step: 10 Validation Loss: 2.287853717803955 \n",
      "     Validation Step: 11 Validation Loss: 3.4069368839263916 \n",
      "     Validation Step: 12 Validation Loss: 3.12314510345459 \n",
      "     Validation Step: 13 Validation Loss: 2.7483692169189453 \n",
      "     Validation Step: 14 Validation Loss: 3.6691770553588867 \n",
      "Epoch: 115\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.152364492416382 \n",
      "     Training Step: 1 Training Loss: 3.837851047515869 \n",
      "     Training Step: 2 Training Loss: 2.095391273498535 \n",
      "     Training Step: 3 Training Loss: 3.264643669128418 \n",
      "     Training Step: 4 Training Loss: 3.08890962600708 \n",
      "     Training Step: 5 Training Loss: 3.081197738647461 \n",
      "     Training Step: 6 Training Loss: 2.213881492614746 \n",
      "     Training Step: 7 Training Loss: 2.431447982788086 \n",
      "     Training Step: 8 Training Loss: 2.3318679332733154 \n",
      "     Training Step: 9 Training Loss: 3.366715908050537 \n",
      "     Training Step: 10 Training Loss: 3.054427146911621 \n",
      "     Training Step: 11 Training Loss: 2.6956419944763184 \n",
      "     Training Step: 12 Training Loss: 3.2714128494262695 \n",
      "     Training Step: 13 Training Loss: 3.4183266162872314 \n",
      "     Training Step: 14 Training Loss: 2.9533331394195557 \n",
      "     Training Step: 15 Training Loss: 2.7094550132751465 \n",
      "     Training Step: 16 Training Loss: 2.261425495147705 \n",
      "     Training Step: 17 Training Loss: 2.6155033111572266 \n",
      "     Training Step: 18 Training Loss: 2.658531665802002 \n",
      "     Training Step: 19 Training Loss: 4.290889739990234 \n",
      "     Training Step: 20 Training Loss: 2.1331334114074707 \n",
      "     Training Step: 21 Training Loss: 2.4254794120788574 \n",
      "     Training Step: 22 Training Loss: 2.6902356147766113 \n",
      "     Training Step: 23 Training Loss: 2.490699052810669 \n",
      "     Training Step: 24 Training Loss: 3.3513426780700684 \n",
      "     Training Step: 25 Training Loss: 3.5978002548217773 \n",
      "     Training Step: 26 Training Loss: 2.7732667922973633 \n",
      "     Training Step: 27 Training Loss: 2.400275707244873 \n",
      "     Training Step: 28 Training Loss: 2.7518017292022705 \n",
      "     Training Step: 29 Training Loss: 4.3497490882873535 \n",
      "     Training Step: 30 Training Loss: 3.736302137374878 \n",
      "     Training Step: 31 Training Loss: 2.8404645919799805 \n",
      "     Training Step: 32 Training Loss: 3.210864305496216 \n",
      "     Training Step: 33 Training Loss: 2.8116226196289062 \n",
      "     Training Step: 34 Training Loss: 2.066890239715576 \n",
      "     Training Step: 35 Training Loss: 3.1381046772003174 \n",
      "     Training Step: 36 Training Loss: 2.60526180267334 \n",
      "     Training Step: 37 Training Loss: 2.730992078781128 \n",
      "     Training Step: 38 Training Loss: 2.9855377674102783 \n",
      "     Training Step: 39 Training Loss: 2.538573980331421 \n",
      "     Training Step: 40 Training Loss: 3.3923683166503906 \n",
      "     Training Step: 41 Training Loss: 2.930882453918457 \n",
      "     Training Step: 42 Training Loss: 3.3310909271240234 \n",
      "     Training Step: 43 Training Loss: 2.8066883087158203 \n",
      "     Training Step: 44 Training Loss: 2.5388340950012207 \n",
      "     Training Step: 45 Training Loss: 3.1249585151672363 \n",
      "     Training Step: 46 Training Loss: 2.9095585346221924 \n",
      "     Training Step: 47 Training Loss: 2.43392014503479 \n",
      "     Training Step: 48 Training Loss: 3.8864641189575195 \n",
      "     Training Step: 49 Training Loss: 3.020242691040039 \n",
      "     Training Step: 50 Training Loss: 2.849588632583618 \n",
      "     Training Step: 51 Training Loss: 2.3365321159362793 \n",
      "     Training Step: 52 Training Loss: 2.203500986099243 \n",
      "     Training Step: 53 Training Loss: 2.926567554473877 \n",
      "     Training Step: 54 Training Loss: 2.147104024887085 \n",
      "     Training Step: 55 Training Loss: 3.5156893730163574 \n",
      "     Training Step: 56 Training Loss: 3.730750560760498 \n",
      "     Training Step: 57 Training Loss: 3.565216302871704 \n",
      "     Training Step: 58 Training Loss: 3.3302111625671387 \n",
      "     Training Step: 59 Training Loss: 3.343583106994629 \n",
      "     Training Step: 60 Training Loss: 2.8026294708251953 \n",
      "     Training Step: 61 Training Loss: 2.397629737854004 \n",
      "     Training Step: 62 Training Loss: 3.376314401626587 \n",
      "     Training Step: 63 Training Loss: 3.505168914794922 \n",
      "     Training Step: 64 Training Loss: 2.813850164413452 \n",
      "     Training Step: 65 Training Loss: 2.2202515602111816 \n",
      "     Training Step: 66 Training Loss: 4.752504348754883 \n",
      "     Training Step: 67 Training Loss: 4.33018684387207 \n",
      "     Training Step: 68 Training Loss: 2.176623821258545 \n",
      "     Training Step: 69 Training Loss: 3.2915115356445312 \n",
      "     Training Step: 70 Training Loss: 3.3865296840667725 \n",
      "     Training Step: 71 Training Loss: 3.7770562171936035 \n",
      "     Training Step: 72 Training Loss: 2.708651304244995 \n",
      "     Training Step: 73 Training Loss: 3.085448741912842 \n",
      "     Training Step: 74 Training Loss: 2.8348565101623535 \n",
      "     Training Step: 75 Training Loss: 3.1176705360412598 \n",
      "     Training Step: 76 Training Loss: 2.981830358505249 \n",
      "     Training Step: 77 Training Loss: 3.1920559406280518 \n",
      "     Training Step: 78 Training Loss: 2.5637001991271973 \n",
      "     Training Step: 79 Training Loss: 2.322854518890381 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.589653491973877 \n",
      "     Validation Step: 1 Validation Loss: 2.944298267364502 \n",
      "     Validation Step: 2 Validation Loss: 3.0999133586883545 \n",
      "     Validation Step: 3 Validation Loss: 3.5940005779266357 \n",
      "     Validation Step: 4 Validation Loss: 3.225945234298706 \n",
      "     Validation Step: 5 Validation Loss: 3.519503116607666 \n",
      "     Validation Step: 6 Validation Loss: 3.009119987487793 \n",
      "     Validation Step: 7 Validation Loss: 3.680816411972046 \n",
      "     Validation Step: 8 Validation Loss: 3.8819925785064697 \n",
      "     Validation Step: 9 Validation Loss: 3.450007438659668 \n",
      "     Validation Step: 10 Validation Loss: 2.7248692512512207 \n",
      "     Validation Step: 11 Validation Loss: 3.0997138023376465 \n",
      "     Validation Step: 12 Validation Loss: 2.7253668308258057 \n",
      "     Validation Step: 13 Validation Loss: 2.2478160858154297 \n",
      "     Validation Step: 14 Validation Loss: 2.8915340900421143 \n",
      "Epoch: 116\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.5114388465881348 \n",
      "     Training Step: 1 Training Loss: 2.109649896621704 \n",
      "     Training Step: 2 Training Loss: 2.2285642623901367 \n",
      "     Training Step: 3 Training Loss: 2.894392490386963 \n",
      "     Training Step: 4 Training Loss: 2.569591999053955 \n",
      "     Training Step: 5 Training Loss: 2.9251997470855713 \n",
      "     Training Step: 6 Training Loss: 3.01206636428833 \n",
      "     Training Step: 7 Training Loss: 2.2960681915283203 \n",
      "     Training Step: 8 Training Loss: 2.799711227416992 \n",
      "     Training Step: 9 Training Loss: 3.318735122680664 \n",
      "     Training Step: 10 Training Loss: 3.0710411071777344 \n",
      "     Training Step: 11 Training Loss: 3.2369000911712646 \n",
      "     Training Step: 12 Training Loss: 2.1425929069519043 \n",
      "     Training Step: 13 Training Loss: 4.385705947875977 \n",
      "     Training Step: 14 Training Loss: 2.945329189300537 \n",
      "     Training Step: 15 Training Loss: 2.0844078063964844 \n",
      "     Training Step: 16 Training Loss: 2.8022756576538086 \n",
      "     Training Step: 17 Training Loss: 3.190603733062744 \n",
      "     Training Step: 18 Training Loss: 4.392192840576172 \n",
      "     Training Step: 19 Training Loss: 4.12007474899292 \n",
      "     Training Step: 20 Training Loss: 2.166764736175537 \n",
      "     Training Step: 21 Training Loss: 3.390718698501587 \n",
      "     Training Step: 22 Training Loss: 3.5200276374816895 \n",
      "     Training Step: 23 Training Loss: 2.7779407501220703 \n",
      "     Training Step: 24 Training Loss: 3.8673243522644043 \n",
      "     Training Step: 25 Training Loss: 2.929924964904785 \n",
      "     Training Step: 26 Training Loss: 3.3838143348693848 \n",
      "     Training Step: 27 Training Loss: 3.653691291809082 \n",
      "     Training Step: 28 Training Loss: 3.1004490852355957 \n",
      "     Training Step: 29 Training Loss: 3.8000712394714355 \n",
      "     Training Step: 30 Training Loss: 3.3374977111816406 \n",
      "     Training Step: 31 Training Loss: 2.4525814056396484 \n",
      "     Training Step: 32 Training Loss: 4.283289909362793 \n",
      "     Training Step: 33 Training Loss: 2.247608184814453 \n",
      "     Training Step: 34 Training Loss: 2.582547187805176 \n",
      "     Training Step: 35 Training Loss: 3.1423239707946777 \n",
      "     Training Step: 36 Training Loss: 3.1690618991851807 \n",
      "     Training Step: 37 Training Loss: 2.255967855453491 \n",
      "     Training Step: 38 Training Loss: 2.614205837249756 \n",
      "     Training Step: 39 Training Loss: 3.563504219055176 \n",
      "     Training Step: 40 Training Loss: 3.2040281295776367 \n",
      "     Training Step: 41 Training Loss: 3.223921775817871 \n",
      "     Training Step: 42 Training Loss: 2.396366596221924 \n",
      "     Training Step: 43 Training Loss: 2.670949935913086 \n",
      "     Training Step: 44 Training Loss: 2.725729465484619 \n",
      "     Training Step: 45 Training Loss: 2.706000804901123 \n",
      "     Training Step: 46 Training Loss: 3.4419779777526855 \n",
      "     Training Step: 47 Training Loss: 3.457613468170166 \n",
      "     Training Step: 48 Training Loss: 3.2006871700286865 \n",
      "     Training Step: 49 Training Loss: 3.34460711479187 \n",
      "     Training Step: 50 Training Loss: 3.3776373863220215 \n",
      "     Training Step: 51 Training Loss: 3.185666084289551 \n",
      "     Training Step: 52 Training Loss: 3.3949289321899414 \n",
      "     Training Step: 53 Training Loss: 3.384164571762085 \n",
      "     Training Step: 54 Training Loss: 2.516188144683838 \n",
      "     Training Step: 55 Training Loss: 2.092073917388916 \n",
      "     Training Step: 56 Training Loss: 2.309370756149292 \n",
      "     Training Step: 57 Training Loss: 4.722136497497559 \n",
      "     Training Step: 58 Training Loss: 3.783351421356201 \n",
      "     Training Step: 59 Training Loss: 2.372407913208008 \n",
      "     Training Step: 60 Training Loss: 2.2272753715515137 \n",
      "     Training Step: 61 Training Loss: 2.9551210403442383 \n",
      "     Training Step: 62 Training Loss: 3.1649792194366455 \n",
      "     Training Step: 63 Training Loss: 3.196277141571045 \n",
      "     Training Step: 64 Training Loss: 2.88639760017395 \n",
      "     Training Step: 65 Training Loss: 3.479543924331665 \n",
      "     Training Step: 66 Training Loss: 3.0024828910827637 \n",
      "     Training Step: 67 Training Loss: 2.6358532905578613 \n",
      "     Training Step: 68 Training Loss: 2.832078695297241 \n",
      "     Training Step: 69 Training Loss: 3.780742883682251 \n",
      "     Training Step: 70 Training Loss: 2.852309226989746 \n",
      "     Training Step: 71 Training Loss: 2.3453757762908936 \n",
      "     Training Step: 72 Training Loss: 2.7398672103881836 \n",
      "     Training Step: 73 Training Loss: 2.388777017593384 \n",
      "     Training Step: 74 Training Loss: 2.8629050254821777 \n",
      "     Training Step: 75 Training Loss: 2.634068489074707 \n",
      "     Training Step: 76 Training Loss: 2.4481234550476074 \n",
      "     Training Step: 77 Training Loss: 3.7633752822875977 \n",
      "     Training Step: 78 Training Loss: 2.072077751159668 \n",
      "     Training Step: 79 Training Loss: 2.3706610202789307 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 2.996957778930664 \n",
      "     Validation Step: 1 Validation Loss: 2.5996549129486084 \n",
      "     Validation Step: 2 Validation Loss: 3.5237832069396973 \n",
      "     Validation Step: 3 Validation Loss: 2.846036911010742 \n",
      "     Validation Step: 4 Validation Loss: 3.755807876586914 \n",
      "     Validation Step: 5 Validation Loss: 3.119060516357422 \n",
      "     Validation Step: 6 Validation Loss: 3.4947004318237305 \n",
      "     Validation Step: 7 Validation Loss: 2.8715219497680664 \n",
      "     Validation Step: 8 Validation Loss: 3.435213565826416 \n",
      "     Validation Step: 9 Validation Loss: 2.869967222213745 \n",
      "     Validation Step: 10 Validation Loss: 2.362992286682129 \n",
      "     Validation Step: 11 Validation Loss: 3.3197643756866455 \n",
      "     Validation Step: 12 Validation Loss: 3.619647979736328 \n",
      "     Validation Step: 13 Validation Loss: 3.848829746246338 \n",
      "     Validation Step: 14 Validation Loss: 2.664219617843628 \n",
      "Epoch: 117\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.114311695098877 \n",
      "     Training Step: 1 Training Loss: 2.101064920425415 \n",
      "     Training Step: 2 Training Loss: 2.799286365509033 \n",
      "     Training Step: 3 Training Loss: 3.1056199073791504 \n",
      "     Training Step: 4 Training Loss: 2.7768421173095703 \n",
      "     Training Step: 5 Training Loss: 3.4538931846618652 \n",
      "     Training Step: 6 Training Loss: 2.4818763732910156 \n",
      "     Training Step: 7 Training Loss: 3.753843069076538 \n",
      "     Training Step: 8 Training Loss: 2.5273890495300293 \n",
      "     Training Step: 9 Training Loss: 3.4054436683654785 \n",
      "     Training Step: 10 Training Loss: 2.1618032455444336 \n",
      "     Training Step: 11 Training Loss: 2.520028591156006 \n",
      "     Training Step: 12 Training Loss: 3.5299072265625 \n",
      "     Training Step: 13 Training Loss: 2.3544769287109375 \n",
      "     Training Step: 14 Training Loss: 3.007082223892212 \n",
      "     Training Step: 15 Training Loss: 2.883880138397217 \n",
      "     Training Step: 16 Training Loss: 3.178610324859619 \n",
      "     Training Step: 17 Training Loss: 3.1875855922698975 \n",
      "     Training Step: 18 Training Loss: 3.1262526512145996 \n",
      "     Training Step: 19 Training Loss: 3.404749870300293 \n",
      "     Training Step: 20 Training Loss: 2.7784814834594727 \n",
      "     Training Step: 21 Training Loss: 3.366955518722534 \n",
      "     Training Step: 22 Training Loss: 3.3685784339904785 \n",
      "     Training Step: 23 Training Loss: 2.6693150997161865 \n",
      "     Training Step: 24 Training Loss: 3.3776140213012695 \n",
      "     Training Step: 25 Training Loss: 2.3252596855163574 \n",
      "     Training Step: 26 Training Loss: 4.3069047927856445 \n",
      "     Training Step: 27 Training Loss: 2.073556661605835 \n",
      "     Training Step: 28 Training Loss: 2.930051803588867 \n",
      "     Training Step: 29 Training Loss: 4.312292575836182 \n",
      "     Training Step: 30 Training Loss: 3.6047749519348145 \n",
      "     Training Step: 31 Training Loss: 2.242342948913574 \n",
      "     Training Step: 32 Training Loss: 3.50056791305542 \n",
      "     Training Step: 33 Training Loss: 4.189430236816406 \n",
      "     Training Step: 34 Training Loss: 2.2826991081237793 \n",
      "     Training Step: 35 Training Loss: 3.1241512298583984 \n",
      "     Training Step: 36 Training Loss: 2.5000009536743164 \n",
      "     Training Step: 37 Training Loss: 3.842679023742676 \n",
      "     Training Step: 38 Training Loss: 2.9329607486724854 \n",
      "     Training Step: 39 Training Loss: 2.4260001182556152 \n",
      "     Training Step: 40 Training Loss: 2.3047237396240234 \n",
      "     Training Step: 41 Training Loss: 2.424482583999634 \n",
      "     Training Step: 42 Training Loss: 3.1439993381500244 \n",
      "     Training Step: 43 Training Loss: 3.158565044403076 \n",
      "     Training Step: 44 Training Loss: 2.7770514488220215 \n",
      "     Training Step: 45 Training Loss: 3.2306618690490723 \n",
      "     Training Step: 46 Training Loss: 2.7263107299804688 \n",
      "     Training Step: 47 Training Loss: 2.7326056957244873 \n",
      "     Training Step: 48 Training Loss: 3.163259506225586 \n",
      "     Training Step: 49 Training Loss: 3.2775418758392334 \n",
      "     Training Step: 50 Training Loss: 2.9964358806610107 \n",
      "     Training Step: 51 Training Loss: 2.86460280418396 \n",
      "     Training Step: 52 Training Loss: 2.9483864307403564 \n",
      "     Training Step: 53 Training Loss: 2.4464495182037354 \n",
      "     Training Step: 54 Training Loss: 3.9108219146728516 \n",
      "     Training Step: 55 Training Loss: 3.107656240463257 \n",
      "     Training Step: 56 Training Loss: 2.642484664916992 \n",
      "     Training Step: 57 Training Loss: 3.3371715545654297 \n",
      "     Training Step: 58 Training Loss: 2.5977046489715576 \n",
      "     Training Step: 59 Training Loss: 2.5443575382232666 \n",
      "     Training Step: 60 Training Loss: 2.902135133743286 \n",
      "     Training Step: 61 Training Loss: 2.9886515140533447 \n",
      "     Training Step: 62 Training Loss: 3.373671531677246 \n",
      "     Training Step: 63 Training Loss: 2.4114623069763184 \n",
      "     Training Step: 64 Training Loss: 4.740794658660889 \n",
      "     Training Step: 65 Training Loss: 2.8937759399414062 \n",
      "     Training Step: 66 Training Loss: 3.7418365478515625 \n",
      "     Training Step: 67 Training Loss: 2.8753745555877686 \n",
      "     Training Step: 68 Training Loss: 2.1679601669311523 \n",
      "     Training Step: 69 Training Loss: 4.171690940856934 \n",
      "     Training Step: 70 Training Loss: 2.655672073364258 \n",
      "     Training Step: 71 Training Loss: 3.5064234733581543 \n",
      "     Training Step: 72 Training Loss: 2.1632070541381836 \n",
      "     Training Step: 73 Training Loss: 3.0994136333465576 \n",
      "     Training Step: 74 Training Loss: 2.5747363567352295 \n",
      "     Training Step: 75 Training Loss: 2.599743366241455 \n",
      "     Training Step: 76 Training Loss: 3.367751121520996 \n",
      "     Training Step: 77 Training Loss: 2.470771312713623 \n",
      "     Training Step: 78 Training Loss: 3.331718921661377 \n",
      "     Training Step: 79 Training Loss: 3.3790760040283203 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.0993833541870117 \n",
      "     Validation Step: 1 Validation Loss: 3.7130582332611084 \n",
      "     Validation Step: 2 Validation Loss: 3.140885829925537 \n",
      "     Validation Step: 3 Validation Loss: 3.0873050689697266 \n",
      "     Validation Step: 4 Validation Loss: 2.91176176071167 \n",
      "     Validation Step: 5 Validation Loss: 3.0080337524414062 \n",
      "     Validation Step: 6 Validation Loss: 3.7297306060791016 \n",
      "     Validation Step: 7 Validation Loss: 2.7209811210632324 \n",
      "     Validation Step: 8 Validation Loss: 3.4268012046813965 \n",
      "     Validation Step: 9 Validation Loss: 3.695101261138916 \n",
      "     Validation Step: 10 Validation Loss: 3.1860296726226807 \n",
      "     Validation Step: 11 Validation Loss: 3.9644832611083984 \n",
      "     Validation Step: 12 Validation Loss: 2.2774436473846436 \n",
      "     Validation Step: 13 Validation Loss: 2.793339252471924 \n",
      "     Validation Step: 14 Validation Loss: 3.7026450634002686 \n",
      "Epoch: 118\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.639540672302246 \n",
      "     Training Step: 1 Training Loss: 4.283015251159668 \n",
      "     Training Step: 2 Training Loss: 2.2183964252471924 \n",
      "     Training Step: 3 Training Loss: 2.7839431762695312 \n",
      "     Training Step: 4 Training Loss: 4.365029335021973 \n",
      "     Training Step: 5 Training Loss: 2.563002109527588 \n",
      "     Training Step: 6 Training Loss: 3.4435577392578125 \n",
      "     Training Step: 7 Training Loss: 3.5329084396362305 \n",
      "     Training Step: 8 Training Loss: 2.1181254386901855 \n",
      "     Training Step: 9 Training Loss: 3.241787910461426 \n",
      "     Training Step: 10 Training Loss: 3.0641772747039795 \n",
      "     Training Step: 11 Training Loss: 2.540522813796997 \n",
      "     Training Step: 12 Training Loss: 3.2491583824157715 \n",
      "     Training Step: 13 Training Loss: 3.1862130165100098 \n",
      "     Training Step: 14 Training Loss: 3.008882522583008 \n",
      "     Training Step: 15 Training Loss: 3.4835093021392822 \n",
      "     Training Step: 16 Training Loss: 2.5733795166015625 \n",
      "     Training Step: 17 Training Loss: 3.193955898284912 \n",
      "     Training Step: 18 Training Loss: 4.447254180908203 \n",
      "     Training Step: 19 Training Loss: 2.7416272163391113 \n",
      "     Training Step: 20 Training Loss: 3.8925094604492188 \n",
      "     Training Step: 21 Training Loss: 2.457242488861084 \n",
      "     Training Step: 22 Training Loss: 2.1317851543426514 \n",
      "     Training Step: 23 Training Loss: 3.753140687942505 \n",
      "     Training Step: 24 Training Loss: 2.799438953399658 \n",
      "     Training Step: 25 Training Loss: 2.6359617710113525 \n",
      "     Training Step: 26 Training Loss: 2.6391191482543945 \n",
      "     Training Step: 27 Training Loss: 3.0226352214813232 \n",
      "     Training Step: 28 Training Loss: 3.18959379196167 \n",
      "     Training Step: 29 Training Loss: 3.199270009994507 \n",
      "     Training Step: 30 Training Loss: 2.133272409439087 \n",
      "     Training Step: 31 Training Loss: 3.0509090423583984 \n",
      "     Training Step: 32 Training Loss: 2.9579033851623535 \n",
      "     Training Step: 33 Training Loss: 3.5375802516937256 \n",
      "     Training Step: 34 Training Loss: 3.7754180431365967 \n",
      "     Training Step: 35 Training Loss: 2.5564520359039307 \n",
      "     Training Step: 36 Training Loss: 3.0461208820343018 \n",
      "     Training Step: 37 Training Loss: 3.419884443283081 \n",
      "     Training Step: 38 Training Loss: 2.4861180782318115 \n",
      "     Training Step: 39 Training Loss: 2.359532356262207 \n",
      "     Training Step: 40 Training Loss: 2.5665206909179688 \n",
      "     Training Step: 41 Training Loss: 3.410984992980957 \n",
      "     Training Step: 42 Training Loss: 2.0673422813415527 \n",
      "     Training Step: 43 Training Loss: 2.1873769760131836 \n",
      "     Training Step: 44 Training Loss: 2.9589924812316895 \n",
      "     Training Step: 45 Training Loss: 3.23801851272583 \n",
      "     Training Step: 46 Training Loss: 3.355760335922241 \n",
      "     Training Step: 47 Training Loss: 2.781277656555176 \n",
      "     Training Step: 48 Training Loss: 2.8946235179901123 \n",
      "     Training Step: 49 Training Loss: 2.969050645828247 \n",
      "     Training Step: 50 Training Loss: 2.8366923332214355 \n",
      "     Training Step: 51 Training Loss: 2.7621686458587646 \n",
      "     Training Step: 52 Training Loss: 2.744326591491699 \n",
      "     Training Step: 53 Training Loss: 3.8569955825805664 \n",
      "     Training Step: 54 Training Loss: 2.991999864578247 \n",
      "     Training Step: 55 Training Loss: 2.7444286346435547 \n",
      "     Training Step: 56 Training Loss: 2.715848207473755 \n",
      "     Training Step: 57 Training Loss: 3.2288572788238525 \n",
      "     Training Step: 58 Training Loss: 2.6201374530792236 \n",
      "     Training Step: 59 Training Loss: 2.508044719696045 \n",
      "     Training Step: 60 Training Loss: 3.256213665008545 \n",
      "     Training Step: 61 Training Loss: 2.3679676055908203 \n",
      "     Training Step: 62 Training Loss: 3.6417887210845947 \n",
      "     Training Step: 63 Training Loss: 2.913808584213257 \n",
      "     Training Step: 64 Training Loss: 2.373244285583496 \n",
      "     Training Step: 65 Training Loss: 4.174419403076172 \n",
      "     Training Step: 66 Training Loss: 3.387328863143921 \n",
      "     Training Step: 67 Training Loss: 3.3940656185150146 \n",
      "     Training Step: 68 Training Loss: 2.5877633094787598 \n",
      "     Training Step: 69 Training Loss: 4.796707630157471 \n",
      "     Training Step: 70 Training Loss: 3.4730257987976074 \n",
      "     Training Step: 71 Training Loss: 3.1623592376708984 \n",
      "     Training Step: 72 Training Loss: 2.5333096981048584 \n",
      "     Training Step: 73 Training Loss: 2.3026559352874756 \n",
      "     Training Step: 74 Training Loss: 2.305558681488037 \n",
      "     Training Step: 75 Training Loss: 3.5059878826141357 \n",
      "     Training Step: 76 Training Loss: 2.1172213554382324 \n",
      "     Training Step: 77 Training Loss: 3.0664968490600586 \n",
      "     Training Step: 78 Training Loss: 2.8103227615356445 \n",
      "     Training Step: 79 Training Loss: 2.3036015033721924 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.1147255897521973 \n",
      "     Validation Step: 1 Validation Loss: 3.0191171169281006 \n",
      "     Validation Step: 2 Validation Loss: 3.7175936698913574 \n",
      "     Validation Step: 3 Validation Loss: 3.492964267730713 \n",
      "     Validation Step: 4 Validation Loss: 3.6346864700317383 \n",
      "     Validation Step: 5 Validation Loss: 3.567767381668091 \n",
      "     Validation Step: 6 Validation Loss: 3.811770439147949 \n",
      "     Validation Step: 7 Validation Loss: 2.29551362991333 \n",
      "     Validation Step: 8 Validation Loss: 3.2547178268432617 \n",
      "     Validation Step: 9 Validation Loss: 3.560353994369507 \n",
      "     Validation Step: 10 Validation Loss: 2.904757261276245 \n",
      "     Validation Step: 11 Validation Loss: 2.8481850624084473 \n",
      "     Validation Step: 12 Validation Loss: 2.587007999420166 \n",
      "     Validation Step: 13 Validation Loss: 2.59348726272583 \n",
      "     Validation Step: 14 Validation Loss: 2.984287738800049 \n",
      "Epoch: 119\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 3.3443241119384766 \n",
      "     Training Step: 1 Training Loss: 2.3782973289489746 \n",
      "     Training Step: 2 Training Loss: 2.693917751312256 \n",
      "     Training Step: 3 Training Loss: 4.337988376617432 \n",
      "     Training Step: 4 Training Loss: 2.915689468383789 \n",
      "     Training Step: 5 Training Loss: 2.2437186241149902 \n",
      "     Training Step: 6 Training Loss: 2.3967041969299316 \n",
      "     Training Step: 7 Training Loss: 2.4045987129211426 \n",
      "     Training Step: 8 Training Loss: 2.1279029846191406 \n",
      "     Training Step: 9 Training Loss: 2.857691764831543 \n",
      "     Training Step: 10 Training Loss: 3.0848872661590576 \n",
      "     Training Step: 11 Training Loss: 3.396183490753174 \n",
      "     Training Step: 12 Training Loss: 2.5701677799224854 \n",
      "     Training Step: 13 Training Loss: 3.94398832321167 \n",
      "     Training Step: 14 Training Loss: 2.823821783065796 \n",
      "     Training Step: 15 Training Loss: 2.5236527919769287 \n",
      "     Training Step: 16 Training Loss: 3.7320163249969482 \n",
      "     Training Step: 17 Training Loss: 2.272620677947998 \n",
      "     Training Step: 18 Training Loss: 2.682178497314453 \n",
      "     Training Step: 19 Training Loss: 3.758547782897949 \n",
      "     Training Step: 20 Training Loss: 2.5161690711975098 \n",
      "     Training Step: 21 Training Loss: 2.6650407314300537 \n",
      "     Training Step: 22 Training Loss: 3.2932426929473877 \n",
      "     Training Step: 23 Training Loss: 3.144105911254883 \n",
      "     Training Step: 24 Training Loss: 2.9515600204467773 \n",
      "     Training Step: 25 Training Loss: 2.2638449668884277 \n",
      "     Training Step: 26 Training Loss: 2.3301165103912354 \n",
      "     Training Step: 27 Training Loss: 3.0524203777313232 \n",
      "     Training Step: 28 Training Loss: 2.2057011127471924 \n",
      "     Training Step: 29 Training Loss: 2.9324142932891846 \n",
      "     Training Step: 30 Training Loss: 2.828829765319824 \n",
      "     Training Step: 31 Training Loss: 3.3273556232452393 \n",
      "     Training Step: 32 Training Loss: 3.2890725135803223 \n",
      "     Training Step: 33 Training Loss: 3.497802972793579 \n",
      "     Training Step: 34 Training Loss: 3.0948591232299805 \n",
      "     Training Step: 35 Training Loss: 3.060394048690796 \n",
      "     Training Step: 36 Training Loss: 2.5205585956573486 \n",
      "     Training Step: 37 Training Loss: 3.150892734527588 \n",
      "     Training Step: 38 Training Loss: 3.5374817848205566 \n",
      "     Training Step: 39 Training Loss: 2.467552661895752 \n",
      "     Training Step: 40 Training Loss: 3.2734904289245605 \n",
      "     Training Step: 41 Training Loss: 2.194334030151367 \n",
      "     Training Step: 42 Training Loss: 3.553924083709717 \n",
      "     Training Step: 43 Training Loss: 2.812267541885376 \n",
      "     Training Step: 44 Training Loss: 2.6738240718841553 \n",
      "     Training Step: 45 Training Loss: 3.1672093868255615 \n",
      "     Training Step: 46 Training Loss: 3.2831621170043945 \n",
      "     Training Step: 47 Training Loss: 2.544614791870117 \n",
      "     Training Step: 48 Training Loss: 2.9051804542541504 \n",
      "     Training Step: 49 Training Loss: 3.3119606971740723 \n",
      "     Training Step: 50 Training Loss: 2.998565673828125 \n",
      "     Training Step: 51 Training Loss: 4.29559850692749 \n",
      "     Training Step: 52 Training Loss: 2.979539394378662 \n",
      "     Training Step: 53 Training Loss: 2.3454740047454834 \n",
      "     Training Step: 54 Training Loss: 3.712247133255005 \n",
      "     Training Step: 55 Training Loss: 3.659151077270508 \n",
      "     Training Step: 56 Training Loss: 2.971073865890503 \n",
      "     Training Step: 57 Training Loss: 3.4549670219421387 \n",
      "     Training Step: 58 Training Loss: 2.9371132850646973 \n",
      "     Training Step: 59 Training Loss: 2.6655631065368652 \n",
      "     Training Step: 60 Training Loss: 4.024302959442139 \n",
      "     Training Step: 61 Training Loss: 2.088395118713379 \n",
      "     Training Step: 62 Training Loss: 4.070937156677246 \n",
      "     Training Step: 63 Training Loss: 2.758918285369873 \n",
      "     Training Step: 64 Training Loss: 3.451594352722168 \n",
      "     Training Step: 65 Training Loss: 2.148129463195801 \n",
      "     Training Step: 66 Training Loss: 4.707956790924072 \n",
      "     Training Step: 67 Training Loss: 2.3271262645721436 \n",
      "     Training Step: 68 Training Loss: 3.3669896125793457 \n",
      "     Training Step: 69 Training Loss: 3.221914768218994 \n",
      "     Training Step: 70 Training Loss: 2.9770069122314453 \n",
      "     Training Step: 71 Training Loss: 2.3546454906463623 \n",
      "     Training Step: 72 Training Loss: 2.9626877307891846 \n",
      "     Training Step: 73 Training Loss: 3.362628936767578 \n",
      "     Training Step: 74 Training Loss: 2.927015781402588 \n",
      "     Training Step: 75 Training Loss: 3.3729238510131836 \n",
      "     Training Step: 76 Training Loss: 2.9957199096679688 \n",
      "     Training Step: 77 Training Loss: 3.1210641860961914 \n",
      "     Training Step: 78 Training Loss: 2.5691945552825928 \n",
      "     Training Step: 79 Training Loss: 2.768237352371216 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6733455657958984 \n",
      "     Validation Step: 1 Validation Loss: 3.019639492034912 \n",
      "     Validation Step: 2 Validation Loss: 3.272742509841919 \n",
      "     Validation Step: 3 Validation Loss: 2.9232053756713867 \n",
      "     Validation Step: 4 Validation Loss: 3.38655424118042 \n",
      "     Validation Step: 5 Validation Loss: 2.2692501544952393 \n",
      "     Validation Step: 6 Validation Loss: 3.1075360774993896 \n",
      "     Validation Step: 7 Validation Loss: 3.1542763710021973 \n",
      "     Validation Step: 8 Validation Loss: 3.624091386795044 \n",
      "     Validation Step: 9 Validation Loss: 2.7081170082092285 \n",
      "     Validation Step: 10 Validation Loss: 3.0489721298217773 \n",
      "     Validation Step: 11 Validation Loss: 3.6231274604797363 \n",
      "     Validation Step: 12 Validation Loss: 3.8984642028808594 \n",
      "     Validation Step: 13 Validation Loss: 3.6296353340148926 \n",
      "     Validation Step: 14 Validation Loss: 2.6839590072631836 \n",
      "Epoch: 120\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 2.8184571266174316 \n",
      "     Training Step: 1 Training Loss: 2.7674596309661865 \n",
      "     Training Step: 2 Training Loss: 3.4459853172302246 \n",
      "     Training Step: 3 Training Loss: 4.399515151977539 \n",
      "     Training Step: 4 Training Loss: 2.3217082023620605 \n",
      "     Training Step: 5 Training Loss: 3.232769727706909 \n",
      "     Training Step: 6 Training Loss: 2.256201982498169 \n",
      "     Training Step: 7 Training Loss: 2.7522172927856445 \n",
      "     Training Step: 8 Training Loss: 3.6892247200012207 \n",
      "     Training Step: 9 Training Loss: 3.0006022453308105 \n",
      "     Training Step: 10 Training Loss: 3.7134017944335938 \n",
      "     Training Step: 11 Training Loss: 2.522428512573242 \n",
      "     Training Step: 12 Training Loss: 2.5861051082611084 \n",
      "     Training Step: 13 Training Loss: 2.393810510635376 \n",
      "     Training Step: 14 Training Loss: 2.4002747535705566 \n",
      "     Training Step: 15 Training Loss: 2.5161662101745605 \n",
      "     Training Step: 16 Training Loss: 4.060181617736816 \n",
      "     Training Step: 17 Training Loss: 2.6435325145721436 \n",
      "     Training Step: 18 Training Loss: 2.1327714920043945 \n",
      "     Training Step: 19 Training Loss: 3.3853721618652344 \n",
      "     Training Step: 20 Training Loss: 3.7346372604370117 \n",
      "     Training Step: 21 Training Loss: 2.454202175140381 \n",
      "     Training Step: 22 Training Loss: 2.099893093109131 \n",
      "     Training Step: 23 Training Loss: 4.218985080718994 \n",
      "     Training Step: 24 Training Loss: 2.1746931076049805 \n",
      "     Training Step: 25 Training Loss: 2.9654293060302734 \n",
      "     Training Step: 26 Training Loss: 3.4265940189361572 \n",
      "     Training Step: 27 Training Loss: 2.872528553009033 \n",
      "     Training Step: 28 Training Loss: 2.801743984222412 \n",
      "     Training Step: 29 Training Loss: 2.5411057472229004 \n",
      "     Training Step: 30 Training Loss: 3.622469902038574 \n",
      "     Training Step: 31 Training Loss: 3.35349178314209 \n",
      "     Training Step: 32 Training Loss: 2.562302589416504 \n",
      "     Training Step: 33 Training Loss: 3.2368721961975098 \n",
      "     Training Step: 34 Training Loss: 2.923961877822876 \n",
      "     Training Step: 35 Training Loss: 2.9005837440490723 \n",
      "     Training Step: 36 Training Loss: 2.950882911682129 \n",
      "     Training Step: 37 Training Loss: 3.169602394104004 \n",
      "     Training Step: 38 Training Loss: 3.487182140350342 \n",
      "     Training Step: 39 Training Loss: 3.1479525566101074 \n",
      "     Training Step: 40 Training Loss: 2.3862578868865967 \n",
      "     Training Step: 41 Training Loss: 2.6191916465759277 \n",
      "     Training Step: 42 Training Loss: 2.8667616844177246 \n",
      "     Training Step: 43 Training Loss: 2.1541309356689453 \n",
      "     Training Step: 44 Training Loss: 3.410121440887451 \n",
      "     Training Step: 45 Training Loss: 2.879927635192871 \n",
      "     Training Step: 46 Training Loss: 3.0943961143493652 \n",
      "     Training Step: 47 Training Loss: 2.939063310623169 \n",
      "     Training Step: 48 Training Loss: 2.736440420150757 \n",
      "     Training Step: 49 Training Loss: 2.1512720584869385 \n",
      "     Training Step: 50 Training Loss: 3.412177085876465 \n",
      "     Training Step: 51 Training Loss: 2.204451084136963 \n",
      "     Training Step: 52 Training Loss: 3.755263328552246 \n",
      "     Training Step: 53 Training Loss: 3.0738682746887207 \n",
      "     Training Step: 54 Training Loss: 2.936056137084961 \n",
      "     Training Step: 55 Training Loss: 3.4027888774871826 \n",
      "     Training Step: 56 Training Loss: 3.3913867473602295 \n",
      "     Training Step: 57 Training Loss: 4.736418724060059 \n",
      "     Training Step: 58 Training Loss: 3.5709056854248047 \n",
      "     Training Step: 59 Training Loss: 3.892063856124878 \n",
      "     Training Step: 60 Training Loss: 2.777259349822998 \n",
      "     Training Step: 61 Training Loss: 3.19533634185791 \n",
      "     Training Step: 62 Training Loss: 2.9460530281066895 \n",
      "     Training Step: 63 Training Loss: 2.9682533740997314 \n",
      "     Training Step: 64 Training Loss: 3.6004772186279297 \n",
      "     Training Step: 65 Training Loss: 2.760150909423828 \n",
      "     Training Step: 66 Training Loss: 4.445935249328613 \n",
      "     Training Step: 67 Training Loss: 3.3407087326049805 \n",
      "     Training Step: 68 Training Loss: 2.65269136428833 \n",
      "     Training Step: 69 Training Loss: 2.1738007068634033 \n",
      "     Training Step: 70 Training Loss: 3.171143054962158 \n",
      "     Training Step: 71 Training Loss: 2.4115469455718994 \n",
      "     Training Step: 72 Training Loss: 3.09187912940979 \n",
      "     Training Step: 73 Training Loss: 3.08156156539917 \n",
      "     Training Step: 74 Training Loss: 2.4558913707733154 \n",
      "     Training Step: 75 Training Loss: 2.122551202774048 \n",
      "     Training Step: 76 Training Loss: 2.921473741531372 \n",
      "     Training Step: 77 Training Loss: 3.0631933212280273 \n",
      "     Training Step: 78 Training Loss: 2.3566360473632812 \n",
      "     Training Step: 79 Training Loss: 2.889777660369873 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.6084280014038086 \n",
      "     Validation Step: 1 Validation Loss: 3.2343337535858154 \n",
      "     Validation Step: 2 Validation Loss: 3.1725916862487793 \n",
      "     Validation Step: 3 Validation Loss: 3.5506176948547363 \n",
      "     Validation Step: 4 Validation Loss: 2.6643166542053223 \n",
      "     Validation Step: 5 Validation Loss: 3.055948257446289 \n",
      "     Validation Step: 6 Validation Loss: 3.697077751159668 \n",
      "     Validation Step: 7 Validation Loss: 2.2536826133728027 \n",
      "     Validation Step: 8 Validation Loss: 3.704617500305176 \n",
      "     Validation Step: 9 Validation Loss: 3.975788116455078 \n",
      "     Validation Step: 10 Validation Loss: 3.033381938934326 \n",
      "     Validation Step: 11 Validation Loss: 2.691626787185669 \n",
      "     Validation Step: 12 Validation Loss: 2.9022395610809326 \n",
      "     Validation Step: 13 Validation Loss: 3.058525323867798 \n",
      "     Validation Step: 14 Validation Loss: 3.484422206878662 \n"
     ]
    }
   ],
   "source": [
    "for i in range(61, 120, 20) :\n",
    "    start_epoch = i\n",
    "    num_epochs = 20 + i\n",
    "    loss_weights = (1.0, 1.0, 1.0)\n",
    "    model_name = \"base_model_unsupervised_physics_constrained.pt\"\n",
    "    train_input,train_output,val_input, val_output = train_validate_ACOPF(model, model_name, optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-593.8486800193787 -243.53982478380203\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 9.08173089e-01, -1.83501959e+00, -5.90927734e+01,\n        -4.32426147e+01],\n       [ 9.80060136e-01,  0.00000000e+00,  9.09702148e+01,\n        -3.20258598e+01],\n       [ 1.03807630e+00, -1.48351908e+00, -5.60315399e+01,\n        -4.20662727e+01],\n       [ 1.03758441e+00,  1.54328156e+00, -8.17986012e-01,\n         7.37901390e-01],\n       [ 1.02849822e+00,  1.60070038e+00,  2.79828215e+00,\n         8.60434711e-01],\n       [ 1.02379331e+00,  1.61828518e+00,  2.95484495e+00,\n         1.07658315e+00],\n       [ 1.05319075e+00,  1.20048451e+00, -6.75554800e+00,\n        -1.33368182e+00],\n       [ 1.04970037e+00,  9.24345732e-01, -1.38833284e+01,\n        -2.18041611e+00],\n       [ 1.05298337e+00,  1.13721991e+00, -8.29765701e+00,\n        -1.68316507e+00],\n       [ 1.03909489e+00,  2.39133453e+00,  2.30472603e+01,\n         6.45330811e+00],\n       [ 1.06325094e+00, -8.12291145e-01, -5.12167511e+01,\n        -1.27561131e+01],\n       [ 1.06275891e+00,  5.54805279e-01, -2.24758530e+01,\n        -7.71057844e+00],\n       [ 1.11087286e+00, -1.08034587e+00, -5.49251671e+01,\n        -2.01222477e+01],\n       [ 1.04399310e+00,  6.38606310e-01, -2.13300972e+01,\n        -3.76218772e+00],\n       [ 1.03638708e+00,  1.57836246e+00, -1.58142567e-01,\n        -2.26446450e-01],\n       [ 1.11484070e+00,  2.71129608e+00,  3.74011345e+01,\n         1.65829735e+01],\n       [ 1.04800790e+00,  1.04877901e+00, -1.08388004e+01,\n        -9.84343350e-01],\n       [ 1.06187564e+00,  4.20533657e-01, -2.57897511e+01,\n        -8.35899353e+00],\n       [ 1.04667220e+00,  8.54233503e-01, -1.57999144e+01,\n        -2.50964928e+00],\n       [ 1.06539757e+00, -6.87378645e-01, -4.90792694e+01,\n        -1.07747526e+01],\n       [ 1.02700549e+00,  1.60033560e+00,  2.77895689e+00,\n         9.69036281e-01],\n       [ 1.03543417e+00,  1.56838799e+00, -4.76727962e-01,\n         2.44593799e-01],\n       [ 1.02984647e+00,  1.59551716e+00,  2.70907640e+00,\n         8.00673664e-01],\n       [ 1.06586664e+00,  5.02006054e-01, -2.36038208e+01,\n        -8.46225548e+00],\n       [ 1.04963691e+00,  1.20602751e+00, -7.01246834e+00,\n        -4.66603100e-01],\n       [ 1.03440482e+00,  1.59806633e+00, -1.05355740e-01,\n        -1.30631268e-01],\n       [ 1.09338767e+00,  2.67419767e+00,  3.54734154e+01,\n         1.68675766e+01],\n       [ 1.05735051e+00,  4.46565151e-02, -3.45174599e+01,\n        -1.08645945e+01],\n       [ 1.04879539e+00,  1.21448874e+00, -6.53895283e+00,\n        -1.32367253e+00],\n       [ 1.04342277e+00,  1.09987926e+00, -9.51759434e+00,\n        -2.23112369e+00],\n       [ 1.10490709e+00, -3.70104313e-02, -3.47962761e+01,\n        -1.86017208e+01],\n       [ 1.04623039e+00,  1.01613450e+00, -1.17663660e+01,\n        -1.00340390e+00],\n       [ 1.02900515e+00,  1.59984922e+00,  2.79258871e+00,\n         8.08014095e-01],\n       [ 1.02530927e+00,  1.62475967e+00,  3.26003313e+00,\n         1.46038699e+00],\n       [ 1.02368844e+00,  1.60961866e+00,  3.02610922e+00,\n         1.62239432e+00],\n       [ 1.06201997e+00, -9.00850296e-02, -3.73605499e+01,\n        -1.08894148e+01],\n       [ 1.03693182e+00,  1.56488276e+00, -3.30797195e-01,\n        -6.23310208e-02],\n       [ 1.06338501e+00,  1.99042082e-01, -3.09224625e+01,\n        -9.73800850e+00],\n       [ 1.04240612e+00,  1.27646208e+00, -5.38701153e+00,\n        -8.20039093e-01],\n       [ 1.04953565e+00,  1.17001128e+00, -7.57848072e+00,\n        -1.56568027e+00],\n       [ 1.06356250e+00,  5.47959566e-01, -2.25301609e+01,\n        -8.53936100e+00],\n       [ 1.05183709e+00,  1.23778319e+00, -5.87703037e+00,\n        -1.50604749e+00],\n       [ 1.05130449e+00,  1.22794557e+00, -6.15515661e+00,\n        -1.32155252e+00],\n       [ 1.05308221e+00,  1.15811086e+00, -7.79037762e+00,\n        -1.58836484e+00],\n       [ 1.04220928e+00,  6.41660690e-01, -2.13405972e+01,\n        -3.74810171e+00],\n       [ 1.04101694e+00,  3.45329285e-01, -2.85472622e+01,\n        -3.89841413e+00],\n       [ 1.05179631e+00,  1.21071792e+00, -6.54533386e+00,\n        -1.52291942e+00],\n       [ 1.04534447e+00,  2.38566589e+00,  2.35531597e+01,\n         7.09362507e+00],\n       [ 1.05292761e+00,  1.19509006e+00, -6.91728497e+00,\n        -1.41930890e+00],\n       [ 1.03545109e+00,  2.31351423e+00,  2.11422005e+01,\n         6.24062538e+00],\n       [ 1.02475579e+00,  1.61665726e+00,  3.06926107e+00,\n         1.43117452e+00],\n       [ 1.02805141e+00,  1.60891628e+00,  2.97897768e+00,\n         1.07247901e+00],\n       [ 1.05195403e+00,  1.03628588e+00, -1.08744020e+01,\n        -1.90766454e+00],\n       [ 1.05946364e+00,  4.51036692e-01, -2.51464062e+01,\n        -8.16252136e+00],\n       [ 1.05379444e+00,  1.13885927e+00, -8.23964405e+00,\n        -1.66914129e+00],\n       [ 1.04030803e+00,  3.25567722e-01, -2.89689369e+01,\n        -5.03463173e+00],\n       [ 1.05277446e+00,  1.11298800e+00, -8.89925766e+00,\n        -1.62536550e+00],\n       [ 1.05321558e+00,  1.16000962e+00, -7.73745680e+00,\n        -1.55750155e+00],\n       [ 1.02827717e+00,  1.60002565e+00,  2.76085043e+00,\n         8.79548252e-01],\n       [ 1.05259497e+00,  1.20167589e+00, -6.75953770e+00,\n        -1.40296769e+00],\n       [ 1.05170781e+00,  9.95320559e-01, -1.19557323e+01,\n        -1.98313093e+00],\n       [ 1.05246201e+00,  1.10347557e+00, -9.11016560e+00,\n        -1.74773812e+00],\n       [ 1.04958122e+00,  8.82838488e-01, -1.49465609e+01,\n        -2.44760275e+00],\n       [ 1.05187961e+00,  1.07718349e+00, -9.81683922e+00,\n        -1.76201677e+00]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(train_output.output[:, 2]), sum(train_output.output[:, 3]))\n",
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-511.95279839634895 -215.83535793423653\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 9.07937301e-01, -1.87421203e+00, -5.86987801e+01,\n        -4.31370201e+01],\n       [ 9.78537469e-01,  0.00000000e+00,  9.15387573e+01,\n        -3.18711929e+01],\n       [ 1.03759433e+00, -1.51700187e+00, -5.56474800e+01,\n        -4.19558372e+01],\n       [ 1.03786753e+00,  1.56929421e+00, -2.91552931e-01,\n         5.73150635e-01],\n       [ 1.02684687e+00,  1.61438823e+00,  2.79138613e+00,\n         9.32665348e-01],\n       [ 1.02212753e+00,  1.63119197e+00,  2.91438818e+00,\n         1.13415098e+00],\n       [ 1.05419103e+00,  1.20319247e+00, -6.55964136e+00,\n        -1.39674544e+00],\n       [ 1.05039056e+00,  9.21392202e-01, -1.37815561e+01,\n        -2.24824166e+00],\n       [ 1.05342338e+00,  1.15047312e+00, -7.85218763e+00,\n        -1.60835695e+00],\n       [ 1.03538874e+00,  2.41840339e+00,  2.30585728e+01,\n         6.94025660e+00],\n       [ 1.06390915e+00, -8.37588310e-01, -5.08798523e+01,\n        -1.26762056e+01],\n       [ 1.06576177e+00,  5.09796381e-01, -2.31083965e+01,\n        -8.29451561e+00],\n       [ 1.10853521e+00, -1.09562016e+00, -5.43942871e+01,\n        -1.96498795e+01],\n       [ 1.04392242e+00,  6.43275738e-01, -2.09690762e+01,\n        -3.62716198e+00],\n       [ 1.03664391e+00,  1.60454869e+00,  3.56376737e-01,\n        -3.78850937e-01],\n       [ 1.11313026e+00,  2.74871421e+00,  3.75459595e+01,\n         1.64656582e+01],\n       [ 1.04838319e+00,  1.05062175e+00, -1.06897240e+01,\n        -9.46598053e-01],\n       [ 1.06335775e+00,  4.12539959e-01, -2.55640736e+01,\n        -8.69000244e+00],\n       [ 1.04673282e+00,  8.57007980e-01, -1.55750837e+01,\n        -2.40364528e+00],\n       [ 1.06625338e+00, -7.12612152e-01, -4.87778969e+01,\n        -1.07718163e+01],\n       [ 1.02520024e+00,  1.60813451e+00,  2.57109118e+00,\n         8.72414589e-01],\n       [ 1.03570543e+00,  1.59455371e+00,  4.24204767e-02,\n         8.58158916e-02],\n       [ 1.02876060e+00,  1.61393094e+00,  2.87852621e+00,\n         9.05761242e-01],\n       [ 1.06686214e+00,  4.90499735e-01, -2.35167732e+01,\n        -8.59967327e+00],\n       [ 1.05005965e+00,  1.21517229e+00, -6.70573092e+00,\n        -3.89456272e-01],\n       [ 1.03465972e+00,  1.62460160e+00,  4.10573572e-01,\n        -2.82017231e-01],\n       [ 1.10395910e+00,  2.77285409e+00,  3.75991096e+01,\n         1.66821957e+01],\n       [ 1.05832637e+00,  1.22981071e-02, -3.46597633e+01,\n        -1.07392855e+01],\n       [ 1.04958635e+00,  1.21394610e+00, -6.45953321e+00,\n        -1.32239532e+00],\n       [ 1.04360254e+00,  1.10471535e+00, -9.33998680e+00,\n        -2.11040664e+00],\n       [ 1.03382131e+00,  2.90489364e+00,  3.31208191e+01,\n         8.79098511e+00],\n       [ 1.04661796e+00,  1.01696968e+00, -1.16286840e+01,\n        -9.67914581e-01],\n       [ 1.02757374e+00,  1.61341977e+00,  2.78863668e+00,\n         8.24258327e-01],\n       [ 1.02408746e+00,  1.63723826e+00,  3.22514486e+00,\n         1.43825531e+00],\n       [ 1.02224260e+00,  1.62494397e+00,  3.07503557e+00,\n         1.71696568e+00],\n       [ 1.06244354e+00, -9.74833965e-02, -3.69519081e+01,\n        -1.09176979e+01],\n       [ 1.03718948e+00,  1.59092259e+00,  1.84925169e-01,\n        -2.17633739e-01],\n       [ 1.06213760e+00,  2.03928709e-01, -3.04188366e+01,\n        -9.28130627e+00],\n       [ 1.04291569e+00,  1.28545308e+00, -5.10510206e+00,\n        -7.30353355e-01],\n       [ 1.05034270e+00,  1.17709088e+00, -7.27609539e+00,\n        -1.56540537e+00],\n       [ 1.06302365e+00,  5.46265602e-01, -2.23000622e+01,\n        -8.24017239e+00],\n       [ 1.05251389e+00,  1.24390006e+00, -5.63780022e+00,\n        -1.48716426e+00],\n       [ 1.05183092e+00,  1.22932982e+00, -6.05599022e+00,\n        -1.26505089e+00],\n       [ 1.05366565e+00,  1.16302705e+00, -7.56571531e+00,\n        -1.54746771e+00],\n       [ 1.04278280e+00,  6.29742146e-01, -2.13358345e+01,\n        -3.73330069e+00],\n       [ 1.04147651e+00,  3.35354567e-01, -2.83615379e+01,\n        -3.88021851e+00],\n       [ 1.05260807e+00,  1.21566653e+00, -6.31445551e+00,\n        -1.53221798e+00],\n       [ 1.03565771e+00,  2.35670972e+00,  2.17177143e+01,\n         6.64623117e+00],\n       [ 1.05362105e+00,  1.19829369e+00, -6.74004698e+00,\n        -1.40678382e+00],\n       [ 1.03633076e+00,  2.34963393e+00,  2.15431976e+01,\n         6.00116968e+00],\n       [ 1.02374913e+00,  1.63063979e+00,  3.08254194e+00,\n         1.36198950e+00],\n       [ 1.02660273e+00,  1.62159944e+00,  2.94081450e+00,\n         1.06041336e+00],\n       [ 1.05243177e+00,  1.03496718e+00, -1.07991285e+01,\n        -1.89943600e+00],\n       [ 1.05596931e+00,  4.97182846e-01, -2.37191906e+01,\n        -8.76403713e+00],\n       [ 1.05413430e+00,  1.14878464e+00, -7.89678144e+00,\n        -1.56792021e+00],\n       [ 1.04069984e+00,  3.14863920e-01, -2.87940044e+01,\n        -4.97088480e+00],\n       [ 1.05342858e+00,  1.11741328e+00, -8.65817070e+00,\n        -1.60007501e+00],\n       [ 1.05359691e+00,  1.16640639e+00, -7.49538946e+00,\n        -1.46536803e+00],\n       [ 1.02733265e+00,  1.61643720e+00,  2.85800314e+00,\n         9.82812881e-01],\n       [ 1.05317327e+00,  1.20813537e+00, -6.50923920e+00,\n        -1.36070895e+00],\n       [ 1.05211938e+00,  9.96773243e-01, -1.17909842e+01,\n        -1.98481584e+00],\n       [ 1.05259060e+00,  1.11171269e+00, -8.83128929e+00,\n        -1.61827540e+00],\n       [ 1.05000777e+00,  8.75979900e-01, -1.49445992e+01,\n        -2.42398238e+00],\n       [ 1.05225712e+00,  1.08231473e+00, -9.59457111e+00,\n        -1.72301269e+00]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(val_output.output[:, 2]), sum(val_output.output[:, 3]))\n",
    "val_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_ACOPFGNN_model(model: ACOPFGNN):\n",
    "    path = r\"./Models/SelfSupervised/hetero_model_unsupervised.pt\"\n",
    "\n",
    "    torch.save(model.state_dict(), path)\n",
    "save_ACOPFGNN_model(embedder_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CHAINED ACOPF MODELS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'SB': tensor([[-0.0883,  0.3326, -0.3693, -0.4353]], grad_fn=<ViewBackward0>), 'PQ': tensor([[-0.3356,  0.2627,  0.2500,  0.3009],\n",
      "        [-0.0228,  0.3043,  0.1143, -0.2969],\n",
      "        [ 0.0462,  0.2698, -0.1480, -0.0254],\n",
      "        [ 0.2407, -0.2489, -0.0174,  0.1618],\n",
      "        [ 0.1517,  0.1745, -0.1778, -0.0521],\n",
      "        [ 0.0635,  0.2112,  0.2692,  0.4399],\n",
      "        [-0.2069,  0.3448, -0.0291,  0.1635],\n",
      "        [ 0.2074, -0.1589,  0.2907,  0.0578],\n",
      "        [-0.0972, -0.0437,  0.3219,  0.0769],\n",
      "        [-0.2497, -0.1490,  0.4195,  0.2763],\n",
      "        [-0.2350,  0.1043,  0.0320, -0.0035],\n",
      "        [-0.2215,  0.1045,  0.4921, -0.0283],\n",
      "        [ 0.1289,  0.0789, -0.1261, -0.2991],\n",
      "        [-0.0843,  0.0816, -0.6240,  0.1517],\n",
      "        [-0.2801, -0.5531, -0.0616, -0.2087],\n",
      "        [ 0.1237,  0.0967,  0.3160,  0.3506]], grad_fn=<ViewBackward0>), 'PV': tensor([[ 1.9474e-02, -1.6949e-01,  2.1136e-01, -1.3980e-01],\n",
      "        [ 4.4139e-04,  2.2290e-01,  8.6621e-03,  2.5655e-01],\n",
      "        [-2.1642e-01, -2.7718e-01,  1.0412e-01,  3.0491e-01],\n",
      "        [ 2.4332e-01, -5.5523e-02,  4.0602e-02, -1.3428e-01],\n",
      "        [-4.1828e-02,  4.1334e-02,  1.5780e-01, -1.2220e-01],\n",
      "        [-1.0450e-01, -3.1357e-01, -6.2277e-02,  1.1286e-01],\n",
      "        [-2.3241e-01, -2.4840e-01, -2.7584e-01,  9.3012e-02],\n",
      "        [ 7.7575e-03,  1.2576e-01,  4.1334e-02, -3.4267e-02],\n",
      "        [ 1.5718e-01, -5.1464e-01, -2.3353e-01, -1.2838e-01],\n",
      "        [-3.2703e-01,  1.3284e-01,  6.9783e-02,  1.5421e-02],\n",
      "        [ 4.3525e-02,  7.4332e-02, -5.6546e-03,  2.7451e-02],\n",
      "        [-4.3762e-02, -1.0963e-01,  1.3595e-01, -1.4521e-01],\n",
      "        [ 3.2266e-02,  2.3634e-01, -3.9007e-01,  1.0855e-01],\n",
      "        [-2.3048e-01, -1.0297e-01, -1.2499e-01,  2.2186e-01],\n",
      "        [-3.8211e-02,  3.5816e-02,  8.5377e-02, -1.7747e-01],\n",
      "        [ 9.3011e-02, -7.4360e-02, -3.1375e-01,  3.3732e-03],\n",
      "        [ 2.8333e-02,  1.3953e-01, -1.6600e-01,  1.9732e-01],\n",
      "        [ 1.0222e-01,  2.2391e-01,  1.8278e-01, -9.0447e-02],\n",
      "        [ 1.9109e-01,  7.5754e-02,  1.6199e-01,  1.1619e-01],\n",
      "        [ 2.1461e-01,  3.7264e-01, -1.0727e-01, -4.7190e-01],\n",
      "        [ 6.5811e-02, -9.2679e-02, -1.7504e-01, -1.3509e-01],\n",
      "        [-2.3054e-01, -2.6304e-02, -6.3707e-02,  2.6792e-01],\n",
      "        [-3.6473e-01, -2.7831e-01,  1.6057e-02, -1.9880e-01],\n",
      "        [-1.4871e-01,  1.2526e-01, -2.0769e-01, -6.7541e-02],\n",
      "        [-2.7338e-01,  3.5241e-01, -2.0386e-01, -1.3321e-01],\n",
      "        [ 4.7972e-02, -3.1010e-01, -1.4042e-01, -1.9997e-01],\n",
      "        [-2.2570e-02,  2.5806e-01, -1.0408e-01, -8.9247e-02],\n",
      "        [ 2.2623e-01,  1.3597e-01,  2.2363e-01, -1.5481e-01],\n",
      "        [-5.2309e-01, -6.4034e-02, -3.4029e-03, -2.8514e-01],\n",
      "        [-1.8612e-02, -4.7425e-01,  1.5029e-01,  1.9696e-01],\n",
      "        [-3.9821e-02,  4.7912e-01,  2.5572e-01, -5.4494e-02],\n",
      "        [ 3.3188e-01,  3.3627e-01, -4.4867e-02,  5.6846e-02],\n",
      "        [-2.2828e-01, -4.5279e-01, -3.5610e-01, -7.6667e-02],\n",
      "        [ 4.0145e-01,  3.5739e-01, -6.8926e-02, -4.5534e-02],\n",
      "        [-2.0602e-01, -2.0090e-01, -2.6006e-01, -1.5708e-01],\n",
      "        [-1.6986e-01, -4.9486e-02,  1.3539e-01,  1.5987e-01],\n",
      "        [-3.9516e-02, -1.6266e-01,  2.6829e-01, -2.7236e-01],\n",
      "        [-3.0389e-01,  2.1376e-01,  2.3814e-01,  1.1530e-01],\n",
      "        [-2.3785e-01, -1.6346e-01,  2.7533e-02, -2.5366e-01],\n",
      "        [-3.4817e-01, -4.6501e-02, -2.9885e-01, -6.4927e-02],\n",
      "        [ 1.7509e-01, -1.6020e-01, -1.7241e-01, -7.6374e-02],\n",
      "        [-5.9193e-02,  1.4306e-01, -1.6215e-01,  1.7070e-01]],\n",
      "       grad_fn=<ViewBackward0>), 'NB': tensor([[-0.0795,  0.0251, -0.2600, -0.1755],\n",
      "        [ 0.1498, -0.3332,  0.1320, -0.0991],\n",
      "        [ 0.0313, -0.4407,  0.4393,  0.1268],\n",
      "        [ 0.2423,  0.2994, -0.2939, -0.1639],\n",
      "        [-0.0938,  0.1742, -0.2466, -0.0346]], grad_fn=<ViewBackward0>)})\n"
     ]
    }
   ],
   "source": [
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "embedder_model = load_ACOPFGeneral_model(grid_name, \"hetero_model_unsupervised.pt\",16,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n",
      "no costs are given - overall generated power is minimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'SB': tensor([[1.7545, 5.1543]], grad_fn=<ViewBackward0>), 'PQ': tensor([[-13.4516,   4.2767],\n",
      "        [  1.3926, -12.6518],\n",
      "        [ 16.1819,   1.9141],\n",
      "        [ -1.2008,  -2.6754],\n",
      "        [ -9.9980,   5.2332],\n",
      "        [ 11.1948,  -2.5181],\n",
      "        [  9.4199,  18.1730],\n",
      "        [ -7.8838,   8.6412],\n",
      "        [ -3.0063,  -4.8404],\n",
      "        [ 13.5166,  -6.0103],\n",
      "        [ -9.9935,  15.4436],\n",
      "        [ -3.5777,  -1.6986],\n",
      "        [ -5.9964,  11.7302],\n",
      "        [-15.0285, -11.6641],\n",
      "        [  0.0987,  -6.1745],\n",
      "        [-14.4643,  -6.0757]], grad_fn=<ViewBackward0>), 'PV': tensor([[-1.1129e+00, -1.6090e+00],\n",
      "        [-2.4894e+00,  3.5176e-03],\n",
      "        [-5.0313e+00,  1.1413e+00],\n",
      "        [ 6.9062e-01,  6.2067e+00],\n",
      "        [ 2.2751e+00,  1.4278e+00],\n",
      "        [-1.2903e+00,  2.8614e+00],\n",
      "        [ 5.5064e+00, -2.4762e+00],\n",
      "        [-1.8402e-01, -3.8038e+00],\n",
      "        [-5.8433e+00, -2.4163e+00],\n",
      "        [ 1.1631e-01,  1.5367e+00],\n",
      "        [ 3.8560e+00, -5.5377e+00],\n",
      "        [-3.6326e+00,  4.6158e-01],\n",
      "        [ 4.7057e+00, -9.7498e-01],\n",
      "        [ 7.4784e+00, -4.1358e+00],\n",
      "        [-5.6681e+00,  2.8702e+00],\n",
      "        [-1.6012e+00,  3.4443e+00],\n",
      "        [-8.1482e+00,  2.3623e+00],\n",
      "        [-5.3072e+00, -4.9774e+00],\n",
      "        [-6.8172e+00,  4.2155e+00],\n",
      "        [ 1.0647e+00,  1.2439e+00],\n",
      "        [-2.1911e+00,  1.5955e+00],\n",
      "        [-4.9684e-01, -1.3922e+00],\n",
      "        [ 3.2857e+00,  2.9071e+00],\n",
      "        [-2.9702e+00, -3.0051e+00],\n",
      "        [ 2.9315e+00, -1.1348e+00],\n",
      "        [ 5.6778e-01,  2.0158e+00],\n",
      "        [ 2.2533e+00,  3.5689e+00],\n",
      "        [-7.8866e-01, -5.9847e+00],\n",
      "        [-2.3329e+00,  1.9742e+00],\n",
      "        [ 4.1871e+00,  3.6582e-01],\n",
      "        [ 1.8556e+00,  5.4211e+00],\n",
      "        [-4.4482e-01,  2.1532e+00],\n",
      "        [ 1.9100e+00,  5.6202e+00],\n",
      "        [-2.1210e+00,  1.3041e+00],\n",
      "        [ 3.0724e+00, -1.3662e+00],\n",
      "        [-3.7155e+00, -1.2975e+00],\n",
      "        [ 9.8515e+00,  3.0274e-01],\n",
      "        [-1.9814e+00, -3.1623e+00],\n",
      "        [-1.4914e+00, -2.9323e+00],\n",
      "        [ 1.0771e+00,  7.2988e+00],\n",
      "        [-1.8220e-01,  1.1550e+00],\n",
      "        [-5.6135e+00,  4.2588e-01]], grad_fn=<ViewBackward0>), 'NB': tensor([[  2.9116,   6.1236],\n",
      "        [ -5.6965,  -8.8414],\n",
      "        [ -7.1524,  -8.3103],\n",
      "        [ -5.2318,  -6.1610],\n",
      "        [ -8.7179, -13.4321]], grad_fn=<ViewBackward0>)})\n"
     ]
    }
   ],
   "source": [
    "# Create Minimizer, Enforcer and Embedder Models\n",
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "index_mappers,net,data = generate_unsupervised_input(grid_name)\n",
    "#minimizer_model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=1)\n",
    "#embedder_model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=1)\n",
    "\n",
    "#minimizer_model = create_ACOPFGNN_model(data, net, index_mappers, hidden_channels=4, num_layers=4)\n",
    "\n",
    "#embedder_model = create_ACOPFEmbedder_model(data, net, index_mappers, hidden_channels=4, num_layers=1)\n",
    "\n",
    "embedder_model = create_ACOPFEmbedder_Bus_model(data, net, index_mappers, hidden_channels=64, num_layers=1)\n",
    "\n",
    "\n",
    "#enforcer_model = create_ACOPFEnforcer_model(data, net, index_mappers, hidden_channels=4, num_layers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Initialize the Optimizer\n",
    "from itertools import chain\n",
    "#optimizer = torch.optim.Adam(chain(minimizer_model.parameters(), enforcer_model.parameters(), embedder_model.parameters()))\n",
    "ACOPF_optimizer = torch.optim.Adam(embedder_model.parameters(), lr=1e-4)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "ACOPF_optimizer.param_groups[0]['lr']*=0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mcanbay96\u001B[0m (\u001B[33mcbml\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\wandb\\run-20231030_004038-5wuxhdbd</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/5wuxhdbd' target=\"_blank\">unique-bird-320</a></strong> to <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/5wuxhdbd' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/5wuxhdbd</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_NOTEBOOK_NAME = \"msi\"\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"ACOPF-GNN-SelfSupervised-Hetero\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            #\"learning_rate\": optimizer.,\n",
    "            \"architecture\": \"Hetero-SelfSupervised-GNN\",\n",
    "            \"grid_name\": grid_name,\n",
    "            \"Training Type\": \"Powers-Voltages\"\n",
    "\n",
    "        }\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load Inputs\n",
    "inputs = load_unsupervised_inputs(grid_name)\n",
    "#inputs = load_multiple_unsupervised_inputs()\n",
    "n = len(inputs)\n",
    "train_inputs = inputs[:int(n*0.8)]\n",
    "val_inputs = inputs[int(n*0.8): int(n*0.95)]\n",
    "test_inputs = inputs[int(n*0.95):]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.897653291527138 \n",
      "     Training Step: 1 Training Loss: 4.475141410209497 \n",
      "     Training Step: 2 Training Loss: 4.7660729803494934 \n",
      "     Training Step: 3 Training Loss: 4.447364343142659 \n",
      "     Training Step: 4 Training Loss: 5.045784837261854 \n",
      "     Training Step: 5 Training Loss: 4.41347313147275 \n",
      "     Training Step: 6 Training Loss: 4.547844136122873 \n",
      "     Training Step: 7 Training Loss: 4.181905242035396 \n",
      "     Training Step: 8 Training Loss: 5.159821306537901 \n",
      "     Training Step: 9 Training Loss: 4.974220929677174 \n",
      "     Training Step: 10 Training Loss: 4.636464399150343 \n",
      "     Training Step: 11 Training Loss: 3.7346541411653082 \n",
      "     Training Step: 12 Training Loss: 3.7235663333428404 \n",
      "     Training Step: 13 Training Loss: 4.788501382430584 \n",
      "     Training Step: 14 Training Loss: 3.723599907818184 \n",
      "     Training Step: 15 Training Loss: 4.389425205858257 \n",
      "     Training Step: 16 Training Loss: 3.437640016696623 \n",
      "     Training Step: 17 Training Loss: 4.753487030528103 \n",
      "     Training Step: 18 Training Loss: 4.673650660853394 \n",
      "     Training Step: 19 Training Loss: 3.9157303034806312 \n",
      "     Training Step: 20 Training Loss: 3.2371867471624567 \n",
      "     Training Step: 21 Training Loss: 3.231924544995482 \n",
      "     Training Step: 22 Training Loss: 3.508715439912938 \n",
      "     Training Step: 23 Training Loss: 3.157306116245948 \n",
      "     Training Step: 24 Training Loss: 3.129793654970618 \n",
      "     Training Step: 25 Training Loss: 4.753780653195291 \n",
      "     Training Step: 26 Training Loss: 4.707592707714649 \n",
      "     Training Step: 27 Training Loss: 4.238636753138428 \n",
      "     Training Step: 28 Training Loss: 3.9990433862402024 \n",
      "     Training Step: 29 Training Loss: 4.1350181692531 \n",
      "     Training Step: 30 Training Loss: 3.351096249118279 \n",
      "     Training Step: 31 Training Loss: 3.9794665311072888 \n",
      "     Training Step: 32 Training Loss: 4.8238187566872845 \n",
      "     Training Step: 33 Training Loss: 4.035051907174411 \n",
      "     Training Step: 34 Training Loss: 4.604665529081455 \n",
      "     Training Step: 35 Training Loss: 3.916216662056085 \n",
      "     Training Step: 36 Training Loss: 3.7369558512282146 \n",
      "     Training Step: 37 Training Loss: 5.006595902994334 \n",
      "     Training Step: 38 Training Loss: 3.4674187399080205 \n",
      "     Training Step: 39 Training Loss: 4.671388304205153 \n",
      "     Training Step: 40 Training Loss: 4.331192650456415 \n",
      "     Training Step: 41 Training Loss: 3.4917832916088183 \n",
      "     Training Step: 42 Training Loss: 4.337951529696179 \n",
      "     Training Step: 43 Training Loss: 3.2561703867620615 \n",
      "     Training Step: 44 Training Loss: 4.641604893355503 \n",
      "     Training Step: 45 Training Loss: 3.798441554536016 \n",
      "     Training Step: 46 Training Loss: 3.3085580068397533 \n",
      "     Training Step: 47 Training Loss: 4.579669973390706 \n",
      "     Training Step: 48 Training Loss: 2.814029081210681 \n",
      "     Training Step: 49 Training Loss: 5.019034610008678 \n",
      "     Training Step: 50 Training Loss: 3.6752322821483916 \n",
      "     Training Step: 51 Training Loss: 2.855018364410031 \n",
      "     Training Step: 52 Training Loss: 4.631441677025222 \n",
      "     Training Step: 53 Training Loss: 5.171242220866179 \n",
      "     Training Step: 54 Training Loss: 4.731581449027331 \n",
      "     Training Step: 55 Training Loss: 3.076066433486638 \n",
      "     Training Step: 56 Training Loss: 4.556400299875327 \n",
      "     Training Step: 57 Training Loss: 5.157593487138956 \n",
      "     Training Step: 58 Training Loss: 3.466685466487741 \n",
      "     Training Step: 59 Training Loss: 4.836692680015016 \n",
      "     Training Step: 60 Training Loss: 4.184112771494513 \n",
      "     Training Step: 61 Training Loss: 4.633960291260939 \n",
      "     Training Step: 62 Training Loss: 2.912615700661504 \n",
      "     Training Step: 63 Training Loss: 5.305525988090095 \n",
      "     Training Step: 64 Training Loss: 4.69849000737926 \n",
      "     Training Step: 65 Training Loss: 4.6199372112405435 \n",
      "     Training Step: 66 Training Loss: 3.8040819079767036 \n",
      "     Training Step: 67 Training Loss: 4.810476153175261 \n",
      "     Training Step: 68 Training Loss: 3.7790939517390676 \n",
      "     Training Step: 69 Training Loss: 4.729480335636267 \n",
      "     Training Step: 70 Training Loss: 4.205393510500362 \n",
      "     Training Step: 71 Training Loss: 3.369030996081693 \n",
      "     Training Step: 72 Training Loss: 3.233257811778252 \n",
      "     Training Step: 73 Training Loss: 4.073513606276985 \n",
      "     Training Step: 74 Training Loss: 3.372837025556542 \n",
      "     Training Step: 75 Training Loss: 3.994126159724068 \n",
      "     Training Step: 76 Training Loss: 4.581215023297762 \n",
      "     Training Step: 77 Training Loss: 4.718741007958845 \n",
      "     Training Step: 78 Training Loss: 4.393628367126994 \n",
      "     Training Step: 79 Training Loss: 4.60999865341926 \n",
      "     Training Step: 80 Training Loss: 4.418479032332165 \n",
      "     Training Step: 81 Training Loss: 3.39960865872543 \n",
      "     Training Step: 82 Training Loss: 3.777959728944891 \n",
      "     Training Step: 83 Training Loss: 4.843071991658197 \n",
      "     Training Step: 84 Training Loss: 3.038651132318725 \n",
      "     Training Step: 85 Training Loss: 3.657312254545992 \n",
      "     Training Step: 86 Training Loss: 3.2302109750682133 \n",
      "     Training Step: 87 Training Loss: 2.8459860693150936 \n",
      "     Training Step: 88 Training Loss: 3.7832455367611404 \n",
      "     Training Step: 89 Training Loss: 4.386432815147702 \n",
      "     Training Step: 90 Training Loss: 4.289592579113626 \n",
      "     Training Step: 91 Training Loss: 4.081572529140615 \n",
      "     Training Step: 92 Training Loss: 3.8059972463312897 \n",
      "     Training Step: 93 Training Loss: 4.273304681054982 \n",
      "     Training Step: 94 Training Loss: 4.636122004168772 \n",
      "     Training Step: 95 Training Loss: 4.106784914884095 \n",
      "     Training Step: 96 Training Loss: 4.139467139958371 \n",
      "     Training Step: 97 Training Loss: 4.4984630876766785 \n",
      "     Training Step: 98 Training Loss: 3.7535730756077212 \n",
      "     Training Step: 99 Training Loss: 3.0202779753554134 \n",
      "     Training Step: 100 Training Loss: 5.23085932196425 \n",
      "     Training Step: 101 Training Loss: 3.8968545608873875 \n",
      "     Training Step: 102 Training Loss: 2.9069131241617474 \n",
      "     Training Step: 103 Training Loss: 3.6614027263656705 \n",
      "     Training Step: 104 Training Loss: 4.7730535252023705 \n",
      "     Training Step: 105 Training Loss: 4.283235404503087 \n",
      "     Training Step: 106 Training Loss: 4.370016800443045 \n",
      "     Training Step: 107 Training Loss: 4.083417799926029 \n",
      "     Training Step: 108 Training Loss: 4.318392466584546 \n",
      "     Training Step: 109 Training Loss: 5.067167727142761 \n",
      "     Training Step: 110 Training Loss: 4.392128437459853 \n",
      "     Training Step: 111 Training Loss: 4.765990707599616 \n",
      "     Training Step: 112 Training Loss: 4.243983039453076 \n",
      "     Training Step: 113 Training Loss: 3.593078953562129 \n",
      "     Training Step: 114 Training Loss: 4.709252072673882 \n",
      "     Training Step: 115 Training Loss: 4.567512844074038 \n",
      "     Training Step: 116 Training Loss: 4.964490395976237 \n",
      "     Training Step: 117 Training Loss: 3.615114069776574 \n",
      "     Training Step: 118 Training Loss: 4.541797800139579 \n",
      "     Training Step: 119 Training Loss: 4.769153944193798 \n",
      "     Training Step: 120 Training Loss: 4.214800590015479 \n",
      "     Training Step: 121 Training Loss: 4.627893995633181 \n",
      "     Training Step: 122 Training Loss: 3.6513985455461335 \n",
      "     Training Step: 123 Training Loss: 3.538393871607846 \n",
      "     Training Step: 124 Training Loss: 3.8198145406409774 \n",
      "     Training Step: 125 Training Loss: 4.809356162160119 \n",
      "     Training Step: 126 Training Loss: 3.113204382304326 \n",
      "     Training Step: 127 Training Loss: 4.596442178556355 \n",
      "     Training Step: 128 Training Loss: 4.229425999175756 \n",
      "     Training Step: 129 Training Loss: 4.305673705488317 \n",
      "     Training Step: 130 Training Loss: 3.774998012128219 \n",
      "     Training Step: 131 Training Loss: 3.3207969029018374 \n",
      "     Training Step: 132 Training Loss: 3.983470018598224 \n",
      "     Training Step: 133 Training Loss: 2.860011838523178 \n",
      "     Training Step: 134 Training Loss: 5.206720985825792 \n",
      "     Training Step: 135 Training Loss: 3.7914628848781193 \n",
      "     Training Step: 136 Training Loss: 3.858550520329401 \n",
      "     Training Step: 137 Training Loss: 3.27281826355305 \n",
      "     Training Step: 138 Training Loss: 3.4306900664681983 \n",
      "     Training Step: 139 Training Loss: 3.808035012656525 \n",
      "     Training Step: 140 Training Loss: 4.295166132111722 \n",
      "     Training Step: 141 Training Loss: 4.515444650026548 \n",
      "     Training Step: 142 Training Loss: 4.843302437234693 \n",
      "     Training Step: 143 Training Loss: 4.682263127521975 \n",
      "     Training Step: 144 Training Loss: 4.3833954327600715 \n",
      "     Training Step: 145 Training Loss: 2.96574323380077 \n",
      "     Training Step: 146 Training Loss: 3.4543586782166873 \n",
      "     Training Step: 147 Training Loss: 4.273830719179478 \n",
      "     Training Step: 148 Training Loss: 2.5832979745313382 \n",
      "     Training Step: 149 Training Loss: 3.6330413805416004 \n",
      "     Training Step: 150 Training Loss: 3.446760768005614 \n",
      "     Training Step: 151 Training Loss: 3.9627568170081946 \n",
      "     Training Step: 152 Training Loss: 4.616476044871878 \n",
      "     Training Step: 153 Training Loss: 3.392122687233785 \n",
      "     Training Step: 154 Training Loss: 3.262119090011687 \n",
      "     Training Step: 155 Training Loss: 3.8760363881115203 \n",
      "     Training Step: 156 Training Loss: 4.038745506062172 \n",
      "     Training Step: 157 Training Loss: 3.841426591155388 \n",
      "     Training Step: 158 Training Loss: 4.169911170137623 \n",
      "     Training Step: 159 Training Loss: 4.032502132484831 \n",
      "     Training Step: 160 Training Loss: 4.7427991179150935 \n",
      "     Training Step: 161 Training Loss: 3.8909108597547437 \n",
      "     Training Step: 162 Training Loss: 4.198390194467336 \n",
      "     Training Step: 163 Training Loss: 3.2577235386470575 \n",
      "     Training Step: 164 Training Loss: 4.633240904527361 \n",
      "     Training Step: 165 Training Loss: 4.561741699348464 \n",
      "     Training Step: 166 Training Loss: 3.2757146303296762 \n",
      "     Training Step: 167 Training Loss: 3.9987294978434846 \n",
      "     Training Step: 168 Training Loss: 3.9984804657318676 \n",
      "     Training Step: 169 Training Loss: 3.8135228969966413 \n",
      "     Training Step: 170 Training Loss: 4.40481571358998 \n",
      "     Training Step: 171 Training Loss: 4.258048543472081 \n",
      "     Training Step: 172 Training Loss: 4.6772480580059135 \n",
      "     Training Step: 173 Training Loss: 4.113853994173266 \n",
      "     Training Step: 174 Training Loss: 4.081786556669634 \n",
      "     Training Step: 175 Training Loss: 4.007659377228854 \n",
      "     Training Step: 176 Training Loss: 4.071648071280623 \n",
      "     Training Step: 177 Training Loss: 3.3865970724202525 \n",
      "     Training Step: 178 Training Loss: 4.368201678514278 \n",
      "     Training Step: 179 Training Loss: 4.2628841125156525 \n",
      "     Training Step: 180 Training Loss: 3.5337070229390655 \n",
      "     Training Step: 181 Training Loss: 4.273925836406546 \n",
      "     Training Step: 182 Training Loss: 4.814881012405707 \n",
      "     Training Step: 183 Training Loss: 2.9039177141594967 \n",
      "     Training Step: 184 Training Loss: 4.6779723413351 \n",
      "     Training Step: 185 Training Loss: 4.672133080208022 \n",
      "     Training Step: 186 Training Loss: 4.695068397332128 \n",
      "     Training Step: 187 Training Loss: 2.577033011154185 \n",
      "     Training Step: 188 Training Loss: 4.581382799078617 \n",
      "     Training Step: 189 Training Loss: 3.36443562697491 \n",
      "     Training Step: 190 Training Loss: 3.0466962498022276 \n",
      "     Training Step: 191 Training Loss: 3.185164358366025 \n",
      "     Training Step: 192 Training Loss: 2.6468805086320693 \n",
      "     Training Step: 193 Training Loss: 3.6981792338258566 \n",
      "     Training Step: 194 Training Loss: 3.103424172441252 \n",
      "     Training Step: 195 Training Loss: 3.737142371487534 \n",
      "     Training Step: 196 Training Loss: 3.2873374778539746 \n",
      "     Training Step: 197 Training Loss: 4.209441396397789 \n",
      "     Training Step: 198 Training Loss: 3.341456791270763 \n",
      "     Training Step: 199 Training Loss: 4.487977032511942 \n",
      "     Training Step: 200 Training Loss: 4.7987332323567 \n",
      "     Training Step: 201 Training Loss: 4.249715688974033 \n",
      "     Training Step: 202 Training Loss: 3.563399979840277 \n",
      "     Training Step: 203 Training Loss: 4.113700524470049 \n",
      "     Training Step: 204 Training Loss: 4.094820372330408 \n",
      "     Training Step: 205 Training Loss: 3.970092644032062 \n",
      "     Training Step: 206 Training Loss: 5.1692421437896945 \n",
      "     Training Step: 207 Training Loss: 3.094310057382765 \n",
      "     Training Step: 208 Training Loss: 3.811998158155406 \n",
      "     Training Step: 209 Training Loss: 3.3296536928326033 \n",
      "     Training Step: 210 Training Loss: 3.4117186185193655 \n",
      "     Training Step: 211 Training Loss: 2.6583188859796274 \n",
      "     Training Step: 212 Training Loss: 4.818333646993401 \n",
      "     Training Step: 213 Training Loss: 4.764188183680213 \n",
      "     Training Step: 214 Training Loss: 4.228105931423107 \n",
      "     Training Step: 215 Training Loss: 5.048515094621729 \n",
      "     Training Step: 216 Training Loss: 5.111794095490546 \n",
      "     Training Step: 217 Training Loss: 4.727457425348017 \n",
      "     Training Step: 218 Training Loss: 5.0753188753486835 \n",
      "     Training Step: 219 Training Loss: 4.860242698621834 \n",
      "     Training Step: 220 Training Loss: 4.116839772874408 \n",
      "     Training Step: 221 Training Loss: 3.7479719261407216 \n",
      "     Training Step: 222 Training Loss: 4.270225182669273 \n",
      "     Training Step: 223 Training Loss: 5.2784616086219565 \n",
      "     Training Step: 224 Training Loss: 3.6484225266867654 \n",
      "     Training Step: 225 Training Loss: 3.317063209442152 \n",
      "     Training Step: 226 Training Loss: 3.6885187791772505 \n",
      "     Training Step: 227 Training Loss: 3.6854436965289743 \n",
      "     Training Step: 228 Training Loss: 4.792900060985402 \n",
      "     Training Step: 229 Training Loss: 3.511010547612982 \n",
      "     Training Step: 230 Training Loss: 3.6240059011452703 \n",
      "     Training Step: 231 Training Loss: 3.3004438140111807 \n",
      "     Training Step: 232 Training Loss: 3.1401127537662794 \n",
      "     Training Step: 233 Training Loss: 3.470032692492043 \n",
      "     Training Step: 234 Training Loss: 4.636653321779426 \n",
      "     Training Step: 235 Training Loss: 3.7491026061959674 \n",
      "     Training Step: 236 Training Loss: 3.74057343159253 \n",
      "     Training Step: 237 Training Loss: 3.646435780509242 \n",
      "     Training Step: 238 Training Loss: 3.708357823019156 \n",
      "     Training Step: 239 Training Loss: 4.622272026804843 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.89258308700282 \n",
      "     Validation Step: 1 Validation Loss: 3.330443659898806 \n",
      "     Validation Step: 2 Validation Loss: 3.234006569726024 \n",
      "     Validation Step: 3 Validation Loss: 4.06783078516916 \n",
      "     Validation Step: 4 Validation Loss: 3.1368986677317614 \n",
      "     Validation Step: 5 Validation Loss: 3.6706460342701206 \n",
      "     Validation Step: 6 Validation Loss: 4.29102344512179 \n",
      "     Validation Step: 7 Validation Loss: 3.752610063164237 \n",
      "     Validation Step: 8 Validation Loss: 4.52003831442207 \n",
      "     Validation Step: 9 Validation Loss: 3.1858491327662213 \n",
      "     Validation Step: 10 Validation Loss: 3.4773929746786902 \n",
      "     Validation Step: 11 Validation Loss: 4.27097311358262 \n",
      "     Validation Step: 12 Validation Loss: 4.586451501782213 \n",
      "     Validation Step: 13 Validation Loss: 4.946583048940046 \n",
      "     Validation Step: 14 Validation Loss: 4.475818588858703 \n",
      "     Validation Step: 15 Validation Loss: 3.485153234762603 \n",
      "     Validation Step: 16 Validation Loss: 3.43808742457127 \n",
      "     Validation Step: 17 Validation Loss: 4.033194852157263 \n",
      "     Validation Step: 18 Validation Loss: 4.428460964911404 \n",
      "     Validation Step: 19 Validation Loss: 4.063515055407862 \n",
      "     Validation Step: 20 Validation Loss: 3.8103445913224916 \n",
      "     Validation Step: 21 Validation Loss: 3.492292988724043 \n",
      "     Validation Step: 22 Validation Loss: 4.749193388248956 \n",
      "     Validation Step: 23 Validation Loss: 3.361190937896504 \n",
      "     Validation Step: 24 Validation Loss: 4.604631195392026 \n",
      "     Validation Step: 25 Validation Loss: 4.421459682221715 \n",
      "     Validation Step: 26 Validation Loss: 4.701353862995797 \n",
      "     Validation Step: 27 Validation Loss: 3.5450040904075752 \n",
      "     Validation Step: 28 Validation Loss: 4.578161783862162 \n",
      "     Validation Step: 29 Validation Loss: 4.643386893097603 \n",
      "     Validation Step: 30 Validation Loss: 4.1653549942759165 \n",
      "     Validation Step: 31 Validation Loss: 4.728335236206102 \n",
      "     Validation Step: 32 Validation Loss: 4.110683654022376 \n",
      "     Validation Step: 33 Validation Loss: 3.4384559471926384 \n",
      "     Validation Step: 34 Validation Loss: 4.9197465159522045 \n",
      "     Validation Step: 35 Validation Loss: 4.176694261091995 \n",
      "     Validation Step: 36 Validation Loss: 3.628067655289371 \n",
      "     Validation Step: 37 Validation Loss: 5.059847467198323 \n",
      "     Validation Step: 38 Validation Loss: 4.552165645087379 \n",
      "     Validation Step: 39 Validation Loss: 4.594227425911974 \n",
      "     Validation Step: 40 Validation Loss: 3.7022092642996376 \n",
      "     Validation Step: 41 Validation Loss: 4.207620449841305 \n",
      "     Validation Step: 42 Validation Loss: 4.579407004233099 \n",
      "     Validation Step: 43 Validation Loss: 4.857808569544585 \n",
      "     Validation Step: 44 Validation Loss: 3.6682189259662956 \n",
      "Epoch: 31\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 5.031661020882924 \n",
      "     Training Step: 1 Training Loss: 3.5878971779736917 \n",
      "     Training Step: 2 Training Loss: 4.3901524457758265 \n",
      "     Training Step: 3 Training Loss: 4.22127752553294 \n",
      "     Training Step: 4 Training Loss: 3.09503100297726 \n",
      "     Training Step: 5 Training Loss: 4.407180152358088 \n",
      "     Training Step: 6 Training Loss: 3.6435529074645996 \n",
      "     Training Step: 7 Training Loss: 2.9445118963902375 \n",
      "     Training Step: 8 Training Loss: 3.951449885225463 \n",
      "     Training Step: 9 Training Loss: 4.077113443584721 \n",
      "     Training Step: 10 Training Loss: 4.136652853082051 \n",
      "     Training Step: 11 Training Loss: 4.52020153434138 \n",
      "     Training Step: 12 Training Loss: 4.107659619152374 \n",
      "     Training Step: 13 Training Loss: 3.347576985774826 \n",
      "     Training Step: 14 Training Loss: 4.42106699372249 \n",
      "     Training Step: 15 Training Loss: 4.492108994409011 \n",
      "     Training Step: 16 Training Loss: 4.863064086211847 \n",
      "     Training Step: 17 Training Loss: 3.974947609740123 \n",
      "     Training Step: 18 Training Loss: 4.794159446921215 \n",
      "     Training Step: 19 Training Loss: 3.994580173799275 \n",
      "     Training Step: 20 Training Loss: 3.6430290043615554 \n",
      "     Training Step: 21 Training Loss: 3.3753206249019727 \n",
      "     Training Step: 22 Training Loss: 4.424862292682088 \n",
      "     Training Step: 23 Training Loss: 3.9129616997479157 \n",
      "     Training Step: 24 Training Loss: 4.085678626366365 \n",
      "     Training Step: 25 Training Loss: 4.5032757788478195 \n",
      "     Training Step: 26 Training Loss: 4.925992389511007 \n",
      "     Training Step: 27 Training Loss: 3.8453483209619064 \n",
      "     Training Step: 28 Training Loss: 4.09258863264103 \n",
      "     Training Step: 29 Training Loss: 4.712976094280386 \n",
      "     Training Step: 30 Training Loss: 4.212015932556152 \n",
      "     Training Step: 31 Training Loss: 5.061471382146386 \n",
      "     Training Step: 32 Training Loss: 3.6067406432369937 \n",
      "     Training Step: 33 Training Loss: 3.48333863989848 \n",
      "     Training Step: 34 Training Loss: 4.496722510970005 \n",
      "     Training Step: 35 Training Loss: 4.422717549746586 \n",
      "     Training Step: 36 Training Loss: 4.164194349130495 \n",
      "     Training Step: 37 Training Loss: 3.783601437326638 \n",
      "     Training Step: 38 Training Loss: 3.749428256761133 \n",
      "     Training Step: 39 Training Loss: 3.0057813366945454 \n",
      "     Training Step: 40 Training Loss: 3.1795002636452474 \n",
      "     Training Step: 41 Training Loss: 3.3273053747517713 \n",
      "     Training Step: 42 Training Loss: 2.701172364169163 \n",
      "     Training Step: 43 Training Loss: 2.9317420702395136 \n",
      "     Training Step: 44 Training Loss: 4.878325693509342 \n",
      "     Training Step: 45 Training Loss: 2.8374548047502945 \n",
      "     Training Step: 46 Training Loss: 3.4839033869325657 \n",
      "     Training Step: 47 Training Loss: 4.549191747910787 \n",
      "     Training Step: 48 Training Loss: 4.4960048486498545 \n",
      "     Training Step: 49 Training Loss: 3.7369796003737323 \n",
      "     Training Step: 50 Training Loss: 3.3687487911631604 \n",
      "     Training Step: 51 Training Loss: 3.615580883037344 \n",
      "     Training Step: 52 Training Loss: 4.987966827557071 \n",
      "     Training Step: 53 Training Loss: 3.7280719788763217 \n",
      "     Training Step: 54 Training Loss: 3.4970217314581187 \n",
      "     Training Step: 55 Training Loss: 2.1936309664094 \n",
      "     Training Step: 56 Training Loss: 3.7313152465842196 \n",
      "     Training Step: 57 Training Loss: 4.524489322188339 \n",
      "     Training Step: 58 Training Loss: 4.959191127098684 \n",
      "     Training Step: 59 Training Loss: 3.818366207678377 \n",
      "     Training Step: 60 Training Loss: 3.9294715613566424 \n",
      "     Training Step: 61 Training Loss: 3.7361237001009546 \n",
      "     Training Step: 62 Training Loss: 4.5851698048241865 \n",
      "     Training Step: 63 Training Loss: 4.724958790883844 \n",
      "     Training Step: 64 Training Loss: 3.812475303504217 \n",
      "     Training Step: 65 Training Loss: 4.520235972680064 \n",
      "     Training Step: 66 Training Loss: 4.2478973502153945 \n",
      "     Training Step: 67 Training Loss: 4.781374267621528 \n",
      "     Training Step: 68 Training Loss: 4.174463607550079 \n",
      "     Training Step: 69 Training Loss: 4.084372797128925 \n",
      "     Training Step: 70 Training Loss: 3.439457277968507 \n",
      "     Training Step: 71 Training Loss: 5.065843067117511 \n",
      "     Training Step: 72 Training Loss: 4.381367335971865 \n",
      "     Training Step: 73 Training Loss: 4.544608468562387 \n",
      "     Training Step: 74 Training Loss: 3.5322596048216237 \n",
      "     Training Step: 75 Training Loss: 3.057035613239963 \n",
      "     Training Step: 76 Training Loss: 3.534262014141629 \n",
      "     Training Step: 77 Training Loss: 4.2868922463712815 \n",
      "     Training Step: 78 Training Loss: 3.9358776529095443 \n",
      "     Training Step: 79 Training Loss: 5.074508067322274 \n",
      "     Training Step: 80 Training Loss: 4.568249681638343 \n",
      "     Training Step: 81 Training Loss: 3.4589745659179574 \n",
      "     Training Step: 82 Training Loss: 3.4955308071378908 \n",
      "     Training Step: 83 Training Loss: 3.334153012344337 \n",
      "     Training Step: 84 Training Loss: 4.106291824091247 \n",
      "     Training Step: 85 Training Loss: 4.025564329033726 \n",
      "     Training Step: 86 Training Loss: 4.530341250331893 \n",
      "     Training Step: 87 Training Loss: 4.609311494191393 \n",
      "     Training Step: 88 Training Loss: 4.630202848957846 \n",
      "     Training Step: 89 Training Loss: 3.884091041616225 \n",
      "     Training Step: 90 Training Loss: 3.451947527899459 \n",
      "     Training Step: 91 Training Loss: 4.203812612308508 \n",
      "     Training Step: 92 Training Loss: 4.1830983035640585 \n",
      "     Training Step: 93 Training Loss: 4.2032576321622175 \n",
      "     Training Step: 94 Training Loss: 4.074833838360384 \n",
      "     Training Step: 95 Training Loss: 3.6144229917164963 \n",
      "     Training Step: 96 Training Loss: 3.221517032616142 \n",
      "     Training Step: 97 Training Loss: 4.352241395384841 \n",
      "     Training Step: 98 Training Loss: 4.443402806300434 \n",
      "     Training Step: 99 Training Loss: 4.323335316613853 \n",
      "     Training Step: 100 Training Loss: 3.472547779886879 \n",
      "     Training Step: 101 Training Loss: 4.519278595144827 \n",
      "     Training Step: 102 Training Loss: 3.4638139880461236 \n",
      "     Training Step: 103 Training Loss: 4.475390845749876 \n",
      "     Training Step: 104 Training Loss: 4.193039879800365 \n",
      "     Training Step: 105 Training Loss: 3.856322271776415 \n",
      "     Training Step: 106 Training Loss: 3.149152006883801 \n",
      "     Training Step: 107 Training Loss: 2.711072387838311 \n",
      "     Training Step: 108 Training Loss: 4.691263987717381 \n",
      "     Training Step: 109 Training Loss: 3.839342385984199 \n",
      "     Training Step: 110 Training Loss: 4.648557291973917 \n",
      "     Training Step: 111 Training Loss: 2.8291184717668325 \n",
      "     Training Step: 112 Training Loss: 3.470369727827582 \n",
      "     Training Step: 113 Training Loss: 4.033846476043386 \n",
      "     Training Step: 114 Training Loss: 4.532464670469999 \n",
      "     Training Step: 115 Training Loss: 3.9062991197641574 \n",
      "     Training Step: 116 Training Loss: 4.507774750091133 \n",
      "     Training Step: 117 Training Loss: 3.9301329529198323 \n",
      "     Training Step: 118 Training Loss: 4.612354695262618 \n",
      "     Training Step: 119 Training Loss: 4.753272737704467 \n",
      "     Training Step: 120 Training Loss: 4.763369869979503 \n",
      "     Training Step: 121 Training Loss: 3.6074442954884294 \n",
      "     Training Step: 122 Training Loss: 3.817689290394709 \n",
      "     Training Step: 123 Training Loss: 4.596932753016099 \n",
      "     Training Step: 124 Training Loss: 3.241332973464515 \n",
      "     Training Step: 125 Training Loss: 4.751073678283995 \n",
      "     Training Step: 126 Training Loss: 2.6045799377843535 \n",
      "     Training Step: 127 Training Loss: 4.57308267914987 \n",
      "     Training Step: 128 Training Loss: 2.982853924868262 \n",
      "     Training Step: 129 Training Loss: 3.334446320854711 \n",
      "     Training Step: 130 Training Loss: 3.1569543935078785 \n",
      "     Training Step: 131 Training Loss: 3.392949008372409 \n",
      "     Training Step: 132 Training Loss: 4.27415936359306 \n",
      "     Training Step: 133 Training Loss: 4.490503221634894 \n",
      "     Training Step: 134 Training Loss: 4.169107814978623 \n",
      "     Training Step: 135 Training Loss: 3.6554335496302395 \n",
      "     Training Step: 136 Training Loss: 4.214315355438504 \n",
      "     Training Step: 137 Training Loss: 4.962904081647635 \n",
      "     Training Step: 138 Training Loss: 4.638935727642764 \n",
      "     Training Step: 139 Training Loss: 4.667901082560304 \n",
      "     Training Step: 140 Training Loss: 4.6136196610188565 \n",
      "     Training Step: 141 Training Loss: 4.0887016024152505 \n",
      "     Training Step: 142 Training Loss: 4.405744566960025 \n",
      "     Training Step: 143 Training Loss: 5.149820996938866 \n",
      "     Training Step: 144 Training Loss: 3.3238386610191903 \n",
      "     Training Step: 145 Training Loss: 4.647191361829784 \n",
      "     Training Step: 146 Training Loss: 4.038661446065128 \n",
      "     Training Step: 147 Training Loss: 3.6054900141025295 \n",
      "     Training Step: 148 Training Loss: 4.001743429017338 \n",
      "     Training Step: 149 Training Loss: 4.508598946527447 \n",
      "     Training Step: 150 Training Loss: 3.8963168965858404 \n",
      "     Training Step: 151 Training Loss: 3.7167679087640106 \n",
      "     Training Step: 152 Training Loss: 5.043600033760845 \n",
      "     Training Step: 153 Training Loss: 4.604487328234031 \n",
      "     Training Step: 154 Training Loss: 4.831620736352136 \n",
      "     Training Step: 155 Training Loss: 5.003008065379433 \n",
      "     Training Step: 156 Training Loss: 3.873788300374704 \n",
      "     Training Step: 157 Training Loss: 4.35537163300208 \n",
      "     Training Step: 158 Training Loss: 4.97390887582842 \n",
      "     Training Step: 159 Training Loss: 3.4000758170657357 \n",
      "     Training Step: 160 Training Loss: 3.3526535927882217 \n",
      "     Training Step: 161 Training Loss: 4.2034902387935436 \n",
      "     Training Step: 162 Training Loss: 4.336238834955117 \n",
      "     Training Step: 163 Training Loss: 3.4183631475342255 \n",
      "     Training Step: 164 Training Loss: 2.847666568732435 \n",
      "     Training Step: 165 Training Loss: 4.278644272236972 \n",
      "     Training Step: 166 Training Loss: 2.6795076260881148 \n",
      "     Training Step: 167 Training Loss: 3.000661852428002 \n",
      "     Training Step: 168 Training Loss: 4.865049924932714 \n",
      "     Training Step: 169 Training Loss: 3.1698071150652902 \n",
      "     Training Step: 170 Training Loss: 4.809940968283546 \n",
      "     Training Step: 171 Training Loss: 4.118160081853592 \n",
      "     Training Step: 172 Training Loss: 4.7675475979568445 \n",
      "     Training Step: 173 Training Loss: 3.8261145855509366 \n",
      "     Training Step: 174 Training Loss: 3.740724783464274 \n",
      "     Training Step: 175 Training Loss: 4.7914704718223655 \n",
      "     Training Step: 176 Training Loss: 4.756736507027738 \n",
      "     Training Step: 177 Training Loss: 4.998364216441107 \n",
      "     Training Step: 178 Training Loss: 4.282638154371609 \n",
      "     Training Step: 179 Training Loss: 3.072129859339043 \n",
      "     Training Step: 180 Training Loss: 3.291760239647506 \n",
      "     Training Step: 181 Training Loss: 3.5767936835526304 \n",
      "     Training Step: 182 Training Loss: 3.922648112913185 \n",
      "     Training Step: 183 Training Loss: 2.6793994946809545 \n",
      "     Training Step: 184 Training Loss: 4.857092647428106 \n",
      "     Training Step: 185 Training Loss: 4.150373332231077 \n",
      "     Training Step: 186 Training Loss: 4.882896533526562 \n",
      "     Training Step: 187 Training Loss: 4.627517280703427 \n",
      "     Training Step: 188 Training Loss: 4.767718062404688 \n",
      "     Training Step: 189 Training Loss: 4.661166814168714 \n",
      "     Training Step: 190 Training Loss: 3.810595482384013 \n",
      "     Training Step: 191 Training Loss: 3.2612198190008868 \n",
      "     Training Step: 192 Training Loss: 4.174282830276447 \n",
      "     Training Step: 193 Training Loss: 3.8366631838683802 \n",
      "     Training Step: 194 Training Loss: 4.0354673183468455 \n",
      "     Training Step: 195 Training Loss: 2.743285297440514 \n",
      "     Training Step: 196 Training Loss: 4.771204826191645 \n",
      "     Training Step: 197 Training Loss: 4.8050153986376705 \n",
      "     Training Step: 198 Training Loss: 3.340876702310948 \n",
      "     Training Step: 199 Training Loss: 4.785875406557939 \n",
      "     Training Step: 200 Training Loss: 4.258567668344047 \n",
      "     Training Step: 201 Training Loss: 3.9039776599120413 \n",
      "     Training Step: 202 Training Loss: 3.935918552640967 \n",
      "     Training Step: 203 Training Loss: 4.120467882157392 \n",
      "     Training Step: 204 Training Loss: 3.068400458007322 \n",
      "     Training Step: 205 Training Loss: 3.845715482725634 \n",
      "     Training Step: 206 Training Loss: 4.893030183346052 \n",
      "     Training Step: 207 Training Loss: 2.750588263061247 \n",
      "     Training Step: 208 Training Loss: 2.7912050686490564 \n",
      "     Training Step: 209 Training Loss: 4.728655032269284 \n",
      "     Training Step: 210 Training Loss: 4.11235612280226 \n",
      "     Training Step: 211 Training Loss: 3.4729120216272413 \n",
      "     Training Step: 212 Training Loss: 4.4833558407944505 \n",
      "     Training Step: 213 Training Loss: 3.648838446536197 \n",
      "     Training Step: 214 Training Loss: 3.7242488025664384 \n",
      "     Training Step: 215 Training Loss: 4.543053511392817 \n",
      "     Training Step: 216 Training Loss: 4.680188287515427 \n",
      "     Training Step: 217 Training Loss: 3.696810270000326 \n",
      "     Training Step: 218 Training Loss: 4.078797654208644 \n",
      "     Training Step: 219 Training Loss: 4.144964039671975 \n",
      "     Training Step: 220 Training Loss: 4.2865595963721415 \n",
      "     Training Step: 221 Training Loss: 3.2205468584821846 \n",
      "     Training Step: 222 Training Loss: 4.283785037138792 \n",
      "     Training Step: 223 Training Loss: 4.138567810327141 \n",
      "     Training Step: 224 Training Loss: 3.720280055123683 \n",
      "     Training Step: 225 Training Loss: 2.971881819922797 \n",
      "     Training Step: 226 Training Loss: 2.5063365703849083 \n",
      "     Training Step: 227 Training Loss: 4.2086745740749185 \n",
      "     Training Step: 228 Training Loss: 3.3454290839189045 \n",
      "     Training Step: 229 Training Loss: 4.522345486263555 \n",
      "     Training Step: 230 Training Loss: 3.9034881907077126 \n",
      "     Training Step: 231 Training Loss: 4.6949371726808495 \n",
      "     Training Step: 232 Training Loss: 4.499747032218565 \n",
      "     Training Step: 233 Training Loss: 3.7297457708054176 \n",
      "     Training Step: 234 Training Loss: 4.911585440442789 \n",
      "     Training Step: 235 Training Loss: 3.6878387312144545 \n",
      "     Training Step: 236 Training Loss: 4.341727849878195 \n",
      "     Training Step: 237 Training Loss: 3.3258320431503163 \n",
      "     Training Step: 238 Training Loss: 4.338286341719571 \n",
      "     Training Step: 239 Training Loss: 3.5491718240732504 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 3.575081233942935 \n",
      "     Validation Step: 1 Validation Loss: 4.494009002579392 \n",
      "     Validation Step: 2 Validation Loss: 3.4000961685291538 \n",
      "     Validation Step: 3 Validation Loss: 3.993860667202254 \n",
      "     Validation Step: 4 Validation Loss: 5.0066039964769695 \n",
      "     Validation Step: 5 Validation Loss: 4.967297529121062 \n",
      "     Validation Step: 6 Validation Loss: 4.691221208207755 \n",
      "     Validation Step: 7 Validation Loss: 4.1277308003419595 \n",
      "     Validation Step: 8 Validation Loss: 3.418147454711856 \n",
      "     Validation Step: 9 Validation Loss: 4.5122324898777615 \n",
      "     Validation Step: 10 Validation Loss: 4.358428624966729 \n",
      "     Validation Step: 11 Validation Loss: 4.744569360406679 \n",
      "     Validation Step: 12 Validation Loss: 4.249665431901271 \n",
      "     Validation Step: 13 Validation Loss: 3.6531275541527526 \n",
      "     Validation Step: 14 Validation Loss: 4.9743158540991965 \n",
      "     Validation Step: 15 Validation Loss: 3.3744664216265488 \n",
      "     Validation Step: 16 Validation Loss: 4.073097247816658 \n",
      "     Validation Step: 17 Validation Loss: 5.04028419701232 \n",
      "     Validation Step: 18 Validation Loss: 4.340202336733976 \n",
      "     Validation Step: 19 Validation Loss: 3.8287088360095436 \n",
      "     Validation Step: 20 Validation Loss: 2.8290139554079814 \n",
      "     Validation Step: 21 Validation Loss: 3.3322846216756883 \n",
      "     Validation Step: 22 Validation Loss: 3.3692949136237464 \n",
      "     Validation Step: 23 Validation Loss: 3.4739433757939584 \n",
      "     Validation Step: 24 Validation Loss: 4.440279000996674 \n",
      "     Validation Step: 25 Validation Loss: 3.781548103682017 \n",
      "     Validation Step: 26 Validation Loss: 4.152638594535021 \n",
      "     Validation Step: 27 Validation Loss: 4.651522134452564 \n",
      "     Validation Step: 28 Validation Loss: 4.676222237209953 \n",
      "     Validation Step: 29 Validation Loss: 3.145205161322137 \n",
      "     Validation Step: 30 Validation Loss: 3.0378335600277913 \n",
      "     Validation Step: 31 Validation Loss: 3.750244979207273 \n",
      "     Validation Step: 32 Validation Loss: 4.793626061179359 \n",
      "     Validation Step: 33 Validation Loss: 3.912277226744535 \n",
      "     Validation Step: 34 Validation Loss: 3.6150207341004044 \n",
      "     Validation Step: 35 Validation Loss: 3.3675569314902365 \n",
      "     Validation Step: 36 Validation Loss: 4.8263137015530155 \n",
      "     Validation Step: 37 Validation Loss: 4.213245549581193 \n",
      "     Validation Step: 38 Validation Loss: 4.739244093343112 \n",
      "     Validation Step: 39 Validation Loss: 4.738486149558616 \n",
      "     Validation Step: 40 Validation Loss: 4.624155952700023 \n",
      "     Validation Step: 41 Validation Loss: 3.3233912243686325 \n",
      "     Validation Step: 42 Validation Loss: 4.694809578387025 \n",
      "     Validation Step: 43 Validation Loss: 3.9525591164835205 \n",
      "     Validation Step: 44 Validation Loss: 4.3598387723595 \n",
      "Epoch: 32\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.398669575735047 \n",
      "     Training Step: 1 Training Loss: 3.442788231500457 \n",
      "     Training Step: 2 Training Loss: 3.931461412129401 \n",
      "     Training Step: 3 Training Loss: 4.683655225178797 \n",
      "     Training Step: 4 Training Loss: 4.256467745686723 \n",
      "     Training Step: 5 Training Loss: 3.4427071932135296 \n",
      "     Training Step: 6 Training Loss: 4.438504938709189 \n",
      "     Training Step: 7 Training Loss: 4.21494152460444 \n",
      "     Training Step: 8 Training Loss: 4.76144201165211 \n",
      "     Training Step: 9 Training Loss: 4.119415764765843 \n",
      "     Training Step: 10 Training Loss: 4.675553392935766 \n",
      "     Training Step: 11 Training Loss: 3.752895429488371 \n",
      "     Training Step: 12 Training Loss: 3.81403086972113 \n",
      "     Training Step: 13 Training Loss: 4.040013821658479 \n",
      "     Training Step: 14 Training Loss: 4.245553242172185 \n",
      "     Training Step: 15 Training Loss: 4.105435896185515 \n",
      "     Training Step: 16 Training Loss: 3.663122919212074 \n",
      "     Training Step: 17 Training Loss: 4.388271413134001 \n",
      "     Training Step: 18 Training Loss: 3.761248959921833 \n",
      "     Training Step: 19 Training Loss: 4.420314540079489 \n",
      "     Training Step: 20 Training Loss: 4.055214381124934 \n",
      "     Training Step: 21 Training Loss: 2.9861143555822416 \n",
      "     Training Step: 22 Training Loss: 2.8538291985043496 \n",
      "     Training Step: 23 Training Loss: 4.30918660666021 \n",
      "     Training Step: 24 Training Loss: 4.050393332968087 \n",
      "     Training Step: 25 Training Loss: 4.101870088869256 \n",
      "     Training Step: 26 Training Loss: 3.234619973661887 \n",
      "     Training Step: 27 Training Loss: 3.241453291139843 \n",
      "     Training Step: 28 Training Loss: 5.012604464221603 \n",
      "     Training Step: 29 Training Loss: 4.006630800297336 \n",
      "     Training Step: 30 Training Loss: 4.653405360645787 \n",
      "     Training Step: 31 Training Loss: 4.17564039352232 \n",
      "     Training Step: 32 Training Loss: 4.354993671135603 \n",
      "     Training Step: 33 Training Loss: 2.398641773868391 \n",
      "     Training Step: 34 Training Loss: 3.8644933387118123 \n",
      "     Training Step: 35 Training Loss: 4.696523238581873 \n",
      "     Training Step: 36 Training Loss: 3.968008004268858 \n",
      "     Training Step: 37 Training Loss: 3.7924355095545836 \n",
      "     Training Step: 38 Training Loss: 3.960671202129887 \n",
      "     Training Step: 39 Training Loss: 3.917221055223534 \n",
      "     Training Step: 40 Training Loss: 4.086850974207003 \n",
      "     Training Step: 41 Training Loss: 4.839646512720906 \n",
      "     Training Step: 42 Training Loss: 4.1491135974808895 \n",
      "     Training Step: 43 Training Loss: 4.116011196794948 \n",
      "     Training Step: 44 Training Loss: 3.955147752079447 \n",
      "     Training Step: 45 Training Loss: 3.6364972194433136 \n",
      "     Training Step: 46 Training Loss: 4.868794946319731 \n",
      "     Training Step: 47 Training Loss: 3.5602725061416765 \n",
      "     Training Step: 48 Training Loss: 2.946345931237215 \n",
      "     Training Step: 49 Training Loss: 4.640379448130209 \n",
      "     Training Step: 50 Training Loss: 4.537724558368322 \n",
      "     Training Step: 51 Training Loss: 3.8498115146117504 \n",
      "     Training Step: 52 Training Loss: 4.435582766388239 \n",
      "     Training Step: 53 Training Loss: 4.517328052197055 \n",
      "     Training Step: 54 Training Loss: 4.263319755758534 \n",
      "     Training Step: 55 Training Loss: 3.402838068372915 \n",
      "     Training Step: 56 Training Loss: 4.668764456148245 \n",
      "     Training Step: 57 Training Loss: 2.8487835991573784 \n",
      "     Training Step: 58 Training Loss: 3.1731794349795535 \n",
      "     Training Step: 59 Training Loss: 3.921778172441272 \n",
      "     Training Step: 60 Training Loss: 4.342894577792034 \n",
      "     Training Step: 61 Training Loss: 4.258550628873124 \n",
      "     Training Step: 62 Training Loss: 4.413373292708928 \n",
      "     Training Step: 63 Training Loss: 3.7638292741459503 \n",
      "     Training Step: 64 Training Loss: 4.381251104864186 \n",
      "     Training Step: 65 Training Loss: 4.13171762274217 \n",
      "     Training Step: 66 Training Loss: 3.912439427497141 \n",
      "     Training Step: 67 Training Loss: 4.492269928569915 \n",
      "     Training Step: 68 Training Loss: 4.262780196863794 \n",
      "     Training Step: 69 Training Loss: 5.138414663442994 \n",
      "     Training Step: 70 Training Loss: 4.128126370525138 \n",
      "     Training Step: 71 Training Loss: 4.556498104202321 \n",
      "     Training Step: 72 Training Loss: 3.6030188656086066 \n",
      "     Training Step: 73 Training Loss: 4.188780122849801 \n",
      "     Training Step: 74 Training Loss: 3.7635875112982538 \n",
      "     Training Step: 75 Training Loss: 3.4666834767720673 \n",
      "     Training Step: 76 Training Loss: 4.6949002164594535 \n",
      "     Training Step: 77 Training Loss: 4.095626927672858 \n",
      "     Training Step: 78 Training Loss: 3.414472103386515 \n",
      "     Training Step: 79 Training Loss: 2.4919634217976014 \n",
      "     Training Step: 80 Training Loss: 2.882411066765057 \n",
      "     Training Step: 81 Training Loss: 3.5122713758900055 \n",
      "     Training Step: 82 Training Loss: 4.267599854478044 \n",
      "     Training Step: 83 Training Loss: 4.3272023236210995 \n",
      "     Training Step: 84 Training Loss: 3.306613318751301 \n",
      "     Training Step: 85 Training Loss: 5.210939851415187 \n",
      "     Training Step: 86 Training Loss: 4.882072295555564 \n",
      "     Training Step: 87 Training Loss: 4.240182181149378 \n",
      "     Training Step: 88 Training Loss: 3.5495519853145305 \n",
      "     Training Step: 89 Training Loss: 4.0942652030684545 \n",
      "     Training Step: 90 Training Loss: 4.196760269802899 \n",
      "     Training Step: 91 Training Loss: 3.9553660477963244 \n",
      "     Training Step: 92 Training Loss: 3.4322014847738758 \n",
      "     Training Step: 93 Training Loss: 4.371625256606466 \n",
      "     Training Step: 94 Training Loss: 4.80619195290766 \n",
      "     Training Step: 95 Training Loss: 5.15436497481594 \n",
      "     Training Step: 96 Training Loss: 4.698903225410818 \n",
      "     Training Step: 97 Training Loss: 3.9797006226389042 \n",
      "     Training Step: 98 Training Loss: 3.7126849345637325 \n",
      "     Training Step: 99 Training Loss: 4.28120529753881 \n",
      "     Training Step: 100 Training Loss: 4.937469770670824 \n",
      "     Training Step: 101 Training Loss: 3.0804008208670153 \n",
      "     Training Step: 102 Training Loss: 4.397288587472502 \n",
      "     Training Step: 103 Training Loss: 4.291749039838993 \n",
      "     Training Step: 104 Training Loss: 4.462956699437128 \n",
      "     Training Step: 105 Training Loss: 4.283627364429175 \n",
      "     Training Step: 106 Training Loss: 3.519793142925964 \n",
      "     Training Step: 107 Training Loss: 4.038588710468875 \n",
      "     Training Step: 108 Training Loss: 4.280341962106476 \n",
      "     Training Step: 109 Training Loss: 3.4540580239255436 \n",
      "     Training Step: 110 Training Loss: 3.6906275208775243 \n",
      "     Training Step: 111 Training Loss: 3.505565005160263 \n",
      "     Training Step: 112 Training Loss: 4.39637044269388 \n",
      "     Training Step: 113 Training Loss: 3.285118655131282 \n",
      "     Training Step: 114 Training Loss: 4.614449615326952 \n",
      "     Training Step: 115 Training Loss: 4.034939515387214 \n",
      "     Training Step: 116 Training Loss: 2.6848102685623356 \n",
      "     Training Step: 117 Training Loss: 3.965004976719283 \n",
      "     Training Step: 118 Training Loss: 3.6088016193471084 \n",
      "     Training Step: 119 Training Loss: 2.6545524306514663 \n",
      "     Training Step: 120 Training Loss: 3.594422454494628 \n",
      "     Training Step: 121 Training Loss: 2.8127580431755894 \n",
      "     Training Step: 122 Training Loss: 3.4651590328923048 \n",
      "     Training Step: 123 Training Loss: 4.90101771675169 \n",
      "     Training Step: 124 Training Loss: 4.241978430223048 \n",
      "     Training Step: 125 Training Loss: 2.7362568912694 \n",
      "     Training Step: 126 Training Loss: 3.9478689244547427 \n",
      "     Training Step: 127 Training Loss: 3.2864444068073198 \n",
      "     Training Step: 128 Training Loss: 4.597298943099774 \n",
      "     Training Step: 129 Training Loss: 3.1872518169776622 \n",
      "     Training Step: 130 Training Loss: 3.579447846996291 \n",
      "     Training Step: 131 Training Loss: 4.381478841391802 \n",
      "     Training Step: 132 Training Loss: 3.4747289427750436 \n",
      "     Training Step: 133 Training Loss: 3.7166279471636754 \n",
      "     Training Step: 134 Training Loss: 3.9542536796565138 \n",
      "     Training Step: 135 Training Loss: 4.886792266756097 \n",
      "     Training Step: 136 Training Loss: 4.5468098796904135 \n",
      "     Training Step: 137 Training Loss: 4.1117244148778305 \n",
      "     Training Step: 138 Training Loss: 3.5917483788529 \n",
      "     Training Step: 139 Training Loss: 2.6972766196595264 \n",
      "     Training Step: 140 Training Loss: 3.6456420013521194 \n",
      "     Training Step: 141 Training Loss: 4.7885455814108 \n",
      "     Training Step: 142 Training Loss: 4.711982940460547 \n",
      "     Training Step: 143 Training Loss: 4.487708667029124 \n",
      "     Training Step: 144 Training Loss: 4.996278915684305 \n",
      "     Training Step: 145 Training Loss: 3.5303950838868268 \n",
      "     Training Step: 146 Training Loss: 4.0540617814789535 \n",
      "     Training Step: 147 Training Loss: 4.648179951130509 \n",
      "     Training Step: 148 Training Loss: 3.5256260117500595 \n",
      "     Training Step: 149 Training Loss: 4.426048143502355 \n",
      "     Training Step: 150 Training Loss: 4.043579607692652 \n",
      "     Training Step: 151 Training Loss: 4.74238186978498 \n",
      "     Training Step: 152 Training Loss: 4.130643757171685 \n",
      "     Training Step: 153 Training Loss: 3.7179536647840363 \n",
      "     Training Step: 154 Training Loss: 4.070302723585543 \n",
      "     Training Step: 155 Training Loss: 3.9191539960617 \n",
      "     Training Step: 156 Training Loss: 4.660821877324903 \n",
      "     Training Step: 157 Training Loss: 4.772462178453869 \n",
      "     Training Step: 158 Training Loss: 4.142479225701763 \n",
      "     Training Step: 159 Training Loss: 3.4909589772321277 \n",
      "     Training Step: 160 Training Loss: 3.439565520640278 \n",
      "     Training Step: 161 Training Loss: 3.609166636088834 \n",
      "     Training Step: 162 Training Loss: 3.639811792576495 \n",
      "     Training Step: 163 Training Loss: 3.3762773841390956 \n",
      "     Training Step: 164 Training Loss: 3.260641805260029 \n",
      "     Training Step: 165 Training Loss: 4.078590204963454 \n",
      "     Training Step: 166 Training Loss: 3.7647655090478778 \n",
      "     Training Step: 167 Training Loss: 2.714831672847661 \n",
      "     Training Step: 168 Training Loss: 2.5279566516353245 \n",
      "     Training Step: 169 Training Loss: 3.8278588798992033 \n",
      "     Training Step: 170 Training Loss: 3.4697514006674224 \n",
      "     Training Step: 171 Training Loss: 4.645327202578352 \n",
      "     Training Step: 172 Training Loss: 4.487466191887611 \n",
      "     Training Step: 173 Training Loss: 4.839495812375884 \n",
      "     Training Step: 174 Training Loss: 4.65171882494785 \n",
      "     Training Step: 175 Training Loss: 4.1978291912708645 \n",
      "     Training Step: 176 Training Loss: 4.7821122462954095 \n",
      "     Training Step: 177 Training Loss: 3.9403798637561898 \n",
      "     Training Step: 178 Training Loss: 4.062367946005881 \n",
      "     Training Step: 179 Training Loss: 4.326876719641878 \n",
      "     Training Step: 180 Training Loss: 4.794243318674896 \n",
      "     Training Step: 181 Training Loss: 4.020433812439883 \n",
      "     Training Step: 182 Training Loss: 4.140365733934798 \n",
      "     Training Step: 183 Training Loss: 4.433555581960941 \n",
      "     Training Step: 184 Training Loss: 4.461587059551133 \n",
      "     Training Step: 185 Training Loss: 4.759445557778277 \n",
      "     Training Step: 186 Training Loss: 4.3339355792487035 \n",
      "     Training Step: 187 Training Loss: 4.445392491860008 \n",
      "     Training Step: 188 Training Loss: 3.3996133720964967 \n",
      "     Training Step: 189 Training Loss: 3.440905193988563 \n",
      "     Training Step: 190 Training Loss: 4.883625240079705 \n",
      "     Training Step: 191 Training Loss: 4.777272063014138 \n",
      "     Training Step: 192 Training Loss: 4.108874036588731 \n",
      "     Training Step: 193 Training Loss: 3.607869906396005 \n",
      "     Training Step: 194 Training Loss: 3.846086512486349 \n",
      "     Training Step: 195 Training Loss: 4.149608074773101 \n",
      "     Training Step: 196 Training Loss: 5.027541734613612 \n",
      "     Training Step: 197 Training Loss: 4.376589093306905 \n",
      "     Training Step: 198 Training Loss: 3.380798826952302 \n",
      "     Training Step: 199 Training Loss: 4.710991703548214 \n",
      "     Training Step: 200 Training Loss: 4.236582637615746 \n",
      "     Training Step: 201 Training Loss: 4.163979821443282 \n",
      "     Training Step: 202 Training Loss: 4.651046170658047 \n",
      "     Training Step: 203 Training Loss: 4.206905856599396 \n",
      "     Training Step: 204 Training Loss: 3.1353415836675427 \n",
      "     Training Step: 205 Training Loss: 3.4534732517610363 \n",
      "     Training Step: 206 Training Loss: 3.9720383732834206 \n",
      "     Training Step: 207 Training Loss: 4.051856083062173 \n",
      "     Training Step: 208 Training Loss: 4.666127213970655 \n",
      "     Training Step: 209 Training Loss: 4.039730688642136 \n",
      "     Training Step: 210 Training Loss: 3.8618536074946803 \n",
      "     Training Step: 211 Training Loss: 3.601140801881924 \n",
      "     Training Step: 212 Training Loss: 3.0095017087577323 \n",
      "     Training Step: 213 Training Loss: 4.030753523593187 \n",
      "     Training Step: 214 Training Loss: 2.3689337910939585 \n",
      "     Training Step: 215 Training Loss: 4.9625190420063205 \n",
      "     Training Step: 216 Training Loss: 2.6926966919879067 \n",
      "     Training Step: 217 Training Loss: 3.020214704911249 \n",
      "     Training Step: 218 Training Loss: 4.611621512815773 \n",
      "     Training Step: 219 Training Loss: 4.2776505650425065 \n",
      "     Training Step: 220 Training Loss: 4.270718675968698 \n",
      "     Training Step: 221 Training Loss: 4.366197091596167 \n",
      "     Training Step: 222 Training Loss: 4.0414502866852455 \n",
      "     Training Step: 223 Training Loss: 4.957352822269391 \n",
      "     Training Step: 224 Training Loss: 4.166478623226618 \n",
      "     Training Step: 225 Training Loss: 5.118760976539635 \n",
      "     Training Step: 226 Training Loss: 4.0768386328235255 \n",
      "     Training Step: 227 Training Loss: 5.070824556945667 \n",
      "     Training Step: 228 Training Loss: 4.97625582028169 \n",
      "     Training Step: 229 Training Loss: 4.087072438793779 \n",
      "     Training Step: 230 Training Loss: 3.055173240109203 \n",
      "     Training Step: 231 Training Loss: 5.21582401010938 \n",
      "     Training Step: 232 Training Loss: 4.080483516338987 \n",
      "     Training Step: 233 Training Loss: 5.043468112718135 \n",
      "     Training Step: 234 Training Loss: 4.25054913040614 \n",
      "     Training Step: 235 Training Loss: 4.481958874862549 \n",
      "     Training Step: 236 Training Loss: 4.5408745746432855 \n",
      "     Training Step: 237 Training Loss: 3.8272885156306464 \n",
      "     Training Step: 238 Training Loss: 3.454138155286426 \n",
      "     Training Step: 239 Training Loss: 4.025321895971631 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 4.073416066152908 \n",
      "     Validation Step: 1 Validation Loss: 4.923208427944474 \n",
      "     Validation Step: 2 Validation Loss: 3.1819922331564747 \n",
      "     Validation Step: 3 Validation Loss: 4.432565952805508 \n",
      "     Validation Step: 4 Validation Loss: 3.907355276138211 \n",
      "     Validation Step: 5 Validation Loss: 4.975344932180465 \n",
      "     Validation Step: 6 Validation Loss: 4.881429400048128 \n",
      "     Validation Step: 7 Validation Loss: 3.8066092113819923 \n",
      "     Validation Step: 8 Validation Loss: 4.533017407216618 \n",
      "     Validation Step: 9 Validation Loss: 4.807123523480558 \n",
      "     Validation Step: 10 Validation Loss: 3.863420996976876 \n",
      "     Validation Step: 11 Validation Loss: 4.21226640869011 \n",
      "     Validation Step: 12 Validation Loss: 4.317254847553413 \n",
      "     Validation Step: 13 Validation Loss: 4.025945280554417 \n",
      "     Validation Step: 14 Validation Loss: 4.65569259525348 \n",
      "     Validation Step: 15 Validation Loss: 3.8890871248578187 \n",
      "     Validation Step: 16 Validation Loss: 4.9956892037205956 \n",
      "     Validation Step: 17 Validation Loss: 5.186755832947078 \n",
      "     Validation Step: 18 Validation Loss: 4.112248333896457 \n",
      "     Validation Step: 19 Validation Loss: 5.111548247344259 \n",
      "     Validation Step: 20 Validation Loss: 4.457585972515188 \n",
      "     Validation Step: 21 Validation Loss: 4.101242762906639 \n",
      "     Validation Step: 22 Validation Loss: 3.7744777438738875 \n",
      "     Validation Step: 23 Validation Loss: 4.1544884003791225 \n",
      "     Validation Step: 24 Validation Loss: 4.7527450007896075 \n",
      "     Validation Step: 25 Validation Loss: 3.9940964691102283 \n",
      "     Validation Step: 26 Validation Loss: 4.146113280497877 \n",
      "     Validation Step: 27 Validation Loss: 3.696090948772591 \n",
      "     Validation Step: 28 Validation Loss: 4.4656055225827105 \n",
      "     Validation Step: 29 Validation Loss: 3.9446773940005517 \n",
      "     Validation Step: 30 Validation Loss: 3.845408849449874 \n",
      "     Validation Step: 31 Validation Loss: 3.079559106689393 \n",
      "     Validation Step: 32 Validation Loss: 4.32653901260232 \n",
      "     Validation Step: 33 Validation Loss: 4.052192229835385 \n",
      "     Validation Step: 34 Validation Loss: 5.340087744747561 \n",
      "     Validation Step: 35 Validation Loss: 3.1217068768259897 \n",
      "     Validation Step: 36 Validation Loss: 4.010455467833414 \n",
      "     Validation Step: 37 Validation Loss: 4.423122208443038 \n",
      "     Validation Step: 38 Validation Loss: 3.5181419646003196 \n",
      "     Validation Step: 39 Validation Loss: 3.58254695070035 \n",
      "     Validation Step: 40 Validation Loss: 4.433849587920198 \n",
      "     Validation Step: 41 Validation Loss: 4.734214870436691 \n",
      "     Validation Step: 42 Validation Loss: 3.773145193767329 \n",
      "     Validation Step: 43 Validation Loss: 3.8447497453076798 \n",
      "     Validation Step: 44 Validation Loss: 3.4189712129407317 \n",
      "Epoch: 33\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 4.046163766770569 \n",
      "     Training Step: 1 Training Loss: 4.983937644254499 \n",
      "     Training Step: 2 Training Loss: 4.654909918984338 \n",
      "     Training Step: 3 Training Loss: 2.9833298143824303 \n",
      "     Training Step: 4 Training Loss: 4.42106583271898 \n",
      "     Training Step: 5 Training Loss: 3.5127502015589904 \n",
      "     Training Step: 6 Training Loss: 3.853669699934829 \n",
      "     Training Step: 7 Training Loss: 2.996531902190349 \n",
      "     Training Step: 8 Training Loss: 4.540796474212781 \n",
      "     Training Step: 9 Training Loss: 3.0265194394571067 \n",
      "     Training Step: 10 Training Loss: 4.3766569960198645 \n",
      "     Training Step: 11 Training Loss: 3.564390761364593 \n",
      "     Training Step: 12 Training Loss: 3.5012264335791112 \n",
      "     Training Step: 13 Training Loss: 3.3348163050450252 \n",
      "     Training Step: 14 Training Loss: 3.0898130295105455 \n",
      "     Training Step: 15 Training Loss: 3.522131381686225 \n",
      "     Training Step: 16 Training Loss: 3.915476009213018 \n",
      "     Training Step: 17 Training Loss: 4.827269678769737 \n",
      "     Training Step: 18 Training Loss: 3.992091611652418 \n",
      "     Training Step: 19 Training Loss: 3.344946580990208 \n",
      "     Training Step: 20 Training Loss: 4.49831360344772 \n",
      "     Training Step: 21 Training Loss: 5.113489878276883 \n",
      "     Training Step: 22 Training Loss: 3.375902189856651 \n",
      "     Training Step: 23 Training Loss: 4.652353426171588 \n",
      "     Training Step: 24 Training Loss: 3.8313172025966824 \n",
      "     Training Step: 25 Training Loss: 3.5493229409152827 \n",
      "     Training Step: 26 Training Loss: 3.3415965388049362 \n",
      "     Training Step: 27 Training Loss: 4.032150009202885 \n",
      "     Training Step: 28 Training Loss: 4.41241832388314 \n",
      "     Training Step: 29 Training Loss: 3.427518807009314 \n",
      "     Training Step: 30 Training Loss: 4.748565416161445 \n",
      "     Training Step: 31 Training Loss: 3.631057060808462 \n",
      "     Training Step: 32 Training Loss: 4.189174516383817 \n",
      "     Training Step: 33 Training Loss: 4.774684048554363 \n",
      "     Training Step: 34 Training Loss: 4.398657227573468 \n",
      "     Training Step: 35 Training Loss: 3.659494808284282 \n",
      "     Training Step: 36 Training Loss: 3.394816793457423 \n",
      "     Training Step: 37 Training Loss: 4.51726386020401 \n",
      "     Training Step: 38 Training Loss: 4.5900704903503 \n",
      "     Training Step: 39 Training Loss: 4.403479486773572 \n",
      "     Training Step: 40 Training Loss: 3.4249771530941957 \n",
      "     Training Step: 41 Training Loss: 4.323104011338229 \n",
      "     Training Step: 42 Training Loss: 5.020808572468155 \n",
      "     Training Step: 43 Training Loss: 4.435514156645096 \n",
      "     Training Step: 44 Training Loss: 3.731004544969917 \n",
      "     Training Step: 45 Training Loss: 4.3792719791771315 \n",
      "     Training Step: 46 Training Loss: 4.22875250508718 \n",
      "     Training Step: 47 Training Loss: 4.960290414266644 \n",
      "     Training Step: 48 Training Loss: 3.661882142379305 \n",
      "     Training Step: 49 Training Loss: 4.600394195481205 \n",
      "     Training Step: 50 Training Loss: 4.129936096822494 \n",
      "     Training Step: 51 Training Loss: 3.2431458050609634 \n",
      "     Training Step: 52 Training Loss: 3.7646679917638273 \n",
      "     Training Step: 53 Training Loss: 4.2434814501529265 \n",
      "     Training Step: 54 Training Loss: 2.646202066584364 \n",
      "     Training Step: 55 Training Loss: 3.7915373243707955 \n",
      "     Training Step: 56 Training Loss: 3.978999849596383 \n",
      "     Training Step: 57 Training Loss: 4.628033619488855 \n",
      "     Training Step: 58 Training Loss: 4.365572933052198 \n",
      "     Training Step: 59 Training Loss: 3.2198983232958422 \n",
      "     Training Step: 60 Training Loss: 5.173413023861456 \n",
      "     Training Step: 61 Training Loss: 4.780345560903565 \n",
      "     Training Step: 62 Training Loss: 2.9789406481458727 \n",
      "     Training Step: 63 Training Loss: 3.9622763221621597 \n",
      "     Training Step: 64 Training Loss: 2.7880237229976585 \n",
      "     Training Step: 65 Training Loss: 4.76870993793195 \n",
      "     Training Step: 66 Training Loss: 4.457525154800483 \n",
      "     Training Step: 67 Training Loss: 3.639863590269562 \n",
      "     Training Step: 68 Training Loss: 4.274363954830293 \n",
      "     Training Step: 69 Training Loss: 4.517299005724198 \n",
      "     Training Step: 70 Training Loss: 3.7380977092066563 \n",
      "     Training Step: 71 Training Loss: 4.73463852930997 \n",
      "     Training Step: 72 Training Loss: 4.208217246612835 \n",
      "     Training Step: 73 Training Loss: 3.5318136004031193 \n",
      "     Training Step: 74 Training Loss: 3.9741829801347426 \n",
      "     Training Step: 75 Training Loss: 4.483581864335186 \n",
      "     Training Step: 76 Training Loss: 4.923590140413251 \n",
      "     Training Step: 77 Training Loss: 4.537082300815057 \n",
      "     Training Step: 78 Training Loss: 5.112797619774899 \n",
      "     Training Step: 79 Training Loss: 4.111766281690957 \n",
      "     Training Step: 80 Training Loss: 4.741063667835412 \n",
      "     Training Step: 81 Training Loss: 5.049817745660169 \n",
      "     Training Step: 82 Training Loss: 4.218400868566601 \n",
      "     Training Step: 83 Training Loss: 3.732031639071692 \n",
      "     Training Step: 84 Training Loss: 4.567843288437127 \n",
      "     Training Step: 85 Training Loss: 4.759460407840495 \n",
      "     Training Step: 86 Training Loss: 4.222691338625645 \n",
      "     Training Step: 87 Training Loss: 4.6504886432865 \n",
      "     Training Step: 88 Training Loss: 3.7800068303027614 \n",
      "     Training Step: 89 Training Loss: 4.762787519943028 \n",
      "     Training Step: 90 Training Loss: 4.375371921961883 \n",
      "     Training Step: 91 Training Loss: 3.762167842535815 \n",
      "     Training Step: 92 Training Loss: 4.682048886571728 \n",
      "     Training Step: 93 Training Loss: 4.606248452077024 \n",
      "     Training Step: 94 Training Loss: 3.6592901937399946 \n",
      "     Training Step: 95 Training Loss: 3.612088594813763 \n",
      "     Training Step: 96 Training Loss: 3.9680483285361468 \n",
      "     Training Step: 97 Training Loss: 4.68217980411009 \n",
      "     Training Step: 98 Training Loss: 3.997926002977825 \n",
      "     Training Step: 99 Training Loss: 2.479362890962921 \n",
      "     Training Step: 100 Training Loss: 4.751267189693978 \n",
      "     Training Step: 101 Training Loss: 3.8903235684780806 \n",
      "     Training Step: 102 Training Loss: 2.9542045180009344 \n",
      "     Training Step: 103 Training Loss: 3.885155813836464 \n",
      "     Training Step: 104 Training Loss: 4.646864633143637 \n",
      "     Training Step: 105 Training Loss: 4.198267268548412 \n",
      "     Training Step: 106 Training Loss: 4.193398597101392 \n",
      "     Training Step: 107 Training Loss: 4.0366998768167255 \n",
      "     Training Step: 108 Training Loss: 3.3383004607950393 \n",
      "     Training Step: 109 Training Loss: 4.559926014535855 \n",
      "     Training Step: 110 Training Loss: 3.6017183219529745 \n",
      "     Training Step: 111 Training Loss: 3.247369434090063 \n",
      "     Training Step: 112 Training Loss: 4.294189524091301 \n",
      "     Training Step: 113 Training Loss: 4.7115345444155325 \n",
      "     Training Step: 114 Training Loss: 4.457754137046687 \n",
      "     Training Step: 115 Training Loss: 3.3039370325648085 \n",
      "     Training Step: 116 Training Loss: 3.739215240071301 \n",
      "     Training Step: 117 Training Loss: 4.008720837243624 \n",
      "     Training Step: 118 Training Loss: 3.915367740967638 \n",
      "     Training Step: 119 Training Loss: 3.823333329346252 \n",
      "     Training Step: 120 Training Loss: 4.504682734906749 \n",
      "     Training Step: 121 Training Loss: 4.462687773659686 \n",
      "     Training Step: 122 Training Loss: 4.993165233808516 \n",
      "     Training Step: 123 Training Loss: 4.196869311350239 \n",
      "     Training Step: 124 Training Loss: 4.2261575581031225 \n",
      "     Training Step: 125 Training Loss: 4.059027590787692 \n",
      "     Training Step: 126 Training Loss: 3.778628045464315 \n",
      "     Training Step: 127 Training Loss: 3.3468391299556193 \n",
      "     Training Step: 128 Training Loss: 3.606662856618235 \n",
      "     Training Step: 129 Training Loss: 5.043952128861188 \n",
      "     Training Step: 130 Training Loss: 3.18121585173363 \n",
      "     Training Step: 131 Training Loss: 3.0358085471844083 \n",
      "     Training Step: 132 Training Loss: 3.85634442915397 \n",
      "     Training Step: 133 Training Loss: 3.407458555963101 \n",
      "     Training Step: 134 Training Loss: 4.339446254799136 \n",
      "     Training Step: 135 Training Loss: 4.629722517078192 \n",
      "     Training Step: 136 Training Loss: 3.984127834188662 \n",
      "     Training Step: 137 Training Loss: 3.8041343539876014 \n",
      "     Training Step: 138 Training Loss: 3.498879265531631 \n",
      "     Training Step: 139 Training Loss: 3.4115640877695053 \n",
      "     Training Step: 140 Training Loss: 4.700345389507577 \n",
      "     Training Step: 141 Training Loss: 4.859941283415954 \n",
      "     Training Step: 142 Training Loss: 2.887086379172508 \n",
      "     Training Step: 143 Training Loss: 4.377621054868156 \n",
      "     Training Step: 144 Training Loss: 4.198017521955067 \n",
      "     Training Step: 145 Training Loss: 4.087136309736804 \n",
      "     Training Step: 146 Training Loss: 3.6099534072823487 \n",
      "     Training Step: 147 Training Loss: 3.5750243831595108 \n",
      "     Training Step: 148 Training Loss: 2.8501043089134597 \n",
      "     Training Step: 149 Training Loss: 2.960515017226337 \n",
      "     Training Step: 150 Training Loss: 3.891295311141802 \n",
      "     Training Step: 151 Training Loss: 2.913673409034528 \n",
      "     Training Step: 152 Training Loss: 3.7879650902522672 \n",
      "     Training Step: 153 Training Loss: 3.452251951487876 \n",
      "     Training Step: 154 Training Loss: 4.796424861076428 \n",
      "     Training Step: 155 Training Loss: 4.858132296445959 \n",
      "     Training Step: 156 Training Loss: 4.073889933410834 \n",
      "     Training Step: 157 Training Loss: 4.660946081339172 \n",
      "     Training Step: 158 Training Loss: 4.611852923663058 \n",
      "     Training Step: 159 Training Loss: 3.6076794353242456 \n",
      "     Training Step: 160 Training Loss: 4.963587077892532 \n",
      "     Training Step: 161 Training Loss: 5.085089576250974 \n",
      "     Training Step: 162 Training Loss: 4.498978628423977 \n",
      "     Training Step: 163 Training Loss: 4.843070419093209 \n",
      "     Training Step: 164 Training Loss: 3.7537947205977553 \n",
      "     Training Step: 165 Training Loss: 3.5114613127140406 \n",
      "     Training Step: 166 Training Loss: 3.8352001904818893 \n",
      "     Training Step: 167 Training Loss: 3.668797040233606 \n",
      "     Training Step: 168 Training Loss: 4.366244361216966 \n",
      "     Training Step: 169 Training Loss: 4.3345386181232906 \n",
      "     Training Step: 170 Training Loss: 3.350758459927759 \n",
      "     Training Step: 171 Training Loss: 4.253712145945279 \n",
      "     Training Step: 172 Training Loss: 4.402677611565965 \n",
      "     Training Step: 173 Training Loss: 4.604651437097214 \n",
      "     Training Step: 174 Training Loss: 4.673643216556116 \n",
      "     Training Step: 175 Training Loss: 4.007560534087849 \n",
      "     Training Step: 176 Training Loss: 4.10994736734418 \n",
      "     Training Step: 177 Training Loss: 3.811561874564997 \n",
      "     Training Step: 178 Training Loss: 4.681404285612133 \n",
      "     Training Step: 179 Training Loss: 4.738859498606617 \n",
      "     Training Step: 180 Training Loss: 4.2402137144824295 \n",
      "     Training Step: 181 Training Loss: 4.105141561598098 \n",
      "     Training Step: 182 Training Loss: 4.525515864378276 \n",
      "     Training Step: 183 Training Loss: 3.1877519786168356 \n",
      "     Training Step: 184 Training Loss: 3.8617476377526456 \n",
      "     Training Step: 185 Training Loss: 4.388451128697557 \n",
      "     Training Step: 186 Training Loss: 3.262224043899781 \n",
      "     Training Step: 187 Training Loss: 3.7292833163471997 \n",
      "     Training Step: 188 Training Loss: 4.228837363798114 \n",
      "     Training Step: 189 Training Loss: 3.9217597014789964 \n",
      "     Training Step: 190 Training Loss: 2.97977901868061 \n",
      "     Training Step: 191 Training Loss: 3.572529503843633 \n",
      "     Training Step: 192 Training Loss: 3.167325227832528 \n",
      "     Training Step: 193 Training Loss: 3.5219402271907794 \n",
      "     Training Step: 194 Training Loss: 4.97092235690055 \n",
      "     Training Step: 195 Training Loss: 4.735835965089804 \n",
      "     Training Step: 196 Training Loss: 3.1271586941773397 \n",
      "     Training Step: 197 Training Loss: 3.4511501312954014 \n",
      "     Training Step: 198 Training Loss: 4.845224041080599 \n",
      "     Training Step: 199 Training Loss: 3.8693784349047426 \n",
      "     Training Step: 200 Training Loss: 4.163604847378257 \n",
      "     Training Step: 201 Training Loss: 4.470752377545959 \n",
      "     Training Step: 202 Training Loss: 3.890482222127468 \n",
      "     Training Step: 203 Training Loss: 4.670938227635197 \n",
      "     Training Step: 204 Training Loss: 4.80790179365117 \n",
      "     Training Step: 205 Training Loss: 4.033393090323567 \n",
      "     Training Step: 206 Training Loss: 3.9158730734770604 \n",
      "     Training Step: 207 Training Loss: 4.357134923916093 \n",
      "     Training Step: 208 Training Loss: 4.020793434804745 \n",
      "     Training Step: 209 Training Loss: 3.9601474193995942 \n",
      "     Training Step: 210 Training Loss: 3.9353786287108043 \n",
      "     Training Step: 211 Training Loss: 3.8983472497926877 \n",
      "     Training Step: 212 Training Loss: 3.4307856281848292 \n",
      "     Training Step: 213 Training Loss: 4.965223422154981 \n",
      "     Training Step: 214 Training Loss: 4.057139314777694 \n",
      "     Training Step: 215 Training Loss: 5.277908797627525 \n",
      "     Training Step: 216 Training Loss: 4.178616080149406 \n",
      "     Training Step: 217 Training Loss: 3.3533527528136373 \n",
      "     Training Step: 218 Training Loss: 3.8883858073427393 \n",
      "     Training Step: 219 Training Loss: 4.869283252322338 \n",
      "     Training Step: 220 Training Loss: 3.242195001267578 \n",
      "     Training Step: 221 Training Loss: 4.512110400233116 \n",
      "     Training Step: 222 Training Loss: 2.8401581677562753 \n",
      "     Training Step: 223 Training Loss: 4.768884321389194 \n",
      "     Training Step: 224 Training Loss: 4.860257669283164 \n",
      "     Training Step: 225 Training Loss: 3.8297830375797726 \n",
      "     Training Step: 226 Training Loss: 4.171213901224445 \n",
      "     Training Step: 227 Training Loss: 3.9551542236263955 \n",
      "     Training Step: 228 Training Loss: 4.002259842274055 \n",
      "     Training Step: 229 Training Loss: 4.92081365710122 \n",
      "     Training Step: 230 Training Loss: 4.365936115258651 \n",
      "     Training Step: 231 Training Loss: 3.1021240539900012 \n",
      "     Training Step: 232 Training Loss: 4.688806284951881 \n",
      "     Training Step: 233 Training Loss: 3.62967527504346 \n",
      "     Training Step: 234 Training Loss: 4.476498689378592 \n",
      "     Training Step: 235 Training Loss: 3.639234623380738 \n",
      "     Training Step: 236 Training Loss: 5.139858107582954 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18688\\3021393557.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mnum_epochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m40\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mloss_weights\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mtrain_input\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mval_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_validate_ACOPF_chained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedder_model\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mACOPF_optimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mtrain_validate_ACOPF_chained\u001B[1;34m(minimizer_model, enforcer_model, embedder_model, ACOPF_optimizer, train_inputs, val_inputs, loss_weights, start_epoch, num_epochs, return_outputs, save_model)\u001B[0m\n\u001B[0;32m   2566\u001B[0m             \u001B[0mout_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0membedder_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpowers_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0medge_idx_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0medge_attr_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2567\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2568\u001B[1;33m             \u001B[0mphysics_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpowers_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2569\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2570\u001B[0m             \u001B[1;31m# physics_loss, mre_loss, penalty_loss, unsupervised_loss = self_supervised_hetero_obj_fn(x_dict,out_dict, bus_idx_neighbors_dict,constraint_dict,scaler)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mself_supervised_embedder_obj_fn\u001B[1;34m(x_dict, out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   2214\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2215\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mself_supervised_embedder_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2216\u001B[1;33m     \u001B[0mphysics_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_physics_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2217\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2218\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mphysics_loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mcalc_physics_loss\u001B[1;34m(x_dict, out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   3168\u001B[0m             volt_angle_i = x_dict[from_bus][bus_idx][1].detach().clone() * torch.tensor(delta_std) + torch.tensor(\n\u001B[0;32m   3169\u001B[0m                 delta_mean)\n\u001B[1;32m-> 3170\u001B[1;33m             \u001B[0mP_i\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mout_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfrom_bus\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbus_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_std\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_mean\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3171\u001B[0m             \u001B[0mQ_i\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mout_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfrom_bus\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbus_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mQ_std\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mQ_mean\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3172\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mformat_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mformat_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mextract_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    198\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m     \u001B[0mstack\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mStackSummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwalk_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m     \u001B[0mstack\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreverse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mstack\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    357\u001B[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001B[0;32m    358\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mfilename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfnames\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 359\u001B[1;33m             \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckcache\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    360\u001B[0m         \u001B[1;31m# If immediate lookup was desired, trigger lookups now.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlookup_lines\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\IPython\\core\\compilerop.py\u001B[0m in \u001B[0;36mcheck_linecache_ipython\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \"\"\"\n\u001B[0;32m    184\u001B[0m     \u001B[1;31m# First call the original checkcache as intended\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 185\u001B[1;33m     \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_checkcache_ori\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    186\u001B[0m     \u001B[1;31m# Then, update back the cache with our data, so that tracebacks related\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m     \u001B[1;31m# to our compiled codes can be produced.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\linecache.py\u001B[0m in \u001B[0;36mcheckcache\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mcontinue\u001B[0m   \u001B[1;31m# no-op for files loaded via a __loader__\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m             \u001B[0mstat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfullname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mcache\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 31\n",
    "num_epochs = 40\n",
    "loss_weights = (1.0, 1.0, 1.0)\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF_chained(None,None, embedder_model,ACOPF_optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 74.10785655703286 \n",
      "     Training Step: 1 Training Loss: 78.96264419612274 \n",
      "     Training Step: 2 Training Loss: 76.77236283351431 \n",
      "     Training Step: 3 Training Loss: 76.25845629619688 \n",
      "     Training Step: 4 Training Loss: 75.83286298836633 \n",
      "     Training Step: 5 Training Loss: 75.3381674617457 \n",
      "     Training Step: 6 Training Loss: 75.37811547601538 \n",
      "     Training Step: 7 Training Loss: 76.05679491374181 \n",
      "     Training Step: 8 Training Loss: 75.12420226002133 \n",
      "     Training Step: 9 Training Loss: 74.50435581560086 \n",
      "     Training Step: 10 Training Loss: 74.34658236591613 \n",
      "     Training Step: 11 Training Loss: 74.97363495732111 \n",
      "     Training Step: 12 Training Loss: 74.9749935821241 \n",
      "     Training Step: 13 Training Loss: 74.74882444381507 \n",
      "     Training Step: 14 Training Loss: 74.54852795205889 \n",
      "     Training Step: 15 Training Loss: 74.56001026661204 \n",
      "     Training Step: 16 Training Loss: 74.31244047077504 \n",
      "     Training Step: 17 Training Loss: 74.41317745561318 \n",
      "     Training Step: 18 Training Loss: 74.49237622431515 \n",
      "     Training Step: 19 Training Loss: 74.52575659117703 \n",
      "     Training Step: 20 Training Loss: 74.77495771642906 \n",
      "     Training Step: 21 Training Loss: 74.5979163952055 \n",
      "     Training Step: 22 Training Loss: 74.71255113361651 \n",
      "     Training Step: 23 Training Loss: 74.80208760099508 \n",
      "     Training Step: 24 Training Loss: 74.63432848675953 \n",
      "     Training Step: 25 Training Loss: 74.61311692537672 \n",
      "     Training Step: 26 Training Loss: 74.33334832265362 \n",
      "     Training Step: 27 Training Loss: 74.36754382424725 \n",
      "     Training Step: 28 Training Loss: 74.50394168140221 \n",
      "     Training Step: 29 Training Loss: 74.43062130275581 \n",
      "     Training Step: 30 Training Loss: 74.38365699576343 \n",
      "     Training Step: 31 Training Loss: 74.85382346735325 \n",
      "     Training Step: 32 Training Loss: 74.42234182983758 \n",
      "     Training Step: 33 Training Loss: 74.5921385911409 \n",
      "     Training Step: 34 Training Loss: 74.23609585191903 \n",
      "     Training Step: 35 Training Loss: 74.42376482352283 \n",
      "     Training Step: 36 Training Loss: 74.61013391514916 \n",
      "     Training Step: 37 Training Loss: 74.41341463277327 \n",
      "     Training Step: 38 Training Loss: 74.51697981114738 \n",
      "     Training Step: 39 Training Loss: 74.25749722550776 \n",
      "     Training Step: 40 Training Loss: 74.31603501715213 \n",
      "     Training Step: 41 Training Loss: 74.46592408351071 \n",
      "     Training Step: 42 Training Loss: 74.70780830320352 \n",
      "     Training Step: 43 Training Loss: 74.30177316737935 \n",
      "     Training Step: 44 Training Loss: 74.62164871098621 \n",
      "     Training Step: 45 Training Loss: 74.61089242449609 \n",
      "     Training Step: 46 Training Loss: 74.48959102997269 \n",
      "     Training Step: 47 Training Loss: 74.52701783092915 \n",
      "     Training Step: 48 Training Loss: 74.60289072841832 \n",
      "     Training Step: 49 Training Loss: 74.55100868422886 \n",
      "     Training Step: 50 Training Loss: 74.37868063125238 \n",
      "     Training Step: 51 Training Loss: 74.25099436743145 \n",
      "     Training Step: 52 Training Loss: 74.88766863027868 \n",
      "     Training Step: 53 Training Loss: 74.98457653988231 \n",
      "     Training Step: 54 Training Loss: 74.52587281490734 \n",
      "     Training Step: 55 Training Loss: 74.85519484636421 \n",
      "     Training Step: 56 Training Loss: 75.52805372891743 \n",
      "     Training Step: 57 Training Loss: 75.49878981537853 \n",
      "     Training Step: 58 Training Loss: 74.92396070784864 \n",
      "     Training Step: 59 Training Loss: 75.10150150479146 \n",
      "     Training Step: 60 Training Loss: 75.40865579827052 \n",
      "     Training Step: 61 Training Loss: 74.65005681761771 \n",
      "     Training Step: 62 Training Loss: 74.6013977957342 \n",
      "     Training Step: 63 Training Loss: 74.69453237676737 \n",
      "     Training Step: 64 Training Loss: 74.69429518584647 \n",
      "     Training Step: 65 Training Loss: 74.70406981942327 \n",
      "     Training Step: 66 Training Loss: 74.53811772127665 \n",
      "     Training Step: 67 Training Loss: 74.8845137668014 \n",
      "     Training Step: 68 Training Loss: 74.65974414009347 \n",
      "     Training Step: 69 Training Loss: 74.68439652130542 \n",
      "     Training Step: 70 Training Loss: 74.47453811411204 \n",
      "     Training Step: 71 Training Loss: 74.3051704046209 \n",
      "     Training Step: 72 Training Loss: 74.58156285008299 \n",
      "     Training Step: 73 Training Loss: 74.52678318893115 \n",
      "     Training Step: 74 Training Loss: 74.18543183778587 \n",
      "     Training Step: 75 Training Loss: 74.86215006460358 \n",
      "     Training Step: 76 Training Loss: 74.62841494100158 \n",
      "     Training Step: 77 Training Loss: 74.57769785909073 \n",
      "     Training Step: 78 Training Loss: 74.62469665735544 \n",
      "     Training Step: 79 Training Loss: 74.49148266624405 \n",
      "     Training Step: 80 Training Loss: 74.38156880033827 \n",
      "     Training Step: 81 Training Loss: 74.36707682323328 \n",
      "     Training Step: 82 Training Loss: 74.19641129891292 \n",
      "     Training Step: 83 Training Loss: 74.27805117366529 \n",
      "     Training Step: 84 Training Loss: 74.34380752920156 \n",
      "     Training Step: 85 Training Loss: 74.51468587864409 \n",
      "     Training Step: 86 Training Loss: 74.82242825027892 \n",
      "     Training Step: 87 Training Loss: 74.2625239495942 \n",
      "     Training Step: 88 Training Loss: 74.77456155725274 \n",
      "     Training Step: 89 Training Loss: 74.29386222858552 \n",
      "     Training Step: 90 Training Loss: 74.46839323728982 \n",
      "     Training Step: 91 Training Loss: 74.35189834787556 \n",
      "     Training Step: 92 Training Loss: 74.4056068081406 \n",
      "     Training Step: 93 Training Loss: 74.44844450417465 \n",
      "     Training Step: 94 Training Loss: 74.23033782827635 \n",
      "     Training Step: 95 Training Loss: 74.72621089389918 \n",
      "     Training Step: 96 Training Loss: 74.73614310039555 \n",
      "     Training Step: 97 Training Loss: 74.53981709853643 \n",
      "     Training Step: 98 Training Loss: 74.45163857035348 \n",
      "     Training Step: 99 Training Loss: 74.25957492063365 \n",
      "     Training Step: 100 Training Loss: 74.83617481341449 \n",
      "     Training Step: 101 Training Loss: 74.61840507160827 \n",
      "     Training Step: 102 Training Loss: 74.41897235143236 \n",
      "     Training Step: 103 Training Loss: 74.6395302160909 \n",
      "     Training Step: 104 Training Loss: 74.9017788865362 \n",
      "     Training Step: 105 Training Loss: 74.4223761469429 \n",
      "     Training Step: 106 Training Loss: 74.5954363740913 \n",
      "     Training Step: 107 Training Loss: 74.52891592753062 \n",
      "     Training Step: 108 Training Loss: 74.88894205147359 \n",
      "     Training Step: 109 Training Loss: 74.44188040598867 \n",
      "     Training Step: 110 Training Loss: 74.4845758323161 \n",
      "     Training Step: 111 Training Loss: 74.58812923725387 \n",
      "     Training Step: 112 Training Loss: 74.46709384907393 \n",
      "     Training Step: 113 Training Loss: 74.66382439275438 \n",
      "     Training Step: 114 Training Loss: 74.39405508624884 \n",
      "     Training Step: 115 Training Loss: 74.43635147280717 \n",
      "     Training Step: 116 Training Loss: 74.91648442546591 \n",
      "     Training Step: 117 Training Loss: 74.88017201927494 \n",
      "     Training Step: 118 Training Loss: 74.41745662394207 \n",
      "     Training Step: 119 Training Loss: 74.28459931212915 \n",
      "     Training Step: 120 Training Loss: 74.57000229992214 \n",
      "     Training Step: 121 Training Loss: 74.56579053822819 \n",
      "     Training Step: 122 Training Loss: 74.6576253422549 \n",
      "     Training Step: 123 Training Loss: 75.01306539450653 \n",
      "     Training Step: 124 Training Loss: 74.57585489237233 \n",
      "     Training Step: 125 Training Loss: 74.27335977208126 \n",
      "     Training Step: 126 Training Loss: 74.24905880357544 \n",
      "     Training Step: 127 Training Loss: 74.4056244866514 \n",
      "     Training Step: 128 Training Loss: 74.84447633664448 \n",
      "     Training Step: 129 Training Loss: 74.32311879540289 \n",
      "     Training Step: 130 Training Loss: 74.27354857316938 \n",
      "     Training Step: 131 Training Loss: 74.49964352204123 \n",
      "     Training Step: 132 Training Loss: 74.60674944583883 \n",
      "     Training Step: 133 Training Loss: 74.2545352745429 \n",
      "     Training Step: 134 Training Loss: 74.54804117630265 \n",
      "     Training Step: 135 Training Loss: 74.488687827505 \n",
      "     Training Step: 136 Training Loss: 74.23556161576371 \n",
      "     Training Step: 137 Training Loss: 74.39998719707381 \n",
      "     Training Step: 138 Training Loss: 74.43564233645786 \n",
      "     Training Step: 139 Training Loss: 74.34836833648737 \n",
      "     Training Step: 140 Training Loss: 75.03418730297922 \n",
      "     Training Step: 141 Training Loss: 74.79360033602833 \n",
      "     Training Step: 142 Training Loss: 74.35277989593712 \n",
      "     Training Step: 143 Training Loss: 74.34044759613278 \n",
      "     Training Step: 144 Training Loss: 74.33301058817193 \n",
      "     Training Step: 145 Training Loss: 74.56601462057743 \n",
      "     Training Step: 146 Training Loss: 74.82704012546081 \n",
      "     Training Step: 147 Training Loss: 74.38350245302426 \n",
      "     Training Step: 148 Training Loss: 74.41664886330645 \n",
      "     Training Step: 149 Training Loss: 74.5248381344173 \n",
      "     Training Step: 150 Training Loss: 75.00623669703445 \n",
      "     Training Step: 151 Training Loss: 74.5503843216375 \n",
      "     Training Step: 152 Training Loss: 74.36032587663381 \n",
      "     Training Step: 153 Training Loss: 74.43167055047371 \n",
      "     Training Step: 154 Training Loss: 74.75249889427035 \n",
      "     Training Step: 155 Training Loss: 74.36207921601081 \n",
      "     Training Step: 156 Training Loss: 74.39012604070888 \n",
      "     Training Step: 157 Training Loss: 74.42014035612873 \n",
      "     Training Step: 158 Training Loss: 74.59294166493565 \n",
      "     Training Step: 159 Training Loss: 74.3640369987822 \n",
      "     Training Step: 160 Training Loss: 74.64290315712103 \n",
      "     Training Step: 161 Training Loss: 74.52974544734796 \n",
      "     Training Step: 162 Training Loss: 74.59188222738355 \n",
      "     Training Step: 163 Training Loss: 74.44759302069113 \n",
      "     Training Step: 164 Training Loss: 74.49061399053753 \n",
      "     Training Step: 165 Training Loss: 74.35903317460796 \n",
      "     Training Step: 166 Training Loss: 74.62358104473881 \n",
      "     Training Step: 167 Training Loss: 74.25438511990177 \n",
      "     Training Step: 168 Training Loss: 74.93800574083538 \n",
      "     Training Step: 169 Training Loss: 74.32606382844473 \n",
      "     Training Step: 170 Training Loss: 74.28726311030533 \n",
      "     Training Step: 171 Training Loss: 74.50332811333652 \n",
      "     Training Step: 172 Training Loss: 74.35186614659443 \n",
      "     Training Step: 173 Training Loss: 74.4489603806475 \n",
      "     Training Step: 174 Training Loss: 74.43330398316648 \n",
      "     Training Step: 175 Training Loss: 74.33157238006144 \n",
      "     Training Step: 176 Training Loss: 74.35305383301919 \n",
      "     Training Step: 177 Training Loss: 74.29793639436825 \n",
      "     Training Step: 178 Training Loss: 74.1033133365307 \n",
      "     Training Step: 179 Training Loss: 74.23035581317586 \n",
      "     Training Step: 180 Training Loss: 74.24998838727012 \n",
      "     Training Step: 181 Training Loss: 74.63615947441487 \n",
      "     Training Step: 182 Training Loss: 74.21083220946576 \n",
      "     Training Step: 183 Training Loss: 74.24968874651579 \n",
      "     Training Step: 184 Training Loss: 74.44189223518063 \n",
      "     Training Step: 185 Training Loss: 74.32917958520937 \n",
      "     Training Step: 186 Training Loss: 74.72250137339931 \n",
      "     Training Step: 187 Training Loss: 74.17688645762357 \n",
      "     Training Step: 188 Training Loss: 74.31804415000367 \n",
      "     Training Step: 189 Training Loss: 74.28715907914966 \n",
      "     Training Step: 190 Training Loss: 74.53176992960809 \n",
      "     Training Step: 191 Training Loss: 74.38449334474254 \n",
      "     Training Step: 192 Training Loss: 74.74590742757192 \n",
      "     Training Step: 193 Training Loss: 74.4041900161566 \n",
      "     Training Step: 194 Training Loss: 74.15564172378004 \n",
      "     Training Step: 195 Training Loss: 74.62140427315782 \n",
      "     Training Step: 196 Training Loss: 74.50649864360648 \n",
      "     Training Step: 197 Training Loss: 74.40848770878719 \n",
      "     Training Step: 198 Training Loss: 74.6360236687948 \n",
      "     Training Step: 199 Training Loss: 74.2865659025618 \n",
      "     Training Step: 200 Training Loss: 74.82859173838615 \n",
      "     Training Step: 201 Training Loss: 74.66504901799851 \n",
      "     Training Step: 202 Training Loss: 74.3908822497578 \n",
      "     Training Step: 203 Training Loss: 74.5647288680797 \n",
      "     Training Step: 204 Training Loss: 74.41915826474215 \n",
      "     Training Step: 205 Training Loss: 74.29965567539992 \n",
      "     Training Step: 206 Training Loss: 74.23779185161159 \n",
      "     Training Step: 207 Training Loss: 74.65984493472085 \n",
      "     Training Step: 208 Training Loss: 74.33963757924622 \n",
      "     Training Step: 209 Training Loss: 74.2447369337741 \n",
      "     Training Step: 210 Training Loss: 74.32807314149875 \n",
      "     Training Step: 211 Training Loss: 75.57117387271744 \n",
      "     Training Step: 212 Training Loss: 74.74332877220556 \n",
      "     Training Step: 213 Training Loss: 74.40943759646336 \n",
      "     Training Step: 214 Training Loss: 74.42094737185414 \n",
      "     Training Step: 215 Training Loss: 74.73262592684459 \n",
      "     Training Step: 216 Training Loss: 74.57366194278495 \n",
      "     Training Step: 217 Training Loss: 74.15200280610638 \n",
      "     Training Step: 218 Training Loss: 74.81895116808468 \n",
      "     Training Step: 219 Training Loss: 74.7784539508721 \n",
      "     Training Step: 220 Training Loss: 74.20078965209439 \n",
      "     Training Step: 221 Training Loss: 74.54517066663996 \n",
      "     Training Step: 222 Training Loss: 74.2443966772725 \n",
      "     Training Step: 223 Training Loss: 74.49656466390257 \n",
      "     Training Step: 224 Training Loss: 74.51474392102695 \n",
      "     Training Step: 225 Training Loss: 74.22452684917734 \n",
      "     Training Step: 226 Training Loss: 74.73730667559927 \n",
      "     Training Step: 227 Training Loss: 74.6655286821863 \n",
      "     Training Step: 228 Training Loss: 74.17423031171748 \n",
      "     Training Step: 229 Training Loss: 74.1430682157014 \n",
      "     Training Step: 230 Training Loss: 74.50623083783529 \n",
      "     Training Step: 231 Training Loss: 74.41934161720154 \n",
      "     Training Step: 232 Training Loss: 74.66291809416431 \n",
      "     Training Step: 233 Training Loss: 74.40578563289039 \n",
      "     Training Step: 234 Training Loss: 74.16495903717508 \n",
      "     Training Step: 235 Training Loss: 74.28627299766659 \n",
      "     Training Step: 236 Training Loss: 74.21202231200553 \n",
      "     Training Step: 237 Training Loss: 74.3193212228812 \n",
      "     Training Step: 238 Training Loss: 74.39416717142133 \n",
      "     Training Step: 239 Training Loss: 74.40489368209593 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 74.49012152763146 \n",
      "     Validation Step: 1 Validation Loss: 75.04346927501982 \n",
      "     Validation Step: 2 Validation Loss: 74.54014927241815 \n",
      "     Validation Step: 3 Validation Loss: 74.78990592241882 \n",
      "     Validation Step: 4 Validation Loss: 74.46126247359744 \n",
      "     Validation Step: 5 Validation Loss: 74.56541810307316 \n",
      "     Validation Step: 6 Validation Loss: 74.91599131583182 \n",
      "     Validation Step: 7 Validation Loss: 74.59027037947924 \n",
      "     Validation Step: 8 Validation Loss: 74.56976804373399 \n",
      "     Validation Step: 9 Validation Loss: 74.4841068559995 \n",
      "     Validation Step: 10 Validation Loss: 74.51372318692887 \n",
      "     Validation Step: 11 Validation Loss: 75.03124280698435 \n",
      "     Validation Step: 12 Validation Loss: 74.54650503954208 \n",
      "     Validation Step: 13 Validation Loss: 74.59739975833565 \n",
      "     Validation Step: 14 Validation Loss: 74.81601787281339 \n",
      "     Validation Step: 15 Validation Loss: 74.1877535453433 \n",
      "     Validation Step: 16 Validation Loss: 74.55728331767169 \n",
      "     Validation Step: 17 Validation Loss: 74.2523369433255 \n",
      "     Validation Step: 18 Validation Loss: 74.24078739525768 \n",
      "     Validation Step: 19 Validation Loss: 74.27338883969512 \n",
      "     Validation Step: 20 Validation Loss: 75.19205231412914 \n",
      "     Validation Step: 21 Validation Loss: 74.8970094018702 \n",
      "     Validation Step: 22 Validation Loss: 74.29819555091129 \n",
      "     Validation Step: 23 Validation Loss: 74.50187286467151 \n",
      "     Validation Step: 24 Validation Loss: 74.44195624281463 \n",
      "     Validation Step: 25 Validation Loss: 74.61852603319778 \n",
      "     Validation Step: 26 Validation Loss: 74.44055171547375 \n",
      "     Validation Step: 27 Validation Loss: 74.26987048841461 \n",
      "     Validation Step: 28 Validation Loss: 74.4894894873052 \n",
      "     Validation Step: 29 Validation Loss: 74.6372021148508 \n",
      "     Validation Step: 30 Validation Loss: 74.24322939727655 \n",
      "     Validation Step: 31 Validation Loss: 74.37110185996804 \n",
      "     Validation Step: 32 Validation Loss: 74.92009443023454 \n",
      "     Validation Step: 33 Validation Loss: 74.54325019444693 \n",
      "     Validation Step: 34 Validation Loss: 74.460539785909 \n",
      "     Validation Step: 35 Validation Loss: 74.28231819413834 \n",
      "     Validation Step: 36 Validation Loss: 74.28445614184575 \n",
      "     Validation Step: 37 Validation Loss: 74.32246736668924 \n",
      "     Validation Step: 38 Validation Loss: 74.36398585529285 \n",
      "     Validation Step: 39 Validation Loss: 74.48813624433525 \n",
      "     Validation Step: 40 Validation Loss: 74.49673936440381 \n",
      "     Validation Step: 41 Validation Loss: 74.85321442250196 \n",
      "     Validation Step: 42 Validation Loss: 74.37220600515624 \n",
      "     Validation Step: 43 Validation Loss: 74.46930916791608 \n",
      "     Validation Step: 44 Validation Loss: 74.26410486717285 \n",
      "Epoch: 71\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 74.49996147171036 \n",
      "     Training Step: 1 Training Loss: 74.30556754688978 \n",
      "     Training Step: 2 Training Loss: 74.2344661932403 \n",
      "     Training Step: 3 Training Loss: 74.23025230175048 \n",
      "     Training Step: 4 Training Loss: 74.34403869749335 \n",
      "     Training Step: 5 Training Loss: 74.7758372883327 \n",
      "     Training Step: 6 Training Loss: 74.60313146271426 \n",
      "     Training Step: 7 Training Loss: 74.39263695162707 \n",
      "     Training Step: 8 Training Loss: 74.20507281098062 \n",
      "     Training Step: 9 Training Loss: 74.53592949307185 \n",
      "     Training Step: 10 Training Loss: 74.30473617059619 \n",
      "     Training Step: 11 Training Loss: 74.19388427723307 \n",
      "     Training Step: 12 Training Loss: 74.09157762216458 \n",
      "     Training Step: 13 Training Loss: 74.25939928091525 \n",
      "     Training Step: 14 Training Loss: 74.44785173495325 \n",
      "     Training Step: 15 Training Loss: 74.154285472915 \n",
      "     Training Step: 16 Training Loss: 74.15177250169725 \n",
      "     Training Step: 17 Training Loss: 74.47763891338491 \n",
      "     Training Step: 18 Training Loss: 74.34729776615491 \n",
      "     Training Step: 19 Training Loss: 74.63953294401605 \n",
      "     Training Step: 20 Training Loss: 74.13542589532187 \n",
      "     Training Step: 21 Training Loss: 74.19011870247763 \n",
      "     Training Step: 22 Training Loss: 74.38241156437013 \n",
      "     Training Step: 23 Training Loss: 74.30043635260674 \n",
      "     Training Step: 24 Training Loss: 74.34091582595656 \n",
      "     Training Step: 25 Training Loss: 74.38627395539676 \n",
      "     Training Step: 26 Training Loss: 74.56172065440485 \n",
      "     Training Step: 27 Training Loss: 74.4231595049007 \n",
      "     Training Step: 28 Training Loss: 74.30190838096932 \n",
      "     Training Step: 29 Training Loss: 74.44887615910152 \n",
      "     Training Step: 30 Training Loss: 74.27924338553801 \n",
      "     Training Step: 31 Training Loss: 74.50466544997217 \n",
      "     Training Step: 32 Training Loss: 74.65649804198677 \n",
      "     Training Step: 33 Training Loss: 74.25346933439454 \n",
      "     Training Step: 34 Training Loss: 74.23410229722302 \n",
      "     Training Step: 35 Training Loss: 74.4872972717126 \n",
      "     Training Step: 36 Training Loss: 74.19597861134154 \n",
      "     Training Step: 37 Training Loss: 74.24097460392859 \n",
      "     Training Step: 38 Training Loss: 74.36511812260443 \n",
      "     Training Step: 39 Training Loss: 74.186203368581 \n",
      "     Training Step: 40 Training Loss: 74.31539821576918 \n",
      "     Training Step: 41 Training Loss: 74.20559470185285 \n",
      "     Training Step: 42 Training Loss: 74.41328845614734 \n",
      "     Training Step: 43 Training Loss: 74.19001872205523 \n",
      "     Training Step: 44 Training Loss: 74.80396492392983 \n",
      "     Training Step: 45 Training Loss: 74.38137873736429 \n",
      "     Training Step: 46 Training Loss: 74.4939880396635 \n",
      "     Training Step: 47 Training Loss: 74.35888427258706 \n",
      "     Training Step: 48 Training Loss: 74.32264391172329 \n",
      "     Training Step: 49 Training Loss: 74.21810414369047 \n",
      "     Training Step: 50 Training Loss: 74.2329425353861 \n",
      "     Training Step: 51 Training Loss: 74.61008698786301 \n",
      "     Training Step: 52 Training Loss: 74.5124749145497 \n",
      "     Training Step: 53 Training Loss: 74.35552803198914 \n",
      "     Training Step: 54 Training Loss: 74.48587549284295 \n",
      "     Training Step: 55 Training Loss: 74.55480226350062 \n",
      "     Training Step: 56 Training Loss: 74.20546531282838 \n",
      "     Training Step: 57 Training Loss: 74.2443357950683 \n",
      "     Training Step: 58 Training Loss: 74.15686234108439 \n",
      "     Training Step: 59 Training Loss: 74.40553852324074 \n",
      "     Training Step: 60 Training Loss: 74.52112986191341 \n",
      "     Training Step: 61 Training Loss: 74.44524220120651 \n",
      "     Training Step: 62 Training Loss: 74.55343569697473 \n",
      "     Training Step: 63 Training Loss: 74.84997305654844 \n",
      "     Training Step: 64 Training Loss: 74.52380066405559 \n",
      "     Training Step: 65 Training Loss: 74.52897078236987 \n",
      "     Training Step: 66 Training Loss: 74.42025585142181 \n",
      "     Training Step: 67 Training Loss: 74.38818335044752 \n",
      "     Training Step: 68 Training Loss: 74.24565054434944 \n",
      "     Training Step: 69 Training Loss: 74.27839897553854 \n",
      "     Training Step: 70 Training Loss: 74.39088235476439 \n",
      "     Training Step: 71 Training Loss: 74.61046363537808 \n",
      "     Training Step: 72 Training Loss: 74.21541948047035 \n",
      "     Training Step: 73 Training Loss: 74.47873806746503 \n",
      "     Training Step: 74 Training Loss: 74.38146205377956 \n",
      "     Training Step: 75 Training Loss: 74.38285507701983 \n",
      "     Training Step: 76 Training Loss: 74.79214195796406 \n",
      "     Training Step: 77 Training Loss: 74.25635049247553 \n",
      "     Training Step: 78 Training Loss: 74.52016685379118 \n",
      "     Training Step: 79 Training Loss: 74.19754651113489 \n",
      "     Training Step: 80 Training Loss: 74.42164680542555 \n",
      "     Training Step: 81 Training Loss: 74.46279312366045 \n",
      "     Training Step: 82 Training Loss: 74.59940609042012 \n",
      "     Training Step: 83 Training Loss: 74.27843136445522 \n",
      "     Training Step: 84 Training Loss: 74.28078355724949 \n",
      "     Training Step: 85 Training Loss: 74.52822873493285 \n",
      "     Training Step: 86 Training Loss: 74.57665436684451 \n",
      "     Training Step: 87 Training Loss: 74.30442666980902 \n",
      "     Training Step: 88 Training Loss: 74.12552189889134 \n",
      "     Training Step: 89 Training Loss: 74.1458726552184 \n",
      "     Training Step: 90 Training Loss: 74.48032980313455 \n",
      "     Training Step: 91 Training Loss: 74.21864392048528 \n",
      "     Training Step: 92 Training Loss: 74.67457727129928 \n",
      "     Training Step: 93 Training Loss: 74.51943150720714 \n",
      "     Training Step: 94 Training Loss: 74.57019114938083 \n",
      "     Training Step: 95 Training Loss: 74.4099421943056 \n",
      "     Training Step: 96 Training Loss: 74.31043857663899 \n",
      "     Training Step: 97 Training Loss: 74.62060881856196 \n",
      "     Training Step: 98 Training Loss: 74.65227880357146 \n",
      "     Training Step: 99 Training Loss: 74.64026422147833 \n",
      "     Training Step: 100 Training Loss: 74.76674106472487 \n",
      "     Training Step: 101 Training Loss: 74.45124732498763 \n",
      "     Training Step: 102 Training Loss: 74.52926940221094 \n",
      "     Training Step: 103 Training Loss: 74.53594915635021 \n",
      "     Training Step: 104 Training Loss: 75.02592160212757 \n",
      "     Training Step: 105 Training Loss: 75.01630287891649 \n",
      "     Training Step: 106 Training Loss: 74.27216341500792 \n",
      "     Training Step: 107 Training Loss: 74.25910030679997 \n",
      "     Training Step: 108 Training Loss: 74.24400876613552 \n",
      "     Training Step: 109 Training Loss: 74.4974783910203 \n",
      "     Training Step: 110 Training Loss: 74.43964331955276 \n",
      "     Training Step: 111 Training Loss: 74.20494244664879 \n",
      "     Training Step: 112 Training Loss: 74.30801902007349 \n",
      "     Training Step: 113 Training Loss: 74.503067190769 \n",
      "     Training Step: 114 Training Loss: 74.30956847161194 \n",
      "     Training Step: 115 Training Loss: 74.33444543513451 \n",
      "     Training Step: 116 Training Loss: 74.46886757749098 \n",
      "     Training Step: 117 Training Loss: 74.65557354349252 \n",
      "     Training Step: 118 Training Loss: 74.37019210175129 \n",
      "     Training Step: 119 Training Loss: 74.24163955134858 \n",
      "     Training Step: 120 Training Loss: 74.46623656698281 \n",
      "     Training Step: 121 Training Loss: 74.32613293440122 \n",
      "     Training Step: 122 Training Loss: 74.64634080439177 \n",
      "     Training Step: 123 Training Loss: 74.3411558430325 \n",
      "     Training Step: 124 Training Loss: 74.45226760305411 \n",
      "     Training Step: 125 Training Loss: 74.69264890412501 \n",
      "     Training Step: 126 Training Loss: 74.49571640883592 \n",
      "     Training Step: 127 Training Loss: 74.48052102272769 \n",
      "     Training Step: 128 Training Loss: 74.53642162345596 \n",
      "     Training Step: 129 Training Loss: 74.22613222058676 \n",
      "     Training Step: 130 Training Loss: 74.63224252423124 \n",
      "     Training Step: 131 Training Loss: 74.28581408848152 \n",
      "     Training Step: 132 Training Loss: 74.36669606866346 \n",
      "     Training Step: 133 Training Loss: 74.37935383511194 \n",
      "     Training Step: 134 Training Loss: 74.6269517214263 \n",
      "     Training Step: 135 Training Loss: 74.63008379287677 \n",
      "     Training Step: 136 Training Loss: 74.21202765564782 \n",
      "     Training Step: 137 Training Loss: 74.47099818624632 \n",
      "     Training Step: 138 Training Loss: 74.14820466448603 \n",
      "     Training Step: 139 Training Loss: 74.50785229170835 \n",
      "     Training Step: 140 Training Loss: 74.60595436073399 \n",
      "     Training Step: 141 Training Loss: 74.49666746698153 \n",
      "     Training Step: 142 Training Loss: 74.40081754792826 \n",
      "     Training Step: 143 Training Loss: 74.5558894354727 \n",
      "     Training Step: 144 Training Loss: 74.3158523213323 \n",
      "     Training Step: 145 Training Loss: 74.3837710420096 \n",
      "     Training Step: 146 Training Loss: 74.31788576197968 \n",
      "     Training Step: 147 Training Loss: 74.15467887285291 \n",
      "     Training Step: 148 Training Loss: 74.34652652084361 \n",
      "     Training Step: 149 Training Loss: 74.43337212091848 \n",
      "     Training Step: 150 Training Loss: 74.5164570334521 \n",
      "     Training Step: 151 Training Loss: 74.6170810809401 \n",
      "     Training Step: 152 Training Loss: 74.87266279336252 \n",
      "     Training Step: 153 Training Loss: 74.69667331962218 \n",
      "     Training Step: 154 Training Loss: 74.59824268225599 \n",
      "     Training Step: 155 Training Loss: 74.77283038352408 \n",
      "     Training Step: 156 Training Loss: 74.38429086065696 \n",
      "     Training Step: 157 Training Loss: 74.42100562128599 \n",
      "     Training Step: 158 Training Loss: 74.36463443553251 \n",
      "     Training Step: 159 Training Loss: 74.37870858067582 \n",
      "     Training Step: 160 Training Loss: 74.29032574603285 \n",
      "     Training Step: 161 Training Loss: 74.20582808333563 \n",
      "     Training Step: 162 Training Loss: 74.39368859770748 \n",
      "     Training Step: 163 Training Loss: 74.60183971297273 \n",
      "     Training Step: 164 Training Loss: 75.13904940107605 \n",
      "     Training Step: 165 Training Loss: 74.35304741117483 \n",
      "     Training Step: 166 Training Loss: 74.25661784845667 \n",
      "     Training Step: 167 Training Loss: 74.40043078444522 \n",
      "     Training Step: 168 Training Loss: 74.1720878691971 \n",
      "     Training Step: 169 Training Loss: 74.47528893654744 \n",
      "     Training Step: 170 Training Loss: 74.4787824119317 \n",
      "     Training Step: 171 Training Loss: 74.22374364015668 \n",
      "     Training Step: 172 Training Loss: 74.60454606572489 \n",
      "     Training Step: 173 Training Loss: 74.46963932035624 \n",
      "     Training Step: 174 Training Loss: 74.40645726817286 \n",
      "     Training Step: 175 Training Loss: 75.13187233867009 \n",
      "     Training Step: 176 Training Loss: 74.5826932887472 \n",
      "     Training Step: 177 Training Loss: 74.65198229067772 \n",
      "     Training Step: 178 Training Loss: 74.84087073505991 \n",
      "     Training Step: 179 Training Loss: 74.73300074777532 \n",
      "     Training Step: 180 Training Loss: 74.7445979589786 \n",
      "     Training Step: 181 Training Loss: 74.409973576842 \n",
      "     Training Step: 182 Training Loss: 74.52230316578374 \n",
      "     Training Step: 183 Training Loss: 74.64090516108172 \n",
      "     Training Step: 184 Training Loss: 74.40153636642398 \n",
      "     Training Step: 185 Training Loss: 74.3852191485315 \n",
      "     Training Step: 186 Training Loss: 74.20841547024239 \n",
      "     Training Step: 187 Training Loss: 74.88525589510968 \n",
      "     Training Step: 188 Training Loss: 74.2937441000069 \n",
      "     Training Step: 189 Training Loss: 74.53445742854653 \n",
      "     Training Step: 190 Training Loss: 74.44159795428088 \n",
      "     Training Step: 191 Training Loss: 74.38580816320801 \n",
      "     Training Step: 192 Training Loss: 74.8562375044304 \n",
      "     Training Step: 193 Training Loss: 74.47130161521366 \n",
      "     Training Step: 194 Training Loss: 74.29646678128252 \n",
      "     Training Step: 195 Training Loss: 74.5101247853543 \n",
      "     Training Step: 196 Training Loss: 74.26582402599121 \n",
      "     Training Step: 197 Training Loss: 74.89898887731742 \n",
      "     Training Step: 198 Training Loss: 74.28311616510165 \n",
      "     Training Step: 199 Training Loss: 74.3383506278734 \n",
      "     Training Step: 200 Training Loss: 74.40849168482002 \n",
      "     Training Step: 201 Training Loss: 74.46472144117034 \n",
      "     Training Step: 202 Training Loss: 74.3724036440877 \n",
      "     Training Step: 203 Training Loss: 74.43940917951006 \n",
      "     Training Step: 204 Training Loss: 74.59013288570965 \n",
      "     Training Step: 205 Training Loss: 74.91948027669297 \n",
      "     Training Step: 206 Training Loss: 74.38802480650372 \n",
      "     Training Step: 207 Training Loss: 74.43156980334798 \n",
      "     Training Step: 208 Training Loss: 74.56098536626153 \n",
      "     Training Step: 209 Training Loss: 74.26331509267536 \n",
      "     Training Step: 210 Training Loss: 74.35314946918902 \n",
      "     Training Step: 211 Training Loss: 74.46913213407905 \n",
      "     Training Step: 212 Training Loss: 74.31765945377538 \n",
      "     Training Step: 213 Training Loss: 74.39007924649958 \n",
      "     Training Step: 214 Training Loss: 74.60040601491828 \n",
      "     Training Step: 215 Training Loss: 74.2733069481569 \n",
      "     Training Step: 216 Training Loss: 74.57177447076138 \n",
      "     Training Step: 217 Training Loss: 74.3400607627318 \n",
      "     Training Step: 218 Training Loss: 74.38710002959333 \n",
      "     Training Step: 219 Training Loss: 74.39993966125006 \n",
      "     Training Step: 220 Training Loss: 74.16698531640526 \n",
      "     Training Step: 221 Training Loss: 74.15202686681624 \n",
      "     Training Step: 222 Training Loss: 74.25781109233859 \n",
      "     Training Step: 223 Training Loss: 74.2882142491193 \n",
      "     Training Step: 224 Training Loss: 74.27345050918677 \n",
      "     Training Step: 225 Training Loss: 74.14547115976744 \n",
      "     Training Step: 226 Training Loss: 74.22202157694838 \n",
      "     Training Step: 227 Training Loss: 74.24238972517387 \n",
      "     Training Step: 228 Training Loss: 74.25932220083142 \n",
      "     Training Step: 229 Training Loss: 74.38220619247599 \n",
      "     Training Step: 230 Training Loss: 74.15652846687804 \n",
      "     Training Step: 231 Training Loss: 74.20319118014152 \n",
      "     Training Step: 232 Training Loss: 74.41692700965505 \n",
      "     Training Step: 233 Training Loss: 74.38958673171315 \n",
      "     Training Step: 234 Training Loss: 74.56414594862322 \n",
      "     Training Step: 235 Training Loss: 74.2721183440412 \n",
      "     Training Step: 236 Training Loss: 74.20266295496765 \n",
      "     Training Step: 237 Training Loss: 74.38421063895917 \n",
      "     Training Step: 238 Training Loss: 74.18632830348866 \n",
      "     Training Step: 239 Training Loss: 74.33213719392349 \n",
      "###########################################\n",
      "   VALIDATION\n",
      "     Validation Step: 0 Validation Loss: 74.21176233091852 \n",
      "     Validation Step: 1 Validation Loss: 74.31007341528094 \n",
      "     Validation Step: 2 Validation Loss: 74.56851281227446 \n",
      "     Validation Step: 3 Validation Loss: 74.21778184599792 \n",
      "     Validation Step: 4 Validation Loss: 74.25061698791386 \n",
      "     Validation Step: 5 Validation Loss: 74.24232427958052 \n",
      "     Validation Step: 6 Validation Loss: 74.26352061433981 \n",
      "     Validation Step: 7 Validation Loss: 74.34044795142012 \n",
      "     Validation Step: 8 Validation Loss: 74.21013723434915 \n",
      "     Validation Step: 9 Validation Loss: 74.34367603416082 \n",
      "     Validation Step: 10 Validation Loss: 74.27663925656724 \n",
      "     Validation Step: 11 Validation Loss: 74.39194563204184 \n",
      "     Validation Step: 12 Validation Loss: 74.29323319683529 \n",
      "     Validation Step: 13 Validation Loss: 74.48978076724788 \n",
      "     Validation Step: 14 Validation Loss: 74.20864742226905 \n",
      "     Validation Step: 15 Validation Loss: 74.13852017533604 \n",
      "     Validation Step: 16 Validation Loss: 74.14284685503146 \n",
      "     Validation Step: 17 Validation Loss: 74.2729176037865 \n",
      "     Validation Step: 18 Validation Loss: 74.21431394053354 \n",
      "     Validation Step: 19 Validation Loss: 74.14230838394178 \n",
      "     Validation Step: 20 Validation Loss: 74.16022991274887 \n",
      "     Validation Step: 21 Validation Loss: 74.20116600741575 \n",
      "     Validation Step: 22 Validation Loss: 74.323083936834 \n",
      "     Validation Step: 23 Validation Loss: 74.5862207057434 \n",
      "     Validation Step: 24 Validation Loss: 74.32993077994855 \n",
      "     Validation Step: 25 Validation Loss: 74.52349072160918 \n",
      "     Validation Step: 26 Validation Loss: 74.47945800442528 \n",
      "     Validation Step: 27 Validation Loss: 74.53283405191002 \n",
      "     Validation Step: 28 Validation Loss: 74.38563182880685 \n",
      "     Validation Step: 29 Validation Loss: 74.45343865815731 \n",
      "     Validation Step: 30 Validation Loss: 74.3025941520288 \n",
      "     Validation Step: 31 Validation Loss: 74.15744658813523 \n",
      "     Validation Step: 32 Validation Loss: 74.23942316212658 \n",
      "     Validation Step: 33 Validation Loss: 74.29831536498406 \n",
      "     Validation Step: 34 Validation Loss: 74.59521831029026 \n",
      "     Validation Step: 35 Validation Loss: 74.20960307333887 \n",
      "     Validation Step: 36 Validation Loss: 74.40279938755843 \n",
      "     Validation Step: 37 Validation Loss: 74.31747580696154 \n",
      "     Validation Step: 38 Validation Loss: 74.25495406377225 \n",
      "     Validation Step: 39 Validation Loss: 74.14048980643652 \n",
      "     Validation Step: 40 Validation Loss: 74.31895768720868 \n",
      "     Validation Step: 41 Validation Loss: 74.58023736477136 \n",
      "     Validation Step: 42 Validation Loss: 74.23964095416369 \n",
      "     Validation Step: 43 Validation Loss: 74.28014372486403 \n",
      "     Validation Step: 44 Validation Loss: 74.14017565633654 \n",
      "Epoch: 72\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 74.22203341941756 \n",
      "     Training Step: 1 Training Loss: 74.20932715406862 \n",
      "     Training Step: 2 Training Loss: 74.50230552075709 \n",
      "     Training Step: 3 Training Loss: 74.32289961015078 \n",
      "     Training Step: 4 Training Loss: 74.1725453067539 \n",
      "     Training Step: 5 Training Loss: 74.15457980498776 \n",
      "     Training Step: 6 Training Loss: 74.27980477052431 \n",
      "     Training Step: 7 Training Loss: 74.23573320075025 \n",
      "     Training Step: 8 Training Loss: 74.21609079347128 \n",
      "     Training Step: 9 Training Loss: 74.72439813007416 \n",
      "     Training Step: 10 Training Loss: 74.3486071061485 \n",
      "     Training Step: 11 Training Loss: 74.38748490823731 \n",
      "     Training Step: 12 Training Loss: 74.42186674295006 \n",
      "     Training Step: 13 Training Loss: 74.65373501610027 \n",
      "     Training Step: 14 Training Loss: 74.51374125740868 \n",
      "     Training Step: 15 Training Loss: 74.52504200165211 \n",
      "     Training Step: 16 Training Loss: 74.17874965995442 \n",
      "     Training Step: 17 Training Loss: 74.45400217869633 \n",
      "     Training Step: 18 Training Loss: 74.32375969342696 \n",
      "     Training Step: 19 Training Loss: 74.40524047215602 \n",
      "     Training Step: 20 Training Loss: 74.13251113278596 \n",
      "     Training Step: 21 Training Loss: 74.39727287585556 \n",
      "     Training Step: 22 Training Loss: 74.57146074530058 \n",
      "     Training Step: 23 Training Loss: 74.38927775826316 \n",
      "     Training Step: 24 Training Loss: 74.21396743520157 \n",
      "     Training Step: 25 Training Loss: 74.61209466107105 \n",
      "     Training Step: 26 Training Loss: 74.80466252748892 \n",
      "     Training Step: 27 Training Loss: 74.43046729132992 \n",
      "     Training Step: 28 Training Loss: 74.8060267115015 \n",
      "     Training Step: 29 Training Loss: 74.36059566334144 \n",
      "     Training Step: 30 Training Loss: 74.57436306231608 \n",
      "     Training Step: 31 Training Loss: 74.89636962741896 \n",
      "     Training Step: 32 Training Loss: 74.37858789877725 \n",
      "     Training Step: 33 Training Loss: 74.48511133450948 \n",
      "     Training Step: 34 Training Loss: 74.43908253550589 \n",
      "     Training Step: 35 Training Loss: 74.50923022790454 \n",
      "     Training Step: 36 Training Loss: 74.42458526972062 \n",
      "     Training Step: 37 Training Loss: 74.56196681790404 \n",
      "     Training Step: 38 Training Loss: 74.44077712572474 \n",
      "     Training Step: 39 Training Loss: 74.11203571674606 \n",
      "     Training Step: 40 Training Loss: 74.30249503247244 \n",
      "     Training Step: 41 Training Loss: 74.70092810992351 \n",
      "     Training Step: 42 Training Loss: 74.45930857484623 \n",
      "     Training Step: 43 Training Loss: 74.72732920024099 \n",
      "     Training Step: 44 Training Loss: 74.87320768329415 \n",
      "     Training Step: 45 Training Loss: 75.36403558761668 \n",
      "     Training Step: 46 Training Loss: 74.46507149786979 \n",
      "     Training Step: 47 Training Loss: 75.00438356438961 \n",
      "     Training Step: 48 Training Loss: 74.96626520111204 \n",
      "     Training Step: 49 Training Loss: 74.57124739347714 \n",
      "     Training Step: 50 Training Loss: 74.56941494063737 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_28700\\2146052178.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mnum_epochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m100\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mloss_weights\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;36m73\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;36m73\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mtrain_input\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mval_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_validate_ACOPF_chained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedder_model\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mACOPF_optimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mtrain_validate_ACOPF_chained\u001B[1;34m(minimizer_model, enforcer_model, embedder_model, ACOPF_optimizer, train_inputs, val_inputs, loss_weights, start_epoch, num_epochs, return_outputs, save_model)\u001B[0m\n\u001B[0;32m   2558\u001B[0m             \u001B[1;31m#physics_loss, mre_loss, unscaled_loss = self_supervised_embedder_obj_fn(res_bus, out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2559\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2560\u001B[1;33m             \u001B[0mphysics_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmre_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpenalty_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munsupervised_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself_supervised_hetero_obj_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mconstraint_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2561\u001B[0m             \u001B[1;31m# Forward pass with enforcer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2562\u001B[0m             \u001B[1;31m# out_dict = enforcer_model(out_dict, constraint_dict, edge_idx_dict, edge_attr_dict, scaler)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mself_supervised_hetero_obj_fn\u001B[1;34m(x_dict, out_dict, bus_idx_neighbors_dict, constraints_dict, scaler)\u001B[0m\n\u001B[0;32m   2196\u001B[0m                                   scaler: StandardScaler):\n\u001B[0;32m   2197\u001B[0m     \u001B[0munsupervised_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_unsupervised_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2198\u001B[1;33m     \u001B[0mphysics_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_physics_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2199\u001B[0m     \u001B[0mconstraint_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalc_constraint_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconstraints_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2200\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mcalc_physics_loss\u001B[1;34m(x_dict, out_dict, bus_idx_neighbors_dict, scaler)\u001B[0m\n\u001B[0;32m   3185\u001B[0m             \u001B[1;31m# Sums of all P_ij and Q_ij equal to P_i and Q_i respectively\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3186\u001B[0m             \u001B[0mP_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3187\u001B[1;33m             \u001B[0msum_P\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3188\u001B[0m             \u001B[0mP_physics\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msum_P\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3189\u001B[0m             \u001B[0mP_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_i\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mformat_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mformat_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mextract_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    198\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_back\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m     \u001B[0mstack\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mStackSummary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwalk_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlimit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m     \u001B[0mstack\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreverse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mstack\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\traceback.py\u001B[0m in \u001B[0;36mextract\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    357\u001B[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001B[0;32m    358\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mfilename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfnames\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 359\u001B[1;33m             \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckcache\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    360\u001B[0m         \u001B[1;31m# If immediate lookup was desired, trigger lookups now.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlookup_lines\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\IPython\\core\\compilerop.py\u001B[0m in \u001B[0;36mcheck_linecache_ipython\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \"\"\"\n\u001B[0;32m    184\u001B[0m     \u001B[1;31m# First call the original checkcache as intended\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 185\u001B[1;33m     \u001B[0mlinecache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_checkcache_ori\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    186\u001B[0m     \u001B[1;31m# Then, update back the cache with our data, so that tracebacks related\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m     \u001B[1;31m# to our compiled codes can be produced.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\linecache.py\u001B[0m in \u001B[0;36mcheckcache\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mcontinue\u001B[0m   \u001B[1;31m# no-op for files loaded via a __loader__\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m             \u001B[0mstat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfullname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mcache\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 71\n",
    "num_epochs = 100\n",
    "loss_weights = (1/73, 10/73, 1.0)\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF_chained(None,None, embedder_model,ACOPF_optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{'SB': tensor([[ 6.0855, -1.1536,  6.4379, -4.8384]]),\n 'PQ': tensor([[-0.2824,  1.3603,  0.1809,  0.1686],\n         [-0.1250,  0.1585,  0.1808,  0.1706],\n         [-0.1259,  0.1522,  0.2675,  0.2229],\n         [-0.1347,  0.1003,  0.8072,  0.5386],\n         [ 0.0017,  0.9427,  0.1777,  0.1698],\n         [-0.2852,  1.0777,  0.1790,  0.1704],\n         [-0.0672,  0.5336,  0.7829,  0.5605],\n         [-0.1090,  1.2983,  0.1582,  0.1465],\n         [-0.1837,  0.5072,  0.1779,  0.1705],\n         [-0.1263,  0.1656,  0.1798,  0.1688],\n         [-0.2874,  1.0945,  0.1818,  0.1696],\n         [-0.1101,  0.5598,  0.3081,  0.2222],\n         [-0.3022,  1.3244,  0.3070,  0.2200],\n         [-0.2846,  1.0818,  0.1790,  0.1714],\n         [-0.1251,  0.1578,  0.1781,  0.1715],\n         [-0.1140,  1.3667,  0.1820,  0.1702]]),\n 'PV': tensor([[ 4.5760e+00, -1.8209e-01,  5.3566e+00,  4.0813e+00],\n         [ 1.5967e+00,  6.8131e-01,  3.4700e+00,  6.6232e+00],\n         [-3.2277e-01,  1.2543e+00,  8.1089e-02,  1.1234e-01],\n         [-3.0086e-01,  1.3178e+00, -1.8759e-01, -4.3205e-02],\n         [-2.6189e-01,  1.4150e+00,  1.2465e-02,  7.1887e-02],\n         [-3.1611e-01,  1.2356e+00, -2.5183e+00, -1.4000e+00],\n         [-1.5332e-01,  4.3070e-01, -1.0926e-01, -1.2705e-04],\n         [ 3.6277e-03,  9.5690e-01, -2.0944e+00, -1.1669e+00],\n         [-5.8869e-02,  5.8428e-01, -4.6212e-01, -2.0310e-01],\n         [-8.8145e-02,  1.5568e+00, -1.6314e-01, -2.9545e-02],\n         [-2.1968e-01,  8.2148e-01, -2.5475e-01, -8.0533e-02],\n         [-2.7909e-01,  1.1051e+00, -2.6458e-01, -8.8273e-02],\n         [-2.7170e-01,  1.5355e+00, -2.0541e+00, -1.1300e+00],\n         [-2.3369e-01,  1.0595e+00, -1.3267e-01, -3.8659e-03],\n         [-2.4051e-01,  1.0142e+00,  7.7394e-03,  6.9893e-02],\n         [-1.4984e-02,  8.4820e-01, -6.6040e-01, -3.3222e-01],\n         [-3.1083e-01,  1.2662e+00,  4.1089e-02,  8.7559e-02],\n         [-1.0966e-01,  5.6269e-01, -1.2325e-01, -5.7864e-03],\n         [-9.7389e-02,  9.2813e-01, -2.0001e-01, -5.0996e-02],\n         [-2.8139e-01,  1.5369e+00, -7.6335e-01, -3.7457e-01],\n         [-3.1854e-01,  1.2825e+00, -4.3700e-01, -1.8769e-01],\n         [-1.5563e-01,  4.1641e-01, -2.0977e-02,  5.3182e-02],\n         [-2.8172e-01,  1.3650e+00,  2.1658e-04,  6.6407e-02],\n         [-1.3296e-01,  1.0451e-01, -1.0052e-01,  4.1680e-03],\n         [-1.2493e-01,  1.5851e-01,  8.7588e-02,  1.1635e-01],\n         [-3.1687e-01,  1.2928e+00,  7.6920e-02,  1.1133e-01],\n         [-2.6274e-01,  1.4095e+00,  3.3414e-02,  8.3426e-02],\n         [-2.9786e-01,  1.3594e+00, -4.8350e-01, -2.1657e-01],\n         [-6.0051e-02,  1.7287e+00, -8.0927e-01, -4.0552e-01],\n         [-1.1529e-01,  1.3878e+00,  7.0184e-02,  1.0348e-01],\n         [-9.8535e-02,  1.4924e+00,  6.4351e-02,  1.0301e-01],\n         [-9.7065e-02,  9.5684e-01, -8.8611e-02,  1.4018e-02],\n         [-9.7943e-02,  1.1538e+00, -2.4139e-01, -6.8684e-02],\n         [-3.2260e-01,  1.2555e+00,  1.8408e-02,  7.8160e-02],\n         [-2.4702e-01,  1.4397e+00, -7.8002e-01, -3.8842e-01],\n         [-1.2456e-01,  1.6082e-01, -1.8670e-02,  5.4901e-02],\n         [-2.5707e-01,  1.4210e+00,  2.9690e-02,  8.2576e-02],\n         [-9.7706e-02,  1.4970e+00,  6.8150e-02,  1.0380e-01],\n         [-9.7567e-02,  1.1561e+00, -1.2043e-01, -4.0874e-03],\n         [-1.0221e-01,  7.6760e-01, -3.1353e-02,  4.5576e-02],\n         [-3.0026e-01,  1.3411e+00, -2.2942e-01, -6.6746e-02],\n         [-3.0325e-01,  1.3889e+00, -5.8324e-02,  3.0973e-02]]),\n 'NB': tensor([[-0.2494,  0.9562,  0.1240,  0.1376],\n         [-0.1844,  0.5117,  0.1240,  0.1376],\n         [-0.2873,  1.0942,  0.1240,  0.1376],\n         [-0.2064,  0.6600,  0.1240,  0.1376],\n         [-0.0150,  0.8481,  0.1240,  0.1376]])}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.9911837643012404 -0.3235414130613208\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 9.00222216e-01,  2.40622153e+00,  2.69094801e+00,\n         3.01999736e+00],\n       [ 8.96686674e-01,  0.00000000e+00, -9.97504425e+00,\n        -2.06655750e+01],\n       [ 9.00363367e-01,  7.30876468e-01,  1.13161886e+00,\n         3.10024142e+00],\n       [ 8.55736819e-01,  8.60785618e+00, -1.66952229e+01,\n         2.04573975e+01],\n       [ 9.00166460e-01,  1.26019844e+01,  2.03076792e+00,\n        -6.20020032e-01],\n       [-1.30996609e-03,  7.43599930e+00,  4.84387445e+00,\n         1.92635429e+00],\n       [ 2.18409706e-04,  9.25921192e+00, -2.39696884e+00,\n         1.34807909e+00],\n       [ 9.96492767e-01,  8.42991867e+00,  1.36777818e-01,\n        -1.25091469e+00],\n       [ 8.99193365e-01,  6.23245706e+00, -1.61245072e+00,\n        -3.52727056e-01],\n       [ 1.09970953e+00, -6.27187977e+00,  9.04278755e+00,\n         1.76663876e+01],\n       [ 6.74170484e-05,  5.62166061e+00,  3.37265444e+00,\n        -3.86806764e-02],\n       [ 2.40513276e-03,  1.00536655e+01, -1.47776442e+01,\n        -7.09674931e+00],\n       [ 1.39897444e-02,  9.55715599e+00, -2.86432381e+01,\n        -1.39734488e+01],\n       [ 3.48268070e-03,  1.45565962e+01, -1.95074696e-02,\n        -1.29492843e+00],\n       [ 5.39557825e-03,  5.33703270e+00,  4.30462313e+00,\n        -1.54034958e+01],\n       [-3.49910828e-04,  9.32149925e+00,  3.40052147e+01,\n         6.34078312e+00],\n       [ 2.63325166e-03,  7.56372156e+00, -7.71955967e+00,\n        -1.41014218e-01],\n       [ 9.67663960e-03,  1.92690491e+01, -1.52345676e+01,\n        -7.33686304e+00],\n       [ 8.98500685e-01,  7.40155878e+00, -1.42098486e-01,\n        -1.24789202e+00],\n       [ 8.97961703e-01,  6.83863010e+00,  2.12125164e-02,\n        -1.35541379e+00],\n       [ 8.86009563e-01,  1.06273083e+01,  1.07470551e+01,\n         1.50097597e+00],\n       [ 8.97322845e-01,  3.73289718e+00,  8.93507290e+00,\n        -6.28697300e+00],\n       [ 8.99090299e-01, -1.68690298e+00, -1.73671973e+00,\n         4.38149548e+00],\n       [ 8.94454887e-01,  5.27878132e+00, -1.48408594e+01,\n        -8.53286648e+00],\n       [ 8.31909288e-03,  6.06275835e+00,  3.34263182e+00,\n         6.69901252e-01],\n       [ 3.96885265e-02,  3.62433925e+00,  1.61857166e+01,\n        -6.31528616e+00],\n       [ 1.10178438e+00,  1.04773163e+01,  3.65960655e+01,\n         1.35938005e+01],\n       [ 1.69969133e-03,  9.40643444e+00, -1.44357109e+01,\n        -7.43677711e+00],\n       [ 1.09912574e+00,  4.80916824e+00, -7.06980288e-01,\n         7.24088073e-01],\n       [-5.40298901e-04,  9.34964202e-01,  8.35287035e-01,\n        -1.31274998e+00],\n       [ 1.09995859e+00,  8.56333389e+00,  5.70626783e+00,\n        -1.19800675e+00],\n       [-2.06547569e-04,  3.70754423e+00,  3.28965902e+00,\n        -1.30587844e-02],\n       [ 7.76376204e-02,  1.09173216e+01,  3.84292269e+00,\n         4.14677238e+00],\n       [ 8.09850866e-01,  2.12075510e+00,  7.58594942e+00,\n         1.40174663e+00],\n       [ 3.43822214e-03,  2.19403019e+00,  3.06244040e+00,\n         1.11395299e+00],\n       [-2.72209780e-03,  6.35774984e+00, -1.89793968e+01,\n        -7.43811560e+00],\n       [ 8.96221369e-01,  5.04650249e+00,  1.22933083e+01,\n         1.27202444e+01],\n       [ 8.96753762e-01, -5.04358921e+00, -1.38897724e+01,\n        -7.34211063e+00],\n       [ 8.93696109e-01,  4.54456844e+00, -1.84013140e+00,\n        -4.20385636e-02],\n       [ 4.18299843e-04,  5.53573837e+00, -1.09918225e+00,\n         9.67723727e-01],\n       [ 8.75814681e-01,  7.11991539e+00, -1.66805267e+01,\n        -7.20818806e+00],\n       [-4.12194431e-04,  3.72061028e+00, -9.43287432e-01,\n         2.79895949e+00],\n       [ 3.59864750e-03,  8.37809124e+00, -1.40117514e+00,\n         6.30856395e-01],\n       [ 1.87475979e-04,  1.20334305e+01, -3.61367285e-01,\n         1.30194068e-01],\n       [ 1.87662393e-03,  7.22458401e+00, -1.06471074e+00,\n        -1.29137099e+00],\n       [ 8.97726440e-01,  7.79869404e+00, -6.46073532e+00,\n        -1.60302961e+00],\n       [ 8.98026137e-01,  5.60726251e+00,  1.68086946e-01,\n        -2.41164804e-01],\n       [ 8.92267955e-01, -2.78472672e+00,  2.32358322e+01,\n         2.06533413e+01],\n       [ 1.18852190e-03,  9.98612919e+00, -1.07855380e-01,\n        -1.60115361e-01],\n       [ 9.00077057e-01,  9.75714435e+00,  3.68839025e+00,\n         1.46999204e+00],\n       [ 1.80650286e-03,  7.41648474e+00,  5.95515299e+00,\n         6.45271301e+00],\n       [ 1.10258761e+00,  4.56929674e+00,  5.52040625e+00,\n         1.28286352e+01],\n       [ 8.93471042e-01,  9.25508442e+00,  1.21340215e-01,\n        -1.22821105e+00],\n       [-2.80968845e-04,  5.38571920e+00, -1.44788437e+01,\n        -7.35318232e+00],\n       [ 8.75745253e-01,  4.33067694e+00,  3.46180379e-01,\n         6.92659259e-01],\n       [ 7.16470453e-04,  9.08708896e+00, -3.90095568e+00,\n        -1.39248788e+00],\n       [ 8.93031242e-01,  8.01899662e+00,  4.01338995e-01,\n        -7.17878461e-01],\n       [ 8.96905726e-01,  9.60958996e+00, -2.83393669e+00,\n         3.63443732e-01],\n       [-1.59869546e-04,  6.60360422e+00, -9.26953852e-01,\n        -3.65674138e-01],\n       [ 8.99199746e-01,  2.39130130e+00, -2.32652092e+00,\n        -1.15576863e-01],\n       [ 8.63390975e-01,  6.06484165e+00, -1.17645888e-02,\n        -4.11413312e-01],\n       [ 1.10145881e+00,  3.31833832e-01, -1.18152201e-01,\n        -2.10612416e-01],\n       [ 1.09870827e+00,  5.08014717e+00,  7.68341422e-02,\n        -1.29272878e+00],\n       [ 1.10130560e+00,  7.21022310e+00, -1.45761073e-01,\n        -1.13693774e+00]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(train_output.output[:, 2]), sum(train_output.output[:, 3]))\n",
    "train_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.896733283996582 12.633705005049706\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 8.06798754e-01, -1.42894740e+01, -1.03776050e+00,\n         1.41678886e+01],\n       [ 4.46618291e-01,  0.00000000e+00, -3.41364479e+01,\n         6.96447182e+00],\n       [ 7.41219607e-01, -1.18690081e+01,  1.89277191e+01,\n         5.89282417e+00],\n       [ 1.50205480e+00, -1.04631300e+01, -2.62057076e+01,\n        -1.86600399e+01],\n       [ 8.27263572e-01, -1.39602194e+01, -1.23557272e+01,\n        -1.65907478e+00],\n       [ 5.83789271e-01, -3.90639019e+00, -1.67516251e+01,\n         7.39064980e+00],\n       [ 1.33020352e+00, -7.40627384e+00,  1.75678158e+01,\n         9.88894761e-01],\n       [ 6.79547397e-01, -2.22582407e+01,  1.12625475e+01,\n        -1.31573534e+00],\n       [ 1.36622314e+00, -1.05890217e+01,  1.76956964e+00,\n         3.17715335e+00],\n       [ 9.39841739e-01, -6.10090923e+00, -2.74808674e+01,\n        -1.28234501e+01],\n       [ 1.19153914e+00, -1.38467674e+01,  2.39529347e+00,\n        -2.41918755e+00],\n       [ 9.74185597e-01, -1.71603322e+01,  2.11971951e+01,\n        -3.27558804e+00],\n       [ 5.96549294e-01,  1.37254524e+00, -4.56995726e+00,\n         7.82400894e+00],\n       [ 9.93877896e-01, -1.96447625e+01,  8.84755230e+00,\n        -1.10131063e+01],\n       [ 1.59328086e+00, -1.93684787e+01,  9.69903469e+00,\n         2.12106511e-01],\n       [ 1.20735363e+00, -4.93679047e-02,  5.96382618e+00,\n         1.21920319e+01],\n       [ 1.48228690e+00, -2.10794067e+00, -1.25532942e+01,\n        -2.68372715e-01],\n       [ 7.46940266e-01, -1.00474911e+01,  7.69040012e+00,\n        -3.84581780e+00],\n       [ 1.73637792e+00, -7.71184921e-01, -2.43640041e+01,\n        -5.61723852e+00],\n       [ 9.18439484e-01, -5.59188175e+00,  2.42491269e+00,\n         3.34797931e+00],\n       [ 6.63518524e-01, -2.03751945e+00,  1.35649958e+01,\n        -1.72645187e+00],\n       [ 1.81687178e+00, -6.77048302e+00,  1.00303164e+01,\n         1.71301174e+01],\n       [ 1.39102894e+00, -1.73678619e+01, -1.90362988e+01,\n         1.92844129e+00],\n       [ 7.73622062e-01,  2.56216621e+00, -7.21583319e+00,\n         1.05083680e+00],\n       [ 6.91667106e-01,  2.97616005e+00, -1.15453119e+01,\n        -3.55488539e+00],\n       [ 4.47790181e-01, -1.77411560e+01, -3.45030546e-01,\n         7.94569874e+00],\n       [ 3.60036052e-01, -1.15947857e+01,  4.76233959e+00,\n        -2.03962688e+01],\n       [ 1.33709523e+00, -2.97428131e-01, -1.05584888e+01,\n         1.42675614e+00],\n       [ 7.55966949e-01, -1.21789384e+01,  2.34333348e+00,\n         9.75261211e+00],\n       [ 1.52224607e+00, -6.74583626e+00,  3.57239175e+00,\n        -8.82819355e-01],\n       [ 1.47871246e+00, -5.62342739e+00, -1.48865404e+01,\n        -2.06946049e+01],\n       [ 8.43133406e-01, -1.45284414e+01,  1.10927229e+01,\n        -3.00125670e+00],\n       [ 1.09287935e+00, -1.07855148e+01, -1.28741465e+01,\n        -5.44153166e+00],\n       [ 1.06122964e+00,  3.74462318e+00, -2.56821518e+01,\n        -2.05483603e+00],\n       [ 1.75828122e+00, -5.85396004e+00,  9.03721905e+00,\n         9.28898621e+00],\n       [ 7.04962574e-01,  8.51726532e-01,  2.05169892e+00,\n         4.82943535e+00],\n       [ 7.17683896e-01, -6.78821182e+00,  1.60036945e+01,\n        -1.26063662e+01],\n       [ 8.60095978e-01, -4.02278805e+00,  1.14069986e+01,\n         2.76839137e+00],\n       [ 2.77153154e-01, -1.21108093e+01,  9.77076817e+00,\n        -4.55856419e+00],\n       [ 7.66768161e-01, -2.04899491e+01,  2.55816002e+01,\n         2.99821198e-01],\n       [ 8.88230410e-01, -9.52615738e-01,  1.09645090e+01,\n         1.45440578e+01],\n       [ 5.84682465e-01, -1.71395602e+01,  1.62291679e+01,\n         7.67673683e+00],\n       [ 4.77353183e-01,  3.29742432e-02, -1.90079665e+00,\n         2.84947085e+00],\n       [ 8.97803012e-01, -1.26408768e+01,  8.45917702e+00,\n         8.96438408e+00],\n       [ 1.81859602e+00, -3.53079987e+00,  5.14611244e+00,\n        -8.16450298e-01],\n       [ 4.20003856e-01, -1.89721069e+01, -2.27098732e+01,\n        -1.02893925e+01],\n       [ 5.32820858e-01, -1.54360383e+01,  1.46637869e+01,\n        -1.30472975e+01],\n       [ 1.16610218e+00,  4.06052208e+00, -2.77419739e+01,\n        -1.33549032e+01],\n       [ 7.40678822e-01, -6.14553928e+00, -4.67558289e+00,\n         2.84938121e+00],\n       [ 6.74709944e-01,  1.65020180e+00, -2.75255756e+01,\n         1.00261154e+01],\n       [ 1.24004822e+00,  3.46774292e+00, -2.76129856e+01,\n        -1.38943024e+01],\n       [ 1.00040554e+00, -1.53938487e+01, -8.40883923e+00,\n         7.27598286e+00],\n       [ 1.01005242e+00, -1.61153710e+01,  4.96469307e+00,\n         2.08186030e+00],\n       [ 5.54946241e-01, -1.66544280e+01,  2.37828598e+01,\n        -3.89743018e+00],\n       [ 1.01588260e+00, -1.04922676e+01,  2.94753532e+01,\n         2.32118344e+00],\n       [ 7.62686851e-01, -1.82306129e+01,  2.11208305e+01,\n        -3.24028945e+00],\n       [ 1.04360990e+00, -1.84315795e+01,  5.00079918e+00,\n         9.25844288e+00],\n       [ 1.21475483e+00, -1.28378787e+01, -1.07481265e+00,\n         5.16030788e+00],\n       [ 1.15744830e+00, -1.70252147e+01,  1.05319090e+01,\n         2.20173621e+00],\n       [ 8.93298270e-01, -8.61983776e+00,  5.47727966e+00,\n        -3.26602197e+00],\n       [ 1.11620081e+00, -1.35442185e+01, -3.75881505e+00,\n        -4.25409269e+00],\n       [ 4.28109602e-01, -8.67969418e+00, -1.69979501e+00,\n         1.53286762e+01],\n       [ 8.92825456e-01, -5.12719631e+00,  1.43590951e+00,\n        -1.77837968e+00],\n       [ 1.58431549e+00, -4.66905594e-01, -4.04823542e-01,\n         7.17001915e+00]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(val_output.output[:, 2]), sum(val_output.output[:, 3]))\n",
    "val_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and Validate Model\n",
    "start_epoch = 10\n",
    "num_epochs = 20\n",
    "loss_weights = (0.33, 0.33, 0.33)\n",
    "train_input,train_output,val_input, val_output = train_validate_ACOPF_chained(None,None, embedder_model,ACOPF_optimizer, train_inputs, val_inputs,loss_weights, start_epoch, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(train_output.output[:, 2]), sum(train_output.output[:, 3]))\n",
    "train_output.output\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(val_output.output[:, 2]), sum(val_output.output[:, 3]))\n",
    "val_output.output"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
