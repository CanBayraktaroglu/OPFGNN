{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric.nn.norm.batch_norm\n",
    "import wandb\n",
    "\n",
    "from helper import *\n",
    "from gnn import GNN\n",
    "from heterognn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The State of the nth node is expressed by 4 real scalars:\n",
    "\n",
    "v_n -> the voltage at the node\n",
    "delta_n -> the voltage angle at the node (relative to the slack bus)\n",
    "p_n -> the net active power flowing into the node\n",
    "q_n -> the net reactive power flowing into the node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical characteristics of the network are described by the power flow equations:\n",
    "\n",
    "p = P(v, delta, W)\n",
    "q = Q(v, delta, W)\n",
    "\n",
    "-> Relate local net power generation with the global state\n",
    "-> Depends on the topology W of the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electrical grid => Weighted Graph\n",
    "\n",
    "Nodes in the graph produce/consume power\n",
    "\n",
    "Edges represent electrical connections between nodes\n",
    "\n",
    "State Matrix X element of  R(N x 4) => graph signal with 4 features\n",
    "    => Each row is the state of the corresponding Node\n",
    "\n",
    "Adjacency Matrix A => sparse matrix to represent the connections of each node, element of R(N x N), Aij = 1 if node i is connected to node j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the GNN as a mode phi(X, A, H)\n",
    "\n",
    "We want to imitate the OPF solution p*\n",
    "-> We want to minimize a loss L over a dataset T = {{X, p*}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Function:min arg H of sum over T L(p*,phi(X, A, H)) and we use L = Mean Squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once GNN model phi is trained, we do not need the costly p* from pandapower to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input Data X - R^(Nx4): Uniformly sample p_ref and q_ref of each load L with P_L ~ Uniform(0.9 * p_ref, 1.1 * p_ref) and Q_L ~ Uniform(0.9 * q_ref, 1.1 * q_ref)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pseudocode for X and y in supervised learning:\n",
    "for each P_L and Q_L:\n",
    "    Create X with sub-optimal DCOPF results\n",
    "    Create y with Pandapower calculating p* ACOPF with IPOPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatio-Temporal GNN -> superposition of a gnn with spatial info and a temporal layer (Temporal Conv,LSTM etc.) ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bus => Node in GNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['1-complete_data-mixed-all-0-sw',\n '1-complete_data-mixed-all-1-sw',\n '1-complete_data-mixed-all-2-sw',\n '1-EHVHVMVLV-mixed-all-0-sw',\n '1-EHVHVMVLV-mixed-all-1-sw',\n '1-EHVHVMVLV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-0-sw',\n '1-EHVHV-mixed-all-0-no_sw',\n '1-EHVHV-mixed-all-1-sw',\n '1-EHVHV-mixed-all-1-no_sw',\n '1-EHVHV-mixed-all-2-sw',\n '1-EHVHV-mixed-all-2-no_sw',\n '1-EHVHV-mixed-1-0-sw',\n '1-EHVHV-mixed-1-0-no_sw',\n '1-EHVHV-mixed-1-1-sw',\n '1-EHVHV-mixed-1-1-no_sw',\n '1-EHVHV-mixed-1-2-sw',\n '1-EHVHV-mixed-1-2-no_sw',\n '1-EHVHV-mixed-2-0-sw',\n '1-EHVHV-mixed-2-0-no_sw',\n '1-EHVHV-mixed-2-1-sw',\n '1-EHVHV-mixed-2-1-no_sw',\n '1-EHVHV-mixed-2-2-sw',\n '1-EHVHV-mixed-2-2-no_sw',\n '1-EHV-mixed--0-sw',\n '1-EHV-mixed--0-no_sw',\n '1-EHV-mixed--1-sw',\n '1-EHV-mixed--1-no_sw',\n '1-EHV-mixed--2-sw',\n '1-EHV-mixed--2-no_sw',\n '1-HVMV-mixed-all-0-sw',\n '1-HVMV-mixed-all-0-no_sw',\n '1-HVMV-mixed-all-1-sw',\n '1-HVMV-mixed-all-1-no_sw',\n '1-HVMV-mixed-all-2-sw',\n '1-HVMV-mixed-all-2-no_sw',\n '1-HVMV-mixed-1.105-0-sw',\n '1-HVMV-mixed-1.105-0-no_sw',\n '1-HVMV-mixed-1.105-1-sw',\n '1-HVMV-mixed-1.105-1-no_sw',\n '1-HVMV-mixed-1.105-2-sw',\n '1-HVMV-mixed-1.105-2-no_sw',\n '1-HVMV-mixed-2.102-0-sw',\n '1-HVMV-mixed-2.102-0-no_sw',\n '1-HVMV-mixed-2.102-1-sw',\n '1-HVMV-mixed-2.102-1-no_sw',\n '1-HVMV-mixed-2.102-2-sw',\n '1-HVMV-mixed-2.102-2-no_sw',\n '1-HVMV-mixed-4.101-0-sw',\n '1-HVMV-mixed-4.101-0-no_sw',\n '1-HVMV-mixed-4.101-1-sw',\n '1-HVMV-mixed-4.101-1-no_sw',\n '1-HVMV-mixed-4.101-2-sw',\n '1-HVMV-mixed-4.101-2-no_sw',\n '1-HVMV-urban-all-0-sw',\n '1-HVMV-urban-all-0-no_sw',\n '1-HVMV-urban-all-1-sw',\n '1-HVMV-urban-all-1-no_sw',\n '1-HVMV-urban-all-2-sw',\n '1-HVMV-urban-all-2-no_sw',\n '1-HVMV-urban-2.203-0-sw',\n '1-HVMV-urban-2.203-0-no_sw',\n '1-HVMV-urban-2.203-1-sw',\n '1-HVMV-urban-2.203-1-no_sw',\n '1-HVMV-urban-2.203-2-sw',\n '1-HVMV-urban-2.203-2-no_sw',\n '1-HVMV-urban-3.201-0-sw',\n '1-HVMV-urban-3.201-0-no_sw',\n '1-HVMV-urban-3.201-1-sw',\n '1-HVMV-urban-3.201-1-no_sw',\n '1-HVMV-urban-3.201-2-sw',\n '1-HVMV-urban-3.201-2-no_sw',\n '1-HVMV-urban-4.201-0-sw',\n '1-HVMV-urban-4.201-0-no_sw',\n '1-HVMV-urban-4.201-1-sw',\n '1-HVMV-urban-4.201-1-no_sw',\n '1-HVMV-urban-4.201-2-sw',\n '1-HVMV-urban-4.201-2-no_sw',\n '1-HV-mixed--0-sw',\n '1-HV-mixed--0-no_sw',\n '1-HV-mixed--1-sw',\n '1-HV-mixed--1-no_sw',\n '1-HV-mixed--2-sw',\n '1-HV-mixed--2-no_sw',\n '1-HV-urban--0-sw',\n '1-HV-urban--0-no_sw',\n '1-HV-urban--1-sw',\n '1-HV-urban--1-no_sw',\n '1-HV-urban--2-sw',\n '1-HV-urban--2-no_sw',\n '1-MVLV-rural-all-0-sw',\n '1-MVLV-rural-all-0-no_sw',\n '1-MVLV-rural-all-1-sw',\n '1-MVLV-rural-all-1-no_sw',\n '1-MVLV-rural-all-2-sw',\n '1-MVLV-rural-all-2-no_sw',\n '1-MVLV-rural-1.108-0-sw',\n '1-MVLV-rural-1.108-0-no_sw',\n '1-MVLV-rural-1.108-1-sw',\n '1-MVLV-rural-1.108-1-no_sw',\n '1-MVLV-rural-1.108-2-sw',\n '1-MVLV-rural-1.108-2-no_sw',\n '1-MVLV-rural-2.107-0-sw',\n '1-MVLV-rural-2.107-0-no_sw',\n '1-MVLV-rural-2.107-1-sw',\n '1-MVLV-rural-2.107-1-no_sw',\n '1-MVLV-rural-2.107-2-sw',\n '1-MVLV-rural-2.107-2-no_sw',\n '1-MVLV-rural-4.101-0-sw',\n '1-MVLV-rural-4.101-0-no_sw',\n '1-MVLV-rural-4.101-1-sw',\n '1-MVLV-rural-4.101-1-no_sw',\n '1-MVLV-rural-4.101-2-sw',\n '1-MVLV-rural-4.101-2-no_sw',\n '1-MVLV-semiurb-all-0-sw',\n '1-MVLV-semiurb-all-0-no_sw',\n '1-MVLV-semiurb-all-1-sw',\n '1-MVLV-semiurb-all-1-no_sw',\n '1-MVLV-semiurb-all-2-sw',\n '1-MVLV-semiurb-all-2-no_sw',\n '1-MVLV-semiurb-3.202-0-sw',\n '1-MVLV-semiurb-3.202-0-no_sw',\n '1-MVLV-semiurb-3.202-1-sw',\n '1-MVLV-semiurb-3.202-1-no_sw',\n '1-MVLV-semiurb-3.202-2-sw',\n '1-MVLV-semiurb-3.202-2-no_sw',\n '1-MVLV-semiurb-4.201-0-sw',\n '1-MVLV-semiurb-4.201-0-no_sw',\n '1-MVLV-semiurb-4.201-1-sw',\n '1-MVLV-semiurb-4.201-1-no_sw',\n '1-MVLV-semiurb-4.201-2-sw',\n '1-MVLV-semiurb-4.201-2-no_sw',\n '1-MVLV-semiurb-5.220-0-sw',\n '1-MVLV-semiurb-5.220-0-no_sw',\n '1-MVLV-semiurb-5.220-1-sw',\n '1-MVLV-semiurb-5.220-1-no_sw',\n '1-MVLV-semiurb-5.220-2-sw',\n '1-MVLV-semiurb-5.220-2-no_sw',\n '1-MVLV-urban-all-0-sw',\n '1-MVLV-urban-all-0-no_sw',\n '1-MVLV-urban-all-1-sw',\n '1-MVLV-urban-all-1-no_sw',\n '1-MVLV-urban-all-2-sw',\n '1-MVLV-urban-all-2-no_sw',\n '1-MVLV-urban-5.303-0-sw',\n '1-MVLV-urban-5.303-0-no_sw',\n '1-MVLV-urban-5.303-1-sw',\n '1-MVLV-urban-5.303-1-no_sw',\n '1-MVLV-urban-5.303-2-sw',\n '1-MVLV-urban-5.303-2-no_sw',\n '1-MVLV-urban-6.305-0-sw',\n '1-MVLV-urban-6.305-0-no_sw',\n '1-MVLV-urban-6.305-1-sw',\n '1-MVLV-urban-6.305-1-no_sw',\n '1-MVLV-urban-6.305-2-sw',\n '1-MVLV-urban-6.305-2-no_sw',\n '1-MVLV-urban-6.309-0-sw',\n '1-MVLV-urban-6.309-0-no_sw',\n '1-MVLV-urban-6.309-1-sw',\n '1-MVLV-urban-6.309-1-no_sw',\n '1-MVLV-urban-6.309-2-sw',\n '1-MVLV-urban-6.309-2-no_sw',\n '1-MVLV-comm-all-0-sw',\n '1-MVLV-comm-all-0-no_sw',\n '1-MVLV-comm-all-1-sw',\n '1-MVLV-comm-all-1-no_sw',\n '1-MVLV-comm-all-2-sw',\n '1-MVLV-comm-all-2-no_sw',\n '1-MVLV-comm-3.403-0-sw',\n '1-MVLV-comm-3.403-0-no_sw',\n '1-MVLV-comm-3.403-1-sw',\n '1-MVLV-comm-3.403-1-no_sw',\n '1-MVLV-comm-3.403-2-sw',\n '1-MVLV-comm-3.403-2-no_sw',\n '1-MVLV-comm-4.416-0-sw',\n '1-MVLV-comm-4.416-0-no_sw',\n '1-MVLV-comm-4.416-1-sw',\n '1-MVLV-comm-4.416-1-no_sw',\n '1-MVLV-comm-4.416-2-sw',\n '1-MVLV-comm-4.416-2-no_sw',\n '1-MVLV-comm-5.401-0-sw',\n '1-MVLV-comm-5.401-0-no_sw',\n '1-MVLV-comm-5.401-1-sw',\n '1-MVLV-comm-5.401-1-no_sw',\n '1-MVLV-comm-5.401-2-sw',\n '1-MVLV-comm-5.401-2-no_sw',\n '1-MV-rural--0-sw',\n '1-MV-rural--0-no_sw',\n '1-MV-rural--1-sw',\n '1-MV-rural--1-no_sw',\n '1-MV-rural--2-sw',\n '1-MV-rural--2-no_sw',\n '1-MV-semiurb--0-sw',\n '1-MV-semiurb--0-no_sw',\n '1-MV-semiurb--1-sw',\n '1-MV-semiurb--1-no_sw',\n '1-MV-semiurb--2-sw',\n '1-MV-semiurb--2-no_sw',\n '1-MV-urban--0-sw',\n '1-MV-urban--0-no_sw',\n '1-MV-urban--1-sw',\n '1-MV-urban--1-no_sw',\n '1-MV-urban--2-sw',\n '1-MV-urban--2-no_sw',\n '1-MV-comm--0-sw',\n '1-MV-comm--0-no_sw',\n '1-MV-comm--1-sw',\n '1-MV-comm--1-no_sw',\n '1-MV-comm--2-sw',\n '1-MV-comm--2-no_sw',\n '1-LV-rural1--0-sw',\n '1-LV-rural1--0-no_sw',\n '1-LV-rural1--1-sw',\n '1-LV-rural1--1-no_sw',\n '1-LV-rural1--2-sw',\n '1-LV-rural1--2-no_sw',\n '1-LV-rural2--0-sw',\n '1-LV-rural2--0-no_sw',\n '1-LV-rural2--1-sw',\n '1-LV-rural2--1-no_sw',\n '1-LV-rural2--2-sw',\n '1-LV-rural2--2-no_sw',\n '1-LV-rural3--0-sw',\n '1-LV-rural3--0-no_sw',\n '1-LV-rural3--1-sw',\n '1-LV-rural3--1-no_sw',\n '1-LV-rural3--2-sw',\n '1-LV-rural3--2-no_sw',\n '1-LV-semiurb4--0-sw',\n '1-LV-semiurb4--0-no_sw',\n '1-LV-semiurb4--1-sw',\n '1-LV-semiurb4--1-no_sw',\n '1-LV-semiurb4--2-sw',\n '1-LV-semiurb4--2-no_sw',\n '1-LV-semiurb5--0-sw',\n '1-LV-semiurb5--0-no_sw',\n '1-LV-semiurb5--1-sw',\n '1-LV-semiurb5--1-no_sw',\n '1-LV-semiurb5--2-sw',\n '1-LV-semiurb5--2-no_sw',\n '1-LV-urban6--0-sw',\n '1-LV-urban6--0-no_sw',\n '1-LV-urban6--1-sw',\n '1-LV-urban6--1-no_sw',\n '1-LV-urban6--2-sw',\n '1-LV-urban6--2-no_sw']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get lists of simbench codes\n",
    "all_simbench_codes = sb.collect_all_simbench_codes()\n",
    "all_simbench_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(0., requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3095, -0.5524,  0.3097,  0.2161],\n",
      "        [-0.4298, -0.2653,  0.2955, -0.1961],\n",
      "        [ 0.0119,  0.1774, -0.1514, -0.0989],\n",
      "        ...,\n",
      "        [ 0.2016,  0.5492,  0.2451, -0.0977],\n",
      "        [-0.3980, -0.2538, -0.3956, -0.2004],\n",
      "        [-0.4785,  0.5502, -0.1866, -0.2399]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-4.8516e-01, -1.6549e-01,  2.0859e-01,  4.4317e-01, -4.0218e-01,\n",
      "         7.5104e-02, -1.4601e-01, -4.6575e-01, -1.1309e-01,  4.5851e-01,\n",
      "         2.2650e-01,  3.4809e-01,  3.0619e-01,  1.7911e-02,  3.5580e-01,\n",
      "         4.9894e-01, -2.1628e-01,  3.9977e-02, -7.5208e-03,  4.2041e-01,\n",
      "        -2.1895e-01,  2.3228e-01,  7.1096e-02, -3.1837e-01, -4.3978e-01,\n",
      "        -1.8905e-01,  3.4342e-01,  2.0580e-01, -1.6092e-01,  6.5101e-02,\n",
      "        -2.1479e-01,  4.2486e-01,  4.4610e-01, -2.1051e-01, -7.0392e-02,\n",
      "         4.4332e-01,  2.6700e-01,  6.0856e-02,  1.4605e-01,  1.1338e-01,\n",
      "        -2.4511e-03, -2.4058e-01,  3.6281e-02, -4.3292e-01,  1.2253e-01,\n",
      "         4.1868e-01,  6.7889e-02,  4.1831e-01,  1.7672e-01,  3.2546e-02,\n",
      "        -4.9021e-01,  1.9441e-01, -1.8551e-01,  1.5761e-01, -2.8780e-01,\n",
      "        -4.9212e-01,  1.6754e-01,  4.9517e-02,  1.5238e-01,  2.1628e-01,\n",
      "         2.1875e-01,  2.0331e-01,  3.5532e-01, -3.9197e-01,  1.2361e-01,\n",
      "        -1.0804e-01, -2.3877e-01,  2.6955e-01,  2.8771e-01,  2.0370e-01,\n",
      "        -1.1833e-01, -4.5295e-01, -2.7124e-01,  4.7631e-01,  4.1980e-01,\n",
      "         1.5135e-01, -4.5365e-01, -1.1869e-01,  2.8255e-01,  5.7351e-02,\n",
      "         4.1278e-01,  1.1790e-01, -6.3696e-02, -3.4828e-01, -2.2358e-01,\n",
      "        -1.4805e-05,  4.3776e-01, -2.3813e-01, -3.6091e-01, -4.8993e-01,\n",
      "        -8.6017e-02, -1.6314e-01, -8.8940e-02, -3.3889e-02,  2.9500e-01,\n",
      "        -2.9871e-01,  2.6157e-01, -1.4190e-01,  4.6653e-02, -5.1090e-02,\n",
      "        -4.8691e-01,  2.2121e-01, -1.0238e-01, -5.2835e-02,  3.1797e-01,\n",
      "         1.1417e-01, -1.4569e-01, -1.2813e-02, -3.1436e-01, -5.4991e-03,\n",
      "         1.4697e-01,  1.9164e-01, -2.9574e-01, -2.9184e-01,  3.5664e-01,\n",
      "         2.8579e-02,  3.0205e-01,  7.7539e-02, -2.0750e-01,  1.7023e-01,\n",
      "        -4.1826e-01, -3.6809e-01,  4.2396e-01, -4.1949e-01, -4.7842e-01,\n",
      "         3.8243e-01,  1.2577e-01, -6.7489e-02,  3.4429e-01,  8.7642e-02,\n",
      "         4.8666e-01,  2.8964e-01, -3.8370e-01,  4.2389e-01,  4.3966e-01,\n",
      "        -4.2862e-01, -4.0108e-01, -3.7094e-01,  1.9517e-01,  4.7426e-01,\n",
      "         3.9446e-01, -3.5079e-01, -1.3117e-01,  4.8031e-01, -1.9298e-01,\n",
      "         3.3159e-01, -2.9716e-02, -3.4174e-01,  4.9846e-01, -9.0724e-02,\n",
      "        -3.5488e-01,  2.0495e-02,  4.5701e-01,  1.6555e-01,  3.0676e-01,\n",
      "        -3.2149e-01,  3.6298e-01,  2.2033e-02,  2.6676e-01, -2.0127e-01,\n",
      "        -1.5231e-01, -4.5368e-01,  3.2318e-01,  1.3047e-01,  4.5178e-01,\n",
      "        -2.2946e-01, -3.3315e-01, -1.1691e-01,  4.9806e-01,  4.6889e-01,\n",
      "        -2.6586e-01, -3.5630e-01, -3.2941e-01, -4.0329e-01, -1.0758e-01,\n",
      "        -2.8917e-01,  4.3011e-02, -4.1978e-01, -4.0224e-02,  4.6298e-01,\n",
      "        -4.4076e-01, -1.7268e-01,  3.2795e-01,  4.6442e-01, -1.7098e-01,\n",
      "        -2.8740e-01,  3.8466e-01, -2.1505e-01, -4.6945e-02,  2.0596e-01,\n",
      "        -2.8298e-01,  3.9783e-01,  3.7469e-01,  3.9978e-01,  1.4120e-01,\n",
      "         2.2139e-01, -1.4719e-01, -2.4003e-01,  5.9806e-02,  3.0415e-01,\n",
      "        -9.4602e-02, -4.3691e-01,  2.5937e-01,  2.7527e-01,  2.7420e-01,\n",
      "         3.8077e-01,  2.3363e-01,  3.2530e-01, -1.2757e-01, -4.1460e-02,\n",
      "        -2.5640e-01,  1.2166e-01,  1.2987e-01, -3.4956e-01,  3.9908e-01,\n",
      "        -7.1000e-02,  1.7899e-01,  2.1821e-01,  1.0086e-01, -4.0729e-01,\n",
      "         2.7550e-01, -3.7742e-01,  9.2325e-02,  4.3780e-01, -4.1946e-01,\n",
      "        -4.4853e-01,  2.4172e-01, -3.6897e-01, -3.5523e-01, -3.5958e-02,\n",
      "        -1.6635e-01, -3.0453e-01,  9.6080e-02, -3.0590e-01, -1.1223e-01,\n",
      "         2.7100e-01,  3.0086e-01,  3.1655e-01, -3.2676e-02,  4.3761e-01,\n",
      "         4.1230e-01, -4.1229e-01,  3.3217e-01,  2.7414e-01,  3.6876e-01,\n",
      "        -7.2065e-02, -7.1284e-02,  6.6275e-02,  2.7175e-01, -1.0656e-01,\n",
      "         2.3156e-01,  2.3801e-01,  1.9819e-01, -4.1264e-01, -1.9198e-01,\n",
      "         3.0572e-01], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2206,  0.3259,  0.4905,  0.3059],\n",
      "        [-0.3320,  0.1371,  0.0478,  0.3361],\n",
      "        [ 0.1332,  0.3118,  0.4643,  0.0289],\n",
      "        ...,\n",
      "        [-0.1253, -0.4170,  0.0514, -0.1053],\n",
      "        [ 0.4961, -0.4934, -0.2859,  0.2386],\n",
      "        [-0.0222, -0.4125,  0.0646,  0.2135]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 2.5838e-01, -3.5717e-01, -3.0885e-02, -2.0781e-01, -3.6063e-02,\n",
      "        -1.8618e-01, -6.7457e-02, -3.0674e-04, -1.9688e-01, -1.9977e-01,\n",
      "         4.1304e-01,  3.7011e-01,  2.4925e-03,  2.9054e-01, -4.8716e-01,\n",
      "         1.3173e-01,  3.6211e-01,  5.5737e-02,  3.5167e-01, -1.7349e-01,\n",
      "        -1.2904e-01,  1.9800e-01, -1.0119e-01, -4.7795e-01,  4.5242e-01,\n",
      "        -1.9102e-01, -2.5208e-01,  4.7648e-01,  3.1717e-01,  3.4162e-01,\n",
      "         3.7027e-01,  1.9704e-01,  3.4222e-01, -1.7750e-01,  1.0760e-01,\n",
      "         1.9567e-01, -4.2835e-01, -7.5664e-02, -2.0390e-01,  2.4162e-01,\n",
      "         4.5742e-01,  6.6600e-02, -3.4454e-02,  4.2842e-01,  8.0767e-02,\n",
      "         5.9296e-02, -4.7939e-01, -1.8810e-01,  1.8934e-01, -3.4466e-01,\n",
      "         3.5435e-01, -3.2311e-02,  3.3187e-01,  8.3976e-02,  3.2039e-01,\n",
      "        -9.6378e-02, -3.3204e-01, -2.7795e-01, -4.7891e-01,  4.6831e-01,\n",
      "         1.0488e-01,  5.0415e-01, -8.8825e-02,  1.7840e-01, -6.0684e-02,\n",
      "        -4.4818e-01, -2.8579e-01, -2.2603e-01,  3.8171e-01,  9.0608e-02,\n",
      "        -4.2743e-01, -2.7690e-01, -2.5721e-01, -3.6871e-01,  4.6453e-01,\n",
      "         2.3735e-01, -4.2848e-01, -2.7562e-01,  4.3519e-01, -4.1782e-01,\n",
      "        -3.8198e-01, -8.5689e-02,  3.3375e-01, -2.6413e-02,  3.2252e-01,\n",
      "         3.3242e-01, -3.4473e-01, -3.5401e-01,  1.6580e-01,  1.7847e-01,\n",
      "         1.7180e-01, -1.8647e-01,  1.0950e-01,  2.3258e-01, -2.8806e-01,\n",
      "        -2.8981e-01,  9.2307e-02, -1.8682e-01,  3.3801e-01, -2.7598e-01,\n",
      "        -3.9278e-01, -4.5453e-01,  4.6058e-01,  1.3425e-01, -2.5809e-01,\n",
      "        -4.4681e-01, -9.5754e-02,  4.1898e-01, -1.9306e-03, -1.4560e-01,\n",
      "        -3.2020e-01,  1.7742e-01,  4.7961e-01,  3.7819e-01,  1.7589e-01,\n",
      "        -3.9379e-01, -3.1762e-01, -6.2888e-02, -7.0024e-02, -4.8142e-01,\n",
      "         2.9225e-01, -2.5413e-01, -2.1021e-01,  3.4584e-01, -7.1088e-02,\n",
      "        -2.6776e-01, -1.7142e-01,  1.3824e-01,  4.2477e-01,  1.3496e-01,\n",
      "         2.1295e-02, -3.7599e-01, -1.2982e-01, -1.1174e-01, -1.8110e-01,\n",
      "        -4.1063e-01,  9.7956e-02,  4.4973e-01,  3.9972e-01, -4.3269e-02,\n",
      "        -2.3898e-02, -3.2037e-01,  3.7659e-01,  2.3366e-01,  2.1583e-01,\n",
      "        -2.6119e-01,  2.9453e-01, -3.1351e-01,  3.0375e-01,  2.5146e-01,\n",
      "        -6.1653e-03, -1.7986e-01, -1.3632e-01,  4.0466e-01, -2.1790e-01,\n",
      "        -2.4835e-01,  5.3211e-02, -7.4699e-02, -4.6064e-01,  8.1053e-02,\n",
      "         3.8999e-01, -1.0140e-01, -9.7196e-02, -4.1093e-01, -4.4017e-02,\n",
      "         2.4479e-01,  3.8629e-01, -3.7008e-02, -1.5938e-01, -3.0185e-01,\n",
      "         2.0558e-01,  4.8305e-01,  3.4364e-01,  3.8184e-01,  3.2919e-01,\n",
      "         2.3675e-01, -4.4453e-01,  1.2585e-01, -5.5124e-02, -3.1494e-01,\n",
      "        -4.5872e-01, -7.0603e-03,  3.4007e-01,  1.9666e-02, -3.4248e-01,\n",
      "         2.9585e-02,  4.1321e-01,  5.2816e-02,  1.3366e-01, -5.1580e-01,\n",
      "        -1.2702e-01, -1.5365e-01,  3.5471e-01,  4.8149e-01,  1.5547e-01,\n",
      "        -4.2550e-01, -1.2416e-03,  1.2761e-01, -3.6484e-01, -2.4532e-01,\n",
      "        -2.0286e-02, -2.8749e-01, -7.9808e-02,  1.5802e-01,  8.7466e-02,\n",
      "        -4.3125e-01,  2.3432e-01,  4.9114e-01, -2.5417e-01, -2.5704e-01,\n",
      "        -2.0618e-01,  2.6058e-01,  2.4872e-01,  5.7787e-02,  2.9361e-01,\n",
      "         1.9354e-01, -7.7889e-02,  2.2604e-01, -2.1691e-01,  3.4076e-01,\n",
      "        -3.2114e-01, -3.4695e-02, -1.8752e-02,  2.6449e-01, -3.6874e-01,\n",
      "         2.7227e-01, -4.8291e-01, -3.6414e-01,  4.2218e-01,  1.9371e-01,\n",
      "        -4.7082e-01,  3.6839e-01,  1.3711e-01, -4.6774e-01,  1.9050e-02,\n",
      "         5.5869e-02,  3.0150e-01, -2.6592e-01, -3.6626e-01, -2.0721e-01,\n",
      "         1.3586e-01,  4.2605e-01, -2.8651e-01,  3.0040e-01,  1.9528e-01,\n",
      "         2.4027e-01,  2.2140e-01, -1.5801e-01,  1.1092e-01,  4.4134e-01,\n",
      "         1.3325e-01, -1.9195e-02,  3.9227e-01,  1.9174e-01,  1.0692e-01,\n",
      "        -4.8233e-01], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0056,  0.2658,  0.3540, -0.0418],\n",
      "        [-0.2129, -0.2013,  0.4520,  0.4619],\n",
      "        [-0.2681, -0.5071, -0.2595, -0.2423],\n",
      "        ...,\n",
      "        [ 0.4449,  0.0716, -0.3564,  0.2901],\n",
      "        [ 0.2022, -0.1798,  0.0867,  0.4894],\n",
      "        [-0.4051, -0.0928, -0.2452,  0.2094]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3328, -0.1701,  0.1309, -0.1619,  0.0332, -0.1597, -0.0853,  0.1019,\n",
      "        -0.4201, -0.2366, -0.2303,  0.1464, -0.4818, -0.0105,  0.2503,  0.1832,\n",
      "        -0.1316,  0.2123,  0.3509, -0.2299, -0.0514, -0.1573, -0.1690,  0.2997,\n",
      "         0.0308, -0.1266,  0.3413, -0.0842, -0.1092,  0.3990, -0.1552, -0.2938,\n",
      "         0.3715, -0.1376,  0.1979, -0.2935,  0.0843, -0.4483, -0.3918, -0.3077,\n",
      "         0.4314, -0.0058,  0.4930,  0.1161, -0.2876,  0.4942, -0.4521, -0.0595,\n",
      "        -0.4400, -0.1646,  0.2468, -0.0633, -0.0021, -0.4619,  0.4356, -0.1115,\n",
      "        -0.3804, -0.1093,  0.1863, -0.3794,  0.2513, -0.2742, -0.4020, -0.4739,\n",
      "        -0.2575,  0.2527, -0.1809,  0.3652,  0.0881, -0.4411,  0.3351,  0.3261,\n",
      "        -0.1657,  0.3329, -0.1748,  0.1142,  0.2983, -0.0587,  0.0947, -0.4071,\n",
      "         0.4436, -0.1569, -0.0182,  0.4446,  0.0440, -0.4573, -0.2748,  0.0227,\n",
      "        -0.2446,  0.0946,  0.1157,  0.2669, -0.2360, -0.3870,  0.1378, -0.3449,\n",
      "         0.3970, -0.3247, -0.3042, -0.1881,  0.3833,  0.2619,  0.2033, -0.4968,\n",
      "         0.1043, -0.2532, -0.4408, -0.2296, -0.3462, -0.1008,  0.0196,  0.2678,\n",
      "        -0.1591,  0.1238, -0.0944,  0.2580,  0.4699, -0.4174, -0.3870,  0.0802,\n",
      "        -0.3001,  0.1786, -0.4786,  0.2529,  0.1736, -0.3046, -0.3954, -0.4456,\n",
      "         0.0845,  0.0389, -0.3019,  0.1451,  0.0714, -0.4433, -0.3082, -0.3885,\n",
      "        -0.4813, -0.0381,  0.0431,  0.1733,  0.4537, -0.0767,  0.3152, -0.3398,\n",
      "         0.4917,  0.3910, -0.4649, -0.2731,  0.1424, -0.2670, -0.3343, -0.4057,\n",
      "        -0.1473, -0.2585, -0.4528, -0.3566,  0.4936, -0.1055, -0.1433,  0.1387,\n",
      "        -0.2559, -0.1646, -0.0045,  0.1323, -0.2663, -0.1255,  0.1603,  0.0970,\n",
      "         0.4925, -0.3951,  0.4975, -0.2936,  0.4972, -0.3211, -0.2010, -0.2397,\n",
      "        -0.4109,  0.3730,  0.2071, -0.3305, -0.3152, -0.2680, -0.3122, -0.0513,\n",
      "        -0.1281, -0.1864,  0.2811,  0.0071, -0.4912,  0.3349,  0.2517, -0.4927,\n",
      "         0.4874, -0.3026, -0.1925, -0.0395, -0.4385, -0.3354,  0.4717,  0.3328,\n",
      "        -0.1163,  0.3127, -0.1931, -0.3978, -0.2721,  0.2589,  0.2198, -0.0720,\n",
      "        -0.4364, -0.0694, -0.0335, -0.2124, -0.4975,  0.0250, -0.3700,  0.4344,\n",
      "         0.0074, -0.1619, -0.0032, -0.3425,  0.3685, -0.4750,  0.3849, -0.2413,\n",
      "        -0.3008,  0.2869, -0.4049, -0.2668,  0.2583,  0.0789, -0.4697, -0.1312,\n",
      "        -0.4866, -0.1272, -0.4238,  0.4566, -0.4029, -0.1375,  0.0827, -0.4172,\n",
      "        -0.3048, -0.2767, -0.0353, -0.1060, -0.1291,  0.3615,  0.2461,  0.0195,\n",
      "        -0.0117, -0.1519, -0.4030,  0.1122,  0.5143,  0.2116,  0.0699,  0.4546],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3668,  0.6264],\n",
      "        [-0.2477,  0.1652],\n",
      "        [-0.0655,  0.2872],\n",
      "        [ 0.2369,  0.3485],\n",
      "        [ 0.4923, -0.5673],\n",
      "        [ 0.1616, -0.5909],\n",
      "        [-0.6172,  0.6593],\n",
      "        [-0.4803,  0.3438],\n",
      "        [-0.0301, -0.1269],\n",
      "        [-0.5375,  0.6247],\n",
      "        [-0.4491, -0.1869],\n",
      "        [-0.4299,  0.3649],\n",
      "        [ 0.3911,  0.1812],\n",
      "        [-0.0324, -0.3804],\n",
      "        [-0.0106,  0.1018],\n",
      "        [-0.5724, -0.5970],\n",
      "        [ 0.4919,  0.3883],\n",
      "        [-0.4890,  0.4999],\n",
      "        [-0.5763,  0.2009],\n",
      "        [-0.5484,  0.4447],\n",
      "        [ 0.4882, -0.2420],\n",
      "        [ 0.3326, -0.4696],\n",
      "        [-0.3943, -0.5903],\n",
      "        [ 0.3031,  0.5647],\n",
      "        [ 0.4316,  0.0281],\n",
      "        [ 0.4523,  0.0976],\n",
      "        [ 0.4044,  0.5120],\n",
      "        [-0.0383,  0.6588],\n",
      "        [-0.4960,  0.6551],\n",
      "        [-0.3137,  0.0339],\n",
      "        [-0.1711, -0.6681],\n",
      "        [-0.1035, -0.2727],\n",
      "        [ 0.0885, -0.1489],\n",
      "        [ 0.6342, -0.1214],\n",
      "        [ 0.1626, -0.1122],\n",
      "        [ 0.0408, -0.4177],\n",
      "        [-0.1617, -0.3755],\n",
      "        [ 0.5855, -0.2863],\n",
      "        [ 0.0940,  0.0888],\n",
      "        [-0.0281,  0.5031],\n",
      "        [ 0.5853,  0.4886],\n",
      "        [-0.6380, -0.4595],\n",
      "        [ 0.3403,  0.0164],\n",
      "        [ 0.0385,  0.5664],\n",
      "        [ 0.5804, -0.5575],\n",
      "        [-0.2668, -0.0173],\n",
      "        [ 0.0567,  0.1154],\n",
      "        [-0.2857,  0.1719],\n",
      "        [-0.6569,  0.3717],\n",
      "        [ 0.1571,  0.1733],\n",
      "        [-0.3151,  0.2904],\n",
      "        [-0.4029, -0.5196],\n",
      "        [-0.1825,  0.2330],\n",
      "        [-0.3250,  0.0340],\n",
      "        [-0.0017,  0.2134],\n",
      "        [ 0.4216,  0.1482],\n",
      "        [ 0.3301, -0.5741],\n",
      "        [ 0.1996,  0.4290],\n",
      "        [-0.1463,  0.2422],\n",
      "        [ 0.5131,  0.4736],\n",
      "        [-0.4088, -0.4488],\n",
      "        [ 0.5968, -0.6091],\n",
      "        [-0.1679, -0.5229],\n",
      "        [-0.0041, -0.0615],\n",
      "        [ 0.0191,  0.5000],\n",
      "        [ 0.3329,  0.4527],\n",
      "        [ 0.2475, -0.1480],\n",
      "        [ 0.0628,  0.5305],\n",
      "        [ 0.2563,  0.1521],\n",
      "        [-0.1068, -0.0859],\n",
      "        [ 0.5469,  0.1107],\n",
      "        [ 0.5875, -0.1426],\n",
      "        [ 0.1751, -0.2049],\n",
      "        [ 0.4380, -0.1865],\n",
      "        [ 0.1635, -0.6419],\n",
      "        [ 0.2437, -0.5521],\n",
      "        [ 0.6086, -0.6736],\n",
      "        [ 0.4353, -0.1884],\n",
      "        [ 0.5619,  0.3713],\n",
      "        [ 0.6794, -0.6898],\n",
      "        [ 0.6756, -0.2365],\n",
      "        [ 0.5226,  0.6646],\n",
      "        [ 0.3890,  0.3590],\n",
      "        [-0.4888, -0.3552],\n",
      "        [ 0.2967,  0.1100],\n",
      "        [ 0.4722,  0.4723],\n",
      "        [ 0.2758, -0.6701],\n",
      "        [ 0.2420,  0.1624],\n",
      "        [-0.2561,  0.2674],\n",
      "        [ 0.6341, -0.2341],\n",
      "        [ 0.6992, -0.6915],\n",
      "        [ 0.4213,  0.0619],\n",
      "        [-0.2963,  0.5719],\n",
      "        [ 0.3793,  0.5436],\n",
      "        [-0.4044, -0.6178],\n",
      "        [-0.0345,  0.0053],\n",
      "        [ 0.2651,  0.5422],\n",
      "        [ 0.0479,  0.0848],\n",
      "        [-0.3994, -0.3576],\n",
      "        [-0.1443, -0.5095],\n",
      "        [ 0.5546, -0.3169],\n",
      "        [-0.2706,  0.2734],\n",
      "        [ 0.5388, -0.7030],\n",
      "        [ 0.0501,  0.2572],\n",
      "        [ 0.3331, -0.5704],\n",
      "        [-0.5536,  0.5530],\n",
      "        [ 0.0083, -0.5039],\n",
      "        [ 0.3281, -0.5998],\n",
      "        [-0.5038,  0.4270],\n",
      "        [-0.4504,  0.3100],\n",
      "        [-0.5244, -0.0668],\n",
      "        [-0.0557, -0.5757],\n",
      "        [-0.4578,  0.4629],\n",
      "        [-0.5234, -0.5858],\n",
      "        [-0.3138, -0.3779],\n",
      "        [ 0.5293, -0.1413],\n",
      "        [ 0.1884,  0.2689],\n",
      "        [ 0.0356, -0.6469],\n",
      "        [ 0.5164,  0.2617],\n",
      "        [-0.1522,  0.0216],\n",
      "        [-0.6400, -0.0081],\n",
      "        [-0.4844,  0.4107],\n",
      "        [-0.6539,  0.2538],\n",
      "        [ 0.5100,  0.1969],\n",
      "        [ 0.6266,  0.1795],\n",
      "        [-0.2087, -0.2222],\n",
      "        [-0.6471, -0.1003],\n",
      "        [ 0.6027, -0.3835],\n",
      "        [ 0.3272,  0.1561],\n",
      "        [-0.2972, -0.6981],\n",
      "        [ 0.0594,  0.2620],\n",
      "        [-0.0021, -0.6092],\n",
      "        [-0.0346, -0.3638],\n",
      "        [ 0.4900, -0.4528],\n",
      "        [-0.2410,  0.5618],\n",
      "        [ 0.4247,  0.0729],\n",
      "        [ 0.1363,  0.3640],\n",
      "        [-0.6559,  0.1928],\n",
      "        [-0.5015,  0.4624],\n",
      "        [-0.3815, -0.6887],\n",
      "        [ 0.4799,  0.1581],\n",
      "        [-0.1831, -0.1994],\n",
      "        [-0.5578, -0.5809],\n",
      "        [-0.6819, -0.0290],\n",
      "        [ 0.5567,  0.2674],\n",
      "        [ 0.6124,  0.0135],\n",
      "        [-0.4698, -0.5975],\n",
      "        [ 0.1165,  0.5392],\n",
      "        [ 0.2667,  0.0647],\n",
      "        [-0.5138, -0.5152],\n",
      "        [ 0.1804, -0.1656],\n",
      "        [ 0.4126,  0.3428],\n",
      "        [ 0.4437, -0.0163],\n",
      "        [ 0.3619, -0.0957],\n",
      "        [-0.4807,  0.3529],\n",
      "        [-0.6416,  0.3448],\n",
      "        [ 0.5285,  0.5839],\n",
      "        [-0.4740,  0.1640],\n",
      "        [-0.5838, -0.1939],\n",
      "        [ 0.0283, -0.1180],\n",
      "        [-0.6371,  0.1287],\n",
      "        [-0.4459,  0.1714],\n",
      "        [ 0.3901, -0.4227],\n",
      "        [-0.3573,  0.0147],\n",
      "        [ 0.2973,  0.4910],\n",
      "        [ 0.2060, -0.3602],\n",
      "        [ 0.6693, -0.1537],\n",
      "        [ 0.5882,  0.6330],\n",
      "        [-0.5557,  0.0051],\n",
      "        [-0.5585,  0.4809],\n",
      "        [-0.6120,  0.0385],\n",
      "        [-0.3029, -0.3120],\n",
      "        [ 0.4057,  0.4161],\n",
      "        [ 0.2294, -0.5451],\n",
      "        [-0.4653, -0.4443],\n",
      "        [-0.0330,  0.6262],\n",
      "        [-0.2801, -0.0300],\n",
      "        [ 0.5395,  0.3281],\n",
      "        [ 0.1679,  0.6937],\n",
      "        [ 0.5572,  0.4882],\n",
      "        [-0.0985,  0.4513],\n",
      "        [ 0.4850,  0.2790],\n",
      "        [ 0.5394, -0.4155],\n",
      "        [ 0.6829,  0.6381],\n",
      "        [-0.0267,  0.6503],\n",
      "        [-0.1973,  0.3125],\n",
      "        [-0.1259,  0.6505],\n",
      "        [ 0.3003, -0.3612],\n",
      "        [-0.0735,  0.3906],\n",
      "        [-0.6000,  0.1971],\n",
      "        [ 0.1415,  0.2318],\n",
      "        [-0.0992,  0.3001],\n",
      "        [-0.0264,  0.6714],\n",
      "        [ 0.1441, -0.0915],\n",
      "        [-0.6423, -0.3495],\n",
      "        [ 0.2744, -0.1366],\n",
      "        [ 0.0664,  0.1141],\n",
      "        [ 0.3395,  0.3449],\n",
      "        [ 0.5209, -0.4615],\n",
      "        [-0.0480, -0.5271],\n",
      "        [-0.1369, -0.1918],\n",
      "        [-0.1699, -0.4507],\n",
      "        [ 0.2967,  0.2468],\n",
      "        [-0.0587,  0.5364],\n",
      "        [ 0.1890, -0.2160],\n",
      "        [-0.6577, -0.7054],\n",
      "        [-0.4618,  0.3416],\n",
      "        [-0.5080,  0.3234],\n",
      "        [ 0.5820, -0.6917],\n",
      "        [-0.6331,  0.3853],\n",
      "        [ 0.4386, -0.1654],\n",
      "        [-0.5931,  0.5166],\n",
      "        [ 0.5059, -0.0781],\n",
      "        [-0.3204, -0.0336],\n",
      "        [ 0.1762,  0.0174],\n",
      "        [ 0.1066, -0.5305],\n",
      "        [ 0.2633,  0.3816],\n",
      "        [-0.1460,  0.2928],\n",
      "        [ 0.2481,  0.3790],\n",
      "        [ 0.2744,  0.3599],\n",
      "        [-0.2811,  0.0820],\n",
      "        [ 0.0327,  0.5388],\n",
      "        [-0.0469,  0.6562],\n",
      "        [-0.5393,  0.6524],\n",
      "        [ 0.1538, -0.5871],\n",
      "        [-0.2953,  0.3852],\n",
      "        [ 0.0977,  0.0171],\n",
      "        [ 0.4053,  0.2610],\n",
      "        [-0.3306,  0.4570],\n",
      "        [ 0.0882,  0.6152],\n",
      "        [-0.4759, -0.4250],\n",
      "        [ 0.2933,  0.6305],\n",
      "        [ 0.5460, -0.3747],\n",
      "        [ 0.4149, -0.1354],\n",
      "        [ 0.2732, -0.6552],\n",
      "        [-0.0043, -0.4065],\n",
      "        [-0.6666, -0.2872],\n",
      "        [ 0.2798, -0.1180],\n",
      "        [ 0.1788,  0.1592],\n",
      "        [ 0.7078,  0.4322],\n",
      "        [ 0.3811,  0.5848],\n",
      "        [-0.0270, -0.3919],\n",
      "        [-0.2789,  0.5960],\n",
      "        [ 0.5657,  0.6835],\n",
      "        [-0.3871, -0.5883],\n",
      "        [-0.1145, -0.1397],\n",
      "        [ 0.3890,  0.2784],\n",
      "        [ 0.1516, -0.4849],\n",
      "        [-0.3780, -0.5605],\n",
      "        [ 0.6037,  0.4944],\n",
      "        [ 0.4209,  0.4434],\n",
      "        [-0.0726,  0.2878],\n",
      "        [-0.1043, -0.6018],\n",
      "        [-0.3722,  0.4242],\n",
      "        [ 0.4081, -0.2233],\n",
      "        [-0.6403,  0.2100]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4213,  0.0641,  0.3384,  0.5127],\n",
      "        [ 0.3397,  0.2595, -0.2097, -0.0310],\n",
      "        [-0.2672, -0.1259,  0.4085,  0.1043],\n",
      "        ...,\n",
      "        [ 0.2291, -0.2622,  0.3451,  0.1949],\n",
      "        [-0.4710,  0.2296, -0.1581, -0.0303],\n",
      "        [-0.3363, -0.6419,  0.1257, -0.3487]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2593, -0.3291, -0.4293, -0.3578, -0.4907,  0.2817,  0.3832, -0.4546,\n",
      "        -0.0705,  0.0064, -0.4296,  0.0908,  0.2175, -0.1402, -0.1640, -0.3183,\n",
      "         0.4717,  0.1072,  0.0189,  0.1912,  0.3215,  0.3893,  0.2930,  0.4554,\n",
      "        -0.0385, -0.4561, -0.1646, -0.3232, -0.2401,  0.2471,  0.3841,  0.4489,\n",
      "         0.2531, -0.1306,  0.4005,  0.3878,  0.4383, -0.4986,  0.1490, -0.0030,\n",
      "         0.3260,  0.2981, -0.2546, -0.0924,  0.0310,  0.4326, -0.2781,  0.0463,\n",
      "         0.4051,  0.0729, -0.2492, -0.2959,  0.4174,  0.1015,  0.1086,  0.0667,\n",
      "        -0.2056,  0.0057, -0.3236, -0.1252,  0.1253, -0.1701, -0.1278, -0.0213,\n",
      "        -0.0037,  0.4590,  0.4241, -0.4106, -0.4992,  0.0942,  0.4230, -0.1578,\n",
      "         0.1627, -0.2552,  0.2569,  0.0613,  0.1700, -0.4934,  0.4631, -0.3494,\n",
      "         0.1534,  0.1305, -0.4524,  0.4780, -0.4370,  0.3369,  0.0197, -0.0920,\n",
      "        -0.2749,  0.0649,  0.3861,  0.3873, -0.2426,  0.0274,  0.4900, -0.3017,\n",
      "         0.1609,  0.4885, -0.2280, -0.0279,  0.0578,  0.2755,  0.3758,  0.1219,\n",
      "        -0.2825, -0.0107, -0.2929, -0.3119, -0.0053,  0.3006,  0.0826, -0.0534,\n",
      "         0.2476, -0.4317,  0.0548,  0.1779,  0.1335,  0.3020, -0.2185, -0.1777,\n",
      "        -0.1596, -0.3729, -0.0997,  0.3751, -0.2224, -0.4616, -0.2304, -0.1976,\n",
      "         0.3009,  0.4209,  0.4020, -0.0297, -0.2351, -0.0412,  0.3175,  0.4440,\n",
      "        -0.1118,  0.2258,  0.4382, -0.2188, -0.4126,  0.3101,  0.4969, -0.2055,\n",
      "         0.0644, -0.2391,  0.4716, -0.2025,  0.3508, -0.4898,  0.2156,  0.1765,\n",
      "        -0.0712,  0.0054, -0.1534,  0.3059, -0.1191,  0.4580,  0.0933, -0.0036,\n",
      "        -0.2096,  0.4275,  0.0281,  0.0437,  0.3977,  0.1846,  0.0504,  0.2851,\n",
      "         0.1851, -0.3398,  0.4129,  0.3590, -0.0609, -0.0553,  0.3657, -0.2573,\n",
      "        -0.4895,  0.3985,  0.0535,  0.0703, -0.1978, -0.1329, -0.4016,  0.4406,\n",
      "         0.1940, -0.1059,  0.3395, -0.3006, -0.4524, -0.2716,  0.0863, -0.1883,\n",
      "        -0.2802,  0.4365, -0.2215,  0.4985, -0.2395, -0.1474, -0.0026,  0.4959,\n",
      "        -0.4144,  0.1025, -0.1993,  0.4681,  0.0142,  0.4489, -0.0702,  0.2623,\n",
      "        -0.4420,  0.1940,  0.4707,  0.3065,  0.2411, -0.2928, -0.4130,  0.4090,\n",
      "        -0.2951, -0.0754,  0.4233, -0.0159, -0.4474, -0.3003,  0.3018,  0.4437,\n",
      "         0.0384, -0.0110,  0.3272, -0.3934, -0.0149, -0.4221,  0.4508, -0.4799,\n",
      "         0.4948, -0.2167,  0.1343, -0.1587, -0.0649,  0.4571,  0.1051,  0.2342,\n",
      "        -0.2411,  0.4281,  0.4679,  0.2083,  0.2646,  0.2584, -0.4548,  0.3678,\n",
      "        -0.2154,  0.3335, -0.3116,  0.4895,  0.0426, -0.4555, -0.2350, -0.3220],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0065,  0.0071,  0.0538,  ..., -0.0498,  0.0586,  0.0373],\n",
      "        [-0.0204,  0.0385,  0.0205,  ...,  0.0118,  0.0434, -0.0175],\n",
      "        [ 0.0250,  0.0296,  0.0010,  ...,  0.0456,  0.0409,  0.0199],\n",
      "        ...,\n",
      "        [-0.0098, -0.0007, -0.0416,  ..., -0.0119, -0.0357, -0.1078],\n",
      "        [-0.0166,  0.0569,  0.0017,  ...,  0.0409, -0.0313, -0.0092],\n",
      "        [ 0.0264, -0.0092, -0.0044,  ..., -0.0319, -0.0010, -0.0493]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.4420e-02, -1.4881e-02, -3.1592e-02,  5.0860e-02,  2.0010e-02,\n",
      "         2.3560e-02, -4.1495e-02, -3.0177e-02,  3.9761e-02,  4.3869e-02,\n",
      "        -4.1972e-02,  4.5870e-02, -3.3029e-02, -3.9230e-02,  2.4492e-02,\n",
      "        -5.5204e-02,  3.9073e-02,  6.2104e-02, -9.1713e-03, -1.1836e-02,\n",
      "         1.4462e-02, -4.2413e-02,  3.2469e-02,  5.4510e-02, -5.6363e-02,\n",
      "        -3.2230e-02,  2.5119e-02, -1.9812e-02, -3.7424e-02, -4.7113e-03,\n",
      "        -4.9608e-02,  4.1923e-03,  6.5763e-03, -4.7952e-02,  2.6426e-02,\n",
      "         4.2525e-02, -7.1161e-03, -5.3497e-02, -6.8916e-03, -4.8482e-02,\n",
      "         2.7310e-02,  4.7369e-02, -2.9524e-02,  4.5642e-02, -5.6557e-02,\n",
      "         5.2556e-02, -2.4636e-02, -4.6441e-02,  2.9820e-04, -1.1224e-02,\n",
      "         2.7672e-02,  6.1860e-02,  4.2654e-02,  2.9337e-02,  2.2951e-02,\n",
      "        -1.8439e-02, -4.4475e-02,  6.0942e-02,  6.1596e-02, -1.0446e-02,\n",
      "        -3.8193e-02, -1.9911e-02, -3.5662e-04, -3.9395e-02, -2.0024e-02,\n",
      "         4.7417e-02, -5.4482e-04,  5.4840e-04, -9.1968e-03,  1.9093e-02,\n",
      "        -2.7214e-02, -4.9658e-02, -4.4997e-02,  3.0809e-02,  1.0684e-02,\n",
      "         1.2131e-02, -3.3101e-02,  5.3349e-02,  5.7775e-02, -3.7432e-02,\n",
      "         4.0936e-02, -4.1622e-02,  6.2301e-03,  4.2324e-02, -5.9481e-04,\n",
      "         4.3182e-02, -2.6158e-02, -4.5465e-02, -5.4647e-02,  4.7544e-02,\n",
      "        -5.4026e-02,  1.2655e-02,  4.9867e-03, -3.0792e-02,  5.2263e-02,\n",
      "         1.1613e-02, -4.6160e-02, -4.0669e-02,  4.9472e-02, -4.5522e-02,\n",
      "         2.6859e-02, -1.5587e-03, -4.1212e-02,  2.9404e-02,  1.8420e-02,\n",
      "        -1.2751e-02,  1.7145e-02, -4.3331e-03, -1.2524e-02,  1.6508e-02,\n",
      "        -2.7700e-03,  2.1090e-02, -2.0326e-02, -5.7138e-02,  5.6711e-02,\n",
      "        -5.3989e-02, -2.5562e-02, -1.5345e-02,  1.1784e-02, -4.5011e-03,\n",
      "         5.3641e-02, -4.3947e-02,  4.0247e-02, -4.5076e-02,  2.1552e-02,\n",
      "         4.4434e-02, -5.0947e-02, -4.2976e-02,  3.3341e-02, -4.9840e-02,\n",
      "        -4.6640e-02, -3.0465e-02,  9.7521e-03,  1.2382e-02,  1.7974e-02,\n",
      "        -4.3894e-02, -6.7897e-03, -1.3872e-02, -2.5662e-02, -8.6639e-03,\n",
      "        -3.1987e-02, -6.1807e-02,  3.6751e-02,  4.4961e-03,  2.9025e-02,\n",
      "        -1.1878e-02,  3.7401e-02,  5.6832e-02, -4.0094e-02, -1.3722e-02,\n",
      "         5.4683e-02,  6.7388e-03,  4.6740e-03, -5.2847e-02,  4.8359e-02,\n",
      "        -7.4148e-03,  1.4570e-02, -1.4682e-02,  4.2469e-02,  5.8446e-02,\n",
      "         3.8906e-02,  4.8311e-02, -4.1413e-03, -5.7399e-02,  1.1771e-02,\n",
      "         4.0460e-02, -5.4333e-02, -2.3988e-02,  2.1468e-02, -3.7870e-02,\n",
      "         5.7923e-03,  1.1049e-02,  3.6736e-02, -9.5486e-03,  6.2138e-02,\n",
      "         4.4190e-02,  1.3099e-02,  2.0749e-03,  5.5859e-02, -9.4177e-05,\n",
      "         4.2795e-03, -3.0647e-03,  5.5946e-03,  4.2508e-02, -4.5960e-02,\n",
      "        -1.0645e-02, -4.8775e-02, -4.6747e-02, -4.0820e-02,  3.3064e-02,\n",
      "        -3.7823e-02, -6.1558e-03, -4.6090e-02, -5.8342e-02,  5.8273e-02,\n",
      "        -5.9298e-02,  2.3764e-03, -2.9103e-02,  3.6510e-02,  2.6526e-03,\n",
      "        -5.5925e-02,  3.6577e-02, -4.2256e-02,  4.9083e-02, -4.3934e-02,\n",
      "         1.5130e-02, -2.5781e-03, -5.1546e-02, -6.0698e-03,  6.6683e-03,\n",
      "         1.2552e-02,  5.4163e-02,  5.1764e-02,  1.0138e-02, -3.9482e-02,\n",
      "         2.2954e-02, -1.1339e-04, -2.7770e-02, -8.4370e-03,  3.9592e-03,\n",
      "         5.6443e-02, -3.5415e-03, -4.6652e-02,  2.4140e-02, -5.5810e-02,\n",
      "         1.4963e-02,  5.6369e-02, -5.6375e-02, -3.0542e-02,  1.6681e-02,\n",
      "        -4.8742e-02,  5.7043e-02,  3.9054e-02, -2.4565e-02,  2.9031e-02,\n",
      "         6.0057e-02,  6.0953e-02,  4.6181e-02,  2.3626e-02, -1.9081e-02,\n",
      "         1.7149e-02, -2.4560e-02, -1.8849e-02, -3.0329e-02, -8.3072e-03,\n",
      "        -5.4669e-02, -3.8426e-02, -1.5942e-02, -4.9394e-02,  5.0898e-02,\n",
      "         9.3331e-04,  5.4507e-02,  4.3862e-02,  5.2544e-02, -3.6442e-02,\n",
      "         9.6842e-03], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0064,  0.0505,  0.0566,  ..., -0.0010,  0.0777,  0.0132],\n",
      "        [-0.0091, -0.0282,  0.0516,  ..., -0.0379, -0.0101,  0.0447],\n",
      "        [-0.0395, -0.0300,  0.0336,  ..., -0.0862,  0.0258,  0.0474],\n",
      "        ...,\n",
      "        [-0.0604,  0.0167,  0.0545,  ...,  0.0741,  0.0215,  0.0010],\n",
      "        [-0.0031, -0.0335,  0.0285,  ...,  0.0557,  0.0247,  0.0023],\n",
      "        [ 0.0656, -0.0824,  0.0321,  ..., -0.0362, -0.0302, -0.0555]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0570, -0.0682,  0.0143,  0.0075, -0.0731, -0.0015, -0.0159,  0.0346,\n",
      "         0.0405, -0.0080, -0.0318, -0.0045,  0.0765,  0.0197, -0.0064,  0.1099,\n",
      "         0.0593, -0.0158, -0.0944, -0.0503,  0.0466,  0.0135,  0.0829, -0.0767,\n",
      "         0.0213, -0.0032, -0.0201, -0.0343,  0.0406,  0.0284, -0.0614,  0.0342,\n",
      "        -0.0382,  0.0433, -0.0211, -0.0262,  0.0404, -0.0716,  0.0232, -0.0314,\n",
      "        -0.0882, -0.0315,  0.0438,  0.0672, -0.0695,  0.0557,  0.0410, -0.0257,\n",
      "        -0.1134,  0.0311,  0.0008,  0.0231, -0.0118,  0.0354, -0.0466, -0.0716,\n",
      "         0.0358,  0.0369,  0.0319,  0.0184, -0.0679,  0.0244, -0.0605, -0.0381,\n",
      "         0.0146, -0.0022,  0.0052, -0.0529,  0.0192, -0.0238, -0.0233, -0.0096,\n",
      "         0.0457,  0.0726, -0.0053, -0.0387,  0.0543,  0.1233,  0.1252,  0.0539,\n",
      "         0.0413, -0.0083, -0.0138, -0.0304, -0.0246,  0.0657,  0.0183, -0.0530,\n",
      "         0.0482, -0.0483,  0.0676, -0.0884,  0.0120,  0.0524,  0.0725,  0.0309,\n",
      "         0.0486,  0.0529,  0.0109,  0.0546,  0.0087,  0.0179,  0.0430, -0.0283,\n",
      "        -0.0369,  0.0073, -0.0863,  0.0626,  0.0122,  0.0268, -0.1179, -0.0817,\n",
      "         0.1038,  0.0657, -0.0610,  0.0246,  0.0311,  0.0508,  0.0692,  0.0453,\n",
      "         0.0786,  0.0544, -0.0252, -0.0788,  0.0252,  0.0542,  0.0933,  0.0027,\n",
      "        -0.0417, -0.0154, -0.0701,  0.0003,  0.0126, -0.0978, -0.0014, -0.0524,\n",
      "        -0.0083, -0.0672, -0.0220, -0.0053,  0.0146,  0.0204, -0.0600, -0.0116,\n",
      "        -0.0210, -0.0016,  0.0027, -0.0131, -0.1136, -0.0460,  0.0036, -0.0149,\n",
      "         0.0887, -0.0311,  0.0962, -0.0277,  0.0856, -0.0655, -0.0412, -0.0033,\n",
      "         0.0363, -0.0834,  0.0715, -0.0223, -0.0410, -0.0596,  0.0170, -0.0793,\n",
      "        -0.0011, -0.0058,  0.0602, -0.0159, -0.0484, -0.0040, -0.0357,  0.0799,\n",
      "         0.0861, -0.0032,  0.0843,  0.0703,  0.0403,  0.0186, -0.0224, -0.0107,\n",
      "        -0.0255,  0.0090, -0.1198, -0.0705, -0.0175,  0.0603,  0.0529,  0.0333,\n",
      "         0.0179, -0.0690,  0.0030, -0.1044,  0.0491, -0.0305,  0.0671,  0.0537,\n",
      "         0.0362,  0.0339,  0.0512,  0.0749, -0.0228, -0.0538,  0.0512,  0.0625,\n",
      "        -0.0007,  0.0564,  0.1020, -0.0386, -0.0709, -0.0733, -0.0491,  0.0551,\n",
      "        -0.0913, -0.0766,  0.0102,  0.0884, -0.0024, -0.0502, -0.0482,  0.0927,\n",
      "         0.0979,  0.0672, -0.0341,  0.0549,  0.0393, -0.0521, -0.0724, -0.0230,\n",
      "        -0.0080,  0.1095,  0.0116,  0.0242, -0.0043, -0.0864, -0.0196, -0.0490,\n",
      "        -0.0482,  0.0419, -0.0119,  0.0350, -0.0727, -0.0650,  0.0954,  0.0854,\n",
      "        -0.0803,  0.0958, -0.0480,  0.0151,  0.1057,  0.0429,  0.0015, -0.0151],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0569,  0.0030,  0.0106,  ..., -0.0303,  0.0726, -0.0339],\n",
      "        [ 0.0672, -0.0005,  0.0461,  ...,  0.0621,  0.0899,  0.0490],\n",
      "        [-0.0194,  0.0189,  0.0458,  ...,  0.0231,  0.0571,  0.0118],\n",
      "        ...,\n",
      "        [-0.0050,  0.0363,  0.0307,  ..., -0.0160, -0.0398,  0.0172],\n",
      "        [ 0.0041,  0.0170, -0.0122,  ...,  0.0338,  0.0030, -0.0037],\n",
      "        [ 0.0306,  0.0237, -0.0196,  ...,  0.0082,  0.0440, -0.0224]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0406,  0.0125,  0.0517, -0.0204, -0.0573,  0.0323,  0.0455,  0.0189,\n",
      "        -0.0264,  0.0188,  0.0258,  0.0481, -0.0045,  0.0038,  0.0339, -0.0227,\n",
      "        -0.0132, -0.0510, -0.0071,  0.0231,  0.0370, -0.0034,  0.0033, -0.0007,\n",
      "        -0.0144, -0.0254, -0.0389,  0.0592,  0.0210, -0.0014,  0.0082, -0.0467,\n",
      "         0.0206,  0.0534,  0.0204, -0.0356, -0.0132, -0.0120,  0.0089, -0.0467,\n",
      "         0.0572,  0.0585,  0.0561, -0.0486, -0.0326, -0.0260, -0.0342, -0.0486,\n",
      "        -0.0396,  0.0039, -0.0016, -0.0067,  0.0062,  0.0080,  0.0278, -0.0290,\n",
      "         0.0502,  0.0447, -0.0480,  0.0151, -0.0045,  0.0158, -0.0372, -0.0471,\n",
      "         0.0127, -0.0074, -0.0439,  0.0618,  0.0204, -0.0229, -0.0382, -0.0496,\n",
      "        -0.0321,  0.0361, -0.0050,  0.0233,  0.0145,  0.0241,  0.0235, -0.0429,\n",
      "        -0.0395, -0.0139, -0.0535,  0.0607, -0.0023, -0.0179,  0.0339, -0.0361,\n",
      "         0.0220, -0.0043, -0.0582,  0.0084, -0.0487,  0.0151, -0.0347, -0.0253,\n",
      "        -0.0102, -0.0338, -0.0265,  0.0488,  0.0032,  0.0147,  0.0531, -0.0471,\n",
      "        -0.0608,  0.0311,  0.0544, -0.0433,  0.0155, -0.0135,  0.0606,  0.0067,\n",
      "         0.0450,  0.0071,  0.0424,  0.0148,  0.0092, -0.0347, -0.0243,  0.0449,\n",
      "        -0.0251, -0.0234,  0.0619, -0.0302,  0.0570, -0.0502, -0.0100,  0.0212,\n",
      "         0.0380, -0.0312, -0.0128,  0.0446, -0.0342, -0.0339,  0.0236,  0.0397,\n",
      "         0.0105, -0.0184, -0.0526, -0.0506, -0.0363, -0.0010,  0.0257,  0.0255,\n",
      "         0.0417, -0.0072,  0.0032, -0.0324, -0.0191,  0.0218, -0.0458, -0.0268,\n",
      "        -0.0416,  0.0245,  0.0187,  0.0137, -0.0151,  0.0186,  0.0490,  0.0479,\n",
      "        -0.0213, -0.0249, -0.0121,  0.0110, -0.0156,  0.0134, -0.0055, -0.0347,\n",
      "        -0.0032,  0.0235, -0.0217, -0.0023, -0.0111,  0.0435,  0.0426,  0.0299,\n",
      "        -0.0347, -0.0088,  0.0616, -0.0338,  0.0560,  0.0099,  0.0126, -0.0364,\n",
      "        -0.0237, -0.0520,  0.0003,  0.0227, -0.0247,  0.0159, -0.0584,  0.0157,\n",
      "         0.0016, -0.0465, -0.0311, -0.0346, -0.0375, -0.0595,  0.0564,  0.0205,\n",
      "         0.0225,  0.0084, -0.0013,  0.0051,  0.0440,  0.0457, -0.0076,  0.0409,\n",
      "        -0.0463,  0.0193,  0.0574, -0.0012,  0.0432,  0.0513,  0.0072, -0.0036,\n",
      "        -0.0287,  0.0367,  0.0265,  0.0348,  0.0478,  0.0119,  0.0123, -0.0380,\n",
      "         0.0545,  0.0294, -0.0224,  0.0099,  0.0177,  0.0601, -0.0012, -0.0064,\n",
      "         0.0417, -0.0453, -0.0309,  0.0533,  0.0219,  0.0209,  0.0421, -0.0445,\n",
      "         0.0318,  0.0002, -0.0437,  0.0433,  0.0275,  0.0429,  0.0412,  0.0212,\n",
      "         0.0211,  0.0580, -0.0580, -0.0455,  0.0037,  0.0602,  0.0580, -0.0316],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.8030e-01, -5.1230e-01],\n",
      "        [-4.2350e-01, -3.0142e-01],\n",
      "        [-2.4808e-01, -6.9734e-01],\n",
      "        [-7.0260e-01,  1.3010e-01],\n",
      "        [-2.9458e-01,  1.4454e-01],\n",
      "        [-2.6273e-03,  5.3966e-02],\n",
      "        [ 7.0294e-03,  5.6257e-01],\n",
      "        [-4.2638e-01,  1.2001e-01],\n",
      "        [ 3.6640e-01,  1.8989e-02],\n",
      "        [-9.2295e-02,  1.1636e-01],\n",
      "        [ 3.9230e-01,  5.2546e-01],\n",
      "        [ 2.5815e-01,  2.3716e-01],\n",
      "        [-1.5109e-01,  1.4800e-01],\n",
      "        [ 6.6562e-02,  6.1424e-01],\n",
      "        [ 9.6763e-02, -4.5978e-01],\n",
      "        [-3.7010e-01,  5.9621e-02],\n",
      "        [-3.0456e-02,  5.7136e-01],\n",
      "        [-1.6527e-01, -2.2604e-01],\n",
      "        [ 2.8555e-01, -2.7221e-01],\n",
      "        [ 3.2933e-01,  2.6682e-01],\n",
      "        [-2.0264e-01, -1.4364e-01],\n",
      "        [ 1.8654e-02,  2.9038e-01],\n",
      "        [-3.6252e-01,  2.2726e-01],\n",
      "        [-7.4070e-02, -5.2576e-01],\n",
      "        [-1.4618e-01,  6.4912e-01],\n",
      "        [-3.9412e-01, -1.3948e-01],\n",
      "        [-6.4311e-01, -7.5631e-02],\n",
      "        [-1.7878e-01, -7.2064e-03],\n",
      "        [-4.4337e-01,  2.1088e-01],\n",
      "        [-1.1205e-01, -4.3791e-01],\n",
      "        [ 2.7905e-01, -2.1816e-01],\n",
      "        [ 1.4890e-01, -7.1257e-02],\n",
      "        [-1.9292e-01,  4.7414e-01],\n",
      "        [-4.9392e-01, -4.9265e-01],\n",
      "        [-3.2716e-01, -1.3898e-02],\n",
      "        [-6.8229e-01, -6.6107e-01],\n",
      "        [-6.3434e-01,  2.8568e-01],\n",
      "        [-1.9540e-01, -1.7496e-01],\n",
      "        [ 5.0583e-01,  2.7725e-01],\n",
      "        [-9.9118e-02, -1.8474e-01],\n",
      "        [ 2.6084e-01, -2.8016e-01],\n",
      "        [-4.5084e-01,  5.3756e-01],\n",
      "        [ 3.6046e-01, -5.2246e-02],\n",
      "        [ 3.0988e-01,  3.7681e-01],\n",
      "        [ 2.9199e-01, -9.8999e-02],\n",
      "        [-2.9754e-01,  1.7893e-01],\n",
      "        [-3.5146e-02, -1.0611e-01],\n",
      "        [-1.1398e-02, -3.1325e-01],\n",
      "        [ 7.1533e-01, -4.4412e-01],\n",
      "        [-1.5843e-01, -5.6588e-01],\n",
      "        [-2.9045e-03,  2.4615e-02],\n",
      "        [ 5.5637e-01,  3.6518e-01],\n",
      "        [ 4.6629e-01, -1.7760e-01],\n",
      "        [ 3.6124e-01,  3.1808e-01],\n",
      "        [-2.4432e-01, -3.6099e-01],\n",
      "        [ 5.4499e-01, -3.2142e-01],\n",
      "        [-1.9190e-01, -1.2378e-01],\n",
      "        [-4.9078e-01,  4.6307e-01],\n",
      "        [ 4.8644e-01,  1.2438e-01],\n",
      "        [ 4.0469e-01,  3.3373e-01],\n",
      "        [-1.7316e-01, -4.6208e-01],\n",
      "        [-5.5396e-01,  6.3311e-01],\n",
      "        [-5.3171e-01, -2.8587e-02],\n",
      "        [ 5.2073e-01, -6.1032e-01],\n",
      "        [-1.4811e-01, -3.4416e-01],\n",
      "        [-3.3118e-01,  7.4116e-02],\n",
      "        [-3.3151e-02, -6.2040e-01],\n",
      "        [-6.9551e-01,  2.9160e-01],\n",
      "        [-4.1706e-01, -5.1592e-01],\n",
      "        [-4.5279e-01,  3.5648e-01],\n",
      "        [-2.7998e-01,  1.1917e-01],\n",
      "        [ 8.4358e-02,  2.2399e-01],\n",
      "        [ 4.1722e-01,  4.5355e-02],\n",
      "        [-6.6263e-01,  2.2108e-01],\n",
      "        [ 8.1029e-02,  1.0146e-01],\n",
      "        [-6.5665e-01, -3.8794e-02],\n",
      "        [ 2.5386e-01,  2.9049e-01],\n",
      "        [ 4.5367e-01,  2.2382e-01],\n",
      "        [-7.0829e-01,  1.9400e-01],\n",
      "        [ 6.3622e-01,  1.5062e-01],\n",
      "        [ 6.6544e-01,  4.3017e-01],\n",
      "        [ 4.5787e-01,  5.2971e-01],\n",
      "        [-3.0126e-02,  4.1330e-01],\n",
      "        [-3.7574e-01,  5.9307e-01],\n",
      "        [ 2.8325e-01, -1.3861e-01],\n",
      "        [-4.5151e-01,  1.9497e-01],\n",
      "        [-3.0442e-01,  1.1464e-01],\n",
      "        [-1.7391e-01,  4.4679e-01],\n",
      "        [ 4.3044e-01,  6.5503e-01],\n",
      "        [-6.8802e-01, -1.1043e-01],\n",
      "        [-5.8485e-01,  3.5071e-01],\n",
      "        [ 5.6005e-02, -4.9941e-01],\n",
      "        [-4.6999e-02,  4.2588e-01],\n",
      "        [-2.2860e-01,  2.8330e-02],\n",
      "        [ 2.1573e-01,  5.4584e-01],\n",
      "        [-5.5577e-01,  1.9548e-01],\n",
      "        [-6.0107e-01, -6.0732e-01],\n",
      "        [-1.5253e-01,  4.7443e-01],\n",
      "        [-2.4625e-01, -5.0768e-01],\n",
      "        [-1.2506e-01,  6.2258e-01],\n",
      "        [ 4.2479e-01, -2.1616e-02],\n",
      "        [ 2.8786e-01, -4.7213e-01],\n",
      "        [ 3.0814e-01,  7.2205e-02],\n",
      "        [ 3.5161e-01, -2.9278e-01],\n",
      "        [ 6.4919e-01, -4.7528e-01],\n",
      "        [-1.9628e-01, -4.6085e-01],\n",
      "        [ 6.6639e-01, -4.7769e-01],\n",
      "        [-2.4644e-01,  2.3144e-01],\n",
      "        [ 3.0441e-01, -3.9394e-02],\n",
      "        [-5.8873e-01,  4.0925e-01],\n",
      "        [-3.3305e-01, -2.0888e-01],\n",
      "        [-2.9009e-01, -5.9197e-01],\n",
      "        [-4.0375e-01,  5.3793e-01],\n",
      "        [-9.3304e-02,  3.8531e-01],\n",
      "        [ 6.6939e-01, -3.8299e-01],\n",
      "        [-6.4707e-01, -5.2943e-01],\n",
      "        [-3.9985e-01, -2.1696e-01],\n",
      "        [-1.2579e-01, -1.8445e-01],\n",
      "        [ 3.8408e-01,  5.3877e-01],\n",
      "        [-3.3792e-01, -5.8572e-01],\n",
      "        [ 1.8199e-01,  8.1621e-02],\n",
      "        [ 1.6207e-01,  5.8654e-01],\n",
      "        [-2.4001e-01,  6.7495e-01],\n",
      "        [ 2.1536e-01, -5.1247e-01],\n",
      "        [ 2.3395e-02,  2.8689e-01],\n",
      "        [-2.2506e-01, -1.2350e-01],\n",
      "        [-4.9853e-01,  4.1836e-01],\n",
      "        [-1.3971e-01, -9.7564e-02],\n",
      "        [ 6.7691e-01,  5.4934e-01],\n",
      "        [-5.3472e-01,  2.0166e-01],\n",
      "        [-6.7007e-01, -6.1104e-01],\n",
      "        [ 6.7871e-01,  5.2936e-01],\n",
      "        [ 3.1153e-02,  2.9873e-01],\n",
      "        [ 2.1591e-01, -3.6634e-01],\n",
      "        [-4.0879e-01,  3.6601e-01],\n",
      "        [ 6.0524e-01, -4.8844e-01],\n",
      "        [ 6.9498e-01, -4.2822e-01],\n",
      "        [-4.0284e-01,  3.1861e-01],\n",
      "        [-3.2857e-01, -1.5353e-01],\n",
      "        [-4.2893e-03, -1.3425e-02],\n",
      "        [ 2.2819e-01, -2.2029e-01],\n",
      "        [-2.2105e-02,  6.0745e-01],\n",
      "        [-4.2167e-01, -3.0373e-01],\n",
      "        [-4.8341e-01,  1.4989e-01],\n",
      "        [-4.7396e-01,  3.9018e-01],\n",
      "        [-1.4012e-01,  5.7672e-01],\n",
      "        [ 2.8017e-01,  5.8933e-01],\n",
      "        [ 9.0349e-02, -8.4127e-02],\n",
      "        [ 1.3460e-01, -5.8385e-01],\n",
      "        [ 1.7164e-01, -1.3152e-01],\n",
      "        [ 2.6209e-01,  7.4169e-02],\n",
      "        [ 5.7579e-01,  1.0357e-01],\n",
      "        [-6.3479e-02,  1.6902e-02],\n",
      "        [-3.9619e-01, -1.1804e-01],\n",
      "        [ 2.1493e-01,  6.2020e-01],\n",
      "        [ 1.3296e-01,  1.0023e-01],\n",
      "        [-1.4817e-01,  3.8743e-01],\n",
      "        [-3.5641e-01, -2.1712e-01],\n",
      "        [ 3.1785e-01,  2.8624e-01],\n",
      "        [-6.8089e-01,  4.3602e-01],\n",
      "        [-3.4775e-01,  2.7579e-01],\n",
      "        [ 4.3048e-01, -4.4538e-01],\n",
      "        [-5.1249e-01,  6.5443e-01],\n",
      "        [ 5.0083e-01,  6.1396e-01],\n",
      "        [ 3.7841e-01, -6.3884e-01],\n",
      "        [ 4.0044e-01, -1.1008e-01],\n",
      "        [-6.0673e-02,  5.6983e-01],\n",
      "        [ 1.5826e-01, -3.5563e-01],\n",
      "        [ 4.7327e-02, -5.6331e-01],\n",
      "        [ 4.8231e-01,  2.3265e-01],\n",
      "        [ 1.9499e-01,  7.9008e-02],\n",
      "        [-3.8958e-01,  4.6010e-01],\n",
      "        [-1.8233e-02,  4.6416e-01],\n",
      "        [ 9.4095e-02, -5.2713e-02],\n",
      "        [-2.1421e-01,  3.9595e-01],\n",
      "        [ 2.0660e-01,  1.4951e-01],\n",
      "        [-5.3057e-01,  3.3069e-01],\n",
      "        [-5.7295e-01, -1.7734e-01],\n",
      "        [ 5.8764e-01,  2.8418e-02],\n",
      "        [-6.4678e-01,  8.5007e-02],\n",
      "        [-4.1468e-01, -7.5240e-02],\n",
      "        [-4.0158e-01, -1.9740e-01],\n",
      "        [-2.4152e-01,  1.4065e-01],\n",
      "        [ 6.5170e-01, -2.2790e-02],\n",
      "        [-1.7933e-01,  5.0762e-02],\n",
      "        [ 2.4775e-01, -4.0217e-01],\n",
      "        [ 5.4626e-01, -1.3876e-01],\n",
      "        [-2.1622e-01, -1.9002e-01],\n",
      "        [-2.2974e-01, -4.8395e-01],\n",
      "        [-4.1912e-01, -3.2959e-01],\n",
      "        [ 6.7197e-01,  4.5777e-01],\n",
      "        [ 3.3167e-01, -4.5184e-01],\n",
      "        [ 2.4148e-04, -3.0315e-01],\n",
      "        [-9.4485e-02, -2.1728e-01],\n",
      "        [-1.9461e-01, -1.4553e-01],\n",
      "        [ 3.5121e-01, -5.3947e-01],\n",
      "        [ 5.9690e-01,  4.0525e-01],\n",
      "        [ 1.7985e-02, -3.8221e-01],\n",
      "        [ 4.7742e-01,  4.9081e-01],\n",
      "        [ 3.4529e-01, -2.7355e-01],\n",
      "        [-6.7352e-01, -2.5117e-02],\n",
      "        [ 1.3556e-01, -5.2813e-01],\n",
      "        [-5.0551e-02,  1.7929e-01],\n",
      "        [-7.4315e-01,  6.1580e-01],\n",
      "        [-5.9336e-01, -2.7361e-01],\n",
      "        [-3.0061e-01,  1.4168e-01],\n",
      "        [ 1.8545e-04,  4.8041e-01],\n",
      "        [ 3.9177e-01,  2.4962e-01],\n",
      "        [ 4.6073e-01,  5.8538e-01],\n",
      "        [ 4.5910e-01,  3.2681e-02],\n",
      "        [-6.8666e-01,  6.1921e-01],\n",
      "        [ 5.3499e-01,  7.7439e-02],\n",
      "        [-5.6921e-01,  2.4562e-01],\n",
      "        [-2.7206e-01,  9.0446e-02],\n",
      "        [-9.3326e-02,  2.8116e-01],\n",
      "        [ 3.4148e-01, -1.1533e-02],\n",
      "        [-2.2871e-02, -6.2891e-01],\n",
      "        [-2.6240e-01, -2.7665e-01],\n",
      "        [ 1.1403e-02,  5.5886e-01],\n",
      "        [-5.8866e-01,  3.6269e-01],\n",
      "        [-5.5847e-01,  8.9073e-02],\n",
      "        [-2.1760e-01, -7.1136e-02],\n",
      "        [-6.0226e-01, -6.1567e-01],\n",
      "        [ 4.1637e-01, -1.0435e-01],\n",
      "        [ 1.1395e-01,  1.8996e-01],\n",
      "        [-3.1438e-01, -3.3452e-01],\n",
      "        [-3.9487e-01,  3.3636e-01],\n",
      "        [-4.4849e-01,  5.5912e-01],\n",
      "        [-3.6864e-01, -4.5518e-01],\n",
      "        [ 2.0333e-01,  5.9376e-01],\n",
      "        [ 5.4922e-01,  7.0504e-02],\n",
      "        [ 5.4695e-01, -3.8181e-01],\n",
      "        [-5.0588e-01, -3.9374e-03],\n",
      "        [-7.0596e-01,  3.1784e-01],\n",
      "        [ 2.8516e-01, -3.3336e-01],\n",
      "        [ 5.5932e-01,  2.3115e-01],\n",
      "        [-4.7462e-01,  5.7098e-01],\n",
      "        [ 2.9850e-01,  9.8861e-02],\n",
      "        [ 3.1736e-01,  2.0465e-01],\n",
      "        [-3.9319e-01,  5.6830e-01],\n",
      "        [-5.9424e-01, -1.1456e-01],\n",
      "        [ 5.6465e-02,  4.1902e-01],\n",
      "        [ 7.2592e-01, -2.8832e-01],\n",
      "        [-6.2336e-01,  1.8290e-01],\n",
      "        [-6.1849e-01,  4.0546e-01],\n",
      "        [ 3.4088e-01,  1.3412e-01],\n",
      "        [-1.5447e-01,  6.6212e-02],\n",
      "        [-6.2136e-03,  6.0408e-01],\n",
      "        [ 3.7061e-01, -4.0323e-01],\n",
      "        [ 2.0883e-01,  1.8043e-01],\n",
      "        [ 7.3038e-02, -1.0135e-01],\n",
      "        [ 2.7489e-01,  3.6791e-01],\n",
      "        [-5.4975e-01,  1.5799e-01],\n",
      "        [ 2.1658e-01,  6.4568e-01],\n",
      "        [ 3.3182e-02,  2.5938e-01],\n",
      "        [ 2.2986e-01,  6.4828e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0517,  0.0203,  0.0549,  ..., -0.0054,  0.0211, -0.0397],\n",
      "        [-0.0349, -0.0819, -0.0188,  ...,  0.0260, -0.0333, -0.0187],\n",
      "        [-0.0111,  0.0520, -0.0338,  ...,  0.0658, -0.0096, -0.0113],\n",
      "        ...,\n",
      "        [ 0.0330,  0.0043, -0.0178,  ..., -0.0014,  0.0651,  0.0700],\n",
      "        [ 0.0238,  0.0169,  0.0255,  ...,  0.0414,  0.0228, -0.0195],\n",
      "        [-0.0442, -0.0638,  0.0744,  ...,  0.0738, -0.0046,  0.0739]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0165, -0.0198,  0.0359, -0.0398, -0.0078, -0.0290, -0.0385, -0.0471,\n",
      "        -0.0242,  0.0188,  0.0009,  0.0284,  0.0604,  0.0452,  0.0134, -0.0446,\n",
      "        -0.0201,  0.0004,  0.0094, -0.0118,  0.0248,  0.0438,  0.0278,  0.0417,\n",
      "        -0.0582,  0.0572, -0.0554,  0.0007, -0.0343,  0.0175,  0.0383,  0.0089,\n",
      "         0.0149,  0.0308,  0.0066,  0.0502,  0.0129, -0.0411, -0.0080, -0.0137,\n",
      "         0.0356, -0.0234,  0.0376,  0.0410, -0.0164, -0.0069, -0.0039, -0.0435,\n",
      "        -0.0437,  0.0582,  0.0252,  0.0232,  0.0422, -0.0140, -0.0497,  0.0477,\n",
      "        -0.0381,  0.0015, -0.0339,  0.0461,  0.0272,  0.0019, -0.0595, -0.0280,\n",
      "        -0.0511, -0.0449, -0.0616,  0.0471,  0.0383,  0.0269,  0.0601, -0.0454,\n",
      "         0.0064,  0.0601, -0.0387,  0.0069,  0.0601, -0.0180,  0.0612, -0.0144,\n",
      "        -0.0502, -0.0158,  0.0031,  0.0526, -0.0317,  0.0342,  0.0380, -0.0111,\n",
      "        -0.0506, -0.0378,  0.0043, -0.0311,  0.0501, -0.0012,  0.0267, -0.0305,\n",
      "        -0.0454,  0.0148, -0.0378,  0.0360, -0.0009, -0.0069, -0.0519, -0.0590,\n",
      "         0.0173, -0.0390,  0.0481, -0.0486,  0.0413, -0.0013,  0.0105, -0.0580,\n",
      "        -0.0287, -0.0252, -0.0448, -0.0464,  0.0311,  0.0503, -0.0025,  0.0063,\n",
      "        -0.0172,  0.0215,  0.0138,  0.0442,  0.0017, -0.0128, -0.0137, -0.0379,\n",
      "         0.0032, -0.0116,  0.0495,  0.0149,  0.0461,  0.0529,  0.0257,  0.0370,\n",
      "         0.0201,  0.0364,  0.0612,  0.0245, -0.0427,  0.0298, -0.0464,  0.0093,\n",
      "         0.0147, -0.0072,  0.0288, -0.0356, -0.0034, -0.0181,  0.0180,  0.0405,\n",
      "         0.0167, -0.0042, -0.0342,  0.0101, -0.0472,  0.0459, -0.0079,  0.0640,\n",
      "         0.0449, -0.0554,  0.0588,  0.0055, -0.0261,  0.0621,  0.0468, -0.0255,\n",
      "        -0.0367, -0.0174, -0.0287,  0.0263, -0.0392, -0.0147, -0.0478,  0.0607,\n",
      "         0.0081,  0.0161, -0.0268,  0.0013,  0.0430,  0.0357,  0.0593, -0.0357,\n",
      "        -0.0365,  0.0405, -0.0408,  0.0601,  0.0342, -0.0355,  0.0207,  0.0437,\n",
      "         0.0504,  0.0231, -0.0404, -0.0472, -0.0122,  0.0372, -0.0311, -0.0220,\n",
      "        -0.0033, -0.0548,  0.0271,  0.0198, -0.0180, -0.0317,  0.0044,  0.0315,\n",
      "         0.0415,  0.0303,  0.0439,  0.0379, -0.0264,  0.0178, -0.0370,  0.0374,\n",
      "         0.0177, -0.0499, -0.0441, -0.0033,  0.0179,  0.0116, -0.0415,  0.0163,\n",
      "         0.0098, -0.0210, -0.0578, -0.0591,  0.0213,  0.0381,  0.0382,  0.0624,\n",
      "         0.0003, -0.0453, -0.0578, -0.0463, -0.0535, -0.0338, -0.0151,  0.0606,\n",
      "        -0.0092,  0.0098,  0.0544,  0.0215,  0.0289, -0.0259, -0.0333, -0.0294,\n",
      "        -0.0170,  0.0271,  0.0234,  0.0465,  0.0350, -0.0120,  0.0326, -0.0087],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0635,  0.0645,  0.0240,  ..., -0.0036,  0.0155, -0.0585],\n",
      "        [-0.0017, -0.0661,  0.0072,  ...,  0.0205,  0.0434, -0.0356],\n",
      "        [ 0.0063,  0.0383,  0.0549,  ..., -0.0467,  0.0405, -0.0651],\n",
      "        ...,\n",
      "        [ 0.0642, -0.0373,  0.0369,  ...,  0.0209,  0.0017,  0.0596],\n",
      "        [-0.0337,  0.0462,  0.0184,  ...,  0.0198,  0.0879, -0.0410],\n",
      "        [-0.0083, -0.0578,  0.0288,  ...,  0.0516,  0.0022,  0.0093]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0552,  0.0118, -0.0077,  0.0413, -0.0492, -0.0403, -0.0327, -0.0049,\n",
      "         0.0491,  0.0098, -0.0370,  0.0143, -0.0020, -0.0355, -0.0090,  0.0359,\n",
      "         0.0119,  0.0286, -0.0442, -0.0563, -0.0492,  0.0573,  0.0371, -0.0132,\n",
      "         0.0226, -0.0293, -0.0194,  0.0316, -0.0470, -0.0077,  0.0215,  0.0530,\n",
      "         0.0372, -0.0625,  0.0256, -0.0013,  0.0129, -0.0010, -0.0199,  0.0289,\n",
      "        -0.0091, -0.0092,  0.0588,  0.0010,  0.0186,  0.0551,  0.0225,  0.0294,\n",
      "        -0.0541, -0.0032, -0.0139,  0.0516, -0.0565,  0.0499, -0.0485,  0.0308,\n",
      "        -0.0129, -0.0093, -0.0148, -0.0434, -0.0426, -0.0202, -0.0477,  0.0382,\n",
      "         0.0204, -0.0551, -0.0579,  0.0161, -0.0351,  0.0032, -0.0447, -0.0404,\n",
      "        -0.0575,  0.0190, -0.0466, -0.0185,  0.0091,  0.0047,  0.0524,  0.0127,\n",
      "         0.0479, -0.0340, -0.0567,  0.0053, -0.0361, -0.0585,  0.0563,  0.0092,\n",
      "         0.0375,  0.0418, -0.0074, -0.0520,  0.0463,  0.0245, -0.0419,  0.0273,\n",
      "        -0.0113, -0.0114,  0.0300,  0.0499, -0.0438,  0.0546, -0.0358,  0.0088,\n",
      "        -0.0535, -0.0077,  0.0542,  0.0483,  0.0218, -0.0036,  0.0616,  0.0152,\n",
      "         0.0138, -0.0091,  0.0056, -0.0325, -0.0231,  0.0344, -0.0189, -0.0348,\n",
      "        -0.0189,  0.0088,  0.0493, -0.0343,  0.0222,  0.0226, -0.0037, -0.0202,\n",
      "        -0.0143, -0.0439, -0.0088,  0.0617, -0.0004,  0.0596,  0.0271, -0.0064,\n",
      "        -0.0456,  0.0572,  0.0073, -0.0093,  0.0223, -0.0408, -0.0455, -0.0518,\n",
      "         0.0494, -0.0567,  0.0329,  0.0004,  0.0530,  0.0542, -0.0010,  0.0027,\n",
      "         0.0087,  0.0021, -0.0480, -0.0517, -0.0242,  0.0272,  0.0304, -0.0277,\n",
      "         0.0456,  0.0555, -0.0123, -0.0312, -0.0557,  0.0103,  0.0048, -0.0454,\n",
      "         0.0338, -0.0583,  0.0558,  0.0384,  0.0169,  0.0505,  0.0275,  0.0231,\n",
      "         0.0194, -0.0467,  0.0147,  0.0342, -0.0024, -0.0495, -0.0491,  0.0046,\n",
      "        -0.0204,  0.0477,  0.0321, -0.0451,  0.0287,  0.0377,  0.0282, -0.0412,\n",
      "         0.0031, -0.0065,  0.0192,  0.0300, -0.0084, -0.0256, -0.0614,  0.0221,\n",
      "        -0.0151, -0.0435,  0.0516, -0.0581,  0.0479, -0.0193, -0.0162,  0.0478,\n",
      "        -0.0093, -0.0306, -0.0189,  0.0092, -0.0586,  0.0388,  0.0055, -0.0005,\n",
      "        -0.0404, -0.0080, -0.0504,  0.0083,  0.0063, -0.0522,  0.0161, -0.0416,\n",
      "         0.0572, -0.0299, -0.0208, -0.0065, -0.0525, -0.0283, -0.0380, -0.0191,\n",
      "         0.0159,  0.0251,  0.0560,  0.0263, -0.0477, -0.0286,  0.0400, -0.0193,\n",
      "        -0.0336,  0.0564, -0.0350, -0.0020, -0.0461, -0.0334, -0.0296, -0.0217,\n",
      "        -0.0075,  0.0459,  0.0198, -0.0353,  0.0127,  0.0604, -0.0220,  0.0060],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0174, -0.0561, -0.0414,  ..., -0.0355, -0.0626, -0.0367],\n",
      "        [-0.0435,  0.0009, -0.0795,  ...,  0.0089,  0.0275,  0.0221],\n",
      "        [-0.0211,  0.0518,  0.0273,  ...,  0.0652,  0.0100,  0.0121],\n",
      "        ...,\n",
      "        [-0.0058,  0.0576, -0.0427,  ...,  0.0008, -0.0145,  0.0117],\n",
      "        [-0.0542, -0.0526,  0.0164,  ...,  0.0290, -0.0192, -0.0106],\n",
      "        [-0.0482,  0.0537, -0.0706,  ...,  0.0310,  0.0100,  0.0482]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0195, -0.0541,  0.0382, -0.0066, -0.0410,  0.0428, -0.0162,  0.0341,\n",
      "         0.0636, -0.0227, -0.0392, -0.0513, -0.0248,  0.0988,  0.1011,  0.0288,\n",
      "         0.0244, -0.0244,  0.0595, -0.0379,  0.0497, -0.0379, -0.0521, -0.0237,\n",
      "         0.0325, -0.0382, -0.0124,  0.0570,  0.0537,  0.0319,  0.0496,  0.0604,\n",
      "         0.0483, -0.0422, -0.0206, -0.0137, -0.0106, -0.0309, -0.0761, -0.0474,\n",
      "        -0.0667,  0.0413, -0.0611,  0.0167,  0.0374,  0.0224, -0.0381, -0.0577,\n",
      "        -0.0598, -0.0430,  0.0215, -0.0230,  0.0346, -0.0043,  0.0073,  0.0807,\n",
      "         0.0255, -0.0579,  0.0761,  0.0043,  0.0345,  0.0537, -0.0695, -0.0585,\n",
      "        -0.0047,  0.0573,  0.0631,  0.0618,  0.0552, -0.0646, -0.0051, -0.0192,\n",
      "         0.0544,  0.0547,  0.0299,  0.0564,  0.0746,  0.0342,  0.0141,  0.0261,\n",
      "         0.0061, -0.0256, -0.0505, -0.0327,  0.0402,  0.0207, -0.0164, -0.0005,\n",
      "        -0.0615,  0.0559,  0.0311,  0.0227, -0.0022,  0.0240,  0.0249, -0.0371,\n",
      "         0.0333, -0.0298,  0.0115,  0.0627,  0.0445, -0.0063,  0.0238,  0.0129,\n",
      "         0.1057, -0.0351,  0.0065,  0.0118, -0.0374,  0.0412, -0.0362, -0.0430,\n",
      "         0.0487,  0.0195,  0.0551,  0.0285, -0.0469,  0.0534, -0.0514,  0.0230,\n",
      "         0.0205, -0.0280,  0.0252,  0.0525,  0.0068,  0.0397, -0.0416, -0.0175,\n",
      "        -0.0155,  0.0557,  0.0556, -0.0345,  0.0292,  0.0152,  0.0081,  0.0122,\n",
      "        -0.0058, -0.0234, -0.0149, -0.0410,  0.0070, -0.0123,  0.0480, -0.0227,\n",
      "         0.0495,  0.0472, -0.0784,  0.0005, -0.0068,  0.0392, -0.0869,  0.0706,\n",
      "        -0.0404,  0.0466,  0.0227,  0.0105, -0.0139, -0.0326, -0.0029, -0.0520,\n",
      "         0.0549, -0.0010, -0.0426,  0.0118,  0.0206,  0.0244,  0.0788, -0.0436,\n",
      "         0.0106, -0.0594, -0.0025, -0.0062, -0.0080, -0.0418,  0.0191, -0.0275,\n",
      "        -0.0636, -0.0219, -0.0421,  0.0145,  0.0199, -0.0896, -0.0871, -0.0660,\n",
      "         0.0971, -0.0558, -0.0195,  0.0280, -0.0456, -0.0445, -0.0360,  0.0316,\n",
      "         0.0872,  0.0217,  0.0261,  0.0405,  0.0589, -0.0803,  0.0390,  0.0247,\n",
      "         0.0225, -0.0415,  0.0418,  0.0115, -0.0648,  0.0212, -0.0144,  0.0509,\n",
      "        -0.0459,  0.0267,  0.0187,  0.0731,  0.0176, -0.0281,  0.0202,  0.0602,\n",
      "         0.0565, -0.0409,  0.0378,  0.0085, -0.0277, -0.0114, -0.0576,  0.0635,\n",
      "         0.0045,  0.0139, -0.0208, -0.0066,  0.0741,  0.0123,  0.0151,  0.0330,\n",
      "         0.0464, -0.0357, -0.0037, -0.0308, -0.0680, -0.0169, -0.0483,  0.0110,\n",
      "        -0.0192,  0.0788,  0.0500, -0.0411,  0.0595,  0.0563,  0.0533,  0.0124,\n",
      "        -0.0164,  0.0198, -0.0225, -0.0097,  0.0448,  0.0300, -0.0812,  0.0042],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0305, -0.0184, -0.0138,  ...,  0.0048,  0.0584, -0.0913],\n",
      "        [-0.0829,  0.0033,  0.0489,  ...,  0.0060, -0.0186, -0.0140],\n",
      "        [ 0.0420, -0.0338,  0.0872,  ..., -0.0054,  0.0004,  0.0440],\n",
      "        ...,\n",
      "        [ 0.0499, -0.0122,  0.0184,  ...,  0.0421,  0.0231,  0.0168],\n",
      "        [ 0.0758, -0.0099,  0.0329,  ..., -0.0344,  0.0105,  0.0064],\n",
      "        [ 0.0293,  0.0406, -0.0250,  ...,  0.0273, -0.0832,  0.0422]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0612, -0.0424, -0.0449, -0.0436, -0.0403, -0.0161, -0.0337,  0.0127,\n",
      "         0.0497,  0.0204,  0.0187, -0.0608, -0.0213, -0.0461,  0.0132,  0.0353,\n",
      "        -0.0112,  0.0070,  0.0183, -0.0048,  0.0339,  0.0159, -0.0325, -0.0310,\n",
      "         0.0258,  0.0043, -0.0270, -0.0019, -0.0233,  0.0101,  0.0242,  0.0316,\n",
      "         0.0346, -0.0033, -0.0553, -0.0188,  0.0125,  0.0532,  0.0096,  0.0481,\n",
      "        -0.0271,  0.0290, -0.0370, -0.0403,  0.0328,  0.0466, -0.0488,  0.0506,\n",
      "        -0.0301,  0.0206, -0.0392, -0.0189, -0.0188, -0.0191, -0.0046,  0.0225,\n",
      "         0.0161, -0.0018, -0.0481,  0.0528,  0.0011,  0.0610,  0.0511, -0.0530,\n",
      "        -0.0270,  0.0033, -0.0471,  0.0613, -0.0520, -0.0387,  0.0063, -0.0002,\n",
      "        -0.0388,  0.0586, -0.0067,  0.0581,  0.0210, -0.0277,  0.0054, -0.0384,\n",
      "         0.0496, -0.0183, -0.0449,  0.0102, -0.0021,  0.0606,  0.0470, -0.0146,\n",
      "         0.0622,  0.0289, -0.0296, -0.0285, -0.0583,  0.0205, -0.0422,  0.0264,\n",
      "         0.0419, -0.0265,  0.0188, -0.0142,  0.0027,  0.0148,  0.0528,  0.0265,\n",
      "         0.0096,  0.0370,  0.0159, -0.0024,  0.0497,  0.0332, -0.0042, -0.0450,\n",
      "         0.0242,  0.0286, -0.0249, -0.0528, -0.0591,  0.0236,  0.0408,  0.0222,\n",
      "         0.0505, -0.0087, -0.0122,  0.0267,  0.0495, -0.0133, -0.0428,  0.0049,\n",
      "        -0.0344,  0.0339,  0.0619,  0.0065,  0.0139,  0.0620, -0.0132,  0.0052,\n",
      "        -0.0078, -0.0466,  0.0568,  0.0622,  0.0153, -0.0537, -0.0180,  0.0141,\n",
      "         0.0077, -0.0083,  0.0078, -0.0544,  0.0316, -0.0119, -0.0482,  0.0098,\n",
      "         0.0189,  0.0563,  0.0208, -0.0328,  0.0088,  0.0591, -0.0077, -0.0575,\n",
      "         0.0046,  0.0610,  0.0070,  0.0098,  0.0107, -0.0126,  0.0281,  0.0410,\n",
      "        -0.0370,  0.0014,  0.0035,  0.0113,  0.0524, -0.0091, -0.0401, -0.0558,\n",
      "        -0.0213,  0.0434, -0.0261,  0.0413, -0.0160,  0.0388, -0.0275,  0.0545,\n",
      "         0.0215, -0.0017,  0.0476,  0.0149, -0.0167,  0.0251,  0.0508, -0.0282,\n",
      "        -0.0016,  0.0039, -0.0245,  0.0386, -0.0353, -0.0148, -0.0405,  0.0491,\n",
      "        -0.0451,  0.0301, -0.0351,  0.0297, -0.0147,  0.0122, -0.0221, -0.0418,\n",
      "        -0.0268,  0.0185,  0.0437, -0.0388,  0.0258, -0.0373,  0.0274, -0.0339,\n",
      "         0.0514, -0.0100,  0.0214,  0.0217, -0.0128,  0.0155,  0.0063, -0.0278,\n",
      "         0.0045, -0.0050,  0.0068, -0.0086,  0.0540,  0.0428,  0.0198, -0.0175,\n",
      "        -0.0184, -0.0058,  0.0230, -0.0060,  0.0198,  0.0417, -0.0532, -0.0234,\n",
      "         0.0305, -0.0462, -0.0361,  0.0144,  0.0574,  0.0370, -0.0377, -0.0113,\n",
      "         0.0104,  0.0098,  0.0473,  0.0583,  0.0085, -0.0445, -0.0614,  0.0324],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4011, -0.1780],\n",
      "        [ 0.4609, -0.2538],\n",
      "        [-0.0655,  0.2702],\n",
      "        [ 0.6603,  0.4463],\n",
      "        [ 0.6705,  0.4379],\n",
      "        [ 0.1013, -0.1296],\n",
      "        [-0.6825, -0.3158],\n",
      "        [ 0.1222,  0.5955],\n",
      "        [-0.6387,  0.4100],\n",
      "        [-0.2146, -0.5669],\n",
      "        [-0.0516,  0.1598],\n",
      "        [ 0.0452, -0.0009],\n",
      "        [ 0.4967, -0.1114],\n",
      "        [-0.7342,  0.4862],\n",
      "        [-0.3569,  0.0832],\n",
      "        [-0.2590, -0.3602],\n",
      "        [-0.0899,  0.2655],\n",
      "        [ 0.7089, -0.2550],\n",
      "        [-0.2995,  0.3333],\n",
      "        [ 0.3736,  0.5091],\n",
      "        [-0.4003,  0.2191],\n",
      "        [ 0.1613,  0.1548],\n",
      "        [ 0.5892, -0.3426],\n",
      "        [-0.0012,  0.1975],\n",
      "        [-0.0020, -0.5255],\n",
      "        [-0.3846,  0.3055],\n",
      "        [-0.2087,  0.2346],\n",
      "        [ 0.2051,  0.4242],\n",
      "        [ 0.2839, -0.2065],\n",
      "        [-0.0665, -0.3321],\n",
      "        [ 0.5834,  0.3805],\n",
      "        [ 0.0278, -0.4599],\n",
      "        [ 0.2596, -0.5025],\n",
      "        [-0.3619, -0.3376],\n",
      "        [ 0.5846, -0.0014],\n",
      "        [ 0.1765, -0.6132],\n",
      "        [-0.2706,  0.4413],\n",
      "        [-0.6346, -0.0800],\n",
      "        [ 0.2130, -0.0439],\n",
      "        [-0.6396,  0.2327],\n",
      "        [-0.1188,  0.4352],\n",
      "        [-0.6356,  0.3196],\n",
      "        [-0.3836,  0.4032],\n",
      "        [ 0.2224,  0.4999],\n",
      "        [-0.2725, -0.4583],\n",
      "        [-0.4777, -0.3224],\n",
      "        [-0.1006, -0.6596],\n",
      "        [-0.0854, -0.0919],\n",
      "        [ 0.2152,  0.1142],\n",
      "        [ 0.5085,  0.2779],\n",
      "        [-0.6775, -0.2712],\n",
      "        [ 0.4016, -0.2911],\n",
      "        [-0.2515,  0.0279],\n",
      "        [-0.1908,  0.1022],\n",
      "        [ 0.0779,  0.3969],\n",
      "        [-0.5355,  0.1195],\n",
      "        [ 0.0945,  0.5540],\n",
      "        [ 0.2633,  0.5231],\n",
      "        [-0.4369,  0.4378],\n",
      "        [ 0.2596, -0.5695],\n",
      "        [ 0.0359,  0.5744],\n",
      "        [ 0.0282, -0.3524],\n",
      "        [-0.3006,  0.0733],\n",
      "        [ 0.4930, -0.3499],\n",
      "        [-0.3365, -0.7072],\n",
      "        [-0.0682, -0.6228],\n",
      "        [-0.0097,  0.1521],\n",
      "        [ 0.0230, -0.1041],\n",
      "        [ 0.4712,  0.4210],\n",
      "        [ 0.3753, -0.4966],\n",
      "        [-0.1895,  0.1686],\n",
      "        [-0.0124,  0.6176],\n",
      "        [ 0.4028, -0.5520],\n",
      "        [-0.1424,  0.4269],\n",
      "        [ 0.2967,  0.5862],\n",
      "        [-0.5297, -0.1577],\n",
      "        [-0.3495,  0.3167],\n",
      "        [-0.1066,  0.3511],\n",
      "        [ 0.6345,  0.1215],\n",
      "        [-0.2073,  0.5939],\n",
      "        [-0.6147,  0.0269],\n",
      "        [-0.4479, -0.5742],\n",
      "        [ 0.1744,  0.4682],\n",
      "        [-0.4462, -0.4384],\n",
      "        [-0.3348,  0.3882],\n",
      "        [-0.0707, -0.0829],\n",
      "        [-0.1008, -0.4743],\n",
      "        [ 0.4087,  0.6385],\n",
      "        [ 0.4115,  0.1491],\n",
      "        [ 0.3676,  0.3915],\n",
      "        [-0.0275,  0.2561],\n",
      "        [-0.5267, -0.6266],\n",
      "        [-0.2360,  0.0905],\n",
      "        [-0.0252,  0.1393],\n",
      "        [ 0.2915,  0.3689],\n",
      "        [ 0.0018,  0.4718],\n",
      "        [ 0.4610, -0.6510],\n",
      "        [ 0.5027, -0.1822],\n",
      "        [ 0.2631,  0.5579],\n",
      "        [-0.4507, -0.3253],\n",
      "        [-0.3535, -0.4466],\n",
      "        [-0.2052,  0.2306],\n",
      "        [ 0.6997,  0.2502],\n",
      "        [ 0.6178,  0.3642],\n",
      "        [-0.3535,  0.0019],\n",
      "        [-0.4511, -0.3014],\n",
      "        [ 0.1842,  0.5384],\n",
      "        [ 0.3681,  0.3353],\n",
      "        [-0.4407, -0.3054],\n",
      "        [-0.7003, -0.2591],\n",
      "        [ 0.5963, -0.0661],\n",
      "        [-0.4718, -0.1422],\n",
      "        [ 0.5963,  0.2444],\n",
      "        [-0.6677,  0.1422],\n",
      "        [-0.0231,  0.5591],\n",
      "        [-0.2526,  0.4278],\n",
      "        [-0.3147,  0.0504],\n",
      "        [-0.2924, -0.4501],\n",
      "        [ 0.4248,  0.1814],\n",
      "        [-0.5403,  0.1449],\n",
      "        [-0.7244,  0.6484],\n",
      "        [ 0.0913,  0.5882],\n",
      "        [-0.1688, -0.0635],\n",
      "        [ 0.6339,  0.3188],\n",
      "        [ 0.2365, -0.5000],\n",
      "        [ 0.1742,  0.1590],\n",
      "        [-0.2840,  0.5329],\n",
      "        [ 0.3825,  0.3157],\n",
      "        [-0.5070, -0.4334],\n",
      "        [-0.1289, -0.3389],\n",
      "        [ 0.2577,  0.4455],\n",
      "        [-0.3310,  0.2936],\n",
      "        [ 0.1334,  0.5453],\n",
      "        [ 0.5400,  0.5561],\n",
      "        [ 0.6312,  0.2066],\n",
      "        [ 0.6588, -0.2052],\n",
      "        [-0.4776,  0.3555],\n",
      "        [-0.5005,  0.4417],\n",
      "        [ 0.1710, -0.2110],\n",
      "        [-0.2618, -0.2306],\n",
      "        [-0.2507, -0.6761],\n",
      "        [-0.2163, -0.4247],\n",
      "        [ 0.1315,  0.1016],\n",
      "        [-0.3010,  0.3246],\n",
      "        [ 0.0389,  0.2478],\n",
      "        [-0.1657,  0.3030],\n",
      "        [ 0.6944, -0.2445],\n",
      "        [-0.3375, -0.6247],\n",
      "        [ 0.3297,  0.4038],\n",
      "        [ 0.6274, -0.1185],\n",
      "        [ 0.3381, -0.1384],\n",
      "        [-0.5898, -0.2312],\n",
      "        [-0.5312,  0.0917],\n",
      "        [-0.3117,  0.3784],\n",
      "        [ 0.6051, -0.1184],\n",
      "        [ 0.5020,  0.5620],\n",
      "        [ 0.4026,  0.4869],\n",
      "        [-0.1982,  0.4190],\n",
      "        [-0.0090, -0.2863],\n",
      "        [ 0.5310, -0.6888],\n",
      "        [-0.3911, -0.4806],\n",
      "        [ 0.1833, -0.3828],\n",
      "        [ 0.4291, -0.1313],\n",
      "        [-0.5471,  0.1694],\n",
      "        [ 0.1519, -0.0317],\n",
      "        [ 0.5122, -0.2597],\n",
      "        [ 0.2460,  0.2333],\n",
      "        [-0.6104,  0.2323],\n",
      "        [-0.2208, -0.1117],\n",
      "        [ 0.4795,  0.5879],\n",
      "        [ 0.4127,  0.5832],\n",
      "        [-0.5051, -0.5646],\n",
      "        [ 0.3765,  0.4764],\n",
      "        [-0.6374,  0.4469],\n",
      "        [ 0.3119,  0.6679],\n",
      "        [ 0.3547, -0.6154],\n",
      "        [ 0.3617,  0.0762],\n",
      "        [-0.3875, -0.5064],\n",
      "        [-0.6261, -0.2162],\n",
      "        [-0.5364,  0.4393],\n",
      "        [ 0.2190, -0.5147],\n",
      "        [ 0.2895, -0.0526],\n",
      "        [ 0.7152, -0.1687],\n",
      "        [ 0.4824,  0.1064],\n",
      "        [-0.3131,  0.0741],\n",
      "        [ 0.5167, -0.4020],\n",
      "        [-0.6797,  0.4773],\n",
      "        [-0.2197,  0.4305],\n",
      "        [ 0.2395,  0.2863],\n",
      "        [-0.4635,  0.4505],\n",
      "        [ 0.2360, -0.5560],\n",
      "        [ 0.6318, -0.0143],\n",
      "        [-0.6083,  0.1766],\n",
      "        [ 0.2087,  0.4611],\n",
      "        [-0.5271, -0.3388],\n",
      "        [-0.2821, -0.6246],\n",
      "        [ 0.0337, -0.4147],\n",
      "        [ 0.2311, -0.1083],\n",
      "        [ 0.0028,  0.2558],\n",
      "        [ 0.3458, -0.0235],\n",
      "        [ 0.4501, -0.0801],\n",
      "        [-0.5303,  0.5188],\n",
      "        [-0.7006,  0.0209],\n",
      "        [ 0.3646, -0.1439],\n",
      "        [-0.0276, -0.4699],\n",
      "        [-0.3585,  0.0539],\n",
      "        [-0.3519,  0.1316],\n",
      "        [ 0.2162, -0.6958],\n",
      "        [-0.2576,  0.6002],\n",
      "        [-0.6494, -0.5913],\n",
      "        [ 0.5197, -0.0552],\n",
      "        [-0.5291,  0.5474],\n",
      "        [ 0.0504, -0.1663],\n",
      "        [-0.6502, -0.4029],\n",
      "        [-0.3373,  0.4638],\n",
      "        [-0.0322,  0.2801],\n",
      "        [-0.1196, -0.6472],\n",
      "        [ 0.3621,  0.2578],\n",
      "        [-0.0351, -0.6054],\n",
      "        [-0.3371,  0.1991],\n",
      "        [-0.5437, -0.5982],\n",
      "        [ 0.6375,  0.5339],\n",
      "        [ 0.6019,  0.3745],\n",
      "        [-0.1860, -0.5854],\n",
      "        [ 0.0433,  0.0136],\n",
      "        [ 0.5371, -0.3179],\n",
      "        [ 0.0226,  0.5793],\n",
      "        [-0.0842,  0.1365],\n",
      "        [ 0.1167,  0.0615],\n",
      "        [ 0.6061, -0.5978],\n",
      "        [-0.6826, -0.5642],\n",
      "        [ 0.3540, -0.4389],\n",
      "        [-0.2525, -0.4006],\n",
      "        [-0.6456,  0.5876],\n",
      "        [-0.3880, -0.5059],\n",
      "        [ 0.0619,  0.0952],\n",
      "        [ 0.2256, -0.0739],\n",
      "        [-0.5973,  0.3123],\n",
      "        [ 0.5939,  0.0741],\n",
      "        [-0.6127, -0.2442],\n",
      "        [ 0.5490,  0.2803],\n",
      "        [-0.4174,  0.2271],\n",
      "        [-0.6231, -0.4667],\n",
      "        [-0.3390, -0.0812],\n",
      "        [-0.6095,  0.5818],\n",
      "        [-0.1328, -0.2212],\n",
      "        [ 0.1525, -0.5815],\n",
      "        [-0.5517, -0.1489],\n",
      "        [-0.3468,  0.4911],\n",
      "        [ 0.6538, -0.4262],\n",
      "        [-0.7162, -0.5183],\n",
      "        [ 0.0186, -0.7238],\n",
      "        [ 0.0631,  0.0368],\n",
      "        [-0.3408,  0.1752],\n",
      "        [ 0.4571, -0.5353],\n",
      "        [-0.6927,  0.6204]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0086, -0.0143, -0.0287,  ..., -0.0433,  0.0201,  0.0091],\n",
      "        [-0.1030,  0.0400,  0.0518,  ...,  0.0693,  0.0604, -0.0556],\n",
      "        [-0.0548,  0.0292, -0.0129,  ...,  0.0211, -0.0477,  0.0622],\n",
      "        ...,\n",
      "        [ 0.0054,  0.0328,  0.0484,  ..., -0.0308, -0.0424, -0.0464],\n",
      "        [ 0.0015,  0.0152,  0.0020,  ..., -0.0442, -0.0437, -0.0277],\n",
      "        [-0.0121,  0.0753,  0.0512,  ..., -0.0104, -0.0413, -0.0614]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 4.7622e-02,  5.4206e-02, -3.6683e-02, -3.2587e-02,  4.3970e-02,\n",
      "        -5.9199e-02, -1.5748e-02, -1.0733e-02,  1.6089e-02,  5.8147e-02,\n",
      "        -4.4613e-02,  4.5136e-02, -2.8066e-02,  5.2097e-02, -5.4549e-02,\n",
      "        -4.3130e-02,  2.0367e-03, -9.8226e-03,  2.9851e-02, -3.8619e-02,\n",
      "        -5.8753e-02, -5.6351e-02, -4.0231e-02, -1.8439e-02, -1.8375e-04,\n",
      "         3.7419e-02,  2.9989e-02,  5.9266e-02, -5.0373e-02, -4.5752e-02,\n",
      "        -1.7181e-02, -3.2319e-02, -6.1239e-02, -1.6859e-02, -2.8889e-02,\n",
      "         1.1189e-02,  3.2138e-03,  9.3473e-03, -2.7802e-02, -5.6503e-02,\n",
      "         3.1946e-02, -2.5992e-02,  5.8419e-02,  5.6338e-02, -6.6293e-03,\n",
      "        -2.2369e-02, -2.6075e-02,  2.8225e-02,  5.7368e-02, -7.7285e-03,\n",
      "         2.2237e-02,  7.1952e-03,  9.9177e-04, -1.2091e-02,  6.0556e-02,\n",
      "        -2.7864e-02,  6.0965e-02, -5.2874e-02, -7.6144e-03,  3.2977e-02,\n",
      "        -5.3814e-02,  4.6951e-02,  2.8803e-02,  5.5321e-02, -4.2925e-02,\n",
      "         5.3506e-02,  6.7436e-03,  2.3167e-02, -1.4011e-02, -5.4096e-02,\n",
      "        -2.5162e-02,  3.8702e-02,  4.5904e-02,  2.5834e-02, -3.9284e-02,\n",
      "        -2.3844e-02, -4.7562e-02,  3.9167e-02,  1.5865e-02, -1.0717e-02,\n",
      "         2.7553e-02, -5.9008e-02, -1.1342e-02, -4.2888e-02, -3.1173e-02,\n",
      "        -3.2288e-02,  3.2688e-02,  5.3973e-02,  2.8742e-03,  2.7311e-02,\n",
      "         1.9822e-02, -2.2495e-02, -5.4083e-03, -4.5008e-02,  2.8158e-02,\n",
      "         1.2691e-02,  1.9118e-02,  1.2022e-02, -2.9905e-04,  1.7804e-02,\n",
      "        -5.6040e-02,  2.3934e-02,  4.5610e-02,  2.7863e-02,  5.3184e-02,\n",
      "         2.5563e-02, -3.9781e-02,  3.2415e-02,  3.5938e-02, -5.9934e-02,\n",
      "        -9.1543e-03, -5.2958e-02,  2.7700e-02,  7.5232e-03, -1.3955e-02,\n",
      "         3.0407e-02, -5.3757e-02,  2.7901e-02, -7.0402e-03, -4.8297e-02,\n",
      "        -2.7827e-02, -3.2463e-02, -6.7483e-03,  1.6211e-02,  1.6784e-02,\n",
      "         3.3675e-02,  7.4574e-03, -9.9414e-03,  2.9955e-02, -1.8250e-02,\n",
      "        -5.4330e-02,  9.3250e-03, -1.8827e-02, -2.1742e-02, -4.1857e-02,\n",
      "         2.1800e-02, -5.4162e-02,  2.4549e-02, -8.1653e-03,  3.8017e-02,\n",
      "         4.1789e-02, -5.4263e-02, -3.8490e-02, -1.6899e-02,  4.5069e-02,\n",
      "        -8.0104e-03,  5.4189e-02,  5.7285e-02,  2.7363e-02, -2.9421e-02,\n",
      "        -2.4129e-02,  1.6098e-02,  3.2433e-02, -5.2072e-02, -4.8291e-02,\n",
      "        -6.0980e-02,  2.0027e-02, -2.5835e-02, -3.4524e-02, -3.1797e-02,\n",
      "        -3.3780e-02,  5.5709e-02,  3.9606e-02, -7.6650e-03,  2.6675e-02,\n",
      "         4.8907e-02, -4.9030e-02, -5.8157e-02, -2.3398e-02,  5.4801e-02,\n",
      "        -6.2331e-02, -3.9078e-02,  5.7603e-02, -1.1370e-02, -1.1089e-02,\n",
      "        -3.2569e-02, -4.9953e-03, -3.2844e-02, -2.4114e-02,  4.6930e-02,\n",
      "        -4.9057e-02, -2.4565e-02, -7.4004e-03, -5.1101e-02,  4.5508e-02,\n",
      "         5.5115e-02,  4.8833e-02, -1.9504e-02, -5.5688e-02, -1.2371e-02,\n",
      "         1.3711e-02,  8.4652e-03,  5.6312e-02, -5.5539e-02, -1.4093e-02,\n",
      "         1.6653e-02, -4.9893e-02, -1.3216e-02,  1.0974e-02, -1.3132e-02,\n",
      "        -5.7711e-02,  6.0995e-02, -4.5852e-02, -9.9971e-03, -4.0262e-02,\n",
      "         3.3704e-02,  2.2423e-02, -6.1505e-02, -2.1267e-02, -1.7714e-02,\n",
      "        -2.9636e-02, -8.9921e-04,  4.2983e-02, -5.7984e-02,  5.2481e-02,\n",
      "        -5.0619e-02,  3.4992e-02, -2.5354e-02, -4.2321e-02, -5.3977e-02,\n",
      "         1.8133e-02,  3.6572e-02,  5.8168e-02, -8.9718e-03,  6.0111e-02,\n",
      "        -4.5898e-02, -5.2919e-02, -2.0397e-02, -5.6561e-02,  2.7277e-02,\n",
      "        -5.3327e-02, -4.1343e-02, -4.0071e-02, -5.4068e-04, -3.6969e-02,\n",
      "        -5.0807e-02, -3.2409e-02, -2.7832e-05, -3.1320e-02,  3.9173e-02,\n",
      "         5.1193e-02,  4.4242e-02,  2.5689e-02,  8.2941e-03,  1.4946e-02,\n",
      "         1.0261e-02,  4.4617e-02,  3.9158e-02,  3.0666e-02,  1.8625e-02,\n",
      "         5.7509e-02,  4.6798e-03, -1.7887e-02,  3.0750e-02, -5.5449e-02,\n",
      "         3.9796e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0284,  0.0276,  0.0542,  ..., -0.0574,  0.0197, -0.0026],\n",
      "        [-0.0852,  0.0690,  0.0664,  ..., -0.0195, -0.0066, -0.0667],\n",
      "        [-0.0427,  0.0493,  0.1076,  ..., -0.0367,  0.0031, -0.0084],\n",
      "        ...,\n",
      "        [-0.0496, -0.0363,  0.0583,  ...,  0.0299,  0.0089, -0.0072],\n",
      "        [-0.0783,  0.0123, -0.0377,  ...,  0.0255,  0.0030, -0.0175],\n",
      "        [ 0.0252, -0.0144, -0.0547,  ..., -0.0194, -0.0162, -0.0602]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0186, -0.0512,  0.0169, -0.0429,  0.0477, -0.0005, -0.0144,  0.0024,\n",
      "         0.0364, -0.0128, -0.0585,  0.0417,  0.0182,  0.0506, -0.0504, -0.0409,\n",
      "         0.0241, -0.0161, -0.0093,  0.0090, -0.0072, -0.0366,  0.0196,  0.0550,\n",
      "        -0.0244, -0.0063,  0.0123, -0.0594,  0.0524, -0.0624,  0.0441,  0.0078,\n",
      "         0.0372, -0.0524, -0.0257, -0.0115, -0.0187,  0.0413, -0.0236, -0.0600,\n",
      "         0.0449, -0.0429,  0.0583,  0.0586, -0.0104, -0.0212,  0.0528, -0.0052,\n",
      "        -0.0291,  0.0349,  0.0315, -0.0464,  0.0372,  0.0092,  0.0281,  0.0538,\n",
      "        -0.0053,  0.0367, -0.0407, -0.0216,  0.0237, -0.0552, -0.0406,  0.0154,\n",
      "        -0.0397, -0.0225,  0.0083,  0.0268,  0.0321,  0.0370, -0.0590,  0.0187,\n",
      "         0.0552, -0.0327, -0.0265,  0.0268,  0.0279, -0.0382, -0.0200, -0.0197,\n",
      "        -0.0122,  0.0294,  0.0592,  0.0372,  0.0354,  0.0138, -0.0422, -0.0553,\n",
      "         0.0211,  0.0191, -0.0383, -0.0121,  0.0587, -0.0534,  0.0560,  0.0467,\n",
      "         0.0575,  0.0290, -0.0459,  0.0439, -0.0325, -0.0125,  0.0014,  0.0288,\n",
      "        -0.0316, -0.0599, -0.0562,  0.0451, -0.0435,  0.0400,  0.0395,  0.0587,\n",
      "        -0.0107, -0.0246, -0.0305, -0.0151,  0.0472,  0.0606,  0.0356,  0.0575,\n",
      "         0.0553,  0.0439, -0.0282, -0.0243, -0.0597, -0.0443,  0.0492,  0.0576,\n",
      "         0.0562, -0.0374, -0.0368, -0.0105, -0.0167,  0.0448, -0.0433,  0.0472,\n",
      "        -0.0248,  0.0016, -0.0380,  0.0378,  0.0294,  0.0549,  0.0538,  0.0239,\n",
      "        -0.0251, -0.0590, -0.0056, -0.0280, -0.0104,  0.0531, -0.0565, -0.0348,\n",
      "        -0.0350,  0.0232, -0.0052, -0.0609, -0.0337,  0.0209, -0.0341,  0.0517,\n",
      "         0.0158,  0.0505,  0.0125, -0.0577, -0.0509, -0.0519,  0.0032,  0.0138,\n",
      "        -0.0176, -0.0303, -0.0299, -0.0207, -0.0332,  0.0386,  0.0195, -0.0026,\n",
      "        -0.0473,  0.0197, -0.0087,  0.0385,  0.0517,  0.0233,  0.0410, -0.0490,\n",
      "        -0.0270, -0.0461, -0.0222,  0.0145,  0.0253, -0.0097, -0.0610,  0.0239,\n",
      "        -0.0192, -0.0191,  0.0258, -0.0533,  0.0003,  0.0567, -0.0454, -0.0488,\n",
      "        -0.0605, -0.0152, -0.0528, -0.0388, -0.0356, -0.0118,  0.0509,  0.0285,\n",
      "        -0.0030, -0.0564, -0.0093,  0.0115,  0.0599,  0.0377,  0.0241, -0.0255,\n",
      "        -0.0012, -0.0584,  0.0355, -0.0576, -0.0564, -0.0613, -0.0478, -0.0065,\n",
      "        -0.0517,  0.0124, -0.0611,  0.0612,  0.0225, -0.0148, -0.0487,  0.0210,\n",
      "        -0.0571, -0.0587,  0.0499,  0.0179, -0.0550,  0.0438, -0.0608,  0.0315,\n",
      "        -0.0360,  0.0053,  0.0044,  0.0584,  0.0361, -0.0023, -0.0172,  0.0375,\n",
      "         0.0253, -0.0426, -0.0482,  0.0234, -0.0048, -0.0516, -0.0027, -0.0152],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0419,  0.0372, -0.0684,  ...,  0.0452,  0.0678, -0.0140],\n",
      "        [-0.0285,  0.0426, -0.0279,  ..., -0.0001, -0.0623,  0.0722],\n",
      "        [ 0.0224,  0.0089,  0.0268,  ...,  0.0277,  0.0566,  0.0251],\n",
      "        ...,\n",
      "        [ 0.0426,  0.0377, -0.0299,  ..., -0.0778,  0.0148, -0.0117],\n",
      "        [ 0.0696, -0.0352,  0.0130,  ...,  0.0295,  0.0269, -0.0330],\n",
      "        [ 0.0686,  0.0367, -0.0160,  ..., -0.0437, -0.0286,  0.0794]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 2.9612e-02, -5.0540e-02,  6.8542e-02, -9.9129e-02, -3.0691e-02,\n",
      "         1.8520e-02, -3.2632e-02,  7.3865e-03,  8.3532e-02, -1.8096e-02,\n",
      "         1.1836e-01,  2.9943e-02, -6.5640e-02, -7.5937e-02, -5.0354e-02,\n",
      "        -7.6098e-02,  5.2344e-03,  3.0016e-02,  1.9255e-02, -8.6750e-02,\n",
      "         5.3075e-03, -7.9190e-02, -1.5296e-01, -3.2552e-02, -1.2358e-02,\n",
      "         6.3902e-03, -7.7706e-02,  3.5253e-02,  3.1045e-02,  9.0214e-02,\n",
      "        -1.9060e-02,  1.7312e-02, -9.6332e-02, -5.5057e-02, -7.7574e-02,\n",
      "         1.0119e-01,  5.6667e-03,  1.1400e-02,  1.0789e-01,  4.5158e-02,\n",
      "        -5.0996e-02,  2.3779e-02,  2.9432e-02, -7.8514e-02,  3.8017e-02,\n",
      "        -1.0424e-01,  5.3042e-02, -2.0045e-02,  8.4912e-02, -4.7965e-02,\n",
      "         6.3878e-02,  9.6215e-02, -1.9087e-02,  1.2619e-02, -2.3735e-03,\n",
      "         1.0005e-02, -8.2835e-02, -1.8997e-02, -6.8339e-02,  2.6835e-02,\n",
      "         1.1131e-02,  4.0042e-02, -3.5532e-02,  3.5482e-02,  4.8473e-03,\n",
      "         9.6268e-03, -3.4674e-02, -2.8754e-02,  6.3021e-02, -1.4206e-03,\n",
      "        -2.1366e-02,  9.4089e-02, -2.3021e-02,  6.6340e-02,  9.9871e-02,\n",
      "         4.6717e-02,  1.3606e-02, -1.1524e-01,  2.8650e-02, -3.6966e-02,\n",
      "        -7.6778e-03, -3.1713e-02,  8.8280e-03,  1.1850e-01,  3.0255e-02,\n",
      "         6.2744e-02,  5.4204e-02, -3.0441e-02,  7.4033e-02, -9.2370e-02,\n",
      "        -8.1624e-02,  2.2891e-02,  3.5376e-02,  1.0532e-02,  2.1982e-02,\n",
      "        -9.3132e-03, -8.8189e-02, -9.1120e-02, -2.7745e-03,  2.1718e-02,\n",
      "         7.8744e-02, -6.2083e-02,  6.9606e-02,  4.9891e-02, -6.3394e-02,\n",
      "        -1.9165e-03,  4.7125e-02,  3.1090e-02, -4.7363e-02,  3.6886e-02,\n",
      "         3.5280e-02, -1.1500e-02, -2.7223e-02, -4.2764e-02,  8.6785e-02,\n",
      "        -9.8885e-02, -1.4430e-02,  4.1457e-02, -4.6215e-02, -9.9292e-03,\n",
      "         4.2206e-02, -5.3775e-02,  3.1377e-02,  3.0590e-03, -7.4100e-02,\n",
      "         7.1199e-02, -1.3058e-01, -2.4231e-02, -7.9608e-03, -1.3946e-02,\n",
      "        -1.2266e-02, -9.8738e-02,  1.0772e-01,  2.1365e-02,  1.1030e-02,\n",
      "        -7.8926e-02, -1.6412e-02,  1.9828e-02, -8.7425e-02,  1.7940e-02,\n",
      "        -5.7412e-03,  3.8157e-02,  5.5473e-02, -3.1907e-02, -1.3769e-02,\n",
      "        -3.5358e-02, -3.8142e-02, -1.0111e-01,  8.1923e-02, -1.4466e-01,\n",
      "         1.1779e-02,  7.1324e-02,  5.2382e-02, -2.0547e-02, -1.2552e-01,\n",
      "         1.4852e-01,  2.7176e-03,  2.2430e-02, -6.4239e-02, -1.0233e-01,\n",
      "        -2.2688e-02,  1.4153e-01,  3.5735e-02,  5.0569e-02, -2.5552e-02,\n",
      "         7.0637e-04, -9.4928e-02, -4.2280e-02,  1.0585e-01, -3.8060e-02,\n",
      "         4.5955e-02, -7.2040e-02,  5.9278e-03, -1.5731e-02, -1.4453e-01,\n",
      "        -2.3160e-03,  9.5489e-02,  6.4563e-02, -4.8065e-02,  6.3888e-02,\n",
      "        -2.8509e-03,  1.4252e-04, -1.2693e-02,  3.4179e-02, -3.1761e-02,\n",
      "        -7.2998e-03,  5.7770e-02, -9.5714e-02, -8.9038e-03,  1.4145e-02,\n",
      "        -1.2713e-01, -1.2602e-02,  1.3072e-02, -3.0096e-02, -2.3301e-02,\n",
      "        -3.8589e-02, -1.0758e-01,  1.9321e-02, -4.4514e-02, -5.2028e-02,\n",
      "        -7.2992e-03, -3.3331e-02,  5.6411e-03,  8.2924e-02, -6.0313e-02,\n",
      "        -2.9541e-02, -4.6322e-02, -5.8462e-02, -5.3749e-02, -1.3295e-02,\n",
      "         3.9615e-02,  6.8020e-03, -9.8141e-02, -9.4520e-02,  6.3441e-03,\n",
      "        -2.6040e-02, -7.0664e-03, -1.1020e-01,  3.2835e-03,  7.9385e-02,\n",
      "        -7.6227e-02, -4.8775e-02, -5.4447e-02, -1.1293e-01,  3.6235e-03,\n",
      "        -2.6836e-03,  2.0023e-02,  4.3784e-02,  7.7721e-02, -5.7259e-02,\n",
      "         6.6860e-03,  2.9616e-02, -6.0495e-02, -7.6379e-02,  1.8802e-02,\n",
      "         4.6976e-02, -1.6392e-02,  4.2782e-02,  2.1204e-02,  5.1345e-02,\n",
      "        -5.3543e-02, -3.7079e-02, -1.1574e-02,  5.6918e-04, -4.4071e-02,\n",
      "         5.7596e-03,  9.3625e-02, -1.0392e-01, -1.0566e-02, -4.8543e-02,\n",
      "        -6.6291e-03,  3.8677e-02, -8.5545e-02,  3.5090e-02, -2.3745e-02,\n",
      "        -5.2089e-03], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0330,  0.0289,  0.0698,  ...,  0.0642, -0.0592,  0.0173],\n",
      "        [-0.0693,  0.0274,  0.1066,  ..., -0.0583, -0.0230,  0.0270],\n",
      "        [ 0.0170,  0.0309, -0.0572,  ..., -0.0338,  0.0423,  0.0461],\n",
      "        ...,\n",
      "        [-0.0195,  0.0066, -0.0054,  ..., -0.0537,  0.0552,  0.0730],\n",
      "        [ 0.0410, -0.0467, -0.0024,  ..., -0.0218,  0.0650,  0.0580],\n",
      "        [ 0.0097, -0.0333,  0.0283,  ...,  0.0463, -0.0263, -0.0041]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0087,  0.0319,  0.0027,  0.0276,  0.0505, -0.0479,  0.0403,  0.0471,\n",
      "         0.0025, -0.0055, -0.0090,  0.0562,  0.0383,  0.0286,  0.0038,  0.0398,\n",
      "        -0.0030, -0.0564, -0.0004,  0.0480,  0.0166, -0.0320, -0.0323,  0.0112,\n",
      "         0.0262,  0.0537,  0.0524,  0.0450, -0.0229,  0.0231, -0.0525, -0.0508,\n",
      "        -0.0621, -0.0252, -0.0261, -0.0614,  0.0091,  0.0040, -0.0536,  0.0599,\n",
      "         0.0108,  0.0332, -0.0621,  0.0058,  0.0156, -0.0172,  0.0370,  0.0239,\n",
      "        -0.0119, -0.0584,  0.0353,  0.0295,  0.0560, -0.0229, -0.0389, -0.0567,\n",
      "        -0.0433, -0.0322,  0.0335,  0.0533,  0.0291, -0.0213, -0.0374,  0.0061,\n",
      "        -0.0314,  0.0538, -0.0057, -0.0258,  0.0031,  0.0017, -0.0545,  0.0341,\n",
      "        -0.0161, -0.0319,  0.0204, -0.0504, -0.0603,  0.0540,  0.0498,  0.0591,\n",
      "        -0.0477,  0.0504, -0.0536,  0.0425, -0.0044,  0.0454, -0.0090,  0.0210,\n",
      "        -0.0615, -0.0557,  0.0408, -0.0581, -0.0128, -0.0174, -0.0499,  0.0430,\n",
      "         0.0232,  0.0418, -0.0068,  0.0119,  0.0191, -0.0300,  0.0410,  0.0188,\n",
      "         0.0173,  0.0079, -0.0324,  0.0141, -0.0407, -0.0511, -0.0124,  0.0162,\n",
      "         0.0330, -0.0291,  0.0458, -0.0312, -0.0449,  0.0434,  0.0043,  0.0029,\n",
      "        -0.0105,  0.0273,  0.0340, -0.0254,  0.0022,  0.0526, -0.0584,  0.0129,\n",
      "        -0.0157, -0.0429,  0.0430, -0.0495,  0.0382,  0.0603, -0.0598,  0.0458,\n",
      "         0.0595, -0.0563, -0.0168,  0.0006, -0.0459,  0.0019,  0.0435,  0.0460,\n",
      "        -0.0446, -0.0624, -0.0615,  0.0155, -0.0156,  0.0143, -0.0556, -0.0134,\n",
      "        -0.0113,  0.0162,  0.0346,  0.0447,  0.0143, -0.0114,  0.0559, -0.0365,\n",
      "        -0.0256,  0.0296, -0.0611, -0.0216, -0.0512,  0.0034, -0.0525, -0.0612,\n",
      "        -0.0334,  0.0315, -0.0054,  0.0522,  0.0329,  0.0223, -0.0454,  0.0339,\n",
      "         0.0619, -0.0505,  0.0454, -0.0253, -0.0104, -0.0475, -0.0133,  0.0391,\n",
      "         0.0513,  0.0098,  0.0202, -0.0499, -0.0288, -0.0093, -0.0304,  0.0187,\n",
      "        -0.0233,  0.0201,  0.0354, -0.0029,  0.0013,  0.0183,  0.0210, -0.0349,\n",
      "         0.0297,  0.0463, -0.0603,  0.0504,  0.0216,  0.0422, -0.0493, -0.0565,\n",
      "         0.0083,  0.0116, -0.0065,  0.0615,  0.0340,  0.0166, -0.0600,  0.0358,\n",
      "        -0.0424, -0.0619,  0.0545,  0.0324, -0.0441,  0.0406,  0.0160,  0.0224,\n",
      "        -0.0213, -0.0109, -0.0601, -0.0456, -0.0561,  0.0568, -0.0085,  0.0139,\n",
      "        -0.0515, -0.0024,  0.0005, -0.0381, -0.0007,  0.0176, -0.0175, -0.0134,\n",
      "        -0.0384, -0.0494, -0.0491, -0.0605,  0.0043,  0.0573, -0.0383, -0.0203,\n",
      "        -0.0254, -0.0382,  0.0565, -0.0295,  0.0577, -0.0024,  0.0437,  0.0257],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-3.5952e-01, -6.3613e-01],\n",
      "        [ 3.3300e-01,  1.9118e-02],\n",
      "        [-1.9214e-01,  8.5812e-03],\n",
      "        [ 3.1751e-01,  3.3989e-01],\n",
      "        [ 6.0887e-01, -4.1660e-01],\n",
      "        [ 6.8183e-02,  1.3882e-01],\n",
      "        [-6.7544e-01,  1.7049e-01],\n",
      "        [ 5.2053e-01, -2.3927e-02],\n",
      "        [-5.6537e-01,  1.3407e-01],\n",
      "        [ 6.0898e-01,  1.1849e-01],\n",
      "        [-6.7345e-01, -3.1385e-01],\n",
      "        [ 6.4504e-01, -3.3149e-01],\n",
      "        [-1.4849e-01,  5.7635e-01],\n",
      "        [ 5.1911e-01,  1.2045e-01],\n",
      "        [-2.3848e-01,  5.2457e-01],\n",
      "        [-2.7175e-02,  2.7211e-01],\n",
      "        [ 3.6894e-01, -3.2461e-01],\n",
      "        [-1.5526e-01,  3.8051e-02],\n",
      "        [ 6.3938e-01, -6.8574e-01],\n",
      "        [ 4.7372e-03,  3.0830e-01],\n",
      "        [ 2.9410e-01,  2.7902e-01],\n",
      "        [ 6.5006e-01, -1.9959e-01],\n",
      "        [ 5.5185e-01,  6.6075e-01],\n",
      "        [ 6.7674e-02,  4.9852e-01],\n",
      "        [-3.2939e-01, -5.1494e-01],\n",
      "        [-2.1845e-01, -1.0671e-01],\n",
      "        [ 4.2771e-01,  2.5388e-01],\n",
      "        [-1.7109e-01,  5.5818e-01],\n",
      "        [ 5.7424e-01, -6.1812e-01],\n",
      "        [ 2.3056e-01, -1.9128e-01],\n",
      "        [ 3.8491e-01,  6.6180e-01],\n",
      "        [-6.5113e-01, -1.4967e-01],\n",
      "        [ 6.0991e-01,  2.0128e-01],\n",
      "        [-7.1254e-01,  6.1656e-01],\n",
      "        [ 2.0808e-01,  2.5360e-01],\n",
      "        [-2.2943e-02, -3.2956e-01],\n",
      "        [-5.3211e-02,  2.7849e-01],\n",
      "        [ 5.3220e-01,  6.4981e-01],\n",
      "        [-5.3632e-01, -2.0835e-01],\n",
      "        [-4.4007e-01,  3.1682e-01],\n",
      "        [ 5.4923e-01, -3.5582e-01],\n",
      "        [-2.5988e-01,  3.3523e-01],\n",
      "        [ 2.4006e-01, -2.5443e-01],\n",
      "        [ 3.7811e-01,  4.5688e-01],\n",
      "        [-3.2955e-03, -2.4715e-01],\n",
      "        [ 4.7543e-01,  8.2834e-02],\n",
      "        [-6.4750e-01,  4.5179e-02],\n",
      "        [ 2.3018e-01, -5.7584e-01],\n",
      "        [-5.4389e-01, -8.5744e-02],\n",
      "        [-4.4887e-01,  3.6695e-01],\n",
      "        [-5.6180e-01,  1.6406e-01],\n",
      "        [-4.1101e-01,  4.3297e-02],\n",
      "        [-6.8873e-01,  3.8308e-01],\n",
      "        [-5.5707e-01, -1.0706e-01],\n",
      "        [-1.8457e-01, -5.7137e-01],\n",
      "        [ 3.5554e-01,  2.2092e-01],\n",
      "        [-3.9104e-01,  3.8222e-01],\n",
      "        [ 2.4599e-01, -1.4812e-01],\n",
      "        [ 2.3281e-01,  4.8176e-01],\n",
      "        [-6.2495e-01,  3.7629e-02],\n",
      "        [ 4.1456e-01, -2.4965e-01],\n",
      "        [ 9.7683e-02,  5.9630e-02],\n",
      "        [ 1.2961e-01,  5.7389e-01],\n",
      "        [-2.0441e-01,  6.5237e-01],\n",
      "        [-4.4425e-01,  2.3752e-01],\n",
      "        [-5.4048e-01,  3.3810e-01],\n",
      "        [ 1.2944e-01,  1.3336e-01],\n",
      "        [ 3.3892e-01, -1.3299e-01],\n",
      "        [-5.8408e-01,  3.4083e-01],\n",
      "        [ 2.3553e-01,  1.0890e-01],\n",
      "        [-7.1545e-01, -7.1798e-02],\n",
      "        [-6.1552e-01, -3.0743e-01],\n",
      "        [ 5.1505e-01, -6.7034e-01],\n",
      "        [-5.7993e-01,  7.7797e-02],\n",
      "        [ 5.4882e-01, -5.4166e-01],\n",
      "        [-6.6674e-02, -4.9652e-01],\n",
      "        [-6.3514e-01,  4.5658e-01],\n",
      "        [ 5.2828e-02,  5.5268e-01],\n",
      "        [-4.8401e-01, -4.2312e-01],\n",
      "        [-5.9322e-01,  5.0864e-01],\n",
      "        [ 5.4403e-01,  6.6810e-01],\n",
      "        [ 1.7494e-01,  3.9919e-01],\n",
      "        [ 1.9395e-01,  6.5544e-01],\n",
      "        [-7.0537e-01, -6.1184e-02],\n",
      "        [-1.4180e-01,  2.1021e-01],\n",
      "        [-2.8110e-01, -3.4438e-01],\n",
      "        [-6.1551e-01, -6.7190e-01],\n",
      "        [ 5.6076e-01, -3.5840e-01],\n",
      "        [-2.5572e-01,  7.2713e-03],\n",
      "        [ 2.7186e-01,  3.1497e-01],\n",
      "        [ 5.5775e-02,  4.5347e-01],\n",
      "        [-6.4742e-01, -3.1845e-01],\n",
      "        [ 3.6533e-01, -4.0671e-01],\n",
      "        [ 4.3880e-01,  6.6537e-01],\n",
      "        [-4.9472e-01, -5.5372e-01],\n",
      "        [-4.6874e-02, -5.2055e-01],\n",
      "        [ 3.1145e-01, -5.6837e-02],\n",
      "        [ 5.5259e-01,  4.0171e-01],\n",
      "        [ 2.1864e-01,  4.0646e-01],\n",
      "        [-6.9676e-01, -4.0687e-01],\n",
      "        [-6.1014e-01,  1.0087e-01],\n",
      "        [-6.2638e-01,  3.6176e-02],\n",
      "        [-3.6634e-01, -2.7449e-01],\n",
      "        [ 1.6934e-01, -3.1910e-01],\n",
      "        [ 4.0338e-01, -1.3627e-01],\n",
      "        [ 3.8814e-01, -2.5862e-01],\n",
      "        [-6.6552e-01,  4.8269e-01],\n",
      "        [-6.1287e-02,  1.4297e-01],\n",
      "        [ 6.2120e-01, -2.2229e-01],\n",
      "        [ 3.7048e-01, -1.7422e-01],\n",
      "        [-3.2799e-01,  1.2880e-01],\n",
      "        [-2.1095e-01,  5.8743e-02],\n",
      "        [-4.6252e-02,  1.0571e-01],\n",
      "        [ 5.2303e-01,  5.3097e-01],\n",
      "        [ 5.9938e-01, -4.9101e-01],\n",
      "        [ 4.3408e-01,  5.3397e-01],\n",
      "        [-4.2803e-01,  2.1994e-01],\n",
      "        [-4.0407e-01, -2.4216e-01],\n",
      "        [ 1.9626e-01, -2.3754e-01],\n",
      "        [ 3.4401e-01,  5.9859e-01],\n",
      "        [ 6.3148e-01, -2.9130e-01],\n",
      "        [-3.0260e-02,  7.0296e-01],\n",
      "        [-6.7911e-01,  5.3970e-01],\n",
      "        [ 7.2623e-01, -3.0059e-01],\n",
      "        [ 2.2910e-01,  6.4292e-02],\n",
      "        [-4.0855e-01,  4.6785e-01],\n",
      "        [ 6.4110e-01,  2.7802e-01],\n",
      "        [-5.8451e-02,  3.3510e-01],\n",
      "        [ 4.0737e-01, -1.3575e-01],\n",
      "        [-5.5768e-01,  4.9090e-01],\n",
      "        [-2.6492e-01,  1.2989e-01],\n",
      "        [ 7.7364e-02,  6.0954e-01],\n",
      "        [-5.3323e-01, -4.2879e-03],\n",
      "        [-5.8795e-01, -4.3412e-01],\n",
      "        [-2.9331e-01,  2.0882e-01],\n",
      "        [ 3.9008e-01,  3.6140e-01],\n",
      "        [-3.0116e-01, -5.1163e-01],\n",
      "        [-4.6700e-01, -5.9964e-01],\n",
      "        [ 5.3984e-01,  2.4596e-01],\n",
      "        [ 3.1060e-01,  3.0076e-02],\n",
      "        [ 3.5370e-01, -5.6033e-01],\n",
      "        [ 1.2278e-01, -2.4789e-01],\n",
      "        [-6.9402e-01, -7.0491e-01],\n",
      "        [-2.3887e-01,  3.6834e-01],\n",
      "        [-2.2507e-01, -5.5297e-01],\n",
      "        [-5.6716e-02,  4.8271e-01],\n",
      "        [-2.1510e-01,  2.6620e-01],\n",
      "        [ 6.2163e-01,  4.0857e-01],\n",
      "        [-2.1860e-01, -4.8961e-02],\n",
      "        [ 4.3124e-01,  1.3065e-01],\n",
      "        [-5.7735e-02,  7.2189e-02],\n",
      "        [-1.7886e-01, -4.0558e-01],\n",
      "        [ 7.1156e-01, -5.4251e-01],\n",
      "        [ 5.4878e-01,  4.4571e-01],\n",
      "        [ 1.9767e-01,  5.4679e-01],\n",
      "        [-5.2312e-01, -2.0685e-01],\n",
      "        [ 6.3208e-01, -3.1210e-01],\n",
      "        [ 6.7483e-01, -1.3759e-01],\n",
      "        [ 4.8872e-01, -2.2233e-01],\n",
      "        [-4.5781e-01,  5.8169e-01],\n",
      "        [ 3.5720e-01,  5.5588e-01],\n",
      "        [-2.9568e-01, -1.1388e-01],\n",
      "        [ 2.5583e-01, -2.3784e-01],\n",
      "        [-3.4981e-01, -3.7403e-04],\n",
      "        [ 3.0756e-01, -1.7134e-01],\n",
      "        [ 4.9050e-01, -3.6152e-01],\n",
      "        [ 6.4065e-01,  3.7896e-01],\n",
      "        [-2.7706e-02, -3.7516e-01],\n",
      "        [-2.6696e-01, -3.3478e-01],\n",
      "        [-1.8779e-02, -2.0988e-01],\n",
      "        [-6.8810e-01,  4.8591e-01],\n",
      "        [ 3.7065e-01, -4.0771e-01],\n",
      "        [-1.0157e-01,  2.5027e-01],\n",
      "        [ 6.6648e-01,  1.6088e-01],\n",
      "        [ 3.5430e-01,  1.9308e-01],\n",
      "        [-4.0515e-01,  2.4397e-01],\n",
      "        [ 4.7107e-01, -1.4975e-01],\n",
      "        [-5.5697e-01, -4.1938e-01],\n",
      "        [ 4.8766e-01, -4.7289e-01],\n",
      "        [-4.4399e-01, -2.3710e-01],\n",
      "        [ 6.5274e-01,  4.1596e-01],\n",
      "        [-4.0192e-01,  4.7323e-01],\n",
      "        [ 3.0450e-01,  1.5692e-01],\n",
      "        [-6.7050e-01, -1.9199e-01],\n",
      "        [ 3.5865e-01, -5.2372e-02],\n",
      "        [ 6.5078e-01,  5.2231e-01],\n",
      "        [-4.8213e-01, -3.2577e-01],\n",
      "        [ 3.7212e-01,  3.1171e-02],\n",
      "        [-3.2086e-01,  3.8359e-01],\n",
      "        [ 2.5988e-01, -1.0123e-01],\n",
      "        [ 5.6999e-01,  2.7862e-01],\n",
      "        [-2.0529e-01, -5.7148e-01],\n",
      "        [ 5.0516e-01,  6.1146e-01],\n",
      "        [-1.0771e-01,  2.9439e-02],\n",
      "        [ 6.7758e-02, -1.1066e-01],\n",
      "        [-2.9629e-01, -1.9449e-01],\n",
      "        [-2.6981e-01,  3.9787e-01],\n",
      "        [ 8.7092e-02, -5.0558e-01],\n",
      "        [ 6.1693e-01, -3.4695e-01],\n",
      "        [ 5.6430e-01,  2.3044e-01],\n",
      "        [ 1.1731e-01, -1.9846e-01],\n",
      "        [ 1.0555e-01,  6.2880e-01],\n",
      "        [ 5.8746e-01, -1.6233e-01],\n",
      "        [-6.3985e-01,  3.1286e-01],\n",
      "        [-1.8712e-01,  1.2982e-01],\n",
      "        [ 9.5396e-02, -9.4755e-02],\n",
      "        [-1.9305e-01,  3.5803e-01],\n",
      "        [ 6.5401e-01,  5.8228e-01],\n",
      "        [ 5.3247e-01, -2.9227e-01],\n",
      "        [ 4.9863e-01,  3.0747e-02],\n",
      "        [-5.9965e-01, -4.3983e-01],\n",
      "        [ 4.2724e-03,  6.6010e-01],\n",
      "        [-4.5606e-01,  5.7001e-01],\n",
      "        [-5.2392e-02,  4.3660e-01],\n",
      "        [-1.6668e-01, -1.4668e-01],\n",
      "        [-2.0321e-02, -4.3830e-01],\n",
      "        [ 1.7286e-01,  4.2116e-02],\n",
      "        [ 7.0423e-01,  4.8311e-01],\n",
      "        [ 1.4220e-01, -5.4920e-01],\n",
      "        [-7.1672e-01, -3.5593e-01],\n",
      "        [ 5.9222e-01, -1.4646e-01],\n",
      "        [-2.1143e-01,  5.4496e-01],\n",
      "        [ 6.2307e-01,  5.6972e-01],\n",
      "        [ 4.0282e-01,  5.8121e-01],\n",
      "        [ 4.4567e-01, -2.1751e-02],\n",
      "        [-6.9393e-01,  3.0969e-01],\n",
      "        [-4.6514e-02, -3.3418e-01],\n",
      "        [ 4.0588e-01, -2.9627e-01],\n",
      "        [-4.8304e-03, -5.5709e-01],\n",
      "        [ 4.2009e-01, -9.2491e-02],\n",
      "        [-2.8380e-01, -3.3433e-01],\n",
      "        [ 1.9458e-01,  3.0793e-01],\n",
      "        [ 6.7321e-01, -1.4887e-01],\n",
      "        [-6.3944e-01,  4.5715e-01],\n",
      "        [ 5.7838e-01, -6.9504e-02],\n",
      "        [ 3.0943e-01, -2.7405e-01],\n",
      "        [ 1.9462e-01, -4.4610e-01],\n",
      "        [ 4.3038e-01, -5.8643e-01],\n",
      "        [-6.0710e-02, -1.6919e-01],\n",
      "        [-6.6533e-02, -6.1675e-01],\n",
      "        [ 1.8626e-01, -4.1334e-01],\n",
      "        [ 4.9355e-02,  2.0992e-01],\n",
      "        [-1.5044e-01, -1.6477e-01],\n",
      "        [ 4.0656e-01, -1.5734e-01],\n",
      "        [-1.5267e-01, -1.8924e-01],\n",
      "        [ 5.0165e-01,  2.7995e-01],\n",
      "        [ 2.4614e-01, -4.9898e-01],\n",
      "        [ 5.2649e-01,  7.1666e-01],\n",
      "        [ 7.8479e-02,  2.0833e-01],\n",
      "        [-5.5806e-01,  4.9787e-01],\n",
      "        [-2.5020e-01, -3.6039e-01],\n",
      "        [-1.2067e-01, -5.1552e-01],\n",
      "        [-4.0788e-02,  3.4091e-01],\n",
      "        [-5.0657e-02, -2.2355e-01],\n",
      "        [-2.7814e-01, -4.7669e-01],\n",
      "        [-1.2526e-01,  1.7641e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0052,  0.1018,  0.0334,  ...,  0.0014, -0.0122,  0.0754],\n",
      "        [-0.0304, -0.0407, -0.0056,  ..., -0.0812,  0.0382, -0.0575],\n",
      "        [ 0.0525,  0.0469,  0.0189,  ...,  0.0803, -0.0206,  0.0423],\n",
      "        ...,\n",
      "        [-0.0057,  0.0544, -0.0434,  ..., -0.0147,  0.0423,  0.0848],\n",
      "        [-0.0124, -0.0020,  0.0332,  ...,  0.0254, -0.0299, -0.0122],\n",
      "        [ 0.0127, -0.0325, -0.0269,  ...,  0.0110, -0.0147,  0.0054]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0619, -0.0471, -0.0624, -0.0287, -0.0119, -0.0135, -0.0275, -0.0057,\n",
      "         0.0212, -0.0586,  0.0497, -0.0067,  0.0598,  0.0394,  0.0593, -0.0337,\n",
      "        -0.0315, -0.0147, -0.0559,  0.0054, -0.0006,  0.0585, -0.0451,  0.0108,\n",
      "        -0.0535, -0.0022, -0.0379, -0.0387, -0.0045, -0.0432,  0.0505, -0.0278,\n",
      "        -0.0400, -0.0250,  0.0075, -0.0327,  0.0367, -0.0505,  0.0143,  0.0566,\n",
      "         0.0561,  0.0467, -0.0216, -0.0270, -0.0499,  0.0609, -0.0345, -0.0318,\n",
      "         0.0418,  0.0625, -0.0578, -0.0188, -0.0314,  0.0557,  0.0018, -0.0393,\n",
      "        -0.0531, -0.0230, -0.0474, -0.0604, -0.0410,  0.0138, -0.0581, -0.0230,\n",
      "         0.0424, -0.0014, -0.0251,  0.0291, -0.0465,  0.0176, -0.0091,  0.0282,\n",
      "        -0.0200, -0.0422, -0.0087,  0.0144, -0.0240, -0.0524, -0.0579,  0.0141,\n",
      "         0.0045, -0.0373,  0.0119, -0.0162,  0.0390, -0.0485,  0.0176, -0.0222,\n",
      "         0.0284, -0.0440, -0.0320, -0.0303, -0.0573, -0.0576,  0.0184,  0.0466,\n",
      "        -0.0460,  0.0126,  0.0456, -0.0458, -0.0383, -0.0282,  0.0452, -0.0592,\n",
      "        -0.0285,  0.0424, -0.0013,  0.0507,  0.0411, -0.0140, -0.0461,  0.0240,\n",
      "         0.0515,  0.0273, -0.0410, -0.0121, -0.0611, -0.0518, -0.0353, -0.0051,\n",
      "         0.0065, -0.0189,  0.0307,  0.0421, -0.0566, -0.0466, -0.0252,  0.0051,\n",
      "        -0.0567, -0.0334, -0.0479, -0.0140,  0.0408,  0.0607, -0.0572, -0.0042,\n",
      "         0.0568,  0.0116, -0.0197, -0.0146,  0.0389, -0.0197, -0.0329,  0.0496,\n",
      "        -0.0129, -0.0538, -0.0397,  0.0104, -0.0256, -0.0464,  0.0159,  0.0607,\n",
      "        -0.0128,  0.0148, -0.0480, -0.0060, -0.0027,  0.0344,  0.0587,  0.0068,\n",
      "        -0.0256,  0.0490, -0.0024, -0.0333,  0.0608, -0.0272,  0.0428,  0.0467,\n",
      "         0.0310,  0.0348, -0.0154,  0.0402,  0.0552,  0.0231,  0.0446, -0.0059,\n",
      "         0.0573, -0.0182, -0.0540, -0.0225,  0.0605,  0.0231, -0.0574, -0.0374,\n",
      "         0.0195,  0.0230,  0.0259,  0.0274, -0.0314, -0.0572, -0.0217,  0.0258,\n",
      "         0.0607,  0.0293,  0.0593, -0.0199,  0.0128,  0.0065, -0.0485,  0.0473,\n",
      "        -0.0240, -0.0324, -0.0619,  0.0049, -0.0210,  0.0577,  0.0464,  0.0367,\n",
      "        -0.0524,  0.0544,  0.0399, -0.0585,  0.0561, -0.0456, -0.0456,  0.0565,\n",
      "        -0.0585,  0.0003,  0.0527, -0.0576,  0.0274,  0.0476,  0.0365, -0.0402,\n",
      "        -0.0590, -0.0486,  0.0272, -0.0483, -0.0373,  0.0576, -0.0197, -0.0607,\n",
      "         0.0497, -0.0380,  0.0377,  0.0173,  0.0225,  0.0380, -0.0250, -0.0182,\n",
      "        -0.0199,  0.0427, -0.0501,  0.0123, -0.0323, -0.0494, -0.0013, -0.0199,\n",
      "         0.0037, -0.0059, -0.0175, -0.0229, -0.0128, -0.0276, -0.0488, -0.0133],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0492, -0.0509, -0.0729,  ...,  0.0230,  0.0055,  0.0151],\n",
      "        [ 0.0498,  0.0572, -0.0309,  ..., -0.0699,  0.0878,  0.0214],\n",
      "        [ 0.0470,  0.0295,  0.0553,  ..., -0.0348, -0.0412, -0.0666],\n",
      "        ...,\n",
      "        [ 0.0194,  0.0141,  0.0663,  ...,  0.0220,  0.0175,  0.0717],\n",
      "        [ 0.0228, -0.0601,  0.0208,  ..., -0.0354,  0.0750, -0.0001],\n",
      "        [ 0.0344,  0.0068, -0.0297,  ..., -0.0030,  0.0117, -0.1017]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 2.6730e-02, -4.8856e-02, -2.1004e-02, -1.1160e-03, -2.7411e-02,\n",
      "        -2.4015e-02,  5.4896e-02,  2.5726e-02, -1.4865e-02,  2.2651e-02,\n",
      "        -3.5831e-02,  3.2285e-02, -4.1322e-02,  9.9303e-03, -2.0215e-02,\n",
      "         3.0749e-02,  4.1804e-02, -3.9483e-02, -2.2288e-02,  3.3681e-02,\n",
      "         5.8310e-02, -1.4942e-02,  3.6176e-02, -1.2400e-02,  1.1510e-02,\n",
      "         9.4884e-03, -5.9491e-02,  4.1457e-02, -1.6705e-02, -8.3279e-03,\n",
      "        -8.2852e-03,  1.4110e-02,  3.1867e-02,  5.5013e-02, -2.2209e-02,\n",
      "        -4.3274e-02,  1.7231e-02, -5.2731e-02, -4.3378e-02, -4.3071e-02,\n",
      "        -6.2112e-02, -1.0359e-03,  2.6422e-02, -3.9176e-02,  5.6495e-02,\n",
      "        -2.5066e-02, -4.7951e-02,  5.5728e-02, -1.5330e-03, -2.4585e-02,\n",
      "        -1.7273e-02,  4.4474e-03,  4.7965e-02, -5.2885e-02, -5.6886e-02,\n",
      "        -5.5704e-03, -1.8284e-02,  3.1067e-02, -6.2474e-02, -4.7756e-03,\n",
      "        -3.4710e-02, -4.2891e-02, -3.4742e-02,  3.7750e-02, -5.2898e-02,\n",
      "        -2.8054e-02,  2.6370e-02, -5.5334e-02, -1.6640e-02,  3.8664e-02,\n",
      "        -4.1181e-02,  5.2005e-02,  4.4581e-02,  1.9153e-03,  3.3433e-02,\n",
      "         5.7333e-02, -3.7188e-03, -3.3436e-02, -2.1806e-02,  4.5150e-02,\n",
      "         5.7874e-02, -5.7519e-02, -4.8673e-02,  1.9199e-02, -5.4594e-02,\n",
      "        -4.4572e-02, -1.4201e-02, -6.2484e-02, -6.2218e-02,  1.2831e-02,\n",
      "        -3.6900e-02, -1.5354e-02, -2.8976e-02, -6.0683e-03,  5.6388e-02,\n",
      "         1.0937e-03, -7.2986e-03, -4.3398e-02,  5.0065e-02,  1.2016e-03,\n",
      "        -2.6976e-02, -3.3283e-02,  4.0171e-02,  5.1272e-02,  4.5033e-02,\n",
      "        -3.0350e-02,  2.4215e-02, -1.2806e-02, -1.5739e-02,  4.6516e-02,\n",
      "         3.1636e-02, -3.2281e-02,  1.7448e-03, -4.3246e-02, -1.5906e-02,\n",
      "         1.8076e-02,  6.8297e-03,  2.7342e-03, -4.5802e-02, -4.4590e-03,\n",
      "        -1.7697e-02, -3.2695e-02, -1.9378e-04,  3.1344e-02,  5.3352e-02,\n",
      "         5.5644e-02,  2.7803e-04,  1.6492e-02,  4.1472e-02, -6.0175e-02,\n",
      "        -2.2254e-02,  1.8968e-02,  2.5187e-02,  3.9605e-02,  2.5045e-02,\n",
      "         3.0357e-02,  6.1679e-02, -1.5370e-02,  6.2458e-02,  9.7113e-03,\n",
      "         2.1516e-02, -4.6933e-02, -8.1299e-05, -6.3485e-03, -1.9354e-02,\n",
      "         1.8636e-02,  1.5531e-02, -6.1695e-02, -1.4691e-02, -6.1865e-02,\n",
      "        -1.8807e-02,  2.4815e-02,  3.9463e-02,  1.6309e-02,  3.5765e-02,\n",
      "         1.6807e-02, -9.7326e-03,  3.5482e-02,  3.8713e-02, -3.6054e-02,\n",
      "         5.8114e-02, -1.3720e-02, -2.2435e-02, -4.0945e-02,  3.6698e-03,\n",
      "         2.4432e-02,  5.1258e-02,  5.6351e-02,  2.8641e-02, -9.6303e-03,\n",
      "        -1.6762e-03, -3.6855e-02,  4.3159e-02, -3.0196e-02, -4.5124e-02,\n",
      "        -2.6456e-02, -2.0368e-02, -4.6157e-02,  4.3282e-03, -5.3783e-02,\n",
      "         2.3384e-02,  1.4883e-02, -1.4757e-02, -2.3409e-02,  4.4026e-02,\n",
      "        -4.3066e-03, -8.0941e-03,  6.0118e-02, -6.6942e-04,  3.7494e-02,\n",
      "        -8.9640e-03, -6.1695e-02,  2.2647e-02,  4.1746e-02, -5.3801e-02,\n",
      "         3.5310e-02,  1.2120e-02,  6.2135e-02,  1.1466e-02, -2.9370e-02,\n",
      "         3.8470e-02,  9.8318e-03, -4.7828e-02, -5.6287e-02,  6.1342e-03,\n",
      "        -3.1773e-02, -5.1915e-02, -3.1771e-02, -1.1422e-03, -2.9980e-02,\n",
      "         5.4565e-04,  5.3374e-02, -2.5626e-02, -2.6708e-02,  4.6639e-02,\n",
      "         8.8777e-03, -5.1565e-02,  5.6948e-02, -5.9622e-02, -1.4571e-02,\n",
      "         4.2982e-02,  5.1333e-02,  6.0989e-03, -2.2856e-02, -1.2424e-02,\n",
      "         4.8599e-02,  6.2316e-02, -3.6039e-02,  2.9469e-02,  3.4694e-02,\n",
      "         4.3259e-02, -3.0512e-02, -8.6379e-03, -2.4568e-02, -4.6730e-02,\n",
      "         3.9132e-03, -3.5355e-02,  3.4614e-02, -1.8616e-02,  3.1074e-03,\n",
      "        -1.2746e-02, -5.2757e-02, -4.2294e-02, -1.2434e-02,  2.2245e-02,\n",
      "         2.4467e-02,  5.7312e-02, -1.4718e-02, -5.7168e-02,  2.9776e-02,\n",
      "         3.3773e-02,  9.7648e-03,  3.9258e-02, -6.3838e-03, -2.2555e-02,\n",
      "         2.5599e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0472,  0.0174, -0.1123,  ...,  0.0353,  0.0306,  0.0360],\n",
      "        [-0.0023, -0.0266,  0.0567,  ...,  0.0169,  0.0348,  0.0019],\n",
      "        [ 0.0082, -0.0929, -0.1166,  ...,  0.0217, -0.1259,  0.0581],\n",
      "        ...,\n",
      "        [ 0.0409,  0.0346,  0.0192,  ..., -0.0237, -0.0446, -0.0034],\n",
      "        [ 0.0007,  0.0345, -0.0575,  ...,  0.0404, -0.0696, -0.0488],\n",
      "        [ 0.0081, -0.0419, -0.0152,  ...,  0.0251, -0.0399,  0.0629]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0003,  0.1200, -0.1702, -0.0017,  0.1088, -0.0978,  0.0271, -0.0571,\n",
      "         0.0064,  0.1079,  0.0455, -0.0682, -0.0079, -0.1422,  0.0023, -0.0838,\n",
      "        -0.0259,  0.0194,  0.0520,  0.0351,  0.0098,  0.0379,  0.0782,  0.0070,\n",
      "         0.0094,  0.1071,  0.0322,  0.0431,  0.0394,  0.0861, -0.0944, -0.0373,\n",
      "        -0.0139,  0.0324, -0.0356, -0.0150,  0.0547,  0.1206,  0.0074, -0.1052,\n",
      "         0.0646, -0.0194,  0.0308, -0.0801, -0.0223, -0.1056, -0.0034, -0.0295,\n",
      "         0.0142, -0.1002,  0.0158,  0.0607,  0.0032, -0.0553,  0.0026,  0.0469,\n",
      "         0.0305, -0.0802, -0.1330, -0.0572, -0.1025,  0.0587,  0.0126, -0.0464,\n",
      "        -0.0089,  0.0248, -0.1603,  0.0215,  0.0268,  0.1687,  0.0562, -0.1046,\n",
      "        -0.0474,  0.0367, -0.0092, -0.0507, -0.0645,  0.0236, -0.0087, -0.0478,\n",
      "         0.0077, -0.0268,  0.0814,  0.0097,  0.0435,  0.0209,  0.1038,  0.0040,\n",
      "        -0.0336,  0.0504, -0.0213, -0.0828,  0.0711, -0.0555, -0.0072,  0.0394,\n",
      "         0.0791, -0.0635,  0.0868, -0.0488,  0.0230,  0.0940,  0.0101,  0.0597,\n",
      "        -0.0658,  0.0149, -0.0022, -0.0432, -0.0134, -0.0273,  0.0161, -0.0599,\n",
      "         0.0892,  0.0680,  0.0499, -0.0228, -0.0398, -0.0421,  0.0055, -0.0721,\n",
      "         0.0695,  0.0320,  0.0736,  0.0039,  0.1066, -0.0230, -0.0820, -0.0274,\n",
      "        -0.0550, -0.0126,  0.0723,  0.0721,  0.0341, -0.1763,  0.0115, -0.0597,\n",
      "         0.0553,  0.0402,  0.0207,  0.0090,  0.0654, -0.0715, -0.0035,  0.0284,\n",
      "         0.0344, -0.0239, -0.0987, -0.1656,  0.0359, -0.0273, -0.0651,  0.0209,\n",
      "        -0.0610, -0.0583,  0.0926, -0.0049,  0.0214, -0.0440, -0.0405, -0.0281,\n",
      "        -0.0389, -0.0394,  0.1249, -0.0077,  0.0478,  0.0707, -0.0097, -0.0947,\n",
      "         0.1496,  0.0867,  0.0301,  0.0790, -0.0419, -0.0588,  0.0688, -0.1045,\n",
      "         0.0464,  0.0592,  0.0394, -0.0403, -0.0769,  0.0709,  0.0096,  0.0186,\n",
      "        -0.0084,  0.0552, -0.0465,  0.1336, -0.0821,  0.0158, -0.0225,  0.0293,\n",
      "         0.1180,  0.0491,  0.1004,  0.0402,  0.0557, -0.0361,  0.0339, -0.0208,\n",
      "         0.0475, -0.0432, -0.0253,  0.1057,  0.0593,  0.0989,  0.0571, -0.1079,\n",
      "         0.0138, -0.1349,  0.0667, -0.0693, -0.0845,  0.0301,  0.0068, -0.0396,\n",
      "        -0.0649,  0.0499, -0.1202, -0.0844,  0.1173,  0.1447, -0.0690,  0.0247,\n",
      "        -0.0127, -0.0752,  0.1072, -0.0207, -0.0598, -0.0172,  0.0552,  0.1091,\n",
      "        -0.0815,  0.0461, -0.0290, -0.0309, -0.0525,  0.0775,  0.0891, -0.0027,\n",
      "         0.0689, -0.0314, -0.0082,  0.0205,  0.0646,  0.1183,  0.0145,  0.0648,\n",
      "        -0.0053, -0.0596,  0.1482,  0.0222, -0.0387, -0.0507, -0.0149, -0.0177],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0094,  0.0483,  0.0106,  ...,  0.0589,  0.0351, -0.0086],\n",
      "        [ 0.0096,  0.0818,  0.0434,  ...,  0.0145,  0.0677,  0.0289],\n",
      "        [-0.1131,  0.0581,  0.0374,  ..., -0.0081, -0.0098,  0.0209],\n",
      "        ...,\n",
      "        [-0.0649,  0.0258, -0.0076,  ...,  0.0008,  0.0434, -0.0144],\n",
      "        [ 0.0234, -0.0616, -0.0008,  ..., -0.0351,  0.0223,  0.0368],\n",
      "        [-0.0251, -0.0774,  0.0145,  ..., -0.0432, -0.0492,  0.0317]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0178,  0.0157, -0.0052,  0.0308,  0.0050, -0.0283, -0.0082,  0.0204,\n",
      "        -0.0124,  0.0097, -0.0429,  0.0252,  0.0515, -0.0043, -0.0204, -0.0406,\n",
      "        -0.0583, -0.0126,  0.0336,  0.0181,  0.0386,  0.0260, -0.0584,  0.0168,\n",
      "         0.0575,  0.0013,  0.0367,  0.0401, -0.0254,  0.0003,  0.0076, -0.0262,\n",
      "        -0.0023, -0.0273, -0.0416,  0.0508,  0.0065,  0.0621, -0.0472,  0.0044,\n",
      "        -0.0198, -0.0211, -0.0276,  0.0329, -0.0420,  0.0325,  0.0452, -0.0272,\n",
      "        -0.0539, -0.0285,  0.0565, -0.0301, -0.0485,  0.0199,  0.0470,  0.0612,\n",
      "         0.0316,  0.0556, -0.0563,  0.0505,  0.0558,  0.0047, -0.0591,  0.0532,\n",
      "        -0.0171,  0.0184,  0.0519,  0.0308, -0.0279,  0.0062, -0.0368,  0.0458,\n",
      "         0.0124, -0.0516,  0.0500,  0.0396,  0.0602, -0.0287,  0.0260,  0.0561,\n",
      "         0.0542,  0.0592,  0.0349,  0.0561, -0.0485,  0.0403,  0.0309, -0.0102,\n",
      "         0.0183,  0.0565, -0.0596, -0.0291, -0.0385, -0.0017, -0.0488,  0.0158,\n",
      "         0.0365, -0.0066, -0.0139,  0.0305,  0.0195,  0.0353,  0.0228,  0.0176,\n",
      "         0.0101, -0.0044, -0.0306, -0.0503,  0.0463, -0.0178,  0.0260,  0.0302,\n",
      "        -0.0091, -0.0382,  0.0379,  0.0389, -0.0480,  0.0314,  0.0335,  0.0150,\n",
      "        -0.0469,  0.0016, -0.0187,  0.0182, -0.0160, -0.0101, -0.0062,  0.0105,\n",
      "        -0.0233, -0.0270,  0.0387, -0.0149,  0.0523, -0.0391,  0.0560,  0.0333,\n",
      "        -0.0479,  0.0506, -0.0357, -0.0589, -0.0358,  0.0243,  0.0594, -0.0257,\n",
      "         0.0021, -0.0153, -0.0086,  0.0586, -0.0446, -0.0304, -0.0126,  0.0078,\n",
      "         0.0263, -0.0240,  0.0019,  0.0519, -0.0569, -0.0212,  0.0022,  0.0104,\n",
      "         0.0028, -0.0364, -0.0082, -0.0560, -0.0188, -0.0576,  0.0047, -0.0221,\n",
      "         0.0274, -0.0123,  0.0512,  0.0107,  0.0463,  0.0558, -0.0377,  0.0332,\n",
      "        -0.0307, -0.0430,  0.0044,  0.0029, -0.0583, -0.0464,  0.0577,  0.0364,\n",
      "         0.0250, -0.0448,  0.0315,  0.0011,  0.0242, -0.0152, -0.0598,  0.0520,\n",
      "        -0.0535, -0.0099, -0.0565, -0.0559, -0.0478,  0.0426, -0.0521, -0.0466,\n",
      "        -0.0592, -0.0461,  0.0485, -0.0589,  0.0228,  0.0080,  0.0087,  0.0359,\n",
      "        -0.0213, -0.0242, -0.0443,  0.0418,  0.0574,  0.0571, -0.0250, -0.0317,\n",
      "         0.0203,  0.0035,  0.0071,  0.0269,  0.0158,  0.0618,  0.0260, -0.0157,\n",
      "        -0.0280,  0.0044,  0.0360,  0.0271, -0.0177, -0.0423,  0.0501, -0.0292,\n",
      "         0.0424, -0.0014, -0.0618, -0.0239, -0.0584, -0.0098,  0.0479, -0.0129,\n",
      "        -0.0284,  0.0598,  0.0521, -0.0390, -0.0133, -0.0039, -0.0319, -0.0411,\n",
      "         0.0611, -0.0089,  0.0483,  0.0515,  0.0451,  0.0207,  0.0365,  0.0016],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8218e-01,  5.6598e-01],\n",
      "        [-6.9473e-01, -4.2731e-01],\n",
      "        [ 6.2984e-01, -1.1337e-01],\n",
      "        [ 1.8601e-01, -6.8517e-01],\n",
      "        [-5.4103e-01, -9.8193e-02],\n",
      "        [ 4.2684e-01,  1.6833e-01],\n",
      "        [ 1.7867e-01,  4.7812e-01],\n",
      "        [ 6.6145e-02,  3.9997e-01],\n",
      "        [-4.3628e-01,  2.1940e-01],\n",
      "        [-6.5069e-01,  5.3343e-02],\n",
      "        [-2.4729e-01,  2.5840e-01],\n",
      "        [ 5.5287e-01,  4.1494e-01],\n",
      "        [-3.2725e-01,  6.9761e-01],\n",
      "        [ 1.3470e-01,  2.3680e-01],\n",
      "        [ 3.6058e-01, -6.8513e-01],\n",
      "        [ 2.1859e-01, -9.4254e-02],\n",
      "        [ 7.0291e-01,  7.6363e-01],\n",
      "        [-4.1586e-01, -6.1532e-01],\n",
      "        [-6.8785e-02, -4.2300e-01],\n",
      "        [ 4.0203e-02,  1.6446e-01],\n",
      "        [ 3.0650e-01,  4.7626e-01],\n",
      "        [-3.4949e-01, -4.5626e-03],\n",
      "        [-3.2168e-01, -4.6285e-01],\n",
      "        [ 1.7756e-01, -5.3282e-01],\n",
      "        [ 3.2232e-01,  3.8295e-01],\n",
      "        [-5.8602e-01, -6.7828e-01],\n",
      "        [ 1.0911e-01, -6.5743e-01],\n",
      "        [-6.3709e-02,  6.0004e-01],\n",
      "        [-3.8324e-01, -4.9023e-01],\n",
      "        [-2.5077e-01, -2.9308e-01],\n",
      "        [ 3.1846e-01,  4.9594e-01],\n",
      "        [-4.1136e-01,  2.2555e-01],\n",
      "        [ 4.8686e-01,  7.2606e-02],\n",
      "        [-1.7545e-01, -3.4878e-01],\n",
      "        [ 2.5424e-01, -5.5030e-01],\n",
      "        [-5.8371e-01, -5.0037e-01],\n",
      "        [ 8.6642e-02, -5.3062e-01],\n",
      "        [-3.9315e-01,  2.2162e-02],\n",
      "        [ 3.7091e-01, -4.6499e-01],\n",
      "        [ 2.8312e-01, -9.8790e-02],\n",
      "        [-1.2442e-01,  6.8280e-01],\n",
      "        [ 1.1932e-02,  4.2443e-02],\n",
      "        [-1.3332e-01,  2.9868e-01],\n",
      "        [ 1.8492e-01,  4.9371e-01],\n",
      "        [-6.6406e-01,  7.0042e-01],\n",
      "        [ 6.7389e-01, -1.4621e-01],\n",
      "        [ 2.8809e-01,  2.2943e-01],\n",
      "        [ 5.3514e-01, -2.4258e-01],\n",
      "        [-3.6934e-01, -1.4305e-01],\n",
      "        [ 3.0424e-01,  3.6547e-01],\n",
      "        [-2.4542e-01, -4.6535e-02],\n",
      "        [ 1.9498e-01, -1.0469e-01],\n",
      "        [-4.6613e-01, -6.7883e-02],\n",
      "        [ 5.7961e-01,  9.3429e-02],\n",
      "        [ 9.0530e-02, -3.4722e-01],\n",
      "        [-2.8207e-01, -4.0972e-01],\n",
      "        [-5.2984e-01,  1.0355e-01],\n",
      "        [ 6.8864e-01, -5.9569e-02],\n",
      "        [ 1.7594e-01,  4.9748e-01],\n",
      "        [ 6.2650e-01,  4.9164e-01],\n",
      "        [ 4.8341e-01,  3.6367e-02],\n",
      "        [-6.4590e-01,  3.1869e-01],\n",
      "        [ 3.0085e-01, -5.8709e-01],\n",
      "        [ 6.3266e-01,  7.1503e-01],\n",
      "        [ 7.5290e-03, -1.9811e-01],\n",
      "        [-3.9923e-01,  5.8812e-01],\n",
      "        [ 3.6592e-01,  4.3763e-01],\n",
      "        [-5.5888e-01,  5.9063e-01],\n",
      "        [ 5.9517e-01, -6.6487e-01],\n",
      "        [-6.4080e-01,  1.1893e-01],\n",
      "        [ 1.8689e-01,  3.8730e-01],\n",
      "        [ 5.7173e-01, -1.9152e-01],\n",
      "        [-3.6902e-02,  3.7012e-01],\n",
      "        [-6.9766e-01,  9.3233e-04],\n",
      "        [ 5.9769e-01,  7.1616e-01],\n",
      "        [ 4.3941e-01, -6.9194e-01],\n",
      "        [-4.7150e-01,  3.3675e-01],\n",
      "        [-9.4762e-02, -1.2224e-01],\n",
      "        [-4.5357e-01,  2.2811e-01],\n",
      "        [ 6.4532e-02,  4.8226e-01],\n",
      "        [ 4.8617e-01,  6.1330e-01],\n",
      "        [ 6.0806e-01, -2.9747e-01],\n",
      "        [-2.2559e-01, -3.7099e-01],\n",
      "        [ 1.6593e-01, -4.6199e-01],\n",
      "        [-3.1245e-02,  5.0036e-01],\n",
      "        [-2.5835e-01,  3.8824e-01],\n",
      "        [-6.3451e-01,  5.1338e-01],\n",
      "        [-4.2733e-01, -3.2051e-01],\n",
      "        [-3.1914e-01,  6.7701e-01],\n",
      "        [-2.6749e-01, -6.7106e-01],\n",
      "        [ 1.0163e-01,  5.1664e-01],\n",
      "        [ 3.5762e-01,  6.3632e-01],\n",
      "        [-6.3536e-01,  1.9885e-01],\n",
      "        [-2.3740e-02, -3.3283e-01],\n",
      "        [ 5.8588e-01,  5.4360e-01],\n",
      "        [ 4.9153e-01, -6.3902e-01],\n",
      "        [-2.6229e-01, -2.4074e-01],\n",
      "        [ 4.3784e-01,  6.3436e-02],\n",
      "        [-4.2065e-01,  4.8072e-01],\n",
      "        [ 4.0540e-01, -1.5288e-01],\n",
      "        [-2.8993e-01,  3.7963e-01],\n",
      "        [-2.9110e-01,  4.0397e-02],\n",
      "        [-3.2273e-01,  2.5497e-01],\n",
      "        [ 1.7669e-01, -3.0646e-01],\n",
      "        [ 4.0838e-01, -1.6013e-01],\n",
      "        [ 1.4467e-01, -4.6378e-01],\n",
      "        [ 3.6170e-01, -4.0853e-01],\n",
      "        [ 2.8877e-02, -6.2970e-01],\n",
      "        [ 3.2962e-01, -6.4160e-01],\n",
      "        [ 3.7938e-03,  7.2947e-01],\n",
      "        [-2.0831e-01,  5.0839e-01],\n",
      "        [-1.5320e-01, -5.2187e-01],\n",
      "        [-2.8124e-01, -1.9718e-01],\n",
      "        [-6.6749e-01,  3.2049e-01],\n",
      "        [ 6.7992e-02, -2.5412e-01],\n",
      "        [-2.5947e-01,  1.3756e-01],\n",
      "        [ 1.9193e-01, -1.7183e-01],\n",
      "        [ 1.5575e-01,  1.3826e-01],\n",
      "        [-2.8117e-01, -6.5160e-01],\n",
      "        [-3.4668e-01,  6.9024e-01],\n",
      "        [-3.2827e-01,  6.7439e-01],\n",
      "        [ 3.0413e-01, -5.5614e-01],\n",
      "        [-3.5469e-01,  5.8687e-01],\n",
      "        [-7.1127e-01,  2.4955e-01],\n",
      "        [-3.7472e-01, -3.5398e-01],\n",
      "        [ 5.6485e-01, -4.0267e-01],\n",
      "        [-4.5363e-01,  3.0025e-01],\n",
      "        [ 3.2351e-01,  3.8077e-01],\n",
      "        [ 5.2973e-01,  5.2814e-01],\n",
      "        [ 6.0477e-01, -1.2397e-01],\n",
      "        [-6.5454e-01,  2.6152e-01],\n",
      "        [-2.6970e-01,  4.9587e-01],\n",
      "        [ 1.0926e-01, -5.9109e-01],\n",
      "        [ 5.6671e-01, -1.1511e-01],\n",
      "        [ 5.0173e-01,  6.3567e-01],\n",
      "        [ 6.5396e-01,  6.2526e-01],\n",
      "        [-9.0124e-02,  2.5546e-01],\n",
      "        [ 4.5982e-01, -8.4426e-02],\n",
      "        [-1.7531e-01,  4.3585e-01],\n",
      "        [-3.0031e-01, -6.9241e-01],\n",
      "        [-1.8099e-01,  2.7968e-01],\n",
      "        [ 2.7887e-01, -5.1518e-01],\n",
      "        [ 2.2536e-01,  2.3591e-01],\n",
      "        [-2.5450e-01,  4.8784e-01],\n",
      "        [-7.1216e-02,  3.0238e-01],\n",
      "        [-5.3021e-02,  2.2160e-01],\n",
      "        [ 5.5632e-01,  1.3984e-01],\n",
      "        [ 7.5872e-01,  7.4015e-03],\n",
      "        [-5.9382e-01,  4.0056e-01],\n",
      "        [ 1.3694e-01,  5.3309e-01],\n",
      "        [ 4.4098e-01,  2.3686e-01],\n",
      "        [ 1.1822e-01,  1.8132e-01],\n",
      "        [ 5.2977e-01, -4.5093e-01],\n",
      "        [ 3.9905e-01,  4.6210e-01],\n",
      "        [-4.1557e-01,  3.0167e-01],\n",
      "        [-3.0462e-01,  2.1707e-01],\n",
      "        [-6.4226e-01,  5.3984e-01],\n",
      "        [ 2.6112e-01, -9.7125e-02],\n",
      "        [ 5.7068e-01,  7.0386e-01],\n",
      "        [ 8.4796e-02, -4.7921e-01],\n",
      "        [ 2.5639e-01, -1.7300e-01],\n",
      "        [-5.8770e-01,  3.3763e-01],\n",
      "        [-3.4795e-01,  2.3334e-01],\n",
      "        [ 3.8979e-01,  3.4102e-01],\n",
      "        [ 1.2926e-01,  1.7111e-01],\n",
      "        [-2.5785e-01, -3.2790e-01],\n",
      "        [ 3.6058e-01,  2.7018e-01],\n",
      "        [ 5.0976e-01,  2.9636e-01],\n",
      "        [-6.0572e-01,  1.5198e-01],\n",
      "        [-2.5515e-01, -3.3210e-01],\n",
      "        [ 3.4910e-01, -5.8228e-01],\n",
      "        [-5.6525e-01, -2.9420e-01],\n",
      "        [ 7.0228e-01,  5.6463e-01],\n",
      "        [ 3.4927e-01, -9.8450e-02],\n",
      "        [ 2.3874e-01,  2.3643e-01],\n",
      "        [ 3.5534e-01,  6.0397e-01],\n",
      "        [-4.7495e-01,  3.0245e-01],\n",
      "        [-6.8794e-01, -2.0195e-01],\n",
      "        [-9.4907e-02, -6.3275e-01],\n",
      "        [-5.4239e-02,  5.4448e-02],\n",
      "        [ 3.2109e-01, -7.0279e-01],\n",
      "        [-5.2258e-01,  4.4647e-01],\n",
      "        [ 3.8752e-01,  5.7168e-01],\n",
      "        [-8.3963e-02,  5.5879e-01],\n",
      "        [-1.7851e-01, -6.0653e-01],\n",
      "        [-6.6934e-02,  3.7513e-01],\n",
      "        [ 5.2057e-01,  5.8036e-01],\n",
      "        [-4.7945e-01,  1.6634e-01],\n",
      "        [ 4.6555e-01,  3.4240e-01],\n",
      "        [-4.3921e-01,  3.8253e-01],\n",
      "        [ 3.1257e-01,  1.3890e-01],\n",
      "        [ 6.0823e-01, -5.6006e-01],\n",
      "        [-4.6574e-01, -4.4670e-01],\n",
      "        [ 1.6154e-01, -6.1186e-01],\n",
      "        [-1.9496e-01, -1.0381e-01],\n",
      "        [ 2.5783e-01, -2.3873e-01],\n",
      "        [-4.1616e-01,  2.5159e-01],\n",
      "        [-8.0303e-02, -3.4124e-02],\n",
      "        [ 6.6704e-02, -1.6296e-01],\n",
      "        [ 2.3636e-01, -3.4552e-01],\n",
      "        [ 6.4014e-01,  2.7087e-01],\n",
      "        [ 3.2597e-01, -3.7484e-01],\n",
      "        [-4.1540e-01,  5.2229e-01],\n",
      "        [-3.9271e-01,  4.6125e-02],\n",
      "        [-2.4716e-01,  9.8247e-02],\n",
      "        [-6.6404e-01, -4.6775e-01],\n",
      "        [-4.6739e-02,  3.4177e-01],\n",
      "        [ 1.3973e-01,  1.6698e-02],\n",
      "        [-2.2407e-04,  4.9755e-01],\n",
      "        [ 3.2219e-01, -9.6152e-02],\n",
      "        [-5.3932e-01,  3.0934e-01],\n",
      "        [ 4.2206e-01,  2.8390e-01],\n",
      "        [ 7.2891e-01, -1.6729e-01],\n",
      "        [-5.6002e-02,  2.2003e-01],\n",
      "        [ 2.8151e-01, -4.1437e-01],\n",
      "        [-3.0315e-01,  4.5058e-01],\n",
      "        [ 5.5197e-01, -6.3226e-01],\n",
      "        [ 2.5079e-01,  7.0992e-01],\n",
      "        [ 4.6836e-01,  5.7135e-01],\n",
      "        [ 5.2375e-01,  8.6223e-02],\n",
      "        [-1.2540e-01, -3.1753e-01],\n",
      "        [-7.0515e-01,  2.3968e-02],\n",
      "        [ 4.5626e-01,  1.3257e-01],\n",
      "        [-5.6764e-01,  2.8327e-01],\n",
      "        [ 2.9107e-01, -6.0650e-01],\n",
      "        [ 1.6766e-01, -5.1105e-01],\n",
      "        [ 5.9060e-03,  6.5314e-02],\n",
      "        [ 2.5235e-02,  3.1346e-01],\n",
      "        [ 7.1164e-01,  6.0289e-01],\n",
      "        [-5.1935e-01, -3.9203e-01],\n",
      "        [-7.2813e-01, -5.5079e-01],\n",
      "        [-6.3216e-01, -6.4425e-01],\n",
      "        [-1.7531e-04,  6.8800e-01],\n",
      "        [ 3.3416e-01, -3.3028e-02],\n",
      "        [ 4.3829e-01,  3.4970e-01],\n",
      "        [ 2.9548e-01,  3.4159e-01],\n",
      "        [ 4.6714e-01,  5.3194e-01],\n",
      "        [-4.5858e-01,  2.8870e-01],\n",
      "        [-6.2463e-01,  1.5259e-01],\n",
      "        [ 3.3562e-01, -4.1907e-01],\n",
      "        [-4.1685e-01, -1.2139e-01],\n",
      "        [ 2.4362e-01, -2.1946e-01],\n",
      "        [ 1.7699e-01,  2.0374e-01],\n",
      "        [-6.4875e-01,  5.6448e-03],\n",
      "        [-4.0931e-01, -1.5298e-01],\n",
      "        [ 1.8487e-01, -2.7861e-01],\n",
      "        [-1.8291e-01,  5.5825e-01],\n",
      "        [-4.4403e-01, -4.2362e-01],\n",
      "        [ 1.9901e-01, -5.0420e-01],\n",
      "        [ 5.2316e-01,  1.6194e-01],\n",
      "        [-6.6703e-01,  6.1817e-02],\n",
      "        [-4.5637e-01, -7.1233e-01],\n",
      "        [-6.5340e-02,  1.4942e-01],\n",
      "        [ 5.8836e-01, -6.9446e-01],\n",
      "        [ 5.2214e-01, -4.6488e-01],\n",
      "        [-1.8038e-01,  5.7456e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0442,  0.0193, -0.0067,  ...,  0.0183,  0.0047, -0.0064],\n",
      "        [ 0.0274, -0.0464, -0.0100,  ...,  0.0227,  0.0437,  0.0190],\n",
      "        [ 0.0046,  0.0123,  0.0315,  ..., -0.0400,  0.0155,  0.0471],\n",
      "        ...,\n",
      "        [ 0.0564,  0.0105,  0.0682,  ...,  0.0344, -0.0224,  0.0427],\n",
      "        [-0.0282, -0.0939, -0.0222,  ..., -0.0372, -0.0585,  0.0516],\n",
      "        [-0.0496,  0.0035,  0.0420,  ..., -0.0329, -0.0617, -0.0513]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.0951e-02,  5.7841e-02, -3.7248e-02, -5.8588e-02, -1.9208e-02,\n",
      "         2.2207e-02, -3.5753e-03,  4.8248e-02, -9.1232e-03,  1.7009e-02,\n",
      "         1.6412e-02,  6.2405e-02,  3.2541e-02, -2.8887e-02, -2.8243e-02,\n",
      "        -5.8109e-02, -6.1998e-02,  3.6875e-02, -4.8019e-02,  5.9659e-02,\n",
      "         4.6987e-03, -3.8773e-02, -4.2779e-02,  6.1361e-02, -5.9022e-02,\n",
      "        -2.0452e-02, -1.8406e-02,  3.5282e-02,  5.9709e-02, -4.9942e-02,\n",
      "         3.5160e-02,  3.0755e-02,  5.2583e-02, -6.0079e-02, -1.4917e-02,\n",
      "         7.6762e-03,  4.9139e-02, -5.6244e-02, -5.3779e-02, -1.2168e-02,\n",
      "        -2.0957e-02, -5.8930e-03,  8.6695e-03, -5.5256e-02,  3.3138e-02,\n",
      "         3.0181e-04,  4.2683e-03, -3.7927e-03, -4.0339e-02, -4.8646e-02,\n",
      "        -5.2320e-02,  8.2003e-03, -1.5234e-02, -3.8362e-03,  5.6544e-02,\n",
      "         5.9934e-03,  8.5806e-04,  1.5542e-02,  1.7344e-02,  3.6105e-02,\n",
      "        -2.4440e-02,  1.1283e-02,  1.6209e-02,  5.4606e-02, -8.5330e-03,\n",
      "        -3.8201e-02,  5.7194e-03, -5.0232e-02, -3.4120e-02, -2.3738e-02,\n",
      "         3.6226e-03, -1.7229e-02, -3.9678e-02,  3.6009e-03,  2.5683e-02,\n",
      "        -3.8281e-02,  2.1487e-02,  6.1130e-02, -9.0778e-03, -1.8095e-02,\n",
      "        -2.4403e-02,  3.2546e-02, -9.9183e-03,  4.3504e-02,  4.9002e-02,\n",
      "         3.9138e-02,  5.6418e-02, -1.3928e-03,  1.2450e-02, -4.2745e-02,\n",
      "         1.3057e-03,  6.0094e-02, -2.0099e-02,  1.4219e-02,  6.0146e-02,\n",
      "        -3.0475e-02, -3.5184e-02, -1.1373e-02,  3.2114e-02, -2.5951e-02,\n",
      "        -5.4836e-02, -5.1665e-02, -1.4098e-02,  3.5141e-02, -4.2220e-03,\n",
      "        -5.3400e-02, -4.7043e-02,  6.3198e-03, -3.6851e-02,  3.7459e-02,\n",
      "         3.6808e-02,  1.7472e-02, -1.5297e-02, -6.0086e-02,  5.4659e-02,\n",
      "         1.7677e-02, -2.3113e-02,  6.6033e-03,  2.4150e-02, -3.2161e-02,\n",
      "         2.5508e-02, -6.1097e-03,  2.3266e-05,  3.5817e-02,  1.8869e-02,\n",
      "         1.7890e-02,  2.0002e-02, -4.6320e-02,  6.0592e-03, -2.5404e-02,\n",
      "        -1.0874e-02, -5.1629e-02, -4.1038e-02,  3.0031e-03,  4.5358e-02,\n",
      "         2.4688e-02, -4.9153e-02, -4.3221e-02,  2.7958e-02,  2.5826e-02,\n",
      "         1.6079e-03,  8.4912e-03,  6.1917e-02,  4.9077e-02,  2.6208e-03,\n",
      "        -2.5297e-02, -2.0278e-02, -2.1311e-02, -5.8728e-02, -3.7306e-02,\n",
      "        -4.4542e-03, -9.1809e-03,  2.8505e-02,  2.8125e-02,  3.1717e-02,\n",
      "        -5.1702e-02,  6.4034e-03, -1.1026e-02,  5.2254e-02, -4.5408e-03,\n",
      "        -5.9369e-02,  5.2362e-04, -2.4694e-02, -1.9925e-02,  3.0106e-02,\n",
      "         1.3313e-03, -3.6649e-02,  5.8802e-03, -2.2050e-02, -1.0553e-02,\n",
      "        -2.4022e-02, -1.8785e-02,  4.1549e-02,  4.9052e-02,  4.5933e-02,\n",
      "        -3.5872e-02,  2.9737e-03, -2.1784e-02,  5.2878e-02, -3.7117e-02,\n",
      "         5.5427e-02, -3.5354e-02, -5.5854e-02,  5.4868e-02, -1.4868e-02,\n",
      "         2.8582e-03, -2.1970e-02, -4.6983e-02,  4.8789e-02, -2.0362e-02,\n",
      "         4.6495e-02, -5.1556e-02,  3.4223e-02,  4.7366e-03,  6.2872e-03,\n",
      "         5.0877e-02,  2.9605e-02,  2.1943e-03,  1.0686e-02, -7.4851e-03,\n",
      "         4.0097e-02,  2.1618e-02,  4.2533e-03,  3.3764e-02, -5.3321e-02,\n",
      "         3.2598e-02, -4.3309e-02, -5.2519e-02, -2.6050e-03, -3.8956e-03,\n",
      "         7.1504e-03, -7.3909e-03,  5.2386e-02, -5.5781e-03, -2.9413e-03,\n",
      "         8.1002e-04, -5.0123e-02, -3.4847e-02,  1.6085e-02, -3.1649e-02,\n",
      "         1.0388e-02,  3.1322e-02,  4.7224e-02,  4.6470e-02, -2.5271e-02,\n",
      "        -3.1758e-02,  2.8052e-02, -4.8810e-02, -5.5855e-03,  5.6014e-02,\n",
      "        -1.8482e-02,  5.4311e-02, -6.1802e-02, -5.5715e-02,  5.7946e-02,\n",
      "        -3.7582e-02,  2.0032e-02,  5.2407e-02, -5.5563e-03, -5.8717e-02,\n",
      "         1.1658e-02,  1.3584e-02, -4.6968e-02,  3.2099e-02,  4.5605e-02,\n",
      "        -1.5046e-03,  1.8088e-02,  1.0092e-02,  5.6361e-02,  5.4706e-02,\n",
      "         7.6314e-03,  5.3528e-02, -3.9944e-02,  3.7512e-02,  1.1702e-02,\n",
      "        -1.2393e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9791, 1.0145, 0.9782, 0.9875, 1.0039, 1.0084, 0.9638, 0.9762, 1.0163,\n",
      "        0.9892, 1.0013, 1.0055, 0.9264, 0.9906, 0.9876, 1.0002, 0.9705, 0.9960,\n",
      "        1.1350, 0.9614, 0.9968, 1.0010, 0.9733, 0.9703, 1.0358, 0.9718, 0.9867,\n",
      "        0.9582, 1.0106, 0.9686, 0.9823, 1.0196, 0.9612, 0.9749, 1.0127, 1.0217,\n",
      "        0.9976, 1.0090, 0.9987, 1.0106, 0.9115, 0.9834, 0.9853, 0.9871, 1.0144,\n",
      "        0.9442, 1.0846, 1.0156, 1.0218, 0.9957, 1.0025, 1.0056, 0.9246, 0.9964,\n",
      "        1.0149, 0.9967, 1.0221, 0.9988, 0.9974, 0.9724, 1.0197, 1.0242, 0.9688,\n",
      "        1.0060, 0.9835, 0.9346, 1.0150, 0.9762, 0.9734, 0.9513, 0.9744, 0.9984,\n",
      "        1.0328, 1.0125, 0.9874, 0.9537, 1.0474, 0.9860, 0.9502, 1.0109, 1.0379,\n",
      "        0.9790, 0.9529, 1.0026, 0.9501, 0.9639, 0.9830, 1.0414, 0.9970, 1.0451,\n",
      "        1.0972, 0.9636, 1.0323, 0.9595, 1.0181, 1.0018, 1.0351, 1.0149, 1.0063,\n",
      "        1.0317, 0.9990, 0.9972, 0.9924, 0.9842, 1.0477, 0.9714, 0.9598, 0.9667,\n",
      "        0.9670, 1.0285, 0.9978, 0.9747, 1.0120, 0.9518, 0.9608, 1.0069, 0.9814,\n",
      "        0.9395, 1.0252, 0.9804, 0.9694, 0.9884, 0.9393, 0.9761, 0.9019, 0.9952,\n",
      "        0.9868, 1.0458, 0.9382, 0.9949, 0.9798, 1.0046, 0.9925, 1.0118, 0.9888,\n",
      "        0.9945, 0.9509, 1.0094, 0.9627, 0.9866, 1.0044, 0.9874, 1.0490, 0.9949,\n",
      "        0.9256, 0.9555, 0.9671, 1.0022, 0.9847, 0.9981, 1.0326, 0.9817, 0.9730,\n",
      "        1.0000, 0.9949, 1.0273, 0.8876, 1.0105, 0.9948, 0.9696, 1.0177, 1.0963,\n",
      "        0.9639, 0.9949, 0.9830, 1.0615, 0.9665, 0.9976, 1.0079, 0.9671, 0.9767,\n",
      "        0.9915, 0.9780, 0.9862, 0.9885, 1.0030, 1.0082, 0.9828, 1.0090, 0.9631,\n",
      "        0.9595, 0.9887, 0.9888, 0.9743, 0.9720, 1.0359, 0.9873, 0.9932, 0.9618,\n",
      "        1.0109, 0.9875, 0.9858, 0.9734, 0.9865, 0.9703, 0.9929, 0.9359, 0.9628,\n",
      "        1.0323, 0.9976, 1.0532, 1.0336, 1.0155, 0.9677, 1.0356, 0.9413, 0.9997,\n",
      "        1.0037, 1.0507, 1.0159, 0.9466, 0.9855, 0.9956, 1.0307, 1.0089, 0.9950,\n",
      "        0.9994, 0.9849, 0.9689, 0.9867, 0.9764, 0.9925, 0.9774, 0.9223, 1.0253,\n",
      "        1.0394, 1.0156, 0.9375, 0.9765, 0.9706, 0.9526, 0.9709, 1.0058, 1.0063,\n",
      "        1.0055, 1.0553, 0.9688, 0.9870, 0.9837, 0.9841, 1.0155, 1.0276, 1.0400,\n",
      "        0.9746, 1.0455, 1.0288, 0.9714, 1.0165, 1.0133, 1.0018, 1.0132, 0.9761,\n",
      "        1.0105, 1.0292, 1.0436, 1.0271], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0428,  0.0401, -0.0462,  0.0284,  0.0088, -0.0689, -0.0070, -0.0836,\n",
      "         0.0845,  0.0338,  0.0375,  0.0136, -0.0382,  0.0050, -0.0373, -0.0385,\n",
      "         0.0116, -0.1110, -0.0258,  0.0066, -0.0402,  0.0137, -0.0185, -0.0134,\n",
      "        -0.0252, -0.0332, -0.0080, -0.0434,  0.0006, -0.0463, -0.0180, -0.0113,\n",
      "        -0.0252,  0.0364,  0.0154,  0.0164,  0.0073,  0.0042,  0.0049,  0.0238,\n",
      "        -0.0196, -0.0200, -0.0110,  0.0165, -0.0082, -0.0268,  0.0395, -0.0837,\n",
      "         0.0298,  0.0290, -0.0533, -0.0281, -0.0162, -0.0382,  0.0048,  0.0065,\n",
      "        -0.0005, -0.0025, -0.0170,  0.0259, -0.0405, -0.0193, -0.1047, -0.0042,\n",
      "        -0.0467, -0.0508, -0.0357, -0.0524, -0.0889, -0.0677,  0.0078, -0.0320,\n",
      "        -0.0166,  0.0120, -0.0289, -0.0294,  0.0436,  0.0008, -0.0211, -0.0661,\n",
      "        -0.0059, -0.0504, -0.0596, -0.0466, -0.0390,  0.0216, -0.0179, -0.0288,\n",
      "         0.0020, -0.0282,  0.0287, -0.0421,  0.0178, -0.0636, -0.0520, -0.0943,\n",
      "        -0.0492, -0.0112, -0.0418, -0.0109, -0.0189, -0.0181,  0.0373, -0.0043,\n",
      "        -0.0227,  0.0255, -0.0275, -0.0027, -0.0257, -0.0334, -0.0365, -0.0811,\n",
      "         0.0370, -0.1052, -0.0663, -0.0120, -0.0141, -0.0157, -0.0219, -0.0080,\n",
      "        -0.0695, -0.0009, -0.0367, -0.0320, -0.0459, -0.0226, -0.0382, -0.0234,\n",
      "        -0.0788, -0.0404, -0.0314,  0.0136, -0.0191, -0.0065, -0.0100,  0.0280,\n",
      "        -0.0240, -0.0193, -0.0383, -0.0524, -0.0413, -0.0305, -0.0644, -0.0630,\n",
      "         0.0026,  0.0034, -0.0883,  0.0079, -0.0089, -0.0239, -0.0340, -0.0439,\n",
      "        -0.0238,  0.0152,  0.0364, -0.0842, -0.0152,  0.0069, -0.0471, -0.0860,\n",
      "        -0.0016, -0.0161, -0.0258, -0.0251, -0.0612, -0.0152,  0.0163, -0.0338,\n",
      "        -0.0396,  0.0142, -0.0247,  0.0307, -0.0843, -0.0382, -0.0341,  0.0023,\n",
      "        -0.0215, -0.0440, -0.0317,  0.0003, -0.0108, -0.0565, -0.0003, -0.0723,\n",
      "        -0.0224, -0.0146, -0.0126, -0.0306, -0.0238, -0.0149, -0.0205, -0.0471,\n",
      "        -0.0172,  0.0226, -0.0463, -0.0014,  0.0134, -0.0081, -0.0333, -0.0548,\n",
      "        -0.0696, -0.0226, -0.0159, -0.0073,  0.1431, -0.0452, -0.0412, -0.0872,\n",
      "         0.0816, -0.0836, -0.0069,  0.0434, -0.0442, -0.0130, -0.0091, -0.0248,\n",
      "        -0.0514, -0.0487, -0.0190, -0.0262, -0.0129,  0.0041,  0.0505, -0.0191,\n",
      "        -0.0440,  0.0019,  0.0318,  0.0029,  0.0396,  0.0074, -0.0570, -0.0489,\n",
      "        -0.0074, -0.0251, -0.0079, -0.0498, -0.0384, -0.0408, -0.0357, -0.0067,\n",
      "        -0.0548,  0.0270,  0.0017, -0.0238, -0.0283,  0.0153, -0.0285, -0.0336,\n",
      "        -0.0802, -0.0485, -0.0814, -0.0218, -0.0150, -0.0181, -0.0377, -0.0520],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9567, 1.0046, 0.9930, 0.9855, 1.0352, 1.0102, 0.9439, 0.9857, 1.0116,\n",
      "        1.0453, 0.9720, 0.9681, 1.0309, 0.9651, 0.9627, 1.0175, 0.9782, 0.9504,\n",
      "        0.9726, 0.9880, 1.0147, 0.9987, 1.0317, 0.9650, 0.9749, 1.0058, 1.0286,\n",
      "        1.0184, 1.0498, 0.9816, 0.9720, 1.0183, 0.9438, 1.0174, 1.0284, 0.9433,\n",
      "        1.0473, 0.9834, 0.9821, 1.0094, 0.9931, 0.9712, 0.9878, 0.9870, 1.0231,\n",
      "        1.0316, 1.0110, 0.9897, 1.0315, 1.0506, 1.0927, 0.9495, 1.3005, 0.9696,\n",
      "        0.9901, 1.0115, 1.0039, 1.0079, 0.9663, 0.9712, 0.9718, 0.9353, 0.9766,\n",
      "        1.0261, 0.9929, 0.9935, 0.9769, 1.0308, 1.0067, 1.0067, 1.1037, 1.0470,\n",
      "        1.0264, 1.0376, 0.9758, 1.0184, 0.9915, 0.9571, 1.0356, 0.9707, 1.0063,\n",
      "        0.9539, 0.9424, 0.9955, 1.0677, 1.0948, 1.0063, 0.9654, 1.0596, 1.0019,\n",
      "        0.9913, 1.0000, 1.0172, 1.0210, 0.9548, 1.0273, 1.0380, 0.9561, 0.9740,\n",
      "        0.9345, 1.0087, 0.9906, 1.0411, 1.0905, 1.0422, 0.9898, 0.9954, 1.0073,\n",
      "        1.0455, 1.0390, 0.9911, 1.0300, 0.9592, 0.9733, 0.9944, 0.9627, 0.9636,\n",
      "        1.0116, 0.9749, 0.9508, 0.9902, 0.9638, 1.0010, 1.0268, 1.0131, 0.9853,\n",
      "        0.9744, 1.0636, 0.9373, 1.0110, 0.9963, 0.9810, 0.9371, 0.9997, 0.9471,\n",
      "        0.9780, 1.0226, 0.9878, 0.9977, 1.0305, 1.0093, 0.9618, 1.0326, 1.0985,\n",
      "        1.0086, 1.0452, 0.9721, 1.0119, 1.0238, 1.1153, 1.0270, 0.9378, 1.0596,\n",
      "        1.0324, 1.0417, 1.0439, 1.0834, 1.0240, 0.9940, 1.0549, 0.9804, 0.9891,\n",
      "        0.9715, 0.9208, 1.0074, 1.0534, 0.9802, 0.9386, 0.9579, 0.9989, 1.0293,\n",
      "        1.0530, 0.9760, 1.0132, 1.0110, 0.9685, 1.0996, 1.0028, 0.9971, 1.0177,\n",
      "        0.9718, 0.9632, 1.0399, 1.0243, 1.1646, 1.0262, 1.0195, 0.9452, 0.9785,\n",
      "        0.9770, 0.9461, 1.0271, 0.9709, 1.0141, 1.0162, 0.9989, 0.9547, 1.0350,\n",
      "        0.9299, 0.9693, 1.0262, 0.9550, 1.0148, 0.9817, 1.0387, 1.0505, 1.0274,\n",
      "        0.9359, 0.9398, 0.9790, 0.9486, 1.0387, 1.0401, 1.0068, 0.9980, 1.0541,\n",
      "        1.0158, 0.9617, 0.9198, 1.0513, 1.0243, 1.0223, 1.0275, 1.0127, 0.9868,\n",
      "        0.9948, 1.0810, 1.0373, 0.9970, 0.9538, 0.9694, 0.9635, 1.0158, 1.0431,\n",
      "        1.0147, 0.9500, 0.9699, 0.9944, 0.9678, 1.0099, 1.0201, 0.9395, 1.0310,\n",
      "        1.0625, 0.9552, 1.0446, 1.1143, 0.9967, 0.9922, 1.0207, 1.0410, 0.9467,\n",
      "        1.0296, 0.9476, 1.0332, 0.9519], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0647, -0.0096, -0.1148, -0.0754, -0.0532, -0.0342, -0.0884, -0.0339,\n",
      "        -0.0639, -0.0142, -0.0557, -0.0827, -0.0013, -0.0767, -0.0074, -0.0294,\n",
      "        -0.0772, -0.0215, -0.0826, -0.0374,  0.0004, -0.0209, -0.0362, -0.0641,\n",
      "        -0.1230, -0.0249, -0.0749, -0.0158, -0.0051, -0.0376, -0.0677, -0.0535,\n",
      "        -0.0232, -0.0520, -0.0654, -0.0947, -0.0043, -0.0683, -0.0600, -0.0482,\n",
      "        -0.0363, -0.0342, -0.0235, -0.0589, -0.0346, -0.0423, -0.0489, -0.0760,\n",
      "         0.0148, -0.0841, -0.0411, -0.1219,  0.1132, -0.0819, -0.0645, -0.0317,\n",
      "        -0.0444, -0.0358, -0.0488, -0.0571, -0.0724, -0.0350, -0.0369, -0.0426,\n",
      "        -0.0499, -0.0904, -0.0503, -0.0574, -0.0261, -0.0727, -0.0194, -0.0800,\n",
      "         0.0014, -0.0394, -0.0552, -0.0128, -0.0280, -0.0376,  0.0013, -0.0578,\n",
      "        -0.0106, -0.0756, -0.0539, -0.0774, -0.0702, -0.0294, -0.0962, -0.0531,\n",
      "        -0.0944, -0.0235, -0.0829, -0.0569, -0.0452, -0.0298, -0.0262, -0.0415,\n",
      "        -0.0436, -0.0891, -0.0921, -0.0755, -0.0090, -0.0522, -0.1062, -0.0150,\n",
      "        -0.0376, -0.0417, -0.0310,  0.0022, -0.0273, -0.0748, -0.1192, -0.0874,\n",
      "        -0.0600, -0.0830, -0.0405, -0.0552, -0.0386, -0.0540, -0.0411, -0.0846,\n",
      "        -0.0747, -0.1189, -0.0081, -0.0750, -0.0210, -0.0802, -0.0728, -0.0109,\n",
      "        -0.0686, -0.0585, -0.0683, -0.0819, -0.0916, -0.0266, -0.0666, -0.0447,\n",
      "         0.0022, -0.0526, -0.0244, -0.0120, -0.0047, -0.0714, -0.0460, -0.0395,\n",
      "         0.0103, -0.0288, -0.0869, -0.0773, -0.0204,  0.0346,  0.0056, -0.0719,\n",
      "        -0.0397, -0.0365, -0.0445, -0.0932, -0.0009, -0.0562, -0.0698, -0.0014,\n",
      "        -0.0467, -0.0388, -0.0610, -0.0735, -0.0638, -0.0571, -0.0169, -0.0447,\n",
      "        -0.0175, -0.0578, -0.0904, -0.0230, -0.0191, -0.0857, -0.0539, -0.0766,\n",
      "        -0.0340, -0.0444, -0.0742,  0.0425, -0.0040, -0.0978, -0.0008, -0.0460,\n",
      "         0.0436, -0.0086, -0.0145, -0.0196, -0.0387, -0.0203, -0.0619, -0.0270,\n",
      "        -0.0323, -0.0200, -0.0081, -0.0542, -0.0510, -0.0532, -0.0388, -0.0104,\n",
      "        -0.0571, -0.0542, -0.0189, -0.1088, -0.0711, -0.0157, -0.1010, -0.0782,\n",
      "        -0.0579, -0.0616, -0.0843, -0.0309, -0.0154,  0.0130, -0.0663,  0.0117,\n",
      "        -0.0248, -0.0626, -0.0750, -0.0263, -0.0294, -0.0393, -0.0060, -0.0216,\n",
      "        -0.0668, -0.0404, -0.0303, -0.0435, -0.0587, -0.1137, -0.0260, -0.0167,\n",
      "        -0.0226, -0.0062, -0.0601, -0.0640, -0.0587, -0.0389, -0.0632, -0.0802,\n",
      "        -0.0077, -0.0486,  0.0164, -0.0236, -0.0432, -0.0206, -0.0341, -0.0821,\n",
      "        -0.0142, -0.0583, -0.0118, -0.0486, -0.0437, -0.0793, -0.0443, -0.0683],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.0523, 1.0256, 1.0703, 1.0104, 1.0188, 1.0003, 0.9992, 1.0193, 0.9860,\n",
      "        1.1494, 1.0102, 1.0242, 1.0646, 0.9891, 1.0311, 0.9912, 1.0563, 1.0911,\n",
      "        1.0581, 0.9915, 1.0671, 1.1213, 1.0807, 0.9841, 0.9686, 1.0355, 1.0102,\n",
      "        1.1032, 0.9580, 1.0129, 0.9928, 1.0121, 1.0204, 1.0090, 1.0361, 0.9651,\n",
      "        1.0066, 1.0229, 1.0536, 1.0179, 0.9859, 1.0249, 0.9627, 0.9726, 1.0117,\n",
      "        1.0025, 1.0012, 1.0395, 1.0115, 0.9982, 0.9879, 1.0504, 1.0789, 1.0578,\n",
      "        1.0085, 1.1119, 1.0439, 0.9607, 1.0091, 0.9956, 1.0426, 0.9915, 1.0515,\n",
      "        1.0236, 1.0023, 1.0074, 0.9818, 1.0155, 0.9857, 1.0348, 1.0818, 1.0114,\n",
      "        1.0355, 1.0449, 0.9776, 1.0147, 1.0330, 1.0374, 0.9785, 1.0167, 1.0218,\n",
      "        1.0467, 1.0087, 0.9952, 0.9839, 1.0454, 1.0146, 1.0222, 1.0029, 0.9714,\n",
      "        1.0374, 0.9864, 1.1522, 1.0977, 1.0682, 0.9657, 0.9847, 1.0683, 0.9773,\n",
      "        1.0370, 1.0359, 0.9818, 0.9979, 0.9933, 1.1084, 1.0458, 1.0075, 1.0139,\n",
      "        1.0179, 1.0475, 1.0418, 1.0474, 1.0208, 1.0384, 1.0217, 0.9517, 1.0469,\n",
      "        0.9945, 1.0008, 1.1358, 0.9893, 1.0155, 1.0491, 1.0975, 0.9936, 1.0236,\n",
      "        0.9842, 1.0323, 1.0765, 0.9808, 0.9978, 1.0609, 0.9924, 0.9693, 0.9893,\n",
      "        1.1191, 0.9836, 1.0429, 1.0056, 0.9889, 1.0093, 1.0196, 0.9867, 1.0171,\n",
      "        0.9657, 1.1689, 1.0012, 1.0190, 1.0015, 0.9831, 1.0307, 0.9714, 0.9922,\n",
      "        1.0049, 1.0223, 1.0040, 1.0040, 1.0060, 0.9822, 0.9969, 1.0227, 1.0112,\n",
      "        0.9789, 1.0253, 1.0349, 1.0847, 0.9881, 1.0398, 1.0280, 1.0149, 1.0410,\n",
      "        1.0346, 0.9873, 1.0607, 0.9580, 1.0429, 1.0349, 1.0617, 0.9635, 0.9866,\n",
      "        1.0219, 1.0489, 1.0187, 1.1269, 1.0620, 1.0380, 1.0372, 1.0288, 1.0336,\n",
      "        1.0616, 1.0041, 1.0566, 1.0469, 0.9876, 0.9880, 1.0193, 1.0329, 0.9967,\n",
      "        1.0453, 1.1913, 1.0216, 0.9876, 1.0145, 1.0545, 1.0024, 1.0249, 1.0298,\n",
      "        1.0068, 1.0789, 0.9820, 0.9899, 1.0289, 1.0429, 1.2023, 0.9905, 1.0390,\n",
      "        1.0591, 1.0007, 0.9824, 1.0428, 1.0104, 0.9599, 1.0242, 0.9505, 0.9992,\n",
      "        1.0206, 0.9684, 1.0774, 0.9769, 1.0238, 1.1004, 0.9942, 0.9603, 1.1045,\n",
      "        0.9849, 1.0193, 1.0952, 0.9930, 1.0431, 1.0240, 0.9624, 1.0693, 0.9895,\n",
      "        1.0114, 1.0484, 1.0226, 0.9791, 0.9876, 1.0201, 0.9739, 1.0721, 1.0196,\n",
      "        1.0098, 1.0356, 0.9666, 0.9921], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0015, -0.0524, -0.1087, -0.0047, -0.1209, -0.0561, -0.0839, -0.0720,\n",
      "        -0.1119, -0.0674, -0.0456, -0.0115, -0.0243, -0.0992, -0.0187, -0.0451,\n",
      "        -0.0488,  0.0315, -0.0198, -0.0830, -0.0212, -0.1120, -0.0247, -0.0544,\n",
      "        -0.0468, -0.0350, -0.0711, -0.0538, -0.1204, -0.0588, -0.0331, -0.0520,\n",
      "        -0.0644, -0.0741, -0.0233, -0.1050, -0.0654, -0.0752,  0.0772, -0.0612,\n",
      "        -0.0250, -0.0500, -0.0786, -0.0450, -0.0497, -0.0138, -0.0582, -0.0665,\n",
      "        -0.0271, -0.0832, -0.0536,  0.0038, -0.0143, -0.0726, -0.0615, -0.0704,\n",
      "         0.0175, -0.0466, -0.0477, -0.1165, -0.0188, -0.0150, -0.1132, -0.0871,\n",
      "        -0.1202, -0.0659, -0.0386, -0.0707, -0.0885, -0.0388,  0.0087, -0.1378,\n",
      "        -0.0504, -0.0552, -0.0638, -0.0166, -0.0096, -0.0732, -0.0844, -0.0529,\n",
      "        -0.0239, -0.1237, -0.1085, -0.0432, -0.0876, -0.0095, -0.0354, -0.1984,\n",
      "        -0.1172, -0.0597, -0.0527, -0.0992, -0.0352, -0.0574, -0.0191, -0.1161,\n",
      "        -0.0701, -0.0739, -0.0698,  0.0009, -0.0295, -0.0644, -0.0895, -0.0502,\n",
      "         0.0061, -0.0836, -0.0929, -0.1084, -0.0829, -0.0532, -0.1112, -0.0538,\n",
      "        -0.0082, -0.0554,  0.0091, -0.0294, -0.1500, -0.0802, -0.0870, -0.0661,\n",
      "        -0.0678, -0.0691, -0.0224,  0.0380, -0.0482, -0.0563, -0.0816, -0.0294,\n",
      "        -0.0835, -0.0470, -0.0409, -0.1134, -0.0927, -0.0860, -0.1233, -0.0501,\n",
      "        -0.0302, -0.0166, -0.0769, -0.0368, -0.0608,  0.0426, -0.0799, -0.0752,\n",
      "        -0.0261, -0.1129, -0.0501,  0.0035, -0.0914, -0.0160, -0.1054, -0.0385,\n",
      "        -0.0565, -0.0471, -0.0239, -0.0624, -0.0472, -0.0798, -0.0592, -0.0998,\n",
      "        -0.0523, -0.0846, -0.0248, -0.0297, -0.0890, -0.0509, -0.1077, -0.0903,\n",
      "        -0.0460, -0.0813, -0.0881, -0.0432, -0.1157, -0.0834, -0.0445, -0.0690,\n",
      "        -0.0204,  0.0131, -0.0547, -0.0705, -0.0096, -0.0539, -0.0558, -0.0180,\n",
      "        -0.1341,  0.0108, -0.0680, -0.0888, -0.1066, -0.0085, -0.0267, -0.0655,\n",
      "        -0.0365, -0.0703, -0.1132, -0.0849, -0.0920, -0.0721, -0.1101, -0.0874,\n",
      "        -0.0907, -0.0423, -0.1420, -0.0422, -0.0428, -0.1611, -0.0857, -0.1021,\n",
      "        -0.0698, -0.0512, -0.0296, -0.0077, -0.0958, -0.0325, -0.0222, -0.0715,\n",
      "        -0.0976, -0.0115, -0.0858,  0.0192, -0.0658, -0.0786,  0.0320, -0.0638,\n",
      "        -0.0683, -0.0499, -0.1128, -0.0104, -0.0429, -0.0953, -0.0390, -0.0806,\n",
      "        -0.0361, -0.0686, -0.0194, -0.1110, -0.0499, -0.0641, -0.0572, -0.0633,\n",
      "        -0.1007, -0.0727, -0.0667, -0.0647, -0.0936, -0.0441, -0.0618, -0.0790,\n",
      "        -0.0539, -0.0729, -0.1073, -0.0827, -0.0423, -0.0170, -0.0807, -0.0241],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9688, 0.9951, 1.0936, 1.0318, 0.9738, 1.0632, 0.9956, 1.0220, 1.0341,\n",
      "        1.0346, 1.0321, 1.0286, 1.0351, 1.0270, 1.0313, 1.0026, 1.0242, 1.0163,\n",
      "        1.0164, 1.0213, 0.9846, 1.0281, 0.9745, 1.0211, 1.0251, 1.1097, 1.0048,\n",
      "        0.9945, 1.0085, 1.0322, 0.9939, 0.9812, 1.0295, 1.0480, 0.9878, 1.0023,\n",
      "        0.9938, 0.9953, 0.9554, 0.9934, 1.0812, 1.0617, 1.0021, 1.0109, 1.0889,\n",
      "        1.0109, 1.0406, 1.0489, 1.0591, 1.0420, 0.9947, 1.0061, 1.0039, 1.0183,\n",
      "        1.0147, 1.0013, 1.0114, 1.0167, 1.0308, 1.0309, 1.0151, 0.9934, 1.0022,\n",
      "        1.0710, 0.9635, 1.0136, 0.9966, 1.0013, 1.0732, 1.0115, 1.0207, 1.0215,\n",
      "        0.9811, 1.0356, 1.0385, 0.9983, 1.0318, 1.0312, 1.0212, 1.0101, 1.0092,\n",
      "        1.0137, 0.9714, 0.9841, 1.0451, 1.0008, 1.0255, 1.0235, 1.0101, 1.0277,\n",
      "        0.9830, 1.0691, 1.0696, 1.0428, 1.0047, 0.9968, 1.0327, 0.9899, 1.0215,\n",
      "        1.0097, 1.0241, 1.0268, 0.9976, 1.0845, 1.0062, 1.0356, 1.0510, 0.9867,\n",
      "        1.0321, 1.0001, 1.0245, 1.0407, 0.9990, 0.9855, 1.0097, 0.9917, 0.9978,\n",
      "        1.0205, 0.9711, 0.9617, 1.0138, 0.9879, 1.0308, 1.0286, 0.9993, 0.9979,\n",
      "        1.0066, 1.0730, 1.0203, 0.9972, 1.0132, 0.9927, 1.0419, 1.0505, 1.0898,\n",
      "        0.9718, 1.0196, 1.0590, 1.0451, 1.0016, 1.0149, 1.0073, 1.0514, 1.0198,\n",
      "        1.0016, 1.0160, 0.9796, 0.9967, 1.0930, 1.0214, 0.9690, 1.0172, 1.1128,\n",
      "        0.9665, 1.0151, 1.0681, 1.0219, 1.0486, 0.9979, 1.0999, 0.9818, 0.9943,\n",
      "        1.0171, 0.9836, 0.9961, 0.9930, 0.9600, 0.9983, 0.9719, 1.0072, 1.0335,\n",
      "        1.0215, 1.0148, 0.9694, 1.0081, 1.0525, 1.0067, 1.0439, 1.0099, 1.0289,\n",
      "        0.9737, 1.0207, 0.9788, 1.0217, 1.0419, 1.0805, 1.0239, 1.0032, 0.9968,\n",
      "        1.0295, 0.9798, 0.9659, 0.9989, 1.0297, 0.9965, 1.0086, 1.0069, 1.0184,\n",
      "        1.0366, 1.0222, 0.9907, 1.0722, 1.0271, 1.0342, 1.0074, 1.0738, 1.0055,\n",
      "        0.9802, 1.0120, 1.0394, 1.0086, 1.0004, 1.0352, 0.9709, 0.9941, 1.0345,\n",
      "        0.9910, 1.0146, 1.0644, 0.9807, 1.0315, 1.0947, 1.0045, 0.9866, 1.0358,\n",
      "        0.9886, 0.9922, 1.0474, 1.0493, 0.9903, 1.0371, 0.9814, 0.9967, 0.9821,\n",
      "        0.9892, 1.0363, 1.0200, 1.0082, 1.0622, 1.0926, 1.0090, 1.0134, 1.0263,\n",
      "        1.0533, 0.9749, 0.9827, 1.0315, 0.9945, 1.0114, 1.0334, 1.0384, 1.0251,\n",
      "        1.0268, 1.0261, 1.0554, 1.0383], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1098, -0.0325, -0.0452, -0.0556, -0.0822, -0.0595, -0.0567, -0.1230,\n",
      "        -0.0913, -0.0547, -0.0942, -0.0717, -0.0656, -0.0171, -0.0806, -0.0855,\n",
      "        -0.1416, -0.0730, -0.1045, -0.0160, -0.1300, -0.0844, -0.0846, -0.0623,\n",
      "        -0.0683, -0.0538, -0.0929, -0.0410, -0.0378, -0.0715, -0.0396, -0.0647,\n",
      "        -0.0773,  0.0105, -0.0707, -0.0650, -0.0540, -0.1423, -0.0662, -0.0951,\n",
      "        -0.0369, -0.0127, -0.0228, -0.0158, -0.0474, -0.0593, -0.0369, -0.0640,\n",
      "        -0.0371, -0.0049, -0.0720, -0.1093, -0.0950, -0.0868, -0.0710, -0.0446,\n",
      "        -0.0251,  0.0013, -0.0989, -0.0264, -0.1092, -0.1019, -0.0303, -0.0929,\n",
      "        -0.0956,  0.0015, -0.0241, -0.1183, -0.1317, -0.0354, -0.0896, -0.0702,\n",
      "        -0.0571, -0.1125, -0.0438, -0.0546, -0.0617, -0.1185, -0.0885, -0.0389,\n",
      "        -0.0742, -0.0633, -0.0738, -0.0555, -0.0423, -0.1167, -0.0637, -0.0303,\n",
      "        -0.0043, -0.0443, -0.0623, -0.0301, -0.0257, -0.0649, -0.1342, -0.0107,\n",
      "        -0.0319, -0.0524, -0.0647, -0.0296, -0.0497, -0.0003, -0.0590, -0.0219,\n",
      "        -0.0226, -0.1334, -0.0785, -0.0314, -0.1100, -0.0898, -0.0668, -0.0406,\n",
      "        -0.0999, -0.0523, -0.1098, -0.0605, -0.0299, -0.0594, -0.0907, -0.1004,\n",
      "        -0.0090, -0.0271, -0.0552, -0.0575, -0.1162, -0.0532, -0.0743,  0.0150,\n",
      "        -0.0304, -0.0471, -0.0807, -0.1128, -0.0223, -0.0130, -0.1525, -0.1055,\n",
      "        -0.0762, -0.0376, -0.0711, -0.1028, -0.0968, -0.0293, -0.0533, -0.0331,\n",
      "        -0.0954, -0.0247, -0.0932, -0.0950,  0.0404, -0.0617, -0.0691, -0.0633,\n",
      "        -0.1032, -0.0525, -0.0820, -0.0715, -0.0094, -0.0361, -0.0372, -0.0633,\n",
      "        -0.0531, -0.0208, -0.0957, -0.0480, -0.0902, -0.1154, -0.1049, -0.0359,\n",
      "        -0.0323, -0.0875,  0.0149, -0.0067, -0.0358, -0.0665, -0.0985, -0.0505,\n",
      "        -0.0802, -0.0766, -0.0822, -0.0398, -0.0579, -0.0252,  0.0056, -0.0943,\n",
      "        -0.1232, -0.0107, -0.0690, -0.0275, -0.0203, -0.0556, -0.1786, -0.0303,\n",
      "        -0.0536, -0.0760, -0.0423, -0.1357, -0.0584, -0.0945, -0.0150, -0.0504,\n",
      "        -0.1332, -0.0200, -0.0938, -0.0289, -0.0140, -0.0158, -0.0332, -0.0851,\n",
      "        -0.1322, -0.0282, -0.1158, -0.0276, -0.0046, -0.0797, -0.0207, -0.0614,\n",
      "        -0.0837, -0.0622, -0.0753, -0.0828,  0.0014, -0.0108, -0.0554, -0.0755,\n",
      "        -0.1280, -0.0718, -0.0522, -0.0179, -0.1432, -0.0174, -0.0591, -0.1310,\n",
      "        -0.0857, -0.1031, -0.1026, -0.0532, -0.0501, -0.1069, -0.0927, -0.0144,\n",
      "        -0.1179, -0.0661, -0.0889, -0.0981, -0.0926, -0.0691, -0.0648, -0.1699,\n",
      "        -0.1201, -0.0147, -0.0324, -0.0403, -0.0203, -0.0427, -0.0627, -0.0774],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9772, 0.9751, 0.9877, 0.9557, 0.9801, 0.9872, 0.9806, 0.9817, 1.0310,\n",
      "        0.9723, 0.9977, 0.9714, 0.9741, 0.9684, 0.9732, 0.9857, 0.9789, 0.9961,\n",
      "        0.9126, 0.9847, 0.9830, 0.8996, 0.9649, 0.9751, 0.9722, 0.9989, 0.9654,\n",
      "        0.9707, 0.9921, 0.9793, 0.9814, 0.9877, 0.9868, 0.9803, 0.9926, 0.9723,\n",
      "        1.0059, 0.9614, 0.9643, 1.0080, 0.9818, 0.9759, 0.9829, 0.9809, 0.9714,\n",
      "        0.9798, 0.9458, 0.9763, 1.0060, 0.9873, 0.9542, 0.9858, 0.9790, 0.9884,\n",
      "        0.9564, 0.9620, 1.0013, 0.9710, 0.9865, 0.9752, 0.9898, 0.9764, 0.9505,\n",
      "        0.9855, 0.9848, 0.9815, 0.9762, 0.9978, 0.9994, 0.9770, 0.9646, 0.9690,\n",
      "        0.9713, 1.0038, 0.9838, 0.9824, 0.9896, 0.9888, 0.9745, 0.9688, 0.9842,\n",
      "        0.9822, 0.9437, 0.9947, 0.9522, 0.9675, 0.9817, 0.9576, 0.9728, 0.9772,\n",
      "        0.9916, 0.9872, 0.9898, 0.9602, 0.9693, 0.9956, 0.9805, 1.0845, 0.9930,\n",
      "        0.9716, 0.9738, 0.9670, 0.6690, 0.9486, 0.9726, 1.0272, 0.9718, 0.9760,\n",
      "        0.9703, 0.9760, 0.9796, 0.9838, 0.9996, 0.9769, 0.9639, 0.9915, 0.9758,\n",
      "        0.9612, 0.9768, 0.9723, 0.9431, 0.9814, 1.0013, 0.9853, 0.9654, 0.9582,\n",
      "        0.9910, 0.9759, 0.9822, 0.9812, 0.9937, 0.9611, 0.9688, 0.9923, 0.9811,\n",
      "        0.9649, 0.9727, 0.9864, 0.9936, 0.9766, 0.9440, 0.9903, 0.9782, 0.9719,\n",
      "        0.9636, 0.9875, 0.9936, 0.9818, 0.9718, 0.9796, 0.9407, 0.9797, 0.9917,\n",
      "        1.0019, 0.9805, 0.9889, 0.9969, 0.9727, 0.9718, 0.9635, 0.9792, 0.9718,\n",
      "        0.9657, 0.9864, 0.9749, 0.9802, 0.9603, 0.9104, 0.9904, 0.9967, 0.9892,\n",
      "        0.9875, 0.9661, 0.9962, 0.9803, 0.9788, 0.9813, 0.9712, 0.9836, 0.9257,\n",
      "        0.9584, 0.9951, 0.9802, 1.0008, 0.9630, 0.9766, 0.9901, 0.9640, 0.9815,\n",
      "        0.9979, 0.9881, 0.9489, 0.9775, 0.9832, 0.9787, 1.0181, 1.0005, 0.9764,\n",
      "        0.9831, 0.9741, 0.9914, 0.9697, 0.9460, 0.9851, 0.9840, 0.9757, 0.9770,\n",
      "        0.9822, 0.9751, 0.9821, 0.9654, 0.9637, 0.9779, 0.9793, 0.9796, 0.9827,\n",
      "        0.9746, 0.9871, 0.9599, 1.0058, 0.9786, 0.9773, 0.9567, 0.9914, 0.9745,\n",
      "        0.9941, 0.9757, 0.9825, 0.8904, 0.9747, 0.9730, 0.9969, 0.9524, 0.9897,\n",
      "        0.9893, 0.9636, 0.9712, 0.9737, 1.0043, 0.9899, 0.9737, 0.9625, 1.0070,\n",
      "        0.9695, 0.9858, 0.9826, 0.9654, 1.0132, 0.9830, 0.9847, 0.9607, 0.9749,\n",
      "        1.0091, 0.9793, 0.9856, 0.9554], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0255, -0.0813, -0.0412, -0.0206, -0.0745, -0.0203, -0.1072, -0.0867,\n",
      "        -0.1023, -0.0452, -0.0970, -0.0813, -0.0976, -0.0566, -0.0317, -0.1032,\n",
      "        -0.0493, -0.0604, -0.0985, -0.0924, -0.0367,  0.0249, -0.0075,  0.0032,\n",
      "        -0.0036, -0.0734, -0.0367, -0.0547, -0.1008, -0.0401,  0.0004, -0.0561,\n",
      "        -0.0453, -0.0331, -0.0873, -0.0830, -0.0815, -0.0074, -0.1072, -0.1170,\n",
      "        -0.0629, -0.0377, -0.1494, -0.0405, -0.0610, -0.0376, -0.0047, -0.0185,\n",
      "        -0.0550, -0.0513, -0.0116, -0.0398, -0.0437, -0.1322, -0.0145, -0.0393,\n",
      "        -0.1034, -0.0586, -0.0617, -0.0844, -0.0203, -0.0418, -0.0514, -0.0902,\n",
      "        -0.0263,  0.0002, -0.0460, -0.0511, -0.0990, -0.0206, -0.1111, -0.0570,\n",
      "        -0.0635, -0.1148, -0.0252, -0.0362, -0.0523, -0.0304, -0.0338, -0.0188,\n",
      "        -0.0526,  0.0366, -0.0537, -0.0454,  0.0164, -0.0201, -0.0446, -0.0359,\n",
      "        -0.0880, -0.0540, -0.0045, -0.0275, -0.0750, -0.0631, -0.0485, -0.0591,\n",
      "        -0.0643,  0.0042, -0.0415, -0.0502,  0.0081, -0.0719, -0.2270,  0.0129,\n",
      "        -0.0463, -0.0227, -0.0685,  0.0066, -0.0332, -0.0279, -0.0165, -0.0707,\n",
      "        -0.0910, -0.0501, -0.0418, -0.0845, -0.0570, -0.0327, -0.0491, -0.0257,\n",
      "        -0.0325, -0.0459, -0.0229, -0.0417, -0.0341,  0.0042, -0.0531, -0.0484,\n",
      "        -0.0163, -0.0922, -0.1250, -0.0436, -0.0109, -0.0387, -0.0028, -0.0558,\n",
      "        -0.0234, -0.0816, -0.1297, -0.0315, -0.0003, -0.0565, -0.0853, -0.0618,\n",
      "        -0.0747, -0.0374, -0.0520, -0.0502, -0.0777,  0.0221,  0.0809, -0.1111,\n",
      "        -0.0813, -0.0071, -0.0616, -0.0557, -0.1042, -0.0204, -0.0318, -0.0770,\n",
      "        -0.0253, -0.0099, -0.0615, -0.0021, -0.0120, -0.0557, -0.0961, -0.0947,\n",
      "        -0.0542, -0.0618, -0.0545, -0.0510, -0.0348, -0.0393, -0.0290, -0.0464,\n",
      "        -0.0575, -0.0196, -0.0226,  0.1039, -0.0877, -0.0963, -0.0421, -0.1383,\n",
      "        -0.0161, -0.0082, -0.0275, -0.0130, -0.0520, -0.0515, -0.0486,  0.0148,\n",
      "        -0.0565, -0.0610,  0.0030, -0.0762,  0.0441, -0.0236, -0.0264, -0.0177,\n",
      "        -0.0773, -0.0401, -0.0320, -0.0153, -0.0270, -0.0249, -0.0515, -0.0597,\n",
      "        -0.0243, -0.0191, -0.0344, -0.1056, -0.1051, -0.0142, -0.0578, -0.0380,\n",
      "        -0.0661, -0.0306, -0.1015, -0.0855,  0.0025, -0.0294, -0.0420, -0.0278,\n",
      "        -0.0394, -0.0628, -0.0443, -0.0831, -0.0507, -0.0013, -0.0049, -0.1175,\n",
      "        -0.0550, -0.1166, -0.0414, -0.0196, -0.0222, -0.0529, -0.1148, -0.0470,\n",
      "        -0.0397, -0.0005, -0.0610, -0.0269, -0.0527, -0.0659, -0.0767, -0.0910,\n",
      "        -0.0455, -0.0366, -0.0270, -0.0429, -0.0059, -0.0591, -0.0152, -0.0064],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0164,  0.0001,  0.0425,  ...,  0.0260, -0.0037, -0.0096],\n",
      "        [-0.0305,  0.0106,  0.0290,  ...,  0.0092,  0.0364,  0.0342],\n",
      "        [-0.0285,  0.0237,  0.0467,  ..., -0.0060,  0.0442,  0.0119],\n",
      "        [-0.0376, -0.0313, -0.0463,  ...,  0.0028,  0.0083, -0.0449]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "             name  vn_kv type zone  in_service  min_vm_pu  \\\n0      EHV Bus 57  380.0   db  NaN        True        0.9   \n2     EHV Bus 143  380.0   db  NaN        True        0.9   \n4    EHV Bus 1649  220.0   db  NaN        True        0.9   \n12      HV1 Bus 1  110.0   db  NaN        True        0.9   \n14      HV1 Bus 3  110.0   db  NaN        True        0.9   \n..            ...    ...  ...  ...         ...        ...   \n124   HV1 Bus 113  110.0   db  NaN        True        0.9   \n126   HV1 Bus 115  110.0   db  NaN        True        0.9   \n128   HV1 Bus 117  110.0   db  NaN        True        0.9   \n130   HV1 Bus 119  110.0   db  NaN        True        0.9   \n132   HV1 Bus 121  110.0   db  NaN        True        0.9   \n\n              substation voltLvl    subnet  max_vm_pu  \n0    EHV_HV_substation_1       1  EHV1_HV1        1.1  \n2    EHV_HV_substation_2       1  EHV1_HV1        1.1  \n4    EHV_HV_substation_3       1  EHV1_HV1        1.1  \n12                   NaN       3       HV1        1.1  \n14                   NaN       3       HV1        1.1  \n..                   ...     ...       ...        ...  \n124                  NaN       3       HV1        1.1  \n126                  NaN       3       HV1        1.1  \n128                  NaN       3       HV1        1.1  \n130                  NaN       3       HV1        1.1  \n132                  NaN       3       HV1        1.1  \n\n[64 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>vn_kv</th>\n      <th>type</th>\n      <th>zone</th>\n      <th>in_service</th>\n      <th>min_vm_pu</th>\n      <th>substation</th>\n      <th>voltLvl</th>\n      <th>subnet</th>\n      <th>max_vm_pu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EHV Bus 57</td>\n      <td>380.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>EHV_HV_substation_1</td>\n      <td>1</td>\n      <td>EHV1_HV1</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EHV Bus 143</td>\n      <td>380.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>EHV_HV_substation_2</td>\n      <td>1</td>\n      <td>EHV1_HV1</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EHV Bus 1649</td>\n      <td>220.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>EHV_HV_substation_3</td>\n      <td>1</td>\n      <td>EHV1_HV1</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>HV1 Bus 1</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>HV1</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>HV1 Bus 3</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>HV1</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>HV1 Bus 113</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>HV1</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>HV1 Bus 115</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>HV1</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>HV1 Bus 117</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>HV1</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>HV1 Bus 119</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>HV1</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>HV1 Bus 121</td>\n      <td>110.0</td>\n      <td>db</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>0.9</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>HV1</td>\n      <td>1.1</td>\n    </tr>\n  </tbody>\n</table>\n<p>64 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "to_json('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYPOWER Version 5.1.4, 27-June-2018 -- AC Optimal Power Flow\n",
      "Python Interior Point Solver - PIPS, Version 1.0, 07-Feb-2011\n",
      "Converged!\n",
      "\n",
      "Converged in 0.74 seconds\n",
      "Objective Function Value = -1533.79 $/hr\n",
      "================================================================================\n",
      "| PyPower (ppci) System Summary - these are not valid for pandapower DataFrames|\n",
      "================================================================================\n",
      "\n",
      "How many?                How much?              P (MW)            Q (MVAr)\n",
      "---------------------    -------------------  -------------  -----------------\n",
      "Buses             64     Total Gen Capacity   3000000000.0       -3000000000.0 to 3000000000.0\n",
      "Generators         3     On-line Capacity     3000000000.0       -3000000000.0 to 3000000000.0\n",
      "Committed Gens     3     Generation (actual)   -845.3             438.9\n",
      "Loads             56     Load                  -888.9             206.2\n",
      "  Fixed           56       Fixed               -888.9             206.2\n",
      "  Dispatchable     0       Dispatchable           0.0 of 0.0        0.0\n",
      "Shunts             0     Shunt (inj)              0.0               0.0\n",
      "Branches         101     Losses (I^2 * Z)        42.90            274.13\n",
      "Transformers     101     Branch Charging (inj)     -               41.4\n",
      "Inter-ties         0     Total Inter-tie Flow     0.0               0.0\n",
      "Areas              1\n",
      "\n",
      "                          Minimum                      Maximum\n",
      "                 -------------------------  --------------------------------\n",
      "Voltage Magnitude   1.011 p.u. @ bus 15         1.100 p.u. @ bus 2   \n",
      "Voltage Angle       0.00 deg   @ bus 1         25.13 deg   @ bus 19  \n",
      "P Losses (I^2*R)             -                  2.31 MW    @ line 17-38\n",
      "Q Losses (I^2*X)             -                 76.98 MVAr  @ line 1-5\n",
      "Lambda P            0.98 $/MWh @ bus 19         2.00 $/MWh @ bus 1   \n",
      "Lambda Q            0.00 $/MWh @ bus 0          0.26 $/MWh @ bus 20  \n",
      "\n",
      "================================================================================\n",
      "|     Area Summary                                                             |\n",
      "================================================================================\n",
      "Area  # of      # of Gens        # of Loads         # of    # of   # of   # of\n",
      " Num  Buses   Total  Online   Total  Fixed  Disp    Shunt   Brchs  Xfmrs   Ties\n",
      "----  -----   -----  ------   -----  -----  -----   -----   -----  -----  -----\n",
      "  1      64       3      3      56     56      0       0     101    101      0\n",
      "----  -----   -----  ------   -----  -----  -----   -----   -----  -----  -----\n",
      "Tot:     64       3      3      56     56      0       0     101    101      0\n",
      "\n",
      "Area      Total Gen Capacity           On-line Gen Capacity         Generation\n",
      " Num     MW           MVAr            MW           MVAr             MW    MVAr\n",
      "----   ------  ------------------   ------  ------------------    ------  ------\n",
      "  1   3000000000.0  -3000000000.0 to 3000000000.0  3000000000.0  -3000000000.0 to 3000000000.0    -845.3   438.9\n",
      "----   ------  ------------------   ------  ------------------    ------  ------\n",
      "\n",
      "Area    Disp Load Cap       Disp Load         Fixed Load        Total Load\n",
      " Num      MW     MVAr       MW     MVAr       MW     MVAr       MW     MVAr\n",
      "----    ------  ------    ------  ------    ------  ------    ------  ------\n",
      "  1        0.0     0.0       0.0     0.0    -888.9   206.2    -888.9   206.2\n",
      "----    ------  ------    ------  ------    ------  ------    ------  ------\n",
      "Tot:       0.0     0.0       0.0     0.0    -888.9   206.2    -888.9   206.2\n",
      "\n",
      "Area      Shunt Inj        Branch      Series Losses      Net Export\n",
      " Num      MW     MVAr     Charging      MW     MVAr       MW     MVAr\n",
      "----    ------  ------    --------    ------  ------    ------  ------\n",
      "  1        0.0     0.0       41.4      42.90  274.13       0.0     0.0\n",
      "----    ------  ------    --------    ------  ------    ------  ------\n",
      "Tot:       0.0     0.0       41.4      42.90  274.13       -       -\n",
      "\n",
      "================================================================================\n",
      "|     Generator Data                                                           |\n",
      "================================================================================\n",
      " Gen   Bus   Status     Pg        Qg      Lambda ($/MVA-hr)\n",
      "  #     #              (MW)     (MVAr)      P         Q    \n",
      "----  -----  ------  --------  --------  --------  --------\n",
      "  0      1      1    -688.52    331.76      2.00      0.00\n",
      "  1      2      1      20.00     74.58      1.06      0.00\n",
      "  2      0      1    -176.75     32.56      1.00      0.00\n",
      "                     --------  --------\n",
      "            Total:   -845.27    438.89\n",
      "\n",
      "================================================================================\n",
      "|     Bus Data                                                                 |\n",
      "================================================================================\n",
      " Bus      Voltage          Generation             Load          Lambda($/MVA-hr)\n",
      "  #   Mag(pu) Ang(deg)   P (MW)   Q (MVAr)   P (MW)   Q (MVAr)     P        Q   \n",
      "----- ------- --------  --------  --------  --------  --------  -------  -------\n",
      "    0  1.100   21.256   -176.75     32.56       -         -       1.000     -\n",
      "    1  1.092    0.000*  -688.52    331.76       -         -       2.000     -\n",
      "    2  1.100   20.567     20.00     74.58       -         -       1.061     -\n",
      "    3  1.067   20.747       -         -         -         -       1.072   0.110\n",
      "    4  1.085   20.691       -         -        3.00      1.19     1.062   0.076\n",
      "    5  1.019   11.284       -         -        3.00      1.19     1.321   0.230\n",
      "    6  1.086   20.383       -         -       -2.68      1.19     1.062   0.062\n",
      "    7  1.086   20.575       -         -      -17.78      1.19     1.061   0.069\n",
      "    8  1.084   20.852       -         -       -6.54      1.19     1.062   0.084\n",
      "    9  1.016   11.306       -         -        8.08     13.63     1.322   0.234\n",
      "   10  1.093   23.913       -         -     -149.06      0.00     0.997   0.045\n",
      "   11  1.030   13.579       -         -      -14.35      6.82     1.255   0.205\n",
      "   12  1.032   16.139       -         -     -127.21     12.50     1.210   0.258\n",
      "   13  1.023   13.867       -         -      -33.28      1.19     1.263   0.249\n",
      "   14  1.038   15.822       -         -         -         -       1.196   0.182\n",
      "   15  1.011   11.130       -         -       36.15     19.65     1.329   0.238\n",
      "   16  1.063   20.664       -         -      -16.20      0.00     1.092   0.174\n",
      "   17  1.054   16.847       -         -      -21.37      6.82     1.160   0.147\n",
      "   18  1.074   19.136       -         -      -22.16      1.19     1.097   0.096\n",
      "   19  1.100   25.134       -         -     -123.15      1.19     0.980   0.046\n",
      "   20  1.031   16.069       -         -        3.00      1.19     1.212   0.259\n",
      "   21  1.081   22.470       -         -         -         -       1.030   0.074\n",
      "   22  1.079   22.316       -         -        3.00      1.19     1.034   0.077\n",
      "   23  1.067   21.266       -         -      -14.65      6.82     1.064   0.113\n",
      "   24  1.068   21.031       -         -       -6.56      0.00     1.068   0.111\n",
      "   25  1.048   17.534       -         -         -         -       1.152   0.159\n",
      "   26  1.021   13.579       -         -       36.15     19.65     1.270   0.249\n",
      "   27  1.029   15.477       -         -      -45.82      6.82     1.226   0.256\n",
      "   28  1.093   24.050       -         -       -4.87      1.19     0.996   0.045\n",
      "   29  1.026   14.113       -         -      -14.16      1.19     1.252   0.227\n",
      "   30  1.052   19.217       -         -        0.47     19.65     1.125   0.185\n",
      "   31  1.037   16.568       -         -      -18.28      0.00     1.191   0.214\n",
      "   32  1.038   15.768       -         -        3.00      1.19     1.198   0.182\n",
      "   33  1.019   11.384       -         -        3.00      1.19     1.318   0.229\n",
      "   34  1.081   22.471       -         -        3.00      1.19     1.030   0.074\n",
      "   35  1.092   21.591       -         -      -51.30      6.82     1.044   0.064\n",
      "   36  1.029   15.477       -         -         -         -       1.226   0.256\n",
      "   37  1.087   20.506       -         -      -32.29      6.82     1.060   0.062\n",
      "   38  1.031   13.486       -         -       -8.18      0.00     1.257   0.204\n",
      "   39  1.085   20.707       -         -       -6.99      1.19     1.062   0.076\n",
      "   40  1.012   11.144       -         -      -14.00      6.82     1.327   0.237\n",
      "   41  1.017   11.319       -         -       -2.06      1.19     1.321   0.232\n",
      "   42  1.087   20.554       -         -       -2.75      1.19     1.060   0.062\n",
      "   43  1.084   20.826       -         -       -5.30      1.19     1.062   0.084\n",
      "   44  1.095   24.429       -         -      -34.37      1.19     0.990   0.045\n",
      "   45  1.069   21.535       -         -      -52.65      0.00     1.075   0.174\n",
      "   46  1.058   19.809       -         -       -3.33      1.19     1.109   0.173\n",
      "   47  1.025   14.108       -         -        7.84     12.50     1.252   0.227\n",
      "   48  1.061   20.340       -         -       -3.45      1.19     1.098   0.174\n",
      "   49  1.091   24.386       -         -        7.84     12.50     0.992   0.048\n",
      "   50  1.080   22.330       -         -        3.00      1.19     1.034   0.077\n",
      "   51  1.018   11.303       -         -        3.00      1.19     1.321   0.232\n",
      "   52  1.038   16.766       -         -      -12.13      1.19     1.186   0.212\n",
      "   53  1.045   18.146       -         -      -21.11      6.82     1.152   0.200\n",
      "   54  1.086   20.388       -         -       -5.96      1.19     1.062   0.062\n",
      "   55  1.083   20.890       -         -      -51.01      1.19     1.063   0.090\n",
      "   56  1.017   11.332       -         -       -8.17      1.19     1.320   0.232\n",
      "   57  1.083   20.853       -         -       -5.41      1.19     1.062   0.086\n",
      "   58  1.056   19.673       -         -        3.00      1.19     1.113   0.176\n",
      "   59  1.061   20.369       -         -       -3.37      1.19     1.098   0.174\n",
      "   60  1.045   18.159       -         -      -13.99      1.19     1.152   0.200\n",
      "   61  1.032   15.488       -         -       -9.01      1.19     1.218   0.220\n",
      "   62  1.095   24.337       -         -      -20.01      1.19     0.991   0.045\n",
      "   63  1.089   20.954       -         -      -10.47      1.19     1.054   0.063\n",
      "                        --------  --------  --------  --------\n",
      "               Total:   -845.27    438.89   -888.90    206.17\n",
      "\n",
      "================================================================================\n",
      "|     Branch Data                                                              |\n",
      "================================================================================\n",
      "Brnch   From   To    From Bus Injection   To Bus Injection     Loss (I^2 * Z)  \n",
      "  #     Bus    Bus    P (MW)   Q (MVAr)   P (MW)   Q (MVAr)   P (MW)   Q (MVAr)\n",
      "-----  -----  -----  --------  --------  --------  --------  --------  --------\n",
      "   0     17     38    107.39      3.66   -105.07      1.68     2.314      6.25\n",
      "   1     13     26     33.28     -1.19    -33.22      1.12     0.061      0.16\n",
      "   2     12     27     62.10     -6.49    -61.85      6.91     0.257      0.70\n",
      "   3     36     27      0.00      0.00      0.00     -0.19     0.000      0.00\n",
      "   4     61     29     91.95     -9.62    -91.16     11.36     0.790      2.14\n",
      "   5     47     29     -7.84    -12.50      7.84     12.47     0.002      0.01\n",
      "   6     30     53     56.41     -1.28    -56.03      1.77     0.385      1.04\n",
      "   7     31     52    -78.91      7.75     79.00     -7.55     0.099      0.27\n",
      "   8     61     31    -87.45      9.03     88.05     -7.75     0.596      1.61\n",
      "   9     14     25    -74.92      5.76     75.74     -4.19     0.816      2.21\n",
      "  10     32     33     73.40     -6.40    -71.40     10.20     1.993      5.39\n",
      "  11      5     51      0.87     16.94     -0.86    -17.01     0.007      0.02\n",
      "  12      6     54     -5.96      1.16      5.96     -1.19     0.000      0.00\n",
      "  13     10     34     36.34      1.72    -36.00     -2.17     0.345      0.93\n",
      "  14     63     35    -51.10      6.97     51.30     -6.82     0.204      0.55\n",
      "  15      6     37    -35.01      7.44     35.04     -7.48     0.027      0.07\n",
      "  16     11     38     14.35     -6.82    -14.34      6.69     0.009      0.02\n",
      "  17      4     39     -3.00     -1.19      3.00      0.99     0.000      0.00\n",
      "  18     39     43    -14.64     13.65     14.66    -13.81     0.016      0.04\n",
      "  19     55     57      6.04    -12.26     -6.03     12.15     0.004      0.01\n",
      "  20      5     40     22.25     25.38    -22.17    -25.45     0.080      0.22\n",
      "  21     41     51      2.14    -15.87     -2.14     15.83     0.003      0.01\n",
      "  22     41     56     -8.17      1.15      8.17     -1.19     0.001      0.00\n",
      "  23      9     41     -8.08    -13.63      8.09     13.54     0.007      0.02\n",
      "  24      7     39    -16.62     14.58     16.63    -14.73     0.019      0.05\n",
      "  25     37     42     -2.75      0.66      2.75     -1.19     0.001      0.00\n",
      "  26      8     43      6.54     -1.19     -6.54      1.06     0.001      0.00\n",
      "  27     43     57     -8.74     12.68      8.74    -12.74     0.003      0.01\n",
      "  28     19     44     61.57     -0.59    -61.30      0.94     0.279      0.75\n",
      "  29      5     38   -114.72      8.89    116.34     -5.03     1.613      4.36\n",
      "  30     16     45    -52.35      0.28     52.65      0.00     0.295      0.80\n",
      "  31     46     48    -74.98      3.13     75.23     -2.65     0.254      0.69\n",
      "  32     58     46    -58.33     -9.37     58.38      9.44     0.056      0.15\n",
      "  33     53     60    -13.99      1.16     13.99     -1.19     0.001      0.00\n",
      "  34     52     53    -72.94      6.96     73.58     -5.76     0.637      1.72\n",
      "  35     16     48     68.55     -0.28    -68.41      0.51     0.143      0.39\n",
      "  36     17     18    -96.70     -7.07     98.21     10.37     1.502      4.06\n",
      "  37     10     44    -74.31      8.01     74.55     -7.59     0.241      0.65\n",
      "  38     44     49      7.86     12.10     -7.84    -12.50     0.018      0.05\n",
      "  39     12     20      3.00      0.48     -3.00     -1.19     0.002      0.00\n",
      "  40     22     50     -3.00     -1.19      3.00      1.01     0.000      0.00\n",
      "  41      3     50    -66.73     -1.87     67.43      2.98     0.696      1.88\n",
      "  42     34     50     35.25      1.99    -35.21     -2.04     0.033      0.09\n",
      "  43     21     34      0.00     -0.00      0.00     -0.48     0.000      0.00\n",
      "  44     15     40    -36.15    -19.65     36.17     18.62     0.024      0.02\n",
      "  45     14     32     74.92     -5.76    -74.90      5.80     0.025      0.07\n",
      "  46      5     33    -69.86     10.87     69.90    -10.79     0.044      0.12\n",
      "  47     10     28     -4.87      0.27      4.87     -1.19     0.004      0.01\n",
      "  48      6      7    -25.47     15.06     25.51    -15.18     0.035      0.09\n",
      "  49      5     29    -92.67     21.92     94.32    -18.19     1.655      4.47\n",
      "  50     23     24     14.65     -6.82    -14.63      6.45     0.023      0.06\n",
      "  51     63      6     61.57     -8.15    -61.35      8.44     0.220      0.59\n",
      "  52     62     10     20.01     -1.19    -19.96      0.60     0.054      0.15\n",
      "  53      5     26    -81.10     25.27     82.29    -22.73     1.187      3.21\n",
      "  54      6     18     87.88     12.51    -87.13    -10.96     0.755      2.04\n",
      "  55      3     25     77.31     -1.21    -75.74      4.19     1.569      4.24\n",
      "  56     58     30     56.83      8.77    -56.65     -8.54     0.181      0.49\n",
      "  57     46     55    -38.46    -23.20     38.93     23.33     0.473      1.28\n",
      "  58     48     59     -3.37      0.95      3.37     -1.19     0.001      0.00\n",
      "  59      3     24    -21.15      6.17     21.19     -6.45     0.038      0.10\n",
      "  60     26     27    -83.76     12.35     84.76    -10.22     1.002      2.71\n",
      "  61      5     33    -69.86     10.87     69.90    -10.79     0.044      0.12\n",
      "  62     32     33     73.40     -6.40    -71.40     10.20     1.993      5.39\n",
      "  63     14     32     74.92     -5.76    -74.90      5.80     0.025      0.07\n",
      "  64     34     50     35.25      1.99    -35.21     -2.04     0.033      0.09\n",
      "  65     10     34     36.34      1.72    -36.00     -2.17     0.345      0.93\n",
      "  66      3     25     77.31     -1.21    -75.74      4.19     1.569      4.24\n",
      "  67     14     25    -74.92      5.76     75.74     -4.19     0.816      2.21\n",
      "  68      3     50    -66.73     -1.87     67.43      2.98     0.696      1.88\n",
      "  69      5     38   -114.72      8.89    116.34     -5.03     1.613      4.36\n",
      "  70      5     29    -92.67     21.92     94.32    -18.19     1.655      4.47\n",
      "  71     61     29     91.95     -9.62    -91.16     11.36     0.790      2.14\n",
      "  72     17     38    107.39      3.66   -105.07      1.68     2.314      6.25\n",
      "  73     61     31    -87.45      9.03     88.05     -7.75     0.596      1.61\n",
      "  74      5     26    -81.10     25.27     82.29    -22.73     1.187      3.21\n",
      "  75     17     18    -96.70     -7.07     98.21     10.37     1.502      4.06\n",
      "  76     26     27    -83.76     12.35     84.76    -10.22     1.002      2.71\n",
      "  77     31     52    -78.91      7.75     79.00     -7.55     0.099      0.27\n",
      "  78     52     53    -72.94      6.96     73.58     -5.76     0.637      1.72\n",
      "  79      6     18     87.88     12.51    -87.13    -10.96     0.755      2.04\n",
      "  80     10     44    -74.31      8.01     74.55     -7.59     0.241      0.65\n",
      "  81     30     53     56.41     -1.28    -56.03      1.77     0.385      1.04\n",
      "  82     58     46    -58.33     -9.37     58.38      9.44     0.056      0.15\n",
      "  83     12     27     62.10     -6.49    -61.85      6.91     0.257      0.70\n",
      "  84     58     30     56.83      8.77    -56.65     -8.54     0.181      0.49\n",
      "  85     10     34     36.34      1.72    -36.00     -2.17     0.345      0.93\n",
      "  86     34     50     35.25      1.99    -35.21     -2.04     0.033      0.09\n",
      "  87      6      7    -25.47     15.06     25.51    -15.18     0.035      0.09\n",
      "  88     34     50     35.25      1.99    -35.21     -2.04     0.033      0.09\n",
      "  89     10     34     36.34      1.72    -36.00     -2.17     0.345      0.93\n",
      "  90      7     39    -16.62     14.58     16.63    -14.73     0.019      0.05\n",
      "  91     39     43    -14.64     13.65     14.66    -13.81     0.016      0.04\n",
      "  92     43     57     -8.74     12.68      8.74    -12.74     0.003      0.01\n",
      "  93     55     57      6.04    -12.26     -6.03     12.15     0.004      0.01\n",
      "  94     19     44     61.57     -0.59    -61.30      0.94     0.279      0.75\n",
      "  95      1      5   -344.26    165.88    345.29    -88.71     0.898     76.98\n",
      "  96      1      5   -344.26    165.88    345.29    -88.71     0.898     76.98\n",
      "  97      2      6     10.00     37.29     -9.91    -36.68     0.005      0.49\n",
      "  98      2      6     10.00     37.29     -9.91    -36.68     0.005      0.49\n",
      "  99      0     10    -88.38     16.28     88.57    -11.87     0.049      4.20\n",
      " 100      0     10    -88.38     16.28     88.57    -11.87     0.049      4.20\n",
      "                                                             --------  --------\n",
      "                                                    Total:    42.904    274.13\n",
      "\n",
      "================================================================================\n",
      "|     Voltage Constraints                                                      |\n",
      "================================================================================\n",
      "Bus #  Vmin mu    Vmin    |V|   Vmax    Vmax mu\n",
      "-----  --------   -----  -----  -----   --------\n",
      "    0      -      0.900  1.100  1.100   154.105\n",
      "    1113522.183   1.092  1.092  1.092113714.139\n",
      "    2      -      0.900  1.100  1.100   332.512\n",
      "    3      -      0.900  1.067  1.100      -    \n",
      "    4      -      0.900  1.085  1.100      -    \n",
      "    5      -      0.900  1.019  1.100      -    \n",
      "    6      -      0.900  1.086  1.100      -    \n",
      "    7      -      0.900  1.086  1.100      -    \n",
      "    8      -      0.900  1.084  1.100      -    \n",
      "    9      -      0.900  1.016  1.100      -    \n",
      "   10      -      0.900  1.093  1.100      -    \n",
      "   11      -      0.900  1.030  1.100      -    \n",
      "   12      -      0.900  1.032  1.100      -    \n",
      "   13      -      0.900  1.023  1.100      -    \n",
      "   14      -      0.900  1.038  1.100      -    \n",
      "   15      -      0.900  1.011  1.100      -    \n",
      "   16      -      0.900  1.063  1.100      -    \n",
      "   17      -      0.900  1.054  1.100      -    \n",
      "   18      -      0.900  1.074  1.100      -    \n",
      "   19      -      0.900  1.100  1.100      -    \n",
      "   20      -      0.900  1.031  1.100      -    \n",
      "   21      -      0.900  1.081  1.100      -    \n",
      "   22      -      0.900  1.079  1.100      -    \n",
      "   23      -      0.900  1.067  1.100      -    \n",
      "   24      -      0.900  1.068  1.100      -    \n",
      "   25      -      0.900  1.048  1.100      -    \n",
      "   26      -      0.900  1.021  1.100      -    \n",
      "   27      -      0.900  1.029  1.100      -    \n",
      "   28      -      0.900  1.093  1.100      -    \n",
      "   29      -      0.900  1.026  1.100      -    \n",
      "   30      -      0.900  1.052  1.100      -    \n",
      "   31      -      0.900  1.037  1.100      -    \n",
      "   32      -      0.900  1.038  1.100      -    \n",
      "   33      -      0.900  1.019  1.100      -    \n",
      "   34      -      0.900  1.081  1.100      -    \n",
      "   35      -      0.900  1.092  1.100      -    \n",
      "   36      -      0.900  1.029  1.100      -    \n",
      "   37      -      0.900  1.087  1.100      -    \n",
      "   38      -      0.900  1.031  1.100      -    \n",
      "   39      -      0.900  1.085  1.100      -    \n",
      "   40      -      0.900  1.012  1.100      -    \n",
      "   41      -      0.900  1.017  1.100      -    \n",
      "   42      -      0.900  1.087  1.100      -    \n",
      "   43      -      0.900  1.084  1.100      -    \n",
      "   44      -      0.900  1.095  1.100      -    \n",
      "   45      -      0.900  1.069  1.100      -    \n",
      "   46      -      0.900  1.058  1.100      -    \n",
      "   47      -      0.900  1.025  1.100      -    \n",
      "   48      -      0.900  1.061  1.100      -    \n",
      "   49      -      0.900  1.091  1.100      -    \n",
      "   50      -      0.900  1.080  1.100      -    \n",
      "   51      -      0.900  1.018  1.100      -    \n",
      "   52      -      0.900  1.038  1.100      -    \n",
      "   53      -      0.900  1.045  1.100      -    \n",
      "   54      -      0.900  1.086  1.100      -    \n",
      "   55      -      0.900  1.083  1.100      -    \n",
      "   56      -      0.900  1.017  1.100      -    \n",
      "   57      -      0.900  1.083  1.100      -    \n",
      "   58      -      0.900  1.056  1.100      -    \n",
      "   59      -      0.900  1.061  1.100      -    \n",
      "   60      -      0.900  1.045  1.100      -    \n",
      "   61      -      0.900  1.032  1.100      -    \n",
      "   62      -      0.900  1.095  1.100      -    \n",
      "   63      -      0.900  1.089  1.100      -    \n",
      "\n",
      "================================================================================\n",
      "|     Generation Constraints                                                   |\n",
      "================================================================================\n",
      " Gen   Bus                Active Power Limits\n",
      "  #     #    Pmin mu    Pmin       Pg       Pmax    Pmax mu\n",
      "----  -----  -------  --------  --------  --------  -------\n",
      "   0     1      -  -1000000000.00   -688.521000000000.00      -  \n",
      "   1     2      -  -1000000000.00     20.001000000000.00      -  \n",
      "   2     0      -  -1000000000.00   -176.751000000000.00      -  \n",
      "\n",
      "Gen  Bus              Reactive Power Limits\n",
      " #    #   Qmin mu    Qmin       Qg       Qmax    Qmax mu\n",
      "---  ---  -------  --------  --------  --------  -------\n",
      "  0    1     -  -1000000000.00    331.761000000000.00      -  \n",
      "  1    2     -  -1000000000.00     74.581000000000.00      -  \n",
      "  2    0     -  -1000000000.00     32.561000000000.00      -  \n",
      "\n",
      "================================================================================\n",
      "|     Dispatchable Load Constraints                                            |\n",
      "================================================================================\n",
      "Gen  Bus               Active Power Limits\n",
      " #    #   Pmin mu    Pmin       Pg       Pmax    Pmax mu\n",
      "---  ---  -------  --------  --------  --------  -------\n",
      "\n",
      "Gen  Bus              Reactive Power Limits\n",
      " #    #   Qmin mu    Qmin       Qg       Qmax    Qmax mu\n",
      "---  ---  -------  --------  --------  --------  -------\n",
      "\n",
      "================================================================================\n",
      "|     Branch Flow Constraints                                                  |\n",
      "================================================================================\n",
      "Brnch   From     \"From\" End        Limit       \"To\" End        To\n",
      "  #     Bus   |If| mu    |If|     |Imax|     |It|    |It| mu   Bus\n",
      "-----  -----  -------  --------  --------  --------  -------  -----\n",
      "   0     17      -      101.97    129.56    101.98      -       38\n",
      "   1     13      -       32.55    129.56     32.54      -       26\n",
      "   2     12      -       60.51    129.56     60.48      -       27\n",
      "   3     36      -        0.00    129.56      0.19      -       27\n",
      "   4     61      -       89.60    129.56     89.55      -       29\n",
      "   5     47      -       14.39    129.56     14.36      -       29\n",
      "   6     30      -       53.66    129.56     53.65      -       53\n",
      "   7     31      -       76.45    129.56     76.46      -       52\n",
      "   8     61      -       85.20    129.56     85.23      -       31\n",
      "   9     14      -       72.36    129.56     72.40      -       25\n",
      "  10     32      -       70.96    129.56     70.79      -       33\n",
      "  11      5      -       16.65    129.56     16.74      -       51\n",
      "  12      6      -        5.59    129.56      5.59      -       54\n",
      "  13     10      -       33.30    129.56     33.37      -       34\n",
      "  14     63      -       47.36    129.56     47.41      -       35\n",
      "  15      6      -       32.95    129.56     32.97      -       37\n",
      "  16     11      -       15.42    129.56     15.36      -       38\n",
      "  17      4      -        2.97    129.56      2.91      -       39\n",
      "  18     39      -       18.45    129.56     18.58      -       43\n",
      "  19     55      -       12.62    129.56     12.52      -       57\n",
      "  20      5      -       33.14    129.56     33.35      -       40\n",
      "  21     41      -       15.75    129.56     15.70      -       51\n",
      "  22     41      -        8.11    129.56      8.12      -       56\n",
      "  23      9      -       15.60    129.56     15.51      -       41\n",
      "  24      7      -       20.36    129.56     20.48      -       39\n",
      "  25     37      -        2.60    129.56      2.76      -       42\n",
      "  26      8      -        6.13    129.56      6.11      -       43\n",
      "  27     43      -       14.21    129.56     14.26      -       57\n",
      "  28     19      -       55.99    129.56     55.98      -       44\n",
      "  29      5      -      112.97    129.56    113.00      -       38\n",
      "  30     16      -       49.25    129.56     49.25      -       45\n",
      "  31     46      -       70.96    129.56     70.96      -       48\n",
      "  32     58      -       55.93    129.56     55.92      -       46\n",
      "  33     53      -       13.43    129.56     13.44      -       60\n",
      "  34     52      -       70.59    129.56     70.63      -       53\n",
      "  35     16      -       64.49    129.56     64.49      -       48\n",
      "  36     17      -       92.01    129.56     91.95      -       18\n",
      "  37     10      -       68.41    129.56     68.43      -       44\n",
      "  38     44      -       13.18    129.56     13.52      -       49\n",
      "  39     12      -        2.95    129.56      3.13      -       20\n",
      "  40     22      -        2.99    129.56      2.93      -       50\n",
      "  41      3      -       62.55    129.56     62.52      -       50\n",
      "  42     34      -       32.67    129.56     32.67      -       50\n",
      "  43     21      -        0.00    129.56      0.44      -       34\n",
      "  44     15      -       40.68    124.22     40.20      -       40\n",
      "  45     14      -       72.36    129.56     72.35      -       32\n",
      "  46      5      -       69.41    129.56     69.42      -       33\n",
      "  47     10      -        4.46    129.56      4.59      -       28\n",
      "  48      6      -       27.24    129.56     27.33      -        7\n",
      "  49      5      -       93.49    129.56     93.64      -       29\n",
      "  50     23      -       15.14    129.56     14.97      -       24\n",
      "  51     63      -       57.04    129.56     57.00      -        6\n",
      "  52     62      -       18.30    129.56     18.27      -       10\n",
      "  53      5      -       83.40    129.56     83.58      -       26\n",
      "  54      6      -       81.70    129.56     81.77      -       18\n",
      "  55      3      -       72.44    129.56     72.40      -       25\n",
      "  56     58      -       54.44    129.56     54.48      -       30\n",
      "  57     46      -       42.47    129.56     41.92      -       55\n",
      "  58     48      -        3.30    129.56      3.37      -       59\n",
      "  59      3      -       20.64    129.56     20.74      -       24\n",
      "  60     26      -       82.89    129.56     82.96      -       27\n",
      "  61      5      -       69.41    129.56     69.42      -       33\n",
      "  62     32      -       70.96    129.56     70.79      -       33\n",
      "  63     14      -       72.36    129.56     72.35      -       32\n",
      "  64     34      -       32.67    129.56     32.67      -       50\n",
      "  65     10      -       33.30    129.56     33.37      -       34\n",
      "  66      3      -       72.44    129.56     72.40      -       25\n",
      "  67     14      -       72.36    129.56     72.40      -       25\n",
      "  68      3      -       62.55    129.56     62.52      -       50\n",
      "  69      5      -      112.97    129.56    113.00      -       38\n",
      "  70      5      -       93.49    129.56     93.64      -       29\n",
      "  71     61      -       89.60    129.56     89.55      -       29\n",
      "  72     17      -      101.97    129.56    101.98      -       38\n",
      "  73     61      -       85.20    129.56     85.23      -       31\n",
      "  74      5      -       83.40    129.56     83.58      -       26\n",
      "  75     17      -       92.01    129.56     91.95      -       18\n",
      "  76     26      -       82.89    129.56     82.96      -       27\n",
      "  77     31      -       76.45    129.56     76.46      -       52\n",
      "  78     52      -       70.59    129.56     70.63      -       53\n",
      "  79      6      -       81.70    129.56     81.77      -       18\n",
      "  80     10      -       68.41    129.56     68.43      -       44\n",
      "  81     30      -       53.66    129.56     53.65      -       53\n",
      "  82     58      -       55.93    129.56     55.92      -       46\n",
      "  83     12      -       60.51    129.56     60.48      -       27\n",
      "  84     58      -       54.44    129.56     54.48      -       30\n",
      "  85     10      -       33.30    129.56     33.37      -       34\n",
      "  86     34      -       32.67    129.56     32.67      -       50\n",
      "  87      6      -       27.24    129.56     27.33      -        7\n",
      "  88     34      -       32.67    129.56     32.67      -       50\n",
      "  89     10      -       33.30    129.56     33.37      -       34\n",
      "  90      7      -       20.36    129.56     20.48      -       39\n",
      "  91     39      -       18.45    129.56     18.58      -       43\n",
      "  92     43      -       14.21    129.56     14.26      -       57\n",
      "  93     55      -       12.62    129.56     12.52      -       57\n",
      "  94     19      -       55.99    129.56     55.98      -       44\n",
      "  95      1      -      349.94    350.00    350.00      -        5\n",
      "  96      1      -      349.94    350.00    350.00      -        5\n",
      "  97      2      -       35.10    300.00     34.97      -        6\n",
      "  98      2      -       35.10    300.00     34.97      -        6\n",
      "  99      0      -       81.69    350.00     81.79      -       10\n",
      " 100      0      -       81.69    350.00     81.79      -       10\n"
     ]
    },
    {
     "data": {
      "text/plain": "        vm_pu  va_degree        p_mw      q_mvar     lam_p         lam_q\n0    1.100000  21.255777  176.751969  -32.559138  1.000000  7.400993e-22\n2    1.092000   0.000000  688.520118 -331.755857  2.000000  7.539920e-21\n4    1.100000  20.566608  -19.999826  -74.579263  1.061247  1.695006e-21\n12   1.067333  20.747374   -0.000000   -0.000000  1.072429  1.100420e-01\n14   1.084556  20.691407    3.000000    1.186000  1.061918  7.639524e-02\n..        ...        ...         ...         ...       ...           ...\n124  1.060835  20.368816   -3.370000    1.186000  1.097938  1.739239e-01\n126  1.044952  18.158529  -13.990000    1.186000  1.151930  2.001434e-01\n128  1.031897  15.487999   -9.010000    1.186000  1.217777  2.203168e-01\n130  1.095114  24.337365  -20.010000    1.186000  0.991370  4.483060e-02\n132  1.088852  20.953750  -10.470000    1.186000  1.053620  6.280899e-02\n\n[64 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vm_pu</th>\n      <th>va_degree</th>\n      <th>p_mw</th>\n      <th>q_mvar</th>\n      <th>lam_p</th>\n      <th>lam_q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.100000</td>\n      <td>21.255777</td>\n      <td>176.751969</td>\n      <td>-32.559138</td>\n      <td>1.000000</td>\n      <td>7.400993e-22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.092000</td>\n      <td>0.000000</td>\n      <td>688.520118</td>\n      <td>-331.755857</td>\n      <td>2.000000</td>\n      <td>7.539920e-21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.100000</td>\n      <td>20.566608</td>\n      <td>-19.999826</td>\n      <td>-74.579263</td>\n      <td>1.061247</td>\n      <td>1.695006e-21</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.067333</td>\n      <td>20.747374</td>\n      <td>-0.000000</td>\n      <td>-0.000000</td>\n      <td>1.072429</td>\n      <td>1.100420e-01</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.084556</td>\n      <td>20.691407</td>\n      <td>3.000000</td>\n      <td>1.186000</td>\n      <td>1.061918</td>\n      <td>7.639524e-02</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>1.060835</td>\n      <td>20.368816</td>\n      <td>-3.370000</td>\n      <td>1.186000</td>\n      <td>1.097938</td>\n      <td>1.739239e-01</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>1.044952</td>\n      <td>18.158529</td>\n      <td>-13.990000</td>\n      <td>1.186000</td>\n      <td>1.151930</td>\n      <td>2.001434e-01</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>1.031897</td>\n      <td>15.487999</td>\n      <td>-9.010000</td>\n      <td>1.186000</td>\n      <td>1.217777</td>\n      <td>2.203168e-01</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>1.095114</td>\n      <td>24.337365</td>\n      <td>-20.010000</td>\n      <td>1.186000</td>\n      <td>0.991370</td>\n      <td>4.483060e-02</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>1.088852</td>\n      <td>20.953750</td>\n      <td>-10.470000</td>\n      <td>1.186000</td>\n      <td>1.053620</td>\n      <td>6.280899e-02</td>\n    </tr>\n  </tbody>\n</table>\n<p>64 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')#'1-HV-mixed--0-no_sw'\n",
    "#Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "#TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "#TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "#NETWORK CONSTRAINTS\n",
    "\n",
    "#Maximize the branch limits\n",
    "\n",
    "#max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "#for i in range(len(max_i_ka)):\n",
    "# max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "#Maximize line loading percents\n",
    "max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo loading percent\n",
    "max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Maximize trafo3w loading percent\n",
    "max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "for i in range(len(max_loading_percent)):\n",
    "    max_loading_percent[i] = 100.0\n",
    "net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "#Cost assignment\n",
    "\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "pp.runopp(net,verbose=True)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Network 1-HV-mixed--0-no_sw..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3644\\281605413.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mread_unsupervised_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'1-HV-mixed--0-no_sw'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mread_unsupervised_dataset\u001B[1;34m(grid_name)\u001B[0m\n\u001B[0;32m   1585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1586\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Extracting Node Types for the grid {grid_name}..\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1587\u001B[1;33m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1588\u001B[0m     \u001B[0midx_mapper\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnode_types_idx_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mextract_node_types_as_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1589\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mprocess_network\u001B[1;34m(grid_name)\u001B[0m\n\u001B[0;32m   1267\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mprocess_network\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrid_name\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1268\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Loading Network {grid_name}..\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1269\u001B[1;33m     \u001B[0mnet\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_simbench_net\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrid_name\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# '1-HV-mixed--0-no_sw'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1270\u001B[0m     \u001B[1;31m# OPERATIONAL CONSTRAINTS\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1271\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\simbench\\networks\\extract_simbench_grids_from_csv.py\u001B[0m in \u001B[0;36mget_simbench_net\u001B[1;34m(sb_code_info, input_path)\u001B[0m\n\u001B[0;32m    367\u001B[0m     \u001B[1;31m# --- remove switches if wanted by sb_code_info\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    368\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0msb_code_parameters\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m6\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# remove Switches\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 369\u001B[1;33m         \u001B[0mgenerate_no_sw_variant\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    370\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    371\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mnet\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\simbench\\networks\\extract_simbench_grids_from_csv.py\u001B[0m in \u001B[0;36mgenerate_no_sw_variant\u001B[1;34m(net)\u001B[0m\n\u001B[0;32m    320\u001B[0m     \u001B[1;31m# fuse buses which are connected via bus-bus switches\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    321\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mb1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb2\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mto_fuse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 322\u001B[1;33m         \u001B[0mpp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfuse_buses\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    324\u001B[0m     \u001B[1;31m# replace auxiliary type of buses with no switch connected anymore\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandapower\\toolbox.py\u001B[0m in \u001B[0;36mfuse_buses\u001B[1;34m(net, b1, b2, drop, fuse_bus_measurements)\u001B[0m\n\u001B[0;32m   1230\u001B[0m         \u001B[1;31m# branch elements which connected b1 with b2 are now connecting b1 with b1. these branch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1231\u001B[0m         \u001B[1;31m# can now be dropped:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1232\u001B[1;33m         \u001B[0mdrop_inner_branches\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbuses\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mb1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1233\u001B[0m         \u001B[1;31m# if there were measurements at b1 and b2, these can be duplicated at b1 now -> drop\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1234\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mfuse_bus_measurements\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mnet\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmeasurement\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandapower\\toolbox.py\u001B[0m in \u001B[0;36mdrop_inner_branches\u001B[1;34m(net, buses, branch_elements)\u001B[0m\n\u001B[0;32m   1430\u001B[0m     'to_bus').\n\u001B[0;32m   1431\u001B[0m     \"\"\"\n\u001B[1;32m-> 1432\u001B[1;33m     \u001B[0m_inner_branches\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbuses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"drop\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbranch_elements\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbranch_elements\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1433\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1434\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandapower\\toolbox.py\u001B[0m in \u001B[0;36m_inner_branches\u001B[1;34m(net, buses, task, branch_elements)\u001B[0m\n\u001B[0;32m   1397\u001B[0m         \u001B[0minner\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSeries\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0melm\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1398\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mbus_type\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mbus_types\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1399\u001B[1;33m             \u001B[0minner\u001B[0m \u001B[1;33m&=\u001B[0m \u001B[0mnet\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0melm\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbus_type\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbuses\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1400\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0melm\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"switch\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1401\u001B[0m             \u001B[0minner\u001B[0m \u001B[1;33m&=\u001B[0m \u001B[0mnet\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0melm\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"element\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbuses\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36misin\u001B[1;34m(self, values)\u001B[0m\n\u001B[0;32m   5023\u001B[0m         \u001B[0mdtype\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5024\u001B[0m         \"\"\"\n\u001B[1;32m-> 5025\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0malgorithms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalues\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   5026\u001B[0m         return self._constructor(result, index=self.index).__finalize__(\n\u001B[0;32m   5027\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"isin\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\core\\algorithms.py\u001B[0m in \u001B[0;36misin\u001B[1;34m(comps, values)\u001B[0m\n\u001B[0;32m    522\u001B[0m         \u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhtable\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mismember\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    523\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 524\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomps\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalues\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    525\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    526\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = read_unsupervised_dataset('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_data[0].has_isolated_nodes()\n",
    "#train_data[0].has_self_loops()\n",
    "#train_data[0].is_undirected()\n",
    "x_dict = train_data[0].to_dict()\n",
    "#to_json(train_dict)\n",
    "ln = len(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"][0])\n",
    "print(x_dict[(\"NB\",\"-\", \"NB\")][\"edge_index\"])#[:, :int(ln/2)]\n",
    "x_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = sb.get_simbench_net('1-HV-mixed--0-no_sw')\n",
    "idx_mapper, node_types_as_dict = extract_node_types_as_dict(net)\n",
    "for key in node_types_as_dict:\n",
    "    print(f\"Bus Type: {key}\")\n",
    "    for i in range(len(node_types_as_dict[key])):\n",
    "        #node_types_as_dict[key][i] = idx_mapper[node_types_as_dict[key][i]]\n",
    "        print(str(node_types_as_dict[key][i]))\n",
    "    print(\"-------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_dict[\"PQ\"]['x'][2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_available = []\n",
    "for nw_name in all_simbench_codes:\n",
    "        net = sb.get_simbench_net(nw_name)\n",
    "        print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "        #dict_probs = pp.diagnostic(net,report_style='None')\n",
    "        #for bus_num in dict_probs['multiple_voltage_controlling_elements_per_bus']['buses_with_gens_and_ext_grids']:\n",
    "        #    net.gen = net.gen.drop(net.gen[net.gen.bus == bus_num].index)\n",
    "\n",
    "        #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "        #Set upper and lower limits of active-reactive powers of loads\n",
    "        min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "        p_mw = list(net.load.p_mw.values)\n",
    "        q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "        for i in range(len(p_mw)):\n",
    "            min_p_mw_val.append(p_mw[i])\n",
    "            max_p_mw_val.append(p_mw[i])\n",
    "            min_q_mvar_val.append(q_mvar[i])\n",
    "            max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "        net.load.min_p_mw = min_p_mw_val\n",
    "        net.load.max_p_mw = max_p_mw_val\n",
    "        net.load.min_q_mvar = min_q_mvar_val\n",
    "        net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "        #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "        ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "        pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "        #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "        #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "        #NETWORK CONSTRAINTS\n",
    "\n",
    "        #Maximize the branch limits\n",
    "\n",
    "        #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "        #for i in range(len(max_i_ka)):\n",
    "        # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "        #Maximize line loading percents\n",
    "        max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo loading percent\n",
    "        max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Maximize trafo3w loading percent\n",
    "        max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "        for i in range(len(max_loading_percent)):\n",
    "            max_loading_percent[i] = 100.0\n",
    "        net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "        #Cost assignment\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "        pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "        try:\n",
    "            pp.runpm_dc_opf(net) # Run DCOPP\n",
    "        except pp.OPFNotConverged:\n",
    "            text = \"DC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "            print(text)\n",
    "            continue\n",
    "        print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR DCOPF\")\n",
    "        grids_available.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_available:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) # dcopp on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grids_ready = []\n",
    "for nw_name in all_simbench_codes[6:]:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(\"Trying Network named \" + nw_name + \"...\")\n",
    "\n",
    "    #OPERATIONAL CONSTRAINTS\n",
    "\n",
    "    #Set upper and lower limits of active-reactive powers of loads\n",
    "    min_p_mw_val, max_p_mw_val, min_q_mvar_val, max_q_mvar_val = [], [], [], []\n",
    "    p_mw = list(net.load.p_mw.values)\n",
    "    q_mvar = list(net.load.q_mvar.values)\n",
    "\n",
    "    for i in range(len(p_mw)):\n",
    "        min_p_mw_val.append(p_mw[i])\n",
    "        max_p_mw_val.append(p_mw[i])\n",
    "        min_q_mvar_val.append(q_mvar[i])\n",
    "        max_q_mvar_val.append(q_mvar[i])\n",
    "\n",
    "    net.load.min_p_mw = min_p_mw_val\n",
    "    net.load.max_p_mw = max_p_mw_val\n",
    "    net.load.min_q_mvar = min_q_mvar_val\n",
    "    net.load.max_q_mvar = max_q_mvar_val\n",
    "\n",
    "    #Replace all ext_grids but the first one with generators and set the generators to slack= false\n",
    "    ext_grids = [i for i in range(1,len(net.ext_grid.name.values))]\n",
    "    pp.replace_ext_grid_by_gen(net,ext_grids=ext_grids, slack=False)\n",
    "\n",
    "    #TODO: reactive power limits for gens?\n",
    "\n",
    "\n",
    "    #TODO: reactive power limits for sgens?\n",
    "\n",
    "\n",
    "\n",
    "    #NETWORK CONSTRAINTS\n",
    "\n",
    "    #Maximize the branch limits\n",
    "\n",
    "    #max_i_ka = list(net.line.max_i_ka.values)\n",
    "\n",
    "    #for i in range(len(max_i_ka)):\n",
    "    # max_i_ka[i] = max(max_i_ka)\n",
    "\n",
    "\n",
    "\n",
    "    #Maximize line loading percents\n",
    "    max_loading_percent = list(net.line.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.line.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo loading percent\n",
    "    max_loading_percent = list(net.trafo.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Maximize trafo3w loading percent\n",
    "    max_loading_percent = list(net.trafo3w.max_loading_percent.values)\n",
    "    for i in range(len(max_loading_percent)):\n",
    "        max_loading_percent[i] = 100.0\n",
    "    net.trafo3w.max_loading_percent = max_loading_percent\n",
    "\n",
    "    #Cost assignment\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.gen.name.values))],et=\"gen\", points=[[[0, 20, 1], [20, 30, 2]] for _ in range(len(net.gen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.sgen.name.values))],et=\"sgen\", points=[[[0, 20, 0.25], [20, 30, 0.5]] for _ in range(len(net.sgen.name.values))])\n",
    "    pp.create_pwl_costs(net, [i for i in range(len(net.ext_grid.name.values))],et=\"ext_grid\", points=[[[0, 20, 2], [20, 30, 5]] for _ in range(len(net.ext_grid.name.values))])\n",
    "\n",
    "    #ac_converged = True\n",
    "\n",
    "    #start_vec_name = \"\"\n",
    "    #for init in [\"pf\", \"flat\", \"results\"]:\n",
    "    #    try:\n",
    "    #        pp.runopp(net, init=init)  # Calculate ACOPF with IPFOPT\n",
    "    #    except pp.OPFNotConverged:\n",
    "    #        if init == \"results\":\n",
    "    #            text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \". SKIPPING THIS GRID.\"\n",
    "    #            print(text)\n",
    "    #            break\n",
    "    #        continue\n",
    "    #    start_vec_name = init\n",
    "    #    ac_converged = True\n",
    "    #    break\n",
    "    #if ac_converged:\n",
    "    #    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + start_vec_name + \".\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        pp.runpm_ac_opf(net) # Run DCOPP\n",
    "    except pp.OPFNotConverged:\n",
    "        text = \"AC OPTIMAL POWERFLOW COMPUTATION DID NOT CONVERGE FOR NETWORK \" + nw_name + \".SKIPPING THIS DATASET.\"\n",
    "        print(text)\n",
    "        continue\n",
    "    print(\"GRID NAMED \"+nw_name+\" CONVERGES FOR ACOPF\" + \"WITH THE START VECTOR OPTION \" + \".\")\n",
    "    grids_ready.append(nw_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf on all grids directly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #julia acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nw_name in grids_ready:\n",
    "    net = sb.get_simbench_net(nw_name)\n",
    "    print(nw_name + \": Number of Buses = \" + str(len(net.bus))) #pp acopf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_acopf_available_grid_names = [\"1-HV-mixed--0-no_sw\",\"1-HV-urban--0-no_sw\", \"1-MV-comm--0-no_sw\", \"1-MV-semiurb--0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_acopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf_and_acopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dcopf_available_grid_names = [\"1-HVMV-mixed-all-0-no_sw\", \"1-HVMV-mixed-1.105-0-no_sw\", \"1-HVMV-mixed-2.102-0-no_sw\",\"1-HVMV-mixed-4.101-0-no_sw\", \"1-HVMV-urban-all-0-no_sw\", \"1-HVMV-urban-2.203-0-no_sw\", \"1-HVMV-urban-3.201-0-no_sw\", \"1-HVMV-urban-4.201-0-no_sw\", \"1-HV-mixed--0-no_sw\", \"1-HV-urban--0-no_sw\", \"1-MVLV-rural-all-0-no_sw\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_df_as_list = []\n",
    "for grid_name in dcopf_available_grid_names:\n",
    "    print(\"Generating datasets for grid \" + grid_name)\n",
    "    while len(datasets_df_as_list) != 100:\n",
    "        net = sb.get_simbench_net(grid_name)\n",
    "        df = create_dataset_from_dcopf(net)\n",
    "        if df is not None:\n",
    "            datasets_df_as_list.append(df)\n",
    "            print(str(len(datasets_df_as_list)) + \" Dataset(s) Generated\" + \" for the grid \" + grid_name)\n",
    "\n",
    "    print(\"Saving the datasets for grid \" + grid_name)\n",
    "    path = os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Unsupervised\\\\Training\\\\\" + grid_name\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0, len(datasets_df_as_list)):\n",
    "        newpath = path + \"\\\\Dataset-\" + str(i) + \".csv\"\n",
    "        datasets_df_as_list[i].to_csv(newpath)\n",
    "\n",
    "    datasets_df_as_list.clear()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this revised version of the compute_node_embeddings function, the node features are weighted by the reverse admittance values in the adjacency matrix before they are summed to compute the node embeddings. The resulting node embeddings will reflect the strength of the connections between the nodes.\n",
    "\n",
    "To use the reverse admittance values as the edge weights, you would need to pass the Ybus matrix as the adjacency matrix when calling the compute_node_embeddings function. The Ybus matrix should be converted to a PyTorch tensor before passing it to the function.\n",
    "\n",
    "use the reverse admittance values as edge weights, you can modify the computation of the node embeddings to weight the node features by the reverse admittance values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the node types\n",
    "node_types = ['Slack Node', 'Generator Node', 'Load Node']\n",
    "\n",
    "# Define the number of nodes of each type in the graph\n",
    "num_nodes = {\n",
    "    'Slack Node': 1,\n",
    "    'Generator Node': 20,\n",
    "    'Load Node': 99\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_names = [_ for _ in os.listdir(os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\data\\\\Supervised\\\\\")]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graphdata_lst = read_multiple_supervised_datasets(grid_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_datasets_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "    run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"OPF-GNN-Supervised-Homo\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Homo-GNN\",\n",
    "    \"num_layers\": num_layers,\n",
    "    \"dataset\": grid_names[i],\n",
    "    \"epochs\": 1000,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"output_channels\": out_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"channel type\": layer_type,\n",
    "    \"norm\": \"BatchNorm\",\n",
    "    \"scaler\": scaler\n",
    "\n",
    "    }\n",
    "    )\n",
    "    num_epochs = wandb.run.config.epochs\n",
    "    run.watch(model)\n",
    "    grid_name = graphdata.grid_name\n",
    "    run.config.dataset = grid_name\n",
    "    train_data = graphdata.train_data\n",
    "    run.config[\"number of busses\"] = np.shape(train_data[0].x)[0]\n",
    "    val_data = graphdata.val_data\n",
    "    test_data = graphdata.test_data\n",
    "    test_datasets_lst.append(test_data)\n",
    "    training_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    validation_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "    #test_loader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "    for _ in range(num_epochs):\n",
    "        #train_one_epoch(i, optimizer, training_loader, model, nn.MSELoss(), edge_index, edge_weights)\n",
    "        train_validate_one_epoch(_, grid_name, optimizer, training_loader, validation_loader, model, nn.MSELoss(), scaler)\n",
    "    print(\"Training and Validation finished \" + \"for GraphData \" + str(i) + \".\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader_lst = []\n",
    "val_loader_lst = []\n",
    "test_loader_lst = []\n",
    "for i, graphdata in enumerate(graphdata_lst):\n",
    "\n",
    "    # Divide training data into chunks, load into Dataloaders and append to the list of training loaders\n",
    "    for train_data in divide_chunks(graphdata.train_data, 5):\n",
    "        train_loader_lst.append(DataLoader(train_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Divide validation data into chunks, load into Dataloaders and append to the list of validation loaders\n",
    "    for val_data in divide_chunks(graphdata.val_data, 5):\n",
    "        val_loader_lst.append(DataLoader(val_data, batch_size=1, shuffle=True))\n",
    "\n",
    "    # Append the test data to the list\n",
    "    for test_data in divide_chunks(graphdata.test_data, 5):\n",
    "        test_loader_lst.append(DataLoader(test_data, batch_size=1, shuffle=True))\n",
    "\n",
    "print(\"Data Preparation finished.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr/100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"OPF-GNN-Supervised-Homo\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Homo-GNN\",\n",
    "    \"num_layers\": num_layers,\n",
    "    \"epochs\": 2000,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"in_channels\": in_channels,\n",
    "    \"output_channels\": out_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"channel type\": layer_type,\n",
    "    \"norm\": \"BatchNorm\",\n",
    "    \"scaler\": scaler\n",
    "    }\n",
    "    )\n",
    "\"\"\"\n",
    "for _ in range(wandb.run.config.epochs):\n",
    "    # Training\n",
    "    random.shuffle(train_loader_lst)\n",
    "    print(\"Training the model for epoch \" + str(_))\n",
    "    train_all_one_epoch(_, optimizer, train_loader_lst, model, nn.MSELoss())\n",
    "\n",
    "    # Validation\n",
    "    random.shuffle(val_loader_lst)\n",
    "    print(\"Validating the model for epoch \" + str(_))\n",
    "    validate_all_one_epoch(_, val_loader_lst, model, nn.MSELoss())\n",
    "\n",
    "    outputs = test_all_one_epoch(test_loader_lst, model, nn.MSELoss())\n",
    "print(\"Training and Validation finished.\")\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = test_all_one_epoch(test_loader_lst, model, nn.MSELoss())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output,target = outputs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\" #os.path.dirname(os.path.abspath(\"gnn.ipynb\")) + \"\\\\Models\\\\Supervised\\\\\" + \"basemodel.pt\" #r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\"\n",
    "torch.save(model.state_dict(), \"supervisedmodel.pt\")\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandapower.plotting.simple_plot import simple_plot\n",
    "from pandapower.plotting.plotly.simple_plotly import simple_plotly\n",
    "#simple_plot(net, plot_gens=True, plot_loads=True, plot_sgens=True, library=\"igraph\")\n",
    "simple_plotly(net, map_style=\"satellite\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runpm_ac_opf(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.runopp(net)\n",
    "net.res_bus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx_mapper, node_types_as_dict = extract_node_types_as_dict(net)\n",
    "node_types_as_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_channels = 4\n",
    "hidden_channels = 256\n",
    "out_channels = 4\n",
    "activation = \"elu\"\n",
    "scaler = StandardScaler()\n",
    "num_layers = 5\n",
    "dropout = 0.0\n",
    "jk = \"last\"\n",
    "lr = 0.00001\n",
    "layer_type = \"TransConv\"\n",
    "# torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_channels, hidden_channels, num_layers, out_channels, dropout=dropout, norm=torch_geometric.nn.norm.batch_norm.BatchNorm(hidden_channels),jk=jk,layer_type=layer_type, activation=activation)#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ordered_dict = torch.load(r\"C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\Models\\Supervised\\supervisedmodel.pt\")\n",
    "\n",
    "model.load_state_dict(ordered_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no costs are given - overall generated power is minimized\n"
     ]
    }
   ],
   "source": [
    "pp.runpm_ac_opf(net)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "net = process_network(grid_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " HETEROGENEOUS GNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "index_mappers,_,data = generate_unsupervised_input('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_dict, constraint_dict, _, _, _,scalers_dict = extract_unsupervised_inputs(data)\n",
    "x_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ACOPFOutput(x_dict, scalers_dict, None, index_mappers).output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 2] = 2 * ((core_features[:, 2] - _min_) / (_max_ - _min_)) - 1\n",
      "C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py:2170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  core_features[:, 3] = 2 * ((core_features[:, 3] - _min_) / (_max_ - _min_)) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_9856\\2647757800.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0msave_unsupervised_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'1-HV-mixed--0-no_sw'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36msave_unsupervised_inputs\u001B[1;34m(grid_name, num_samples)\u001B[0m\n\u001B[0;32m   1917\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_samples\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1918\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Epoch: {i}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1919\u001B[1;33m         \u001B[0mindex_mappers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnetwork\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgenerate_unsupervised_input\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrid_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1920\u001B[0m         \u001B[0m_input_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mACOPFInput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnetwork\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex_mappers\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1921\u001B[0m         \u001B[0minputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_input_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mgenerate_unsupervised_input\u001B[1;34m(grid_name, suppress_info)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1748\u001B[0m     \u001B[1;31m# Process the network via Grid Name\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1749\u001B[1;33m     \u001B[0mnet\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_network\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrid_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1750\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1751\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0msuppress_info\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mprocess_network\u001B[1;34m(grid_name, suppress_info)\u001B[0m\n\u001B[0;32m   1255\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0msuppress_info\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1256\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Loading Network {grid_name}..\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1257\u001B[1;33m     \u001B[0mnet\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_simbench_net\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrid_name\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# '1-HV-mixed--0-no_sw'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1258\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1259\u001B[0m     \u001B[1;31m# Sample loads uniformly\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\simbench\\networks\\extract_simbench_grids_from_csv.py\u001B[0m in \u001B[0;36mget_simbench_net\u001B[1;34m(sb_code_info, input_path)\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m     \u001B[1;31m# --- get_extracted_csv_data and convert this data to pandapower net\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 362\u001B[1;33m     \u001B[0mcsv_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_extracted_csv_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrelevant_subnets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    363\u001B[0m     \u001B[0mfilter_unapplied_profiles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcsv_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    364\u001B[0m     \u001B[0mfilter_loadcases\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcsv_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\simbench\\networks\\extract_simbench_grids_from_csv.py\u001B[0m in \u001B[0;36mget_extracted_csv_data\u001B[1;34m(relevant_subnets, input_path, sep, **kwargs)\u001B[0m\n\u001B[0;32m    282\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mtablename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcsv_tablenames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'elements'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'profiles'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'types'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'cases'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    283\u001B[0m             csv_data[tablename] = _get_extracted_csv_table(relevant_subnets, tablename,\n\u001B[1;32m--> 284\u001B[1;33m                                                            input_path=input_path)\n\u001B[0m\u001B[0;32m    285\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mcsv_data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    286\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\simbench\\networks\\extract_simbench_grids_from_csv.py\u001B[0m in \u001B[0;36m_get_extracted_csv_table\u001B[1;34m(relevant_subnets, tablename, input_path, sep)\u001B[0m\n\u001B[0;32m    245\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_get_extracted_csv_table\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrelevant_subnets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtablename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\";\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    246\u001B[0m     \u001B[1;34m\"\"\" Returns extracted csv data of the requested SimBench grid. \"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 247\u001B[1;33m     \u001B[0mcsv_table\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mread_csv_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtablename\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtablename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    248\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mtablename\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"Switch\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    249\u001B[0m         \u001B[0mnode_table\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mread_csv_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtablename\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"Node\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\simbench\\converter\\read_and_write.py\u001B[0m in \u001B[0;36mread_csv_data\u001B[1;34m(path, sep, tablename, nrows)\u001B[0m\n\u001B[0;32m     88\u001B[0m             csv_tables[i] = pd.read_csv(\n\u001B[0;32m     89\u001B[0m                     \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"%s.csv\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 90\u001B[1;33m                     dtype=dict(zip(get_columns(i), get_dtypes(i))))\n\u001B[0m\u001B[0;32m     91\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mFileNotFoundError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m'Node'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Load'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 586\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    587\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    486\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    487\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mparser\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 488\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mparser\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    489\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    490\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1057\u001B[0m             \u001B[0mnew_rows\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1058\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1059\u001B[1;33m         \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcol_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1060\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1061\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_currow\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mnew_rows\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    612\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    613\u001B[0m             \u001B[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 614\u001B[1;33m             \u001B[0mmgr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdict_to_mgr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtyp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmanager\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    615\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMaskedArray\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    616\u001B[0m             \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmrecords\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mmrecords\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001B[0m in \u001B[0;36mdict_to_mgr\u001B[1;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    464\u001B[0m     return arrays_to_mgr(\n\u001B[1;32m--> 465\u001B[1;33m         \u001B[0marrays\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata_names\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtyp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtyp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconsolidate\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    466\u001B[0m     )\n\u001B[0;32m    467\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001B[0m in \u001B[0;36marrays_to_mgr\u001B[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[0;32m    134\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mtyp\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"block\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    135\u001B[0m         return create_block_manager_from_arrays(\n\u001B[1;32m--> 136\u001B[1;33m             \u001B[0marrays\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marr_names\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconsolidate\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconsolidate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    137\u001B[0m         )\n\u001B[0;32m    138\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mtyp\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"array\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001B[0m in \u001B[0;36mcreate_block_manager_from_arrays\u001B[1;34m(arrays, names, axes, consolidate)\u001B[0m\n\u001B[0;32m   1771\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1772\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1773\u001B[1;33m         \u001B[0mblocks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_form_blocks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrays\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconsolidate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1774\u001B[0m         \u001B[0mmgr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBlockManager\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mblocks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1775\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001B[0m in \u001B[0;36m_form_blocks\u001B[1;34m(arrays, names, axes, consolidate)\u001B[0m\n\u001B[0;32m   1837\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mitems_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"NumericBlock\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1838\u001B[0m         numeric_blocks = _multi_blockify(\n\u001B[1;32m-> 1839\u001B[1;33m             \u001B[0mitems_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"NumericBlock\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconsolidate\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconsolidate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1840\u001B[0m         )\n\u001B[0;32m   1841\u001B[0m         \u001B[0mblocks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnumeric_blocks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001B[0m in \u001B[0;36m_multi_blockify\u001B[1;34m(tuples, dtype, consolidate)\u001B[0m\n\u001B[0;32m   1927\u001B[0m         \u001B[1;31m# Type[complex], Type[bool], Type[object], None]\"; expected \"dtype[Any]\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1928\u001B[0m         values, placement = _stack_arrays(\n\u001B[1;32m-> 1929\u001B[1;33m             \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtup_block\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m  \u001B[1;31m# type: ignore[arg-type]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1930\u001B[0m         )\n\u001B[0;32m   1931\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001B[0m in \u001B[0;36m_stack_arrays\u001B[1;34m(tuples, dtype)\u001B[0m\n\u001B[0;32m   1957\u001B[0m     \u001B[0mstacked\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1958\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marr\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrays\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1959\u001B[1;33m         \u001B[0mstacked\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1960\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1961\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mstacked\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mplacement\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "save_unsupervised_inputs('1-HV-mixed--0-no_sw', 1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "inputs = load_unsupervised_inputs('1-HV-mixed--0-no_sw')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs[0].x_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display Plot of Customized Sigmoid Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as p\n",
    "def custom_sigmoid_slope(x, lower_bound, upper_bound, slope=0.1):\n",
    "    # Scale and shift the sigmoid function\n",
    "    scaled_sigmoid = 1 / (1 + torch.exp(-slope * (x - (lower_bound + upper_bound) / 2)))\n",
    "\n",
    "    # Scale and shift the result to fit within the specified lower_bound and upper_bound\n",
    "    scaled_sigmoid = (scaled_sigmoid - 0.5) * (upper_bound - lower_bound) + (lower_bound + upper_bound) / 2\n",
    "\n",
    "    return scaled_sigmoid\n",
    "\n",
    "lst = []\n",
    "min_val = 270\n",
    "max_val = 330\n",
    "for i in range(200,400,1):\n",
    "    val = custom_sigmoid_slope(torch.tensor(i), min_val, max_val, 0.1)\n",
    "    lst.append(val)\n",
    "\n",
    "# Now, plot the values in lst\n",
    "p.plot(range(200, 400, 1), lst)\n",
    "p.xlabel('Input')\n",
    "p.ylabel('Value')\n",
    "p.title('Plot of Enforcing Activation Function')\n",
    "p.grid(True)\n",
    "p.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Heterogeneous Self Supervised Model and Process Inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mcanbay96\u001B[0m (\u001B[33mcbml\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.11 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.10"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\canba\\OneDrive\\Masaüstü\\OPFGNN\\code\\wandb\\run-20230930_215555-nxejzovq</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/nxejzovq' target=\"_blank\">colorful-star-9</a></strong> to <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/nxejzovq' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/nxejzovq</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the Parameters\n",
    "grid_name = '1-HV-mixed--0-no_sw'\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Create ACOPFGNN model and Optimizer\n",
    "model = create_ACOPFGNN_model()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"ACOPF-GNN-SelfSupervised-Hetero\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            #\"learning_rate\": optimizer.,\n",
    "            \"architecture\": \"Hetero-SelfSupervised-GNN\",\n",
    "            \"num_layers\": model.num_layers,\n",
    "            \"epochs\": 1000,\n",
    "            \"grid_name\": grid_name,\n",
    "            \"activation\": model.act_fn,\n",
    "            \"dropout\": model.dropout,\n",
    "            \"in_channels\": model.in_channels,\n",
    "            \"output_channels\": model.out_channels,\n",
    "            \"hidden_channels\": model.hidden_channels,\n",
    "            \"channel type\": \"TransformerConv\",\n",
    "            \"norm\": \"BatchNorm\",\n",
    "            \"scaler\": \"MinMax\"\n",
    "        }\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load Inputs\n",
    "inputs = load_unsupervised_inputs(grid_name)\n",
    "train_inputs = inputs[:750]\n",
    "val_inputs = inputs[750:900]\n",
    "test_inputs = inputs[900:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'SB': tensor([[0., 0., 0., 0., 0., 0., 0.]]),\n 'PQ': tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5959e-03, 3.9876e-03, 1.7753e-02,\n          1.2136e-02],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1203e-02,\n          7.6585e-03],\n         [0.0000e+00, 0.0000e+00, 1.5324e-01, 1.9339e-01, 6.9735e-01, 2.1326e-01,\n          7.6182e-01],\n         [0.0000e+00, 0.0000e+00, 1.0000e+00, 9.9951e-01, 9.9965e-01, 1.0000e+00,\n          1.0000e+00],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3068e-03, 3.7816e-03, 4.8358e-03,\n          3.3057e-03],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0947e-03, 3.6305e-03, 4.2155e-03,\n          2.8817e-03],\n         [0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.8808e-01,\n          9.9185e-01],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4940e-03, 3.9150e-03, 5.3265e-03,\n          3.6411e-03],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7342e-03, 3.3736e-03, 1.3666e-02,\n          9.3417e-03],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3503e-04, 3.8126e-04, 0.0000e+00,\n          0.0000e+00],\n         [0.0000e+00, 0.0000e+00, 1.4600e-01, 1.7985e-01, 6.3260e-01, 1.7692e-01,\n          6.7631e-01],\n         [0.0000e+00, 0.0000e+00, 1.4600e-01, 2.0577e-01, 6.5107e-01, 1.1648e-01,\n          6.3498e-01],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8106e-03, 3.4280e-03, 5.5474e-03,\n          3.7922e-03],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6703e-03, 4.0407e-03, 7.6080e-03,\n          5.2008e-03],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3787e-02, 9.8245e-03, 1.2636e-02,\n          8.6377e-03]]),\n 'PV': tensor([[1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000],\n         [0.4074, 0.4074, 0.8569, 0.1436, 0.8571, 0.1420, 0.8571],\n         [0.0000, 0.0000, 0.0068, 0.9977, 0.0085, 0.9913, 0.0036],\n         [0.0000, 0.0000, 0.0533, 0.9539, 0.0081, 0.9742, 0.0035],\n         [0.0000, 0.0000, 0.0187, 0.9860, 0.0080, 0.9866, 0.0033],\n         [0.0000, 0.0000, 0.4572, 0.5772, 0.0000, 0.8267, 0.0000],\n         [0.0000, 0.0000, 0.0427, 0.9646, 0.0497, 0.9797, 0.0213],\n         [0.0000, 0.0000, 0.3899, 0.6373, 0.0877, 0.8506, 0.0351],\n         [0.0000, 0.0000, 0.1009, 0.9101, 0.0088, 0.9566, 0.0032],\n         [0.0000, 0.0000, 0.0484, 0.9589, 0.0000, 0.9759, 0.0000],\n         [0.0000, 0.0000, 0.0643, 0.9452, 0.0504, 0.9709, 0.0204],\n         [0.0000, 0.0000, 0.0667, 0.9416, 0.0084, 0.9693, 0.0035],\n         [0.0000, 0.0000, 0.3774, 0.6522, 0.0091, 0.8556, 0.0032],\n         [0.0000, 0.0000, 0.0436, 0.9644, 0.0504, 0.9794, 0.0213],\n         [0.0000, 0.0000, 0.0187, 0.9866, 0.0000, 0.9867, 0.0000],\n         [0.0000, 0.0000, 0.1395, 0.8752, 0.0507, 0.9443, 0.0212],\n         [0.0000, 0.0000, 0.0135, 0.9915, 0.0086, 0.9884, 0.0032],\n         [0.0000, 0.0000, 0.0421, 0.9642, 0.0080, 0.9779, 0.0031],\n         [0.0000, 0.0000, 0.0000, 0.9973, 0.1326, 1.0000, 0.0616],\n         [0.0000, 0.0000, 0.0548, 0.9529, 0.0000, 0.9735, 0.0000],\n         [0.0000, 0.0000, 0.1564, 0.8567, 0.0480, 0.9348, 0.0178],\n         [0.0000, 0.0000, 0.0979, 0.9168, 0.0534, 0.9571, 0.0188],\n         [0.0000, 0.0000, 0.0237, 0.9819, 0.0000, 0.9849, 0.0000],\n         [0.0000, 0.0000, 0.0201, 0.9851, 0.0083, 0.9860, 0.0032],\n         [0.0000, 0.0000, 0.0416, 0.9629, 0.0470, 0.9772, 0.0184],\n         [0.0000, 0.0000, 0.0049, 1.0000, 0.0090, 0.9920, 0.0036],\n         [0.0000, 0.0000, 0.0070, 0.9974, 0.0084, 0.9910, 0.0034],\n         [0.0000, 0.0000, 0.0149, 0.9910, 0.0093, 0.9880, 0.0033],\n         [0.0000, 0.0000, 0.1043, 0.9070, 0.0088, 0.9554, 0.0033],\n         [0.0000, 0.0000, 0.1605, 0.8542, 0.0000, 0.9350, 0.0000],\n         [0.0000, 0.0000, 0.0088, 0.9965, 0.0092, 0.9905, 0.0035],\n         [0.0000, 0.0000, 0.0092, 0.9963, 0.0094, 0.9899, 0.0031],\n         [0.0000, 0.0000, 0.0359, 0.9705, 0.0084, 0.9803, 0.0032],\n         [0.0000, 0.0000, 0.0635, 0.9468, 0.0513, 0.9699, 0.0190],\n         [0.0000, 0.0000, 0.0169, 0.9883, 0.0085, 0.9875, 0.0036],\n         [0.0000, 0.0000, 0.1555, 0.8597, 0.0094, 0.9370, 0.0036],\n         [0.0000, 0.0000, 0.0237, 0.9816, 0.0082, 0.9852, 0.0037],\n         [0.0000, 0.0000, 0.0152, 0.9905, 0.0091, 0.9880, 0.0034],\n         [0.0000, 0.0000, 0.0089, 0.9957, 0.0085, 0.9902, 0.0033],\n         [0.0000, 0.0000, 0.0416, 0.9659, 0.0092, 0.9784, 0.0034],\n         [0.0000, 0.0000, 0.0263, 0.9795, 0.0085, 0.9840, 0.0034],\n         [0.0000, 0.0000, 0.0601, 0.9488, 0.0094, 0.9714, 0.0031],\n         [0.0000, 0.0000, 0.0308, 0.9755, 0.0087, 0.9823, 0.0034]]),\n 'NB': tensor([[0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0.]])}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0].constraint_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "###########################################\n",
      "   TRAINING\n",
      "     Training Step: 0 Training Loss: 1148.899658203125 \n",
      "     Training Step: 1 Training Loss: 1140.7869262695312 \n",
      "     Training Step: 2 Training Loss: 1140.4244384765625 \n",
      "     Training Step: 3 Training Loss: 1145.0829772949219 \n",
      "     Training Step: 4 Training Loss: 1144.630224609375 \n",
      "     Training Step: 5 Training Loss: 1144.2822672526042 \n",
      "     Training Step: 6 Training Loss: 1139.7433035714287 \n",
      "     Training Step: 7 Training Loss: 1138.1900329589844 \n",
      "     Training Step: 8 Training Loss: 1136.9691975911458 \n",
      "     Training Step: 9 Training Loss: 1138.1966430664063 \n",
      "     Training Step: 10 Training Loss: 1135.9930641867898 \n",
      "     Training Step: 11 Training Loss: 1136.831034342448 \n",
      "     Training Step: 12 Training Loss: 1137.2754469651443 \n",
      "     Training Step: 13 Training Loss: 1136.9620710100446 \n",
      "     Training Step: 14 Training Loss: 1135.2868489583334 \n",
      "     Training Step: 15 Training Loss: 1133.9073181152344 \n",
      "     Training Step: 16 Training Loss: 1134.8352768841912 \n",
      "     Training Step: 17 Training Loss: 1135.952121310764 \n",
      "     Training Step: 18 Training Loss: 1136.3995297080592 \n",
      "     Training Step: 19 Training Loss: 1136.173077392578 \n",
      "     Training Step: 20 Training Loss: 1135.2641543433779 \n",
      "     Training Step: 21 Training Loss: 1134.1910622336648 \n",
      "     Training Step: 22 Training Loss: 1133.9650029721467 \n",
      "     Training Step: 23 Training Loss: 1133.7769877115886 \n",
      "     Training Step: 24 Training Loss: 1133.075595703125 \n",
      "     Training Step: 25 Training Loss: 1132.0431753305288 \n",
      "     Training Step: 26 Training Loss: 1132.425623010706 \n",
      "     Training Step: 27 Training Loss: 1132.4494105747767 \n",
      "     Training Step: 28 Training Loss: 1132.7084161166488 \n",
      "     Training Step: 29 Training Loss: 1133.4037475585938 \n",
      "     Training Step: 30 Training Loss: 1133.5771287487398 \n",
      "     Training Step: 31 Training Loss: 1132.9691619873047 \n",
      "     Training Step: 32 Training Loss: 1132.806522253788 \n",
      "     Training Step: 33 Training Loss: 1133.001989028033 \n",
      "     Training Step: 34 Training Loss: 1132.8168108258928 \n",
      "     Training Step: 35 Training Loss: 1132.869167751736 \n",
      "     Training Step: 36 Training Loss: 1133.3305070206925 \n",
      "     Training Step: 37 Training Loss: 1132.8673352693256 \n",
      "     Training Step: 38 Training Loss: 1133.0077060797275 \n",
      "     Training Step: 39 Training Loss: 1132.9636474609374 \n",
      "     Training Step: 40 Training Loss: 1133.1043611852135 \n",
      "     Training Step: 41 Training Loss: 1132.9819626581102 \n",
      "     Training Step: 42 Training Loss: 1132.3674600290697 \n",
      "     Training Step: 43 Training Loss: 1132.725752397017 \n",
      "     Training Step: 44 Training Loss: 1132.371226671007 \n",
      "     Training Step: 45 Training Loss: 1132.5637711234715 \n",
      "     Training Step: 46 Training Loss: 1132.317349048371 \n",
      "     Training Step: 47 Training Loss: 1132.198969523112 \n",
      "     Training Step: 48 Training Loss: 1131.6511928013392 \n",
      "     Training Step: 49 Training Loss: 1131.959921875 \n",
      "     Training Step: 50 Training Loss: 1132.0000239353553 \n",
      "     Training Step: 51 Training Loss: 1131.5569481482873 \n",
      "     Training Step: 52 Training Loss: 1131.725275003685 \n",
      "     Training Step: 53 Training Loss: 1131.2095743815105 \n",
      "     Training Step: 54 Training Loss: 1131.1998845880682 \n",
      "     Training Step: 55 Training Loss: 1131.3615504673548 \n",
      "     Training Step: 56 Training Loss: 1131.6400339226973 \n",
      "     Training Step: 57 Training Loss: 1131.995165594693 \n",
      "     Training Step: 58 Training Loss: 1131.5396356097722 \n",
      "     Training Step: 59 Training Loss: 1131.276837158203 \n",
      "     Training Step: 60 Training Loss: 1131.4963759125255 \n",
      "     Training Step: 61 Training Loss: 1131.752155919229 \n",
      "     Training Step: 62 Training Loss: 1131.7612285311259 \n",
      "     Training Step: 63 Training Loss: 1132.22998046875 \n",
      "     Training Step: 64 Training Loss: 1132.4347430889422 \n",
      "     Training Step: 65 Training Loss: 1132.3235011245265 \n",
      "     Training Step: 66 Training Loss: 1132.5061654617537 \n",
      "     Training Step: 67 Training Loss: 1132.164504107307 \n",
      "     Training Step: 68 Training Loss: 1132.1389372452445 \n",
      "     Training Step: 69 Training Loss: 1132.5028390066964 \n",
      "     Training Step: 70 Training Loss: 1132.3256595235475 \n",
      "     Training Step: 71 Training Loss: 1132.6312527126736 \n",
      "     Training Step: 72 Training Loss: 1132.65299757866 \n",
      "     Training Step: 73 Training Loss: 1132.4947526261612 \n",
      "     Training Step: 74 Training Loss: 1132.6428515625 \n",
      "     Training Step: 75 Training Loss: 1132.8167628238075 \n",
      "     Training Step: 76 Training Loss: 1132.9844732903814 \n",
      "     Training Step: 77 Training Loss: 1133.2042032877605 \n",
      "     Training Step: 78 Training Loss: 1133.4066069397745 \n",
      "     Training Step: 79 Training Loss: 1133.6175811767578 \n",
      "     Training Step: 80 Training Loss: 1133.3886115933642 \n",
      "     Training Step: 81 Training Loss: 1133.4647990901296 \n",
      "     Training Step: 82 Training Loss: 1133.190223785768 \n",
      "     Training Step: 83 Training Loss: 1133.271940685454 \n",
      "     Training Step: 84 Training Loss: 1133.4485466452206 \n",
      "     Training Step: 85 Training Loss: 1133.6221668332123 \n",
      "     Training Step: 86 Training Loss: 1133.4265080594469 \n",
      "     Training Step: 87 Training Loss: 1133.1307220458984 \n",
      "     Training Step: 88 Training Loss: 1133.09964503599 \n",
      "     Training Step: 89 Training Loss: 1132.955900065104 \n",
      "     Training Step: 90 Training Loss: 1133.1463421832075 \n",
      "     Training Step: 91 Training Loss: 1133.2697674295177 \n",
      "     Training Step: 92 Training Loss: 1133.511997017809 \n",
      "     Training Step: 93 Training Loss: 1133.4799752742686 \n",
      "     Training Step: 94 Training Loss: 1133.4637001439144 \n",
      "     Training Step: 95 Training Loss: 1133.4803733825684 \n",
      "     Training Step: 96 Training Loss: 1133.318853948534 \n",
      "     Training Step: 97 Training Loss: 1133.5046523736448 \n",
      "     Training Step: 98 Training Loss: 1133.3682084517045 \n",
      "     Training Step: 99 Training Loss: 1133.5003662109375 \n",
      "     Training Step: 100 Training Loss: 1133.6415556350557 \n",
      "     Training Step: 101 Training Loss: 1133.5511941348805 \n",
      "     Training Step: 102 Training Loss: 1133.6403784890776 \n",
      "     Training Step: 103 Training Loss: 1133.772729726938 \n",
      "     Training Step: 104 Training Loss: 1133.8088843936011 \n",
      "     Training Step: 105 Training Loss: 1133.9535741266216 \n",
      "     Training Step: 106 Training Loss: 1133.7664087598569 \n",
      "     Training Step: 107 Training Loss: 1133.9510531955295 \n",
      "     Training Step: 108 Training Loss: 1134.099572417933 \n",
      "     Training Step: 109 Training Loss: 1134.1394941850142 \n",
      "     Training Step: 110 Training Loss: 1134.2869466145833 \n",
      "     Training Step: 111 Training Loss: 1134.3917367117745 \n",
      "     Training Step: 112 Training Loss: 1134.2361120713495 \n",
      "     Training Step: 113 Training Loss: 1133.9744873046875 \n",
      "     Training Step: 114 Training Loss: 1134.0220320991848 \n",
      "     Training Step: 115 Training Loss: 1134.1891437398976 \n",
      "     Training Step: 116 Training Loss: 1133.9496308677217 \n",
      "     Training Step: 117 Training Loss: 1134.065352100437 \n",
      "     Training Step: 118 Training Loss: 1133.8678758124342 \n",
      "     Training Step: 119 Training Loss: 1133.7983530680337 \n",
      "     Training Step: 120 Training Loss: 1133.5688103289644 \n",
      "     Training Step: 121 Training Loss: 1133.7147907194544 \n",
      "     Training Step: 122 Training Loss: 1133.658164419779 \n",
      "     Training Step: 123 Training Loss: 1133.429936562815 \n",
      "     Training Step: 124 Training Loss: 1133.181876953125 \n",
      "     Training Step: 125 Training Loss: 1133.1393461681548 \n",
      "     Training Step: 126 Training Loss: 1133.3166292445867 \n",
      "     Training Step: 127 Training Loss: 1133.3244152069092 \n",
      "     Training Step: 128 Training Loss: 1133.5122448825098 \n",
      "     Training Step: 129 Training Loss: 1133.6038123497597 \n",
      "     Training Step: 130 Training Loss: 1133.7053893577051 \n",
      "     Training Step: 131 Training Loss: 1133.4784342447917 \n",
      "     Training Step: 132 Training Loss: 1133.3067773804628 \n",
      "     Training Step: 133 Training Loss: 1133.118472882171 \n",
      "     Training Step: 134 Training Loss: 1132.955835865162 \n",
      "     Training Step: 135 Training Loss: 1132.9542918485754 \n",
      "     Training Step: 136 Training Loss: 1132.816730582801 \n",
      "     Training Step: 137 Training Loss: 1132.8222532410553 \n",
      "     Training Step: 138 Training Loss: 1132.8942396863758 \n",
      "     Training Step: 139 Training Loss: 1132.8436628069196 \n",
      "     Training Step: 140 Training Loss: 1132.6398267813609 \n",
      "     Training Step: 141 Training Loss: 1132.488154021787 \n",
      "     Training Step: 142 Training Loss: 1132.3059372268356 \n",
      "     Training Step: 143 Training Loss: 1132.1668752034504 \n",
      "     Training Step: 144 Training Loss: 1132.002490234375 \n",
      "     Training Step: 145 Training Loss: 1131.999648838827 \n",
      "     Training Step: 146 Training Loss: 1131.8807364742772 \n",
      "     Training Step: 147 Training Loss: 1131.9611981366131 \n",
      "     Training Step: 148 Training Loss: 1131.8139476392093 \n",
      "     Training Step: 149 Training Loss: 1131.7840030924478 \n",
      "     Training Step: 150 Training Loss: 1131.6303816031148 \n",
      "     Training Step: 151 Training Loss: 1131.5003702264082 \n",
      "     Training Step: 152 Training Loss: 1131.320250268076 \n",
      "     Training Step: 153 Training Loss: 1131.204780083198 \n",
      "     Training Step: 154 Training Loss: 1131.3411227318547 \n",
      "     Training Step: 155 Training Loss: 1131.4465676332131 \n",
      "     Training Step: 156 Training Loss: 1131.3245219819864 \n",
      "     Training Step: 157 Training Loss: 1131.4711921788469 \n",
      "     Training Step: 158 Training Loss: 1131.5050363600628 \n",
      "     Training Step: 159 Training Loss: 1131.5744232177735 \n",
      "     Training Step: 160 Training Loss: 1131.5273043235636 \n",
      "     Training Step: 161 Training Loss: 1131.5605634524497 \n",
      "     Training Step: 162 Training Loss: 1131.411105103288 \n",
      "     Training Step: 163 Training Loss: 1131.3986198611376 \n",
      "     Training Step: 164 Training Loss: 1131.5187063506155 \n",
      "     Training Step: 165 Training Loss: 1131.416857616011 \n",
      "     Training Step: 166 Training Loss: 1131.534033495509 \n",
      "     Training Step: 167 Training Loss: 1131.4356718517486 \n",
      "     Training Step: 168 Training Loss: 1131.4743276742788 \n",
      "     Training Step: 169 Training Loss: 1131.564017980239 \n",
      "     Training Step: 170 Training Loss: 1131.5028404548154 \n",
      "     Training Step: 171 Training Loss: 1131.5832256938136 \n",
      "     Training Step: 172 Training Loss: 1131.5777990087608 \n",
      "     Training Step: 173 Training Loss: 1131.5819253154184 \n",
      "     Training Step: 174 Training Loss: 1131.4986614118304 \n",
      "     Training Step: 175 Training Loss: 1131.5427051890981 \n",
      "     Training Step: 176 Training Loss: 1131.5969293454273 \n",
      "     Training Step: 177 Training Loss: 1131.6779812587781 \n",
      "     Training Step: 178 Training Loss: 1131.543095594012 \n",
      "     Training Step: 179 Training Loss: 1131.691872829861 \n",
      "     Training Step: 180 Training Loss: 1131.808669285221 \n",
      "     Training Step: 181 Training Loss: 1131.9127881395948 \n",
      "     Training Step: 182 Training Loss: 1132.0268608051572 \n",
      "     Training Step: 183 Training Loss: 1132.0058374819548 \n",
      "     Training Step: 184 Training Loss: 1132.078282701647 \n",
      "     Training Step: 185 Training Loss: 1132.1662013556368 \n",
      "     Training Step: 186 Training Loss: 1132.1304383303393 \n",
      "     Training Step: 187 Training Loss: 1132.165159184882 \n",
      "     Training Step: 188 Training Loss: 1132.0643200748182 \n",
      "     Training Step: 189 Training Loss: 1132.1639776932566 \n",
      "     Training Step: 190 Training Loss: 1131.997642956479 \n",
      "     Training Step: 191 Training Loss: 1131.9998423258464 \n",
      "     Training Step: 192 Training Loss: 1132.0003402789023 \n",
      "     Training Step: 193 Training Loss: 1131.9393952359858 \n",
      "     Training Step: 194 Training Loss: 1131.9134102063301 \n",
      "     Training Step: 195 Training Loss: 1131.8392109773597 \n",
      "     Training Step: 196 Training Loss: 1131.7443587404823 \n",
      "     Training Step: 197 Training Loss: 1131.8321169458254 \n",
      "     Training Step: 198 Training Loss: 1131.8334948669126 \n",
      "     Training Step: 199 Training Loss: 1131.7551092529297 \n",
      "     Training Step: 200 Training Loss: 1131.693013812772 \n",
      "     Training Step: 201 Training Loss: 1131.75362041209 \n",
      "     Training Step: 202 Training Loss: 1131.7334122681266 \n",
      "     Training Step: 203 Training Loss: 1131.794064390893 \n",
      "     Training Step: 204 Training Loss: 1131.6532506431022 \n",
      "     Training Step: 205 Training Loss: 1131.5665804668538 \n",
      "     Training Step: 206 Training Loss: 1131.6749650890702 \n",
      "     Training Step: 207 Training Loss: 1131.630623450646 \n",
      "     Training Step: 208 Training Loss: 1131.5136782997533 \n",
      "     Training Step: 209 Training Loss: 1131.5384730747767 \n",
      "     Training Step: 210 Training Loss: 1131.5474222915432 \n",
      "     Training Step: 211 Training Loss: 1131.4970058225235 \n",
      "     Training Step: 212 Training Loss: 1131.5584550597857 \n",
      "     Training Step: 213 Training Loss: 1131.4404159973715 \n",
      "     Training Step: 214 Training Loss: 1131.5001640852108 \n",
      "     Training Step: 215 Training Loss: 1131.3703353316696 \n",
      "     Training Step: 216 Training Loss: 1131.366930983583 \n",
      "     Training Step: 217 Training Loss: 1131.2971023419582 \n",
      "     Training Step: 218 Training Loss: 1131.195169805936 \n",
      "     Training Step: 219 Training Loss: 1131.2763294566762 \n",
      "     Training Step: 220 Training Loss: 1131.3607531241162 \n",
      "     Training Step: 221 Training Loss: 1131.4099291552295 \n",
      "     Training Step: 222 Training Loss: 1131.3065535883197 \n",
      "     Training Step: 223 Training Loss: 1131.3005000523158 \n",
      "     Training Step: 224 Training Loss: 1131.2416981336805 \n",
      "     Training Step: 225 Training Loss: 1131.2716669403346 \n",
      "     Training Step: 226 Training Loss: 1131.3169967046392 \n",
      "     Training Step: 227 Training Loss: 1131.2209751062226 \n",
      "     Training Step: 228 Training Loss: 1131.1420615916688 \n",
      "     Training Step: 229 Training Loss: 1131.0859831436821 \n",
      "     Training Step: 230 Training Loss: 1130.9664665981804 \n",
      "     Training Step: 231 Training Loss: 1130.9994796226765 \n",
      "     Training Step: 232 Training Loss: 1131.0385600732632 \n",
      "     Training Step: 233 Training Loss: 1131.1495063977363 \n",
      "     Training Step: 234 Training Loss: 1131.250454517121 \n",
      "     Training Step: 235 Training Loss: 1131.256217309984 \n",
      "     Training Step: 236 Training Loss: 1131.1786573707807 \n",
      "     Training Step: 237 Training Loss: 1131.1114132664784 \n",
      "     Training Step: 238 Training Loss: 1131.066499207309 \n",
      "     Training Step: 239 Training Loss: 1131.0913248697916 \n",
      "     Training Step: 240 Training Loss: 1131.1007191511605 \n",
      "     Training Step: 241 Training Loss: 1131.1355873297068 \n",
      "     Training Step: 242 Training Loss: 1131.2122119542503 \n",
      "     Training Step: 243 Training Loss: 1131.2629659683978 \n",
      "     Training Step: 244 Training Loss: 1131.3795963209502 \n",
      "     Training Step: 245 Training Loss: 1131.2968129724022 \n",
      "     Training Step: 246 Training Loss: 1131.3475534539473 \n",
      "     Training Step: 247 Training Loss: 1131.3799369565902 \n",
      "     Training Step: 248 Training Loss: 1131.2936188111823 \n",
      "     Training Step: 249 Training Loss: 1131.3389868164063 \n",
      "     Training Step: 250 Training Loss: 1131.4385572942604 \n",
      "     Training Step: 251 Training Loss: 1131.3893699040489 \n",
      "     Training Step: 252 Training Loss: 1131.432299225698 \n",
      "     Training Step: 253 Training Loss: 1131.3922729492188 \n",
      "     Training Step: 254 Training Loss: 1131.420916149663 \n",
      "     Training Step: 255 Training Loss: 1131.45063829422 \n",
      "     Training Step: 256 Training Loss: 1131.4596773733888 \n",
      "     Training Step: 257 Training Loss: 1131.3697452988736 \n",
      "     Training Step: 258 Training Loss: 1131.317447853825 \n",
      "     Training Step: 259 Training Loss: 1131.3427274263822 \n",
      "     Training Step: 260 Training Loss: 1131.384106164691 \n",
      "     Training Step: 261 Training Loss: 1131.3538618014968 \n",
      "     Training Step: 262 Training Loss: 1131.2805598153814 \n",
      "     Training Step: 263 Training Loss: 1131.3059188380387 \n",
      "     Training Step: 264 Training Loss: 1131.2473688089622 \n",
      "     Training Step: 265 Training Loss: 1131.3330151550751 \n",
      "     Training Step: 266 Training Loss: 1131.3793597846443 \n",
      "     Training Step: 267 Training Loss: 1131.358175249242 \n",
      "     Training Step: 268 Training Loss: 1131.2550566152127 \n",
      "     Training Step: 269 Training Loss: 1131.1508938259549 \n",
      "     Training Step: 270 Training Loss: 1131.1688547732645 \n",
      "     Training Step: 271 Training Loss: 1131.215544756721 \n",
      "     Training Step: 272 Training Loss: 1131.2157031607715 \n",
      "     Training Step: 273 Training Loss: 1131.2796702141309 \n",
      "     Training Step: 274 Training Loss: 1131.3105806107953 \n",
      "     Training Step: 275 Training Loss: 1131.258075216542 \n",
      "     Training Step: 276 Training Loss: 1131.1436705881938 \n",
      "     Training Step: 277 Training Loss: 1131.0524441286814 \n",
      "     Training Step: 278 Training Loss: 1131.1262123900929 \n",
      "     Training Step: 279 Training Loss: 1131.0652731759208 \n",
      "     Training Step: 280 Training Loss: 1130.998773649494 \n",
      "     Training Step: 281 Training Loss: 1130.9806786936226 \n",
      "     Training Step: 282 Training Loss: 1131.0037393199261 \n",
      "     Training Step: 283 Training Loss: 1131.0648120289118 \n",
      "     Training Step: 284 Training Loss: 1131.1089612458882 \n",
      "     Training Step: 285 Training Loss: 1131.1225022536057 \n",
      "     Training Step: 286 Training Loss: 1131.0585231448715 \n",
      "     Training Step: 287 Training Loss: 1131.0720409817166 \n",
      "     Training Step: 288 Training Loss: 1131.052333950584 \n",
      "     Training Step: 289 Training Loss: 1131.0888398269128 \n",
      "     Training Step: 290 Training Loss: 1131.145493969475 \n",
      "     Training Step: 291 Training Loss: 1131.1147924971908 \n",
      "     Training Step: 292 Training Loss: 1131.1572344783224 \n",
      "     Training Step: 293 Training Loss: 1131.1948819322652 \n",
      "     Training Step: 294 Training Loss: 1131.2436196537342 \n",
      "     Training Step: 295 Training Loss: 1131.2825280266838 \n",
      "     Training Step: 296 Training Loss: 1131.2271720295403 \n",
      "     Training Step: 297 Training Loss: 1131.2726026701448 \n",
      "     Training Step: 298 Training Loss: 1131.313069117109 \n",
      "     Training Step: 299 Training Loss: 1131.2443603515626 \n",
      "     Training Step: 300 Training Loss: 1131.2747993342505 \n",
      "     Training Step: 301 Training Loss: 1131.331584197796 \n",
      "     Training Step: 302 Training Loss: 1131.3954373098443 \n",
      "     Training Step: 303 Training Loss: 1131.3918276335064 \n",
      "     Training Step: 304 Training Loss: 1131.3636962890625 \n",
      "     Training Step: 305 Training Loss: 1131.294098897697 \n",
      "     Training Step: 306 Training Loss: 1131.2623287039394 \n",
      "     Training Step: 307 Training Loss: 1131.2174254578429 \n",
      "     Training Step: 308 Training Loss: 1131.146133570995 \n",
      "     Training Step: 309 Training Loss: 1131.1612942603326 \n",
      "     Training Step: 310 Training Loss: 1131.1661526106561 \n",
      "     Training Step: 311 Training Loss: 1131.2139755640274 \n",
      "     Training Step: 312 Training Loss: 1131.2152786011131 \n",
      "     Training Step: 313 Training Loss: 1131.1376933687052 \n",
      "     Training Step: 314 Training Loss: 1131.1955500527033 \n",
      "     Training Step: 315 Training Loss: 1131.1139800639091 \n",
      "     Training Step: 316 Training Loss: 1131.0333167235558 \n",
      "     Training Step: 317 Training Loss: 1131.0641233816086 \n",
      "     Training Step: 318 Training Loss: 1130.9954003600117 \n",
      "     Training Step: 319 Training Loss: 1130.919716644287 \n",
      "     Training Step: 320 Training Loss: 1130.8688493294878 \n",
      "     Training Step: 321 Training Loss: 1130.9028737322885 \n",
      "     Training Step: 322 Training Loss: 1130.8498217697852 \n",
      "     Training Step: 323 Training Loss: 1130.8938365041474 \n",
      "     Training Step: 324 Training Loss: 1130.8273189603365 \n",
      "     Training Step: 325 Training Loss: 1130.8789092455904 \n",
      "     Training Step: 326 Training Loss: 1130.9524873972669 \n",
      "     Training Step: 327 Training Loss: 1130.973765303449 \n",
      "     Training Step: 328 Training Loss: 1130.9371790552575 \n",
      "     Training Step: 329 Training Loss: 1130.8561157226563 \n",
      "     Training Step: 330 Training Loss: 1130.8959964625426 \n",
      "     Training Step: 331 Training Loss: 1130.846141033862 \n",
      "     Training Step: 332 Training Loss: 1130.7740060617257 \n",
      "     Training Step: 333 Training Loss: 1130.723095922413 \n",
      "     Training Step: 334 Training Loss: 1130.7539394093983 \n",
      "     Training Step: 335 Training Loss: 1130.7169516427177 \n",
      "     Training Step: 336 Training Loss: 1130.7303256705536 \n",
      "     Training Step: 337 Training Loss: 1130.7118002987472 \n",
      "     Training Step: 338 Training Loss: 1130.7638861282035 \n",
      "     Training Step: 339 Training Loss: 1130.8149776683135 \n",
      "     Training Step: 340 Training Loss: 1130.7530739524148 \n",
      "     Training Step: 341 Training Loss: 1130.7921071191977 \n",
      "     Training Step: 342 Training Loss: 1130.8076278642038 \n",
      "     Training Step: 343 Training Loss: 1130.7821882380995 \n",
      "     Training Step: 344 Training Loss: 1130.7053296959918 \n",
      "     Training Step: 345 Training Loss: 1130.7593404957324 \n",
      "     Training Step: 346 Training Loss: 1130.7162382362212 \n",
      "     Training Step: 347 Training Loss: 1130.7462652798356 \n",
      "     Training Step: 348 Training Loss: 1130.7800233507567 \n",
      "     Training Step: 349 Training Loss: 1130.8100708007812 \n",
      "     Training Step: 350 Training Loss: 1130.8665194171786 \n",
      "     Training Step: 351 Training Loss: 1130.9433600685813 \n",
      "     Training Step: 352 Training Loss: 1130.9704582927584 \n",
      "     Training Step: 353 Training Loss: 1131.0143684344102 \n",
      "     Training Step: 354 Training Loss: 1131.0139366472272 \n",
      "     Training Step: 355 Training Loss: 1130.9342809098491 \n",
      "     Training Step: 356 Training Loss: 1130.9546883206408 \n",
      "     Training Step: 357 Training Loss: 1130.9153097994501 \n",
      "     Training Step: 358 Training Loss: 1130.9385132856023 \n",
      "     Training Step: 359 Training Loss: 1130.9138458251953 \n",
      "     Training Step: 360 Training Loss: 1130.9178889478012 \n",
      "     Training Step: 361 Training Loss: 1130.8486375334512 \n",
      "     Training Step: 362 Training Loss: 1130.7987635588843 \n",
      "     Training Step: 363 Training Loss: 1130.718986092033 \n",
      "     Training Step: 364 Training Loss: 1130.7368013564856 \n",
      "     Training Step: 365 Training Loss: 1130.6710201742872 \n",
      "     Training Step: 366 Training Loss: 1130.6427181566119 \n",
      "     Training Step: 367 Training Loss: 1130.673350126847 \n",
      "     Training Step: 368 Training Loss: 1130.7151689400196 \n",
      "     Training Step: 369 Training Loss: 1130.7013820338893 \n",
      "     Training Step: 370 Training Loss: 1130.6859802081578 \n",
      "     Training Step: 371 Training Loss: 1130.68598593435 \n",
      "     Training Step: 372 Training Loss: 1130.6696266808394 \n",
      "     Training Step: 373 Training Loss: 1130.6919774346172 \n",
      "     Training Step: 374 Training Loss: 1130.7170927734376 \n",
      "     Training Step: 375 Training Loss: 1130.747915714345 \n",
      "     Training Step: 376 Training Loss: 1130.6857401799775 \n",
      "     Training Step: 377 Training Loss: 1130.7366313631571 \n",
      "     Training Step: 378 Training Loss: 1130.7101615966153 \n",
      "     Training Step: 379 Training Loss: 1130.760758570621 \n",
      "     Training Step: 380 Training Loss: 1130.812698003814 \n",
      "     Training Step: 381 Training Loss: 1130.7525698676784 \n",
      "     Training Step: 382 Training Loss: 1130.7350271040714 \n",
      "     Training Step: 383 Training Loss: 1130.766429901123 \n",
      "     Training Step: 384 Training Loss: 1130.817097453328 \n",
      "     Training Step: 385 Training Loss: 1130.734501181489 \n",
      "     Training Step: 386 Training Loss: 1130.77097496013 \n",
      "     Training Step: 387 Training Loss: 1130.800946422459 \n",
      "     Training Step: 388 Training Loss: 1130.8071336133314 \n",
      "     Training Step: 389 Training Loss: 1130.8452289287861 \n",
      "     Training Step: 390 Training Loss: 1130.85051007283 \n",
      "     Training Step: 391 Training Loss: 1130.799971911372 \n",
      "     Training Step: 392 Training Loss: 1130.845467370885 \n",
      "     Training Step: 393 Training Loss: 1130.7924882143282 \n",
      "     Training Step: 394 Training Loss: 1130.771514660799 \n",
      "     Training Step: 395 Training Loss: 1130.7018367882931 \n",
      "     Training Step: 396 Training Loss: 1130.6966792570254 \n",
      "     Training Step: 397 Training Loss: 1130.7579155543342 \n",
      "     Training Step: 398 Training Loss: 1130.8157175776355 \n",
      "     Training Step: 399 Training Loss: 1130.8837966918945 \n",
      "     Training Step: 400 Training Loss: 1130.837678447923 \n",
      "     Training Step: 401 Training Loss: 1130.8933436455418 \n",
      "     Training Step: 402 Training Loss: 1130.984108141575 \n",
      "     Training Step: 403 Training Loss: 1130.9866656312847 \n",
      "     Training Step: 404 Training Loss: 1131.0526035638502 \n",
      "     Training Step: 405 Training Loss: 1131.0514240734683 \n",
      "     Training Step: 406 Training Loss: 1131.0075131727963 \n",
      "     Training Step: 407 Training Loss: 1131.0042401482078 \n",
      "     Training Step: 408 Training Loss: 1130.9789558541222 \n",
      "     Training Step: 409 Training Loss: 1130.9392057093178 \n",
      "     Training Step: 410 Training Loss: 1130.963039723046 \n",
      "     Training Step: 411 Training Loss: 1130.9883997648665 \n",
      "     Training Step: 412 Training Loss: 1131.0115783543622 \n",
      "     Training Step: 413 Training Loss: 1131.0105178022154 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_4260\\4200414919.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Train and Validate Model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mtrain_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_validate_ACOPF\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_epoch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\helper.py\u001B[0m in \u001B[0;36mtrain_validate_ACOPF\u001B[1;34m(model, optimizer, train_inputs, val_inputs, start_epoch, num_epochs, return_outputs)\u001B[0m\n\u001B[0;32m   2032\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2033\u001B[0m             \u001B[1;31m# Make predictions for this batch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2034\u001B[1;33m             \u001B[0mout_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconstraint_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0medge_idx_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0medge_attr_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbus_idx_neighbors_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2035\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2036\u001B[0m             \u001B[1;31m# Compute the loss and its gradients for RMSE loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\code\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1188\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1191\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1192\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\heterognn.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x_dict, constraint_dict, edge_idx_dict, edge_attr_dict, bus_idx_neighbors_dict, check_nans)\u001B[0m\n\u001B[0;32m    198\u001B[0m                     \u001B[0mconstrained_Q_i_upper_bound\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msquare\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmax_apparent_pow\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msquare\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mP_i\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m                     \u001B[0mlearnable_Q_i_constraint\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequires_grad\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 200\u001B[1;33m                     \u001B[0mQ_i\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcustom_sigmoid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mQ_i\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreactive_pow_lower_bound\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconstrained_Q_i_upper_bound\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreactive_pow_lower_bound\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconstrained_Q_i_upper_bound\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mslope\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlearnable_Q_i_constraint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    201\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[0mcheck_nans\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mQ_i\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0many\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    202\u001B[0m                         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Nan Value present in Q_i after S enforcement\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\OneDrive\\Masaüstü\\OPFGNN\\code\\heterognn.py\u001B[0m in \u001B[0;36mcustom_sigmoid\u001B[1;34m(x, lower_bound, upper_bound, slope)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mcustom_sigmoid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlower_bound\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mupper_bound\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mslope\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;31m# Scale and shift the sigmoid function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[0mscaled_sigmoid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mslope\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlower_bound\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mupper_bound\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[1;31m# Scale and shift the result to fit within the specified lower_bound and upper_bound\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train and Validate Model\n",
    "train_output, val_output = train_validate_ACOPF(model, optimizer, train_inputs, val_inputs, start_epoch=1, num_epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a781bdd9e59e4a198588f28e80b84968"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">colorful-star-9</strong> at: <a href='https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/nxejzovq' target=\"_blank\">https://wandb.ai/cbml/ACOPF-GNN-SelfSupervised-Hetero/runs/nxejzovq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230930_215555-nxejzovq\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
